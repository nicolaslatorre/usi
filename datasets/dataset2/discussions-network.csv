0,A,"Sending and receiving data over a network using TcpClient I need to develop a service that will connect to a TCP server. Main tasks are reading incoming messages and also sending commands to the server in ten minutes like a synchronize command. For example I used the TcpClient object as shown below: ... TcpClient tcpClient = new TcpClient(); tcpClient.Connect(""x.x.x.x"" 9999); networkStream = tcpClient.GetStream(); clientStreamReader = new StreamReader(networkStream); clientStreamWriter = new StreamWriter(networkStream); while(true) { clientStreamReader.Read() } Also when I need to write out something in any method I use:  clientStreamWriter.write(""xxx""); Is this usage correct? Or is there a better way? By the way you can use serialization technology to send strings numbers or any objects which are support serialization (most of .NET data-storing classes & structs are [Serializable]). There you should at first send Int32-length in four bytes to the stream and then send binary-serialized (System.Runtime.Serialization.Formatters.Binary.BinaryFormatter) data into it. On the other side or the connection (on both sides actually) you definetly should have a byte[] buffer which u will append and trim-left at runtime when data is coming. Something like that I am using: namespace System.Net.Sockets { public class TcpConnection : IDisposable { public event EvHandler<TcpConnection DataArrivedEventArgs> DataArrive = delegate { }; public event EvHandler<TcpConnection> Drop = delegate { }; private const int IntSize = 4; private const int BufferSize = 8 * 1024; private static readonly SynchronizationContext _syncContext = SynchronizationContext.Current; private readonly TcpClient _tcpClient; private readonly object _droppedRoot = new object(); private bool _dropped; private byte[] _incomingData = new byte[0]; private Nullable<int> _objectDataLength; public TcpClient TcpClient { get { return _tcpClient; } } public bool Dropped { get { return _dropped; } } private void DropConnection() { lock (_droppedRoot) { if (Dropped) return; _dropped = true; } _tcpClient.Close(); _syncContext.Post(delegate { Drop(this); } null); } public void SendData(PCmds pCmd) { SendDataInternal(new object[] { pCmd }); } public void SendData(PCmds pCmd object[] datas) { datas.ThrowIfNull(); SendDataInternal(new object[] { pCmd }.Append(datas)); } private void SendDataInternal(object data) { if (Dropped) return; byte[] bytedata; using (MemoryStream ms = new MemoryStream()) { BinaryFormatter bf = new BinaryFormatter(); try { bf.Serialize(ms data); } catch { return; } bytedata = ms.ToArray(); } try { lock (_tcpClient) { TcpClient.Client.BeginSend(BitConverter.GetBytes(bytedata.Length) 0 IntSize SocketFlags.None EndSend null); TcpClient.Client.BeginSend(bytedata 0 bytedata.Length SocketFlags.None EndSend null); } } catch { DropConnection(); } } private void EndSend(IAsyncResult ar) { try { TcpClient.Client.EndSend(ar); } catch { } } public TcpConnection(TcpClient tcpClient) { _tcpClient = tcpClient; StartReceive(); } private void StartReceive() { byte[] buffer = new byte[BufferSize]; try { _tcpClient.Client.BeginReceive(buffer 0 buffer.Length SocketFlags.None DataReceived buffer); } catch { DropConnection(); } } private void DataReceived(IAsyncResult ar) { if (Dropped) return; int dataRead; try { dataRead = TcpClient.Client.EndReceive(ar); } catch { DropConnection(); return; } if (dataRead == 0) { DropConnection(); return; } byte[] byteData = ar.AsyncState as byte[]; _incomingData = _incomingData.Append(byteData.Take(dataRead).ToArray()); bool exitWhile = false; while (exitWhile) { exitWhile = true; if (_objectDataLength.HasValue) { if (_incomingData.Length >= _objectDataLength.Value) { object data; BinaryFormatter bf = new BinaryFormatter(); using (MemoryStream ms = new MemoryStream(_incomingData 0 _objectDataLength.Value)) try { data = bf.Deserialize(ms); } catch { SendData(PCmds.Disconnect); DropConnection(); return; } _syncContext.Post(delegate(object T) { try { DataArrive(this new DataArrivedEventArgs(T)); } catch { DropConnection(); } } data); _incomingData = _incomingData.TrimLeft(_objectDataLength.Value); _objectDataLength = null; exitWhile = false; } } else if (_incomingData.Length >= IntSize) { _objectDataLength = BitConverter.ToInt32(_incomingData.TakeLeft(IntSize) 0); _incomingData = _incomingData.TrimLeft(IntSize); exitWhile = false; } } StartReceive(); } public void Dispose() { DropConnection(); } } } That is just an example you should edit it for your use.  First of all TCP does not guarantee that everything that you send will be received with the same read at the other end. It only guarantees that all bytes that you send will arrive and in the correct order. Therefore you will need to keep building up a buffer when reading from the stream. You will also have to know how large each message is. The simplest ever is to use a non-typeable ASCII character to mark the end of the packet and look for it in the received data. thanks for your reply.I will try to read bytes in buffer ...In my question also i wonder is that good way listening incoming data with while(true)..  First I recommend that you use WCF .NET Remoting or some other higher-level communication abstraction. The learning curve for ""simple"" sockets is nearly as high as WCF because there are so many non-obvious pitfalls when using TCP/IP directly. If you decide to continue down the TCP/IP path then review my .NET TCP/IP FAQ particularly the sections on message framing and application protocol specifications. Also use asynchronous socket APIs. The synchronous APIs do not scale and in some error situations may cause deadlocks. The synchronous APIs make for pretty little example code but real-world production-quality code uses the asynchronous APIs.  I have had luck using the socket object directly (rather than the TCP client). I create a Server object that looks something like this (I've edited some stuff such as exception handling out for brevity but I hope that the idea comes across.)... public class Server() { private Socket sock; // You'll probably want to initialize the port and address in the // constructor or via accessors but to start your server listening // on port 8080 and on any IP address available on the machine... private int port = 8080; private IPAddress addr = IPAddress.Any; // This is the method that starts the server listening. public void Start() { // Create the new socket on which we'll be listening. this.sock = new Socket( addr.AddressFamily SocketType.Stream ProtocolType.Tcp); // Bind the socket to the address and port. sock.Bind(new IPEndPoint(this.addr this.port)); // Start listening. this.sock.Listen(this.backlog); // Set up the callback to be notified when somebody requests // a new connection. this.sock.BeginAccept(this.OnConnectRequest sock); } // This is the method that is called when the socket recives a request // for a new connection. private void OnConnectRequest(IAsyncResult result) { // Get the socket (which should be this listener's socket) from // the argument. Socket sock = (Socket)result.AsyncState; // Create a new client connection using the primary socket to // spawn a new socket. Connection newConn = new Connection(sock.EndAccept(result)); // Tell the listener socket to start listening again. sock.BeginAccept(this.OnConnectRequest sock); } } Then I use a separate Connection class to manage the individual connection with the remote host. That looks something like this... public class Connection() { private Socket sock; // Pick whatever encoding works best for you. Just make sure the remote // host is using the same encoding. private Encoding encoding = Encoding.UTF8; public Connection(Socket s) { this.sock = s; // Start listening for incoming data. (If you want a multi- // threaded service you can start this method up in a separate // thread.) this.BeginReceive(); } // Call this method to set this connection's socket up to receive data. private void BeginReceive() { this.sock.BeginReceive( this.dataRcvBuf 0 this.dataRcvBuf.Length SocketFlags.None new AsyncCallback(this.OnBytesReceived) this); } // This is the method that is called whenever the socket receives // incoming bytes. protected void OnBytesReceived(IAsyncResult result) { // End the data receiving that the socket has done and get // the number of bytes read. int nBytesRec = this.sock.EndReceive(result); // If no bytes were received the connection is closed (at // least as far as we're concerned). if (nBytesRec <= 0) { this.sock.Close(); return; } // Convert the data we have to a string. string strReceived = this.encoding.GetString( this.dataRcvBuf 0 nBytesRec); // ...Now do whatever works best with the string data. // You could for example look at each character in the string // one-at-a-time and check for characters like the ""end of text"" // character ('\u0003') from a client indicating that they've finished // sending the current message. It's totally up to you how you want // the protocol to work. // Whenever you decide the connection should be closed call // sock.Close() and don't call sock.BeginReceive() again. But as long // as you want to keep processing incoming data... // Set up again to get the next chunk of data. this.sock.BeginReceive( this.dataRcvBuf 0 this.dataRcvBuf.Length SocketFlags.None new AsyncCallback(this.OnBytesReceived) this); } } You can use your Connection object to send data by calling its Socket directly like so... this.sock.Send(this.encoding.GetBytes(""Hello to you remote host."")); As I said I've tried to edit the code here for posting so I apologize if there are any errors in it. I usually expend a thread to create each connection. To do that in the Connection constructor instead of calling BeginReceive() directly do something like this: Thread t = new Thread(new ThreadStart(this.BeginReceive)); t.Start(); Using an approach like this many clients can connect simultaneously. Have I understood your question correctly? You mentioned ""remoting"" but I'm not sure if you are referring to .Net Remoting specifically or if you just mean ""connecting from a remote client"". hi my application schema will connect to a 3rd party tcp server..I have two main tasks.one is sending command(this task can be accomplished with any internal client application in our network) and other task is listening incoming data this can be done with a centralized windows service application.So my question command sender client apps of our network should connect to our windows service app. via remoting to send commands or the client applications should connect to 3rd party tcp server with own tcpclient object to send command directly?thnx Hi thnx for reply..i also have 2 applications to connect 3rd party tcp server..I think one application to send command to 3rd party server and other application(as windows service) will just read incoming bytes..In this statue when i send data on 1st application with an other TcpClient object data is sent but when i read data over with 2nd application tcpclient object incoming data not changes..in this statue do I have to generate all read and write methods in a windows service and client applications should connect with remoting??more than one client can connect to service..as web application Sorry for the delay (I haven't yet figured out how to get email notifications when comments are added.) From your description it does sound as though you could use remoting (or another Windows-centric technology like WCF) to send messages from your client applications to your Windows service then have your Windows service relay those messages using the socket connection. Or you could have the client applications connect directly removing the need for the intermediate Windows service."
1,A,"C++ network programming Hey I would like to expand my knowledge in C++ so the first thing I'm taking on is network programming. I want to make an IRC bot(which hopefully will teach me about socket programming and networking topics) but I have no idea where to start. If anyone could explain to me how IRC bots work and how to make them and direct me to some learning resources that would be really great. Simple snippets as well would be awesome... Thanks! edit: forgot to mention that I use ubuntu so the windows way is not an option Reading a book about sockets and TCP/IP would be favourite. Start with a simple client-server example. It's very easy with Qt framework. For example: server.cpp: #include <QTcpSocket> #include <QTcpServer> int main() { QTcpServer *tcpServer = new QTcpServer(); //creates TCP-based server tcpServer->listen(QHostAddress(""172.16.254.1"")5300); //listen on your IP adress port 5300 while ( tcpServer->isListening() ) //while server is listening { QTcpSocket* tcpSocket; //define TCP-based socket tcpServer->waitForNewConnection(); //server waits for connection if ( (tcpSocket = tcpServer->nextPendingConnection()) ) //if there are connections to be processsed { tcpSocket->write(""hello""6); //write ""hello"" to the socket client is connected to tcpSocket->flush(); } } } client.cpp: #include <QDebug> #include <QTcpSocket> int main() { QTcpSocket *tcpSocket = new QTcpSocket(); //create TCP-based socket tcpSocket->connectToHost(""172.16.254.1""5300); //connect socket to server tcpSocket->waitForConnected(); //wait tcpSocket->waitForReadyRead(); qDebug() << tcpSocket->readAll(); } All you need to do is to run the first program in one terminal window and the second in the other. You will find more Qt network examples here  To understand sockets and use them right you need The Sockets Bible: W. Richard Stevens Unix Network Programming Volume 1: The Sockets Networking API (3rd Edition) You absolutely must have this book before you sit down to write a line of sockets code. Don't leave home without it. Really. Starting around $35 used at Amazon. EDIT: The OP asked about other volumes. Here are two others:   W. Richard Stevens UNIX Network Programming Volume 2: Interprocess Communications (2nd Edition)   W. Richard Stevens TCP/IP Illustrated Vol. 1: The Protocols They are of Stevens's usual and expected superb quality. I don't know what his plans were for integrating all these books I use linux.. That book says Unix. Will that make any difference? @MisterSir - The networking API is virtually identical. Stevens is an excellent book for you. What about the rest of the volumes? Are they relevant? @MisterSir -- I think you should also consider the boost.asio library that @ildjarn recommends. I only just learned about it but it looks very good to me. Much as I think that each of us needs to know sockets it seems from the docs that boost.asio would save you at least a couple of weeks of coding and testing. But you've used it and you don't like it: you must know something that I don't. @MisterSir -- I've added two other books. Take a look; they might be relevant. thank you very much =]  The best guide to learn socket programming in C/C++ must be Beej's Guide to Network Programming by far. It goes through all of the steps you need to know both with examples and detailed description. As far as I know the only information this site lacks is of IPv6 Multicasting.  My recommendations: I'd first write the bot in fast-to-write powerful high-level language such as python. Get used to working with net tools the IRC protocol and stuff. Learn about sockets and networking at low-level. For Unix I'd say take a look at Unix Network Programming. Write your bot in C++! Make mistakes fix them and keep at it. I really know just a bit of Python so if you could just show me some examples that could be helpful. thanks I'm really short on time right now but found you this: http://www.osix.net/modules/article/?id=780 hope it helps  boost.asio is (in my opinion) the de facto standard for writing platform independant networking code in modern C++. Do you think using boost.asio obviates the need to learn the socket API? Or should we recommend that a person write traditional socket programs first and then switch to boost.asio? @Rob Adams : Much as I think one should learn `std::vector<>` and `std::string` before raw pointers/C-arrays and C-strings I think that one should write boost.asio programs first then learn traditional sockets afterwards. I use some of boost's features but I really didn't like boost::asio. Or maybe I just didn't understand it well. +1 I didn't know anything about boost.asio before I saw your answer but after looking at it I think it really is the best way to get something running quickly. Thanks for that very important useful pointer."
2,A,What happens to not accepted connection? Assume a listening socket on localhost:80 and a client connecting using: telnet localhost 80 The problem is that I want only to accept a limited number of concurrent clients assume only one. After that I simply don't accept any. The problem that I saw using: netstat -a is that next client connection was established. Yes I don't process it but on a system level it is there shown as ESTABLISHED client can send data and probably cause extra overhead to the system. The only way I see is to continue accepting clients but disconnect them. Am I right? You should simply close the listening socket when you no longer wish to accept more connections and open it again when you do wish to accept connections. The listen backlog wont help you at all as it's simply for 'half open' connections that the TCP/IP stack has accepted but that the application program hasn't yet accepted.  The listen() function has a backlog parameter that specifies how many outstanding sockets are allowed to hang around in the operating system kernel waiting for the server to accept() them. On my Linux system the man page for listen() says that most of the time the client will get a connection refused error - just the same as if the socket wasn't listening at all. If you only ever want to handle one connection that's fine you can just do: listen(s 0); while ((new_fd = accept(s)) >= 0) { process(new_fd); } It would be somewhat harder if you want to handle more than one. You can't just set the backlog parameter to the number of concurrent connections since the parameter doesn't take into account how many connections are already active.  If you stop listening on that port it should not be allowing any more incoming connections. Make sure the listener closes after accepting the first connection. Two other options: Use Raw Sockets (if you OS supports them). And manually handle the TCP connections. This will involve a lot of extra code and processing though. Use UDP. They are stateless connections but then you will have to accept/reject packets based on something else. This doesn't have the overhead of a TCP connection though. Also you will not be able to use things like telnet for testing. that would do but it will hit performance because we will start/stop listening each time when number of concurrent clients pass the threshold.
3,A,Can a Java applet retain a socket connection when going to a new web page? I have an applet that I want to maintain a socket connection between multiple web pages. Currently when a transition is made to a different page the socket connection is closed. Is there anyway of maintaining that connection between web pages? Or do you have to reconnect the socket after each web page transition? Why do you want to retain it? I have a game server that is managing the users that are logged in. I could reconnect but I was just curious if there is a way without having to reestablish the socket on entry into every new web page. Why are we reloading the page? Can't one manage all the data to be changed in a stream to the applet directly or AJAX javascript? Maybe I'm approaching the problem wrong. Is it better to open and close a socket per network packet transmission? The transmissions won't happen that often. At it's highest frequency the packets might be sent at 1Hz. what are you doing having separate java applets on separate html pages performing separate little functions? It's one applet that must be reload when the web page changes. This causes the socket to disconnect. Unless you can put the applet in a separate frame or load the rest of the page via AJAX the applet is being reloaded when the page is. So no there really isn't a way.  The socket is getting disconnected because the page reloading is causing the applet to run the destroy and other cleanup methods. Even if you did override the destroy and socket disconnects it would be poor programming to not release those resources when a user navigated away from the page. As Ed sugested your best bet is to load the applet in a frame you can go the ajax route but no sense in rewriting the page when you can just split it into different frames and keep the reload in place.
4,A,Increase SO_RSVBUF beyond 262142 bytes in linux In my LAN only based application increasing the transmit and receive buffers has a big impact on performance. Under Linux I can increase the transmit and receive buffer to 262142 bytes. Is it possible to get around this limit without making kernel changes? I'm using ubuntu server and if I compile a custom kernel I'm concerned I'll loose changes if there is an update applied. Can I set the limit somehow perhaps in a shell script when the machine starts? EDIT: I forgot to point out that I'm using UDP. cat /proc/sys/net/core/rmem_max ? See sysctl.conf(5) for details on how to use the /etc/sysctl.conf and /etc/sysctl.d/ configuration for changing net.core.rmem_max and net.core.rmem_default. Heh any luck with the huge socket buffer? I haven't heard of the kernel making such a gross difference in size; something small to round to happy-alignment internal data structures would make sense but doubling the size you asked for seems strange. :) A slight increase in performance. Once you get beyond a certain size increasing it makes no real difference. That makes sense; did you find out why the number from `getsockopt(2)` was twice the number you gave `setsockopt(2)`? Thanks this has made a difference. But oddly enough using setsockopt I don't get what I ask for I now get a larger buffer. Is that a bug? I set it to 1048576 and then check it with getsockopt which returns that the buffer is now 2097150??? Also found man udp and man tcp provide some details. I found the answer in `tcp(7)`: _Note that TCP actually allocates twice the size of the buffer requested in the setsockopt(2) call and so a succeeding getsockopt(2) call will not return the same size of buffer as requested in the setsockopt(2) call._
5,A,"Insert port traffic into SQL table I'm working on a project where a hardware device will be sending small chunks of text data (around every 5 seconds) to a port on the Windows 2003 server. I need to capture that data and insert it into SQL server for later retrieval display analysis etc. Obviously I need something to monitor the port for the incoming traffic and then some way to run an INSERT using it. I'm an advanced asp.net and sql developer but this is a little beyond that scope. Define ""port"". Hardware or network? USB or serial? Three-pronged or two-pronged? Female or male? New York or Newark? Network they'll be sending the data to a specific IP and port on the server. You can write a client application in your language of choice which captures the data from the port and uses SQL to insert it. Since you say you're an expert with ASP.NET this should be fairly simple for you once you're shown the proper classes. You're looking for System.Net.Sockets; there's a lot of fun toys in there. It's not a much different methodology from working with requests in ASP.NET; deep down a request that you handle in ASP.NET was started with a socket. If your hardware device is sending UDP (it should be it's a lot easier to handle both in your client program and on the device itself) receiving that data is trivial. TCP is not much harder. Thanks I'll look around that namespace. It sounds like a little C# console app would be the way to go then. Sorry @JeffS after 2 year waking you up ;). Can you give us an example or at least a URL to an example? That truly makes this process much more easier. Thanks."
6,A,"how to deserialize json to a dictionary or keyvaluepair? I have to deserialize a JSON object like this [{""Key"":{""id"":0 ""Name"":""an Object""} ""Value"":true} {""Key"":{""id"":0 ""Name"":""an Object""} ""Value"":true}] I know how to deserialize arrays and singleobjects or variables. but I'm in the blue about dictionaries. I'm using the following to read an array NetworkEvent n = (NetworkEvent) evt; byte[] data = (byte[]) n.getMetaData(); AnObject[] anObject= null; try { JSONArray json = new JSONArray(new String(data ""UTF-8"")); anObject= AnObject.getAnObjects(json); } catch (Exception ex) { ex.printStackTrace(); } The final code solution:  Object[] objects= new Object[json.length()]; for (int i = 0; i < json.length(); ++i) { Key key= null; Value value = null; try { JSONObject keyValuePair = json.getJSONObject(i); key= Key.getKey(keyValuePair.getJSONObject(""Key"")); value= keyValuePair.getBoolean(""Value""); } catch (JSONException ex) { ex.printStackTrace(); } Object object= new object(); object.setKey(key); object.setValue(value); Objects[i] = object; } return objects; Try the Jackson library. I'm using json.org --Changing libraries in this phase of the project is not an option. But it looks a great library  What you have there is not a JSON object. It is an array of JSON objects and therefore your current code should work. I think that your ""problem"" is that you are using the wrong terminology. This is an attribute or name/value pair: ""id"":0 This is an object: {""id"":0 ""Name"":""an Object""} This is also an object: {""Key"":{""id"":0 ""Name"":""an Object""} ""Value"":true} This is an array (of objects) [{""Key"":{""id"":0 ""Name"":""an Object""} ""Value"":true} {""Key"":{""id"":0 ""Name"":""an Object""} ""Value"":true}] For more details refer to the json.org site. that clarified a lot :)"
7,A,What is the right approach to checksumming UDP packets I'm building UDP server application in C#. I've come across a packet checksum problem. As you probably know each packet should carry some simple way of telling receiver if packet data is intact. Now UDP already has 2-byte checksum as part of header which is optional at least in IPv4 world. Alternative method is to have custom checksum as part of data section in each packet and to verify it on receiver. My question boils down to: is it better to rely on (optional) checksum in UDP packet header or to make a custom checksum implementation as part of packet data section? Perhaps the right answer depends on circumstances (as usual) so one circumstance here is that even though code is written and developed in .NET on Windows it might have to run under platform-independent Mono.NET so eventual solution should be compatible with other platforms. I believe that custom checksum algorithm would be easily portable but I'm not so sure about the first one. Any thoughts? Also shouts about packet checksumming in general are welcome. Do you need to count errors? If the UDP checksum is wrong then any router could just throw the packet away and you'd never get anything. Of course if there's an error in the header (such as destination IP address) you also wouldn't get it. Also the UDP checksum algorithm is completely standardized although the function call to turn checksums on or off does vary by platform although it usually involves setsockopt. @Andrew: That's an overly broad claim which is the main reason that it's false. Perhaps most routers don't verify the checksum but many also do. For example in linux-based routers the usual configuration of netfilter will discard packets with a bad checksum. @Andrew: good point I didn't think of counting errors before. Answer seems obvious now. As of cross-platform implementation System.Net.Sockets.SocketOptionName.ChecksumCoverage appears to be supported on all platforms. Routers don't check the checksum; that's why it makes sense to turn them off in the first place.
8,A,"Does JXTA support direct P2P communication even if both peers are behind NATs? With two JXTA based peers each one behind its own NAT is it possible for them to send direct messages to each other? It is ok to use a rendezvous server for initial connection but the real message with the payload needs to be sent directly from one peer to the other without a ""gateway"" on the internet. I think Skype does that. I have heard about a trick using UDP instead of TCP. But does JXTA support that? Is it even possible with Java? If both peers have a public IP address then yes it is possible. But since this is very rare those peers will need a relay most of the time. JXTA does not support direct connection between NAT-ed peers if they have private addresses. It is possible to achieve this with Java when NATs are traversable but it requires a sophisticated solution for TCP. For UDP it is simpler. thanks for the clarification @JVerstry I thought JXTA could do the direct connection. Instead it couldn't. So what are the alternatives? Let's say if 2 devices are turning on their WIfi but they want to communicate each other without even connected to any network?"
9,A,"Can 'connect' call on socket return sucessfully without server calling 'accept'? Server has created a socket and bound to a port and started a thread which is in loop to accept the connection. Sometime later loop exited due to an exception resulting in thread exit but socket is still bounded to port. Now if client does a 'connect' to this server it is succeeding. How is it possible? If I understand correctly 'connect' returns only after server does 'accept' on the listening socket. Am I missing something here? The connected sockets go into a queue waiting for the receiving process to accept() them. There is a limited backlog of these once it is reached the OS will start either rejecting connections or ignoring them. My question is when ""connect"" call returns? After server calls ""accept"" on connecting socket or when it successfully goes to listening queue? If server doesn't call ""accept""  can we expect ""connect"" in client succeed? You are not correct connect will NOT wait for the server to call accept(). It only waits for a response from the OS which generally queues the request.  If I understand correctly 'connect' returns only after server does 'accept' on the listening socket. Am I missing something here? Yes. TCP establishes the connection - the 3-way handshake - under the covers and puts it in a completed connection queue when it is ready. Accept() returns the next waiting connection from the front of this queue. From the client's perspective it is ""connected"" but it won't be talking to anyone until the server accepts and begins processing. Sort of like when you call a company and are immediately put in the hold queue. You are ""connected"" but no business is going to be done until someone actually picks up and starts talking. Your individual thread may have died but the process is still alive and the file descriptor still open so TCP doesn't know what is going on at the application level. The difference between the phone service hold queue and a TCP connection which has been not-yet accept()ed: whatever bytes are sent down the TCP connection are buffered by the operating system and are received once the call to accept() has been made. On the phone system hold line whatever is said by the caller is discarded. I was debating whether to add that so thanks. Of course there is nothing to say that the phone service *isn't* recording you while you think you are on hold and it is piping out muzak. :) It's also worth pointing out that this is what the `backlog` parameter to the `listen()` call is all about - the backlog is the number of connections that can be in that connnected-but-not-accepted state at any one time. Excellent. Now I understand this behavior better. Thanks a lot Duck and Heath."
10,A,"C# bandwidth monitor excluding local traffic I am trying to find some example code to build a bandwidth monitor to keep tabs on my daily on/off peek download usage. I have a semi working solution but the data it shows includes all data transferred over my LAN as well as via my ISP. Is there some way to separate out LAN from ISP bound traffic in data totals. Thanks. Sounds like you've already got this part covered but I had good results using WinPCap + the SharpPCap wrapper for the actual capture. Both are free. @Matt // Grab the stats for that interface IPv4InterfaceStatistics interfaceStats = nic.GetIPv4Statistics(); but as the comment shows this data is only per interface :/ @romkyns I look into those tonight Im new to c# just started back coding after 5year break... How are you collecting the data from the network? Promiscuous socket? Other? Simply include only the traffic where either the source or the destination IP address is non-local. 192.168.*.* and 10.*.*.* are two such ""local"" address patterns. That would mean tracking all data transferred via the application aside from any impact on the data transfer itself this would cause I was hoping to use the system counters to provide me with the data if its already been stored; don't want to remake the wheel. Maybe manually tracking the data transfers is the only way to do it but I really hope not :/ I suspect the system counters don't let you see this. SharpPCap lets you see the IP addresses with relative ease (although probably not the best way to start coding again after a long break). In any case if you don't want to reinvent the wheel then why not use a bandwidth tracker made by someone else - people at superuser.com should be able to recommend you a bunch. I want to take the monitor a step further after getting it running one one PC i'd like to have it scan the local network for other LAN users with it running and share its information. This would allow true monitoring of a shared line; atm I cant find anything already made to do this. I will give superuser.com a try though ty for the pointer. http://superuser.com/questions/40518/are-there-any-free-bandwidth-meters-available BWMeter looks like it will do the job shame its $30"
11,A,How to read a FSM diagram How do i take this diagram and translate it into a useable program. I'm not really sure how to read this diagram. If someone could just kind of walk me through maybe show an example of code and how it relates to the diagram that be great. Thanks! Circles with text inside are the states. Text describes what the state is. Dashed arrow points to starting state. Outgoing arrows determine where this state could change. Beside of the arrow is the text divided by the line into upper part and lower part. Lower part is actions that should take place when arrow transition is executed. Upper part is conditions. When they are true - this transition is executed (and so lower part). Lambda symbol means you should do nothing except changing current state when transition takes place. So lower parts have coarse corresponding to your functions. And upper parts are the points where you should wait for conditions - polling or async waiting for pending I/O packets whatever. Here is some pseudo-code similar to C (I've written it just here so do not assume it works or even compiles): enum State { WaitFor0Call WaitForAck0 WaitForCall1 WaitForAck1 } int main() { State state = WaitFor0Call; while (1) { switch (state) { case WaitFor0Call: if (rdt_rcv(rcvpkt)) continue; if (rdt_send(data)) { state = WaitForAck0; sndpkt = make_pkt(0 data checksum); udt_send(sndpkt); start_timer(); } break; case WaitForAck0: // ...similar code... break; case WaitForCall1: // ...similar code... break; case WaitForAck1: // ...similar code... break; } } } You should also take into account that receive and send functions could be blocking so code if (rdt_rcv(rcvpkt)) whatever; is technically incorrect as you don't check for rdt_send until it returns control. So FSM communicates only logical flow not technical aspects of how it should be organized thread management etc. And my code doesn't show this aspects also because it could be decently complicated depending on your needs and because you didn't give enough details to make informed advices on these sort of things :) My only guess is that you would have some sort of bi-directed stream (for input and output respectively) and conditions would be like if (there_is_ready_for_consuming_packet_in_the_input_queue) continue; and if (data_was_put_to_outgoing_stream_successfully) ...; Hmmm ok. That helps. I'll see what i can come up with. I'm really just trying to implement RDT 3.0. I've implemented some low-level network protocols before but unfortunately I'm unfamiliar with RDT. So I could only share with you general info about FSMs. Well shared already :)
12,A,Limiting upload speed on Java? I'd like to programmatically limit an upload or download operation in Java. I would assume that all I'd need to do was do check how fast the upload is going and insert Thread.sleep() accordingly like so: while (file.hasMoreLines()) { String line = file.readLine(); for (int i = 0; i < line.length(); i+=128) { outputStream.writeBytes(line.substr(i i+128).getBytes()); if (isHittingLimit()) Thread.sleep(500); } } Will the above code work? If not is there a better way to do this? Is there a tutorial which describes the theory? That will probably slow down the up/down load rate. Although depending how much data is sent on each iteration and the length of the sleep period it might be hard to limit accurately (i.e. the connection speed will change but it is not clear by how much). To make it finer tuned in theory you would want to reduce both the amount of data sent at each iteration (from a line to a character for example) and the amount of time spent resting when a limit is hit. This should result in more frequent and finer adjustments... not posting as an answer since I have no tutorials and have not verified it. Also: it's not a good idea to use a Reader nor read by line since you are basically decoding data from bytes (UTF-8 or such) into characters just to re-encode take some of bytes drop others. Rather read certain number of bytes from InputStream send those and avoid decoding/encoding that is not needed here. You'll need some way for isHittingLimit to know how many bytes have been transmitted over how long. There's an interesting approach in this thread that you may be able to adapt.  Token Bucket Algorithm is a way to limit an upload or a download's bandwidth. You should read this article : it explains the use of this algorithm.
13,A,"Where can I get started with basic online functionality for a game in VB.NET? I am creating a Boggle clone game and it works very well so far. Right now it is only single player. I would like to make it so that other players can connect and show up in a listbox and then I can select that person and play a 3 min round with them. What would be the best way to accomplish this? I know a lot about VB.NET but I have little experience with networking. In terms of a server what would I need? I want it to be as simple as possible. All I want to send to the other person is the array of 16 letters and I want to receive the player's score every time the timer ticks. Now I really fixed the question. Play nice children. ""My friend and I really like Boggle"" - good for you! but not a real question.... @Mitch Wheat are you honestly saying that this is not a question? There fixed the question :-) Participants in networked games most commonly connect indirectly to each other via a server but that is certainly not required. If you want to start simple I'd suggest using TCP sockets to connect the two players. Send simple game messages over the sockets. Test two instances of your process on the same machine first. Then once working test across different networks. That's when you'll come across firewall and routing issues. From there you can configure your router to let through the traffic or try to tunnel through SSH or use a server. In short: start simple with TCP sockets on the same machine. Go from there. Honestly Rich why on earth would you edit out my signature? Signatures and taglines are discouraged on StackOverflow.com. Do you have a citation? @Kent: Not like I should need to produce one but: http://meta.stackexchange.com/questions/2950/ The top-ranked answer disagrees with what you did which is to edit only to remove my signature. Linking to a question that you asked with an answer that you accepted is hardly an authoritative source. @Kent: Had to get your attention somehow. Now you know to stop this behavior. Well I've gotten it to send messages accross my router but when I use my real IP (67.68....) it doesn't work how could I solve this. Thanks @unknown: Post it as a new question with new details."
14,A,"Need of Formula for Accurate Bandwith for 1 Gigabit NIC Card I am Need of Formula to Accurately Calculate Bandwith for 1 Gig Nic Card. What i am doing is send Layer 2 Packets @ 1Gbps but my software is showing 6oo Mbps. The whole experiment is Back to Back. No switch No Router. Here is what i did. // LinkSpeed = 1Gb UINT nBandwidth = LinkSpeed/100;//Mbps nBandwidth = nBandwidth/8; //Bytes/sec nBandwidth = nBandwidth/FrameLength; //Frames/Sec. Frame Length = 1518 UINT FramesPerBurst = (nBandwidth*Sleeptime)/1000; //Frames/Burst UINT nBufferSpaceNeededPerFrame = FrameLength-4 + sizeof(dump_bpf_hdr)); UINT nTxBufferSize = FramesPerBurst * nBufferSpaceNeededPerFrame; unsigned char* pTxBuffer = new unsigned char[m_nTxBufferSize]; If you're really doing all those calculation using integers you're going to be getting some strange results... The hardware which talks to the cable will 99.999% certainly be capable of doing the full 125MBytes/second and a lightly-loaded switch will probably keep up too. Unless you have a disaster you won't be seeing any significant error rate on the wire either. Your real performance is most likely affected by the platform you're using to transmit the packets - you don't say much about that.  In ethernet you also have to take into account the interframe gap which is at minimum 96 quantum time that is the quantum time being the time to send a bit which is 1ns in GigaEthernet (1 second / 1000000000). Also if you get a collision there will be backoff time which quantum is chosen randomly between 0 and 2^<nb collisions> - 1. A minor point - you won't get a collision on Gigabit because *all* real-world GigE systems are full-duplex. @WillDean what about when two unrelated stations try to transmit simultaneously? I don't know about GigE but if it's just faster ethernet then that's also a possible collision. Transmit simultaneous on which shared medium? mahesh uses a straight connection and otherwise your GigE NIC connects to a port on a switch. In either case there are no more than two full-duplex transmitters per cable segment. Not necessarily a switch. If they're all on a hub they're basically sharing the same line that's why promiscuous mode works on NICs to let them see all traffic. In that case two can transmit at once which is why ethernet has the back-off timers.  Just because your card is a 1 Gigabit card that doesn't mean you will get that entire speed. On top of what Mat said you have to worry about signal attenuation and interfearence. If the router or switch gets congested this will also slow down your transfer speed. No formula can give you a completely accurate number for real world data transfer rates.  First you need a ""long"" at a minimum to store the no. of frames you received. To calculate the bandwidth being used a = GetIntfCounters() Start a timer (timeout) in seconds b = GetIntfCounters() Pkts/sec = (b - a)/timeout Bits/sec = (Pkts/sec * pktsize) Bytes/sec = (Bits/sec)/8 The GetIntfCounters() would depend on the software platform that you are using. Instead of a timer you can use a sleep for a given interval and then calculate the pps over that interval. However the only realistic calculation of bandwidth at which you device is receiving frames will be if you take interface counters into consideration."
15,A,"Maintaining a database externally in a text file What is the simplest way of maintaining a txt based database file that allows the program the write in new or edit existing entry during execution time. to be specific the program must be capable of storing a client ip and port when it logs in and remove accordingly when the client logs out without using ""internal"" approaches like linkedlists. EDIT: thanks first to the suggestions however there is a restriction in the file that i forgot to mention the file must be in .txt format. the exact format is User Name IP Address Port Number Alice 122.33.44.23 1045 Bob 121.23.12.34 1078 which different field must be separated by a no i just want to know what tools are available to achieve the result as my tutor failed to even read my emails. I dont have i have abused the use of SO because 1) i have googled for solutions before posting here; 2) i'm not asking any of you to provide the exact codes as solutions. no i just want to know what tools are available to achieve the result as my tutor failed to even read my emails. I dont think i have abused the use of SO because 1) i have googled for solutions before posting here; 2) i'm not asking any of you to provide the exact codes as solutions This question just sounds as if you want SO to write the code for your homework assignment for you. better use xml then or json text format. I've used JAXB to serialize modest-sized data structures (e.g. lists) to XML file and back. Advantages: JAXB is part of Java SE 6 no extra libraries needed. (De)serialization takes about two lines of code. XML is human readable. Of course it's no replacement to any real database. For instance any change in the data forces you to rewrite the whole file - but for small files it really doesn't matter. Sometimes it's simplicity that's needed.  It's not Java but MySQL supports a CSV table type. http://dev.mysql.com/doc/refman/5.1/en/csv-storage-engine.html MySQL would handle all the hard stuff like locking and multiple access but leaving you with a file you could read directly without MySQL.  You might want to look at data-serialization methods such as JSON Yaml or a lightweight (file-based but not human-readable) database such as SQLite. JSON/YAML are text based so thats easy. Just make sure your reads/writes are serialized and you implement any locking needed and they should suit fine.  Installing a database as several other posters have suggested will buy you certain things such as transaction security and the possibility of expanding what you store (at the moment you need just IP/port but maybe later you'll store more things and maybe more permanently?) However if your requirements are going to remain as simple as you state then I'm going to controversially suggest that using a SQL database isn't the simplest solution (even though as I say for certain requirements a database does buy you certain things). A very simple solution would be simply to have some directory in which every time a client logs in you create a file whose name encodes the information you want (or a hash of identifying information and store extra info in the file). Then when the client logs off you delete the file. Issues you'll need to be careful of include what happens when your app exits abnormally splitting among several directories if you have more than say a couple of thousand clients (Windows in particular seems to go ape if you have too many files in a directory even though principle you should be able to store as many as you like) and managing filing system ""issues"" (the virus checker is accessing a file just as you need to delete...). This simple solution isn't actually as bad as it sounds: the filing system is actually designed to access and index things efficiently just like a database.  I would go also with hsql which has a mode to store data in a text file http://hsqldb.org/ still the db text file is not a simple txt file it contains some sql statements with the data. http://hsqldb.org/web/hsqlDocsFrame.html sample TEXT database file: INSERT INTO YOUR_TABLE VALUES('CLIENT1''xyz' .....) INSERT INTO YOUR_TABLE VALUES('CLIENT2''xyz' .....) I forgot to say the HSQL runs in embedded mode you dont need to set up any database server! this is a big advantage for simplicity As said above use HSQL its a good embedded DB. Its easier to do this than to use a on the fly reinvent the wheel mechanism.  The easiest way is to maintain it in a database of some sort. You have not specified the requirements for the file format but the obvious choice would be a CSV text format. The H2 Database in particular has support for CSV tables.  You can use JDBC-ODBC bridge driver and link to a TEXT-BASED ODBC connection. After that you can just use JDBC to select insert update or delete entries on the file. But even though that will work I would recommend you using HSQL SQLite or any other lightweight database though. It will be faster and more reliable. Good luck. so there is no other way of accomplishing the requirement? coz my work is about network programming and i honestly dont believe i can learn about network programming by messing databases around.  I suggest JavaDB. Many features such as embedded support and mobile support that is available."
16,A,"Passing structures using sento() How do I pass following message format from a UDP client to a UDP server? ----------------------------- |Ver|p|fff| length| checksum| ---------------------------- | Customer id | ----------------------------- | Amount | ----------------------------- |other | ----------------------------- how do I write raw data from a structure onto the wire? If you want reliablility don't use UDP. Attempts to make UDP reliable basically involve re-implementing TCP which is not a sensible thing to do. Why don't you just use tcp if you need reliability? You've got problem with declaring scruct with specified fields filling instance of it and writing it to a buffer? http://stackoverflow.com/questions/107668/what-do-you-use-when-you-need-reliable-udp OP doesn't specify anything about reliability. I removed that part because it diluted my main concern. I'm assuming you have a structure for this. You will need to handle any big/little endian issues if you are transmitting to different architectures. 1) Either copy the structure to a buffer and pass the pointer to the buffer to sento // rough example (not tested) unsigned char buffer[1024]; struct yourstruct_t data; memcpy_s(buffer sizeof(buffer) &data sizeof(yourstruct_t)); sendto(socket buffer sizeof(yourstruct_t) ...); 2) Pass the struct directly to sento // rough example (not tested) struct yourstruct_t data; sendto(socket (unsigned char *) &data sizeof(yourstruct_t) ...); Check out this Socket tutorial.  See related question/answers here: http://stackoverflow.com/questions/1229321/sending-structure-using-recvfrom-and-sendto Keep in mind that if the client and server have different machine architectures (Big Endian vs Little Endian) or even different compilers/interpreters then sending a raw structure is not a good idea. I have seen cases where machines of the same architecture did not view a structure layout the same because the compilers used for the client and server code had optimized the struct storage differently. So instead of sending the whole struct consider encoding each field into a buffer using htons() htonl() for integers longs etc. Then send that buffer rather than the original struct. On the server side decode the received buffer using ntohs() ntohl() etc.. to reconstruct the struct. Using UDP you will have to be aware that the network may lose the message. If your client and server on on the same local LAN the chances of a lost packet are low. If the client and server are talking across the Internet then the chances go up considerably. You can add acknowledgment messages and timeouts but then you are starting down the path of re-inventing a reliable transport like TCP (e.g. you need to handle cases where the original message made it but only the acknowledgement was lost.) Not a terrible thing to do but just be aware of what you are getting into. You can instead use a TCP connection and possibly keeping it open to exchange more information between client and server. In this case you will probably want to add some delimiter values between messages and ""escape"" those delimiters when they occur within the message buffer payloads. The reason for this is that TCP really gives you a bidirectional byte stream so you have to be careful about where one message ends and the next begins. UDP is ""message oriented"" such that one recvfrom will get you one complete message whereas with TCP when you read bytes from the socket you could be reading the tail end of one message and the first few bytes of the next message thus the need for delimiters.  As others have pointed out though you need to be very careful with endianess (especially with the length field) when sending across platforms. You also need to be careful with structure packing and alignment I believe. I would create the following structure and cast it as a char * in sendto/recvfrom. You weren't specific about the field types so I assumed unsigned char. //assume the following structure on both the server and client #pragma pack(1) //windows only typedef struct _data{ unsigned char Ver; unsigned char p; unsigned char fff; unsigned short length; unsigned short checksum; unsigned long Customer_id; unsigned long Amount; unsigned char other[1485]; //1500 (normal MTU)-15 used } data; //on the server data data_to_send; data_to_send.length=sizeof(data); data_to_send.Ver=1; data_to_send.p=1; //[...] fill in other data_to_send fields here data_to_send.length += 64; //just an example of using 64 bytes in ""other"" data_to_send.checksum=calcChecksum(data_to_rxdata_to_rx.length); sendto(socket (char *) &data_to_send data_to_send.length ...); //on the client data data_to_rx; unsigned short checksum; n=recvfrom(socket(char *)&data_to_rxsizeof(data)...); //validate the incoming data_to_rx fields if(data_to_rx.Ver!=1) //drop it checksum=calcChecksum(data_to_rxdata_to_rx.length); if(checksum!=data_to_rx.checksum) //drop it [...]  You have two choices. 1) You can marshall the structure into a wire format before sending and then unmarshall it after receiving. Unless you are sending a lot of data (or sending a little data very often) the overhead should not be noticeable. 2) Write the raw data onto the wire. If you do this you'll want to make sure any numbers are in network order (see ntohl() htonl() ntohs() htons()). how do I write raw data from a structure onto the wire?  You will need to emulate parts of TCP see all of section 5 on that FAQ. That said once you've done that look into using sendto how you format the data is entirely up to you and how you intend to process it but keep in mind that sockets are very good at sending byte-streams and not much else. why do you think the OP needs to emulate parts of TCP?"
17,A,"Java : Socket programming example What is wrong with the following program to fetch time from time server. public class SocketTest { public static void main(String[] args) { Socket s = new Socket(“time-A.timefreq.bldrdoc.gov” 13); BufferedReader in = new BufferedReader( new InputStreamReader(s.getInputStream())); String line; do { line = in.readLine(); //read line of text from socket if (line != null) System.out.println(line); } while(line != null); } } The quotation-marks should be "" instead of ”. That is you should have Socket s = new Socket(""time-A.timefreq.bldrdoc.gov"" 13); rather than Socket s = new Socket(“time-A.timefreq.bldrdoc.gov” 13); Also you'll need to encapsulate the IO operations in a try/catch block or declare the method to throw IOException. Other than that I don't have much complaints. If you import the classes properly it will print something like 55486 10-10-17 05:30:44 22 0 0 604.7 UTC(NIST) * This is roughly how I would have written it import java.io.*; import java.net.*; public class SocketTest { public static void main(String[] args) { try { Socket s = new Socket(""time-A.timefreq.bldrdoc.gov"" 13); BufferedReader in = new BufferedReader(new InputStreamReader( s.getInputStream())); String line; while ((line = in.readLine()) != null) System.out.println(line); s.close(); } catch (IOException ioex) { ioex.printStackTrace(); } } } Put the s.close() inside a finally block if you're picky and the program is larger than just a test-program like this one. oops! thanks @aioobe s.close() should probably be inside the finally block. Yes I agree. :-)  You need to use double quotes ("") to enclose the strings looks like you are using inverted quotes: new Socket(“time-A.timefreq.bldrdoc.gov” 13); ^ ^"
18,A,"File decriptors and socket connections I am trying to understand how are file descriptors related to sockets. As per my understanding you listen on a particular file descriptor once a connection comes in  you accept it  which returns you another file descriptor ( 2 in all ) and you use this 2nd descriptor to send/recv data. The strange behaviour i am observing is that after accept  i have 3 file descriptors instead of two.... and i am not sure why is this the case.... I am either using lsof or /proc/pid to observe the increase in number of fd's. ps : these are af_unix sockets. EDIT : CODE Here is the code to create the scoket.  int s s2 len; socklen_t t; struct sockaddr_un local remote; if ((s = socket(AF_UNIX SOCK_STREAM 0)) == -1) { syslog(LOG_ERR""Failed to create a socket""); exit(1); } int flags = fcntl(s F_GETFD); if (flags == -1) { syslog(LOG_ERR""Failed to get socket flags""); exit(1); } flags |= FD_CLOEXEC; if (fcntl(s F_SETFD flags) == -1) { syslog(LOG_ERR""Failed to set socket flags""); exit(1); } local.sun_family = AF_UNIX; strcpy(local.sun_path SOCK_PATH.c_str()); unlink(local.sun_path); len = strlen(local.sun_path) + sizeof(local.sun_family); if (bind(s (struct sockaddr *)&local len) == -1) { syslog(LOG_ERR""Failed to bind socket""); exit(1); } if (listen(s 5) == -1) { syslog(LOG_ERR""Failed to listen at socket""); exit(1); } Code where connection is accepted  while (1) { stat =0; execReturn=0; t = len; read_fds = master; if (select(fdmax+1 &read_fds NULL NULL &tv) != -1) { if(FD_ISSET(s&read_fds)) { //Accept new connection //fork child -> fork grand child //child will return value back if ((s2 = accept(s (struct sockaddr*)&remote &t)) == -1) { syslog(LOG_ERR""Failed to acceptconnection at socket""); exit(1); } I am stepping through gdb and exactly after accept  the fd's become 3. The OS is fedora core 13. The reason i need to validate this is i do not want my process to hold on to FD's ; since being a daemon over time it may walk the system into a corner... This did seem odd behaviour. After closing the accepted connection i am still left with two fd's . i.e. one for listen and one ghost fd... Whats even more strange is that even if 10 connections are made  only one ghost fd remains at the end of all of them closing.... It does sound like OS specific implementation.. Cheers! it sounds like gdb is up to something. Hard to say without more code and evidence of the symptoms you report. (Do you syslog right after the accept? That'd burn an fd.) However `lsof` and `/proc/$pid/fd` should let you determine whether that unexpected fd is a duplicate of the bound or accepted socket... Is there a small snippet of code you have that reproduces this problem? You are right in that it's two. You must be confusing the third with something else. Without more information it's hard to help.  The easiest way to debug this sort of issues is to run your app under the strace(1) utility. Check what system calls are made what the parameters and return values are and correlate that to file descriptors used.  More code please. But I am guessing that you are looking at the OS implementation of the socket. It probably uses one file descriptor for reading and the other for writing (but that is a guess). What does it matter to you what the OS is doing in /proc/pid the stuff in there is not really there for your direct usage.  Your extra file descriptor is most likely related to syslog. Syslog has to open a socket to the syslogd to report messages. Unless you explicitly call openlog this socket is opened upon the first call to syslog and since you aren't calling syslog until you have an error you are most likely observing syslog's side effects. Exactly the problem. Thanks ! :)) Also : There is a explicit call to openlog  yet the socket appears after the first syslog call."
19,A,"How to broadcast a message in a network? I'm working on a client-server application written in C I want to broadcast a message to all the machines available in the network. How can I do that using the usual socket system calls in C ? Thank you! I meant in the local network. Thanks bortzmeyer i found my answer: 192.168.0.255 :) ""in the network"" is too vague. Do you mean only the local network? you have to use UDP to send a broadcast message over a network. when creating your socket using the socket() function specify AF_INET for the family parameter and SOCK_DGRAM for the type parameter. on some systems you have to enable the sending of broadcast packet by setting the SO_BROADCAST socket option to 1 using setsockopt(). then use the sendto() function call to send a datagram and use 255.255.255.255 as the destination address. (for datagram sockets you don't need to call connect() since there is no 'connection'). in standard implementations this address broadcasts to all computer in the local network this means that the packet will not cross gateway boundaries and will not be received by computers using a network mask diferent from the network mask of the sending computer.  You can use the special address of 255.255.255.255 to send a broadcast message to every computer on the local network. For more info see section IP Network Broadcasting.  Just send the message to the broadcast address of your subnet which for 192.168.0.0/24 is 192.168.0.255 or just broadcast to 255.255.255.255.  Have a look at udp sockets. I recomend beej guide have a look at the 6.3 Datagram Sockets"
20,A,"Using Tcp why do large blocks of data get transmitted with a lower bandwidth then small blocks of data? Using 2 PC's with Windows XP 64kB Tcp Window size connected with a crossover cable Using Qt 4.5.3 QTcpServer and QTcpSocket Sending 2000 messages of 40kB takes 2 seconds (40MB/s) Sending 1 message of 80MB takes 80 seconds (1MB/s) Anyone has an explanation for this? I would expect the larger message to go faster since the lower layers can then fill the Tcp packets more efficiently. the data size less than 64kb will pass through straight forward and data will lager than 64kb will be transfered in chunks of 64kb that process will take more time to split and Marge the data that's why it takes more time. @moon: Hogwash. Any time you'd save in splitting things up would be lost in network latency of opening so many connections. @Pieter: Does the behavior still occur if you use the native socket facilities rather than the ones provided by Qt? @Billy Good idea. I wouldn't be surprised if it was a QT bug - wouldn't be the first. There's definitely something wrong either in Qt or in the OS. I wrapped a function around QTcpSocket::write that cuts up all my messages in chunks of 40kB. Now all my data gets sent at a rate of about 40MB/s. Cookies for the one who finds the cause of this :) I reported this to Qt apparently they changed the management of the buffers in between 4.5.3 and 4.7 This is hard to comment on without seeing your code. How are you timing this on the sending side? When do you know you're done? How does the client read the data does it read into fixed sized buffers and throw the data away or does it somehow know (from the framing) that the ""message"" is 80MB and try and build up the ""message"" into a single data buffer to pass up to the application layer? It's unlikely to be the underlying Windows sockets code that's making this work poorly. Edit: Interesting why the down votes? It's slightly more useful to leave a comment when you down vote to say why that way all of us can learn.  Bug in Qt 4.5.3 ..................................  TCP from the application side is stream-based which means there are no packets just a sequence of bytes. The kernel may collect multiple writes to the connection before sending it out and the receiving side may make any amount of the received data available to each ""read"" call. TCP on the IP side is packets. Since standard Ethernet has an MTU (maximum transfer unit) of 1500 bytes and both TCP and IP have 20-byte headers each packet transferred over Ethernet will pass 1460 bytes (or less) of the TCP stream to the other side. 40KB or 80MB writes from the application will make no difference here. How long it appears to take data to transfer will depend on how and where you measure it. Writing 40KB will likely return immediately since that amount of data will simply get dropped in TCP's ""send window"" inside the kernel. An 80MB write will block waiting for it all to get transferred (well all but the last 64KB which will fit pending in the window). TCP transfer speed is also affected by the receiver. It has a ""receive window"" that contains everything received from the peer but not fetched by the application. The amount of space available in this window is passed to the sender with every return ACK so if it's not being emptied quickly enough by the receiving application the sender will eventually pause. WireShark may provide some insight here. In the end both methods should transfer in the same amount of time since an application can easily fill the outgoing window faster than TCP can transfer it no matter how that data is chunked. I can't speak for the operation of QT however. Qt builds a framework on top of it you can find a more detailed explanation here: http://stackoverflow.com/questions/3778049/what-is-qtcpsockets-behaviour-when-the-network-is-overloaded"
21,A,"""select"" in C socket Sorry if my question is trivial. Iam using select() in my server program to look for new connections as well as for data on exisitng connections. However when I get data on one connection/socket my program reads the data and store it in an array. Here The server will not reply back to client [ though client is in recv() mode ] . Now when server goes back to select() call it is getting stuck there. Not sure why. My intention is that server should come out of select [ if there is no data ]. I kept timeout on the select() as well. Any suggestions in this regards would be of great help. Regards Roopesh M. Are you re-initializing the `fd_set`'s every time in your select loop = No. Iam not. Is it required ? Yes. After `select` returns the only members in the `fd_set` will be those which were ready. That's how `select` indicates to you which were ready. Guys i got it resolved. Seems the timeout was not working fine earlier. I have fixed it. It is working fine now..Thanks to all. If i have any doubts will ping back you guys.. Thanks a lot. @Roopesh: My suggestion is to either delete the question or add an answer yourself and set that as the accepted one (it's really your choice). That way this question won't show up as unanswered. Thanks.. i did it just now. Guys i got it resolved. Seems the timeout was not working fine earlier. I have fixed it. It is working fine now..Thanks to all. If i have any doubts will ping back you guys.. Thanks a lot. maybe you could mark this answer as ""accepted"" so the question doesn't continue to show up as unanswered. cheers :-) Yes Lee.. But when i selected it as ""accepted"" it is saying you can accept your own answer only after 2 days. So will do it after 2 days."
22,A,"what does this mean ""couldn't read CGI input from STDIN after alloc read 0 "" Iam trying to read a response from a CGI page from Java where I send a POST request with appropriate params using the apache commons HTTPClient library. I have framed the request with the request headers as like as browser the java code responds with ** ""couldn't read CGI input from STDIN after alloc read 0"" ** When the same CGI when used in the browser works fine what could be the reason for this or Iam missing any params. Here is the code post.addHeader(""User-Agent"" ""Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.6; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3""); post.addHeader(""Host""""www.XXX.yyy.in""); post.addHeader(""Accept"" ""text/htmlapplication/xhtml+xmlapplication/xml;q=0.9*/*;q=0.8""); post.addHeader(""Accept-Language"" ""en-usen;q=0.5""); post.addHeader(""Accept-Encoding"" ""gzipdeflate""); post.addHeader(""Accept-Charset"" ""ISO-8859-1utf-8;q=0.7*;q=0.7""); post.addHeader(""Keep-Alive"" ""115""); post.addHeader(""Connection"" ""keep-alive""); post.addHeader(""Content-Header""""10000""); post.addHeader(""Referer"" ""http://www.abc.info/""); post.getParams().setParameter(""buttonSubmit.x"" ""90""); post.getParams().setParameter(""buttonSubmit.y"" ""12""); post.getParams().setParameter(""lccp_1"" ""123""); post.getParams().setParameter(""lccp_2"" ""4567890""); post.getParams().setParameter(""texNumber"" ""1234567890""); // code which submits to the CGI page Care to share some code? ya done  thanks http://hc.apache.org/httpcomponents-client-ga/tutorial/html/fundamentals.html 1.1.7.2. HTML forms Do you need to use setEntity()? It would seem like the java code is detecting the presence of a request but is not receiving any actual data from it."
23,A,"Connecting from host to virtual server fails with network unreachable from Java I have a server that has several virtual machines running on it. I'm trying to connect to one of these servers using Java but it fails with ""Network unreachable"". The usecase is Hudson connecting to run a slave on a windows-machine. It works using python sockets so it's quite odd. This is the network configuration on the host machine http://dpaste.com/168704/ . The problem is reproduced using this snippet http://dpaste.com/168708/ . Any ideas? I've only managed to reproduce this problem with java. ssh ping python and nc was tested as well and they work. Edit: Seems that all outbound connections from java suffers the same fate from the host machine. I was hit by this bug http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=560056 . So works now with the quick fix :-)  It's likely that the wrong address is being resolved for the host name. Add this debugging to your test: InetSocketAddress saddr = new InetSocketAddress(""waltraction.dhcp.samfundet.no"" 135); InetAddress addr = saddr.getAddress(); if (addr == null) { System.out.println(""Unresolved address.""); else System.out.println(addr.getHostAddress()); Simply seeing the address being used may point out a problem but if not update the question and get more help. Thanks for the answer. The correct address is resolved so the problem is somewhere else."
24,A,Java Servlet says file does not exist I have developed a java servlet that monitors a folder on a network drive for new files then does some actions on them depending on what kind of file it is. It worked in Eclipse when Eclipse and Tomcat were running with each other but now that I have deployed it onto a server(different machine) the servlet keeps logging that it cannot find the folder to be mapped. The exact same network drive is mapped and the folder definitely exists. This problem only occurs when the servlet is run on the server not on the development machine. Thanks! PS: It is a Windows Server 2003 Enterprise Server with Tomcat v6 installed. Are you using relative paths in your code to find the file? The paths to reach a file in your filesystem when the code runs in your IDE or local Tomcat is more than likely not the same path to reach the file as when it runs on other machines/environments. Best bet is to remove the paths from your code completely and load them from a configuration file instead. I did and reconfigured the configuration file for the server however because it is a network drive the file is located on the E drive for both machines.  I finally figured it out. It turns out that when Tomcat runs it (obviously) is running as a service. The service does not see things the same way as a user does. It turns out that Windows ONLY mounts network drives at the USER level. Therefore according to the SYSTEM the drive does NOT exist. The workaround was to use UNC pathnames. I tried that originally but it did not work and the reason for that was because the service did not have correct permissions. Thanks to everyone who helped me. http://wiki.apache.org/tomcat/FAQ/Windows#Q7  Can you try output of this API to check if the file path is what you are expecting http://java.sun.com/j2ee/sdk_1.3/techdocs/api/javax/servlet/ServletContext.html#getRealPath(java.lang.String) may be you want to check this link too http://www.coderanch.com/t/360208/Servlets/java/getServletConfig-does-not-work I got it working and the file path is indeed not what I am expecting. Thanks now to find out what to do to fix it! I am stumped again. What do I do now that I figured out that the file path is NOT what I expected? Can I change the servlet context? please forgive me if this is a dumb question but i keep getting a NullPointerException for the line this.getServletContext(); Why would this be null? Adding the file path in some configuration file might be a good idea
25,A,"DHCP : Cant receive reply from server I am working on Ubuntu 9.04. I am running this on VMware workstation. Here is my C code: int sockfdcntaddrlen; const int on = 1; struct sockaddr_in servaddrcliaddr; char reply[512]; sockfd = socket(AF_INET SOCK_DGRAM 0); if (sockfd < 0) { perror(""socket""); exit(1); } setsockopt(sockfdSOL_SOCKETSO_REUSEADDR &onsizeof(on)); bzero(&cliaddr sizeof(cliaddr)); cliaddr.sin_family = AF_INET; cliaddr.sin_addr.s_addr = htonl(INADDR_ANY); cliaddr.sin_port = htons(68); addrlen = sizeof(servaddr); if (bind(sockfd (struct sockaddr *) &cliaddr sizeof(cliaddr)) < 0) { perror(""bind""); exit(1); } while(1) { cnt = recvfrom(sockfd reply sizeof(reply) 0(struct sockaddr *) &servaddr &addrlen); if (cnt < 0) { perror(""recvfrom""); exit(1); } printf(""\nReply Received\n""); } I run this program in one terminal and run 'dhclient' on another. I receive no datagrams. What am I doing wrong? I'm not sure what you're doing wrong but if I were you I'd write my own client which is very simple and see if it can talk to your server code above (who knows what dhclient might do outside of contact your code). I'd also temporarily change the port number to something not well-known. This way I wouldn't be interfering with any other programs and interfaces.  Looks like you're listening on UDP port 68 for a broadcasted message from the client? If I'm reading DHCP correctly the client will send its broadcase 'discover' request FROM UDP port 68 but TO UDP port 67 on the server so you would need to be listening on port 67 to receive it. An easy 'first' test to test you're code before trying it with dhclient would be to try talking to your server with netcat. a command line like echo ""Foo"" | netcat -u localhost 68 Should cause a packet to be received by your current code. Another good debugging tool is wireshark which will let you see exactly what UDP packets are being sent by dhclient and what they contain.  I recommend this tutorial. Also are you running as root? You can't get that low-numbered port otherwise. Yes I am running as root"
26,A,How to reduce remote SQL Server loads? I want to create an application in C# with client and server sides. It will work over local network. Client sides must check for updates on a remote SQL Server. Lets say we've set update time interval to 2 seconds. If i have 20 client side applications then they'll send query to the remote SQL Server every 2 sec and it will load server quite a lot. Now I want to know is there any way to reduce server load or it's only way to check for updates? What is the data do you need to check for so often? it's queue management system From my point of view there is no need to allow clients to connect the DB serer directly. There should be one more tier here which will only connect to the server and cache information about the updates. Your clients should connect to this additional information and work with the cached info. UPDATE As far as I understand the problem appears because all your clients ping your DB server every two seconds. The solution to this problem is to create a special module which will only have access to the DB server and asks it for the update. For example every two seconds. If the update is ready it should be able to fetch it from the DB and store. This is what I meant under the additional tier. Now let's return to your clients. They should be able to communicate with this module and get information from it about a ready update (this information is cached and thus it is really fast to obtain it. Also you needn't ping the server at every client request). If update is ready fetch it to the client side and work on client side. As for the communication between this additional tier and clients. Since you are working with .NET I would suggest that you take a look at the WCF which from my point of view becomes a standard approach of implementing the between-process communication in .NET. There are a lot of information in the network about it I will post the links shortly. Here is my favorite WCF book: Programming WCF Services MSDN entry: Windows Communication Foundation If I understand you properly you suggest the server to notify clients that the update is ready and then clients to download them. Right? please explain more detailed if you can thx in advance @user841470: In other words call a middle tier layer. You could use something like a SqlCacheDependency that caches a query or table. This way the cache always has the latest data. This could be implemented as a Web Service or WCF Service which is doing the caching. The app calls the web/WCF service to which the relevant data is returned. @user841470 I have updated the answer but in this case i need to write bunch of client side ip adresses to server side pc how can i automate this process? @Tural T. You needn't write a bunch of ip addresses using WCF. All you need to do is to publish an end-point on the server side which is known by the client. The client will connect to the WCf service and obtain the required information. Clients can communicate with mid-tier using Publish-Subscribe messaging so clients just subscribing for a specific update type and thats it pubsub model effectively reduce redundant periodic checks from client to mid-tier.
27,A,"Is there an algorithm for fingerprinting the TCP congestion control algorithm used in a captured session? I would like a program for determining the TCP congestion control algorithm used in a captured TCP session. The referenced Wikipedia article states: TCP New Reno is the most commonly implemented algorithm SACK support is very common and is an extension to Reno/New Reno. Most others are competing proposals which still need evaluation. Starting with 2.6.8 the Linux kernel switched the default implementation from reno to BIC. The default implementation was again changed to CUBIC in the 2.6.19 version. Also: Compound TCP is a Microsoft implementation of TCP which maintains two different congestion windows simultaneously with the goal of achieving good performance on LFNs while not impairing fairness. It has been widely deployed with Microsoft Windows Vista and Windows Server 2008 and has been ported to older Microsoft Windows versions as well as Linux. What would be some strategies for determining which CC algorithm is in use (from a third party capturing the session)? Update This project has built a tool to do this: The Internet has recently been evolving from homogeneous congestion control to heterogeneous congestion control. Several years ago Internet traffic was mainly controlled by the standard TCP AIMD algorithm whereas Internet traffic is now controlled by many different TCP congestion control algorithms such as AIMD BIC CUBIC CTCP HSTCP HTCP HYBLA ILLINOIS LP STCP VEGAS VENO WESTWOOD+ and YEAH. However there is very little work on the performance and stability study of the Internet with heterogeneous congestion control. One fundamental reason is the lack of the deployment information of different TCP algorithms. The goals of this project are to: 1) develop tools for identifying the TCP algorithms in the Internet 2) conduct large-scale TCP-algorithm measurements in the Internet. There are many more congestion control algorithms than you mention here off the top of my head the list includes: FAST Scalable HSTCP HTCP Bic Cubic Veno Vegas. There are also small variations of them due to bug fixes in actual implementations and I'd guess that implementations in different OSes also behave slightly different from one another. But if I need to try to come up with an idea it would be to estimate the RTT of the connection you can try to look at the time it took between the third and the fourth packets as the first and second packets may be tainted by ARPs and other discovery algorithms along the route. After you have an estimate for RTT you could try to refine it along the way I'm not exactly sure how you could do that though. But you don't require a full spec for the program just ideas :-) With the RTT figured out you can try to put the packets into RTT bins and count the number of in flight data packets in each bin. This way you'll be able to ""plot"" estimated-cwnd (# of packets in bin) to time and try some pattern matching there. An alternative would be to go along the trace and try to ""run"" in your head the different congestion control algorithms and see if the decision at any point matches with the decision you would have done. It will require some leniency and accuracy intervals. This definitely sounds like an interesting and challenging task!"
28,A,getaddrinfo inconsistent behavior I'm using getaddrinfo to start a local server accepting connections only on the localhost: struct addrinfo *res; struct addrinfo hints = {0}; hints.ai_family = AF_UNSPEC; hints.ai_socktype = SOCK_STREAM; hints.ai_protocol = IPPROTO_TCP; getaddrinfo(NULL portbuf &hints &res); This seems to work fine giving me the IPv6 address ::1 when IPv6 is present and IPv4 address when it is not. Except sometimes on Windows 2008 R2 (at least that's the only system where I've seen it but I saw it twice both times on a customer system) it gets bound to 127.0.0.1 instead of IPv6! This then messes me up because if you bind ::1 it will accept both connections to 127.1 and to ::1 but if you bind 127.1 then IPv6 connections are not accepted. getaddrinfo() returns you a list of matching addresses in an unspecified order. You should traverse the list (following the ai_next pointer) and create a listening socket bound to each address returned. @MK: It's just the most reliable way that's all. If you'd rather just listen on one address you could traverse the list until you find the best one (presumably the `AF_INET6` address if there is one). Don't know.. I'm not convinced. I thought it was OK to just listen on IPv6 and rely on the fact that IPv4 will also be accepted (above Windows 2003). I do loop through all addresses when I connect but why bather when accepting? I do realize that I will have to make changes since I can't rely on IPv6 address being the first one returned. What I want to understand is what causes it to be not the first one. Because all systems that I built have IPv6 first but all systems that this customer builds get IPv4 first. There must be something in the configuration.
29,A,Accessing data link layer packets I want to create a socket for accessing IPv4 packets from data link layer. From unix network programming V1 socket(PF_PACKET SOCK_DGRAM htons(ETH_P_IP)) 1)I am implementing a dhcp client is this the correct way of doing that? (means without accessing data link layer i cannot receive reply from dhcp server) or is there any other easier way? also since this socket will receive all IPv4 packets destined for my system how should I distinguish dhcp reply packet from other packets? 2)please suggest me a good link/tuorial for network programming with data link layer access. In the above book it is not detailed description. This is my code I got where ETH_IP is . SO I have changed the question. I apologise for that. Please answer the above new question Do you need the link layer headers too? If so You need to use SOCK_RAW—SOCK_DGRAM will remove the link layer header before feeding it to your application. You can identify DHCP requests by the source and destination ports since DHCP generates traffic on UDP ports 67 and 68. I think I wont need link layer header bcoz I am implementing DHCP client so I need only IPv4 header.  Did you tried looking at PCAP libraries? It provides nice filtering functions on IP port and other things. pcap does not parse the IP headers or the DHCP packets. It just allows you as mentioned by Jack to *filter* in the kernel so your application is not overwhelmed by packets you do not want. I want to parse IP frame on my own so that I can learn.
30,A,Binding to Loopback Adapter in Windows 2008 Server For testing purposes I'm trying to run a number of client applications connecting to a server all on the same host machine. The host machine is running Windows 2008 Server Standard 32-bit with Service Pack 2. On my XP development machine I installed the Microsoft Loopback Adapter in order to run multiple clients connecting from separate IP addresses. My LAN network is 192.168.1.xxx and so I chose 192.168.5.xxx for the loopback adapter addresses. On my development machine everything works fine and I am able to see the client applications binding to the loopback adapter using TCPView. When I try and run the same setup on the Windows 2008 server with the same settings the client applications are unable to bind to the loopback adapter addresses. There is no activity in TCPView and so I believe there's possibly some configuration setting I'm missing in setting up the loopback adapter. Can anyone help? Thanks! Nick. Ok problem solved. I was specifying that the client connect to 127.0.0.1 but when I asked it to connect to a 192.168.5.x address it worked. The server was bound to 0.0.0.0 which suggests that it should still have been bound to 127.0.0.1 anyway but it just didn't seem to work! This may have been a firewall issue BTW. @Ben: Firewall is turned off.
31,A,Network file transfer in Windows I want to transfer files across the network using C or C++. What topics should I look up? How can I do this? Not that I feel confident that I'll be able to answer the question anyway but... this question sounds lacking in detail. Across *what* network? If possible just create a network share and copy the files across like you would anywhere else. I would recommend looking through documentation of Windows Sockets and boost asio. +1 for boost.asio  You should start by choosing a protocol. HTTPS and SFTP are both good choices but there are obviously others. Once you have that straight you can look up choices for client and server libraries. @tusbar: if you have secure libraries why not use them? For completeness there's also scp. libcurl was suggested here: http://stackoverflow.com/questions/360259/sftp-c-library/360276#360276 What about FTP?? FTP is insecure and should be used only inside a firewall or over a secure layer (e.g. http://en.wikipedia.org/wiki/FTPS) the OP doesn't necessarily need to encrypt the transmitted data.  While you could use ReadFile to read the file's contents and then send it over a socket Windows also provides the TransmitFile API to enable you to read a file's data and send it over a socket with one system call. TransmitFile only transmit the contents of file. It also allows you to prepend and append data to the file contents (such as an HTTP header). This is what IIS does.  there is sendfile function in C. Just check it out.
32,A,"Difficulties displaying the text in simple client/server chat program (current input gets displayed) - C I am writing a chat program for my networking class and I have all the networking setup perfectly. My problem is if a client is currently writing a message and he receives a message from a different client then his current input gets displayed with the received message. For example if a client is writing a message ""Hi there how are you?"" and receives a message ""Good day to you!"" while in the middle of writing their message it gets displayed as: Hi there hoGood day to you! ->w are you? Where -> is the area for the user to type in the message. What I would like to happen is to just display the message received and have the area -> retain all the previous text that was written before the message was received. Please make note that what the client is typing in is still in fact ""there"" when he receives a message. If he completes his message his full message will be sent. Also note that my client uses pthreads. One thread to read messages from the server and display them to the users screen and one thread to read from stdin and send the messages to the server. I do believe that my problem is arising because I am using pthreads and the threads share the same stdin stdout stderr. Maybe this is a misconception and wrong? I hope I have been clear on my problem. If not sorry. Please let me know what I can clarify for you. I started doing some research and came upon these links: ANSI Escape Characters Thread from Stackoverflow I was thinking about trying to go up lines and move the cursors around and stuff but don't know if that is the most effective way to do so. Firstly because I don't know how to capture the information that is in the terminal waiting to ""entered""/sent to stdin. Maybe I just haven't found out how to do that. Also I was wondering if there was a way to work/manipulate file descriptors to solve the problem? Maybe that wouldn't even solve it? Thanks for reading and your time. I appreciate all your help. Use a mutex to prevent another thread from writing output at the same time. Designate a thread as the IO thread and sent the messages to be displayed to that thread through a blocking queue (or circular buffer). Does C have those? (I use Java currently).  By default user input is handled by the terminal itself so a mutex alone wouldn't cut it if you want real-time updates. If you wanted a line-input mode solution you could log incoming messages and commit them every time a message is sent and before the next one is read. Else your best bet would be using curses as suggested. A scrollok(3x) enabled window can be used like a terminal easily using waddstr(3x) and wgetnstr(3x) no need to micromanage that if you use an IRC-like UI. Note that using curses doesn't mean you don't have to use a mutex around your curses functions. Else when you less expect it the screen will become full of garbage.  The problem involves threading. Your solutions are to either use one display and block the incoming message until the user finishes with the current input or use two ""windows"". Many conversation programs have two windows: one for incoming data (or the current conversation) and another to build the next message. The standard C language does not have facilities for threading windowing or cursor positioning. You'll just have to use platform specific features. Since you didn't specify your platform you will have to look these up yourself.  Using a library such as curses to manage text 'windows' will be easier than trying to manipulate the screen by hand. This is the best way. You want to keep a variable representing the position of the end of the received text which will tell you where to insert new messages. The sequence should be something like: 1) stop responding to user keypresses 2) grab existing user input 3) clear the user input 4) insert the new message 5) record where the end of that message is 6) re-insert user input 7) act on any buffered input recived while inserting the message  I am not an expert in unix network programming but I am pretty much convinced that the problem is with multithreading itself rather than some stdin/stdout quirks. What I see here is multiple threads accessing the same resource (terminal session) without any synchronization. This inevitably leads to race conditions between them. I would recommend you to read this free e-book on sychronization problems which is especially helpful for those who are only slightly familliar with sychronization: http://www.greenteapress.com/semaphores/"
33,A,"Does OSX support poll()? I have just been reading the section on the poll() function in ""Advanced Unix Programming"" second edition by Marc Rochkind. In this section the author mentions that poll() is not supported under Darwin 6.6 and I have seen other items on the internet that suggests that poll() is emulated on OSX using the select() system call. I am wanting to use poll() for a class on which I am working specifically because it may be servicing a large number of sockets and I am concerned about exceeding the limit of the number of file handles that select() can use. If the statements that I have read are true is there a suitable alternative to select() or is there a way to overcome the FD_SETSIZE limitation? poll(2) was introduced as a layer over select(2) in 10.3 and got a native implementation in 10.4. Most modern BSD-based systems have implemented poll() over the last few years. If you get actual benefits from poll() (vs. select()) I'd recommend using it. Even on systems which still emulate poll() via select() at worst your performance will be that of select() with a marginal penalty. If you're in a situation where the scalability of select() would never be acceptable BSD-derived systems tend to have a very tuned kqueue(2). According to http://marc.info/?l=log&m=111515776629581&w=2 there was a problem with the implementation of poll() in 10.4 that created problems for anyone wanting to use DJB's daemontools. I don't know if this ever got fixed."
34,A,"Domain name interpretation utility for java I find myself with a need for a java utility for taking a fully-qualified hostname and producing the domain name from that. In the simple case that means turning host.company.com into company.com but this gets rapidly more complicated with host.subdomain.company.com for example or host.company.co.uk where the meaning of ""domain name"" gets a bit fuzzy. Throw in complications with the definition of SLD and ccSLD and it gets messy. So my question is whether there's a 3rd-party library out there that understands these things and can give me sensible interpreations. I don't think such a thing exists since it's an adminstrative rather than technical issue and a very multi-lateral one at that. If you end up rolling your own this page on the Mozilla wiki looks like a good starting point with lots of references. Looks like a major headache though. Just look at the rules for Japan. Ouch. For cookies it becomes a technical issue. (Looking at that table I thought there were 10 exceptional second level domains not 8.)  Mozilla regularly maintains the rules that it uses in its browser for cookie security in a format that can be parsed and used by others: http://publicsuffix.org/ Searching Google there are probably Java libraries that can parse the list but I don't know the quality of any of them. Do note that this list is unofficial incomplete (eu.org is missing among others) and not up to date (since rules always change). I think that's the closest I'm going to get. Thanks for that.  Not sure if it's for the same purpose I do something similar in my code. When I set cookies I want to set the domain as close to top as possible so I need to find the domain one-level lower than a public suffix. For example the highest domain you can set cookie for host.div.example.com is .example.com. For host.div.example.co.jp is .example.co.jp. Unfortunately the code is not in the public domain. It's very easy to do. I basically use following 2 classes from Apache HttpClient 4 org.apache.http.impl.cookie.PublicSuffixFilter org.apache.http.impl.cookie.PublicSuffixListParser I forgot the exact reason but we had to make some very minor tweaks. You just walk the domain from top to bottom first valid cookie domain is what you need. You need to download the public suffix list from here and include it in your JAR http://mxr.mozilla.org/mozilla-central/source/netwerk/dns/src/effective%5Ftld%5Fnames.dat?raw=1"
35,A,"Winsock in VB.net not working Hey all i am trying to get this code that worked in VB6 just fine to work in VB.net 2008. It doesnt seem to want to connect (but has no error after it goes past the sockMain.Connect(). sockMain.RemoteHost = ""192.168.1.77"" sockMain.RemotePort = 77 sockMain.Connect() Now when i do this: On Error GoTo oops sockMain.SendData(txtSend.Text) oops: If Err.Number = 40006 Then MsgBox(""It doesnt seem that the server is running. Please check it and try again"") End If I get the It doesnt seem that the server is running. Please check it and try again. error. What am i missing?? David The first thing you're missing is that VB 6 and VB.NET are *completely* different languages with only some superficially similar syntax. Namely `On Error GoTo` is *more* than deprecated in VB.NET since structured exception handling (`try`/`catch`) is available to you now. Get a good book and learn not only the .NET idioms but also object-oriented programming in general. You'll be doing yourself a huge favor in the long run because the re-write you do from VB 6 to VB.NET will actually be worth doing. if you want the feeling of the vb6 winsock in .net world try this beware it was not updated since 2008 and there is a few bug look at the comment at the end of the acticle for more information  As I explained in a comment VB.NET and VB 6 are almost entirely different programming languages. You're not doing yourself any favors by trying to write VB 6 code in VB.NET. There's no reason to migrate at all if you're not going to take advantage of the new features provided by the .NET platform. Beyond the structured exception handling that I mentioned already you should ditch the old WinSock control in favor of the classes found in the System.Net.Sockets namespace. Try replacing what you have with something like the following code: Dim tcpClient As New System.Net.Sockets.TcpClient() tcpClient.Connect(""192.168.1.77"" 77) Dim networkStream As NetworkStream = tcpClient.GetStream() If networkStream.CanWrite And networkStream.CanRead Then ' Do a simple write. Dim sendBytes As [Byte]() = Encoding.ASCII.GetBytes(""Is anybody there"") networkStream.Write(sendBytes 0 sendBytes.Length) ' Read the NetworkStream into a byte buffer. Dim bytes(tcpClient.ReceiveBufferSize) As Byte networkStream.Read(bytes 0 CInt(tcpClient.ReceiveBufferSize)) ' Output the data received from the host to the console. Dim returnData As String = Encoding.ASCII.GetString(bytes) Console.WriteLine((""Host returned: "" + returnData)) Else If Not networkStream.CanRead Then Console.WriteLine(""Cannot not write data to this stream. "" & ""Please check the server and try again."") tcpClient.Close() Else If Not networkStream.CanWrite Then Console.WriteLine(""Cannot read data from this stream. "" & ""Please check the server and try again."") tcpClient.Close() End If End If End If  The Connect call can take a little while to complete. Even if your client has made the physical connection to the server you have to give it a wee while to establish the TCP virtual circuit. If you put a breakpoint on the call to SendData and wait just a second or two then continue you'll probably find that it works OK."
36,A,"What is the difference between a banyan network and a folded banyan network? What is the difference between a folded banyan and a regular banyan network. Teradata replication is apparently based on a folded banyan design but many of the google top ranked results to explain folded banyan contain incorrect or contradictory information (hopefully they're not Bing-bait). Banyan Network The Banyan network is a type of multistage interconnection networks (MINs) frequently implemented as a basic building block in ATM switching fabric architectures. It is named so for its complex connecting patterns that resembles the Banyan tree's aerial root system. As a variety of the Banyan network the folded Banyan network is a buffered multistage burst cross-connect network that uses dual-redundant bi-directional switch elements. A folded Banyan network has self-routing capability modularity linear scalibility and enhanced fault-tolerance. BYNET The description found in the IT Toolbox article is supported by the Teradata Online Manual Introduction to Teradata Warehouse where it describes the BYNET as possessing high-speed logic providing bi-directional broadcast multicast and point-point communication with merge functions. It goes on to state there are multiple BYNETs to create a fault-tolerant environment and enhance interprocessor communication. Hope this helps. I was hoping for more of a hard-core network topology compare and contrast. I've been able to find that for a banyan network in academic papers but I have no idea what a ""folded"" network is.  I notice your question was a long time ago but hope this is helpful for the curious... In the early days of telephones each household had a line which ran to the telephone exchange. The human operator could patch your line to somebody else's line with a patch chord direct from socket to socket. Obviously for say 100 lines the operator needed 100 sockets but only a handful of patch chords - corresponding to the maximum number of simultaneous calls that might be needed. But when mechanised with electro-mechanical switches it was not possible to make a switch that could switch between too many possible connections and the switching system needed to be modular and extensible. So each individual switch could route a call to one of ten destinations. Each of those destinations could be another 10-way switch and so on. Each incoming line went to a 10-way switch. The first digit you dialled set that first switch connecting the line to the next switch. The second digit you dialled set that switch connecting to the next. And so on in turn. (See ""stepping switch"" or uniselector on Wikipedia.) If you had (say) 1000000 lines you'd need 6-digit phone numbers. Ostensibly you'd need 1000000 x log(1000000) = 6000000 switches in the exchange to connect any pairs. But notice there might be fewer than 1000 calls going on at any one time so a great majority of the switches would be idle - making the exchange way more expensive than necessary. So a more clever scheme was needed - the Banyan network named after the tree. It works pretty much the same as before but now the first three digits route the million input lines through the old 10-way switch network in three stages down to one of just 1000 intermediate points. The last three digits route the 1000 intermediate points back out through another expanding network of 10-way switches to the million lines. Like the human operator in the original exchanges it can't cope with more than 1000 simultaneous calls and only then if the numbers called are fortuitous with the first three digits all different! But now notice... that the network of switches is symmetrical: 1000000 to 100000 to 10000 to 1000 to 10000 to 100000 back to 1000000. With analogue telephony switches are switches - they don't care which direction electricity is flowing through them just so long as they are switched to the correct position. So the cunning step is to ""fold"" the network: 1000000 to 100000 to 10000 to 1000 and back out on the same array of switches on a different route (provided you can hook up the intermediate points appropriately). And hey - you have a folded Banyan network. It uses half the resources of a Banyan network with only a bit of extra complication routing the calls. Lastly the biggest expense in such switching networks was the first layer of switches (1000000 of them in our example one per line). On larger exchanges they were replaced with ""linefinders"" - when you lift the telephone handset the exchange connected you to the first free switch in a much smaller bank of switches. Since only 1000 calls can be active at one time only 1000 switches are really needed in this first bank. (So you knew you had been allocated one you got a ""dial tone"" when one was available.) Of course many other optimisations and reductions were employed that was just the start! Nowadays everything is digital of course but ATM networks can and do use ""folded banyan"" topology at least in broad shape. Many input points multiplexed down to fewer and fewer intermediate nodes then back out to many output points - using the same hardware but with a different route to the appropriate output point. And the biggest users of ATM networks? Telephone companies! Hope that helps. Mi5ke"
37,A,"DNS client in C I am currently working on a school project which asks me to implement a DNS client without using any library functions. I have got to the point where i send a DNS request and Receive the Reply. I'm getting stuck at the parsing of the reply. I receive the reply in a char* array and i want to convert it into some meaningful structure from which i can parse the answer. I went through the RFC and i read about the packet structure but implementing it in C is giving me problems. Can anyone give me any examples in C or maybe in any other language that explains how this is done. Or any reference to a book is also fine. Additional Details: So the following are the structures that i'm using. struct result{ int type; struct res_ip_cname ip_cname; struct res_error error; struct res_mx_ns mx_ns; }; struct res_ip_cname{ char* lst; int sec; char* auth_flag; }; struct res_error{ char * info; }; struct res_mx_ns{ char * name; unsigned short pref; int sec; char* auth_flag; }; I have a char* buffer[] where im storing the response the i receive from the server. And i need to extract information from this buffer and populate the structure result. Thanks Chander The request format and response format are quite similar - both contain variable length fields which I guess is what you're stuck on - but if you've managed to form a request properly you shouldn't have too much trouble parsing the response. If you can post some more details like where exactly you're stuck we could help better.  Your structures don't look like anything I recognise from the RFCs (yes I've written lots of DNS packet decoding software). Look at RFC 1035 in particular - most of the structures you need can be mapped directly from the field layouts show therein. For example you need a header (see s4.1.1): struct dns_header { uint16_t query_id; uint16_t flags; uint16_t qdcount; uint16_t ancount; uint16_t nscount; uint16_t arcount; }; Don't forget to use ntohs() to convert the wire format of these fields into your machine's native byte order. The network order is big-endian and most machines these days are little-endian. You'll need a ""question"" structure (see s4.1.2) and a generic ""resource record"" structure too (see s4.1.3). Note however that the wire format of both of these starts with a variable length ""label"" which can also include compression pointers (see s4.1.4). This means that you can't in these cases trivially map the whole wire block onto a C structure. Hope this helps...  If I were you I'd be using wireshark (in combination with the RFC) to inspect the packet structure. Wireshark captures and displays the network packets flowing through your computer. It lets you see both the raw data you will be receiving and the decoded packet structure. For example in the screenshot below you can see the IP address of chat.meta.stackoverflow.com returned in a DNS Response packet rendered in three different ways. Firstly you can see a human readable version in the middle pane of the screen. Secondly the highlighted text in the lower left pane shows the raw DNS packet as a series of hexadecimal bytes. Thirdly in the highlighted text in the lower left pane you can see the packet rendered as ASCII text (in this case mostly but not entirely gobbledigook).  My advice is don't make a meal of it. Extract QDCOUNT and ANCOUNT from the header then skip over the header skip QDCOUNT questions and start parsing answers. Skipping a label is easy (just look for the first byte that's 0 or has the high bit set) but decoding one is a little bit more work (you need to follow and validate ""pointers"" and make sure you don't get stuck in a loop). If you're only looking up addresses (and not PTR records) then you really never need to decode labels at all. It's typical for even `A` answers to use labels - normally the `name` field in the answer is a label pointing back into the question. But there's no reason to parse labels when looking up an address unless you want to (1) check that the answer is actually an answer to your query or (2) get the canonical name when the name you looked up resulted in a `CNAME` record and corresponding `A` or `AAAA` records. And skipping them is a lot easier than parsing them. :-)"
38,A,"Pacman in Java questions For my university assignment I have to make a networkable version of pacman. I thought I would best approach this problem with making a local copy of pacman first and then extend this functionality for network play. I would have to say that I am relatively new to java GUI development and utilizing such features within java. http://www.planetalia.com/cursos/Java-Invaders/ http://javaboutique.internet.com/PacMan/source.html I have started following the above links with regards to game development within java and an example of the pacman game. I decided to represent the maze as an int array with different values meaning different things. However when the paint method inside the main game loop is run i am redrawing the whole maze with this method.  for (int i : theGame.getMaze()) { if (i == 4) { g.setColor(mazeWallColour); g.fillRect(curX curY cellSize cellSize); curX += 25; } else { curX += cellSize; } index++; // Move to new row if (index == 25) { index = 0; curX = 10; curY += cellSize; } } However this is providing me with less then 1fps. Although i've noticed the example linked above uses a similar way of redrawing each time the paint method is called and i believe does this on a image that is not viewable (kinda like double buffering [I've used a BufferStrategy like the first link explains]) What would be a better way to redraw the maze? Any pointers/advice with this would be useful. Thank you for your time. http://pastebin.com/m25052d5a - for the main game class. Edit: I have just noticed something very weird happening after trying to see what code was taking so long to execute. In the paintClear(Graphics g) method i have added ocean = sprites.getSprite(""oceano.gif""); g.setPaint(new TexturePaint(ocean new Rectangle(0tocean.getWidth()ocean.getHeight()))); g.fillRect(10 10getWidth() - 20getHeight() - 110); which made the whole thing run smoothly - however when i removed these lines the whole thing slowed down? What could have caused this? Updated code Malachi post your entire code for the game. I'd rather be able to *run* something. Zip it and put it on filedropper As mentioned by Simucal it's difficult to debug your program without access to the entirety of the source. With the code above no problems are immediately apparent and the first place I'd recommend looking is in your implementation of the getSprite method. The code you listed above can't be the source of the 1fps problem... I have code doing far more than this that runs far faster. Can you benchmark that code and make sure it's the root of the problem?  Java/Swing double-buffers by default. If you're using Swing you don't need to double-buffer separately like other answers suggest. I agree with Allain that the code you listed can't be the cause of 1fps. I've written highly inefficient Java/Swing animation code that runs much faster than you describe. Do some more testing to narrow down the cause of the slowness.  It looks like your call to Thread.sleep doesn't do what you intended but I don't think it's the source of your trouble. You have: Thread.sleep(Math.max(0 startTime - System.currentTimeMillis())); startTime will always be less than System.currentTimeMillis() so startTime - System.currentTimeMillis() will always be negative and thus your sleep will always be for 0 milliseconds. It's different from the example you showed because the example increments startTime by 40 milliseconds before doing the calculation. It is calculating how long to sleep for to pad out the drawing time to 40 milliseconds. Anyway back to your problem. I'd recommend measurement to figure out where your time is being spent. There's no point optimising until you know what's slow. You already know how to use System.currentTimeMillis(). Try using that to measure where all the time goes. Is it all spent drawing the walls? EDIT - I see this got marked as accepted so should I infer that the problem went away when you fixed the sleep time? I don't have a lot of Java GUI experience but I can speculate that perhaps your code was starving out other important threads. By setting your thread to have maximum priority and only ever calling sleep(0) you pretty much guarantee that no other thread in your process can do anything. Here's a post from Raymond Chen's blog that explains why.  If possible you should keep an image of the maze and draw it in one library call. It probably doesn't need to be full resolution either -- if you want a blocky 8-bit feel I expect the graphics library will be more than happy to oblige 8^) Also as others have mentioned you can save time by redrawing only those parts of the screen that need updating. This can be annoying to implement but it may allow you to significantly improve your frame rate. Be sure to do some experiments to make sure this is the case before exerting the required effort!  Just so you don't worry that it's Java I worked on a Spectrum Analyzer (Like an o-scope) where the entire GUI portion(the trace menus button & wheel handling) was done in Java. When I got there it was getting 1fps when I left it was 12-20. That had a lot of processing going on and was running on a very slow processor. Look at only updating parts of the GUI that you need to update. Often you can redraw the entire screen but just set a clipping region to the part that is truly updated. Be careful about inner loops--they are The Speed Killer. Try to avoid allocating and freeing huge numbers of objects. I'm not saying don't use objects I'm saying don't create one for each pixel :) Good luck  Wow that's a pretty tough problem to give someone just learning Java. My advice? Think in terms of objects. Can you write something WITHOUT a user interface that mimics the behavior of the game itself? Once you get that working you can concentrate on the special problems of the user interface. Yes start with a local version before the networked piece. I'm not a gamer. I wonder what Java2D API would offer to make your life better? How much time do you have to finish it? Yeah it does seem a bit extreme considering the module itself is about networks - I think i might scrap trying to build this game for myself and adapt the source code included above as we're only getting marked on the networking aspects.  First off I'd recommend that you use named constants rather than having random magic numbers in your code and consider using enums for your cell types. While it won't make your code run any faster it certainly will make it easier to understand. Also 'i' is normally used as a counter not for a return value. You should probably call it cellType or something similar. I'd also recommend that you use a 2D array for your stage map since it makes a number of things easier both logistically and conceptually. That said here are a few things to try: Pull the setColor() out of the loop and do it once. The compiler might be able to do loop-invariant hoisting and thus do this for you (and probably will) but conceptually you should probably do this anyway since it appears you want all of your walls to be one color anyway. Try calling drawRect() instead of fillRect() and see if that draws faster. I don't think it will but it is worth a shot even if it looks uglier. Similarly you can try creating an Image and then drawing that. This has the advantage that it is really easy to tell your Graphics object to implement a transform on your image. Also consider taking this out completely and make sure that it is being a significant performance hit. Also normally you don't need to ask for the parent for its Graphics object and implement painting directly on it. Rather you should override its paintComponent() method and just utilize the Graphics given to you (possibly calling helper methods as you do). Swing components are double-buffered by default so you don't need to implement that yourself; just let the swing object do its job and let you know when to paint. Also you end up repainting the entire screen which is something of overkill. If you call repaint(Rectangle) Swing can choose to redraw only the sections of your board that are explicitly marked dirty. When you update one of your sprites call repaint(r) only on the area of the sprite's old and new locations. When you complete a level and need a new board then you can call repaint() (without parameters) to redraw the entire map. You should also look at Sun's tutorial to get some tips for efficiency in Swing.  I'm no game developer but that framerate seems very slow. I'm not quite sure how your code is working but one possibility for improving rendering performance would be to find those parts of the display that don't change much (such as the walls of the maze) and avoid re-creating them for each frame. Create a BufferedImage containing the constant elements (maze? background) and then re-draw it first for each frame. On top of this Buffered image draw the variable elements (PacMan ghosts dots etc). This technique along with many other Java2D performance tips is discussed in Romain Guy's excellent book Filthy Rich Clients.  I still consider myself a beginner with Java but I recently developed a Frogger-esque game with dynamic map and editor using some of the techniques you've mentioned and I'm only too happy to provide some help. As mentioned enum's are the way to go. I set my map up as a 2-dimensional array and set an enum for each different type writing a method inside my map class to take in one image and divide each square in the map to each value in my enum. A tutorial that helped me with mapping can be found on Coke and Code. All the source code is there if you need a hand with any of it although you do seem to have a decent grasp of what you're doing. If you still need help I could always drag out some source code.  This might sound obvious but your performance problem is because you are redrawing the entire maze which doesn't need to be done instead you need to redraw only changed parts of your maze. The way I've approached this issue before is by seperating the updating of the maze from the actual redrawing into different threads (kind of like a threaded MVC). Every time you change a cell in your maze you would mark it as ""dirty"" your redraw thread will check every now and then to redraw only the dirty cells. Sorry for the extremly generic advice Even redrawing the entire screen shoud yield more than 1fps. you are probably right"
39,A,"Ignoring incoming data in TcpClient / NetworkStream For communication with some third-party software I need to establish an unidirectional connection over TCP. My software only needs to send data to the other side and never will read any data. Currently I'm using the TcpClient. What would happen if there are incoming packets nonetheless and I never read them? Would they pile up somewhere and lead to some errors or the like? How would I configure the TcpClient to ignore or discard all incoming data? The whole design is not exactly what I would do but I can't change the other software and need to bear with this. Some nice hints on the bits inside a TcpClient would be very helpful! I think some of the data will be buffered waiting for you to read it. Not sure how large the buffer size is however. I don't think it will lead to errors immediately but if the sender is expecting to be able to write at some point the write may time out and the other party may choose to close the connection. You mean that there should be a lot of ""hey don't send me anymore data I'm full"" packets going all over the internet? I don't think so... No I'm not saying that's what happens. I'm just saying the sender participates in the buffering of data and that the received has the right to limit the buffer or not buffer at all. I think the sender basically gets data from the receiver as to how to control flow. The other party will be able to send data even if the OPs receive buffer is full. When the receiving party's buffer is full I don't see how the sending party can logically transmit any more data. Someone has to store it and it isn't going to be the routers. It seems logical to me that the sending party has to buffer it. FYI the buffer probably doesn't exist at the TcpClient level - it's probably pretty deep in the networking stack.  Nothing will happen from your point of view. Data will be discarded."
40,A,"N*(connect + send + close) vs (Nagle disable + connect + N*send + close)  N > 1 I'm new to socket programming (as you already figure out by my silly question) but keeping my shame aside I'm writing a program using TCP posix. My constrain is the following: The message to be sent from the client to the servershould be read as byte stream and while my application is not high performance the message should be deliver as soon as possible. I wrote a TCP client class with the intention of doing the following: 1 connect - many send - and 1 close at the end of the streaming. The problem is that the messages does not get deliver in near-real-time (I'm assuming its waiting to have a larger package for better throughput) After doing some research online I found that while you can disable the Nagle algorithm (NA) it is s a very bad idea to do so. Since I'm new on socket programming I don't want to disable features that I don't fully understand. So I'm left with two (bad?) options: connect - send- close per message 1 connect - send multiple times and do 1 close at the end with the NA disabled. While I read the consequences of disabling the NA It seems to me that opening and closing a socket every time just to send a message is an expensive price to pay as well. Are there other solutions without leaving sockets? Thanks. +1 for doing good research before asking. In your case disabling Nagle is exactly what you want to do. Just remember that every call to write() is going to transmit your data immediately. So be sure you are packing your entire message together and then calling write() (or writev()) once when you are ready to send; do not call write() repeatedly with small payloads because that will be slow. Situations like yours are exactly why they let you disable Nagle. I agree. Disabling Nagle is waaaaaay better than doing a bunch of extra three-way handshakes plus extra FIN packets.  @Nemo has given great advice for TCP. But I suggest you look at UDP instead. TCP can introduce arbitrary delay during packet loss and ""TCP fairness"" works based on forcing packet loss to occur. It's not ideal for low-latency transfers. Wanting to disable Nagle is a strong sign you're using the wrong protocol. That what was my initial reaction but the the constraint that pushed me away from UDP is data integriy. Datum have to arrive in order and correct...other wise the app will not make sense. @MSalters: That's exactly the idea. Although because UDP already has an error-detecting code intervening routers will most likely detect errors and drop the entire packet. So here it's Forward *Erasure* Correction. Codes which have good performance for error correction also have great erasure reconstruction performance though. @Armando: If you want timely AND correct your choice seems to be FEC. A simple ""send multiple copies of every packet"" should suffice. With sequence numbers you can throw away the duplicates and ensure the right order. If multiple copies of every packet would waste too much bandwidth you can use some error-correcting code of which parity is the simplest (note run it across packets since an entire packet is likely to go away at once). FEC = Forward Error Correction btw - sending more information up front so that a significant number of transmission errors can be corrected receiver-side. This improves latency at the cost of bandwidth. TCP's error recovery involves resending the data which optimizes bandwidth over latency."
41,A,"How should I architect my (mostly) text-based game server? Think MUDs/MUCKs but maybe with avatars or locale illustrations. My language of choice is ruby. I need to handle multiple persistent connections with data being asynchronously transferred between the server and its various clients. A single database must be kept up-to-date based on activity occurring in the client sessions. Activity in each client session may require multiple other clients to be immediately updated (a user enters a room; a user sends another user a private message). This is a goal project and a learning project so my intention is to re-invent a wheel or two to learn more about concurrent network programming. However I am new to both concurrent and network programming; previously I have worked almost exclusively in the world of non-persistent synchronous HTTP requests in web apps. So I want to make sure that I'm reinventing the right wheels. Per emboss's excellent answer I have been starting to look at the internals of certain HTTP servers since web apps can usually avoid threading concerns due to how thoroughly the issue is abstracted away by the servers themselves. I do not want to use EventMachine or GServer because I don't yet understand what they do. Once I have a general sense of how they work what problems they solve and why they're useful I'll feel comfortable with it. My goal here is not ""write a game"" but ""write a game and learn how some of the lower-level stuff works"". I'm also unclear on the boundaries of certain terms; for example is ""I/O-unbound apps"" a superset of ""event-driven apps""? Vice-versa? I am of course interested in the One Right Way to achieve my goal if it exists but overall I want to understand why it's the right way and why other ways are less preferable. Any books ebooks online resources sample projects or other tidbits you can suggest are what I'm really after. The way I am doing things right now is by using IO#select to block on the list of connected sockets with a timeout of 0.1 seconds. It pushes any information read into a thread-safe read queue and then whenever it hits the timeout it pulls data from a thread-safe write queue. I'm not sure if the timeout should be shorter. There is a second thread which polls the socket-handling thread's read queue and processes the ""requests"". This is better than how I had it working initially but still might not be ideal. I posted this question on Hacker News and got linked to a few resources that I'm working through; anything similar would be great: The C10k Problem Beej's Guide to Network Programming FWIW in software development there is almost never ""one right way"" and in real world scenarios in a game like this the database would be your primary limiting factor. The performance of the application code will most likely far exceed the performance of the database and you would need to look into caching solutions or database sharding solutions. Sharding could be very appropriate with a game since you could shard on game areas etc. I think the in-game resources are going to be slim enough that I can keep the entire world in memory while maintaining an ""update queue"" of every change that gets written to the database in its own separate thread. That way none of the game itself is bound on database operations and the only ""read"" would be on bootup. Lots of risk in terms of error handling but convenient as hell if I can make it work. That's plausible for a low scale solution but I think you might be underestimating your game in regards to DB usage. You need to keep track of inventories player and monster stats/health/location etc. Inventory database lag is one of the most common problems that plague MMO development. I'd recommend reading a bit about unicorn web-server design. That should give you some insight into thread vs. process discussion. http://unicorn.bogomips.org/DESIGN.html http://tomayko.com/writings/unicorn-is-unix  I don't know much about Ruby - sorry - but I think the architecture needs to be driven by your requirements (motherhood apple pie...I know). If you're building something that needs to scale to large numbers of users your architecture needs to reflect that - and you end up making all sorts of decisions you don't necessarily need to make if you're operating at more modest scale. Response time also plays a big part - I don't think that is such a big deal with MUD style games but for web servers or FPS games it's a huge issue. Having said that - the only systems similar to what you describe that I know in any detail use an event driven programming model - clients trigger events the system updates its internal state and notifies affected clients. The ""internal state"" is actually stored in a separate application again communicating using sockets - this allows the app to be scaled by adding more servers to handle client interaction. Not sure you need that level of complexity. Threading is indeed a real issue and it creates hard-to-test code so whilst dash-tom-bang's answer is indeed a little off topic testability is a significant concern.  Although you probably don't like to hear it I would still recommend to start investigating HTTP servers first. Although programming for them seemed boring synchronous and non-persistent to you that's only because the creators of the servers did their job to hide the gory details from you so tremendously well - if you think about it a web server is so not synchronous (it's not that millions of people have to wait for reading this post until you are done... concurrency :) ... and because these beasts do their job so well (yeah I know we yell at them a lot but at the end of the day most HTTP servers are outstanding pieces of software) this is the definite starting point to look into if you want to learn about efficient multi-threading. Operating systems and implementations of programming languages or games are another good source but maybe a bit further away from what you intend to achieve. If you really intend to get your fingers dirty I would suggest to orient yourself at something like WEBrick first - it ships with Ruby and is entirely implemented in Ruby so you will learn all about Ruby threading concepts there. But be warned you'll never get close to the performance of a Rack solution that sits on top of a web server that's implemented in C such as thin. So if you really want to be serious you would have to roll your own server implementation in C(++) and probably make it support Rack if you intend to support HTTP. Quite a task I would say especially if you want your end result to be competitive. C code can be blazingly fast but it's all to easy to be blazingly slow as well it lies in the nature of low-level stuff. And we haven't discussed memory management and security yet. But if it's really your desire go for it but I would first dig into well-known server implementations to get inspiration. See how they work with threads (pooling) and how they implement 'sessions' (you wanted persistence). All the things you desire can be done with HTTP even better when used with a clever REST interface existing applications that support all the features you mentioned are living proof for that. So going in that direction would be not entirely wrong. If you still want to invent your own proprietary protocol base it on TCP/IP as the lowest acceptable common denominator. Going beyond that would end up in a project that your grand-children would probably still be coding on. That's really as low as I would dare to go when it comes to network programming. Whether you are using it as a library or not look into EventMachine and its conceptual model. Overlooking event-driven ('non-blocking') IO in your journey would be negligent in the context of learning about/reinventing the right wheels. An appetizer for event-driven programming explaining the benefits of node.js as a web server. Based on your requirements: asynchronous communication multiple ""subscribers"" reacting to ""events"" that are centrally published; well that really sounds like a good candidate for an event-driven/message-based architecture. Some books that may be helpful on your journey (Linux/C only but the concepts are universal): Programming with POSIX threads UNIX Network Programming Vol. I & II Advanced Programming in the UNIX environment TCP/IP Illustrated Vol. I - IV (Those were the classics) The Linux programming interface - if you just intend to buy one book let it be this one I'm not entirely through yet but it is truly amazing and covers all the topics you need to know about for your adventure Projects you may want to check out: Apache 2 thin mongrel nginx lighttpd ... any web server there is EventMachine (I'm sorry :) node.js Efficient networking in a game (Quake 3 sources) Thanks! OK I see then good luck and have fun :) Yes you're absolutely right there. HTTP is poor when it comes to receiving events from a server. Most of the time this is handled by polling continuously (apparently not ideal) or by something really sophisticated as Comet so HTTP is really acting more on the pull than on the push side. I was just having a web-based game in mind when I read/wrote this so I thought you'd have to use HTTP anyway(maybe I misunderstood). But if that's not a requirement then you are free to choose your protocol and I'm almost certain that there is something (TCP-based) that fits better than HTTP in that case. Thanks for the thorough response! I expressed myself poorly I didn't mean that I don't *want* to understand how EventMachine works. What I meant was I don't want people to say ""Just use EventMachine"". So no need to apologize! That is not what you are saying and I appreciate the suggestion :-) By the way I was thinking about what you wrote and I don't see how HTTP would be the best tool for this. I loved your explanation of why HTTP servers are a good learning resource and I agree but the need for real-time updates being pushed from server to client make HTTP a poor choice for the task of a game server. It takes clever tricks like [Comet](http://en.wikipedia.org/wiki/Comet_(programming)) to achieve what TCP sockets get you for free. Am I off-base here? Like TCPServer? I can direct you to resources and suggest an architecture approach for using TCPServer with Telnet Negotiation if that's what you're looking for. Yeah that would be pretty rad!  The right way is to use test driven development. You will then discover what you need to reinvent at the exact moment that you need to invent it. Start with a test that ""connects"" and assert that the message ""hello user"" is returned. Take it one step at a time from there. I can't resist... are you really suggesting that by adhering carefully to the tenets of TDD I will produce performant concurrent code with no race conditions memory leaks or unexpected behavior whatsoever without referring to *ANY* other resources about this problem domain? Yeah Test Driven Development (as opposed to simply unit testing) guides your architecture but yours is a common misunderstanding. That's cool though no hard feelings; we can't all have winning answers. :) Anyway- my opinion is that there's no better way to learn than to define requirements and then implement to those requirements. I downvoted your answer because I asked about architecture and your answer was about programming methodology which makes it irrelevant. I'm actually a huge fan of test-driven development and I have been using it on this very project; but its utility is orthogonal to the question of high-level server architecture. Sorry my understanding was that you wanted to ""reinvent a wheel or two"" and that you were not targeting a world class epic server to handle any possible loads. When *I* want to learn how to do something new this is how I do it. If you want to create something best-of-breed well clearly you'll need to see what others have done and build on that."
42,A,"Connect to Socket with IPAddress.Any hej Hej My Question is related to my previous post: Get right ip adress of pc with multiple network cards I changed my code to this: TextFileTracer.Write(String.Format(""Create endpoint for host {0}:{1}"" MultipleNetworkMod.BindToMultipleNetworks(hostEntry.AddressList(0)).ToString() _port) TextFileTracer.TraceSource.Framework) Dim ep As IPEndPoint = New IPEndPoint(IPAddress.Any _port) TextFileTracer.Write(""Creating socket..."" TextFileTracer.TraceSource.Framework) s = New Sockets.Socket(ep.AddressFamily Sockets.SocketType.Stream Sockets.ProtocolType.Tcp) s.Blocking = True TextFileTracer.Write(""Connecting to endpoint..."" TextFileTracer.TraceSource.Framework) s.Connect(ep) TextFileTracer.Write(""Socket connected!"" TextFileTracer.TraceSource.Framework) But when connecting tot he Socket I got the exception: the requested address is not valid in its context 0.0.0.0: ""MyPortNr"" (not a string just an int number) Before we created an IPEndpoint with the first ipaddress from the hostEntry list but beceause of multiple network cards that does not work. Can someone help me and put me on the right track again? Greetz Jonathan I solved it see my other thread. I let the user decide on what networkinterface card to bind then I determined the IPAddress from this Card."
43,A,"several UDP sockets bound to same port? first - it's not a question of ""how to bind to port of another software"". now i have a client-server app based on TCP and i'd like to make it UDP but i'm missing something.. i have 2 rules (which i put) to this app: 1) the ""server"" can stay behind router without any port forwarding configuration. 2) the ""client"" can listen to only one port. in TCP i do it like this: 1) the server opens initial connection to the client on port X. 2) when the client wants to open communication channel to the server it uses the initial socket to ask the server for a new one. 3) the server creates a new socket and connect to the client on port X. 4) the client accept this new connection on port X and now the client talk with the server on this new socket. this way i can have multiple connections on the same port. in UDP i have a little problem.. 1) the server sends the initial connection dgram to the client on port X. 2) when the client wants to open communication channel to the server it sends request for a new socket to the initial socket's addr. 3) the server receives the message creates a new udp socket and use it to send data to the client on port X. 4) the client receives the new dgram and ....? basically what i want to happen now is to ""accept"" this connection. meaning: to create a new UDP socket to bind it also to port X and receive data only from that specific incoming socket addr (ipport). but i cannot do that because i can't bind multiple socket to same port. so what is the way to create multiple udp connections on one port? (in networking way not just create a ring buffer of dgrams and send to the right socket) thanks :) You can't have ""connection"" with UDP because UDP is not a ""connected"" protocol but datagram protocol. and i did was very careful not to say the ""connection"" word in the UDP matter. when i say ""connection"" i mean to open a gate for answering.. so what i want is just the server side (which is blocked from outside) to sends a UDP dgram to the client so the client could later on send dgram to that socket and receive dgrams from only that specific socket (like tcp accept) it was possible if i could bind a new socket on the same port and receive data only from the (ipport) that i want. @sfk Despite UDP being stateless protocol connect() call does influence socket operations in a major way. For instance on FreeBSD you can't get errors (like ICMP destination unreachable) from UDP socket unless it's connected. As UDP is connectionless protocol on step 4 you check the contents of UDP message and decide how to handle it. In other words the type of message is defined only by it's contents. However I have a feeling that your whole design is a bit wrong. It's much more common for the client to be behind firewall (just because there exist more clients than servers). If you need to put the server behind firewall you just configure the firewall to allow connections to the set of ports. Even when you have just one more port opened nothing prevents the client from connecting to the same server port several times in parallel. well this solution means just to change the design of the software and the protocol so that in each dgrarm there'll be a header which says what should i do with it. i'm searching for something that won't change the design so i could switch easily between TCP and UDP modes. about the server-client thing.. the ""server"" is a mobile phone.. so i don't have valid ip. @RoeeK yes it does cause your current ""design"" is far from optimal. i can't think of a better way to create a server-client communication where the server has no valid ip address. thats why it's so strange.. because in order the client can talk with the server the server needs to open a socket to the client side.. which makes the client side a server.. so i just put the most clear terminology that i could.. but actually both sides are client and servers @RooeK I think you need to change client and server terms then think again and find out that the task has been solved lots of times before. Your ""client"" is a server and what it does is similar to FTP server in Passive mode where the server tells the client on which port data connection must be established. correct the only problem is my limit: the client side can listen to only one port. in tcp it's enough.. i understand that in UDP it's not so imposible? @RoeeK Established TCP connection is defined by 4 ""attributes"" -- server address server port client address and client port. This is why it's enough - this combination is always unique. In UDP you have a ""mailbox"" on certain address and port. You receive messages sent to it and then you can dispatch them based on their contents and on information what IP address sent those messages. But IP address is not unique (you can have multiple senders on the same computer) so if you use a single address and port for a ""mailbox"" you must dispatch messages based on their contents. that's the part that i needed to understand. thanks."
44,A,"What alternative is there to writing a TCP/IP data relay? I am about to write a tcp/ip data relay - application that passes a one way stream of data from one host/port to another host/port. Initially it will be generic but later on i will customize it to the need of a specific business request. I am guessing that something generic already exists out there so my question is: Has anyone used a third party (preferably open source) data relay in a production environment if so what is and do you recommend it? Any platform is fine. Thanks. I am the author of yProxy which can be used as a transparent TCP proxy server. I use it in production as a simple transparent proxy. I don't know of any open source proxies but that's what you're looking for. Look for an open source ""transparent proxy server"" also sometimes called ""tunneling"". A transparent tcp proxy or tunnel is about the easiest thing to write if you're familiar with socket programming. The hardest thing about it is dying gracefully. There are so many ways that you can lose your connection you must ensure that your program can handle all eventualities.  I've used Delegate for proxying in production use it's been solid.  GNU netcat can be configured to do this and is open source multi platform. Cool I've used netcat for other things and have found it reliable but I've not used it for and didn't realize it did tunneling."
45,A,xml data over network tcp . please help I have devices. Each one sends data to (server IP-address) over a particular port in a random time. Because it's an alarm device and it’s possible that more than one device fired in the same time. The data is in XML Format. Now I already got a small windows application which listen for the data But I got a few problems problem is that the application is sniffing the packets. Mean that I can get the data in the TCP packets ....that’s mean that I will get the XML divided in the packets And not in sequence (order by time)...that’s make it so hard to read the XML data Or just arrange it and read from it. I need to read that data coming from the device and save it in XML format. even if I'm listening to one device  packets not coming in order Any idea how to do that. I saw some other Questions here and I tried all this answers but nothing talking about reading the XML file. I’m using the same code as in this article http://www.codeproject.com/KB/IP/CSNetworkSniffer.aspx Do you absolutely need to use low-level tcp sockets? It seems like using a WCF service would be easier as long as it meets your requirements. If you're using TCP then the data for each connection should be arriving in order. Make sure you are using a separate connection for each device and storing the data for each device separately. nothing working from the above..i tried alot of methods to get it working but nothing working . data not coming in order at all Why are you using a sniffer to receive XML data? Why not just listen on a TCP port yourself? You are sniffing the RAW packets that are passing through the network driver. At this low level it will be up to you to process and reconstruct the messages by analysing the packets and using things like the TCP seq the ack/nak messages etc. Here is the wikipedia article on the TCP protocol that might help you get started before delving into the deeper side of things. http://en.wikipedia.org/wiki/Transmission_Control_Protocol And of course the most critical source the RFC. http://tools.ietf.org/html/rfc793 i assume you are sniffing the data because you have some other application that is actually acting as a server and listening for the data. If now you really should just write an application that listens on the TCP port and reads the data from there. This way the TCP protocol will ensure that you receive all the data in the right order etc. To get started with this you could use the TcpListener class or you go use the lower level Socket classes thanks a lot...that's helped me..all problems solved
46,A,Interprocess Communication over Network in C++ I'm using C++ to develop the server side of a game which multiple servers need to communicate with each other using Publisher/Subscriber pattern. Each server subscribes to some events on different servers. All servers are located on the same network so latency and packet-loss should be very low. The inter-server communication should be as efficient as possible since many events happen each second. Are there any libraries out there i can use for this purpose? Any help is much appreciated What is your OS and compiler? The server is Linux based and we used gcc to compile it You're essentially talking about a socket library. That's different from IPC which refers to Interprocess Communication between two or more processes on the same computer - as opposed to over a network. Network communication requires socket programming so you probably want a good high-performance socket library. Boost.ASIO is a very good C++ portable socket library which provides both TCP and UDP sockets. In fact IPC can mean both: communication of multiple processes on the same computer or one multiple computers connected via network. Nevertheless TCP/IP sockets are the tool of choice for that.  You should be able to combine Boost.ASIO (for async sockets I/O) with Boost.Signals (for Observer pattern) or Boost.Signals2 (threadsafe version of Boost.Signals) to achieve what you want. There is a simple example showing how this might work here. Note that this uses a wrapper on Boost.Signal but you get the idea.
47,A,"FSM to actual java code. Not sure if code is correct This is for school but not homework. I'm trying to understand how RDT is implemented and i was able to find another school that had a simulator already created but just have to fill in the sender and receiver. Anyway I can't get it work properly. There is some Pseudo code and an FSM diagram for both the sender and receiver. I believe the sender is correct but I'm not too sure about the receiver. I've never dealt with finite state machine diagrams before so I'm still trying to figure that out too. I'll list the receiver to make sure that's right and post more code if need be. Global Variables:  astate =0 // can be 0-3 corresponds to state diagram for a/sender (3.15) astored_pkt // saved in case we need to retransmit - for a/sender bstate = 0 // can be 0-1state diagram for b (3.14) bOnceThru = 0 // A flag to track if this is the first time through the receiver bstored_pkt // saved in case we need to retransmit - for b/sender Pseudo Code: bInput if pkt is corrupt if bOnceThru==1 send bstored_pkt else do nothing else if (seqno==0 && bstate==0) || (seqno==1 && bstate==1) deliver data to layer 5 Print ""B: got packet #"" create bstored_pkt sendpkt(bstored_pkt) Print ""B: sending ACK #"" incr bstate mod 2 bOnceThru=1 else if bstate==1 || bOnceThru==1 send bstored_pkt Print ""B: sending ACK #"" Java Code: protected void bInput(Packet packet){ if(!isPktCorrupt(packet)){ if(bOnceThru == 1){ toLayer3(1bstored_pkt); }else{ return; } }else{ if((packet.getSeqnum() == 0 && bstate == 0) || (packet.getSeqnum() == 1 && bstate == 1)){ toLayer5(packet.getPayload()); System.out.println(""B: got packet #""); bstored_pkt = new Packet(packet); toLayer3(1bstored_pkt); System.out.println(""B: send ACK "" + packet.getAcknum()); bstate = (bstate + 1) % 2; if(packet.getSeqnum() == 0) bOnceThru = 1; }else{ if(bstate == 1 || bOnceThru == 1){ toLayer3(1bstored_pkt); System.out.println(""B: sending ACK "" + packet.getAcknum()); } } } } toLayer3 and toLayer5 are methods implemented already nothing i need to worry about. Also isPktCorrupt is a method to verify the check sum of the current packet with what it has stored. Does this seem correct? Is the pseudo code even correct? It runs ok for the first packet and then the second packet just keeps trying to resend. I can't see why though. If i need to i'll post the Sender as well. Thanks! This code is correct. I did find some errors in my sender (minor) but it threw it completely off. I had to change one thing in the receiver code though and that was the increment of bstate. So in case anyone is wondering or has a similar question this code does correlate to the FSM and it does work."
48,A,"Proper way to create packets along with sending and receiving them in socket programming using C I had written a small client-server code where in I was sending integers and characters from my client to server. So I know the basics of socket programming in C like the steps to follow and all. Now I want to create a packet and send it to my server. I thought that I will create a structure  struct packet { int srcID; long int data; ..... ..... ..... }; struct packet * pkt; Before doing send() I thought that I will write values inside the packet using  pkt-> srcID = 01 pkt-> data = 1 2 3 4 I need to know whether I am on the right path and if yes then can I send using  send(sockfd &packet sizeof(packet) 0) for receiving  recv(newsockfd &PACKET sizeof(PACKET) 0) I have just started with network programming so I am not sure whether i am on the right path or not. It would be of great help if anyone can guide me with my question in any form (theoreticalexamples etc). Thanks in advance. Since last night when I started with this program when i am compiling I am getting a lot of dereferencing errors I have just started with C and C++ so a bit of amateur with pointers I am posting my code can you explain me where i am going wrong: @Nemo: This depends on on the transport protocol. In TCP yes recv() may return fewer bytes but in UDP recv() returns the whole packet. Yes you are on the right track. However if you want to be forward-looking you cannot assume that the sender and receiver are the same architecture... So you want to think hard about how to encode your data on the wire. Search for ""serialiazation"". Also `recv` can return fewer bytes than you requested so you always need to call it in a loop until you get all the bytes you are looking for. Thanks for your response....will keep your tip in mind. Directly writing structs out to the network is seductively clean simple neat... and unfortunately wrong. (This doesn't stop a lot of people including many who should know better from doing it anyway). There's several reasons for this1: Data representation. This includes the sizes of various types concerns like endianness and floating point formats. You can't assume that these are the same on the other end of the connection as they are on your end; they vary a lot by architecture. Structure layout. Compilers typically add invisible padding between members of structs when they have uneven size - and the layout of this padding also varies by architecture. Bitfields are another structure layout variable. Canonicalisation. In particular pointer members are meaningless to send across the socket - they don't mean anything to the other side. You have to send what it points to instead. Another aspect of this is sending an entire array when only some of it is filled with meaningful values - the uninitialised members might leak memory contents that you don't want them to (and sending them is wasteful anyway). Partial reads. You end up needing to use a char * pointer when reading anyway because you might have to resume reading partway through a struct (eg. your struct might be 830 bytes long but the first read returned only 500 bytes). The right way to go about this is to use a process called serialisation: for each data structure you want to send across the network you have a function which canonicalises the contents and packs them into a char buffer in a defined format. This includes converting integers to a specific endianness (functions like htonl() are useful here). There's also a corresponding function that unpacks a char buffer into the struct form used on the recieving side. There are several existing libraries of code for serialisation - for example Google's Protocol Buffers. The other workable alternative is to serialise your data structures into a textual format like JSON. This is very good for troubleshooting because it means that your network protocol is somewhat human-readable. 1. Anticipating some objections: Yes there are workarounds for most of those issues. But that's just what they are: workarounds that often rely on compiler-specific features cancel out most of the simplicity benefits and still aren't completely reliable anyway.  Packet Structure When you are going to send the data across the network then you need to consider having a fixed size header followed by a variable length payload. [1 byte header] [Variable byte Payload] The header should give you the size of data you are going to send so that the receiver will always read a fixed size header and then determine the packet length and read the rest of the bytes. eg: int nRet = recv(nSock(char*)pBufferMESSAGE_HEADER_LENGTH0); if (nRet ==MESSAGE_HEADER_LENGTH) { int nSizeOfPayload = //Get the length from pBuffer; char* pData = new Char(nSizeOfPayload ); int nPayloadLen = recv(nSock (char*)pDatanSizeOfPayload 0); } Variable length data in payload If your structure is having string you should always have the size of the string appended before the string. Endianess If you are sending the packet to two different applications running in different machines you need to agree before hand on how you are representing your bytes i.e whether you are going to send MSB first or LSB first.  The pointer pkt is NOT defined in your application. You have two options: 1) Declare pkt as a normal variable  struct packet pkt; pkt.srcID = 01; .... send(sockfd &pkt sizeof(struct packet) 0); 2) The second approach is useful when your packet contains a header followed by a payload:  char buffer[MAX_PACKET_SIZE]; struct packet *pkt = (struct packet *) buffer; char *payload = buffer + sizeof(struct packet); int packet_size; /* should be computed as header size + payload size */ pkt->srcID = 01; ... packet_size = sizeof(struct packet) /* + payload size */ ; send(sockfd pkt packet_size 0); .... UPDATED (to answer your comment): First you should know that receiving from a TCP socket MAY NOT provide the whole packet. You need to implement loop (as suggested by Nemo) to read the whole packet. Since you prefer the second option then you need two loops. The first loop is to read the packet header to extract the payload size and the second loop to read the data. In case of UDP you don't need to worry about partial receiving. Here is a sample code (without looping) where sockfd is a UDP socket: char buffer[MAX_PACKET_SIZE]; struct packet *pkt = (struct packet *) buffer; char *payload = buffer + sizeof(struct packet); int packet_size; /* should be computed as header size + payload size */ ..... /* read the whole packet */ if (recv(sockfd pkt MAX_PACKET_SIZE 0) < 0) { /* error in receiving the packet. It is up to you how to handle it */ } /* Now you can extract srcID as pkt->srcID */ /* you can get data by processing payload variable */ Remember: * you need to implement serialization as mentioned by other users * UDP is unreliable transport protocol while TCP is a reliable transport protocol. Thanks for your reply. I would like to go with the second option (header+data). At the receiver when I want to store the data part of packet I can do recvd_data.srcID = myID; something like that right?? i was looking for concepts on serialization in ""GOOGLE"" specifically for socket programming....Is ther any good guide available......"
49,A,"Transferring binary data using sockets? I am thinking of making a server/client application that does this: the client(s) will connect to the server and the server will send back a list of folders (probably music or videos) that the client will be able to stream or download. I am going to make a GUI for this so that it won't be a text-only interface. I want it so that when the client connects to the server and gets the information about the media files that can be streamed/downloaded the client will show the folders/files in an explorer-like interface (folders will show up with a folder icon videos will show up with a video icon etc.) This seems really big to me but I really want to learn more about socket programming with C# and feel that getting my hands really dirty is the best way. As you can imagine I have a lot of confusion about how to start this and what it is that I need to do to make this work. The first part of my question is: If I want to let a client receive a list of files/folders and also have the ability to download/stream them do I need to do something with transferring binary data using sockets or am I completely wrong on that? If I'm wrong do I need to use something else like file readers and somehow use them over sockets? The second part to my question is: when I transfer this information over to a client how can I make the client show the appropriate icons for videos folders mp3s etc.? I'm thinking that if I have to transfer binary data I will have to use that data to somehow populate the client GUI with the correct icons/data. I am stuck on exactly how to handle that. Are there certain methods/classes that I can use to parse that data and do what I want to do with it? Again I don't know if I'm wrong about needing to transfer binary data but either way I am confused on what it is that's necessary to handle this. I apologize if any of my terminology is wrong. I am obviously not an expert in C# yet ;) Getting a file icon: http://stackoverflow.com/questions/2701263/get-the-icon-for-a-given-extension Wow thanks! Guess I should explore the .NET framework a little more... You're welcome. That and explore StackOverflow - if there's a common programming problem that needs solving it'll probably already be here - Enjoy! Any time you are sending data between machines (or between processes) you are already talking binary data (you could argue that everything is binary data but when working with it as an OO model it doesn't feel like it). The point here is that all you need to do is serialize your messages. There are a myriad of serialization frameworks - some aimed at convenience some aimed to range-of-features and som can use aimed at performance (minimum bandwidth etc). Even the inbuilt ones would work - for example you can use XmlSerializer etc to write to a Stream (such as the NetworkStream you typically use with a socket). But please - not BinaryFormatter (it will hurt you honest). Personally I'd use something a bit less verbose such as protobuf-net but I'm somewhat biased since I wrote it ;p However note that when working with sockets you are typically sending multiple (separate) messages as part of the conversation. In order for each end to correctly and conveniently read the right data per-message from the stream it is common to prefix each message with the length (number of bytes) - for example as a little-endian int taking a fixed 4 bytes. So the reading process is: read 4 bytes interpret that as an int let's say n read n bytes run that data through your deserializer perhaps via MemoryStream process the message and the sending process might be: serialize the message perhaps to a MemoryStream query the length and form the 4 byte prefix write the prefix write the data Thanks Marc this answered a bunch of my outstanding questions :)  You don't have to go to as low level as Socket unless you just want to learn of course. I suggest using WCF. It will completely meet your requirements will be flexible and you will not have to deal with sockets directly. WCF is a big framework but you can quickly learn how to use it. Introduction to WCF Here you can find how to retrieve and show file icon in Windows. If you client is hosted by Windows you don't need to transfer icons you can just retrieve image on the client by filename/extension. Thanks for you recommendation. I thought of maybe picking up WCF but I think that I'd still like to have the knowledge and experience of how things work at the socket level. I may take your suggestion and go with WCF but I am still having a really difficult time of wrapping my head around how all of this kind of stuff would work over a network. All these different types of data protocols and structures are confusing me. I haven't done much programming with projects that work over the internet... @Aaron - Socket is not difficult especially if you use TCP. It is just sending raw bytes back and forth between client and server. However what is more difficult is to format and parse data make sure you resend/reconnect properly and many other low level issues. WCF solves those issues for you and you can focus on business logic and requirements instead of reinventing the wheel. I see. Is WCF generally the standard nowadays? Is using sockets in C# considered ""old"" or ""deprecated"" these days? @Aaron - WCF is kind of a standard. It allows you to abstract you business logic from how data a transferred between peers. You can set WCF to transfer data vias SOAP or MSMQ or net-tcp in your case without changing code just changing settings file. If you use net-tcp binding it will use Socket underneath. However you can't say that WCF replaces Sockets. There are legit cases to use Socket but this is definitely not your case. Okay great. Thanks very much again for all of your help on this! @Aaron - just to say: there are many opinions on this subject. Personally I appreciate that WCF is *convenient* but the trade-off there is that it is pretty bloated - you get a *significant* amount of overhead on your data. Sockets are still very much in usage too. Either is a valid choice."
50,A,Measuring link quality between two machines Are there some standard methods(libraries) for measuring quality of link/connection between two computers. This results would be used to improve routing logic. If connection condition is unacceptable stop data transfer to that computer and initiate alternative route for that transfer. It looks like Skype has some of this functionality. I was thinking to establish several continuous testing streams that can show bandwidth problems and some kind of ping-pong messaging logic to show latency values. What kind of routing logic would you provide at the host-level? Are you talking about routing to various servers in a pool or using something like IP source-routing? I am talking about routing to other available servers in a pool. Link Reliability I usually use a continuous traceroute (i.e. mtr) for isolating unreliable links; but for your purposes you could start with average ping statistics as @recursive mentioned. Migrate to more complicated things (like a UDP/TCP echo protocol) if you find that ICMP is getting blocked too often by client firewalls in the path. Bandwidth / Delay Estimation For bandwidth and delay estimation yaz provides a low-bandwidth algorithm to estimate throughput / delay along the path; it uses two different endpoints for measurement so your client and servers will need to coordinate their usage. Sally Floyd maintains a pretty good list of bandwidth estimation tools that you may want to check out if yaz isn't what you are looking for.  Ping is good for testing latency but not bandwidth.
51,A,"Testing for a closed socket I'm trying to test for a closed socket that has been gracefully closed by the peer without incurring the latency hit of a double send to induce a SIGPIPE. One of the assumptions here is that the socket if closed was gracefully closed by the peer immediately after it's last write / send. Actual errors like a premature close are dealt with else where in the code. If the socket is still open there will be 0 or more bytes data which I don't actually want to pull out of the socket buffer yet. I was thinking that I could call int ret = recv(sockfd buf 1 MSG_DONTWAIT | MSG_PEEK); to determine if the socket is still connected. If it's connected but there's no data in the buffer I'll get a return of -1 with errno == EAGAIN and return the sockfd for reuse. If it's been gracefully closed by the peer I'll get ret == 0 and open a new connection. I've tested this and it seems to work. However I suspect there is a small window between when I recv the last bit of my data and when the peer FIN arrives in which I could get a false-positive EAGAIN from my test recv. Is this going to bite me or is there a better way of doing this? An alternate way is select() (or poll()) since it can control a fd both rfor read and for write but I dont know if it actually solve your problem about the FIN timing. @Giuseppe: No I don't think select would solve the timing problem. OK so I ran some more tests and this is what I found. I set my client up to send HTTP/1.1 Connection: close messages to the server causing the server to call close after it's last write of data. When my client finished reading data from a GET transaction it would test the socket to see if it was still open using the above method and then attempt to issue another GET. What I found is that approximately 30% of the time my test would occur before the server's FIN arrived leading to false-positives and failed operations. Probably the only way to make this reasonably reliable say close to 99% would be to introduce artificial delays related to the connection latency between the last read and the attempted socket reuse - however that would pretty much murder performance. So I have to conclude that while this tool is useful it's only marginally so. +1 : interesting tests. One can understand why in telecom 90% of the code is for managing exceptional behavior :)  (Slightly ""off topic"" sorry). Perhaps this discussion may be helpful four you. It still doesn't answer to you ""FIN"" question but may help you to react in a more easy way to the peer shutdown while your program is sending. Bye  Do you control the other end? The most reliable way to perform a ""clean"" shut down of a socket is for the other end to send a ""goodbye"" message of some sort. No I don't control the other end. However the other end **should** always send a application level message indicating that they are closing from their end. It's just that in the real world that doesn't always happen. I could treat it like a regular error but I would like to try and actively catch it most of the time."
52,A,"libpcap: how to get active network interface (Mac OSX) I am using libpcap to sniff traffic. I would like to do it on the currently active network device (e.g. one that has an assigned IP address etc). What's the best way to do this? I'm assuming I would have to do the following: pcap_findalldevs(&alldevs errbuf) to get all the network devices then loop through and check which one is currently active. Edit: The following function (pcap_lookupnet(dev &net &mask errbuf) returns the network address and subnet mask for a network device. I ran some tests with the different ethernet adapters on my computer and it returns -1 when I call it on an adapter that is not connected to a network. Would this be the bulletproof way to get an active interface? Are there any edge cases it would miss? What if I'm using two different networks at the same time or even just two adapters connected to the same network? I actually employ this scenario on a regular basis assigning the adapters to different processes. going back to [adirau's answer](http://stackoverflow.com/questions/4358284/libpcap-how-to-get-active-network-interface/4358782#4358782) If I was to run netstat -nr (unix) while 2 network adapters were connected would it show both adapters for the default route? the API pcap has for looking up interfaces conforming to some user defined rules is trivial. You could indeed use pcap_findalldevs() to interate over all suitable-to-use network devices or use pcap_lookupdev() to get the next network device that you can use with pcap. Defining what is the interface you want to use with your sniffer may be problematic (code wise) on systems with multiple network devices and you would want to define more explicit rules for choosing such an interface. Such rules are usually statically defined (like ""the active interface with the default route installed""). However you may have multiple default routes (think load balancing) and here you may either want to sniff on all of them or (for example) only on the ppp interface. So choosing the target interface I would say is a task to be resolved outside the sniffer rather than at runtime in the sniffer code. For example: If by ""active interface"" we understand the interface on which the default route is installed (i assume a linux system here): ip route show 0.0.0.0/0 | awk ' { print $5 ; } ' | xargs ./sniffer if you want to get the active interface which has the default route installed on from your sniffer code you would rather use netlink(7) or proc(5) (/proc/net/route) than pcap's device lookup api but the complexity is high. In conclusion the interface lookup logic can be easily scripted into some wrapper program on any system and the result(s) passed as parameter(s) to your sniffer. I've updated my question would pcap_lookupnet suffice? Ideally I would like to check within the sniffer code. if you're using pcap_findalldevs() to loop over all interfaces you don't need pcap_lookupnet() to get ipv4 infos on each device returned by the former. this info is self contained in the devices information returned by pcap_findalldevs(). again it really depends on what you understand by an ""active interface"". if it's just any interface that is up and has an ipv4 address configured then yes. it is not clear to me what pcap_lookupnet() will return on some systems (like FreeBSD) if multiple aliases are loaded on the same interface (no virtual interfaces prolly first ip will be returned)  According to pcap_open_live (3): DESCRIPTION pcap_open_live() is used to obtain a packet capture handle to look at packets on the network. device is a string that specifies the network device to open; on Linux systems with 2.2 or later kernels a device argument of ""any"" or NULL can be used to capture packets from all interfaces. But it seems it's deprecated now you should use pcap_create(3) Sorry I forgot to mention I'm doing this on OS X. That would have been very comvenient!  Why don't you capture on 'any' device (Pseudo-device that captures on all interfaces) ? Any way here is a little snippet that will help you find 'active' interfaces #include <stdio.h> #include <pcap.h> #include <sys/socket.h> #include <netinet/in.h> #include <arpa/inet.h> static void dump_addresses (pcap_addr_t *addresses) { pcap_addr_t *addr = addresses; printf(""(""); while (addr) { struct sockaddr_in *ip = (struct sockaddr_in *)addr->addr; struct sockaddr_in *nm = (struct sockaddr_in *)addr->netmask; if (ip && nm) printf(""%s/%s "" inet_ntoa(ip->sin_addr) inet_ntoa(nm->sin_addr)); addr = addr->next; } printf("")""); } static void devs_dump (pcap_if_t *devs) { pcap_if_t *dev = devs; while (dev) { printf(""dev: %s - %s - "" dev->name dev->description); dump_addresses(dev->addresses); printf(""\n""); dev = dev->next; } } int main(int argc char *argv[]) { int r; char errbuf[PCAP_ERRBUF_SIZE]; pcap_if_t *devs; r = pcap_findalldevs (&devs errbuf); if (r) { printf(""Findalldevs: %d (%s)\n"" r errbuf); return -1; } devs_dump(devs); pcap_freealldevs (devs); return 0; } Can you elaborate? ""any"" is not a supported keyword. pcap_open_live(""any"" SNAP_LEN 1 1000 errbuf) gives an error ""could not open device any"" Is there significant overhead associated with capturing on all devices? I would like the sniffer to run in the background with minimal resource usage. I'm asking because I read that the only way to sniff on all devices would involve sniffing each network interface on a separate thread. Is that true? I think that's is more efficient to capture on 'any' instead on eth{01...N} How would you capture on 'any; device? use 'any' as device name  I've been down this road several times before and usually find myself falling back to adding a -i switch to allow the user to precisely identify the interface. It makes your job simpler and avoids any edge conditions."
53,A,"Boost ASIO async acceptor not opening a listening port OS : linux 64 bit ARCH. BOOST : 1.46.1 COMPILER : clang++ / GCC. I have a code fragment that has the wire up of a tcp acceptor modelled on a boost::asio example (Chat Server). However when I run the fragment No listening TCP socket shows up in a netstat listening(linux) . However the chat server example when compiled does show up. Could someone please point out what I am doing wrong ? #include <boost/asio.hpp> #include <boost/shared_ptr.hpp> #include <boost/bind.hpp> #include <list> #include <iostream> using namespace boost::asio; using namespace boost::asio::ip; class ClientConnection { public: ClientConnection(io_service & io_s) : m_socket(io_s) {} tcp::socket & socket() { return m_socket; } private: tcp::socket m_socket; }; typedef boost::shared_ptr<ClientConnection> client_connection_ptr; class ClientConnectionAcceptor { public: ClientConnectionAcceptor(unsigned short port) : m_io_service() m_port(port) m_endpoint(tcp::v4() m_port) m_acceptor(m_io_service m_endpoint) { std::cout << ""acceptor is open : "" << m_acceptor.is_open() << std::endl; client_connection_ptr ccp(new ClientConnection(m_io_service)); m_acceptor.async_accept( ccp->socket() boost::bind(&ClientConnectionAcceptor::handle_acceptthis ccp placeholders::error)); } void handle_accept(client_connection_ptr ccp const boost::system::error_code & error) { std::cout << ""in handle_accept"" << std::endl; if(!error) { // m_rpc_oracle.AddNewClient(ccp); client_connection_ptr new_ccp(new ClientConnection(m_io_service)); m_acceptor.async_accept( new_ccp->socket() boost::bind(&ClientConnectionAcceptor::handle_acceptthis ccp placeholders::error)); } } io_service & io_service() { return m_io_service; } private: boost::asio::io_service m_io_service; tcp::endpoint m_endpoint; tcp::acceptor m_acceptor; unsigned short m_port; }; int main() { ClientConnectionAcceptor acceptor(5000); acceptor.io_service().run(); } I got an error when compiling your code async_accept.cc:56: error: declaration of ‘boost::asio::io_service& ClientConnectionAcceptor::io_service()’ /opt/local/include/boost/asio/io_service.hpp:186: error: changes meaning of ‘io_service’ from ‘class boost::asio::io_service’ changing io_service & io_service() { return m_io_service; } to io_service & get_io_service() { return m_io_service; } seems to resolve the compiler failure. Running the resulting binary shows a listen socket in netstat -l -t for me.  I found that I could get this to work on my Windows machine if I changed the endpoint and acceptor to shared pointers and instead of creating them by passing them as arguments in the constructor I specifically created the shared pointers inside the constructor. I'm not exactly sure why this works. My only guess is that perhaps there are no guarantees that the constructor arguments are passed or created in the order they appear and thus you may try to create the acceptor with an endpoint that isn't correctly initialized yet? That's really my only guess. Let me know if this works for you. I could successfully connect via localhost on port 5000. Without these changes the client that I tried to connect with via localhost told me the connection was actively refused. This arrangement was successful however and seems to deviate as little as possible from your original code. Hope it helps. #include <boost/asio.hpp> #include <boost/shared_ptr.hpp> #include <boost/bind.hpp> #include <list> #include <iostream> using namespace boost::asio; using namespace boost::asio::ip; class ClientConnection { public: ClientConnection(io_service & io_s) : m_socket(io_s) {} tcp::socket & socket() { return m_socket; } private: tcp::socket m_socket; }; typedef boost::shared_ptr<ClientConnection> client_connection_ptr; class ClientConnectionAcceptor { public: ClientConnectionAcceptor(unsigned short port) : m_io_service() m_port(port) { // now initializing endpoint and acceptor as shared pointers inside the constructor m_endpoint = boost::shared_ptr<tcp::endpoint>(new tcp::endpoint(tcp::v4() m_port)); m_acceptor = boost::shared_ptr<tcp::acceptor>(new tcp::acceptor(m_io_service *m_endpoint)); std::cout << ""acceptor is open : "" << m_acceptor->is_open() << std::endl; client_connection_ptr ccp(new ClientConnection(m_io_service)); m_acceptor->async_accept( ccp->socket() boost::bind(&ClientConnectionAcceptor::handle_acceptthis ccp placeholders::error)); } void handle_accept(client_connection_ptr ccp const boost::system::error_code & error) { std::cout << ""in handle_accept"" << std::endl; if(!error) { // m_rpc_oracle.AddNewClient(ccp); client_connection_ptr new_ccp(new ClientConnection(m_io_service)); m_acceptor->async_accept( new_ccp->socket() boost::bind(&ClientConnectionAcceptor::handle_acceptthis ccp placeholders::error)); } } io_service & io_service() { return m_io_service; } private: boost::asio::io_service m_io_service; boost::shared_ptr<tcp::endpoint> m_endpoint; boost::shared_ptr<tcp::acceptor> m_acceptor; unsigned short m_port; }; int main() { ClientConnectionAcceptor acceptor(5000); acceptor.io_service().run(); } EDIT After some further investigation it was discovered that the problem was actually related to the initializer list for the ClientConnectionAcceptor class. In the class definition the member m_port was declared after m_endpoint and m_acceptor. As a result even though the initializer list appeared to set up the port number before the endpoint and acceptor were created in fact the port value was not valid or initialized until after the endpoint and acceptor were already created. Changing the class definition to have the member m_port declared before endpoint and acceptor fixes the problem. wow that did solve the problem thank you. It probably has something to do with the initialization order in the initialization list. I thought it was top down . http://stackoverflow.com/questions/4037219/order-of-execution-in-constructor-initialization-list. User ""in silico""'s response My assumptions where that initializer list order determined order-of-evaluation. But it's the order of declaration that determines the order. I still have to try it out though. Does that answer your question then or was there something else? Yes it does It is still odd that it works for others though :D Yeah -- I just read over that answer and your code example does have the declarations in the correct order also so I'm not sure what the problem could be exactly... @Hassan Syed I've found the problem. I'll edit the original answer to add it in. Much Appreciated :D"
54,A,Network topology information I Have a small business network with a couple of switches end devices but only one router. I want to display the network topology like a graph would like (with the router on top). I'm only have access to network layer addresses so I managed to obtain for every device on the network it's ip address and ip network i.e 192.168.2.9 and 192.168.2.0 (mask 255.255.255.0) for every interface that device has. My guess is that I could analyze the data and build up the network's logical connections. So What I want to ask is if I'm on the right path to know the network topology (at least for it's logical connections). This is all done programmatically (c and objective-c) and is for a school project. PLUS: Does anyone know any library that would draw (given this information) the topology? aha - that's cool too. Never worked with those libs personally Don't know about the network topology and stuff but maybe [Graphviz](http://www.graphviz.org/) can provide the drawing abilities you need (see also [Wikipedia article](http://en.wikipedia.org/wiki/Graphviz)). I don't know how it's implemented but I know *many* monitoring tools (including OpenNMS) do this (albeit in Java for OpenNMS). Likewise I know that HP's SAV tool (that comes with HP's Server Automation tool) can do this in conjunction with Network Automation. I presume you've already `nmap` 'd your environment? also this looks like a build-upon your previous question: http://stackoverflow.com/questions/3509876/network-topology @warren yes it is. Any problem that I would make another question out of it? no problem just wanted to make folks aware that you'd asked a previous question on the topic and this is a follow-on :) Ok It's just that I don't know all the rules for this community. That's why I asked! @warren I didn't nmap'd the environment. The project was designed to do it from the ground up. It's all done using c libraries like pcap For graphing the easiest approach might be to output a file in dot and then graph it with graphviz. Thanks @Codeninja The final answer would be to do both things run the pseudo algorithm @Tim proposed and on the way writing a dot file with the network especification.  So you have every device's IP address already a useful start. From there a 'manual algorithm' might be: for each ip in devices traceroute ip for each hop in traceroute add hop to graph (if it's not there already) What you're doing is adding each network hop between yourself and the device to a graph structure. If a node (hop) is already found then you add a new edge. If not you add an edge and a vertex. The end result will be a graph of every node on the network and the paths taken to reach them - your topology. So all you've got to do is implement traceroute yourself build the graph structure to store the results of your traceroute runs and then make something to plot it all nicely! Each of these could generate many questions of their own. As you've tagged this Objective-C I'll make a leap and assume you're doing this on a Mac. If that is the case your graphics needs are met nicely with Cocoa's drawing API. yeah I did it both in Linux and Mac so I can use Cocoa's drawing. The thing is I haven't used cocoa and don't have much time to learn. So I have to use whatever library is the fastest to learn and have the most documentation I don't know the full scope of your project but if you can get away with @CodeninjaTim's suggestion below then that'd be much much easier for you (the difficult bit isn't drawing the lines and rectangles it's figuring out how to lay out the plot which Cocoa won't help you with). Cocoa drawing is pretty straightforward and Apple's tutorials are quite good but I found this http://cocoadevcentral.com/d/intro_to_quartz/ very helpful as a first intro if you do go that route.
55,A,"Bluetooth library for Windows Where can I find the bluetooth API details for Windows Xp dlls etc for PAN connection etc. Thanks and Regards Anjali Which language/Framework are you using? ""MSDN Bluetooth"" on any search engine: http://msdn.microsoft.com/en-us/library/aa362932(VS.85).aspx"
56,A,"How do we define 'true p2p' I have seen a number of questions in which the term 'true p2p' is used. What exactly is meant by this term if it has an exact meaning? I am familiar with the term p2p and I can think of several possible meanings for 'true p2p'. What does it mean to you? What are some examples of: a) 'true' p2p? b) 'untrue p2p' I think I would differ slightly from Tom - I think a piece of software can be regarded as peer-to-peer if peer communication is a substantial part of the way it works. So Skype and Napster are/were P2P even though they require co-ordinating servers. I've not come across the term 'true P2P' - it may be referring to the fact that some people use it as a marketing buzzword without knowing what it means.  I think that by ""true P2P"" you mean ""pure P2P"". Pure P2P means totally decentralization. I.e. true P2P doesnt have a central coordinator server."
57,A,Packet Sniffing using Raw Sockets in Linux in C I need to write a packet sniffer in Linux that detects HTTPS packet that are sent and save the url from the request. I found code for this in security-freak and ran it. This code runs and only sniffs the received packet but I need to get the sent packet in the sniffer. How do I get the sent packet in this code? I can't use any library like libcap (forbidden). The code is :sniffer.c You should be using ETH_P_ALL instead of ETH_P_IP as the protocol. ETH_P_IP only listens for incoming IP packets. If the goal is to see http packets why not use ETH_P_IP? You don't need everything. The goal is to see outgoing http packets. ETH_IP_P only sees incoming packets. See thread: http://lkml.indiana.edu/hypermail/linux/kernel/9604.1/0603.html  Why can't you use any library? Homework? It's hard to answer without having examples from your code for example how you set sll_pkttype. The urlsnarf tool in the dnsiff suite could be worth a look. post edited and attach my code  With appropriate libpcap or DNET usage You should be able to get all network traffic on the desired layer (protocol - 5) (also this outgoing). But You should know that already. You need to go through the above libraries manuals and find the appropriate filtering. we cant use any library like libcap or dnet Are you sure that your system allows detecting outgoing packets? You could use libpcap to determine if it's even possible.
58,A,"Error with exit value: {function_clause[{inettcp_close[[]]}{}]} I am getting this error in my server when client runs more than 5 minutes? I am spawning my server each time whenever a client connects. But if the client is running considerable time with server i.e multiple times regularly I am getting the *"" Error in process <0.111.0> with exit value: {function_clause[{inettcp_close[[]]}{run_serverfunction8}]}"".* I think it is from inet options.... please any one can give some idea to overcome the error. You are passing in an empty list to inet:tcp_close/1 instead of a port. Not without arguments but with an empty list. Otherwise he would be getting an undef error and the stack trace would show `{inettcp_close[]}` rather than `{inettcp_close[[]]}`. Thanks for being more thorough than I am. thanks dude its right."
59,A,"MAC Address from IP across network I hope you're all well. I'm wondering if you could help me or point me in the right direction. I'm currently working on a project that centers around network management. Due to severe time constraints where possible I'm using opensource code. The issue I'm having is that part of the project requires me to be able to capture the MAC addresses of all of the devices that are connected to the network. My knowledge of network orientated programming is limited as I have been working in other areas of software engineering for the past 4 years. The approach I have taken is to use nmap as a basis to get the ip address and other information I need. The MAC address is not included in the nmap out put and from what I have read it seems to be a bit flakey. (i could be wrong). Therefore I have tried to do this in a two stage approach firstly I get the data including ip address from nmap which works fine. My next step and the bit I'm having difficulty with is I ping the ip address (from within my python program) which works. But how do I get the MAC Address from the IP address? I initially thought ping the ip and grab the MAC from the ARP but I think that will only work if the IP address is on the same subnet. to compound the problem on deployment there could be up to 5000 computers on the network that needs to be logged. To show you my python ping approach this is the code: import pdb os import subprocess import re from subprocess import Popen PIPE # This will only work within the netmask of the machine the program is running on cross router MACs will be lost ip =""192.168.0.4"" #PING to place target into system's ARP cache process = subprocess.Popen([""ping"" ""-c""""4"" ip] stdout=subprocess.PIPE) process.wait() result = process.stdout.read() print(result) #MAC address from IP pid = Popen([""arp"" ""-n"" ip] stdout=PIPE) s = pid.communicate()[0] # [a-fA-F0-9] = find any character A-F upper and lower case as well as any number # [a-fA-F0-9]{2} = find that twice in a row # [a-fA-F0-9]{2}[:|\-] = followed by either a ?:? or a ?-? character (the backslash escapes the hyphen since the # hyphen itself is a valid metacharacter for that type of expression; this tells the regex to look for the hyphen character and ignore its role as an operator in this piece of the expression) # [a-fA-F0-9]{2}[:|\-]? = make that final ?:? or ?-? character optional; since the last pair of characters won't be followed by anything and we want them to be included too; that's a chunk of 2 or 3 characters so far # ([a-fA-F0-9]{2}[:|\-]?){6} = find this type of chunk 6 times in a row mac = re.search(r""([a-fA-F0-9]{2}[:|\-]?){6}"" s).groups()[0] #LINUX VERSION ARP mac = re.search(r""(([a-f\d]{12}\:){5}[a-f\d]{12})"" s).groups()[0] #MAC VERSION ARP print(mac) I have looked for some information but what I have found seems a bit vague. If you know of any ideas or avenues of research that may help me I would be greatful Cheers Chris I'd love to be proved wrong but I doubt you'll be able to get MAC addresses in other subnets. You cannot get the original MAC address of the host if you are not connected to the same subnet - you would just get the MAC address of the last router. The only way to get all MAC addresses would be to setup a server to catch them in each subnet but this seems to me a bit crazy idea. Note also that nowadays it is very easy to spoof the MAC address and it is not reliable at all. Over all I think you should use a different approach; for instance there are plenty of network inventory systems you could just use one of them and interface with it. Thanks for your quick response I'll have to look into network inventory systems  You can't directly get the MAC address of a machine outside your subnet. A common strategy for network management applications is to query machines that do have this information such as the routers and switches connecting the machines using SNMP. Routers have arp tables for the subnets to which they are directly connected (as they need this to do their job) and this information can be acquired from the router. The answers to this question might help with finding python library code to help in this endeavor. Cheers for your response and suggestions. It sounds like it could be just what I've been looking for."
60,A,"How to analyze/intercept packets before they are sent/received by the OS? I have always wondered how software firewalls work under the covers and would like to be able to write my own custom tools to analyze or intercept packets before they are sent or received by the OS. I'm fairly acquainted with core networking principles; I just have no clue where to start if I want to write software that fits inside the networking stack similar to the way firewalls do. Could anyone give me some pointers? I would be especially interested if this can be accomplished using C# but I can do other languages too. I am mainly focusing on Windows but would like to know if there were any cross-platform libraries out there as well. EDIT Using an NDIS driver (as Wireshark does) sounds like a good option and Vista's packet filtering capabilities sound neat but how do firewalls do it say on Windows XP? They don't have to install a special driver that I know of. On Windows Vista and up you might want to look at the Windows Filtering Platform. In earlier versions of Windows you need to use filter drivers (the linked to MSDN page mentions what technologies WFP replaces.) +1 Thanks a lot I was searching for this API from a long time.  As I recall it involves writing an NDIS driver. This sits practically on top of the NIC (Network Interface Card) and you have absolute control of what goes in or comes out of the NIC before anything else - right down to the ethernet packet level. This cannot be accomplished with C#. You really need to use C or C++ for this task. UPDATE: I last did this in Window XP days. I see from another response there is a new and by the looks of it simpler API if you are using Windows Vista onwards.  Not sure if it's ""before the O/S"" but have a look at WireShark and the library it uses libpcap. I do not think libpcap can choose to discard or modify packets before they are sent.  Take a look at WinPcap - it uses an NDIS driver to implement its packet filtering capabilities. This library can probably provide an excellent base for any packet inspection / firewall program you'd want to write and it's open-source. From http://www.winpcap.org/docs/docs_40_2/html/group__internals.html: First a capture system needs to bypass the operating systems's protocol stack in order to access the raw data transiting on the network. This requires a portion running inside the kernel of OS interacting directly with the network interface drivers. This portion is very system dependent and in our solution it is realized as a device driver called Netgroup Packet Filter (NPF); we provide different versions of the driver for Windows 95 Windows 98 Windows ME Windows NT 4 Windows 2000 and Windows XP. These drivers offer both basic features like packet capture and injection as well as more advanced ones like a programmable filtering system and a monitoring engine. The first one can be used to restrict a capture session to a subset of the network traffic (e.g. it is possible to capture only the ftp traffic generated by a particular host) the second one provides a powerful but simple to use mechanism to obtain statistics on the traffic (e.g. it is possible to obtain the network load or the amount of data exchanged between two hosts)."
61,A,How to fake source ip-address of a udp-packet? Think about the following: Your ISP offers you a dynamic ip-address (for example 123.123.123.123). My question is simple (the answer may not be): Is it possible to send a single udp-packet with an outer source-ip (for example 124.124.124.124) to a fixed-ip server? I don't need to get a answer from the server. I just want to know if/how this one way communication can be done using a faked source-ip address. The server and noone else should not be able to find out the real client ip. I'm sorry for my bad English! Thanks for you help in advance! The UDP packet does not actually have the source(your) IP address. The source IP address is part of the packet it is sent in. So you would have to modify the packet it is enclosed in. So while it is non-trivial it is possible. The packet structure for UDP and the enclosing packets for reference. So I need to set up the hole ipv4 packet (containing the faked ip) and send it to server? That is correct. Essentially you create a fake packet containing the UDP info and send that. May I ask why you are trying to do this? If I'm able to set up a prototype that can perfom the task I may write a security application as my final year project which allows multiple clients to exchange data over the internet without someone else knowing that there is a connection between the partners. Do you have any suggestion how to implement sending of the fake packet? Honestly I am not sure how do this and only knew about this because I am taking a Network Security class at the moment. Maybe make another SO question about that and you might get some suggestions from others. Thanks a lot! Can you please explain what a SO question is? SO = StackOverflow oh thx! see: http://stackoverflow.com/questions/2494489/how-to-send-raw-data-over-a-network Be careful the UDP header does contain the IP address ! Just read a bit more in the article you provide : http://en.wikipedia.org/wiki/User_Datagram_Protocol#Checksum_computation I found a way to send the manipulated packet (checked with wireshark)... but I assume the ISP drops it... because it cannot be received @raisyn why is it that it cannot be received? How does an ISP detect it to be suspicious?  This is IP-spoofing. Unless you ISP is a dodgy russian one it will probably prevent you from doing that (the first router will just drop the packet because it is suspicious). If you don't want to be identified you should try to find a proxy supporting UDP... Or you can buy a botnet. :) (if you didn't get it that's a joke don't do that)  You will need to have access your ISP Router in order to do that. If you send a raw UDP-Packet with all the information to the other server the Router will encapsulate it in another Package with your real ip. so you think it is not possible? as @jschoen says it is not trivial and it is probably against your ISP's Terms of Service. Do you have any idea how to send a enclosed packet (like jschoen says) using C# or C/C++? I have no idea but would look first in documentation from a Network Driver and see if that is possible in your ethernet card... but still I think that encapsulation takes place on the Router which your network card connects to so there will be trouble in trying to send a packet without your real-IP information to be sent. A router simply forwards the packets it receives. It may (depending on configuration) discard a packet having an obviously fake source but as a rule routers do not change the source address. NAT gateways and transparent proxies would. But only a VPN endpoint would actually encapsulate the packet further. @BenVoigt How is an obvious fake source determined? @Alex: [Reverse path filtering](http://tldp.org/HOWTO/Adv-Routing-HOWTO/lartc.kernel.rpf.html)
62,A,"how does CFNetworkCopyProxiesForURL work? Is it using DHCP? DNS? what techniques does it use to return a set of proxy suggestions to the caller? Is it parallel to the ""Auto Proxy Discovery"" in the OSX network settings? I can't find any information about this online. Apple's reference may just as well be written in egyptian symbols. I can't figure out what goes in ""CFDictionaryRef proxySettings"" in that method call. It uses SystemConfiguration.framework to look up the proxy information. The URL determines the type of service which determines which proxy setting to use. It may also affect how PAC files are evaluated. I believe that's correct. so you're saying when i set up the myriad of proxy options in my MacOSX settings/network/advanced/proxies and use a browser to go to a certain URL the **same mechanism** that would work for that would work to resolve which proxy (if any) to use would be used for the URL I dish out to this function?"
63,A,"comparing values sent to/from server by NetworkStream When u know why the sent string ""kamote"" to server and the string received ""kamote"" from server are not the same.. CLIENT  tcpClient = new TcpClient(); tcpClient.Connect(ServerIP Port); connectionState = (HandShake(""kamote"" tcpClient)) ? ""Connected to "" + ServerIP.ToString() : ""Host unreachable.""; private bool HandShake(String str TcpClient tcpClient) { using (NetworkStream ns = tcpClient.GetStream()) { byte[] toServer = Encoding.ASCII.GetBytes(str); ns.Write(toServer0toServer.Length); ns.Flush(); byte[] fromServer = new byte[10025]; ns.Read(fromServer 0 (int)tcpClient.ReceiveBufferSize); return Encoding.ASCII.GetString(fromServer).Equals(str); } } SERVER  TcpClient tcpClient = new TcpClient(); tcpClient = tcpListener.AcceptTcpClient(); NetworkStream ns = tcpClient.GetStream(); byte[] fromClient = new byte[10025]; ns.Read(fromClient 0 (int)tcpClient.ReceiveBufferSize); byte[] toClient = fromClient; ns.Write(toClient 0 toClient.Length); ns.Flush(); Client sent ""kamote"" Server received ""kamote"" Server sent ""kamote"" Client received ""kamote"" HandShake() always returns false. How can I fix this? As in the previous question you asked you're not keeping track of the number of bytes you received. So what's happening is this: On the client you send the string ""kamote"". On the server it receives that string into a buffer that's 10025 bytes long. The server then sends the entire buffer back to the client -- all 10025 bytes The client receives all or part of those 10025 bytes and converts them to a string. The string that gets converted is really ""kamote"" with a bunch of 0's after it. You must use the return value from Read to know how many bytes you received. there.. i got it.. i use this return Encoding.ASCII.GetString(fromServer).Substring(0str.Length).Equals(str); to terminate the extra.. anyway how to know if what is the length of bytes for this string ""kamote"" ?  Did you try limiting the string length to the actual read bytes like this: noOfBytes = ns.Read(bytes 0 ...); Encoding.ASCII.GetString(bytes 0 noOfBytes);  You are including a lot of 0 characters since you are including the entire fromServer in getstring. 0s don't print but they are there. You must tell it the correct number of bytes to decode."
64,A,"How does MW2 hide connection information from netstat? Today I was playing Modern Warfare 2. I was hosting ground war. There were something like 20 people playing. A notoroius blatant hacker joined. I wanted to get rid of him. My idea was to start adding ports to my firewall to find him and block him out. When I opened up netstat this is what I found. The last entry is immediately after I quit the game. The last IP shown belongs to steam. How does MW2 hide all the incoming connections from netstat? How can I overcome this? if your look to kick someone out of a game your hosting try this site http://www.adivinedude.com it has a tool with all the instructions and source code needed  It'll be using UDP not TCP so won't have fixed connections. Unfortunately you're not going to be able to find his IP from netstat for UDP; you'll have to look it up in-game. To get more useful information from netstat you wanted netstat -ano: that'll also show your open UDP listen sockets and the program IDs that own each one. You can then use task manager (view select columns add PID column) to identify which ones are owned by MW2. But UDP doesn't have fixed connections so it won't show information about who externally is using your UDP connections. I have to be able to push information out to the players as a server. How can I find that IP info? The MW2 server code will know it. There must be a way to query MW2 - an admin console or something? At a pinch you can install an ethernet logger such as Ethereal and watch the packets generated by the game but I'd be very surprised if there wasn't an admin / ban interface inside the game. (But I've never played it.) There is no admin/Ban interface. There are no interfaces at all. Infinitywar/valve made it so there is NO control by players. There are no admins. It sucks. Ethereal looks promising. There is a lot of data collected and there appears to be an API. There must also be some kind of .NET framework to do the same thing.  Yes there are admin tools for mwf2allso writing status in the console ""pressing ctrl+alt+supr in game so u can see console behind"". Also aIWNet_crt_u.exe ""cheat reporting tool"" will give u the ips and names and xuid.  Use a tcpdump/wireshark (used to be ethereal) type of a tool to analyze the traffic. The problem then is that you will have to pinpoint which udp stream belongs to the nasty person. If you are hosting the game and are able to kick people out you might want to record traffic and then kick him out and see which stream stops. Another option is to send him private messages and try to see where they are sent."
65,A,Bully Algorithm Does anyone know if it is possible to find a working implementation of the Bully algorithm in C# or Java somewhere? The pseudo code in the articles I have found is terrible. Many thanks. http://stackoverflow.com/questions/10210516/suspicious-code-output-for-bully-algorithm This paper gives a pretty good rundown and has a java version at the end.  There are probably errors in my implementation but it may help. https://github.com/benaston/taskr  Another java version here I think http://en.pudn.com/downloads111/sourcecode/internet/detail458790_en.html Thank you for the link! It was definitely what I needed. I apologize for not stating from the beginning that I needed the processes to be running in separate address spaces.
66,A,"Best practices of high-performance network applications While testing out a UDP multicast server that I've written on Windows 7 Ultimate x64 I came across a most curious thing. Playing music with foobar2000 in the background significantly improved the server's transmission rate yet also incurred minor packet loss. Turning the music off immediately dropped the transmission rate to below acceptable levels but also produced 0 packet loss. (I have a client application which talks to the server and reports back unacknowledged packets) I am aware of Vista's (and up) throttling behavior to make media and network applications play well together but I certainly did not expect that playing music would improve network performance nor that turning it off degraded network performance so significantly. What can I do about this from a code standpoint in my server application so that it performs consistently whether playing music or not on Vista and up? I would certainly like to avoid having to inform all my clients about how to tweak their registry to get acceptable transmission rates and would also like to avoid having them simply ""play music"" in order to get acceptable transmission rates as well. The application should ""just work"" in my opinion. I'm thinking the solution involves something along the lines of process priorities MMCSS or possibly some other obscure Windows API call to get it to do The Right Thing(TM) here. Also sorry but creating a reproducible test case is a non-trivial amount of work. The throttling behavior occurs only when the driver for the physical NIC is actively doing work and cannot be reproduced using the loopback interface. One would need a client implementation a server implementation and physical network hardware to test with. @reinier: Yes foobar2000 is slowly loading data off the HD and streaming audio to my external audio interface via USB 2.0. when you say ' is playing music' do you mean it is playing music of your HD and using the soundcard? Or is it streaming via the same network card? What you observe is the side-effect of your media player setting clock resolution of your machine to 1 ms. This happens only during the play The side-effect is - your app has smaller timeslices and this imporves your app because you probably had lot of CPU stolen from your app and with longer timeslices - for longer time. To test it you can simply set the timer resolution within your app to 1ms and compare performance without media playing. Should be the same as if with no clocres setting but with media playing.  It has been many years since I wrote network protocol related code but I'll offer a guess. I suspect this is an issue of throughput and latency. Playing music is introducing I/O contention and adding latency in transmitting the packets. However the added latency is likely causing the packets to queue and thus batched increasing throughput. To address this in your code you might try sending the packets in batches yourself. I am assuming that you are sending each packet to the system for transmission as the data becomes ready. Group multiple packets and send them to the system at the same time. Even just a group of two or three packets could make a dramatic difference especially if you are introducing your own small delay between each system call. I couldn't find any directly relevant links from a quick search on Google. However you can see the concept in this discussion of network tuning for Linux or in this FAQ which describes techniques such as batching to improve throughput. Your comment seems to indicate the opposite of your question. Are you really trying to send 1024 packets every 50-100 µs? That's insane throughput. And is the problem really the throughput or the data loss? Oh no no. The 50-100 microsecond delay is the delay between sending individual packets within the 1024 packet sector. Also I don't believe I mentioned my 50-100 microsecond delay in this question. Are you cross-referencing my SO questions? :) I am already batching packets together in groups of 1024. The packets are about 1400 bytes each. I get about 50-60 negatively acknowledged packets by the client out of the 1024. When I turn the media player off on the server machine the NAK count drops immediately to 0 and better throughput is achieved. This seems to indicate that the server is responsible for the packet loss. I'm wondering what I can do from the server side if anything to automatically accommodate for this I/O contention.  Foobar got many plugins written by different people. These may be the cause of your issue. I propose you to come closer to the real reason. Try to switch off plugins one by one performing your test each time a plugin is disabled. Hope the idea will help. I highly doubt the problem is foobar2000 specific. I will try other media players to diagnose the issue further. It *might* have something to do with the USB 2.0 audio interface I use. I will run some tests with the onboard sound card as well.  This sounds like TSP/IP managing throughput based on it's primitive algorithm. The white paper here should give more background. http://www.asperasoft.com/?gclid=CICSzMqD8Z0CFShGagod%5FltSMQ Their product is a UDP protocol that works very well. @Mike: Do you think the UDP packet loss on the server side is somehow related to the TCP communication going on? TCP is only used to notify all clients that the next batch of data is about to be sent over UDP. There is also a 'sector complete' message that is sent after a short delay from the server. While a UDP batch transfer is in place there are no TCP messages sent back and forth by the application except the normal TCP traffic flow that the OS does in order to keep the connection established. I should mention that I use both TCP and UDP protocols within the same application. TCP is only for control information and coordination of all clients; it should not be the bottleneck here. I use UDP purely for data transfer and keep the packets under 1500 bytes. I was using all UDP but ran into serious synchronization issues and was about to re-invent the TCP wheel so I said why not just use TCP. TCP/IP is the first place I would look for your problem."
67,A,"Network client simulator design I am trying to design a software in c++ that will send request bytes (following a standard **application level** protocol whose fields to be populated will be taken from a text file) using **UDP protocol**. Now this client must be able to send these requests at very high rate..upto **2000 transactions per second** and should also receive the response if it gets within a specified timeout else don't receive it I will be using boost library for all the socket things but I am not sure about the design of it for such high speed application :( I think I have to use a highly multi-threaded application (again Boost will be used). Am I right ? Do I have to create a seperate thread for each request ? But I think only one thread must be waiting to recieve the response else if many threads are waiting for a response how can we distinguish for which threads request we have got the response for !! Hope that question is clear. I just need some help regarding the design points and suspected problems that I may face. I'm not expert in this field so I won't post an answer but: no you don't necessarily need multithreading (though it may be useful) and no you certainly don't want to create 2000 threads per second. Consider a preforking approach with the number of threads at roughly 2*#CPU cores. I'm halfway through a network client of my own at the moment so perhaps I can impart some advice and some resources to look at. There are many more experienced in this area and hopefully they'll chime in :) Firstly you're about boost. Once you get used to how it all hangs together boost::asio is a great toolkit for writing network-code. Essentially you create an io_service and call run to execute until all work is complete or runOne to perform a single IO action. On their own this isn't that helpful. The power comes from when you either run runOne in it's own loop: boost::asio::io_service myIOService; while(true) { myIOService.runOne(); }  or run the run function on one (or more) threads: boost::thread t(boost::bind(&boost::asio::io_service::run &myIOService)); However it is worth noting that run returns as soon as there is no work to do (so you can say goodbye to that thread). As I found out here on Stackoverflow the trick is to ensure it always has something to do. The solution is in boost::asio::io_service::work: boost::asio::io_service::work myWork(myIOService); // just some abstract ""work"" The above line ensures your thread won't stop when nothing is going on. I view is as a means to keep-alive :) At some point you're going to want to create a socket and connect it somewhere. I created a generic Socket class (and derived a text-socket from that to create buffered input). I also wanted an event-based system that worked very much like C#. I've outlined this stuff for you below: First step we need a generic way of passing arguments around hence EventArgs: eventArgs.h  class EventArgs : boost::noncopyable { private: public: EventArgs(); virtual ~EventArgs() = 0; }; // eo class EventArgs: Now we need an event class which people can subscribe/unsubscribe to: event.h // STL #include <functional> #include <stack> // Boost #include <boost/bind.hpp> #include <boost/thread/mutex.hpp> // class Event class Event : boost::noncopyable { public: typedef std::function<void(const EventArgs&)> DelegateType; typedef boost::shared_ptr<DelegateType> DelegateDecl; private: boost::mutex m_Mutex; typedef std::set<DelegateDecl> DelegateSet; typedef std::stack<DelegateDecl> DelegateStack; typedef DelegateSet::const_iterator DelegateSet_cit; DelegateSet m_Delegates; DelegateStack m_ToRemove; public: Event() { }; // eo ctor Event(Event&& _rhs) : m_Delegates(std::move(_rhs.m_Delegates)) { }; // eo mtor ~Event() { }; // eo dtor // static methods static DelegateDecl bindDelegate(DelegateType _f) { DelegateDecl ret(new DelegateType(_f)); return ret; }; // eo bindDelegate // methods void raise(const EventArgs& _args) { boost::mutex::scoped_lock lock(m_Mutex); // get rid of any we have to remove while(m_ToRemove.size()) { m_Delegates.erase(m_Delegates.find(m_ToRemove.top())); m_ToRemove.pop(); }; if(m_Delegates.size()) std::for_each(m_Delegates.begin() m_Delegates.end() [&_args](const DelegateDecl& _decl) { (*_decl)(_args); }); }; // eo raise DelegateDecl addListener(DelegateDecl _decl) { boost::mutex::scoped_lock lock(m_Mutex); m_Delegates.insert(_decl); return _decl; }; // eo addListener DelegateDecl addListener(DelegateType _f) { DelegateDecl ret(bindDelegate(_f)); return addListener(ret); }; // eo addListener void removeListener(const DelegateDecl _decl) { boost::mutex::scoped_lock lock(m_Mutex); DelegateSet_cit cit(m_Delegates.find(_decl)); if(cit != m_Delegates.end()) m_ToRemove.push(_decl); }; // eo removeListener // operators // Only use operator += if you don't which to manually detach using removeListener Event& operator += (DelegateType _f) { addListener(_f); return *this; }; // eo op += }; // eo class Event Then it was time to create a socket class. Below is the header: socket.h (Some notes: ByteVector is typedef std::vector<unsigned char>) #pragma once #include ""event.h"" // boost #include <boost/asio/ip/tcp.hpp> #include <boost/asio/buffer.hpp> // class Socket class MORSE_API Socket : boost::noncopyable { protected: typedef boost::shared_ptr<boost::asio::ip::tcp::socket> SocketPtr; private: ByteVector m_Buffer; // will be used to read in SocketPtr m_SocketPtr; boost::asio::ip::tcp::endpoint m_RemoteEndPoint; bool m_bConnected; // reader void _handleConnect(const boost::system::error_code& _errorCode boost::asio::ip::tcp::resolver_iterator _rit); void _handleRead(const boost::system::error_code& _errorCode std::size_t read); protected: SocketPtr socket() { return m_SocketPtr; }; public: Socket(ByteVector_sz _bufSize = 512); virtual ~Socket(); // properties bool isConnected() const { return m_bConnected; }; const boost::asio::ip::tcp::endpoint& remoteEndPoint() const {return m_RemoteEndPoint; }; // methods void connect(boost::asio::ip::tcp::resolver_iterator _rit); void connect(const String& _host const Port _port); void close(); // Events Event onRead; Event onResolve; Event onConnect; Event onClose; }; // eo class Socket And now the implementation. You'll notice it calls another class to perform DNS resolution. I will show that afterwards. Also there are some EventArg-derivatives I have ommitted. They are simply passed as EventArg parameters when socket events occur. socket.cpp #include ""socket.h"" // boost #include <boost/asio/placeholders.hpp> namespace morse { namespace net { // ctor Socket::Socket(ByteVector_sz _bufSize /* = 512 */) : m_bConnected(false) { m_Buffer.resize(_bufSize); }; // eo ctor // dtor Socket::~Socket() { }; // eo dtor // _handleRead void Socket::_handleRead(const boost::system::error_code& _errorCode std::size_t _read) { if(!_errorCode) { if(_read) { onRead.raise(SocketReadEventArgs(*this m_Buffer _read)); // read again m_SocketPtr->async_read_some(boost::asio::buffer(m_Buffer) boost::bind(&Socket::_handleRead this _1 _2)); }; } else close(); }; // eo _handleRead // _handleConnect void Socket::_handleConnect(const boost::system::error_code& _errorCode boost::asio::ip::tcp::resolver_iterator _rit) { m_bConnected = !_errorCode; bool _raise(false); if(!_errorCode) { m_RemoteEndPoint = *_rit; _raise = true; m_SocketPtr->async_read_some(boost::asio::buffer(m_Buffer) boost::bind(&Socket::_handleRead this _1 _2)); } else if(++_rit != boost::asio::ip::tcp::resolver::iterator()) { m_SocketPtr->close(); m_SocketPtr->async_connect(*_rit boost::bind(&Socket::_handleConnect this boost::asio::placeholders::error _rit)); } else _raise = true; // raise complete failure if(_raise) onConnect.raise(SocketConnectEventArgs(*this _errorCode)); }; // eo _handleConnect // connect void Socket::connect(boost::asio::ip::tcp::resolver_iterator _rit) { boost::asio::ip::tcp::endpoint ep(*_rit); m_SocketPtr.reset(new boost::asio::ip::tcp::socket(Root::instance().ioService())); m_SocketPtr->async_connect(ep boost::bind(&Socket::_handleConnect this boost::asio::placeholders::error _rit)); }; void Socket::connect(const String& _host Port _port) { // Anon function for resolution of the host-name and asynchronous calling of the above auto anonResolve = [this](const boost::system::error_code& _errorCode boost::asio::ip::tcp::resolver_iterator _epIt) { // raise event onResolve.raise(SocketResolveEventArgs(*this !_errorCode ? (*_epIt).host_name() : String("""") _errorCode)); // perform connect calling back to anonymous function if(!_errorCode) this->connect(_epIt); }; // Resolve the host calling back to anonymous function Root::instance().resolveHost(_host _port anonResolve); }; // eo connect void Socket::close() { if(m_bConnected) { onClose.raise(SocketCloseEventArgs(*this)); m_SocketPtr->close(); m_bConnected = false; }; } // eo close As I said about DNS resolution the line Root::instance().resolveHost(_host _port anonResolve); calls this to perform asynchronous DNS:  // resolve a host asynchronously template<typename ResolveHandler> void resolveHost(const String& _host Port _port ResolveHandler _handler) { boost::asio::ip::tcp::endpoint ret; boost::asio::ip::tcp::resolver::query query(_host boost::lexical_cast<std::string>(_port)); m_Resolver.async_resolve(query _handler); }; // eo resolveHost Finally I need a text-based socket that raised an event every time I line was received (which is then processed). I'll omit the header file this time and just show the implementation file. Needless to say it declares an Event called onLine which it fires every time a line is received in it's entirety: // boost #include <boost/asio/buffer.hpp> #include <boost/asio/write.hpp> #include <boost/asio/placeholders.hpp> namespace morse { namespace net { String TextSocket::m_DefaultEOL(""\r\n""); // ctor TextSocket::TextSocket() : m_EOL(m_DefaultEOL) { onRead += boost::bind(&TextSocket::readHandler this _1); }; // eo ctor // dtor TextSocket::~TextSocket() { }; // eo dtor // readHandler void TextSocket::readHandler(const EventArgs& _args) { auto& args(static_cast<const SocketReadEventArgs&>(_args)); m_LineBuffer.append(args.buffer().begin() args.buffer().begin() + args.bytesRead()); String::size_type pos; while((pos = m_LineBuffer.find(eol())) != String::npos) { onLine.raise(SocketLineEventArgs(*this m_LineBuffer.substr(0 pos))); m_LineBuffer = m_LineBuffer.substr(pos + eol().length()); }; }; // eo readHandler // writeHandler void TextSocket::writeHandler(const boost::system::error_code& _errorCode std::size_t _written) { if(!_errorCode) { m_Queue.pop_front(); if(!m_Queue.empty()) // more to do? boost::asio::async_write(*socket().get() boost::asio::buffer(m_Queue.front() m_Queue.front().length()) boost::bind(&TextSocket::writeHandler this _1 _2)); } else close(); }; // eo writeHandler void TextSocket::sendLine(String _line) { Root::instance().ioService().post(boost::bind(&TextSocket::_sendLine this _line)); }; // eo sendLine // _sendLine void TextSocket::_sendLine(String _line) { // copy'n'queue _line.append(m_EOL); m_Queue.push_back(_line); if(m_Queue.size() == 1) // previously an empty queue must start write! boost::asio::async_write(*socket().get() boost::asio::buffer(m_Queue.front() m_Queue.front().length()) boost::bind(&TextSocket::writeHandler this _1 _2)); }; // eo sendLine Some things to note about the class above... it uses boost::asio::post to send lines. This allows it to all occur on the threads that ASIO manages in a thread-safe way and allows us to queue up lines to be sent as and when. This makes it very scalable. I am sure there are many more questions and maybe my code isn't helpful. I spent several days piecing it all together and making sense of it and I doubt it's actually any good. hopefully some better minds will glance over it and go ""HOLY CRAP THIS @ArunMu actually i only use the mutex to allow the event handlers to be modified from anywhere (for example lets say you receive some data and want to add or remove your handler when you get it... you receive data on the boost thread.. what if the UI thread is adding a handler?). With actual data sending I use the `boost::asio::io_service::post` method which ensures that the specified function is called on a thread it made so no mutexes needed. @Moo-Juice:: ohk..getting it now :D. Thanks again for your patience in answering my queries. @ArunMu no problem. Also note that my code uses a single thread to do all the network I/O. In your case with multiple threads you will want to look in to using strands on your post/receiving handlers to ensure that a single function call is not accessed by more than one thread at one time. Once you get your head around all this stuff it all makes sense but it is quite daunting at the start I admit :) @Moo-Juice : Will it be possible for you to give a high level architecture of your Network client ? Just need to know how you have placed the Boost asio api's to construct the application. @ArunMu. I have a root object called `Root` (surprise!) which acts as both a container for my client classes (which in turn contain the socket mentioned above). It also contains the `boost::asio::io_service` instance. In the constructor it initializes some work (passing that io-service in to the ctor) and starts a new thread off using the `run` method of `io_service`. When root has finished constructing io_service is up and running and due to the addition of the `work` it's happy to sit there until I give it something more to do. (more coming). @ArunMU in the destructor of root I close the io_service down and join the threads `boost::thread::join`. When one of my client classes is instructed to connect somewhere it is constructed with the host details. It constructs the socket and resolves the host name using the resolver method with callback as mentioned above. Handlers are attached to the various socket events by the client to report name-resolution connection status and when data is ready to be read. The root object keeps track of all the clients within itself. (ran out of room!)... IS BAD!"". If you have any questions let me know. I know some code/definitions are missing but there's limited space :) +1 good answer but this is likely overcomplicated for someone just starting to use Boost.Asio. The OP would be better suited to study the examples included with the documentation. Thanks a lot for your answer(though I didnt understand much :P)..from your post looks like boost:asio is the thing I should go for. But what about sending high number of events per second and waiting for their respective responses. What method you employ or going to employ (threading ?). @ArunMu as small_duck said in his reply you can start multiple threads (basically the line in my code that starts a boost thread? Having more!). I would start with one thread and see how the performance is scaling up if needed. I'd start with one boost thread posting to a processing thread and see what the throughput was like perhaps adding another thread and other mechanisms to improve performance. @Moo-Juice : One last doubt  can i use two different instances of io_service for both sending data to server and receiving data from the server or do I have to use strands only ? Also i intend to use two different threads for both send and receive io_services and my main code will join these threads. @Arunmu I'm not sure why you'd throw two different io_service's at that and not even sure if it would work unless you were using one socket to send and one to receive (as a socket gets bound to an io_service). As for separate threads for sending/receiving it is my understanding that boost::asio uses the threads as it sees fit but I MAY BE WRONG on this. It might be worth checking out the documentation a little more deeply on that fact. Either way I am sure you'll get the throughput you need :) Yes you are correct even I doubted it would work. I wanted it because I wanted to make both send and receive operations completely exclusive to each other.Like just go on sending events(ofcourse making sure server doesnt gets killed :) ) and receive the events as it comes by @ArunMu you'll get that behavior anyway thanks to the asynchronous nature of the beast. Whether ""under the hood"" you are receiving at the same time as sending is not really worth worrying about :) Yup its working ...asio is really amazing..I can really feel the power :) @ArunMu awesome :) Happy Asynchronous coding! Thanks a lot for introducing me to it and answering my queries @ArunMU anytime. That's what this site is for! :) @Moo-Juice : Thanks a lot for your explanation :) .I am starting to get this I guess. Something very similar to Pyhton's Twisted module I think.. Now from what I understood ..when we need to send or read data from the socket it calls the repective handler...But as mentioned by small-duck...for asynchronous communication threading is usually not required..but from you code I see you use thread and mutex (to protect some data) ...can you also brief on that too ?  I'm not sure you need to go ""heavy"" multi-thread. Most high speed applications use the polling mechanisms of the operating system which generally scale better than threads. The architecture will depend a lot on how reactive your application needs to be in terms of what components are responsible for generating inputs and outputs and doing the actual processing. A way to approach your problem using boost::asio would be to have a communication thread that runs the boost::asio::io_service::run method. The io_service listens on the various UDP sockets and processes messages as they arrive possibly sending them down a queue so that the application can process them in the main thread. From the main thread you can post messages to the io_services for them to be sent by the main system. This should allow you to climb up to 2000 messages per seconds without much difficulties. An alternative would be to start several communication threads by calling the boost::asio::io_service::run method several times from several threads allowing for messages to be processed in parallel by their communication thread. One word of advice with Asio though: because of its asynchronous architecture it works better if you go within its logic and use it the way it is meant to. If you find out that you are using a lot of locks and managing many threads yourself then you are probably doing it wrong. Look closely at the thread safety guarantees of the various methods and study the provided examples. +1 for studying the asio examples they really are helpful. Thanks for your inputs . I will try to get the nitty gritty of boost asio . Even I will be happy if I dont need to use threads :).. Do we have any other good source for understanding boost asio other than the standard documentation ?"
68,A,"Simplest Way To Open and Use a Socket in C I'm just starting out doing some basic network programming with C and I found all these things about sockets and they all seem very convoluted. Maybe opening sockets with C is just convoluted itself but I would like to know the simplest and most effective way to open and write data to a socket in the C programming language. Thanks Reading and writing from basic sockets is not any harder than reading and writing normal files (just use recv instead of read and send instead if write). Things get a little trickey when you need to open a socket. The reason for that is because there are many different ways to communicate using sockets (TCP UDP etc). what you say is true on real operating systems but unfortunately Windows makes it somewhat harder than using a file simply by drawing a distinction between sockets and files.  You might want to try Tcp4u it's free any makes socket programming very easy. http://www.jounin.net/tcp4u.html  You don't mention what platform you are on but a copy of Unix Network Programming by Stevens would be a good addition to your bookshelf. Most operating systems implement Berkley Sockets using socket bind connect etc. I was trying to remember the name of that book for my own answer but it's currently in storage. This is the bible for socket programming. Yeah that is a great book. I keep it within arm's reach at work. All Stevens' books are great but UNP is the best. Both volumes.  Much good advice here so far. I generally write in C++ but you can find some use in a white paper I wrote ""How to Avoid the Top Ten Sockets Programming Errors"" - ignore the advice to use the ACE toolkit (since it requires C++) but take note of the socket errors in the paper - they're easy to make and hard to find especially for a beginner. http://www.riverace.com/sockets10.htm  You're right using sockets in C has a difficult syntax. Later languages like Java and Python make it a snap by comparison. The best tutorial I've found for doing socket programming in C is Beej's Guide to Network Programming. I recommend you start at the beginning to get a good overview but if you just need to get some code working now you can skip ahead to the section titled Client-Server Background. Good luck! This is silly. Sockets are *defined* as a C API and ""later languages"" have to make all those C calls at some level. There are ""later libraries"" in C that will also do it easily right up to making say an HTTP request instead of mucking around with sockets. You can easily have a client function that will take an IP address in host or dot notation as a `char *` string a port number as an `int` and return you a `FILE *` stream denoting the connected circuit or a null pointer with `errno` set to something useful.  The Definitive Guide to Linux Network Programming describe socket easy and explain many things like server threading protocol design etc... Also TCP/IP Sockets in C Second Edition is good.  Unless you write a network daemon most networking in C can be done at a higher level than using directly the sockets by using appropriate libraries. For instance if you just want to retrieve a file with HTTP use Neon or libcurl. It will be simpler it will be at a higher level and you will have gratis SSL IPv6 etc. Several down votes and not one comment to explain why. This tells a lot about the technical level of many SO users... More likely it's a battle between people who agree with you and people who think you are not answering the question asked. there you go now you are at 0. Being right is not easy If people downvote because they disagree they can say so in the comments. Replying with a referral (you asked how to do X but I suggest that you do Y instead) is perfectly acceptable when the requirments are not clear (if the OP had said ""my manager [or my teacher] required the use of sockets"" things would have been different)"
69,A,changing the protocol for client to server I have to test the value from client to server using different protocols (Tcp  UDP  Sctp). Please guide how it can be possible. Is there any way in windows to change the protocol or is there anyway to find it by using software like packet tracer . Thanks While the question is not entirely clear it sounds as though your interested in seeing the information sent between the client and the server when each of those protocols is used. Windows does not provide a built in utility to view packet data but it can be viewed using a packet analyser such as Wireshark. In order to see the values sent by each protocol you must run a client for each of the protocols and use it to connect to the server for that protocol. If you don't have a server to connect to you may need to run one on your local machine. You can narrow down the data captured to just the protocol you're interested in using a filter in Wireshark If you don't know the protocol being used you can filter by the port number used for that connection which can be established using the netstat command. You may need to use netstat -b to show you which programs are using which ports.  If you just have to generate packets using different protocols then the tool like netcat can also help. It supports TCP and UDP and has been ported to windows .
70,A,"What are often used network programming functions/code snippets? All of us who still do some kind of network programming (TCP/UDP DNS or Client/Server) in C repeatedly use some code snippets again and again. We do use some standard libraries but then also we do write some code very often which is not there in one library. Is there a collection of such code snippets that are used very often. If not then lets build it here. Here is UNIX Network Programming Volume 1 Third Edition Source Code Here  W. Richard Stephens wrote a collection of such snippets: UNIX Network Programming Volume 1 Second Edition: Networking APIs: Sockets and XTI  Good question! Here is a Name resolution Function  struct hostent { char *h_name; // main name char **h_aliases; // alternative names (aliases) int h_addrtype; // address type (usually AF_INET) int h_length; // length of address (in octets) char **h_addr_list; // alternate addresses (in Network Byte Order) }; #define h_addr h_addr_list[0] // First address of h_addr_list. struct hostent *info_stackoverflow; int i = 0; info_stackoverflow = gethostbyname( ""www.stackoverflow.com"" ); printf(""The IP address of %s is %s"" info_stackoverflow->h_name inet_ntoa( * ((struct in_addr *)info_stackoverflow->h_addr ))); /* aliases */ while( *(pc_ip->h_aliases + i) != NULL ) { printf(""\n\tAlias: %s"" *(pc_ip->h_aliases + i) ); i++; }"
71,A,setsotimeout in java I am trying to send data through DatagramSocket and would like to do this that if the data sent has exceeded the timeout of its acknowledgement then it should be resend. Can we use the option of DatagramSocket.SetSoTimeout for it ?? If yes how can I ?? For eg try { while(true) { socket.send(data); } }catch (SocketTimeoutException e) { // resend for which it occured } IS this possible to do ?? The documentation seems rather clear about setSoTimeout purpose: a call to receive() for this DatagramSocket will block for only this amount of time First it doesn't have anything to do with send and second it only timeout if it's blocking for a certain amount of time. If you want reliability use TCP. If you absolutely need/want to use UDP you'll have to devise your own reliability mechanism. Here's another SO question about this particular problem: What do you use when you need reliable UDP? Basically it really depend on what you're doing because if you need a generic solution you'll end up reinventing TCP!  UDP is unreliable it has no ack's. i.e. There's no acknowledgement timer that can be exceeded. Hey I am programming for acks and thus would like to know... Its totally for my learning purposes So the answer is no. @user506710 Since you are programming the ack you also need to program the timeout. Basically with UDP you have to many of the things TCP does for you yourself (which is why many people end up using TCP)
72,A,"Why do we say the IP protocol in TCP/IP suite is connectionless? Why is the IP called connectionless protocol? If so what is the connection-oriented protocol then? Thanks. Update - 1 - 20:21 2010/12/26 I think to better answer my question it would be better to explain what ""connection"" actually is both physically and logically. Update - 2 - 9:59 AM 2/1/2013 Based on all the answers below I come to the feeling that the 'connection' mentioned here should be considered as a set of actions/arrangements/disciplines. Thus it's more an abstract conecpt rather than a concrete object. I've edited my answer. TCP is the connection part of TCP/IP. IP's the addressing. Or as an analogy IP is the address written on the envelope TCP is the postal system which uses the address as part of the work of getting the envelope from point A to point B. thanks for your reply. So IP is the connection-less counterpart of TCP. But isn't UDP the counterpart of TCP? IP is indeed the addressing but technically there is nothing that prevents us using IP as transport protocol. If you don't need the added benefit of port-numbers you can use IP to transfer data directly. TCP and UDP would get embedded in an IP packet. IP by itself has no concept of port numbers. It's purely concerned with host-level transfers. I agree that IP has the responsibility of addressing. But I don't agree that TCP is the postal system. In a end-to-end transmission scenario TCP only exists at both ends not the intermediate nodes. The intermediate nodes such as routers only need to implement up to the IP level of the protocol stack. IP layer not only takes care of addressing but also delivery.  The basic idea is pretty simple: with IP (on its own -- no TCP UDP etc.) you're just sending a packet of data. You simply send some data onto the net with a destination address but that's it. By itself IP gives: no assurance that it'll be delivered no way to find out if it was nothing to let the destination know to expect a packet much of anything else All it does is specify a minimal packet format so you can get some data from one point to another (e.g. routers know the packet format so they can look at the destination and send the packet on its next hop). TCP is connection oriented. Establishing a connection means that at the beginning of a TCP conversation it does a ""three way handshake"" so (in particular) the destination knows that a connection with the source has been established. It keeps track of that address internally so it can/will/does expect more packets from it and be able to send replies to (for example) acknowledge each packet it receives. The source and destination also cooperate to serial number all the packets for the acknowledgment scheme so each end knows whether packets it sent were received at the other end. This doesn't involve much physically but logically it involves allocating some memory on both ends. That includes memory for metadata like the next packet serial number to use as well as payload data for possible re-transmission until the other side acknowledges receipt of that packet. @smwikipedia: generally no -- router configuration is mostly done with RIP (e.g. RFC 2453) not based on individual packets. The obvious exception is a NAT box which stores where you've sent data so it can accept replies back from there and ignore other incoming packets. This is more or less independent of TCP connections though so (for example) a reply to a UDP packet gets routed correctly. Thanks. As you said establishing a connection involves allocating some memory and some other things happening on both ends. So is that all about connection? Is there anything else involved in the **middle** of the 2 ends? For example all the intermediate nodes such as routers? Because I tend to think there should be something **physically** connect the both ends. All the computers that connect to Internet could be considered connected physically but there should be more than that when 2 hosts have explicitly established a connection.  TCP/IP means ""TCP over IP"". TCP -- IP TCP provides the ""connection-oriented"" logic ordering and control IP provides getting packets from A to B however it can: ""connectionless"" Notes: UDP is connection less but at the same level as TCP Other protocols such as ICMP (used by ping) can run over IP but have nothing to do with TCP Edit: ""connection-oriented"" mean established end to end connection. For example you pick up the telephone call someone = you have a connection. ""connection-less"" means ""send it see what happens"". For example sending a letter via snail mail.a So IP gets your packets from A to B maybe in any order not always eventually. TCP sorts them out acknowledges them requests a resends and provides the ""connection"" Thanks. I have never interpreted the slash between TCP and IP as ""over"". Please see my update to my post.  When two hosts want to communicate using connection oriented protocol one of them must first initiate a connection and the other must accept it. Logically a connection is made between a port in one host and other port in the other host. Software in one host must perform a connect socket operation and the other must perform an accept socket operation. Physically the initiator host sends a SYN packet which contains all four connection identifying numbers (source IP source port destination IP destination port). The other receives it and sends SYN-ACK the initiator sends an ACK then the connection are established. After the connection established then the data could be transferred in both directions. In the other hand connectionless protocol means that we don't need to establish connection to send data. It means the first packet being sent from one host to another could contain data payloads. Of course for upper layer protocols such as UDP the recipient must be ready first (e.g.) it must perform a listen udp socket operation. The connectionless IP became foundation for TCP in the layer above In TCP at minimal 2x round trip times are required to send just one packet of data. That is : a->b for SYN b->a for SYN-ACK a->b for ACK with DATA b->a for ACK. For flow rate control Nagle's algorithm is applied here. In UDP only 0.5 round trip times are required : a->b with DATA. But be prepared that some packets could be silently lost and there is no flow control being done. Packets could be sent in the rate that are larger than the capability of the receiving system.  I just came across this question today. It was bouncing around in my head all day and didn't make any sense. IP doesn't handle transport. Why would anyone even think of IP as connectionless or connection oriented? It is technically connectionless because it offers no reliability no guaranteed delivery. But so is my toaster. My toaster offers no guaranteed delivery so why not call aa toaster connectionless too? In the end I found out it's just some stupid title that someone somewhere attached to IP and it stuck and now everyone calls IP connectionless and has no good reason for it. Calling IP connectionless implies there is another layer 3 protocol that is connection oriented but as far as I know there isn't and it is just plain stupid to specify that IP is connectionless. MAC is connectionless. LLC is connectionless. But that is useless technically correct info.  Connectionless means that no effort is made to set up a dedicated end-to-end connection While Connection-Oriented means that when devices communicate they perform handshaking to set up an end-to-end connection. IP is an example of the Connectionless protocols  in this kind of protocols you usually send informations in one direction from source to destination without checking to see if the destination is still there or if it is prepared to receive the information . Connectionless protocols (Like IP and UDP) are used for example with the Video Conferencing when you don't care if some packets are lost  while you have to use a Connection-Oriented protocol (Like TCP) when you send a File because you want to insure that all the packets are sent successfully (actually we use FTP to transfer Files). Edit : In telecommunication and computing in general a connection is the successful completion of necessary arrangements so that two or more parties (for example people or programs) can communicate at a long distance. In this usage the term has a strong physical (hardware) connotation although logical (software) elements are usually involved as well. The physical connection is layer 1 of the OSI model and is the medium through which the data is transfered. i.e. cables The logical connection is layer 3 of the OSI model and is the network portion. Using the Internetwork Protocol (IP) each host is assigned a 32 bit IP address. e.g. 192.168.1.1 Thanks for your reply. Please see my update 1 to question. Your modified answers is also good. Unforunately I can only choose one as the answer."
73,A,"What's the best way to transfer data with a remote application across the internet? I'm making a relatively simple program which will also be running on a few friends computers and they need to share some information. They will need to exchange ips in case they are changed via dhcp and maybe a few other things in the future but right now that's it (this will likely be used to update the program if I ever change it too I figure). If there's a better way without having a middleman server to keep them from losing the ip that would be helpful too but worst case I can just call them up and ask since this would so rarely happen if ever. Our isps renew every 30 days I believe and they often keep the same one anyways so I doubt that'll ever be an issue but if so it's so rare it'd be a minor inconvenience. I haven't done much network programming/scripting before so I'm not sure where to approach this from. I've used urllib/urllib2 and mechanize but I'm guessing those while they could work are not an elegant solution. I was thinking the pcs would just communicate via a specified port and just listen through there but I don't know what module would handle such a thing. Thanks friends. ""I'm guessing those while they could work are not an elegant solution"" Why not? Please explain why one of you can't run an FTP server and the others run FTP clients? If change of IP address is your main concern a service like dyndns.com would certainly be helpful. (You can get automatic clients as well which will update your DNS entry when your IP address changes.) After this for data transfer you're probably better off using existing protocols (e.g. HTTP FTP ...). There's plenty of existing HTTP server libraries out there for example. Perhaps something based on this would be of interest: http://docs.python.org/library/basehttpserver.html I thought about that too as I have (very little) experience using that module but I figured there might be an even leaner way to do it. That could certainly work though if nothing better is suggested. I'll second this HTTP is by far the easiest way to transfer files over the network. Python's builtin HTTP client libraries are sort of awful though - consider using httplib2 if they start getting on your nerves ;) Ok well after 2 suggestions towards this I think this is the approach I will take and thanks for the recommendation towards httplib2 static hadn't heard of that before."
74,A,WinPac-8000 OPC-server network access I am to build an automation solution based on WinPac-8000 controller (http://www.icpdas.com/products/PAC/winpac/introduction.htm). I split this project into 2 programs: 1st is running on the controller (it does some business logic) and second operates on user machine (it displays current results). The problem is: I use build-in OPC server (named 'quicker'). It has nice intergration with C# and easy to use in the 1st program to access connected devices. But I dont know how to access OPC-server from the second program through network. I have absolutely no idea where to start. Links or any kind of advice will be very appreciated. I would like to merely to add a 'comment' to your question but do not have the necessary rep so I shall comment as an answer... Please could you clarify what you would like the second program to do? OPC has 'client' and 'server' components. The OPC 'server' would run on the WinPAC controller and you would run OPC 'client' software on the PC. Here are some OPC .NET links: Open Automation OPC and .NET. However if you merely wish to monitor and control digital/analog points the MODBUS/TCP protocol is much simpler and cleaner to implement (CodeProject). OPC is quite heavy and I have not found it particularly robust in unstable networks. MODBUS/TCP is a simple socket connection and can easily be implemented in any language supporting TCP sockets. ICPDAS and MODBUS/TCP has worked extremely well for me. Basically the second program allows user to read and write some OPC tags (seems like some kind of a 'OPC-client'). The only conceptual difference from the 1st program is that it is not located on WinPac and I dont know which 'technology' or 'library' to use to connect this program and OPC-server Depends on exactly what you want to do with the OPC tags? For basic tasks SCADA (supervisory control and data acquisition) or HMI (Human Machine Interface) software will do the trick. For more sophisticated control/monitoring you can use the .NET OPC SDK (http://www.opcfoundation.org/Downloads.aspx?CI=281).
75,A,"In c# client/server program Data that sent from server is changed when received at client Server side code: byte[] size = new byte[4]; size = BitConverter.GetBytes(fileData.Length); stream.Write(size 0 4); Client side code: byte[] size = new byte[4]; ReadWholeArray(s size); int fileSize = BitConverter.ToInt32(size 0); Definition of ReadWholeArray method: public static void ReadWholeArray(Stream stream byte[] data) { int offset = 0; int remaining = data.Length; while (remaining > 0) { int read = stream.Read(data offset remaining); if (read <= 0) throw new EndOfStreamException (String.Format(""End of stream reached with {0} bytes left to read"" remaining)); remaining -= read; offset += read; } } The Program sends fileData.Length (the value for this instance is 2422) from server. On receiving this data at client side the value of received data is -772097985 Why the sent data is not received without alteration in value? What is the problem? This works fine: private void button2_Click(object sender RoutedEventArgs e) { MemoryStream ms = new MemoryStream(); byte [] original = BitConverter.GetBytes((int)2224); // 176 8 0 0 ms.Write(original 0 original.Length); ms.Seek(0 SeekOrigin.Begin); byte [] data = new byte[4]; int count = ms.Read(data 0 4); // count is 4 data is 176 8 0 0 int fileSize = BitConverter.ToInt32(data 0); // is 2224 return; } Can you use WireShark or something to intercept the bytes? What kind of connection are you using? Could more data be being sent (i.e. telnet control characters at the start of the stream)? Can you debug each end and verify these values or write the byte array contents to a log file? By the way calling this: ""byte[] size = new byte[4];"" is wasteful because BitConverter.GetBytes() returns a new array.  Okay simple diagnostics to start with: log the individual contents of the byte array at both ends so you can see what's going on there. That way you can see if it's the binary data which is getting corrupted in your communication protocol or whether BitConverter is causing your problem. For example you could have a big-endian BitConverter at one end and a little-endian BitConverter at the other. That seems unlikely but it's possible - particularly if one of your server or client is running Mono rather than .NET itself. If that does turn out to be the problem you might want to use my EndianBitConverter class from MiscUtil which lets you specify the endianness. If the problem is in the communications layer you quite possibly want to install Wireshark to see what's happening at the network level. Are you sure you've read all the data you're meant to have read so far for example? (If you've only read 15 bytes before this and the size is written at offset 16 then obviously you'll get the ""extra"" byte first.)"
76,A,"what type of network programming is necessary in iPhone online multiplayer game? I am asking this question as a small part of my question series regarding game programming . Refer this question as the main one . Now suppose I want to develop a game on iphone - a simple online multiplayer board game. Suppose its a casino table. In the main question ChrisF has tell me of sort of Client - Server architecture for iphone online multiplayer game. I want to know what sort network programming I have to do for this type of application . What will be the responsibilities and activities carried out by client and server . You can provide me link tutorials or to the point answers  anything will be great help for me and will be really appreciable . thanks Hi. First of all you do not always need a server: if your project is a game which will be played by users in the same room you can use Bluetooth and therefore you do not need a network connected to the internet. Therefore you should maybe precise your question by answering: will the players be in the same room ? Because the design will be quite different if you use Bluetooth or if you use a server on the internet. Hi FIfifox I have left just one word in above question that is ""online"" I am correcting it by editing the question You'll want to write a socket application running on a server. When you have access to a wifi access point or edge/3g you can send data to it from your iphone application. This server can then handle the incoming data and send an appropriate reply to the people connected. For server socket programming take a look at this guide - http://beej.us/guide/bgnet/. For iphone specific socket programming take a look at the samples supplied with the Iphone SDK. This link also has some basic information. Hey chaoz thanks for your answer but I want to know the client side part also i.e. iPhone . What sort of things are applicable for iPhone network programming ? is it in c ? @ Chaoz +1 for beej's guide +1 because nothing beats Beej's guide  A simple online multiplayer board game Given that the iPhone isn't always connected to the internet you might need a server to store state. Alternatively you could always stipulate that if one person loses their connection the game finishes. Client to client would be the obvious choice for the latter. Both clients have a port they listen to and send the other commands based on the board state. Like almost all online games the obvious choice would be to use UDP as it's fast and compact. For the server architecture you will of course need some kind of server listening for commands and a game number. It would store your state in a datastore on the server a mySQL database for example. UDP or even SOAP or JSON over HTTP would be the two obvious choices for this. This second approach using JSON/SOAP route would be a lot easier for you to get started with assuming the iPhone has a decent JSON or SOAP library which is not likely. I have no idea about UDP in Objective C however in C it requires a certain level of knowledge which won't get you something to play with quickly. Thanks @Chris S I got lot of things from your answer You should consider the fact that you might not get the performance you need when using HTTP; If you need a fast responce but high reliability go with TCP sockets. SQLite is a lightweight alternative to MySQL.  As you already said you will need a server but you can have two kinds of design: The server can serve only as a gateway between the players to connect one to each other: it's two uses are first to list the running games and second to list the IP addresses of the players so that each client will read the IP addresses and connect to them. This will require less bandwidth and processing power for the web server and more for the client which will host the game. In this configuration one of the clients will act as a server and will send data to the others. If your game has only one of the players playing at a time and not a huge lot a players this is what you should use as what you pay is the server's power. The server can also store all games' states: it might require far more processing power and/or bandwidth depending on your game. Anyway most of the time you will want only one machine (which can change during the game as in the first case) to do the processing and the others will only receive the computed data. To program a networked game you will need knowledge of sockets (deep knowledge in the first case because you will have to deal with NAT issues routers blocking the way between clients). The guide to that is Beej's Guide to Network Programming the number one ressource on this topic although it doesn't focus on games. If not too much processing is needed on the WWW server you could deal with it with server scripting languages like PHP along with MySQL but you're most likely to use your own server programmed in C++ (or in C). If you use C++ you might want to use an already existing library such as RakNet. The client will obviously be programmed in Objective-C as it's on the iPhone. I believe there is a good networking framework looking at the number of online games so you might as well not want to use an external server networking library. It might be a bit too much for what you want to do but you could also use the well known Torque Engine."
77,A,"Retreiving the protocol of a socket in winsock Hey I am working in networking reliability simulation I need to simulate packet dropping based on a quality of service percentage. Currently I have a DLL that hooks into send sendto recv and recvfrom. My hooks then 'drop' packets based on the quality of service. I just need to apply the hook to UDP packets and not disturb TCP (TCP is used for remote debugging). Is there a way that I can query WinSock for the protocol that a socket is bound to? int WSAAPI HookedSend(SOCKET s const char FAR * buf int len int flags) { //if(s is UDP) //Drop according to QOS else //Send TCP packets undisturbed return send(s buf len flags); } Thanks I think you could get the socket type by using getsockopt: int optVal; int optLen = sizeof(int); getsockopt(socket SOL_SOCKET SO_TYPE (char*)&optVal &optLen); if(optVal = SOCK_STREAM) printf(""This is a TCP socket.\n""); else if(optVal = SOCK_DGRAM) printf(""This is a UTP socket.\n""); else printf(""Error""); Found in getsockopt documentation that the option SO_BSP_STATE returns addres port and protocol. Thank you"
78,A,Can read() function on a connected socket return zero bytes? I know that read() is a blocking call unless I make the socket non-blocking. So I expect read() call which requests 4K of data should return a positive value ( no of bytes read) or -1 on error ( possible connection reset by client etc). My question is: Can read() return '0' on any occasion? I am handling the read() this way:  if ((readval = read(acceptfd buf sizeof(buf) - 1)) < 0) { } else { buf[readval] = 0; //Do some thing with data } This code bombs if read() return zero and I know how to fix it. But is it possible for read() to return zero? When a TCP connection is closed on one side read() on the other side returns 0 byte. Shouldn't the read return -1 with errno set to ECONNRESET? It is actually an error condition if other side has closed the connection. Should we consider return of zero also as error condition? +1 read() returns zero when the connection is closed. @kumar: returning zero allows you to determine that an orderly shut down occurred as opposed to a real error. @Richard Thanks. So I should consider zero as error condition and close the connected socket. Right? Yes you should close the connected socket. However it is not an error.
79,A,"ACE Reactor quits on interrupted system call I have an ACE reactor that accepts socket connections and listens for the incoming data on those connections. The reactor runs in a dedicated thread. This is the thread's entry function: int TcpServer::svc() { LogDebug(""The TCP server on %i is running"" mLocalAddr.get_port_number()); // The current thread will own the reactor. By default a reactor is owned by // the creating thread. A reactor cannot run from not owning thread. if (mReactor.owner(ACE_Thread::self()) != 0) { LogThrow(""Could not change the owner of the reactor""); } if (mReactor.run_reactor_event_loop() != 0) { LogWarning(""Reactor loop has quit with an error.""); } return 0; } Once in a while run_reactor_event_loop exits with -1 and errno reports that the reason is ""interrupted system call"". How can I handle the situation? From what I know I have two options: call run_reactor_event_loop again or configure the interrupted call to be called again using sigaction and SA_RESTART. Is it safe to call run_reactor_event_loop again? What does ACE_Reactor::restart method do? It looks like it is supposed to restart the loop? Will it help? How safe it to turn on SA_RESTART? Does it mean for example that ^C won't stop my application? Are there any other ways to handle the situation? I've updated the question. Interrupted system call causes the problem Did you take a look at the source code? @Peter G. I did. I see that it returns -1 if ACE_Reactor::handle_events() returns -1. Code becomes more complicated there but I'll try to dig deeper. Did you debug it ? @DumbCoder this a part of a huge system that has been running without problems. Recently once in a while I see the described behavior and I don't have any ideas what could cause it. It seems that no one stops the reactor so what can cause an error? Check how Reactor is constructed. ACE_Reactor::open() cal takes ""restart"" parameter (default = false) that tells it to restart handle_events method automatically after interruption."
80,A,"Handle Socket.ReceiveFrom with timeout without spamming console I am writing a ServerLocator that basically broadcast a port to find a server which will respond with an IPEndPoint and I need the search to be able to timeout if nothing is found on the current IPHost and then move on with the next one. Right now I am doing something like this (I have removed some parts of this code so it only includes what is needed to display my problem. There is also some client bindings going on here) string serverIp = string.Empty; while(string.isNullOrEmpty(serverIp)) { foreach (IPAddress adress in ipHosts.AddressList) { using(Socket client = new Socket(AddressFamily.InterNetwork SocketType.Dgram ProtocolType.Udp) { try { client.ReceiveFrom(buffer ref tempRemoteEP); //Get server IP serverIp = tempRemoteEP.ToString().Split("":"".ToCharArray() 2)[0]; break; } catch(SocketException e) { // We expect connection attempt to time out if we cant find any server on this port and nic. Just continue with the next if (e.SocketErrorCode == SocketError.TimedOut) { continue; } } } } } This works as expected except that the console gets spammed with: A first chance exception of type 'System.Net.Sockets.SocketException' occurred in System.dll Is there a good way to handle exceptions like this without spamming the console? Or could I handle this in some other way to be able to avoid the need of exception from timeout? Thanks. That doesn't happen when you build your project in RELESE configuration. That is true but if it is to much spam it gets hard to work with There's really no need to worry about this if the program keeps running there's a lot of these exceptions being sent in a program. See this article for more on ""First time exceptions"". Also check this link to see how you can configure Visual Studio on how to handle the exceptions. If you configure these you can break (instead of continue) on the exceptions and see what's really going on. However do note that hiding the exceptions does not seem to work in debug see here or here But as @Cipi pointed out it should not be visible in Release. Thanks. It is not that I am worried I just like to keep it clean. Somehow it feels like using exceptions as a part of the logic is wrong. So you chose C# why? :) hehe. True that :)"
81,A,"internal working of the recv socket api I am working on TCP client server application using c++.third party lib are now allowed in this project. Here exchange between client server takes using well define protocol format.once the client receives the packet it will send it for parsing.I have protocol manager which will take care of the parsing activity. I have following doubt When the data arrives at client from the network the OS buffers it until application calls recv() function. So two message msg1 and msg2 arrives at the buffer a call to recv will return msg1+msg2. Now this may result in failure of the parsing activity. My queries 1. whether above mentioned assumption is correct or not ? 2. If above mentioned assuption is correct then how can resolve this issue. Thanks cpx I think it should work You are correct: `recv` might return both messages in their entirety. In other situations `recv` might return only part of a message or it might return all of msg1 and part of msg2. In short `recv` has no knowledge of and no respect for the sequence of `send` calls that generated the data flow. What TCP will guarantee is this: all of the bytes it does return will be in order with nothing skipped. (N.b. other rules apply for UDP or other protocols.) Take a look at [this](http://stackoverflow.com/questions/4896411/tcp-client-message-handling) question It talks about how you should handle messages from server over TCP. TCP emulates a stream so in TCP there is no notion of messages. If you want messages your application has to have some protocol to seperate them. UDP does have a messages so there it is possible to retrieve separate messages. Sjoerd I agree with you.Can you Give me some example or case anything... so that i can start on this. will peeking the header and getting message size and then read rest of the byte will be appropriate here?  You can use a LV protocol (Length-Value) for your message. Encode the message length in the first (1-4) bytes then put the message. Something like this for example : 14""Hello world\0"" In your server when a client is sending something you'll have to recv() the first (1-4) bytes then recv() the length of the message."
82,A,"C# Asynchronous Network IO and OutOfMemoryException I'm working on a client/server application in C# and I need to get Asynchronous sockets working so I can handle multiple connections at once. Technically it works the way it is now but I get an OutOfMemoryException after about 3 minutes of running. MSDN says to use a WaitHandler to do WaitOne() after the socket.BeginAccept() but it doesn't actually let me do that. When I try to do that in the code it says WaitHandler is an abstract class or interface and I can't instantiate it. I thought maybe Id try a static reference but it doesnt have teh WaitOne() method just WaitAll() and WaitAny(). The main problem is that in the docs it doesn't give a full code snippet so you can't actually see what their ""wait handler"" is coming from. its just a variable called allDone which also has a Reset() method in the snippet which a waithandler doesn't have. After digging around in their docs I found some related thing about an AutoResetEvent in the Threading namespace. It has a WaitOne() and a Reset() method. So I tried that around the while(true) { ... socket.BeginAccept( ... ); ... }. Unfortunately this makes it only take one connection at a time. So I'm not really sure where to go. Here's my code:  class ServerRunner { private Byte[] data = new Byte[2048]; private int size = 2048; private Socket server; static AutoResetEvent allDone = new AutoResetEvent(false); public ServerRunner() { server = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); IPEndPoint iep = new IPEndPoint(IPAddress.Any 33333); server.Bind(iep); Console.WriteLine(""Server initialized..""); } public void Run() { server.Listen(100); Console.WriteLine(""Listening...""); while (true) { //allDone.Reset(); server.BeginAccept(new AsyncCallback(AcceptCon) server); //allDone.WaitOne(); } } void AcceptCon(IAsyncResult iar) { Socket oldserver = (Socket)iar.AsyncState; Socket client = oldserver.EndAccept(iar); Console.WriteLine(client.RemoteEndPoint.ToString() + "" connected""); byte[] message = Encoding.ASCII.GetBytes(""Welcome""); client.BeginSend(message 0 message.Length SocketFlags.None new AsyncCallback(SendData) client); } void SendData(IAsyncResult iar) { Socket client = (Socket)iar.AsyncState; int sent = client.EndSend(iar); client.BeginReceive(data 0 size SocketFlags.None new AsyncCallback(ReceiveData) client); } void ReceiveData(IAsyncResult iar) { Socket client = (Socket)iar.AsyncState; int recv = client.EndReceive(iar); if (recv == 0) { client.Close(); server.BeginAccept(new AsyncCallback(AcceptCon) server); return; } string receivedData = Encoding.ASCII.GetString(data 0 recv); //process received data here byte[] message2 = Encoding.ASCII.GetBytes(""reply""); client.BeginSend(message2 0 message2.Length SocketFlags.None new AsyncCallback(SendData) client); } } Have you looked at the MSDN example? Asynchronous Server Socket Example Also: Asynchronous Socket Programming Asynchronous Socket Programming in C#: Part I"
83,A,"Heartbeat Protocols/Algorithms or best practices Recently I've added some load-balancing capabilities to a piece of software that I wrote. It is a networked application that does some data crunching based on input coming from a SQL database. Since the crunching can be pretty intensive I've added the capability to have multiple instances of this application running on different servers to split the load but as it is now the load balancing is a manual act. A user must specify which instances take which portion of the input domain. I would like to take that to the next level and program the instances to automatically negotiate the diving up of the input data and to recognize if one of them ""disappears"" (has crashed or has been powered down) so that the remaining instances can take on the failed instance's workload. In order to implement this I'm considering using a simple heartbeat protocol between the instances to determine who's online and who isn't and while this is not terribly complicated I'd like to know if there are any established heartbeat network protocols (based on UDP TCP or both). Obviously this happens a lot in the networking world with clustering fail-over and high-availability technologies so I guess in the end I'd like to know if maybe there are any established protocols or algorithms that I should be aware of or implement. EDIT It seems based on the answers that either there are no well established heart-beat protocols or that nobody knows about them (which would imply that they aren't so well established after all) in which case I'm just going to roll my own. While none of the answers offered what I was looking for specifically I'm going to vote for Matt Davis's answer since it was the closest and he pointed out a good idea to use multicast. Thank you all for your time~ Do you know if it is possible to customize WebLogic's native heartbeat messages to add some additional information such as current CPU and/or network load? (to allow load-balancing algorithms that use that information to avoid overloading a struggling server with more requests) That's more of a question for Pascal (http://stackoverflow.com/questions/1442189/heartbeat-protocols-algorithms-or-best-practices/1442255#1442255). I'm not that familiar with WebLogic - in my case I ended up using what I had already started working on which was a custom solution based on UDP. I'm not sure this will answer the question but you might be interested by the way Weblogic Server clustering work under the hood. From the book Mastering BEA WebLogic Server: [...] WebLogic Server clustering provides a loose coupling of the servers in the cluster. Each server in the cluster is independent and does not rely on any other server for any fundamental operations. Even if contact with every other server is lost each server will continue to run and be able to process the requests it receives. Each server in the cluster maintains its own list of other servers in the cluster through periodic heartbeat messages. Every 10 seconds each server sends a heartbeat message to the other servers in the cluster to let them know it is still alive. Heartbeat messages are sent using IP multicast technology built into the JVM making this mechanism efficient and scalable as the number of servers in the cluster gets large. Each server receives these heartbeat messages from other servers and uses them to maintain its current cluster membership list. If a server misses receiving three heartbeat messages in a row from any other server it takes that server out of its membership list until it receives another heartbeat message from that server. This heartbeat technology allows servers to be dynamically added and dropped from the cluster with no impact on the existing servers’ configurations. Multicast may not work over a WAN. The advantage of multicast IMO is that you don't need to know how you're sending to (don't need to configure a list of nodes or peers). Agreed with both points. BTW I should have mentioned that starting with Weblogic 10 BEA/Oracle supports communication between cluster members using Unicast (which is encouraged) in addition to Multicast. Do you know if it is possible to customize WebLogic's native heartbeat messages to add some additional information such as current CPU and/or network load? (to allow load-balancing algorithms that use that information to avoid overloading a struggling server with more requests) @XpiritO Please open a new question for this (properly tagged etc). The comment system is not ideal to answer questions your queston may be useful for other readers and you may get better answers than just mine :) done that. Thanks in advance.  Distribued Interactive Simulation (DIS) which is defined under IEEE Standard 1278 uses a default heartbeat of 5 seconds via UDP broadcast. A DIS heartbeat is essentially an Entity State PDU which fully defines the state including the position of the given entity. Due to its application within the simulation community DIS also uses a concept referred to as dead-reckoning to provide higher frequency heartbeats when the actual position for example is outside a given threshold of its predicted position. In your case a DIS Entity State PDU would be overkill. I only mention it to make note of the fact that heartbeats can vary in frequency depending on the circumstances. I don't know that you'd need something like this for the application you described but you never know. For heartbeats use UDP not TCP. A heartbeat is by nature a connectionless contrivance so it goes that UDP (connectionless) is more relevant here than TCP (connection-oriented). The thing to keep in mind about UDP broadcasts is that a broadcast message is confined to the broadcast domain. In short if you have computers that are separated by a layer 3 device e.g. a router then broadcasts are not going to work because the router will not transmit broadcast messages from one broadcast domain to another. In this case I would recommend using multicast since it will span the broadcast domains providing the time-to-live (TTL) value is set high enough. It's also a more automated approach than directed unicast which would require the sender to know the IP address of the receiver in order to send the message.  Broadcast a heartbeat every t using UDP; if you haven't heard from a machine in more than k*t then it's assumed down. Be careful that the aggregate bandwidth used isn't a drain on resources. You can use IP broadcast addresses or keep a list of specific IPs you're doing work for. Make sure the heartbeat includes a ""reboot count"" as well as ""machine ID"" so that you know previous server state isn't around. I'd recommend using MapReduce if it fits. It would save a lot of work. Thanks the the very basic UDP pattern you mention is exactly what I had in mind. The MapReduce is certainly something I'll have to look into but I won't be able to use it for this application because the app is already implemented differently.  In addition to trying hardware load-balancers you can also try a free-open-source load-balancing software application such as HAProxy available for Linux and the BSDs.  Cisco content switches are a hardware solution for this problem. They implement a virtual IP address as a front end to multiple real servers whose real IP addresses are known to the switch. The switch periodically sends HTTP HEAD requests to the web servers to verify they are still running (which the switch software calls a ""keepalive"" although this doesn't keep the server itself alive). The Cisco switch accepts traffic on the virtual IP and forwards it to the actual web servers using configurable load balancing such as round-robin or user-defined load balancing. These switches retail in the $3-10K range although my business partner picked one up on eBay for about $300 a year ago. If you can afford one they do represent a proven hardware solution to the question of how to have a service spread transparently across multiple servers. Redhat includes a built-in port configuration so that you could implement your own Cisco switch using a cheap RedHat box. Google for ""virtual ip address"" and ""cisco content router"" for more information. @Paul thanks for the tip but those switches only allow you to know whether the server is still alive and not whether the app is still running - which is what I'm interested in. Ah in our case the server *is* the app. But you might look into the Linux-based option - since your app is already distributed it must have some communication infrastructure so you might be able to piggyback on that to provide a polling interface to the front-end. Most of these options offer a custom keepalive interface so you could write your own method. Good luck!"
84,A,"Device driver development Whether Objective C is used for any Device driver developments? now I am working as a ios developer.Is it possible to work in any other domains with this experience I'm at a loss as to what the last sentence means. Do you mean it as a question? Or as the reason you are working as an iOS developer. its a question....i am having experience in c++ and c...but i got a new job as ios developer ....so now am studying Objective C..i thinking about my future I comes from kernel level programming and now working on iphone and android developement..And I will tell you from my experience that going opposite (application programming to system level.) is like climbing everest..There is nothing to compare between two.. 99 percent of system level coding happens in C and for kernel programming purest form of C is used..Debugging is very difficult there a segmentation fault in application programming is a kernel panic (only solution to it is reboot) in kernel programming..It is not about having to reboot but on a reboot we loss entire information (crash information about our failed project)so that it is very difficult to debug..Surely kernel programming and device driver development is more challenging.. i know both C and C++.... and now am studying Objective C .. i am a beginner in Objective C and Xcode ""Surely kernel programming and device driver development is more challenging"". It may be more challenging but it is not something which requires creativity that you need at application development level. The challenge in DD development is because we have these monolithic kernel based operating system which were designed in older days of computing and during DD development you will be fighting against the kernel API etc more than you own driver logic I agree with you there..you are always fighting against the kernel itself..And it will be very difficult for someone who worked in the application level to comprehend..There is more chance for you to get stuck more chance to bang your head..Also most importantly to get help when you are stuck is even more difficult..  Device driver development is very specific domain. If you have been a application developer and even though you know a programming language which support driver development it would not be just an easy switch to driver development as it will be all together different concepts APIs etc that you need to learn and get a good hold of because when a driver (kernel mode) crashes it brings down the whole system. I don't think Objective-C is used for driver development. This area is for C or in some cases C++. I have done some driver development and felt application development is much more fun then driver development as in driver development you would be writing code that talk to hardware and thats it just pass data back and froth between hardware and OS where as in application development you can do a whole lot of amazing things your imagination is the only limit :) i know both C and C++.... and now am studying Objective C .. i am a beginner in Objective C and Xcode and i iphone development  Since you’ve tagged your question as mac I’m assuming you mean device drivers for Mac OS X. In that case they’re developed in C++ and use I/O Kit. More information here and here."
85,A,Coding a caching server I have been researching and have some few ideas about a distributed caching system for a in memory key-value store with replication and all the jazz associated. SO I wanted to know from the community what is the best language/framework/technology mix i should go for. Why not use the ones already there? Well memcached might be there but the thing which I am implementing should in theory gives upto 30-40% better performance than memcached for small cache sizes. Hence an almost from scratch work is needed. Surely you know there's stuff like memcached out there? It's powering some of the busiest sites on the web. No need to reinvent the wheel here. If you're going write your own anyway you want to make it as fast as possible so I'd choose C or C++. Fast widely supported easy to write bindings for other languages. Well memcached might be there but the thing which I am implementing should in theory gives upto 30-40% better performance than memcached for small cache sizes. Thanks for the answer though I was contemplating between Java and C++ till now mainly because of the the support for distributed systems.
86,A,Network Programming? I want to leave strictly desktop programming and start exploring networking. I want to make a little program that just sends data from computer A to B running the same program. Just a few questions before I start... 1) What is a better language for net programming Python or C#? 2) Could you recommend some stuff to help me understand how data is sent over the web? Thanks! I think C++ is the best if you consider the efficiency and hardware interactions. just google it. You'll find everything about network socket API's in C++. If you fail to do so please don't hesitate to ask. Could you please provide a reference for network development in C++. Thanks in Advance  I do not want to compare languages but as you wanted recommendation I recommend you python and twisted framework  It really depends on what you're doing. Both Python and C# have very capable modules for network communication - I'd say you'd be equally well-off in either given no knowledge of what you're doing. The decision between Python and C# will probably come down to whether your application lends itself more to the dynamic typing of Python or the static typing of C#. As for your second question I've always found it interesting to look at network traffic using a packet sniffer such as Wireshark. Browse to a website in Firefox and (assuming GZip encoding is disabled in the web-browser or on the server) you'll be able to see how the data is transferred. This works for other protocols as well. Reading the RFCs for various protocols can also give you some insight. For a few examples IRC (1459) FTP (959) HTTP 1.1 (2616). You can find them at the Internet Engineering Task Force website. Sorry for the lack of links the spam protection shot me down for not having enough reputation. +1 for some reputation. Plus it's a good answer :) Right now I'm making apps for iPhone but thats kinda getting boring (i haven't even been doing it for long) now im getting into networking with programs and then web development. Will the info from this help me later with web development? @shorty876 Understanding the HTTP protocol will help you with web development more than Network Programming. I'm not saying that Network programming won't help but web programming exclusively deals with HTTP. Sounds good Thanks everyone. +1 for good Request For Comment's  Both languages are equally capable. It is a matter of personal preference. What resources you need will depend on the application you intend to write. The two most important things you will need to know: The Application Layer Transport Layer & Internet Layers of the Internet Socket Programming You're diving into a very broad subject with a lot of information. I found this book to be helpful to me. @shorty876: Once you have implemented at least a couple of existing simple protocols via socket programming you may want to investigate higher level solutions such as various RPC mechanisms and WCF in .NET/C#.
87,A,"Network connection setup in constructor: good or bad? I'm working on a class that handles interaction with a remote process that may or may not be available; indeed in most cases it won't be. If it's not an object of that class has no purpose in life and needs to go away. Is it less ugly to: Handle connection setup in the constructor throwing an exception if the process isn't there. Handle connection setup in a separate connect() method returning an error code if the process isn't there. In option 1) the calling code will of course have to wrap its instantiation of that class and everything else that deals with it in a try() block. In option 2 it can simply check the return value from connect() and return (destroying the object) if it failed but it's less RAII-compliant Relatedly if I go with option 1) is it better to throw one of the std::exception classes derive my own exception class therefrom roll my own underived exception class or just throw a string? I'd like to include some indication of the failure which seems to rule out the first of these. Edited to clarify: The remote process is on the same machine so it's pretty unlikely that the ::connect() call will block. If the connection would take a long time it is more reasonable to put the code in another method. Still you can (and you should) use exceptions to inform the caller whether your connect() method has been successful or not instead of returning error codes. It is also more advisable to create a new exception class derived from std::exception instead of throwing plain data or even throwing other STL exceptions. You may also derive your exception class from a more specific description of your error (for example deriving from std::runtime_error) but this approach is less common.  I would go with the second one since I believe that the constructor should not do any other thing than initialize the private members. Besides that it's easier to deal with failures (such as not connecting). Depending on what you're exactly going to do you could keep the object alive and call the connect method when you need it minimizing the need of creating another object. As for the exceptions you should create your own. This will allow the caller to take specific rollback actions when needed.  I think Option 1 is a better approach but you need to think how would you expect the consumer of the class to use this? Just the fact that they have wired it up is good enough to go ahead and connect (Option 1) or the fact they should have the option to call Connect() when they are good and ready (Option 2)? RAII also supports the DRY principle (don't repeat yourself). However with Option 1 you need to ensure you Exception handling is spot on and you don't get into race conditions. As you know if there is an exception thrown in the constructor the destructor wont be called to clean up. Also be vary of any static functions you might have as you will need locks around those as well - leading you down a spiral path. If you haven't seen this post yet its a good read.  If your connection object is effectively non-functional if the connection fails then it doesn't make sense to have the object exist if all its other methods will always do nothing or throw exceptions. For this reason I would perform the connect in a constructor and fail by throwing an exception (derived from std::exception) if this method fails. However you are right that clients of the class may need to be aware that the constructor might block or fail. For this reason I might choose to make the constructor private and use a static factory method (named constructor idiom) so that clients have to make an explicit MakeConnection call. It is still the client's responsibility to determine if not having a connection is fatal to it or whether it can handle an offline mode. In the former case it can own a connection by value and let any connection failure propogate to its clients; in the latter it can own the object via a pointer preferably 'smart'. In the latter case it might choose to attempt construction of the owned connection in its constructor or it might defer it until needed. E.g. (warning: code all completely untested) class Connection { Connection(); // Actually make the connection may throw // ... public: static Connection MakeConnection() { return Connection(); } // ... }; Here's a class that requires a working connection. class MustHaveConnection { public: // You can't create a MustHaveConnection if `MakeConnection` fails MustHaveConnection() : _connection(Connection::MakeConnection()) { } private: Connection _connection; }; Here's a class that can work without one. class OptionalConnection { public: // You can create a OptionalConnectionif `MakeConnection` fails // 'offline' mode can be determined by whether _connection is NULL OptionalConnection() { try { _connection.reset(new Connection(Connection::MakeConnection())); } catch (const std::exception&) { // Failure *is* an option it would be better to capture a more // specific exception if possible. } } OptionalConnection(const OptionalConnection&); OptionalConnection& operator=(const OptionalConnection&); private: std::auto_ptr<Connection> _connection; } And finally one that creates one on demand and propogates exceptions to the caller. class OnDemandConnection { public: OnDemandConnection() { } OnDemandConnection(const OnDemandConnection&); OnDemandConnection& operator=(const OnDemandConnection&); // Propgates exceptions to caller void UseConnection() { if (_connection.get() == NULL) _connection.reset(new Connection(Connection::MakeConnection())); // do something with _connection } private: std::auto_ptr<Connection> _connection; }  Another thing that was unclear in my original post is that the client code doesn't have any interaction with this object once it's connected. The client runs in its own thread and once the object is instantiated and connected the client calls one method on it that runs for the duration of the parent process. Once that process ends (for whatever reason) the object disconnects and the client thread exits. If the remote process wasn't available the thread exits immediately. So having a non-connected object lying around isn't really an issue. I found another reason not to do the connection in the constructor: it means that I either have to handle teardown in the destructor or have a separate disconnect() call with no separate connect() call which smells funny. The teardown is non-trivial and might block or throw so doing it in the destructor is probably less than ideal.  Under the RAII mind of thought isn't this by definition good? Acquisation is Initialization.  Don't connect from the constructor a constructor that blocks is unexpected and bad API design. Write a connect method and mark your class noncopyable. If you rely on instances being connected already make the constructor private and write a static factory method to get pre-connected instances.  I consider it bad to do a blocking connect() in a constructor because the blocking nature is not something one typically expects from constructing an object. So users of your class may be confused by this functionality. As for exceptions I think it is generally best (but also the most work) to derive a new class from std::exception. This allows the catcher to perform an action for that specific type of exception with a catch (const myexception &e) {...} statement and also do one thing for all exceptions with a catch (const std::exception &e) {...}. See related question: How much work should be done in a constructor? There are exceptions of course where blocking constructors make sense RAII-based locks for example. Of course then documentation and appropriate naming are important. +1 By doing blocking operations in a separate connect method you leave the door open for a `cancel` method to cancel that operation.  Regarding throwing exceptions its perfectly fine to create your own classes. As a hypothetical user I'd prefer if they derived from std::exception or perhaps std::runtime_error (which allows you to pass an error string to the ctor). Users who want to can catch your derived type but the common idiom of:  try { operation_that_might_throw (); } catch (std::exception& e) { cerr << ""Caught exception: "" << e.what() << endl; } will work for your new exception types as well as anything thrown by the C++ runtime. This is basically the Rule of Least Surprise."
88,A,"Concern over handling bad call to accept() I'm writing a MUD server for personal learning purposes and I've happily managed to wrap up the socket stuff up into a couple of classes and everything appears to be working correctly; the server listens for and accepts connections and currently takes text from the client and sends it right back. The thing is I'm not quite sure what to do with a call to accept() that returns something other than WSAEWOULDBLOCK or a valid socket. Do I just reset the new socket to 0 and return with maybe an error message saying something bad happened? This is what I'm currently doing now with the addition of if it happens 20 times I'll shut down the server. void MUDControlSocket::Poll() { // create a new connection here timeval timeout; FD_ZERO(&ReadSet); FD_ZERO(&WriteSet); FD_ZERO(&ExceptionSet); TopSocket = GetSocket(); NewSocket = 0; FD_SET( GetSocket() &ReadSet ); if( SocketList.size() > 0 ) { for( sockIter iter = SocketList.begin(); iter != SocketList.end(); ++iter ) { FD_SET((*iter)->GetSocket() &ReadSet); FD_SET((*iter)->GetSocket() &WriteSet); FD_SET((*iter)->GetSocket() &ExceptionSet); TopSocket = (*iter)->GetSocket(); } } if( select( TopSocket+1 &ReadSet &WriteSet &ExceptionSet &timeout ) == SOCKET_ERROR ) { cout << ""Error on select() call: "" << SocketErrorType(WSAGetLastError()) << endl; delete this; exit(EXIT_FAILURE); } // as long as everything is working correctly this if block should always be entered UNLESS a new connection is accepted if( (NewSocket = accept(GetSocket() NULL NULL) ) == INVALID_SOCKET ) { if( WSAGetLastError() == WSAEWOULDBLOCK ) // it's not an actual problem. just nothing to connect to yet return; NewSocket = 0; static int count = 0; cout << ""Error on accepting new connection: "" << SocketErrorType(WSAGetLastError()) << endl; if( ++count >= 20 ) done = true; return; } SocketList.push_back(new MUDSocket(NewSocket)); // only happens if accept DOES NOT return a value of INVALID_SOCKET i.e. a new connection was accepted TopSocket = NewSocket; NewSocket = 0; } TopSocket and NewSocket are of type SOCKET and declared at file scope. SocketList is a std::list of MUDSocket* and MUDControlSocket is derived from MUDSocket as a singleton. Let me know if you need more info and thanks for any help. Return the error and let the calling code deal with it appropriately. The whole point is I'm not sure what the appropriate way to handle it would be. This isn't something that I plan on passing out for use elsewhere at least not right now so I'm trying to get some ideas on how I should handle it.  First: don't set the socket to 0: that's a valid fd for sockets on some *NIX systems and a bad habit to take. Assume the only invalid socket fd is -1. Doing anything else will give you real bugs in real software later on (trust me: I'm speaking from experience debugging code that used 0 as an invalid socket fd). Other than that I'd say just raise an exception: accept shouldn't fail unless you run out of resources which should be both exceptional and an error. C++ has a mechanism for handling such things and that's exceptions. BTW: delete this is almost always a very bad idea exiting in the middle of your code may make it hard to debug (throw an exception in stead) and let the caller do the exiting if need be) and in stead of trying to accept a socket with accept you can use select to tell you whether there is anything to accept - and move the special-case handling out of the function to only select in there. You could go a bit further and implement a specialized observer pattern (like I did on my podcast about a month ago) to practice not only your networking code but your design patterns as well. That would also help make your code more portable and re-usable later. HTH Thanks for the info.  Some of the other errors possible from accept is that memory is low the number of connections is exhausted etc. etc. Perhaps it could be handled by closing unused or forgotten connections or simply giving up and throwing an exception. I currently have some code set up that will on a return of SOCKET_ERROR from a recv() on a socket splice that bad socket into another list which would at some point get cleared out by calling delete on everything in it then empty the list."
89,A,"What is the best way to implement a cross-platform multi-threaded server in C/C++? Part of the development team I work with has been given the challenge of writing a server for integration with our product. We have some low-level sensor devices that provide a C SDK and we want to share them over a network for use by people collecting data. Sounds simple right? Someone would connect a sensor device to their machine in one part of the building and run our server thus sharing the device(s) with the rest of the network. Then a client would connect to that server via our application and collect sensor readings from the device. I created a simple language-agnostic network protocol and a reference implementation in Java. The problem is creating an implementation that will work with our devices that only provide an SDK written in C. We were thinking of doing the following: Create polling threads that collect and store the most recent readings from each connected device. Use a multi-threaded server to spin off each incoming connection to a worker thread. When a worker thread receives a request for a sensor reading the most recent value collected by the polling thread is sent back to the client. That's a lot of threading especially in C. So to review the general requirements are: Runs on Windows XP/Vista Linux and OS X machines Written in C or C++ to interact with the C SDK we have Accepts a variable number of simultaneous connections (worker threads) Must use threads not forking (don't want to deal with another layer of IPC) Can anyone suggest a library and preferably some example code to get use started? I'd also like to recommend The Spread Toolkit which (according to the project's web site) is ""an open source toolkit that provides a high performance messaging service that is resilient to faults across local and wide area networks"". I have used it couple of times in situations which sound very similar to yours. Essentially it gives you the server that frankodwyer suggests. The Spread daemon (i.e. the server) ain't multithreaded but it's really fast and scales well up to at least hundreds or thousands of clients. Moreover the protocol caters for reliable IP multicasting which (in a multi-client environment) may give you (performance-wise) a definite edge against anything implemented using point-to-point TCP or UDP connections only. (But: do not try to implement reliable IP multicasting yourself... apparently the Spread project has produced a number of PhD/MSc theses as a side-product - or is it the toolkit that is the side-product while the main emphasis was always academic research? I don't really know...). Since Spread has C and Java client APIs (plus Python) it sounds like a very good match to your problem. They have two licensing models; the first alternative is close to BSD's. And it's cross-platform of course (regarding both the client and the server). However Spread will (of course) not do everything for you. Most prominently perhaps it does not do persistence (i.e. if your client is offline or otherwise unable to receive messages Spread will not buffer them beyond a very small number of msgs at least). But fortunately it's not really too difficult to roll your own persistence implementation on top of what Spread does guarantee (well I don't even know if such an issue is important for you or not). Second Spread limits your messages to 100 kilobytes each but that limit is also quite easy to circumvent simply by making the sender chop a big message into a number of smaller ones and then concatenating 'em at the receiver.  I agree with frankodwyer flip the protocol from a pull model to a push model. Have the computer with the connected sensor broadcast the readings over UDP multicast at 100Hz (or whatever makes since for your sensors) whenever the sharing service is running. Then write clients that read the multicast data. Alternatively you could use broadcast UDP instead of multicast. BTW this is how many GPS Lidar and other sensors do things. nice use of multicast but they risk losing readings with UDP (no guaranteed delivery - the network is free to drop UDP packets on the floor). I don't know if anyone has come up with a 'multicast TCP' as yet - though I do know that people have worked on that problem it is not easy. @frankodwyer: yes they have see my reply :) (http://stackoverflow.com/questions/383371/what-is-the-best-way-to-implement-a-cross-platform-multi-threaded-server-in-cc#383519) ... of course the guarantees are not really equal to TCP's but then again it's a different problem. thanks i didn't know about spread it sounds really interesting. reliable multicast could be an elegant solution. Cisco also have something called PGM that does reliable multicast - it works best if you have Cisco kit though. Do you know the likelihood of a UDP packet getting dropped on a LAN? Unless you have your switches running at 95% utilization or higher UDP is probably safe. @ceretullis - that's probably correct (I haven't tested it myself). However I know that with syslog-udp people have tested it under heavy load and found almost 50% of messages go missing even on a local domain socket! Also UDP can be dropped anywhere including from a host stack even the transmit q! @frankodwyer I think everyone knows there is potential for UDP to be dropped. I think also think everyone widely overestimates the importance of their data when deciding against UDP. For sensors you could have the clients implement a Kalman filter to deal with a dropped reading or two.  Douglas Schmidt's ACE (Adaptive Communications Environment) is a mature highly portable open-source framework for building high-performance multithreaded servers. It's mainly aimed at telecommunication applications but has been used for a variety of projects. It also comes with an object request broker called TAO (if you're into CORBA) One claim to fame of the framework is that it supports many threading models (thread pool thread per request asynchronous + threads etc.) so you can use its thread management in a way that's optimal for your application. This is actually the most interesting feature of the system - the server framework functionality comes out of the box. Most of the other libraries I've seen suggested here would still require you to implement much of this functionality yourself. There is quite a bit of electronic documentation and also several books written about it. It's not the most warm and fluffy of systems and is really built for speed rather than comfort - but we are using C++. You will probably find that it's much less effort to get your head around ACE than try to rebuild the functionality and debug all the synchronisation. Oh and by the way it's free as in speech - and as in beer. If you want to a commercial route there is also an ecosystem of consultants that will provide support and mentoring services for it. A tutorial with some code snippets can be found here.  I've used Boost.Thread & Boost.Asio to build a multi-threaded server on Windows & Linux systems. The tutorials made it easy to get started. This combination appears to be exactly what I need. Thanks! You can't go wrong with Boost for your C++ code. :) I would strongly recommend you don't write a server and consider refactoring it out of your design. See my answer for more detail. I agree with frankodwyer no need for a server for this application... but if you insist on writing a server for this you use a single-thread and multiplex the network I/O make sure to async I/O as well (Boost.Asio should be good for this).  The best way to write such a server is not to write one and to rearchitect your system so it is not necessary and/or to reuse components that already exist. Because: Someone would connect a sensor device to their machine in one part of the building and run our server thus sharing the device(s) with the rest of the network. This also has the potential to share the entire machine with rest of the network if your code has a vulnerability (which it probably will as you're writing it in C++ from scratch and inventing a new protocol). So do it the other way around. Install a simple client on the machine that has the sensor hardware then run it either all the time or periodically and have it push (post) results to a central server. The central server could even be a standard web server. Or it could be a database. (Notice that both of these have been written already - no need to reinvent the wheel ;-) Your application then works the same way you have in mind now however it collects data from the database rather than the sensors. The part running on the machine with the sensor however has shrunk from a multi-threaded custom server nightmare to a nice little single threaded command line client that only makes outgoing connections and which can be run from cron (or equivalent on windows). Even if you need real time data collection (and from your description it sounds like you do not) it still may be better for the sensor collector be a client and not a server. Let it open a long lived connection to a central collector (or a group of them) and await instructions to provide its data. edit: ceretullis and pukku's answers suggest a nice variation on this using multicast - see this answer and the comments One more benefit in the separate server solution (centralized or distributed) becomes clear when you need to add *another* sensor which all the clients need to see as well... Implementing that may be tedious given the server-per-sensor approach but should be a breeze in frankodwyer's setting.  I use libevent to multiplex network I/O. You should consider it as an alternative to Boost.Asio. The libevent API provides a mechanism to execute a callback function when a specific event occurs on a file descriptor or after a timeout has been reached. Currently libevent supports /dev/poll kqueue event ports select poll and epoll.  I would use QT. It has a cross-platform threading support. Good documentation: QT Threading Documentation Their signal/slot messaging mechanism works seamlessly between threads too. Cool -- I never realized QT had an easy to use cross-platform threading component to it. Yeah QT has excellent non-GUI support networking too. Qt isn't free though and for small projects the price can be rather alot. It is now for most definitions of free (http://www.qtsoftware.com/about/news/lgpl-license-option-added-to-qt)  Use a Cross-Platform API or Create your own API which you change on each Architecture. Also revise this: http://www.goingware.com/tips/getting-started/  I highly recommend you consider the prototype design pattern. I used this pattern to write a protocol agnostic server in C++ that I have used for everything from HTTP Web Services to custom proprietary binary protocols. Basically the idea is this: The Server takes care of accept()ing incoming connections on a particular port and creating threads (or processes) to handle those connections. When you're trying to build a generic server you realize that you cannot really read or write any data without making assumptions about the protocol... So the trick is to use the prototype pattern. Create a ""ConnectionHandlerBase"" class with a pure ""HandleConnection() = 0"" method. Make the users of the server class subclass this class with their own implementation. Additionally this class implements a ""Clone()"" method that returns a copy of itself... This way the server can create new instances of it without needing to know its type... Then when you get a connection call ""Clone()"" on your prototypical instance and have the handling thread call ""HandleConnection()"" on this object. At application startup the user of the server class has to call something like this: ""Server.AttachConnectionPrototype( &MyConnectionObject );""  Not really an answer to your question but since it sounds like you are actually more familiar with Java than C/C++ why not continue with your reference implementation and connect to the SDK using Java Native Interface. (I never really used it but I figured it would be useful exactly for these kinds of situations.) Alternatively you could easily write a simple C program that employs the SDK and then sends the data to your Java program using socket-based streams for example. This way you could again handle the more difficult stuff in Java. We actually considered this but we decided finding a cross-platform threading library would be easier and less troublesome than trying to learn JNI -- no one in the team is really familiar with it. Since JNI is one of those easy-to-screw-up things threads seemed that safer option :-) JNI is a bit tricky to get started with but we're talking about two days to get it working. It would have been my suggestion as well considering that the target hardware is powerfull and that code in java already exists.  If you want to use C (and not C++) the NSPR library might provide what you need..."
90,A,"Socket connection to my ip rather than 127.0.0.1 Well i am at new at Socket programming in java. I tried to implement a simple socket program to send the message with 127.0.0.1:4242 as localhost. But i want to send message to specific IP. How can i achieve that? Will it be possible to send message to my own IP as client-server running simultaneously? An endpoint in socket communications is an endpoint. Anything you can do using 127.0.0.1 can be done using that machines ip address. See here for more details.  Every IP datagram has a source address and a destination address in the IP header plus a transport protocol number which is for majority of Internet traffic is either TCP or UDP. Then the header for that transport protocol lists source and destination port numbers. So here you have it - sending or better said ""client"" application gets assigned source address and port usually automatically - address determined by the local routing table port number assigned out of range of ephemeral ports while ""server"" application listens on a well known port bound to an address at a particular machine. This tuple (source IP source port destination IP destination port) is enough for the datagram to get from here to there. 127.0.0.1 and actually all the addresses in the range 127/8 are reserved for the loopback a virtual local interface i.e. it's the way to say ""no matter what my real address is or even if I don't have one connect to this machine I'm at right now"". Read up on the TCP/IP suite of protocols - it's a fairly simple concept (with ton of interesting details of cource)."
91,A,"Listening for connections in a separate thread This is part of a messenger project in java. Because clients use direct connections to chat I want eavry client to listen on some port and others to make a socket to that address. but when i call ServerSocket.accept() in another thread it appears that all threads have been suspended. which means nothings happens after executing that command. Here is the code which makes new thread. java.awt.EventQueue.invokeLater(new Runnable() { @Override public void run() { try { while(true){ System.out.println(""flag1""); Socket socket = listeningSocket.accept(); System.out.println(""flag2""); new Chat(socket).setVisible(true);; jTextArea1.append(""successfully connected\n""); } } catch (NullPointerException e) { System.out.println(""i know""); } catch (IOException e) { e.printStackTrace(); jTextArea1.append(""error in recieving connection\n""); } } }); any ideas how to solve this? it should be blocked most the time bus sometimes should respond to others @kvphxga mre is referring to the fact that you have asked eight questions on Stack Overflow but accepted answers to only one of them. This is considered rude unless you truly feel no one answered any of your questions adequately. when i call ServerSocket.accept() in another thread it appears that all threads have been suspended Appears how? accept() only blocks the current thread. Are you calling it in the AWT thread? e.g. an actionPerformed() method? Don't do any network operations in those methods use separate threads. the GUI of the program isn't loaded correctly. by the way ... this code is in the constructor. this code is in the constructor of a JFrame. Is it in a AWT thread? thanks ... you helped me alot :)  Socket.accept() DOES block the CURRENT thread. You'll see ""flag2"" printed only after a connection is received. But it blocks only CURRENT thread. I suspect you are not running the separate thread correctly (you're calling yourThreadHere.start() not .run() right?). no ... I've updated the question ... u may see. EJP is right you are executing that code in the AWT/Swing Thread that will freeze all your program interface. Read about the AWT/Swing threading model. ummm ... what's that thread? this code is in the constructor of a JFrame object. Is it in the AWT/SWing thread? Yes java.awt.EventQueue.invokeLater does not launch another thread it executes the given runnable in the main interface thread. great ... you were right ... thanks very much :)"
92,A,"BSD Sockets don't behave in a iPhone 3G environment I noticed that many times while developing for an iPhone 3G BSD socket functions will simply fail. I also noticed at the time the 3G antenna wasn't even ON nor was there WIFI Access to back up the network call (So it seems ridiculous that it doesn't turn on to support the network request).. This information was verified with an app from Apple in the SDK called Connectivity Test or something of the sort. Basically if you load Safari or something then quickly load up the App it would be fine.. Of course that's not ideal. Apparently to apple gethostbyname() or something of the sort is by no means a reason to turn on the Antenna. I contacted Apple about this and they said that the BSD functions do not switch the Antenna on but calling all of the Objective-C CFNetwork functions do. I want portable code so is there a way to keep my existing BSD setup? I really dislike coding in Objective-C so if anyone knows a work around that would be awesome. Kyle to answer the question you posed in your edit comments: There's no reason to tag your question ""programming"" - if it weren't related in some way to programming you shouldn't have posted it to SO. That's why I removed that tag. Understood. Maybe it shouldn't be allowed that'll save everyone time. It was sort of an instinct to tag it with that. The CFStream Socket Additions are what Apple recommends you use instead of the direct BSD sockets. They specifically warn about using the BSD sockets here: Although BSD (POSIX) networking APIs are available in iPhone OS you should avoid using them. If you communicate directly with sockets certain networking capabilities of iPhone OS such as VPN On Demand do not work. Use the APIs provided in CFStream Socket Additions instead. Note that CFNetwork and the like aren't Objective-C but straight C for almost everything. I wonder why they omitted the fact that it doesn't turn on the ant. CFNetwork it is then!.. Then back to standards for everyone else lol. @Kyle: To be fair can you use your BSD socket code as-is on Android WebOS or Windows Phone 7 Series? It tends to be hard to make elements like this truly platform independent (as opposed to data model or core logic code). Android NDK and WebOS PDK yea however I think 7 series will give developers alot of porting to do with all the managed code and .NET libraries. I'm not sure if they'll allow for DLL lookups or not but I would suspect we're in for a nightmare with that."
93,A,"Copy files to network path or drive using python on OSX I have a similar question like the one asked here but I need it to work on OSX. http://stackoverflow.com/questions/2625877/copy-files-to-nework-path-or-drive-using-python So i want to save a file on a SMB network share. Can this be done? Thanks! Yes it can be done. First mount your SMB network share to the local filesystem by calling a command like this from Python: mount -t smbfs //user@server/sharename share (You can do it using the subprocess module). share is the name of the directory where the SMB network share will be mounted to and I guess it has to be writable by the user. After that you can copy the file using shutil.copyfile. Finally you have to un-mount the SMB network share: umount share Probably it's the best to create a context manager in Python that takes care of mounting and unmounting: from contextlib import contextmanager import os import shutil import subprocess @contextmanager def mounted(remote_dir local_dir): local_dir = os.path.abspath(local_dir) retcode = subprocess.call([""/sbin/mount"" ""-t"" ""smbfs"" remote_dir local_dir]) if retcode != 0: raise OSError(""mount operation failed"") try: yield finally: retcode = subprocess.call([""/sbin/umount"" local_dir]) if retcode != 0: raise OSError(""umount operation failed"") with mounted(remote_dir local_dir): shutil.copy(file_to_be_copied local_dir) The above code snippet is not tested but it should work in general (apart from syntax errors that I did not notice). Also note that mounted is very similar to the network_share_auth context manager I posted in my other answer so you might as well combine the two by checking what platform you are on using the platform module and then calling the appropriate commands. Cool! Got it working! Thanks for the quick (and elaborate) reply! (Would like to vote up but haven't got enough rep :-| )"
94,A,"Is there any data missing? Here is some code quoted from Douglas.E.Comer's < Computer Networks and Internets > 4th edition. This program will send back any data it received. ... while((len = recv(conn buff BUFFERSIZE0)) >0) // receive data send(conn buff len 0); // send it back ... I am wondering what if some data arrived when the code is executing in send(..) function will it miss that data? Because the recv() function is not being executed. If no data is missed where is the data kept? And by whom? Thanks... Incoming data is buffered by the system until the next time the recv() function is called. This data that hasn't been read yet is stored in buffers inside the operating system and not inside your application. You will not ""miss"" incoming data using a loop like this. Thanks Greg. So there're actually 2 type of buffers. My application's buffer and the OS's buffer. If there's too much data could the OS's buffer be flooded? What happens then? If your receiving application doesn't read enough data fast enough then the OS will tell the *sending* machine to stop sending more data. In turn the sending machine's operating system will have its own outgoing data buffers to hold data passed by `send()` from the sending application and if *those* fill up then the sending application will block in its calls to `send()`. So it is all properly arranged by the OS or library method such as send(). And it seems we just need to care about our programming logic to use the infrastructure properly.  Every socket has a associated memory buffer where the data is being buffered when you call recv API the data is read from buffer if some data is present in buffer other wise the call wait till some data is available in socket buffer. Thanks Ankur. Please see my reply to Greg Hewgill. Your understanding is correct i.e. you just need to focus on your application logic. TCP was build to assure packets delivery. I am not sure if this will be the same case as in a UDP socket"
95,A,WCF or Sockets for Communication in a real-time environment? I have a scenario where I need to send a series of data across 2 clients. The data includes Serialized XML that will contain commands that the other client will need to react to. I will also need to send images across the wire as I need to provide a chat facility in the form of video/audio chat. I would like a single communication medium for both as the number of messages/commands might be a few. WCF or Sockets? Here are some of the things I would think about: WCF might be useful if you anticipate several different kinds of client connecting to your server. Maybe in the future you might want to let other people write clients more or less independent of you. On the other hand if it's a closed system then you might prefer to write your own sockets code. WCF gives you a higher-level abstraction so presumably you can write your system more quickly. In particular things like XML encoding and session management are not really part of your application domain and so you don't want to spend much time working on them. But higher abstraction typically involves a performance cost because the abstraction layer is more general-purpose than any one application needs. With plain sockets you can tailor your system to your own needs and that might allow for higher performance (at the cost of more fiddly development and bug-fixing.) You might want to send your data/commands and video in separate streams. Presumably the data/commands must be sent over a reliable transport but the video can suffer some loss. Or maybe the video should be sent with a high QOS whereas the data/commands can suffer latency. I've never actually used QOS and so I don't know what the issues are here but it could impact your decision about WCF (either favourably or negatively.) You can host your server in its own process or in IIS. If you host it yourself then you can do things your own way. I believe that WCF and IIS are good friends so if you're thinking IIS then WCF might make life a lot easier. If you choose IIS (or any established web server) over your own host you can take advantage of their infrastructure - scalability reliability encryption and so on. The downside is that you might get locked into that server but that might not be a problem in practice. Depending on your environment you may be able to mix and match the various technologies and features. For example we have a system that sounds vaguely similar to yours and we opted for: plain sockets in the client; plain sockets in the server but with an option to host the server in Apache instead; a custom XML library that does just what we need; embedded OpenSSL; COM at the core of the system but with dependencies on .NET. In particular we used SOAP in our first prototype because its messaging and RPC was a perfect match for our design but found it added too much complexity and replaced it with our own protocol. If you have the time then I suggest that you build a quick prototype in WCF and see what you think. Hope that it works out but don't be afraid to dump it if it doesn't. The main principle is to deliver maximum business value to your customers as efficiently as possible and that usually means that you should spend your efforts in your application domain rather than on infrastructure. But at the same time don't ignore secondary principles such as performance reliability scalability maintenance extensibility and so on.  WCF or Sockets??? These are not alternatives: WCF includes the NetTcpBinding and NetPeerTcpBinding for TCP/IP communication. If your clients are both Windows you should use WCF. Pieter's question is based on my comment to his previous question.  WCF is API for building service oriented applications. I don't thing that Video/Voice chat is such kind of application. First of all what transport features do you require for such application? My knowledge of Voice/Video transmission is very poor but I expect: Duplex communication where client connects to chat server sends his voice/video data and receives voice/video data of other client. Streaming - the amount of data can be pretty big so it would be nice to work with data when you start to read them. Moreover some encoding algoritms are supposed to be used with streaming. Quality of service - controling sustainable pace of transmission. (If you want to make conference video chat you also need coordination of multiple streams) So by simply describing these three expectations I already see the problem. WCF build-in Net.Tcp binding does not allow duplex communication and streaming together. Also be aware that performance of Net.tcp communication over WCF is much worse than plain socket communication. It is because WCF simplifies a lot of things but those simplification and generalizations slows processing. Also based on selected algorithm this could be a problem. Here you can find some describtion of Video chat in Silverlight 4. Silverlight allows WCF and net.tcp binding but the communication is still developed on sockets. Here you can find description of widely used protocols for voice over IP - RTP for data transmission and RTCP for coordination and QoS. These protocols are also used for video transmission. By googling I found implementation for .NET (I don't know how good the implementation is I just used google ...)
96,A,"Correct way to use Kernel threads in freebsd (+ networking) I have an assignment to build support for distributed mutual exclusion using raymond's algorithm in freebsd. This requires a kernel thread to always listen on a udp port for messages from other systems and act accordingly. I'm creating a thread using thread_create but inside every call to socreate creates a kernel panic. What's the best way to do what I'm doing? I couldn't find any good tutorials on kernel networking in freebsd. On another note maybe the mainproc variable is not set correctly. How can I find out current struct proc* or struct thread*? My current code is like this: static struct proc *mainproc; static int val; static int main_thread_finish; static struct socket *listenso; static struct sockaddr_in listenadr; static void main_thread(void* data) { static int res; printf(""In thread\n""); res = socreate(AF_INET &listenso SOCK_DGRAM IPPROTO_UDP mainproc->p_ucred mainproc->p_singlethread); printf(""socreate res: %d\n"" res); listenadr.sin_family = AF_INET; listenadr.sin_port = htons(1234); listenadr.sin_addr.s_addr = 0; res = sobind(listenso (struct sockaddr*)&listenadr mainproc->p_singlethread); printf(""bind res: %d\n"" res); while(!main_thread_finish) { pause(""DUMMY"" hz); } printf(""kthread exiting...\n""); kthread_exit(); } static int raymond_module_load(struct module *module int cmd void *arg) { int error = 0; switch (cmd) { case MOD_LOAD : val = 12345; main_thread_finish = 0; kproc_create(main_thread NULL &mainproc 0 0 ""raymond_main_thread""); printf(""Module loaded - kthread created\n""); break; case MOD_UNLOAD : main_thread_finish = 1; printf(""Waiting for thread to exit...\n""); pause(""TWAIT"" 3*hz); printf(""Module unload...\n""); break; default : error = EOPNOTSUPP; break; } return (error); } static moduledata_t raymond_module_data = { .name = ""raymond_module"" .evhand = raymond_module_load .priv = NULL }; DECLARE_MODULE(raymond_module raymond_module_data SI_SUB_KLD SI_ORDER_ANY); The call to socreate panics because mainproc->p_singlethread is NULL. This variable is not the thread associated with the process but is used to enforce ""single-threading"" within a process (see the function thread_single() in sys/kern/kern_thread.c for more details). Probably what you wanted to do was was to use curthread res = socreate(AF_INET &listenso SOCK_DGRAM IPPROTO_UDP curthread->td_ucred curthread); ... res = sobind(listenso (struct sockaddr*)&listenadr curthread);  Consider using netgraph it has ready to use ng_ksocket module."
97,A,"How to Create a Virtual Network Adapter in .NET? I would like to create/add a virtual network adapter to a client operating system at runtime (via code) preferably in C#. Something similar to that of what VirtualBox/VMware/Himachi creates when you install their software. I am guessing this will require some C/C++ shenanigans for the driver integration but if it is doable with only C# all the better. I am aware of OpenVPN their stuff is primarily in C and I am also aware of the TUN/TAP drivers floating around I just didn't know if these were the only solutions not requiring me creating a fully loaded network driver for Windows. I'm curious to know if it is possible to do **any** low-level Windows drivers like this in managed .NET code. I don't think I've ever heard of anything like this being done though I may be wrong. If you need simple funcionality then you can use Microsoft Loopback Adapter. To install it use devcon tool. Here is some info about it http://support.microsoft.com/kb/311272. devcon -r install %WINDIR%\Inf\Netloop.inf *MSLOOP After that you can use WMI query with C# to obtain new connection name and then netsh to configure it (ie. netsh int ip set address name=""Local Area Connection 2"" static 192.168.0.3 255.0.0.0) That is exactly what I was looking for. I am familiar with creating/utilizing MS Loopbacks but didn't know about devcon's existence. Excellent answer have yet to find another as logical in my searches. Thanks alot! For those on Win7 x64 you may be interested in http://superuser.com/questions/305685/devcon-exe-not-working-in-windows7-x64 MD5 of the x64 version: 3904d0698962e09da946046020cbcb17 `devcon failed` for me."
98,A,"""Sliding Window"" - Is it possible to add reliability to a protocol and avoid flow control Implementation? As a part of a personal project I am making an application level protocol (encapsulated in UDP) which is reliable. For the implementation of reliability I have to keep track of which packets i send and which packets are received at the other receiver end. This is done with the help of a sliding window and it also maintains the flow-control. Is there a way to implement reliability apart from standard sliding window/flow control technique. If No will someone share his experience/design rationale/code and discuss it over this post. If Yes have you implemented it or if you know any implementation of the concept. You might want to look at SCTP - it's reliable and message-oriented.  It's sad that the TCP/IP stack doesn't include a reliable datagram protocol but it just doesn't. You can search for lots of attempts and proposals. If this is a personal project your time is probably the most scarce resource and unless your goal is to reinvent this particular wheel just build your protocol on top of TCP and move on.  You could have a simple Send->Ack protocol. For every packet you require an Ack before proceding with the next (Effectivly this is windows size = 1 packet - which I wouldn't call a sliding window :-) yes thats fine rather i can also have a window of size say 5 to better throughput but for ultimate throughput from sender i would need a max window and for maximum throughput of the network i would need a congestion window. i am confused what to do  After reading all these answers learning from other implementations and reading some papers I am writing/sharing what is most pertinent. First let me talk about Flow control and later i will talk about reliability. There are two kinds of flow control-- Rate based -- Packets Transmission is done in a timely manner. Window based -- The Standard Window based can static or dynamic (sliding window). Rate Based flow controls are difficult to implement as they are based on RTT(round trip time -- its not as simple as ping's RTT) calculation. If you decide on providing a proprietary congestion control system and you are providing it from present release then you can go for rate-based flow control. Explicit congestion control is also there which is router dependentso its out of picture. Window based flow control a window is used to keep track of all the sent packet until the sender is sure that receiver has received them. Static window is simple to implement but throughput will be miserable. Dynamic window (also know as sliding window) is a better implementation but a little complex to implement and depends on various kind of Acknowledgement mechanisms. Now Reliability... Reliability is making sure the receiver has received your packet/information. Which means that receiver has to tell the sender yes I got it. This notification mechanism is called Acknowledgement. Now one ofcourse needs throughput for the data transfered also. So you should be able to send as many packets as you can rather MAX[sender's sending limit receiver's receiving limit] provided you have available bandwidth at both the ends and throughout the path. If you combine all the knowledge although reliability and flow control are different concepts but implementation wise the underlying mechanism is best implemented when you use a sliding windows. so finally in short I and anyone else if designing a new app protocol and needs it to reliable sliding window shall be the way to achieve it. If you are planning to implement congestion control then u might as well use a hybrid(window+rate based) approach (uDT) for example.  It might be a better approach to maintain local state in the UDP applications to check that necessary data has been transferred for confirming reliability -- rather than trying to do complete packet level reliability and flow-control. Trying to replicate reliability and flow-control mechanisms of TCP in a UDP path is not a good answer at all.  I sort of agree with Nik on this one it sounds like you're using UDP to do TCP's job reliable transmission and flow control. However sometime's their are reasons to do this yourself. To answer you're questions there are UDP based protocols that do reliable transmission and don't care much about ordering meaning only dropped packets have a performance penalty for making it to the destination (by requiring retransmission). The best example of this we use daily is a protocol called RADIUS which we use for Authentication and Accounting on our EVDO network. Each packet between the source and destination get's an identifier field (radius only used 1 byte you may want more) and each identifier needs to be acked. Now Radius doesn't really use the concept of a sliding window since it's really just a control plane traffic but it's entirely viable to use the concept from TCP. So because each packet needs to be acked you buffer copies of outgoing packets until they are acked by the remote end point and everyone is happy. Flow control can use feedback from this acknowledgement mechanism to know it can scale up/down the rate of packets which may be most easily controlled by the size of the list you have for packets transmitted but awaiting acknowledgement. Just keep in mind there are literally decades of research into the use of sliding window and adjustments to TCP/IP stacks around packet loss and the sliding window so you may have something that works but it may be difficult to replicate the same thing you get in a highly tweaked stack on a modern OS. The real benefit to this method is you need reliable transport but you really don't need ordering because you're sending disjoint pieces of information. This is where TCP/IP breaks down because a dropped packet stop all packets from making it up the stack until the packet is retransmitted.  You could have something like the following: Part 1: Initialization 1) Sender sends a packet with the # of packets to be sent. This packet may require some kind of control bit to be set or be a different size so that the receiver can distinguish it from regular packets. 2) Receiver sends ACK to Sender. 3) Repeat steps 1-2 until ACK received by Sender. Part 2: Send bulk of data 4) Sender then sends all packets with a sequence number attached to the front of the data portion. 5) Receiver receives all the packets and arranges them as specified by the sequence numbers. The receiver keeps a data structure to keep track of which sequence numbers have been received. Part 3: Send missing data 6) After some timeout period has elapsed with no more packets received the Receiver sends a message to the Sender requesting the missing packets. 7) Sender sends the missing packets to the Receiver. 8) Repeat steps 6-7 until Receiver has received all required packets. 9) Receiver sends a special ""Done"" packet to the Sender. 10) Sender sends ACK to Receiver. 11) Repeat steps 10-11 until ACK is received by Receiver. Data Structure There are a few ways the Receiver can keep track of the missing sequence numbers. The most straight-forward way would be to keep a boolean for each sequence number however this will be extremely inefficient. You may want to keep a list of missing packet ranges. For example if there are 100 total packets you would start with a list with one element [(1-100)]. Each time a packet is received simply increment the number on the front of that range. Let's say you receive packets 1-12 successfully miss packets 13-14 received 15-44 miss 45-46 then you end up with something like: [(13-14) (45-46)]. It would be quite easy to put this data structure into a packet and send it off to the Sender. You could maybe make it even better by using a tree instead not sure. @MahlerFive -- 1. By definition SACK implementation is complicated and is useful when you have large buffers I have to investigate why? 2. How would tree fit in here it would be interesting to see if it works. 3. I was thinking of making a circular queue of elements as pointer to the packet seqno and bool as a Data-Structure and implementing a cummulative Ack system. Do you think if this is enough/efficient. Please if you other idea i would be pleased."
99,A,"More data in packet payload I have the following code int ParseData(unsigned char *packet int len) { struct ethhdr *ethernet_header; struct iphdr *ip_header; struct tcphdr *tcp_header; unsigned char *data; int data_len;  /* Check if any data is there */ if(len > (sizeof(struct ethhdr) + sizeof(struct iphdr) + sizeof(struct tcphdr))) { ip_header = (struct iphdr*)(packet + sizeof(struct ethhdr)); data = (packet + sizeof(struct ethhdr) + ip_header->ihl*4 + sizeof(struct tcphdr)); data_len = ntohs(ip_header->tot_len) - ip_header->ihl*4 - sizeof(struct tcphdr); if(data_len) { printf(""Data Len : %d\n"" data_len); PrintData(""Data : "" data data_len); printf(""\n\n""); return 1; } else { printf(""No Data in packet\n""); return 0; } } } I am trying to print in ASCII the payload and with a simple function like this PrintData(char *mesg unsigned char *p int len) { printf(mesg);  while(len--) { if(isprint(*p)) printf(""%c"" *p); else printf("".""); p++; } } The code looks good no compile problems/warning. The problem is that the first payload character is not being print at position 0 but 12 bytes later. I thought that all the ""len"" bytes are the exact data I have to print. My data point at data = (packet + sizeof(struct ethhdr) + ip_header->ihl*4 + sizeof(struct tcphdr)); however data[0] is not printable. What is the problem? Do I miss something? Do I have to check for the TCP options part maybe? Thanks That's right adding the sizeof(struct tcphdr) is only going to get you past the header not the options. To get to the actual data you should use the 'offset' field from the TCP header. The offset is calculated from the start of the TCP header and is in 4-byte units e.g. if the offset is 8 then the header + options length is 32. In other words instead of sizeof(struct tcphdr) I need to put the tcp_header->doff*4 Yes exactly right."
100,A,"how to use XML sent by html form? i have html form with textarea in which i paste some XML for example: <network ip_addr=""10.0.0.0/8"" save_ip=""true""> <subnet interf_used=""200"" name=""lan1"" /> <subnet interf_used=""254"" name=""lan2"" /> </network> When user submit form that data is send to Java server so in headers i get something like that: GET /?we=%3Cnetwork+ip_addr%3D%2210.0.0.0%2F8%22+save_ip%3D%22true%22%3E%0D%0A%3Csubnet+interf_used%3D%22200%22+name%3D%22lan1%22+%2F%3E%0D%0A%3Csubnet+interf_used%3D%22254%22+name%3D%22lan2%22+%2F%3E%0D%0A%3C%2Fnetwork%3E HTTP/1.1 how can i use that in my Java applications? I need to make some calculations on that data and re-send new generated XML. This answer shows how to use the URLDecoder/URLEncoder classes to decode and encode url strings. It should work if you passed the 'GET' string to the URLDecoders decode method. To answer your following question (comment) First you need to extract this xml based response from the url string. Maybe it's enough to create a substring starting with the first < char. The String should be fed into a XML parser to create a DOM document. The last easy task would be walking through that document and copying the values to your internal network model. Do not think about using RegExp to extract the data. Use a parser. Thanks great. Now i have string like: /?we= How can i put it to for example an array?"
101,A,"Question about using XML as an application-level protocol I am wondering if I could use XML as the application-level protocol. The XML will just sit on-top of the TCP protocol and the 2 applications will just need to know how to parse XML. But the problem is XML is not compact enough. Should I use some algorithm to compact the XML payload before send it with TCP? If I do compact it then the compressing/decompressing cost will be involved on both ends. What a dilemma. Any good suggestions? Or I take the wrong approach? Many thanks. I'd say that you can use straight XML for your messages and don't bother with compression or anything (XML isn't exactly blazingly fast) if you need a simple way of specifying how your messages look. If you need something faster and more compact take a look at Google Protocol Buffers or Thrift (I personally prefer Protocol buffers but everyone is not like me..). Using XML can for instance be a good way if you need to make your interface be easy to interoperate with lots of different clients for instance (such as Web Services). If on the other hand you are always in charge of both ends of the communication you can always do something quick to get going and then optimize later.  Define ""compact enough"" have you measured it to be too slow for your application? Avoid premature optimisation. As with any protocol there are trade-offs in various directions. XML buys you a well-known cross-platform format with libraries for just about any language that's capable of representing all kinds of structured data. XMPP opts for this and uses optional compression for bandwidth-constrained setups. Experiments in the XMPP world with alternate representations have rarely proven worth the effort. A notch down from XML but still providing many of the advantages is JSON. While it lacks namespacing it's fairly simple and libraries are near as common as XML ones. Still JSON is text-based and may still be verbose for some situations. The last sensible choice would be a binary protocol. This has advantages in that you can tailor and optimise it specifically to your application. The disadvantages are that you have to write the parsing and serialization yourself although there are tools to automate this such as Google's Protocol Buffers project. Ultimately all of these are suitable in different places and the choice is up to the application developer for which one they should use for a given project."
102,A,"Daemon network process (perl) under Redhat RHEL5 is denying network connection I wrote a program that is using the Perl POE Framework to realize a json webservice. So far so good. I have no problems running that application under debian systems. But when i run my application under RHEL5 the network connection is refused. I have no setuid so the service is running as root and the port (9991) is out of the serviceport-range. My workaround ist to start the application as nondaemon with nohup $CMD &. Thats more than stupid. But I've absolute no idea my init.d script DEBIAN: #!/bin/sh -e ### BEGIN INIT INFO # Provides: jobserver # Required-Start: $local_fs $network $syslog # Required-Stop: $local_fs $network $syslog # Default-Start: 2 3 4 5 # Default-Stop: 0 1 6 # Short-Description: start/stop jobserver ### END INIT INFO APPLICATION_CONFIG=""/etc/jobserver/jobserver.conf"" test -f $APPLICATION_CONFIG && . $APPLICATION_CONFIG set -e if [ ! -x $APPLICATION_PATH ] ; then echo ""No jobserver installed"" exit 0 fi #load init.d helper functions . /lib/lsb/init-functions PIDFILE=$APPLICATION_PIDFILE CONF=$APPLICATION_CONFIG DAEMON=$APPLICATION_DAEMON PARAMETER=""--configuration $CONF --daemon"" if [ -z ""$PIDFILE"" ] ; then echo ""ERROR: APPLICATION_PIDFILE needs to be defined in application config"" >&2 exit 2 fi jobserver_start() { #log_daemon_msg ""Starting Jobserver Daemon"" log_success_msg ""Starting Jobserver Daemon"" start-stop-daemon --start --quiet --oknodo --make-pidfile --pidfile ""$PIDFILE"" --exec ""$DAEMON"" -- $PARAMETER #log_end_msg $? } jobserver_stop() { log_success_msg ""Stopping Jobserver Daemon"" start-stop-daemon --stop --quiet --oknodo --pidfile ""${PIDFILE}"" rm -f ""${PIDFILE}"" #log_end_msg $? } case $1 in start) jobserver_start ;; stop) jobserver_stop ;; restart) jobserver_stop jobserver_start ;; *) log_success_msg ""Usage: /etc/init.d/jobserver {start|stop|restart}"" exit 1 ;; esac exit $?; REDHAT: # Source function library. . /etc/rc.d/init.d/functions APPLICATION_CONFIG=""/etc/jobserver/jobserver.conf"" test -f $APPLICATION_CONFIG && . $APPLICATION_CONFIG PIDFILE=$APPLICATION_PIDFILE CONF=$APPLICATION_CONFIG DAEMON=$APPLICATION_DAEMON PARAMETER=""--configuration $CONF --daemon"" RETVAL=0 if [ -z ""$PIDFILE"" ] ; then echo ""ERROR: APPLICATION_PIDFILE needs to be defined in application config"" >&2 exit 2 fi jobserver_start() { echo -n $""Starting jobserver daemon: "" daemon --pidfile=$PIDFILE $DAEMON $PARAMETER RETVAL=$? if [ $RETVAL -ne 0 ]; then failure; fi; echo return $RETVAL } jobserver_stop() { echo -n $""Stopping jobserver daemon: "" if [ ! -f $PIDFILE ]; then echo -n $""Jobserver daemon is not running: ""; else killproc -p $PIDFILE RETVAL=$? fi echo return $RETVAL; } case $1 in start) jobserver_start ;; stop) jobserver_stop ;; restart) jobserver_stop jobserver_start ;; *) echo ""Usage: /etc/init.d/jobserver {start|stop|restart}"" exit 1 ;; esac exit $?; Daemonprocess: sub daemonize { # start a new child process if (fork()) { exit(0); } # become process group leader unless (POSIX::setsid) { die(""POSIX setsid failed: $!""); } # change to root dir chdir(""/""); foreach (0 .. (POSIX::sysconf(&POSIX::_SC_OPEN_MAX) || 1024)) { POSIX::close($_); } # allow only user based io umask(077); # reopen pipes open(STDIN ""<"" ""/dev/null""); open(STDOUT "">"" ""/dev/null""); open(STDERR "">"" ""/dev/null""); # Advisory. Fork one more time; this is not ""necessary"" for most toolserver # daemons but it is a best practice as there are situations where # forking twice is needed to avoid zombies. A second fork also # prevents the daemon from ever re-acquiring a terminal by making # the main daemon process not be the process group leader if (fork()) { exit(0); } # write pidfile my $pidfile = ""/var/run/jobserverd.pid""; open(PIDFILE "">$pidfile""); print(PIDFILE ""$$""); close(PIDFILE); } Serverfront: sub listen { my $self = shift; logInfo(""Starting webservice. Listening to port "".$self->{'_port'} 1); # Spawn the webservice POE::Component::Server::HTTP->new ( Port => $self->{'_port'} ContentHandler => { ""/json/"" => \&dispatchJSONService } Headers => { ""Server"" => ""Perl JobServer version "".$self->{'_version'} } ); $poe_kernel->run(); } RHEL + connection problems... Is SELinux enabled and in ""enforcing"" mode? Run `tail -f /var/log/audit/audit.log` in another terminal while trying to start the script. If you see errors SELinux is at fault. Problem solved. The Problem was the double fork. Thanks to all for reading. Never trust advisories! ;) How was the problem the double fork? I've to admit that i don't know the details. After i removed the sec. fork the daemon runs normal. I don't know. Good to know anyway; thanks!"
103,A,"How to get a mac address Can I get a MAC address that are connected to my site. this code get mac address host and return error permission.  String macadress = string.Empty; foreach (NetworkInterface nic in NetworkInterface.GetAllNetworkInterfaces()) { OperationalStatus ot = nic.OperationalStatus; if (nic.OperationalStatus == OperationalStatus.Up) { macadress = nic.GetPhysicalAddress().ToString(); break; } } return macadress; now how can get mac address users??? 2. how can get ip users??? You cannot get the MAC address of the end-user's machine. You can get the user's public IP address using Request.UserHostAddress. Note the IP address this will not be unique per-user. If multiple users are behind the same proxy or are on a corporate network they will usually share the same address. You can check the X-Forwarded-For header to get a little more information. Note that this header can be chained or faked.  public string GetMacAddress(string ipAddress) { string macAddress = string.Empty; System.Diagnostics.Process pProcess = new System.Diagnostics.Process(); pProcess.StartInfo.FileName = ""arp""; pProcess.StartInfo.Arguments = ""-a "" + ipAddress; pProcess.StartInfo.UseShellExecute = false; pProcess.StartInfo.RedirectStandardOutput = true; pProcess.StartInfo.CreateNoWindow = true; pProcess.Start(); string strOutput = pProcess.StandardOutput.ReadToEnd(); string[] substrings = strOutput.Split('-'); if (substrings.Length >= 8) { macAddress = substrings[3].Substring(Math.Max(0 substrings[3].Length - 2)) + ""-"" + substrings[4] + ""-"" + substrings[5] + ""-"" + substrings[6] + ""-"" + substrings[7] + ""-"" + substrings[8].Substring(0 2); return macAddress; } else { return ""not found""; } }  You cannot get the MAC address from the request however you can get the IP with Request.UserHostAddress  Unfortunately you can't get the user's MAC address in the way that you want. It's my understanding that MAC addresses are stripped from packets when they leave your local network. You can try to get the user's address from Request.UserHostAddress. However if you are behind a load balancer or content distribution network then you might want to try looking in Request.Headers[""X-Forwarded-For""] first - this is where the users original IP address will often be written when the request is forwarded on by something. The approach I'll usually take is to try something along the lines of: var address = Request.Headers[""X-Forwarded-For""]; if (String.IsNullOrEmpty(address)) address = Request.UserHostAddress; The last project I worked on we actually logged both in case the forwarded for header had been faked. _Unfortunately?_ Unfortunately for GodIsLive ;) MAC addresses are a feature of the _Local_ Area Network - the MAC address you would see in a packet would be from your router because that is the thing in your LAN that forwarded that packet to you."
104,A,"My python program always brings down my internet connection after several hours running how do I debug and fix this problem? I'm writing a python script checking/monitoring several server/websites status(response time and similar stuff) it's a GUI program and I use separate thread to check different server/website and the basic structure of each thread is using an infinite while loop to request that site every random time period(15 to 30 seconds) once there's changes in website/server each thread will start a new thread to do a thorough check(requesting more pages and similar stuff). The problem is my internet connection always got blocked/jammed/messed up after several hours running of this script the situation is from my script side I got urlopen error timed out each time it's requesting a page and from my FireFox browser side I cannot open any site. But the weird thing is the moment I close my script my Internet connection got back on immediately which means now I can surf any site through my browser so it must be the script causing all the problem. I've checked the program carefully and even use del to delete any connection once it's used still get the same problem. I only use urllib2 urllib mechanize to do network requests. Anybody knows why such thing happens? How do I debug this problem? Is there a tool or something to check my network status once such situation occurs? It's really bugging me for a while... By the way I'm behind a VPN does it have something to do with this problem? Although I don't think so because my network always get back on once the script closed and the VPN connection never drops(as it appears) during the whole process. [Updates:] Just found more info about this problem when my program brings down the internet connection well it's not totally ""down"" I mean I cannot open any site in my browser or always get urlopen error timed out but I still can get reply using ""ping google.com"" in cmd line. And when I manually dropped the VPN connection then redial without closing my program it starts to work again and also I can surf the net through my browser. Why this happening? @Shane: Still sounds like you have too many open connections. Opening a webpage creates a TCP connection pinging sends an ICMP package you are running out of TCP connections because you aren't using close(). del is rarely useful. I assume you close()? That *should* be enough. No idea on how to help you debug it though. @Lennart: So: `response = urllib2.urlopen(""http://google.com"")` would `del response` close this connection? No `del response` just removes the local variable `response` thus removing one reference to the urllib object. If it is the only reference to the response object it will be deleted and yes closed. But that shouldn't be done manually with `del` instead you should let it fall out of scope. If you are unsure you can manually call `close()` on objects but that shouldn't be necessary. But it doesn't sound like a problem with your application. @Shane: No. del would mark it for garbage collection. It would be closed when collected which could be some other time completely. You should call response.close(). @Rosh: I just don't know why my script brings down my network and once the script closed my network gets back on immediately. And it's caused by `urlopen error timed out` every time. This really pissed me off @Shane: Are you calling .close() or not? @Lennart: Not yet I'm gonna give it a try although I don't think it's the problem... I just don't know where to look at and how to locate the problem @Lennart: I just found out I can still get reply from ""ping google.com"" while my network is ""down"" although I cannot surf any site in my browser what kind of network problem is this? @Shane: I think it's the problem. You are going to end up with loads of open connections until your computer runs out of handles. Make sure you close() the connections and life will be fine. `del` is pretty much useless as you have no control over when or even if the connection gets closed. You could possibly be creating more threads than you expect - monitor the result of threading.active_count() to test this. If possible try to rule out the VPN at your end (or post the relevant guts of the code so we can test it). (Nettiquete) If you're not doing so already only use network.http.max-connections-per-server threads per monitored site/host. (For reference) urlopen returns a file-like object - use .close() or del on this object or the socket will be sat in a CLOSE_WAIT state until a timeout. Hopefully these points are well pointers.  This may or may not be the problem but it's a good idea to always use context managers when dealing with things that opens resources like files or urls. Since Python 2.5 you can do this with files: with open('/tmp/filename' 'rt') as infile: data = infile.read() whatever(data) And the file will be automatically closed at the end of the block. urllib2 doesn't support this automatically but you can use contextlib to help you: >>> import contextlib >>> with contextlib.closing(urllib2.urlopen('http://www.python.org')) as page: ... for line in page: ... print(line) <html> blablablabla</html> This way the connection will be both closed and deleted at the end of the with-block so you don't have to think about it. :-) Thanks a lot man. It's indeed `del` causing all the problems I found tons of ""Close_Wait"" connections using `netstat` and these connections used up all the resources that's why I got urlerror timeout from then on. After changing all `del` to `.close()` everything works fine now. ^_^ Is there a way/tool to check what's going on in my network and what python is doing with my network while my problem occurs?"
105,A,How to find out how much I should read from a socket? In .NET there is the DataAvailable property in the network stream and the Available property in the tcp client. However silverlight lacks those. Should I send a header with the lenght of the message? I'd rather not waste network resources. Is there any other way? What do you mean by 'waste network resources'? Every network read API I am aware of returns the actual number of bytes read somehow. What's the actual problem here? Silverlight is non-blocking. And I would like to check if I should read and how much to read before I read anything. @the_drow it doesn't make any difference whether it is blocking or non-blocking. The read API will *tell* you how much it read. @EJP: True but the call will still occupy the socket which is somewhat wasteful. @the_drow that is completely meaningless. Wasteful how? Non-blocking is non-blocking. Period. If you want to know whether to read the non-blocking socket at all that's what select() is for. @the_drow I don't. I just select or block until something arrives. @EJP: Why would you use DataAvailable and Available then? I am using it like this: `if (stream.Available) { byte[] buffer = new byte[tcpClient.Available]; stream.BeginRead(buffer 0 buffer.Length callback null);}`  You are micro-optimizing. Why do you think that another 4 bytes would affect the performance? In other words: Use a length header. Update I saw your comment on the other answer. You are using BeginRead in the wrong way. It will never block or wait until the entire buffer have been filled. You should declare a buffer which can receive your entire message. The return value from EndRead will report the number of bytes received. You should also know that TCP is stream based. There is no guarantees that your entire JSON message will be received at once (or that only your first message is received). Therefore you must have some sort of way to know when a message is complete. And I say it again: A length header will hardly affect the performance. @jgauffin: So why do the DataAvailable and Available exists than? What I mean is that just because you send 15 bytes doesn't mean that you'll receive 15 bytes. It depends on what the socket implementation does with your send request. `DataAvailable` isn't really useful when working with `BeginRead` since it's an asynchronous operation. @jgauffin: Sure it does. This is TCP. UDP doesn't guarantee that all data will be received. You might have to call `BeginReceive` twice to get all 15 bytes. TCP only guarantees that everything will be received and in the correct order. Not that everything will be received with the same number of calls to `Receive` as the number of calls to `Send`. @jgauffin: Why isn't there a BeginReceive until the buffer is full then? because you are using it incorrectly. Call it as soon as the socket is connected and call `EndReceive` in the callback. Or switch to non-blocking sockets and `Receive`. Do not mix. In my case they might. I might delay other network events that are much more important. The TCP header is between 20 and 60 bytes. And you got the Nagle Algorithm that delays your packets. I seriously doubt that 4 bytes will affect performance. If you have smaller packets use a `byte` or a `Int16` to reduce the number of bytes then. @jgauffin: Well I'm working with IP over high voltage. I need to use as less bandwidth as possible. How large is an average message? And what do the messages contain? @jgauffin: JSON-RPC messages that may or may not contain binary data of arbitrary size. But I'd rather reduce the noise as much as possible. IP over high voltage in certain implementations is half duplex. see my updated answer. @jgauffin: So it won't wait until the buffer is filled? Why is that? Because TCP is stream based. You never know the number of bytes that you'll receive in a read.
106,A,How to do IP walk in C my computer's IP is 192.168.1.101 over eth0. I want to know what are other active/used IP under 192.168.1.* I am expecting a list of IP address that is ping-able under 192.168.1.* How can I do that in C? And preferably in linux platform. Any C functions available? @cdhowie - if i cannot ping (because of different reasons like firewall) then my C should skip that address. @Neilvert: Then how about pinging each address and seeing which ones respond? I'm just curious how to do that in C. If there's existing function for it etc... Thanks for the comment:-) What have you tried so far? Are you allowed to ping the broadcast address on your network? Used IP addresses may not necessarily be pingable. Per-machine firewalls can block pings easily. You might get better answers if you clarify *for what purpose* you want this information. http://nmap.org/ <- does that in C. Try this: http://beej.us/guide/bgnet/ There is no built-in function in C that sends ping packet. There is however function that just sends packet. There is also a lot of code in the internet that already implements ping. What you have to do is just take one of them (this for example) and ping in a loop for all addresses in your network. You should know however that ping is not a reliable way of saying which addresses are in use. RFC 792 - Internet Control Message Protocol says: The Internet Protocol is not designed to be absolutely reliable. The purpose of these control messages is to provide feedback about problems in the communication environment not to make IP reliable. There are still no guarantees that a datagram will be delivered or a control message will be returned. Some datagrams may still be undelivered without any report of their loss. which means that any message can be easily lost with no notification. Furthermore target does not have to respond.
107,A,How to send raw data over a network? I've same data stored in a byte-array. The data contains a IPv4 packet (which contains a udp-packet). I want to send these array raw over the network using C# (preferred) or C++. I don't want to use C#'s udp-client for example. Does anyone know how to perform this? Sorry for my bad English and thanks for your help in advance!!! If not UdpClient then what? TcpClient? no... see Matthias Wandel answer it's going into the right way... What do you expect the difference to be when you use raw sockets? What are you hoping to achieve? see: http://stackoverflow.com/questions/2493384/how-to-fake-source-ip-address-of-a-udp-packet using System.Net; using System.Net.Sockets; public class Test { public void Send(byte[] rawData IPEndPoint target) { // change what you pass to this constructor to your needs Socket s = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.IPv4); try { s.Connect(target); s.Send(rawData); } catch(Exception ex) { // handle this exception } } } Thank you very much!!! Can I specify the source ip-address as well? That's going to add IP+TCP headers on top of your existing byte[]. Doesn't your packet contain the destination address in the header already why would you then specify _target_ when sending it? yes the rawData-Array already contains a fully-formatted ipv4 packet!!! (so source and destination are specified in the array). I just want to array! `s.Connect(target);` confused me! Socket s = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.IPv4); throws an SocketException when I execute it to get rid of the exception use ProtocolType.Tcp instead of IPv4. But I agree with Ben Voigt - unless you want IP+TCP headers on top of your existing byte[] you should not use the code I gave you. If you don't care it will work fine. @ben voigt: any suggestions to get rid of the ip+tcp headers on the top of my existing byte[]???  Here is a way to send raw data over NIC http://www.codeproject.com/KB/IP/sendrawpacket.aspx As mentioned above Windows is restricting raw socket operations you have to modify NDIS driver to be able to send whatever you want. Of course you will then have a problem with digital driver signing on Vista/7 (can be temporary bypassed with test mode).  When you have raw data (ie a byte-array) and you want to send it over a network then you need some sort of encoding: If you send multiple blocks (whole arrays) the recipient needs to be able to differentiate between the end of one and the start of the next. If the data is really large its is better to split it into smaller blocks (yes packets) to play well with other users of the network. You need to know that the data is error-free at the client as networks have the tendency to be unreliable at exactly the wrong time for you. Encoding solves the first point above. TCP is the conventional solution to the second two points. Examples of encoding are: HTTP encodes the length in cr delimited lines then a pure binary blob. Text Files could be ctrl-z delimited. XML can be delimited simply by its syntax of tags. the byte[] already contains an IPv4 packet. He doesn't need more encoding he needs to stop the TCP/IP stack from encoding it further (by adding another IP+UDP header envelope etc). the array is very small it fits easily into one packet. The main problem is that the metainformation (source-ip destination-ip...) should be send raw (there are stored in the array).  I found a solution for may problem: see answer at: http://stackoverflow.com/questions/2493384/how-to-fake-source-ip-address-of-a-udp-packet  Try raw sockets (specify SOCK_RAW for the socket type). You will be responsible for calculating the IP checksums as well. This can be a little annoying. thanks... I'm trying! +1 for your nice answer!!! do you think it could work with: Socket socket = new Socket(AddressFamily.InterNetwork SocketType.Raw ProtocolType.Raw); Just so you know receiving via raw sockets on Windows requires admin privileges. Transmitting via raw sockets is restricted in several ways beginning with Windows XP SP2 (http://msdn.microsoft.com/en-us/library/ms740548(VS.85).aspx).
108,A,Are singleton network event processors okay? Suppose you have a system on the other side of a network that sends events and data that needs to be cached to some intermediate broker. Instead of giving every component of your application that needs to be informed of such events a new subscription to the broker I decide for performance and simplicity (the third party library that handles broker subscriptions isnt pretty) I should have only one Event Processor that subscribes to the broker and programatically fires events as it receives them to subscribed listeners provided by the components. The cached data can also be shared from this singleton. This will greatly reduce network connections. However according to most discussions about singletons they are always evil PERIOD unless for concurrency reasons or hardware reasons you need only one access point. This is not my situation since every component could have their own subscription and their own personal cache of data since all the data can be requested over the broker. However this could easily add 200 more network connections. Because singletons are evil does that mean 200 more connections to a broker with 200 copies of data is better than using singleton I don't need to use? After all this slows things down quite a bit but its not game breaking the application is still usable. There's nothing inherently wrong with your broker client object servicing multiple clients within your process. All the talk about singletons being evil is really about global variables being evil. A singleton becomes evil because it provides a static access point to mutable state not because there is only one instance of it. In that light you might want to use dependency injection to hook it up rather than calling Broker.getInstance(). This avoids client code making the assumption that it is in fact a singleton.
109,A,"UPnP announce goes out but device is not discovered i am writing code for a UPnP device to announce itself. i have no interest in it being able to discover other UPnP devices on the network. i am sending out the following messages (taken from wireshark) NOTIFY * HTTP/1.1\r\n NT: upnp:rootdevice\r\n USN: uuid:0000-1111-2222-3333::upnp:rootdevice\r\n NTS: ssdp:alive\r\n LOCATION: http://192.168.40.8:80/rdd2.xml\r\n HOST: 239.255.255.250:1900\r\n CACHE-CONTROL: max-age=900\r\n Content-Length: 0\r\n \r\n AND NOTIFY * HTTP/1.1\r\n NT: urn:schemas-upnp-org:device:BinaryLight:1\r\n USN: uuid:0000-1111-2222-3333::urn:schemas-upnp-org:device:BinaryLight:1\r\n NTS: ssdp:alive\r\n LOCATION: http://192.168.40.8:80/rdd2.xml\r\n HOST: 239.255.255.250:1900\r\n CACHE-CONTROL: max-age=900\r\n Content-Length: 0\r\n \r\n AND NOTIFY * HTTP/1.1\r\n NT: uuid:0000-1111-2222-3333\r\n USN: uuid:0000-1111-2222-3333\r\n NTS: ssdp:alive\r\n LOCATION: http://192.168.40.8:80/rdd2.xml\r\n HOST: 239.255.255.250:1900\r\n CACHE-CONTROL: max-age=900\r\n Content-Length: 0\r\n \r\n the device will not discover (either on the Win 7 page where i see my other UPnP devices or on Intel's Device Spy for UPnP Technologies -- see update). i have 2 questions. first are all of these messages necessary? and second can anyone see why my device would not be discovered? other info: my rdd2.xml device description file <?xml version=""1.0""?> <root> <specVersion> <major>1</major> <minor>0</minor> </specVersion> <device> <deviceType>urn:schemas-upnp-org:device:BinaryLight:1</deviceType> <friendlyName>RFLC</friendlyName> <manufacturer>Legrand</manufacturer> <manufacturerURL>http://www.legrand.us/</manufacturerURL> <modelDescription>Legrand Low Cost RF Lighting Control</modelDescription> <modelName>X-10L1</modelName> <modelNumber>L1</modelNumber> <modelURL>http://www.legrand.us/</modelURL> <serialNumber>0000001</serialNumber> <UDN>uuid:0000-1111-2222-3333</UDN> <UPC>00000-00001</UPC> <presentationURL>pres.html</presentationURL> </device> </root> and some info from wireshark (a view down one msgs frame list) + Frame ..... + Ethernet II Src: my device  Dst: 239.255.255.250 + Internet Protocol Src Port: ssdp (1900) Dst Port: ssdp (1900) - Hypertext Transfer Protocol ... msgs from above are here... UPDATE: i CAN see the device on intel's upnp utilities. that was a mistake writing that. the device was running under a debugger and once rebooted with no debugger it DID discover in the utilities. i still cannot see it on windows 7 though? any thoughts? Have you compared to data obtained via wireshark for devices that _do_ get discovered? ahh yes good point i forgot to mention that. i HAVE and the only thing i can see different is the stuff that should be different (i.e. uuid location etc). It appears that SERVER: is a required header -- at least one client I was playing with did not like notifications with no SERVER:  For some crazy reason I had better luck when I set this field to have extra spaces: CACHE-CONTROL: max-age = 900\r\n"
110,A,Sockets in windows vs linux Are there differences in how Windows and Linux implements sockets? Is this a os concert or rather one entirely dependant on programming platform? Presumably their implementations are very different (Though it's hard to tell as the windows source code isn't available.). Both provides a rather similar API for programmers to use based on the BSD socket api  and the socket API for various programming languages are a wrapper around that native C API.
111,A,"Who captures packets first - kernel or driver? I am trying to send packets from one machine to another using tcpreplay and tcpdump. If I write a driver for capturing packets directly from the NIC which path will be followed? 1) N/w packet ----> NIC card ----> app (no role of kernel) 2) N/w packet -----> Kernel -----> NIC card ---> app Thanks What operating system? Also your question is ill-formed; a driver is just as much a part of the kernel as anything else in the kernel. @Nemo your comment about the driver being part of the kernel is what brought me to this question. I was going to point out the same flaw. A driver is the piece of code that interacts directly with the hardware. So it is the first piece of code that will see the packet. However a driver runs in kernel space; it is itself part of the kernel. And it will certainly rely on kernel facilities (e.g. memory management) to do its job. So ""no role of kernel"" is not going to be true. Thanks. Yes my question is not so clear I agree sorry I wish I knew how to put it properly.  It's usually in this order: NIC hardware gets the electrical signal hardware updates some of its registers and buffers which are usually mapped into computer physical memory Hardware activates the IRQ line Kernel traps into interrupt-handling routine and invokes the driver IRQ handling function The driver figures out whether this is for RX or TX For RX the driver sets up DMA from NIC hardware buffers into kernel memory reserved for network buffers The driver notifies upper-layer kernel network stack that input is available Network stack input routine figures out the protocol optionally does filtering and whether it has an application interested in this input and if so buffers packet for application processing and if a process is blocked waiting on the input the kernel marks it as runnable At some point kernel scheduler puts that process on a CPU and resumes is application consumes network input Then there are deviations from this model but those are special cases for particular hardware/OS. One vendor that does user-land-direct-to-hardware stuff is Solarflare there are others. Thanks for taking time out to answer this not-so-clear question. Your explanation is in detail. Do you happen to know any resource/tutorial where I can read more about it (and stop getting embarrassed by asking such questions? ) Don't have just one reference for you. All books on OS internals and networking (especially Stevens) help. If you are into Linux you might find ""Linux Device Drivers"" ( http://lwn.net/Kernel/LDD3/) very educational."
112,A,"How can I ensure two tasks are running on different threads or even different processors with the new Task Parallel Library? I have two long running tasks that read and write simultaneously from a network stream which requires me to use two different threads. I have marked them as long running but according to the docs marking them as long running only recommends but doesn't guarantee that they will run on different threads. Do I need to write my own Task Scheduler? Should I use the thread pool instead? The LongRunning enum method overload tells the Task library to dedicate a thread to each task regardless of the number of cores it can use. I know that but it's just a suggestion. Isn't it>  If ""network stream"" means the NetworkStream class I'm not sure why you really need two separate threads here. Are you basing that on the following quote from the documentation: ""As long as there is one unique thread for the write operations and one unique thread for the read operations there will be no cross-interference between read and write threads and no synchronization is required"" If so that's only a suggestion - they're saying that if you do things this way you don't need to do any synchronization. The main reason they're saying this is that NetworkStream is unusual in that it supports access from two threads simultaneously as long as one is reading and one is writing. But it's clear from the documentation that you also have the option to perform synchronization - they're just saying that synchronization is unnecessary if you choose to go down the path of having exactly one read thread and exactly one write thread. But if for some reason you really do need two dedicated threads then...I'd just use the Thread class. Trying to get Task to do that for you (either directly or indirectly via a TaskScheduler) is probably just making things more complex. This doesn't prevent you from using the TPL - you could use the TaskCompletionSource to create tasks and finish them at your own leisure. (TCS is the normal solution for situations where you want to control how tasks execute but still expose them as a Task object.) I need to both read and write at the same time because I am streaming. So if I am getting it right I should use the tasks only for executing tasks inside the thread right? I wasn't questioning whether you need to read and write simultaneously. I was questioning the need for (specifically) two threads. You might have any number of threads - as long as no more than 1 reads at any one time and no more than 1 writes at any one time you don't need to dedicate one thread to reading and one thread to writing. There are a couple of reasons to use Task: a) use a built-in Task if its execution model suits you - not the case here - or b) implement your own Task if you have an unusual execution model but still want Task semantics. b) might suit you here perhaps.  LongRunning is a hint that it should be executed on a new Thread instead of an existing thread in the ThreadPool. This is so you wont exhaust the ThreadPool with long-running tasks. There are no guarantees that two different threads will be used neither in TPL nor with the ThreadPool. For example it could be the same thread executing both tasks synchronously. Have you looked into using asynchronous io instead? Do you mean using BeginXXX and EndXX yes but that also can complete synchronously.  string[] forums = File.ReadAllLines(Environment.CurrentDirectory + @""\forums.txt""); Task[] tasks = new Task[forums.Length ]; int ctr = 0; DateTime Start = DateTime.Now;` foreach(string s in forums)` { object state = s; var task = Task.Factory.StartNew(() => DoSomeWork(state)TaskCreationOptions.LongRunning); tasks[ctr] = task; ctr++; } Task.WaitAll(tasks); DateTime end = DateTime.Now; TimeSpan elapsed = end - Start; string totalMs = elapsed.TotalMilliseconds.ToString(); Console.WriteLine(""DONE in "" +totalMs + "" ms. Any Key to quit.""); Console.ReadKey(); That won't guarantee my tasks to run on different threads"
113,A,"How can I reverse engineer an application's protocol? I'm using an application (an instant messenger) which is not very popular. I'm trying to find the protocol that it uses. I know it's using TCP/IP but I want to find out all the commands that it is sending to the server and receiving from the server. I tried a couple of sniffers but they can not recognize this application by name and more over all I got was some unrelated hexadecimal codes. Is there any idea how I can find the application's specifications? (Please note: I googled it and found nothing and also there is no documentation by the author.) Wireshark will tell you the protocol. The fact that you cannot read the messages in clear text on the wire is a good thing isn't it? The fact you can't read the clear text on the wire isn't good for two reasons: 1) It suggests that the protocol is secure when in fact it's just ""security by obscurity"" - which means that it's inconvenient to read not secure. 2) It's easier to debug plain text protocols with tools like wireshark. **However** being a binary protocol suggests that it might be a lot more efficient in it's representation of data in packets - which could be very useful. How do you know it's ""security by obscurity"" it could be something like https?  There are generally two approaches to reverse engineering something like this: You could try disassembling it with a tool like IDA PRO. You could try sniffing its traffic with a tool like Wireshark Either way it's likely to be a LOT of work."
114,A,"Looking for solutions to limit http requests to multiple hosts to what can be afforded while maximizing throughput In an application which downloads many documents over http in parallel I would like to make optimal use of the network connection without pushing it beyond its limits and getting timeouts. I am thinking this has to do with congestion control. Perhaps a gradual increase in request frequency until the network connection appears to be overburdened followed by a slight drop in request frequency followed by continuous monitoring to adjust the rate. The bit I'm having trouble with is how best to detect the overburdened network condition. If I were to measure the time between issuing a request and the beginning of the response that would effectively give me a round trip time. If the average of this time increases significantly then we have an overburdened network. I wonder what 'significantly' should mean in this case. Does this sound about right? Can you shed any more light on this problem? Anyone out there coded this scenario? I have tagged this question .net because that is the framework I'm using and if there is framework support for this scenario then I'd like to know. EDIT To clarify I am talking about many hosts here and only one instance of the application. I already have in place a system to avoid simultaneous connections to the same server (requests are delivered end to end) so the question is not so much how to saturate the pipe (I know how to do this) but how best to limit requests so as to avoid timeout errors. Unless you are coding this just for personal use you should also consider what happens if multiple clients are hitting the same server at the same time using your algorithm. Traditionally Web browsers limited themselves to two simultaneous connections per Web server. IE8 increased this to six pissing off lots of Web server admins. See here for more discussion of this issue. Note that TCP already has congestion control algorithms that attempt to saturate the pipe even for one (1) connection. If the documents you are downloading are not tiny (10s of kilobytes or more) you will probably find that opening tons of connections to the same server will not speed things up and may slow them down. The only way lots of connections to the same server will help is if (a) it is heavily loaded and your goal is just to suck up more than your ""fair share"" of the server's bandwidth; or (b) you are downloading lots of tiny files over distinct HTTP connections so the TCP algorithm does not have enough time to adapt itself to the available bandwidth of the link. My suggestion which I doubt you will like is to open a fixed number of connections per server (e.g. two) and just let TCP do its job. Good advice thank you for your answer. However I have made some edits to my question to clarify: I am talking about multiple hosts. OK that's a better question :-). Hm. Maybe keep track of the download bandwidth and keep opening new connections until that value stops improving? You would need to do it slowly of course and use a moving average for the bandwidth...  Thanks Nemo. I implemented your suggestion of monitoring bandwidth with a moving average. I use this value to adjust a value representing target number of outstanding requests. I orchestrate the issuing of new requests such as to tend towards this moving target. Someone also suggested using a bandwidth-limiting proxy."
115,A,"Network receipt timer to ms resolution My scenario I'm collecting network packets and if packets match a network filter I want to record the time difference between consecutive packets this last part is the part that doesn't work. My problem is that I cant get accurate sub-second measurements no matter what C timer function I use. I've tried: gettimeofday() clock_gettime() and clock(). I'm looking for assistance to figure out why my timing code isn't working properly. I'm running on a cygwin environment. Compile Options: gcc -Wall capture.c -o capture -lwpcap -lrt Code snippet : /*globals*/ int first_time = 0; struct timespec start end; double sec_diff = 0; main() { pcap_t *adhandle; const struct pcap_pkthdr header; const u_char *packet; int sockfd = socket(PF_INET SOCK_STREAM 0); .... (previous I create socket/connect - works fine) save_attr = tty_set_raw(); while (1) { packet = pcap_next(adhandle &header); // Receive a packet? Process it if (packet != NULL) { got_packet(&header packet adhandle); } if (linux_kbhit()) { // User types message to channel kb_char = linux_getch(); // Get user-supplied character if (kb_char == 0x03) // Stop loop (exit channel) if user hits Ctrl+C break; } } tty_restore(save_attr); close(sockfd); pcap_close(adhandle); printf(""\nCapture complete.\n""); } In got_packet: got_packet(const struct pcap_pkthdr *header const u_char *packet pcap_t * p){ ... { ....do some packet filtering to only handle my packets set match = 1 if (match == 1) { if (first_time == 0) { clock_gettime( CLOCK_MONOTONIC &start ); first_time++; } else { clock_gettime( CLOCK_MONOTONIC &end ); sec_diff = (end.tv_sec - start.tv_sec) + ((end.tv_nsec - start.tv_nsec)/1000000000.0); // Packet difference in seconds printf(""sec_diff: %ld\tstart_nsec: %ld\tend_nsec: %ld\n"" (end.tv_sec - start.tv_sec) start.tv_nsec end.tv_nsec); printf(""sec_diffcalc: %ld\tstart_sec: %ld\tend_sec: %ld\n"" sec_diff start.tv_sec end.tv_sec); start = end; // Set the current to the start for next match } } } I record all packets with Wireshark to compare so I expect the difference in my timer to be the same as Wireshark's however that is never the case. My output for tv_sec will be correct however tv_nsec is not even close. Say there is a 0.5 second difference in wireshark my timer will say there is a 1.999989728 second difference. I don't think that it is the clocks that are your problem but the way that you are waiting on new data. You should use a polling function to see when you have new data from either the socket or from the keyboard. This will allow your program to sleep when there is no new data for it to process. This is likely to make the operating system be nicer to your program when it does have data to process and schedule it quicker. This also allows you to quit the program without having to wait for the next packet to come in. Alternately you could attempt to run your program at really high or real time priority. You should consider getting the current time at the first instance after you get a packet if the filtering can take very long. You may also want to consider multiple threads for this program if you are trying to capture data on a fast and busy network. Especially if you have more than one processor but since you are doing some pritnfs which may block. I noticed you had a function to set a tty to raw mode which I assume is the standard output tty. If you are actually using a serial terminal that could slow things down a lot but standard out to a xterm can also be slow. You may want to consider setting stdout to fully buffered rather than line buffered. This should speed up the output. (man setvbuf) polling functions like `poll` `select and `epoll` put your program to sleep while it waits for new input. Linux (linux_kbhit gave away what OS you were using) will sometimes punish programs for hogging the CPU by knocking down its dynamic priority and/or boosting the dynamic priority of processes that do a lot of sleeping (which happens when you are waiting for IO or call `pause()` or `nanosleep()`). Correct me if I'm wrong but I thought my main while statement is doing the polling you describe I see if a packet arrives or if the user inputs the quit character then act on it. How could I do this with less resource usage?  Basically you will want to use a timer with a higher resolution Also I did not check in libpcap but I am pretty sure that libpcap can give you the time at which each packet was received. In which case it will be closest that you can get to what Wireshark displays. Turns out you are right - libpcap does handle it. In my got_packet function I pass const struct pcap_pkthdr *header which contains sec and usec resolution when the packet was received. I call it with: header->ts.tv_secheader->ts.tv_usec and it gives me the best timing yet."
116,A,Are multiple ASIO io_services a good thing? I've begun using Boost.ASIO for some simple network programming my understanding of the library is not a great deal so please bear with me and my newbie question. At the moment in my project I only have 1 io_service object. Which use for all the async I/O operations etc. My understanding is that one can create multiple threads and pass the run method of an io_service instance to the thread to provide more threads to the io_service. My question: Is it good design to have multiple io_service objects? say for example have 2 distinct io_service instances each with 2 threads associated do they somehow know about each other (and hence cooperate with each) or if not would they negatively affect each other? My intention is to have 1 io_service for socket based I/O and another for serial based (tty) I/O. I think its better to have 1 single io_service per application and just add as many threads as you like to service the operations. IIRC during Michael Caisse's Boostcon ASIO talk (which is worth watching anyway) I believe this question is explicitly asked by an audience member and ok'd as a potential solution. I take from that that it's not wrong per se and can be used that way according to your design. @Default: I see the same problem as well is it available on youtube? Hrm. Well it's the same result from the link in the Boost Dev mailing list on Sep 28 2010 which I watched back then. Dunno why it's broken :( The link says it's not ready to be watched.. Do everyone get that error message? Some slides from ppt http://dl.dropbox.com/u/10282384/asio_presentation_with_story.pdf http://stackoverflow.com/a/6795367/333137  We use multiple io_service's because some of the components in our application need to run all their worker threads at certain fixed priorities different for each component. Thus each component is given its own io_service and each component has its own pool of threads executing run(). Other designs I could think of would be if a different number of threads in the pool is required for each IO or more relevant to your case is if the pool cannot be shared because for example if your network IO can take out every thread and leave your serial IO waiting.  This discussion may be enlightening: http://thread.gmane.org/gmane.comp.lib.boost.asio.user/1300 I don't have the code right here but why would you use multiple io_services? I thought it used one io_service and multiple threads executing run on that one io_service. IIUC each io_service owns a select/epoll/whatever queue so having multiple io_services is akin to having multiple independent select/epoll loops. In some situations eg. large numbers of sockets and multiple CPUs this might help. Something I'm less sure about is with multiple threads all running io_service::run (with the same io_service). I think this just means the handlers are run concurrently while the select/epoll/etc. loop is 'shared'. I think this is best for when your handlers are relatively long-running operations.
117,A,"jar file from j2me won't instal on my phone. Any idea why? Today I tried to install my first j2me file (jar file) on my phone but I am getting the error below. Can any of you kindly explain me why and how I can fix this network error or what else I need to instal on my mobile. Basically it is a demo sms application I got it from netbeans. I wanted to install on my mobile to try it as I need to develop a similar thing for my coursework. Thank you guys. ERROR MESSAGE: ""application cannot be installed. requires the use of a particular network resource to listen for network information. This network resource it's not supported on this device."" I found the answer maybe someone else need it. the problem is that the network provider on the phone restricts this type of communication we needed to have the url of their port or number of the provider ""stations"" to be able to install and run on my mobile. As I don't have that I can just build my application and run from netbeans to demonstrate how it works.  Hey Hi check this link this may be help you.How to Send Text SMS using J2ME"
118,A,"Diference between dot notation and string based IP When setting the IP for where the client should connectdo I need to make distinguish between and IP like 208.56.123.1 and one like ""www.domain.com"" ? The client must use DNS to resolve www.domain.com into a IP. that means using gethostbyname ? IP is only the number. So 208.56.123.1 is IP but www.domain.com is not - it's a symbolic name. Both are addresses but only the number is IP. Symbolic name will usually resolve to an actual IP using DNS server. As to your question will both work the same the answer is - it only depends on how your client is implemented. Both addresses should refer to the same location though it might be better to target the symbolic name because IP is more likely to change.  Depends on the use case. Most programming languages / network libraries come with built-in support for name resolution. Low-level system calls like bind() or connect() do not support name resolution and require you to get the IP."
119,A,"network programming in .NET I'm looking for some good resources about network programming in .NET (c# preferably) I managed to find out few e-books but they are all the books back in 2002-2003. If you know any e-book video websites ... or any other resources will be thanks full to comment them for me. ""Network Programming in .Net"" is the best book I've read on the subject and most of it is still quite relevant. The things that it would leave out are Sockets (in v2.0 although it does cover the concept) and the WCF stack both of which can be covered by other material. What an ironic answer. :) This is the book I found it few days ago and i red some parts tooIts a good book man but since it works with pre versions of .NET do you have any comment that which parts may have changed in newer versions I know its a heavy book and u don't remember it all but i just need a clue. There isn't much changed from v1.1 (which the book covers) to v4.0 as far as the network stack is concerned. Of course in v2.0 Socket support was added and in v3.0 WCF joined the fray but these subjects would likely just be supplemental chapters in that book and readings on them can be found elsewhere. System.Network.Dns also got a makeover in v2.0 from what I remember and this is well documented in MSDN. I'm reading Network Programming in .Net currently and I would not recommend it because the example codes in the book are poorly written and I'm considering to leave it because I'm so annoyed with the author's negligence.  The best book I've read is C# Network Programming. In my opinion the best way to learn network programming is to pick one of the many examples off codeproject (like tcp chat) and review the code. Many of the core concepts like TCP UDP Berkley Sockets etc have been around way before C# came about and cross the boundries of programmer and admin (which just happens to be one of my favorite things to do).  You don't say what level of abstraction you're looking for. If you're interested in higher-level network development look at the WCF Beginner's Center. My main focus is on remote application programming. Here are few of the tasks I'm usually work onIt may help to give you an overview: 1-secure file transferring (both over LAN & WAN) 2-Remote access (both to a specific software on remote machine or entire OS); Yes look at WCF.  If you need information on bare-bones TCP communication using the Socket class I have a FAQ on the subject geared towards people not familiar with TCP/IP. You may want to start off with a higher-level technology though such as WCF.  http://www.devarticles.com/c/a/C-Sharp/Socket-Programming-in-C-Part-I/1/ is a resource I've used. :)"
120,A,how can we tell if the remote server is multi-threaded? My customer did not gave me details regarding the nature of it's application. It might be multithreaded it might be not. His server serves SOAP messages (http requests) Is there any special trick in order to understand if the peer is single or multi threaded? I don't want to ask the customer and I don't have access to his server/machine. I want to find it myself. Why does it matter for you ? I do care about the order of the requests and the performance of the feature. Also it may use some kind of connection pool.. @cateof being multithreaded does not imply high(or low) performance. In many cases the oposite might be true. I suggest you simply ask the customer. It might be multithreaded it might be not. It might change in the future. Even if it's multi-threaded and sports high-performance it may not be thread-safe. ... or it could be single threaded and still be able to handle multiple requests in parallel. TAO CORBA ORBs is able run in a single thread and at the same time reuse the thread for different requests when it would otherwise be blocked in a synchronous request... which means that when developing for TAO with that configuration you have to write thread safe code even if there is a single thread... Unfortunately no at least not without accessing the computer directly. Multiple connections can even be managed by a single thread however the good news is that this is highly unlikely. Most servers use thread pooling and assign a thread to a connection upon a handshake. Is there a particular reason why you need to know? If you're presumably going to work on this server you'll know first-hand how it works. If his implementation is single threaded I will also implement a single thread. To what end? You're writing a program to talk to a server? You'd only need multiple threads if you intended to use multiple connections at once though more often than not that's not necessary. Only the server has to worry about multiple connections not the client.  It's irrelevant. Why do you feel it matters to you? A more useful question would be: Can the server accept multiple simultaneous sessions? The answer is likely to be 'yes of course' but it's certainly possible to implement a server that's incapable of supporting multiple sessions. Just because a server supports multiple sessions it doesn't mean that it's multi-threaded. And just because it's multi-threaded doesn't mean it will have good performance. When servers need to support many hundreds or thousands of sessions multi-threading may be a very poor choice for performance. Are you asking this question because you want to 'overlap' SOAP messages on the same connection - in other words have three threads send requests and then all three wait for a response? That won't work because (like HTTP) request and response messages are paired together on each connection. You would need to open three connections in order to have three overlapped messages. Thanks for your analysis. The question then is how can we find if the server can accept multiple simultaneous sessions? Just try and open two connections at once. If the second one fails then you'll know. And you can do this test in one thread.  It doesn't matter if the server is multithreaded or not. There are good and efficient ways to implement I/O multiplexing without threads [like select(2) and suchlike] if that's what worries you.
121,A,"How to communicate within this system? We intend to design a system with three ""tiers"". HQ with a single server lots of ""nodes"" on a regional basis users with iPads. HQ communicates 2-way with the nodes which communciate 2-way with the users. Users never communicate with HQ nor vice-versa. The powers that be decree a Windows app from HQ (using Delphi) and a native desktop app for the users' iPads. They have no opinion on the nodes. If there are compelling technical arguments I might be able to beat them down from ""decree"" to ""prefer"" on the Windows program (and for isntance make it browser based). The nodes have no GUI they just sit there playing middle-man. What's the best way for these things to communicate (SOAP/HTTP/AJAX/jQuery/home-brewed-protocol-on-top-of-TCP/something-else?) Is it best to use the same protocol end to end or different protocols for hq<-->node and node<-->iPad? Both ends of each of those two interfaces might wish to initiate a transaction (which I can easily do if I roll my own protocol) so should I use push/pull/long-poll or what? I hope that this description makes sense. Please ask questions if it does not. Thanks. Update: File size is typcially below 1MB with nothing likely to be above 10MB or even 5MB. No second file will be sent before a first file is acknowledged. Files flow ""downhill"" from HQ to node to iPad. Files will never flow ""uphill"" but there will be some small packets of data (in addition to acks) which are initiated by user action on the iPad. These will go to the local node and then to the HQ. We are probably talking <128 bytes. I suppose there will also be general control & maintenance traffic at a low rate in all directions. For push / pull (publish / subscribe or peer to peer communication) cross-platform message brokers could be used. I am not sure if there are (iOS) client libraries for Microsoft Message Queue (MSMQ) but I would also evaluate open source solutions like HornetQ Apache ActiveMQ Apollo OpenMQ Apache QPid or RabbitMQ. All these solutions provide a reliable foundation for distributed messaging like failover clustering persistence with high performance and many clients attached. On this infrastructure message with any content type (JSON binary plain text) can be exchanged and on top messages can contain routing and priority information. They also support transacted messaging. There are Delphi and Free Pascal client libraries available for many enterprise quality open source messaging products. (I am am the author of some of them supporting ActiveMQ Apollo HornetQ OpenMQ and RabbitMQ) +1 (again) Thanks. So much info to digest @Mawg: from the message brokers which are supported by Habari JMS Clients I would recommend to take a look at Apache ActiveMQ. Commercial support available through Fusesource http://fusesource.com/ Fuse message broker is based on ActiveMQ. Two sample chapters (PDF) of the book ""ActiveMQ in Action"" are at http://www.manning.com/snyder/ a slideshow presentation is at http://www.slideshare.net/bruce.snyder/messaging-with-activemq-presentation. Second would be OpenMQ a lean and solid product. The Habari client libraries are available from http://www.habarisoft.com/habari.html I think you should provide URLs for them :-) +1 & thanks. YOu have listed many option but if you *had* to choose then which would you choose? I think you should have mentioned that you are the author of some of those client libraries.  Check out MessagePack: http://msgpack.org/ Also here's more RPC discussion on SO: RPC frameworks available? MessagePack: fast cross-platform serializer and RPC - please share experience ICE might be of interest to you: http://zeroc.com/index.html They have an iOS layer: http://zeroc.com/icetouch/index.html +1 Thanks some good info there I will check it all out ...  IMHO there are too little requisites to decide what technology to use. What data are exchanged how often what size? Are there request/response time constraints? etc. etc. Never start selecting a technology before you understand your needs deeply. +1 a fair enough comment. In fct I have updated the question."
122,A,How many clients can servers handle at a time? I suppose the answer would depend a lot on what kind of activities the clients will have but suppose I want to make a client/server architecture which only involves of connecting and disconnecting. That is opening a TCP connection and maintaining it for say three hours. Is there a hard limit (set by the operating system or by the protocols etc) on how many possible connections there can be at once? Also what overheads will this have on the server? if 10000 clients would connect using TCP and maintain that connection (assuming TCP has its own keepalive functionality) for 3 hours will the server have to process anything besides keepalive the connection and disconnection actions? I blogged about this here: http://www.serverframework.com/asynchronousevents/2010/12/one-million-tcp-connections.html On Windows there are some resource limits which might cause you problems but 10000 connections is easy. In fact I've run more than 70000 connections on a pretty low spec VM see here http://www.lenholgate.com/blog/2005/11/windows-tcpip-server-performance.html for details. It will most probably be what you do in YOUR code that limits the number of connections that you can handle the modern operating system will easily handle more than you can in your code.
123,A,"How do I read strings in J2ME? I'm using the MIDP 2.0 (JSR 118) and I just noticed that there is no reader for strings in J2ME. Does anyone know how you are supposed to read Strings from an InputStream or InputStreamReader in a platform independent way (i.e. between two java enabled cell phones of different models)? Well... I know this was a LONG time ago. You need to do exactly what John said and it is VERY simple. It almost took me 5 hours to figure this one out the first time... I still wonder why j2ME didn't include something as essential as the BufferedReader method for sockets it's not like the freakin cellphones will crash with it... and yes I don't give a rat's ass if my app runs 1ms slower than it should. (I'm just going to put the relevant code I assume you know how to form classes and import the required libraries) ServerSocketConnection listener = (ServerSocketConnection)Connector.open(""socket://:1235""); System.out.println(""Waiting for connection...""); StreamConnection server = listener.acceptAndOpen(); InputStream is = server.openInputStream(); //Now comes the fake BufferedReader equivalent part int ch = 0; StringBuffer sb = new StringBuffer(); while ((ch = is.read()) != -1){ sb.append((char)ch); if(sb.charAt(sb.length()-1) == 13 ) { //Carriage return was received or ENTER was pressed break; //Exit loop and print input } } As you can see the is.read() method will lock the thread till new input is received from the user ONE BYTE AT A TIME. This means if you use telnet to test each keystroke will make the loop iterate once hence we simply concatenate char by char in a StringBuffer until char 13 is received. System.out.println(sb.toString()); I hope this helps people trying to do a socket server on j2ME. I already crafted a fully functional multithreaded version of this for blackberry in case anyone needs it. Thanks or the edit gnat. Thumbs up  Which profile are you using? The MID profile in JSR 118 specifies InputStreamReader (not StringReader but that wouldn't help you read from an InputStream anyway). EDIT: To reflect the change to the question :) You use InputStreamReader.read(char[] int int) and when you've read all you want to create a new string from a char array. If you want to read a line at a time as you would from BufferedReader you basically need to implement the functionality of BufferedReader yourself (keeping a buffer of ""read but not consumed"" chars) and keep reading until you hit a line break. Yeah I'm using MIDP 2.0 (JSR 118). Edited my question. I want to know if there is some way to read strings from any given InputStream.  Alternatively have a look at DataInputStream.readUTF(). It does required that the string being read off the InputStream be encoded appropriately (as in by a corresponding DataOutputStream.writeUTF(String)) so it might not be what you're looking for - but it does work across different phones/models etc.  Would you be able to provide an example of this? You use InputStreamReader.read(char[] int int) and when you've read all you want to create a new string from a char array. If you want to read a line at a time as you would from BufferedReader you basically need to implement the functionality of BufferedReader yourself (keeping a buffer of ""read but not consumed"" chars) and keep reading until you hit a line break."
124,A,"http response to GET request - working in FF not Chromium For fun I'm trying to write a very simple server in C. When I send this response to Firefox it prints out the body ""hello world"" but with Chromium it gives me a Error 100 (net::ERR_CONNECTION_CLOSED): Unknown error. This I believe is the relevant code: char *response = ""HTTP/1.0 200 OK\r\nVary: Accept-Encoding Accept-Language\r\nConnection: Close\r\nContent-Type: text/plain\r\nContent-Length:20\r\n\r\nhello world""; if(send(new_fd response strlen(response) 0) == strlen(response)) { printf(""sent\n""); }; close(new_fd); What am I missing? Thanks! Please mark the correct answer as Accepted. @Steven You have to wait 15 minutes Ah I see. Thanks for explaining. Content-Length seems to be 12 not 20. http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.4: When a Content-Length is given in a message where a message-body is allowed its field value MUST exactly match the number of OCTETs in the message-body. HTTP/1.1 user agents MUST notify the user when an invalid length is received and detected Doesn't that mean FF violates specs? (Well you are using HTTP/1.0 so maybe not.) That was very silly of me. I assumed the Content-Length was unimportant as long it was big enough. Thank you. Just checked with HTTP/1.1 in FF it does indeed allow for incorrect message length."
125,A,"Optimum Update frequency for a client server based multiplayer game I am making a multiplayer game in c++ : The clients simply take commands from the users calculate their player's new position and communicate it to the server. The server accepts such position updates from all clients and broadcasts the same about each to every. In such a scenario what parameters should determine the time gap between consecutive updates ( i dont want too many updates hence choking the n/w). I was thinking the max ping among the clients should be one of the contributing parameters. Secondly how do i determine this ping/latency of the clients ? Other threads on this forum suggest using ""raw sockets"" or using the system's ping command and collecting the output from a file .. do they mean using something like system('ping ""client ip add"" > file') or forking and exec'ing a ping command.. First of all your basic concept is flawed. Send the COMMANDS to the server NOT the positions. Otherwise you'll end up with more cheaters than you can think of. Secondly the update rate depends on the type of your game in case of my http://github.com/BonsaiDen/NodeGame-Shooter thing 50-300ms is enough. The bigger the timespan between your updates the more interpolation you'll have to do. Ivo u r right there is a lot of scope for cheating clients if i send the positions but i figured that since i m using UDP to send these update packets if ""position"" updates are lost in transit then the damage to the game wouldn't be as severe as if the 'commans' updates are lost in which case ill also have to keep synchronizing regularly with each client their correct positions ... Are u also suggesting that the update frequency should not be a function of the network performance (for example if the network is heavily loaded / relatively free) ? I you base the frequency on player ping it'd be easy for some one to slow down the whole game just by increasing his ping. Have you done some testing? Is UDP a fundamental requirement? What about non position stuff? Like firing casting clicking UI. Do you want to use UDP for that too? You'll end up with a lot of client side logic and tons of crazy server side validation code this way. everywhere i read  they tell me i gotta use UDP for sending update packets as they are gonna be frequent nad real time (a lost packet is better than a delayed packet right ?). Although u have raised the serious issue about scalability in the type of commands that can be handled by the client .. (the server checks keep getting more complicated). So whats the solution ? sending only the commands from client to server and then also keeping the universe synchronized by sending periodic synchronizing updates ? Sending commands is the easiest way to get things right and getting things right is always better than just getting them to perform well. I don't know the scale of your game but I would suggest that you go with TCP at first. Since it seems that this is your first multiplayer project it's far more important that you grasp the concepts you need to keep the world secure and in sync. Games like WoW will do a mix of TCP and UDP but keep in mind they have 1000 times the experience and 100x the people to program both server side and client side checks(like warden which watch the game process). The enet library does a lot of the networking for you. It calculates latency as well. i dont really wanna use a library for a single feature ..  This answer is going to depend on what kind of a multiplayer game you are talking about. It sounds like you are talking about an mmo-type game. If this is the case then it will make sense to use an 'ephemeral channel' which basically means the client can generate multiple movement packets per second but only the most recent movement packets are sent to the server. If you use a technique like this then you should base your update rate on the rate in which players move in the game. By doing this you can ensure that players don't slip through walls or run past a trigger too quickly. Your second question I would use boost::asio to set up a service that your clients can 'ping' by sending a simple packet then the service would send a message back to the client and you could determine the time it took to get the packet returned. For movement it's common to have a client send its claimed position and for the server to verify it. Sending 'commands' doesn't work well for fine-grained movement activities. This is doubly important when using UDP because you don't want to have to retransmit such data. the suggestion makes a lot of sense .. however in my scenario this wont really work .. In my game the client sends the position of its player the server also does some calculations based on this position and the position of other players and other objects in the scene. If the client sends only some of the position upadtes then the server doesnt always know accurate positions of clients and hence may perform incorect calculations @AnkurVj: Why does the client send the position of the player? Is there some unusual requirement here? Typically the client sends player commands and the server determines things like positions. Otherwise people are going to hack the client or connection to allow themselves to be wherever they like whenever they like. @David Thornley: You are exactly right the players can easily hack their clients or do things like speed hacks with the greatest of ease. @AnkurVj: you might rethink your design the server really does need to be the ultimate authority on player position. @Kyle David absolutely valid point please see the comments to my question btw the asio thing would help me a lot with its ""timer"" feature  nice  If you're going to end up doing raw-packet stuff you might as well roll your own ICMP packet; the structure is trivial (http://en.wikipedia.org/wiki/Ping)."
126,A,"Simple way to simulate a slow network in python Scenario. I have a client with two network connections to a server. One connection uses a mobile phone and the other is using a wlan connection. The way that I have solved this is by having the server listen at two ports. But the mobile connection should be slower than the wlan connection. The data that is sent is usually just a line of text. I have solved the slower connection issue by allowing the mobile connection to send data in a five second interval the wlan connection have an interval of one second. But is there a better way of doing the slowdown? Perhaps by sending smaller packets meaning I need to send more data? Any ideas on a good way to slow down one of the connections? Orjanp A simple client example with only one connection. def client(): import sys time socket port = 11111 host = '127.0.0.1' buf_size = 1024 try: mySocket = socket.socket(socket.AF_INET socket.SOCK_STREAM) mySocket.connect((host port)) except socket.error (value message): if mySocket: mySocket.close() print 'Could not open socket: ' + message sys.exit(1) mySocket.send('Hello server') data = mySocket.recv(buf_size) print data time.sleep(5) mySocket.close() client() A simple server listening to one port. def server(): import sys os socket port = 11111 host = '' backlog = 5 buf_size = 1024 try: listening_socket = socket.socket(socket.AF_INETsocket.SOCK_STREAM) listening_socket.setsockopt(socket.SOL_SOCKETsocket.SO_REUSEADDR1) listening_socket.bind((host port)) listening_socket.listen(backlog) except socket.error (value message): if listening_socket: listening_socket.close() print 'Could not open socket: ' + message sys.exit(1) while True: accepted_socket adress = listening_socket.accept() data = accepted_socket.recv(buf_size) if data: accepted_socket.send('Hello and goodbye.') accepted_socket.close() server() What are you trying to slow down for? Related: http://stackoverflow.com/questions/1094760/network-tools-that-simulate-slow-network-connection This is a difficult issue.. mobile connections mean longer traveling times / more dropped packages etc. It's no real simulation to to just send them at slower intervals. @ Terry: Just to have a connection that it would take a longer time to send data trough. @ Christophe: Yes I have read that question but I wanted to make this as simple as possible and I feel that external software would complicate it to much. @ Shrinkrin: My bad. Should have specified the issue more. I'm not after doing a realistic simulation of a mobile network. Just slow down the speed on the mobile connection. At the risk of not answering the question you asked I would look for software that does this at a lower level. Netlimiter does this for Windows. I think BWMeter and Bandwidth Controller can do it too. pyshaper is a similar tool for Linux. Open source. You might just be able to import it into your Python program. (Another thing to consider is that you might already have a router capable of shaping traffic the way you want. That's a pretty big dependency to add to your software though and it might be more work to configure.) I did consider using software that did this. But the traffic is local on my machine or between a virtual machine om my computer and the computer itself. So the traffic never reaches outside my computer. So I thought the simplest solution was to implement it directly in the client. Btw. Thanks for the pyshaper tip.  Well what makes a network connection ""slower"" than another is due to latency and/or bandwidth. So if you want to have a realistic simulation you need to find the bandwidth of your mobile phone connection as well as its latency and simulate that in your client program. But you seem to imply that you send very little data so bandwidth is probably not really going to affect your connection speed. So you can just simulate latency and that's doing what you do: sleep(latency) between each packet sent. 5 second seems like a lot though. But if you think bandwidth might be relevant it's actually quite simple to simulate: bandwidth is the max number of bytes per second that you can send and latency is the duration it's going to take to get to its destination. How to do it: have a global timestamp ""blockedUntil"" representing the time until your connection becomes free again to send data. Initialise to 0 at the beginning of your program as we assume it's not being used yet. Then everytime you have a packet to send If ""_blockedUntil"" is less than now() set it to now(). Then calculate the time it would take to write to your fictitious ""wire"" by doing: packet.size() / bandwidth that'll get you a time duration add the latency and add that to ""blockedUntil"". Now compute dt = blockedUntil - now() add the packet to the queue and add a timer firing in ""dt"" which will pop the first packet in the queue and send it. There you go you've simulated bandwidth and latency. Edit: as someone mentioned there's the question of dropped packets too. You can simulate that by having a probability of dropping packets. Note: This is solely possible when one is manipulating packets from an unconnected protocol such as Ethernet or UDP. In the case of TCP for example it won't work. You are a 100% right I got carried away last time I did that I was manipulating ethernet frames so it would work fine. I was assuming the same for the OP but it's obviously not the case got carried away. My bad. You can't really cause a packet to be dropped when you're working on the socket layer. You *can* model the **effect** though which is an additional delay (timeout + retransmit) with the same probability as that of a dropped packet. Well if the packet is dropped just don't send it. That's what I meant. TCP doesn't work that way. Connections are ""reliable"" which means that if an IP datagram carrying some TCP payload is dropped the data will be re-sent. From the application level working with TCP sockets you can't tell when this happens nor simulate it. Dropping some bytes that are passed to socket.send to simulate packet loss is an incorrect simulation of packet loss. The simulation does not need to be realistic as long at the result is that it takes longer time to send data over the mobile connection compared to the wlan connection. So dividing the data in multiple packets and sleeping between each packet is a feasible approach. Just need to tell the server when all the data is sent. Seems simple enough.  Aside from using an external tool to simulate the kind of network you're interested in one good approach is to use a substitute implementation of socket. This involves making the socket construction a parameter to your function instead of importing the socket module and using it directly. For normal operation you will pass in the real socket type but when you want to test various adverse network conditions you can pass in an implementation that simulates those conditions. For example you might create a socket type which parameterizes latency and bandwidth (untested code beware): import time socket class ControllableSocket: def __init__(self latency bandwidth): self._latency = latency self._bandwidth = bandwidth self._bytesSent = 0 self._timeCreated = time.time() self._socket = socket.socket() def send(self bytes): now = time.time() connectionDuration = now - self._timeCreated self._bytesSent += len(bytes) # How long should it have taken to send how many bytes we've sent with our # given bandwidth limitation? requiredDuration = self._bytesSent / self._bandwidth time.sleep(max(requiredDuration - connectionDuration self._latency)) return self._socket.send(bytes) If you implement the other socket methods connect recv etc you can substitute an instance of this class for an instance of the real socket type. This leaves all the rest of your program completely free of any knowledge of your simulation simplifying it while also letting you try out many different network configurations by just implementing a new socket type that simulates them. This idea is one of the reasons Twisted explicitly separates the idea of ""protocols"" - objects which know how to interpret bytes from the network and generate new bytes to send to the network - from ""transports"" - objects which know how to get bytes off the network and put bytes onto it. The separation eases testing and allows novel configurations like this one where a simulation of some other network conditions (which may be difficult to produce for real) is provided by the transport. I'll have a look into this approach since I also need a even slower connection than the mobile one. Here I don't have to worry about telling the server that all the data has been sent. Quite an elegant solution. Thanks. :)"
127,A,"How to disable all network connections in Java Is there a way to run Java VM (java.exe) on Windows Server 2008 and disable all network connections using a command line argument or a system variable? The solution should be able to disable network per Java VM process and should be defined from outside the process (not having the process disable its own network). This could have been asked a bit better couldn't it. you should have said that in the first place. You've been a SO user long enough to know the importance of providing relevant details when asking questions. You can do this by enabling default Java security manager. By default no security is enforced so you are allowed to do anything but if security manager is enabled it will restrict network access file access and lots of other things unless you specify otherwise in the security policy file. To enable the default security manager pass this argument to JVM on start. java -Djava.security.manager=default my.main.Class By doing this any network access attempt from inside JVM will throw java.net.NetPermission. This will also break things like file access so if you need to allow it you will need to specify those in a special security policy file (-Djava.security.policy=path/to/policy.file). There should be plenty of examples of how to set it up just search for ""java permissions"" to get you started.  Well I haven't tried that but theoretically you could set the system properties for SOCKS proxy to a non-existent one and according to the documentation all TCP sockets will be tried through the SOCKS proxy - and fail. Something like this: java -DsocksProxyHost=127.0.0.1 SomeClass I could set a dummy proxy and have it fail but I would prefer to be able to tell Java ""you may not use the network""."
128,A,"How do you get user info from a company's domain/network settings in C#? I was told that user info like name address phone etc are stored on the network (obviously only if a person has given that info). This is the info that Outlook gets when searching for users on the network (the info that Outlook populates the Contact card with). This obviously works because I can search for anyone in my company's network and I get results for people all over the world that are on our network. The program I am working on is internal to our company and one of my tasks is to pre-populate a form with that info. My question is how do I get this information? Where is it stored? What object do I use to get it? EDIT: I was a little general on this. What I really need is the current user's info ('current' being whoever is logged on to the computer and using my program). What is the best way to get it? If you're using Active Directory then you can query that information from there given the domain and username. It's basically an LDAP store but there's tonnes of info on the web for how to implement it. Here's another question that specifically asks how to retrieve this information from Active Directory: How to get the current user's Active Directory details in C# In a desktop app to get the current logged-on user's Windows username you can use Environment.UserName. Updated my question. Is this still the same approach I need to take? In a desktop app to get the current logged-on user's Windows username you can use `Environment.UserName`.  To answer your question of ""is there a way to get the current domain controller..."" one way that has been helpful for me is to use the nltest command [1] like this: C:\>nltest /dsgetdc:yourdomain.com It should output all kinds of useful stuff. [1] http://support.microsoft.com/kb/247811  To read from Active Directory the classes you need are in System.DirectoryServices.dll. The important ones are DirectorySearcher and DirectoryEntry. Take a look at the the first 2 answers to this other question to get more code: How to get the current user's Active Directory details in C# Update: To get the current user's info take their logon name and then do a search in Active Directory for a user with the same user ID. Something like this: adSearch.Filter = ""(sAMAccountName="" + Environment.UserName + "")""; (In ASP.Net you would get the user name elsewhere.) In the link you supplied it has '(""LDAP://MyDomainController"")' as the argument for the DirectoryEntry. Is there a way to get the current domain controller or another way to initialize a DirectoryEntry for the current domain using .NET 2.0? I cannot link to System.DirectoryServices.AccountManagement to get the domain controller because I am constrained to using .NET 2.0. Updated my question. Is this still the same approach I need to take?  One additional tip to go along with the answers David and Neil Barnwell provided: You can get their ID using HttpContext.Current.User.Identity.Name. If you want use this from an assembly or some other back end code make sure you add using System.Web."
129,A,"why would I get EADDRINUSE not from bind() but from listen()? In a C++ Linux application I'm calling socket() bind() and listen() to create a server socket. Usually if the application is started twice (with same server port) in the second process bind() will fail with EADDRINUSE error. However now I have a case where bind() has apparently succeeded but the subsequent listen() call has thrown the EADDRINUSE error... This is probably a rare race condition but I'd be still interested in what cases it could happen that the second bind() succeeds but the second listen() does not. Does anyone know more about such a case? This is on 32-bit RHEL 5.3. setsockopt(.... SOL_SOCKET SO_REUSEADDR ...) should fix your problem. See setsockopt(2) and socket(7) (as to why the second bind actually succeeds no idea... actually this should already fail too) Glad we agree. But then `SO_REUSEADDR` doesn't *fix* the problem it hides it. @Ben Voigt: Sure is though I think _not only_ `listen` should fail but already `bind` before you ever get to `listen`. Though Remy Lebeau's theory about _delayed binding_ is probably the reason it makes perfect sense anyway. I some way no in some way yes. Since the problem is that two programs cannot use the same port at the same time there is not really a solution. By giving `SO_REUSEADDR` you are basically telling the operating system ""In case it looks like someone else is still using this port I promise that it's ok... no other program _really_ uses that port any more"". It's somewhat similar to doing a cast in C/C++ where you basically tell the compiler ""look it's ok I promise that I know what I'm doing"". The second instance failing is a ***good*** thing.  Not sure about Linux but on Windows if a wildcard IP (INADDR_ANY etc) is specified when calling bind() the underlying binding may be delayed until listen() or connect() is called as the OS has a better chance of deciding at that time which network interface is best to use. bind() will not report an error in that situation. Thanks that seems to be the explanation. Apparently the rules are: listen() will actually reserve the port in the kernel and will raise EADDRINUSE if another process has called listen() on the port; bind() does _not_ reserve the port but will raise EADDRINUSE if another process has called listen() on the port"
130,A,"How to retrieve foreign host's MAC address in C++ Currently we're parsing arp request output from the command line. string cmd = ""arp -n ""; cmd.append(ipaddress); cmd.append("" | grep ""); cmd.append(ipaddress); fgets( line 130 fp); fgets( line 130 fp); ret.append(line); ... It works but is there a way to do this using a library function that wont depend so much on the native command line interface? The project is using libpcap currently. I am a little worried about the retrieve foreign host’s MAC address part of your question. I hope that you are aware that you only get entries in the ARP table for machines on the same network. If you connect to a machine via a router then you will only see the routers MAC address in the ARP table. So there is no way of knowing the foreign host’s MAC address unless it's a host on the same network (no routers involved). So the answer is that it is impossible for hosts that are reached via a router and in case the host is reachable on the local network you can use only platform specific access methods as even the output of the arp command may differ between platforms.  In general this will depend on your OS. There's no real standard API for this. Assuming you're on linux open and parse /proc/net/arp. The format is similar to that of the output from the arp command. Note that you must send a packet to the IP in question at least once recently in order to have it in the ARP table and of course you won't get anything outside of your local segment. Getting an IP into this cache is easy enough - just send a UDP packet to it on some unused port for example - then poll until it shows up. Another alternative would be to use a raw socket to craft and send your own arp packet but that's much more complex :) If you do want to go down this route study the source code for arping (page is down today but the git repo is online) which might also give you a bit more portability by using platform-independent-ish libraries like libpcap (but requires root) But that is the arp cache so it would only work if we had send a request to that IP before. However that does provide a more platform independent way of reading it in after submitting the request. It's not more platform independent. There is no platform independent way. And if you want to send a request to that IP it's easy enough - just throw some random UDP packet at it. I'm sorry its more distro independent. I misspoke. I'm going to give this a look as we already need root for this project anyway. Thanks!"
131,A,"Application control of TCP retransmission on Linux For the impatient: How to change the value of /proc/sys/net/ipv4/tcp_retries2 for a single connection in Linux using setsockopt() ioctl() or such or is it possible? Longer decription: I'm developing an application that uses long polling HTTP requests. On the server side it needs to be known when the client has closed the connection. The accuracy is not critical but it certainly cannot be 15 minutes. Closer to a minute would do fine. For those not familiar with the concept a long polling HTTP request works like this: The client sends a request The server responds with HTTP headers but leaves the response open. Chunked transfer encoding is used allowing the server to sends bits of data as they become available. When all the data is sent the server sends a ""closing chunk"" to signal that the response is complete. In my application the server sends ""heartbeats"" to the client every now an then (30 seconds by default). A heartbeat is just a newline character that is sent as a response chunk. This is meant to keep the line busy so that we notify the connection loss. There's no problem when the client shuts down correctly. But when it's shut down with force (the client machine loses power for example) a TCP reset is not sent. In this case the server sends a heartbeat which the client doesn't ACK. After this the server keeps retransmitting the packet for roughly 15 minutes after giving up and reporting the failure to the application layer (our HTTP server). And 15 minutes is too long a wait in my case. I can control the retransmission time by writing to the following files in /proc/sys/net/ipv4/: tcp_retries1 - INTEGER This value influences the time after which TCP decides that something is wrong due to unacknowledged RTO retransmissions and reports this suspicion to the network layer. See tcp_retries2 for more details. RFC 1122 recommends at least 3 retransmissions which is the default. tcp_retries2 - INTEGER This value influences the timeout of an alive TCP connection when RTO retransmissions remain unacknowledged. Given a value of N a hypothetical TCP connection following exponential backoff with an initial RTO of TCP_RTO_MIN would retransmit N times before killing the connection at the (N+1)th RTO. The default value of 15 yields a hypothetical timeout of 924.6 seconds and is a lower bound for the effective timeout. TCP will effectively time out at the first RTO which exceeds the hypothetical timeout. RFC 1122 recommends at least 100 seconds for the timeout which corresponds to a value of at least 8. The default value of tcp_retries2 is indeed 8 and my experience of 15 minutes (900 seconds) of retransmission is in line with the kernel documentation quoted above. If I change the value of tcp_retries2 to 5 for example the connection dies much more quicker. But setting it like this affects all the connections in the system and I'd really like to set it for this one long polling connection only. A quote from RFC 1122: 4.2.3.5 TCP Connection Failures Excessive retransmission of the same segment by TCP indicates some failure of the remote host or the Internet path. This failure may be of short or long duration. The following procedure MUST be used to handle excessive retransmissions of data segments [IP:11]: (a) There are two thresholds R1 and R2 measuring the amount of retransmission that has occurred for the same segment. R1 and R2 might be measured in time units or as a count of retransmissions. (b) When the number of transmissions of the same segment reaches or exceeds threshold R1 pass negative advice (see Section 3.3.1.4) to the IP layer to trigger dead-gateway diagnosis. (c) When the number of transmissions of the same segment reaches a threshold R2 greater than R1 close the connection. (d) An application MUST be able to set the value for R2 for a particular connection. For example an interactive application might set R2 to ""infinity"" giving the user control over when to disconnect. (e) TCP SHOULD inform the application of the delivery problem (unless such information has been disabled by the application; see Section 4.2.4.1) when R1 is reached and before R2. This will allow a remote login (User Telnet) application program to inform the user for example. It seems to me that tcp_retries1 and tcp_retries2 in Linux correspond to R1 and R2 in the RFC. The RFC clearly states (in item d) that a conforming implementation MUST allow setting the value of R2 but I have found no way to do it using setsockopt() ioctl() or such. Another option would be to get a notification when R1 is exceeded (item e). This is not as good as setting R2 though as I think R1 is hit pretty soon (in a few seconds) and the value of R1 cannot be set per connection or at least the RFC doesn't require it. After some thinking (and googling) I came to the conclusion that you can't change tcp_retries1 and tcp_retries2 value for a single socket unless you apply some sort of patch to the kernel. Is that feasible for you? Otherways you could use TCP_KEEPALIVE socket option whose purpose is to check if a connection is still active (it seems to me that that's exactly what you are trying to achieve so it has sense). Pay attention to the fact that you need to tweak its default parameter a little because the default is to disconnect after about 2 hrs!!!  I suggest that if the TCP_USER_TIMEOUT socket option described by Kimvais is available you use that. On older kernels where that socket option is not present you could repeatedly call the SIOCOUTQ ioctl() to determine the size of the socket send queue - if the send queue doesn't decrease over your timeout period that indicates that no ACKs have been received and you can close the socket.  Looks like this was added in Kernel 2.6.37. Commit diff from kernel Git and Excerpt from change log below; commit dca43c75e7e545694a9dd6288553f55c53e2a3a3 Author: Jerry Chu Date: Fri Aug 27 19:13:28 2010 +0000 tcp: Add TCP_USER_TIMEOUT socket option. This patch provides a ""user timeout"" support as described in RFC793. The socket option is also needed for the the local half of RFC5482 ""TCP User Timeout Option"". TCP_USER_TIMEOUT is a TCP level socket option that takes an unsigned int when > 0 to specify the maximum amount of time in ms that transmitted data may remain unacknowledged before TCP will forcefully close the corresponding connection and return ETIMEDOUT to the application. If 0 is given TCP will continue to use the system default. Increasing the user timeouts allows a TCP connection to survive extended periods without end-to-end connectivity. Decreasing the user timeouts allows applications to ""fail fast"" if so desired. Otherwise it may take upto 20 minutes with the current system defaults in a normal WAN environment. The socket option can be made during any state of a TCP connection but is only effective during the synchronized states of a connection (ESTABLISHED FIN-WAIT-1 FIN-WAIT-2 CLOSE-WAIT CLOSING or LAST-ACK). Moreover when used with the TCP keepalive (SO_KEEPALIVE) option TCP_USER_TIMEOUT will overtake keepalive to determine when to close a connection due to keepalive failure. The option does not change in anyway when TCP retransmits a packet nor when a keepalive probe will be sent. This option like many others will be inherited by an acceptor from its listener. Signed-off-by: H.K. Jerry Chu <hkchu@google.com> Signed-off-by: David S. Miller <davem@davemloft.net>"
132,A,"Wireshark: Parsing packets I am wondering how does Wireshark parse 802.11 packets. For example A probe request packet has a sequence number: 2327. In the packet details in hexadecimal it is ""70 91"" while in ASCII it is ""p."" Then how does wireshark get the value ""2327"" from the packet? Is there a similar example in C? The `.` is simply a placeholder: `0x91` isn't a printable [iso-8859-1](http://en.wikipedia.org/wiki/Iso-8859-1#Codepage_layout) character; it isn't in any of the [iso](http://en.wikipedia.org/wiki/ISO/IEC_8859#Table) tables. It isn't a legal [UTF-8](http://en.wikipedia.org/wiki/UTF-8) character either. The 802.11 Sequence Control field is a 16 bit little-endian field that contains two subfields - the upper 12 bits contain the Sequence Number and the lower 4 bits contain the Fragment Number. In this case: Sequence Control = 0x9170 Sequence Number = 0x917 = 2327 decimal Fragment Number = 0x0 = 0  @caf was rightanyway i post the code on how i extract the Sequence Number.. //this is the subfields typedef struct seqctl_subfields { unsigned fragment:4; unsigned seq_num:12; }; struct seqctl_fields *se = (struct seqctl_fields*)p->sc // where p is a struct of the 802.11 headerp->sc points to the sequence control field of the 802.11 header std::cout << se->seq_num << std::endl;"
133,A,"UNIX nonblocking I/O: O_NONBLOCK vs. FIONBIO In every example and discussion I run across in the context of BSD socket programming it seems that the recommended way to set a file descriptor to nonblocking I/O mode is using the flag to fcntl() e.g. int flags = fcntl(fd F_GETFL 0); fcntl(fd F_SETFL flags | O_NONBLOCK); I've been doing network programming in UNIX for over ten years and have always used the FIONBIO ioctl() call to do this: int opt = 1; ioctl(fd FIONBIO &opt); Never really gave much thought to why. Just learned it that way. Does anyone have any commentary on the possible respective merits of one or the other? I imagine the portability locus differs somewhat but do not know to what extent as ioctl_list(2) doesn't speak to that aspect of individual ioctl methods. I believe fnctl() is a POSIX function. Where as ioctl() is a standard UNIX thing. Here is a list of POSIX io. ioctl() is a very kernel/driver/OS specific thing but i am sure what you use works on most flavors of Unix. some other ioctl() stuff might only work on certain OS or even certain revs of it's kernel. I've used FIONBIO on AIX Solaris Linux *BSD and IRIX with no problems. But yeah I understand that it's not going to work on Windows for example--it's a low-level interface to a very specific kernel implementation. Still I wonder if there are any other differentiating factors.  Prior to standardization there was ioctl(...FIONBIO...) and fcntl(...O_NDELAY...) but these behaved inconsistently between systems and even within the same system. For example it was common for FIONBIO to work on sockets and O_NDELAY to work on ttys with a lot of inconsistency for things like pipes fifos and devices. And if you didn't know what kind of file descriptor you had you'd have to set both to be sure. But in addition a non-blocking read with no data available was also indicated inconsistently; depending on the OS and the type of file descriptor the read may return 0 or -1 with errno EAGAIN or -1 with errno EWOULDBLOCK. Even today setting FIONBIO or O_NDELAY on Solaris causes a read with no data to return 0 on a tty or pipe or -1 with errno EAGAIN on a socket. However 0 is ambiguous since it is also returned for EOF. POSIX addressed this with the introduction of O_NONBLOCK which has standardized behavior across different systems and file descriptor types. Because existing systems usually want to avoid any changes to behavior which might break backward compatibility POSIX defined a new flag rather than mandating specific behavior for one of the others. Some systems like Linux treat all 3 the same and also define EAGAIN and EWOULDBLOCK to the same value but systems wishing to maintain some other legacy behavior for backward compatibility can do so when the older mechanisms are used. New programs should use fcntl(...O_NONBLOCK...) as standardized by POSIX. I tend to use ioctl() for this because it costs me only one syscall to enable non-blocking mode rather than two for fcntl(). In addition the Windows ioctlsocket() API is equivalent to ioctl() for the purposes of this functionality. nginx does this if it can and marks it with a comment ""ioctl(FIONBIO) sets a non-blocking mode with the single syscall."" Now there is accept2 which lets you accept a connection and put it into non-blocking mode in the same syscall.  As @Sean said fcntl() is largely standardized and therefore available across platforms. The ioctl() function predates fcntl() in Unix but is not standardized at all. That the ioctl() worked for you across all the platforms of relevance to you is fortunate but not guaranteed. In particular the names used for the second argument are arcane and not reliable across platforms. Indeed they are often unique to the particular device driver that the file descriptor references. (The ioctl() calls used for a bit-mapped graphics device running on an ICL Perq running PNX (Perq Unix) of twenty years ago never translated to anything else anywhere else for example.)"
134,A,"Network byte order conversion with ""char"" I've always been taught that if an integer is larger than a char you must solve the byte ordering problem. Usually I'll just wrap it in the hton[l|s] and convert it back with ntoh[l|s]. But I'm confused why this doesn't apply to single byte characters. I'm sick of wondering why this is and would love for a seasoned networks programmer to help me shed some light on why byte orderings only apply for multibyte integers. Thanks! Ref: http://beej.us/guide/bgnet/output/html/multipage/htonsman.html Appreciate the answers had one of those ""do'h"" moments! What you are looking for is endianness. A big-endian architecture stores the bytes of a multibyte data type like so: while a little-endian architecture stores them in reverse: When data is transferred from one machine to another the bytes of a single data type must be reordered to correspond with the endianness of the destination machine. But when a data type only consists of one byte there is nothing to reorder. +1 but you might want to reconsider stealing the bandwidth of another site. @Bill Wikipedia's policy on hotlinking is at http://commons.wikimedia.org/wiki/Commons:Reusing_content_outside_Wikimedia#Hotlinking -- but wikimedia.org is blocked here so I can't read it myself. Would you mind paraphrasing it? Thanks! I learned something new today thanks! Hotlinking paraphrased: ""Go to it but the images might change. Caveat scriptor."" Bummer about the blocking.  In addition to all the other ways folks have put it: Endianness is about the order of bytes in an integer not about the order of bits in a byte. The order of bits in a byte is the same even across big-endian and little-endian machines. The only difference is the order in which the bytes themselves are used to store an integer (or short or what-have-you). And so because there's only one byte in a char there's no difference in how that value is stored either in a big-endian or little-endian architecture.  You need to consider what each function does. From that you need to apply that knowledge to the size of the type you intend to modify. Consider the following: #include <stdio.h> #include <netinet/in.h> int main () { uint16_t i = 42; uint8_t c = 42; // a char printf (""(uint16_t ) %08X (%d)\n"" i i); printf (""( htons ) %08X (%d)\n"" htons(i) htons(i)); printf (""( uint8_t ) %08X (%c)\n"" c c); printf (""( htons ) %08X (%c)\n"" htons(c) htons(c)); return 0; } (uint16_t ) 0000002A (42) ( htons ) 00002A00 (10752) ( uint8_t ) 0000002A (*) ( htons ) 00002A00 () Thanks for the sample code ezpz!  Exactly how many ways can you order the bytes in a single char? -1 for snark. You know the answer to a question the OP didn't. Good for you. Don't be a jerk about it. Lighten up - the OP obviously knows about byte ordering and hton etc - why you don't need this for a byte is just one of those d'oh moments. +1 for snark. To spell it out: by definition in C and C++ a char is the smallest addressable unit of storage.  You don't read individual bits off the wire just bytes. Regardless of the endianness a single byte is the same backwards and forwards just like the word ""I"" is the same backwards and forwards. Many ethernet systems do encode individual bits - see http://en.wikipedia.org/wiki/MLT-3 though the faster ones are nibbles.  ezpz code is incorrect.. uint16_t i = 65534; printf (""(uint16_t ) %08X (%d)\n"" i i); Returns... (uint16_t ) 0000FFFE (-2) You should use a unsigned int instead so it's not interpreted as a signed int uint16_t i = 65534; printf (""(uint16_t ) %08X (%u)\n"" i i); Returns... (uint16_t ) 0000FFFE (65534)  Your networking stack will handle the bits inside the bytes correctly you must only concern yourself with getting the bytes in the right order."
135,A,How to Automatically start Application I work on C#.recently i work on Tcp server-client .I write a client application .want it's start automatically when client start os .Actually i have an exewant it active when user start his computer.What i need to do?Thanks.if have any query plz ask. Making your server a windows service is a better option. This way even if the no one is logged on to the computer your program will start and run. Generally services are a better choice for server applications requiring to run on OS startup. You can read about how to create a service in C# in the following article  There are many ways that you can make an application start at run time. For a list of locations. Check this article To summarize they are Start->Programs->StartUp folder HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\Run HKEY_CURRENT_USER\Software\Microsoft\Windows\CurrentVersion\Run HKEY_LOCAL_MACHINE\Software\Microsoft\Windows\CurrentVersion\Policies\Explorer\Run  The Window Autostart Folder can be very useful. I normally put my applications there.  Basically there are two options: Create a shortcut to your program in the Startup folder of your start menu Create an entry in the registry in the Run key will you plz tell me how to add on registry. I think you can use google yourself can't you?
136,A,"Arithmetic with IPv6 addresses (large integers) I'm working with IPv6 addresses in the form: FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF:FFFF Internally I store them in an array: TIp6Bytes = array [0..15] of Byte; I need to manipulate the IPv6 addresses in a number of ways including adding dividing multiplying etc. Can anyone suggest a good way to do this? I guess I should have mentioned that I'm working with Delphi 2009 Why wouldn't you store them internally as a 2D array? 8x2 bytes just like an IPv4 address would be manipulated as 4x1 bytes. I'm using the same type that is used in Synapse so I can use several of the functions it already provides. Not sure why they don't store it as 8x2 but that's the way it is :) After trying many of the suggestions I could not find a library that fulfilled all my needs and were bug free. I searched a little harder and found a relatively new library by Alex Ciobanu which does BigIntegers (and Big Cardinals) seamlessly allowing you to manipulate them in much the same way as you manipulate normal Integers Cardinals etc. As well as BigIntegers the library also provides a number of very useful features. From the readme: A set of generic collections classes (List Dictionary HashSet etc). Date/Time functionality all combined in a few structures (somehow equivalent to .NET's DateTime structure) Type Support concept that defines a set of default ""support classes"" for each built-in Delphi types (used as defaults in collections). Custom ""type support"" classes can be registered for your custom data types. BigCardinal and BigInteger data types. Smart pointers in Delphi The library is being actively developed. In fact the author fixed a small bug I found within a day. You can read more about the library on Alex's blog and download DeHL from Google code.  I would say that if you can add you can then use it to subtract multiply and divide using addition. Should I assume overflows will be simply ignored? I seem to recall a method of adding bit-oriented variables using XOR. I am looking for that answer now. Hopefully this will point you in the right direction. If I can find that XOR code I will post it for you. Here it is: Bitwise operation Exclusive disjunction is often used for bitwise operations. Examples: 1 xor 1 = 0 1 xor 0 = 1 1110 xor 1001 = 0111 (this is equivalent to addition without carry) And the reference is: http://www.absoluteastronomy.com/topics/Exclusive_disjunction  Jes Klinke wrote a bignum unit for Pascal here. Disclaimer : I have not used this library personally. Wow... handles Int128 Int256 ...all the way up to Int3k plus a few just in case. :) Nice find it looks like it would solve the problem well although I have not dug too deep into the pieces yet. (+1 wish I could give more) I tried using this library but it has a major bug in the subtraction function. I tried to contact the author but have not received a reply. See my own answer to this question for another option.  I once wrote a IPv4 and IPv6 conversion unit including a custom variant type for both types of IP addresses. For instance with these Variant types the following example arithmetics and conversions are possible: procedure TForm1.Log(const S: String); begin Memo.Lines.Add(S); end; procedure TForm1.Button1Click(Sender: TObject); var I4: TIPv4; I6: TIPv6; V1 V2 V3 V4: Variant; begin I4 := StrToIPv4('192.0.2.128'); I6 := IPv4ToIPv6(I4); V1 := VarIPv6Create('2001:db8:85a3:0:0:8a2e:0370:7334'); V2 := IPv6ToVar(I6); V3 := V1 - V2; V4 := V1 or V2; if V3 < V4 then Log(V3 + ' is smaller than ' + V4); if V2.Equals('::ffff:192.0.2.128') or V2.IsZero then Log('OK'); Log('V1 = ' + V1.AsStringOutwritten); Log('V2 = ' + V2.AsURL); Log('V3 = ' + V3.AsStringCompressed); V4.Follow; end; Custom variant types really are quite powerfull."
137,A,"is there any C library available that contains APIs for ifconfig? I want to set the MTU in or to get some device details but I don't want to use system(""ifconfig ...""). Is there any way to do it in C? Some like libifconfig.so? The source code of MTU can be found here. But I really don't too much time right now. How can I get the name of ALL the available interfaces (I have plenty in my system) On my debian box I can sudo apt-get install iproute{-dev-doc} sudo apt-get build-dep iproute apt-get source iproute --compile That will give me the source to ip (the new ifconfig) the documentation and the development header for building iproute extensions. Oh and the source will have been compiled :) Access the documentation via /usr/share/doc/iproute-do (the examples/ looks interesting).  It can be done with ioctl. See netdevice(7). I'm guessing SIOCGIFMTU and SIOCSIFMTU are what you're looking for. I don't know if it's standard update your question if you need to support other Unixes. EDIT To get the interfaces on your machine (untested): int sock; struct ifconf *conf; sock = socket(AF_INET SOCK_STREAM 0); ioctl(sock SIOCGIFCONF  (char*) conf); Don't be sloppy and don't forget to check the return values. I don't check them here because I don't care; you probably do. I updated my question @cateof As I have updated my answer. getifaddrs on Linux also appropriate for understanding the MTU on a NIC.  Of course there are ways to do it in C. Ask yourself what ifconfig is written in? Since Linux is open source you can download the source code for ifconfig and look at the APIs that it uses. That is the easiest method in my opinion. If I feel like looking at the code myself I may later edit this to include the APIs. Sensible but more of a comment really. I answered the question that was asked in the title. If that was not the question then it needed a better title.  if_nameindex is the function to get a list of all interfaces. Actually doing much with them relies on system-specific ioctl calls. Honestly your best bet rather than wasting your time obtaining and reading 100x over-complicated legacy sources of ifconfig etc. is to just run strace ifconfig ... and look at what ioctl syscalls it's making. Actually this is a great general approach to replicating behavior from programs which have overly complicated source logic or for which you don't have the source. that's clever. strace is always a nice tip for low level dev..."
138,A,"How can i get IP Address of my 3G modem? My GPRS modem has got a sim card. it can connect Web. Web service give it a ip number. i need it. like that: http://www.your-ip-address.com/ How can i do that C#? Maybe you shouldn't rephrase your whole question after you have gotten an answer... it is very confusing. Agreed; I have rolled back the question so that the answers make sense. You can get a list of your IP addresses via DNS using the following code: var name = Dns.GetHostName(); var entry = Dns.GetHostEntry(name); foreach (var address in entry.AddressList) { Console.WriteLine(address); } If you want the IP address as a property of the hardware you can use the System.Management.ManagementClass with the name Win32_NetworkAdapterConfiguration. See http://msdn.microsoft.com/en-us/library/system.management.managementclass.aspx for details. That depends. When I am using my dial-up it uses PPP and my connection is assigned a public IP-adress. So the code above returns the same IP address as your-ip-address.com. This code will return only *local* IP addresses or am I mistaken? In many cases that would not be the same as the *remote* address reported by `your-ip-address.com`  You can create a WebRequest to http://whatismyip.com/automation/n09230945.asp which houses only your IP address Start here i want to make it with C#? @ykaratoprak: so go ahead. What's preventing you from ""make it with C#""? i need static ip : http://www.your-ip-address.com/ this site give me what i desire ip. i need how to do that C# code http://whatismyip.com/automation/n09230945.asp gives you your IP just the same as your-ip-address.com and the page I quote explains how to create a web request with timeouts etc. as per your requirements  You can use the static method WebClient.DownloadString(url) to read your external IP address from any web service providing such data: string ip = System.Net.WebClient.DownloadString(""http://whatismyip.org/""); If you are going to use this in a production environment better make sure that the URL you are pointing to is guaranteed to stay around for the entire lifespan of your application. The best way is probably to host the web service yourself. Also you should add some error checking around this code as it will fail if the internet connection or the web service is unavailable."
139,A,"Where is the Don't Fragment Bit of the IP Flags used? I am curious to know where the ""Don't Fragment"" [DF] Bit of the IP Flags is used. As fragmentation is invisible to higher layers and they don't care too. I am also looking for an example. Thanks a lot in advance. Do note that there is no standard way to set DF in C. On Linux this code works: result = setsockopt(mysocket IPPROTO_IP IP_MTU_DISCOVER IP_PMTUDISC_DO sizeof(int)); but it does not on FreeBSD 6 Also Path MTU discovery is extremely unreliable on the real Internet. Too many broken firewalls and middleboxes filter out ICMP ""Packet too big"" messages (here is a good way to test a candidate network administrator during an interview: ask him/her to stop ping and he/she will probably block completely ICMP.) See RFC 2923: ""TCP Problems with Path MTU Discovery"" That's the reason why the IETF now suggest a new way to test the MTU without relying on Path MTU Discovery: RFC 4821: ""Packetization Layer Path MTU Discovery""  In addition to @Pax's answer (or perhaps as part of the testing he mentioned) the DP flag is also used in path MTU discovery. This is when you try to figure out what the largest packet that can be sent without being fragmented is for a given link. It is often useful to avoid fragmentation even though higher-level protocols are in theory isolated from the mechanics of it they can still ""feel"" the consequences. If a single application-level write() to the network socket ends up being fragmented because it is too large and one of the fragments is lost in the network the entire IP packet will be lost. This of course affects throughput. For this reason it is often desirable to know the maximum transmission unit i.e. the largest packet that can be sent to a destination without being fragmented. Path MTU discovery is used to find this size by simply setting the DF bit and sending successively larger packets until the network reports (over ICMP) a failure. That's very handy to know. +1.  Fragmentation is not always invisible to all upper layers. Some early (and probably even current) micro-controller TCP/IP stacks did not implement the full capabilities such as fragmentation handling. Use of the flag in that situation would ensure that the packet arrived in its original form instead of a lot of fragments which the other end couldn't handle. In addition when using UDP it's not necessary for all the fragments to arrive at the destination so preventing fragmentation means the message either arrives or doesn't arrive - there is no possibility that only a bit of the UDP datagram will reach the destination. I can't recall how long the TCP/IP stack held on to unassembled IP packets waiting for missing fragments but use of the DF flag meant there were no unnecessary resources tied up during that time. Finally you can use it for testing behavior of network infrastructure such as what happens when you get a packet that's bigger than the maximum transmission unit (DF will prevent that packet from being fragmented to 'squeeze through' the hole)."
140,A,"How to tell *which* member in MIB_IPFORWARDROW structure is invalid? A call CreateIpForwardEntry returns ERROR_INVALID_PARAMETER. The PSDK documentation describes this error as: The pRoute parameter is NULL SetIpForwardEntry is unable to read from the memory pointed to by pRoute or one of the members of the MIB_IPFORWARDROW structure is invalid. I know for certain that pRoute is not NULL and that no call to SetIpForwardEntry is involved which leaves the only cause for the error as ""one of the members of the MIB_IPFORWARDROW structure is invalid."" How do I know which of the members of MIB_IPFORWARDROW is invalid? To further clarify my question: I know exactly what value each of these members has (I log them). At this point however I can't tell which one is incorrect. They all look valid to me. It would be nice to know why one of the members is invalid. But for that I first need to know which of them is invalid. For example in one of the test cases (which produce the aforementioned error) I have: dwForwardDest = 199.239.136.200 dwForwardMask = 255.255.255.255 dwForwardPolicy = 0 dwForwardNextHop = 127.0.0.1 dwForwardIfIndex = 1 dwForwardType = 0 dwForwardProto = 0X2 dwForwardAge = 0 dwForwardNextHopAS = 0 dwForwardMetric1 = 1 dwForwardMetric2 = -1 dwForwardMetric3 = -1 dwForwardMetric4 = -1 dwForwardMetric5 = 0 Any idea why CreateIpForwardEntry() wouldn't agree to accept the above parameters? Update: The tips provided below were very helpful but didn't really provide a method how to tell which member in MIB_IPFORWARDROW structure is invalid. I did correct the two members mentioned but the problem persists: Dest = 199.239.136.200 Mask = 255.255.255.255 Policy = 0 NextHop = 127.0.0.1 IfIndex = 1 Type = 0X3 Proto = 0X3 Age = 0 NextHopAS = 0 Metric1 = 1 Metric2 = -1 Metric3 = -1 Metric4 = -1 Metric5 = -1 Any idea why CreateIpForwardEntry() wouldn't agree to accept the above parameters? Tips ideas insights would be greatly appreciated. Thanks. I don't know anything about IP routing but the documentation for MIB_IPFORWARDROW doesn't list 0 as a valid value for dwForwardType. Also the documentation for CreateIpForwardEntry indicates that dwForwardProto must be 3 (MIB_IPPROTO_NETMGMT). +1 for finding in the documentation what I haven't been able to find. I am using the documentation version that comes with my PSDK version (Server 2003 R2) and the one in your links is the latest & greatest. I would have given you +10 but StackOverflow only allows +1. :) Now... I am rushing to read & try what you pointed me to and try to understand why I wasn't be able to spot that information (sleep deprivation?). I will report back if this was it and mark accepted accordingly. Update: I just checked the PSDK Server 2003 R2 documentation: (1) I indeed missed the dwForwardType requirement which is mentioned in the older documentation. (2) MIB_IPPROTO_NETMGMT is nowhere to be found. I will try the implied code fixes and report back. Final comment which allows accepting this answer as the original question was *How* to tell... The quickest way to determine which of the parameters is incorrect is to try them on the command line using the Windows command ROUTE ADD."
141,A,Closing/unbinding a UDP socket in c# Is there a way I can force a UDP socket (which is used by another process) to close or unbind in c# programmatically? You could release the network connection (like ipconfig /release).. but I guess that isnt the answer you're looking for ;) Only if you close that process. I hoped there would be sth like SocketPool... :(
142,A,two bytes swapped in packet received from Python raw socket My Python program is receiving ICMP destination unreachable messages from a raw socket. The socket is created with the following code: socket.socket(socket.AF_INET socket.SOCK_RAW socket.IPPROTO_ICMP) ICMP destination unreachable messages include a copy of a portion of the original packet. The problem I am experiencing is that the 2-byte total-length field in the embedded IPv4 header has its bytes swapped when receiving it from the socket. Because of this is fails the checksum check in my program. The packet is received with the following code: packet sockaddr = socket_.recvfrom(bufsize) When I noticed that the checksum test was failing I inspected the packets with tcpdump. Here is an example: 11:54:36.377352 IP (tos 0xc0 ttl 64 id 36894 offset 0 flags [none] proto ICMP (1) length 112) 10.200.200.200 > 10.175.211.104: ICMP host 10.175.211.13 unreachable length 92 IP (tos 0x0 ttl 63 id 49898 offset 0 flags [none] proto ICMP (1) length 84) 10.175.211.104 > 10.175.211.13: ICMP echo request id 29449 seq 6 length 64 0x0000: 45c0 0070 901e 0000 4001 3807 0ac8 c8c8 0x0010: 0aaf d368 0301 fcfe 0000 0000 4500 0054 0x0020: c2ea 0000 3f01 fcea 0aaf d368 0aaf d30d 0x0030: 0800 403f 7309 0006 9b43 7718 e292 a89a 0x0040: 28cf 5283 dc65 8219 2778 837b 6c54 92f4 0x0050: 5092 e976 568c 6681 2afc 2b82 628d d0f6 0x0060: 2fa7 3493 1f48 9fdb ed98 2f53 da0e 87a6 I then printed out the packet from my Python code. Here is the same packet from the tcpdump example: 45c05c0901e00401387ac8c8c8 aafd36831fcfe0000450540 c2ea003f1fceaaafd368aafd3d 80403f739069b437718e292a89a 28cf5283dc6582192778837b6c5492f4 5092e976568c66812afc2b82628dd0f6 2fa734931f489fdbed982f53dae87a6 The total-length field of the embedded header is the last two bytes on the 2nd line. 0054 from tcpdump and 5400 from my program. The total-length field of the outer IP header is also wrong (5c00 versus 0070 from tcpdump). I am running... Python 2.6.1 (r261:67515 Jun 24 2010 21:47:49) [GCC 4.2.1 (Apple Inc. build 5646)] on darwin The checksum field in the ICMP destination unreachable packet from tcpdump is correct. It has a value of fcfe. Here is the packet without the out IP header. 0x0010: 0301 fcfe 0000 0000 4500 0054 0x0020: c2ea 0000 3f01 fcea 0aaf d368 0aaf d30d 0x0030: 0800 403f 7309 0006 9b43 7718 e292 a89a 0x0040: 28cf 5283 dc65 8219 2778 837b 6c54 92f4 0x0050: 5092 e976 568c 6681 2afc 2b82 628d d0f6 0x0060: 2fa7 3493 1f48 9fdb ed98 2f53 da0e 87a6 What confuses me is that only the total-length field is corrected for network byte order. Other multi-byte fields are not changed and print out the same as my Python program. For example the two bytes after the total-length make up the identification field with a value of c2ea in both tcpdump and my print out. How can I determine which fields should be swapped for the purposes of checking the checksum? This is not really a question about checksums. It's a question about why two bytes are being swapped when you read from a socket. Perhaps you should edit your question title and body to reflect that! Those fields are likely in network byte-order (i.e. big-endian) and you are using a little-endian machine. So byte-swapping is needed. The struct library built into Python should help you with deserialization of these messages. Please refer to section 7.3.2.1. Byte Order Size and Alignment. I'd recommend referring to the RFC document for each protocol (e.g. RFC 791 and RFC 792) in use to gain further knowledge of how the fields are serialized. @Brian A checksum is normally calculated in the order in which the bytes are placed on the wire. You shouldn't have to do byte swapping before calculating the checksum. Thanks! I played around with struct.unpack which led me to question why total-length is the only multibyte field that is swapped by tcpdump and the only one that needs to be swapped for calculating checksum. I added more info to my initial post.
143,A,"Where are these IP addresses coming from? #include <stdio.h> #include <string.h> #include <sys/types.h> #include <sys/socket.h> #include <netinet/in.h> #include <netdb.h> #include <arpa/inet.h> int main(int argc char *argv[]) { //set up hints struct addrinfo hints; struct addrinfo *res; struct sockaddr their_addr; socklen_t sin_size = sizeof their_addr; char ipaddr[INET6_ADDRSTRLEN]; int sockfd new_fd; int sent; memset(&hints 0 sizeof(hints)); hints.ai_family = AF_UNSPEC; hints.ai_socktype = SOCK_STREAM; hints.ai_flags = AI_PASSIVE; getaddrinfo(NULL argv[1] &hints &res); sockfd = socket(res->ai_family res->ai_socktype res->ai_protocol); //bind a port to listen on bind(sockfd res->ai_addr res->ai_addrlen); listen(sockfd 10); new_fd = accept(sockfd (struct sockaddr *)&their_addr &sin_size); inet_ntop(AF_INET their_addr.sa_data ipaddr sizeof ipaddr); printf(""Connection recieved from %s"" ipaddr); sent = send(new_fd ""Hello world!"" 13 0); } This is the output from server when I telnet in: tyler@tkahn-server:~$ ./server_setup 1299 Connection recieved from 186.32.0.0 tyler@tkahn-server:~$ ./server_setup 1299 ^C tyler@tkahn-server:~$ ./server_setup 1290 Connection recieved from 231.237.0.0 tyler@tkahn-server:~$ ./server_setup 1290 Connection recieved from 231.241.0.0 tyler@tkahn-server:~$ BTW I'm running under a firewall so my server address is 192.168.101 and my local machine is 192.168.1.104 EDIT I suspect the troubled area is the function call inet_ntop(AF_INET their_addr.sa_data ipaddr sizeof ipaddr); This is where the stored ip address is converted into a string. According to the documentation http://linux.about.com/library/cmd/blcmdl3_inet_pton.htm it would be a good idea to retrieve the return value of inet_ntop and check for any errors. Do you connect to the server via the localhost/127.0.0.1 loopback interface? You're using INET6_ADDRSTRLEN shouldn't that be INET_ADDRSTRLEN for ipv4? Good point. It should be avoided to mix up those two however since INET6_ADDRSTRLEN > INET_ADDRSTRLEN I don't think that any overflowed buffer is causing this behavior. inet_ntop expects a struct in_addr pointer as the second argument so try something like this: inet_ntop(AF_INET &((struct sockaddr_in*)&their_addr)->sin_addr ipaddr sizeof ipaddr); thanks! it worked what does the 's' mean in sin_*? Basically it just stands for ""sockaddr_in"". sockaddr works kinda like a union in this case the sa_data[14] translates to a unsigned short port (sin_port) followed by a 4 byte address (sin_addr) followed by 8 dead bytes."
144,A,"Getting gateway to use for a given ip in ANSI C I have looked around like crazy but don't get a real answer. I got one example but that depended on the individuals own library so not much good. At first I wanted to get the default gateway of an interface but since different IP's could be routed differently I quickly understood that what I want it get the gateway to use for a given destination IP by using an AF_ROUTE socket and the rtm_type RTM_GET. Does anyone have an example where I actually end up with a string containing the gateways IP (or mac address)? The gateway entry seem to be in hex but also encoded in /proc/net/route where I guess the AF_ROUTE socket get's it info from (but via the kernel I guess). Thanx in advance and p.s. I just started using stack overflow and I must say all of you guys are great! Fast replies and good ones! You are my new best friends ;) This is OS specific there's no unified(or ANSI C) API for this. Assuming Linux the best way is to just parse /proc/net/route  look for the entry where Destination is 00000000  the default gateway is in the Gateway column  where you can read the hex representation of the gateway IP address (in big endian  I believe) If you want to do this via more specific API calls you'll have to go through quite some hoops here's an example program: #include <netinet/in.h> #include <net/if.h> #include <stdio.h> #include <string.h> #include <stdlib.h> #include <unistd.h> #include <sys/socket.h> #include <sys/ioctl.h> #include <linux/netlink.h> #include <linux/rtnetlink.h> #include <sys/types.h> #include <sys/socket.h> #include <arpa/inet.h> #define BUFSIZE 8192 char gateway[255]; struct route_info { struct in_addr dstAddr; struct in_addr srcAddr; struct in_addr gateWay; char ifName[IF_NAMESIZE]; }; int readNlSock(int sockFd char *bufPtr int seqNum int pId) { struct nlmsghdr *nlHdr; int readLen = 0 msgLen = 0; do { /* Recieve response from the kernel */ if ((readLen = recv(sockFd bufPtr BUFSIZE - msgLen 0)) < 0) { perror(""SOCK READ: ""); return -1; } nlHdr = (struct nlmsghdr *) bufPtr; /* Check if the header is valid */ if ((NLMSG_OK(nlHdr readLen) == 0) || (nlHdr->nlmsg_type == NLMSG_ERROR)) { perror(""Error in recieved packet""); return -1; } /* Check if the its the last message */ if (nlHdr->nlmsg_type == NLMSG_DONE) { break; } else { /* Else move the pointer to buffer appropriately */ bufPtr += readLen; msgLen += readLen; } /* Check if its a multi part message */ if ((nlHdr->nlmsg_flags & NLM_F_MULTI) == 0) { /* return if its not */ break; } } while ((nlHdr->nlmsg_seq != seqNum) || (nlHdr->nlmsg_pid != pId)); return msgLen; } /* For printing the routes. */ void printRoute(struct route_info *rtInfo) { char tempBuf[512]; /* Print Destination address */ if (rtInfo->dstAddr.s_addr != 0) strcpy(tempBuf inet_ntoa(rtInfo->dstAddr)); else sprintf(tempBuf ""*.*.*.*\t""); fprintf(stdout ""%s\t"" tempBuf); /* Print Gateway address */ if (rtInfo->gateWay.s_addr != 0) strcpy(tempBuf (char *) inet_ntoa(rtInfo->gateWay)); else sprintf(tempBuf ""*.*.*.*\t""); fprintf(stdout ""%s\t"" tempBuf); /* Print Interface Name*/ fprintf(stdout ""%s\t"" rtInfo->ifName); /* Print Source address */ if (rtInfo->srcAddr.s_addr != 0) strcpy(tempBuf inet_ntoa(rtInfo->srcAddr)); else sprintf(tempBuf ""*.*.*.*\t""); fprintf(stdout ""%s\n"" tempBuf); } void printGateway() { printf(""%s\n"" gateway); } /* For parsing the route info returned */ void parseRoutes(struct nlmsghdr *nlHdr struct route_info *rtInfo) { struct rtmsg *rtMsg; struct rtattr *rtAttr; int rtLen; rtMsg = (struct rtmsg *) NLMSG_DATA(nlHdr); /* If the route is not for AF_INET or does not belong to main routing table then return. */ if ((rtMsg->rtm_family != AF_INET) || (rtMsg->rtm_table != RT_TABLE_MAIN)) return; /* get the rtattr field */ rtAttr = (struct rtattr *) RTM_RTA(rtMsg); rtLen = RTM_PAYLOAD(nlHdr); for (; RTA_OK(rtAttr rtLen); rtAttr = RTA_NEXT(rtAttr rtLen)) { switch (rtAttr->rta_type) { case RTA_OIF: if_indextoname(*(int *) RTA_DATA(rtAttr) rtInfo->ifName); break; case RTA_GATEWAY: rtInfo->gateWay.s_addr= *(u_int *) RTA_DATA(rtAttr); break; case RTA_PREFSRC: rtInfo->srcAddr.s_addr= *(u_int *) RTA_DATA(rtAttr); break; case RTA_DST: rtInfo->dstAddr .s_addr= *(u_int *) RTA_DATA(rtAttr); break; } } //printf(""%s\n"" inet_ntoa(rtInfo->dstAddr)); if (rtInfo->dstAddr.s_addr == 0) sprintf(gateway (char *) inet_ntoa(rtInfo->gateWay)); //printRoute(rtInfo); return; } int main() { struct nlmsghdr *nlMsg; struct rtmsg *rtMsg; struct route_info *rtInfo; char msgBuf[BUFSIZE]; int sock len msgSeq = 0; /* Create Socket */ if ((sock = socket(PF_NETLINK SOCK_DGRAM NETLINK_ROUTE)) < 0) perror(""Socket Creation: ""); memset(msgBuf 0 BUFSIZE); /* point the header and the msg structure pointers into the buffer */ nlMsg = (struct nlmsghdr *) msgBuf; rtMsg = (struct rtmsg *) NLMSG_DATA(nlMsg); /* Fill in the nlmsg header*/ nlMsg->nlmsg_len = NLMSG_LENGTH(sizeof(struct rtmsg)); // Length of message. nlMsg->nlmsg_type = RTM_GETROUTE; // Get the routes from kernel routing table . nlMsg->nlmsg_flags = NLM_F_DUMP | NLM_F_REQUEST; // The message is a request for dump. nlMsg->nlmsg_seq = msgSeq++; // Sequence of the message packet. nlMsg->nlmsg_pid = getpid(); // PID of process sending the request. /* Send the request */ if (send(sock nlMsg nlMsg->nlmsg_len 0) < 0) { printf(""Write To Socket Failed...\n""); return -1; } /* Read the response */ if ((len = readNlSock(sock msgBuf msgSeq getpid())) < 0) { printf(""Read From Socket Failed...\n""); return -1; } /* Parse and print the response */ rtInfo = (struct route_info *) malloc(sizeof(struct route_info)); //fprintf(stdout ""Destination\tGateway\tInterface\tSource\n""); for (; NLMSG_OK(nlMsg len); nlMsg = NLMSG_NEXT(nlMsg len)) { memset(rtInfo 0 sizeof(struct route_info)); parseRoutes(nlMsg rtInfo); } free(rtInfo); close(sock); printGateway(); return 0; }  I decided to go the ""quick-and-dirty"" way to start with and read out the ip from /proc/net/route using netstat -rm. I thought I'd share my function... Note however that there is some error in it and prehaps you could help me find it and I'll edit this to be without faults. The function take a iface name like eth0 and returns the ip of the gateway used by that iface. char* GetGatewayForInterface(const char* interface) { char* gateway = NULL; FILE* fp = popen(""netstat -rn"" ""r""); char line[256]={0x0}; while(fgets(line sizeof(line) fp) != NULL) { /* * Get destination. */ char* destination; destination = strndup(line 15); /* * Extract iface to compare with the requested one * todo: fix for iface names longer than eth0 eth1 etc */ char* iface; iface = strndup(line + 73 4); // Find line with the gateway if(strcmp(""0.0.0.0 "" destination) == 0 && strcmp(iface interface) == 0) { // Extract gateway gateway = strndup(line + 16 15); } free(destination); free(iface); } pclose(fp); return gateway; } The problem with this function is that when I leave pclose in there it causes a memory corruption chrash. But it works if I remove the pclose call (but that would not be a good solution beacuse the stream would remain open.. hehe). So if anyone can spot the error I'll edit the function with the correct version. I'm no C guru and gets a bit confused about all the memory fiddling ;) There's nothing in the code you posted that would cause a memory-corruption crash. I confirmed that by running it with valgrind."
145,A,Master-Slave control architecture in .net across multiple systems I've been analyzing my next project and writing up requirements one of the things I'd like to do involves me communicating over 64 serial ports (16 ports x 4 windows pc's) The best was I can think to do this is a master/slave application model where the master application is controlled by an operator and the slaves simply execute commands issued by the master (remote procedure calls? I'm not great at the lingo yet I'm a junior engineer) my questions are as follows: In .net what is the best way to issue said commands and recieve responses from the remote processes? Is there a common design pattern to handle this sort of scenario? If the users don't have direct access to 3 of the 4 systems (maybe even all 4) how can i ensure that the slave processes are spawned on the remote computers? crash recovery is also a concern here is there an easy way to tell a remote pc to execute something? this seems like a HUGE security issue. Edit: Glomek asks (in short) Will anyone be using software on the slaves? Why would the users not have access to the systems? The systems are strictly pizza boxes no monitors these are minimum wage test operators testing products the 4 computers should communicate with each other via network communications but the 4 computers then communicate with 64 units under test that all have RS232 serial communications. Could you elaborate on what you are actually trying to accomplish? Why are you using serial instead of network communications? Will anyone be interactively using software on the slaves or are they just a computer farm for the master? Why would the users not have access to the systems? Regarding the authentication you could use certs from your ICA (if you have one).  In .net what is the best way to issue said commands and recieve responses from the remote processes? You can look at .NET Remoting. Other valid options would be to implement them as server (eg soap). A typical approach might be using queues. Is there a common design pattern to handle this sort of scenario? For that have a look at Enterprise Integration Patterns (Gregor Hohpe and Bobby Woolf). Great book. If the users don't have direct access to 3 of the 4 systems (maybe even all 4) how can i ensure that the slave processes are spawned on the remote computers? crash recovery is also a concern here is there an easy way to tell a remote pc to execute something? this seems like a HUGE security issue. Write an API that relies on authentication. You may use webservices with windows authentication enabled. Webservices may also be used to query the state of spawned processes. This could be done using the message based approach too. There must be a service running always on the remote machines. Either inside iis or using a windows service with some other sort of communication (eg queues) will start the executables with the Process class. This service would monitor the processes too. okay lets assume I don't care about the security of starting executables on remote systems how would I go about starting them on demand?
146,A,"Simulating host unreachable - how to achieve/implement it Here is my scenario: A is a provisioning server and B is an client. Whenever there is any change in B's setup it uploads the appropriate config file to A. I am working as an automation engineer to automate it. One of the scenario says to disconnect A from network or stop the server A. perform some changes to B and make sure that B failed to upload the files to provisioning server A. To automate it the simple way to stop the server A and do the appropriate actions. Since A and B are also used for other purposes by other parties so I can not either disconnect A or B from network OR stop the server at A. So I am looking forward for any solution so that I can simulate the host (provisioning server) unreachable scenario. So when B will send an update to A it will fail but in actual A is running as usual. Please suggest me some way to achieve it. I am using Perl as a programming language but I am fine if solution is available in other language. Test::MockObject::Extends - great for modifying small parts of modules to create specific testing scenarios. Works great for things that you can't test well because they affect things in production or in places that you don't control. #!/usr/bin/perl use strict; use warnings; use Test::MockObject::Extends; #Fake module that has your remote connect subroutine use Fake::Module; my $object = Fake::Module->new(); #replace your obj with a copy that Test::MO:E will let us mess with $object = Test::MockObject::Extends->new( $object ) #replace your connect function with a temp fake version $object->mock( 'your_remote_connect_sub' => sub { #Whatever data that should returned by your connect function if the server is unavailable return undef; } ); #test your sub now if ( !defined( $object->your_remove_connect_sub() ) ) { print ""Remote server unavailable\n""; }  Another perhaps easier option is to change the configuration on B to have a bogus IP address for A (e.g. 192.0.2.0) when performing the test. I wish I could do that. As I mentioned the A and B are used by other parties so I can not perform any change on existing setup.  I've done this before using a null route. This is something that best done from the shell with the ip command. # blackhole all packets destined for 192.168.2.1 ip route add blackhole 192.168.2.1 # to delete the same route replace add with del ip route del blackhole 192.168.2.1 Depending on your use case an unreachable route may work better as it returns ICMP-unreachable instead of discarding the packets although they tend to have the same effect. ip route add unreachable 192.168.2.1 And for thoroughness if you really wanted to simulate a host-unreachable situation (vs a network-unreachable) you would have to do that at the firewall level. # resond with icmp-host-unreachable for *any* outbound packet to 192.168.2.1 iptables -I OUTPUT -d 192.168.2.1 -j REJECT --reject-with=icmp-host-unreachable # delete the same rule (without looking up the rule #) iptables -D OUTPUT -d 192.168.2.1 -j REJECT --reject-with=icmp-host-unreachable sell = cli ? Thanks Jim for your prompt answer. I am new to networking domain. If I am not wrong I need to perform these changes on the server side? @converter42 - thanks :) @user502937 - nope you would add the route on the client using the server's IP. This will make it appear that the server ""disappeared"" (or more technically the route to the server is gone). Bravo. So much good can come from tests that properly simulate failures. There's nothing like knowing what your code will do when other systems fail and that it will correctly point the finger in that direction when the time comes so you won't get the irate 0230 phone call."
147,A,"Anyone has sample code that connects two socket in PHP? I'm trying to write a php code that acts as a server. All it would do is connects two socket it opens a file socket and pipes everything from it into another socket that it connects to. It would run 24/7. I'm not familiar with PHP but this project requires it. Any help? Here's a great resource that does exactly what you need: http://devzone.zend.com/article/1086 An actual answer would be pretty long winded for this Q/A format... +1 Had the same link; it is indeed a great resource for socket programming. I'm not sure where to start which socket would I need? It needs to transfer each byte as they come in. Any help please? @erotsppa: Have you read the article? It seems to cover the basics pretty well. Just read it from the beginning. I would even say the example given is already similar to what you want to do. If you don't know PHP in general you might have some problems with that. The code clearly shows you how to do it. Basically you create a listening socket and a while loop that never terminates that regularly checks the socket if there's incoming data. Which part are you not following? ""Which socket""? Well the one you create. Listening to the port you specify. Have you done socket programming before in another language? http://devzone.zend.com/article/1086 is broken  any other source please"
148,A,How do I disconnect a TcpClient to allow new connections? I have a TcpListener which waits for new socket connections. When a new connection is made some other code services messages from the remote user. I periodically check for new connections and if a new one is made the old one should be dropped. Previously I'd just open the new connection which for the most part worked okay. Occasionally though I'd have problems re-using the same port so I figured it would be better to close that connection (which I think should also give the old user an indication that the connection died). The code below is my attempt to do that but for some reason the Disconnect call seems to block indefinitely. Is there a way of stopping that? ...or am I missing something else here? while(1) { // FYI m_server is a TcpListener^ and m_client is a TcpClient^ // Check for new client connections if (m_server->Pending() == true) { if( m_stream != nullptr ) { m_client->Client->Shutdown( SocketShutdown::Both ); // Stop sending / receiving m_client->Client->Disconnect(true); // Disconnect the underlying network stream m_client->Client->Close(); // Disconnect the underlying network stream m_client->Close(); delete m_client; m_client = nullptr; } m_client = m_server->AcceptTcpClient(); //grab the TCP client m_stream = m_client->GetStream(); //create the stream at the same time } // ...go and service pending messages on the client stream } The docs on the Disconnect() function suggest there's a timeout I can set but they don't mention how? If you need to call Disconnect without first calling Shutdown you can set the DontLinger Socket option to false and specify a nonzero time-out interval to ensure that data queued for outgoing transmission is sent. Disconnect then blocks until the data is sent or until the specified time-out expires. If you set DontLinger to false and specify a zero time-out interval Close releases the connection and automatically discards outgoing queued data. [Edit] I found a solution to the timeout problem by changing m_client->Client->Disconnect(true); to m_client->Client->Disconnect(false); but that still doesn't warn my client that the socket has closed. I've tried everything I can think of to test but they all succeed: // m_client is a TcpClient^ bool stillConnected = ( (m_client != nullptr) && (m_client->Connected == true) && (m_client->Client != nullptr) && (m_client->Client->Connected == true) ); stillConnected is always true even after another client has connected to the server and therefore called all the shutdown bits and pieces above. It's like the shutdown stuff I mentioned at the top of my question isn't closing the socket properly or something? That's C++/CLI? isn't it? Yes it is but I belive the problem is something to do with .net / windows networking rather than the language I'm using to interact with the underlying hardware? Disconnection of a TCP socket (FIN flag being sent from the remote end) is detected when a read returns success and a length of zero bytes. Your suggestion put me on the right track; I needed to perform a read operation to detect the drop.
149,A,"Network programming in Python What library should I use for network programming? Is sockets the best or is there a higher level interface that is standard? I need something that will be pretty cross platform (ie. Linux Windows Mac OS X) and it only needs to be able to connect to other Python programs using the same library. the answer depends on what you are trying to do. ""What library should I use for network programming?"" is pretty vague. for example if you want to do HTTP you might look at such standard libraries as urllib urllib2 httplib sockets. It all depends on which protocol you are looking to use and which network layer you want to work at. there are libraries in python for various network tasks... email web rpc etc etc... for starters look over the standard library reference manual and see which tasks you want to do then go from there: http://docs.python.org/library/index.html  There is a framework that you may be interested in: Twisted  Personally I just use asyncore from the standard library which is a bit like a very cut-down version of Twisted but this is because I prefer a simple and low level interface. If you want a higher level interface especially just to communicate with another instance of your own program you don't necessarily have to worry about the networking layer and can consider something higher level like RPyC or pyro instead. The network then becomes an implementation detail and you can concentrate on just sending the information.  You just want to send python data between nodes (possibly on separate computers)? You might want to look at SimpleXMLRPCServer. It's based on the inbuilt HTTP server which is based on the inbuilt Socket server neither of which are the most industrial-strength servers around but it's easy to set up in a hurry: from SimpleXMLRPCServer import SimpleXMLRPCServer server = SimpleXMLRPCServer((""localhost"" 9876)) def my_func(ab): return a + b server.register_function(my_func) server.serve_forever() And easy to connect to: import xmlrpclib s = xmlrpclib.ServerProxy('http://localhost:9876') print s.my_func(23) >>> 5 print type(s.my_func(23)) >>> <type 'int'> print s.my_func(23.0): >>> 7.0 Twisted is popular for industrial applications but it's got a brutal learning curve. +1 for being another person who realises that the OP doesn't necessarily want networking just communications.  The socket module in the standard lib is in my opinion a good choice if you don't need high performance. It is a very famous API that is known by almost every developpers of almost every languages. It's quite sipmple and there is a lot of information available on the internet. Moreover it will be easier for other people to understand your code. I guess that an event-driven framework like Twisted has better performance but in basic cases standard sockets are enough. Of course if you use a higher-level protocol (http ftp...) you should use the corresponding implementation in the python standard library.  As previously mentioned Twisted is the most popular (by far). However there are a lot of other alternative worth exploring. Tornado and Diesel are probably the top two contenders. A more complete comparison is found here. If you add a link to Twised I can consider your answer to be better than Jeramy's.  A lot of people like Twisted. I was a huge fan for awhile but on working with it a bit and thinking about it more I've become disenchanted. It's complex and last I looked a lot of it had the assumption that your program would always be able to send data leading to possible situations in which your program grows memory usage endlessly buffering data to send that isn't being picked up by the remote side or isn't being picked up fast enough. In my opinion it depends a lot on what kind of network programming you want to do. A lot of times you don't really care about getting stuff done while you're waiting for IO. HTTP for example is very request-response oriented and if you're only talking to a single server there is little reason to need something like Twisted and plain sockets or Python's built-in HTTP libraries will work fine. If you're writing a server of any kind you almost certainly need to be event-driven. Twisted has a slight edge there but it still seems overly complex to me. Bittorrent for example was written in Python and doesn't use Twisted at all. Another factor favoring Twisted is that there is code for a lot of protocols already written for it. So if you want to speak an existing protocol a lot of hard work may already have been done for you."
150,A,Searching for patterns to create a TCP Connection Pool for high performance messaging I'm creating a new Client / Server application in C# and expect to have a fairly high rate of connections. That made me think of database connection pools which help mitigate the expense of creating and disposing connections between the client and database. I would like to create a similar capability for my application and haven't been able to find any good examples of how to apply this pattern. Do I really need to spin up an instance of a TcpClient every time I want to send a message to the server and receive a receipt message? Each connection is expected to transport between 1-5KB with each receiving a 1KB response message. I realize this question is somewhat vague but I am starting from scratch so I am open to suggestions. Even if that means my suppositions are all wrong. Do you need all these connections to stay connected or establish a new connection each time they're used? Are they all to the same server? If it's to the same server there might be better ways to share the same connection than create multiple connections even if they're all connected all the time. This will probably be a lot more efficient too. The pattern will need to support n number of clients eaching sending one message and receive one back from a central server. Any one client may have multiple concurrent connections each sending and receiving messages. This is due to background threads each collecting data and then needing to send it to the server (request/response). Initially my thinking was to have one connection pool per client servicing all concurrent requests for connections for that client possibly reusing connections/sockets/etc. Introducing connection pool is a sort of optimization. Premature optimization can be bad. I would recommend you start develpment without introducing connection pool. When client and server code is stable enough you can create load tests and detect performance problems. Connection pool is required if time to create a connection is considerable compared to the rate at which data is coming to/from server (load tests should indicate that). If data from client is send not that often you may not even need connection pool. Thanks for the feedback... In my case the test data already shows that applying a connection pool is an appropriate next step.
151,A,"Everything a c++ developer should know about network programming? So I am doing a lot of high performance network programming using Boost::Asio (or just Asio if you will) and have a pretty solid grasp of the essentials of both TCP and UDP protocols. I am wondering though because I still don't consider myself an expert in networking despite my knowledge what is a good way to frame the essentials of what networking programmers should know especially for those trying to push the performance of their large networking based applications? There is a great essay on programmers and what they should know about memory (see below) so I'm wondering if someone has put together something similar for networking. What every programmer should know about memory Some bullet points off the top of my head of things you should know: How and why TCP works... 3-way handshakes acknowledgement delayed ack nagling sliding window protocol. There's a concrete reason for every one of those features... and they can all destroy your application's performance if handled improperly. UDP multicast... even if you never think you'll use it you need to know why it exists so you can make educated decisions when designing systems. IP fragmentation and the impact of MTU. Binary serialization and network byte ordering (even if you're just going to use Google proto buffers it's nice to understand why they are efficient). Ascii serialization and message framing (what does \r\n\r\n mean in HTTP?) Different I/O dispatch models: Apache-style preforking thread-per-connection event-based single-threaded event-based with worker threads etc. The impact of buffer-overflow vulnerabilities in a networked app Protocol-based design as opposed to API- or library-based design asynchronous vs synchronous protocols. Many high-performance systems are asynchronous. HTTP is synchronous unless you use pipelining and even then there are many restrictions on what is possible... no out-of-order responses for example. Update: What does protocol-based design mean? Consider HTTP the protocol of the web. Apache IIS Lighttpd Firefox Opera WebKit etc... All of these pieces of software speak HTTP. It's quite possible that none of them are sharing the code to do so. The downside of course is the increased likelihood of bugs due to the net volume of code. There are numerous upsides: Any program can communicate via HTTP regardless of implementation language Lightweight/embedded environments can pick and choose a subset of the protocol rather than using the whole thing It's possible to optimize a protocol handler for particular situations. It's not possible to optimize a library without sacrificing generality. A variety of different implementations forces library providers to address bugs (rather than just blowing them off because well everyone uses the same library). There is no organizational or contractual burden on users of HTTP no licensing fees. When you design a network protocol you can build yourself several APIs each tailored towards specific use-cases. Or you can build one it's up to you. Networked software components can be upgraded independent of each other. Basically everything you hear that's good about Java/C# Interfaces and C++ abstract classes but applied at the network layer rather than the programming language layer. Thanks great list. Can you expand on this point though? ""Protocol-based design as opposed to API- or library-based design"" Ah got you that makes total sense. Thanks for the clarification. Any recommendation on where to read up on these things? Besides the multi-volume TCP book series anything a little more condensed and geared towards devs? Sorry I don't really know of any reference material for this... it's all nuggets I've picked up on the job. One cannot forget server implementation strategies (1 process per connection 1 thread per connection etc) and handling all possible scenarios in a client server application. All this is covered in Unix network progamming http://www.amazon.com/Unix-Network-Programming-Addison-Wesley-Professional/dp/0131411551/ref=sr_1_1?ie=UTF8&s=books&qid=1244659765&sr=1-1"
152,A,twisted not detecting client disconnects Does anybody have experience with this? I have a twisted app. The clients connect to the server. I added a feature so that if a client connects to a server but there's already a client from that IP address running it disconnects the new client. Once in a while I shutdown a client computer (or VM to be precise) without manually turning off the Python program. When I do this once in a while but pretty often the server does not detect any disconnect. When the computer comes back up and tries to reconnect the server insists that there is a connection from that IP already. The only solution I've found so far is to restart the server. Could it be strange networking things not having the disconnect go through? Twisted bug? I'm 99% certain it's not a bug with my code to handle disconnects. My code is set up such that connectionLost is called whenever a connection is lost including most cases of shutting down a machine and it either logs a string saying what disconnected or throws an exception if something strange happened. Neither of these things showed up in the log. Twisted framework has events and eventHandler defined at all conceptual levels - transport/ connection protocol etc to handle a disconnect. So examining the code would help. You can post a simplified version of code that exhibits that behavior. @pyfunc: I'll dig around the code. I suspect network issues for now cause I made the client keep attempting to reconnect and in some cases it was able to connect eventually (though in other cases not). This is a Twisted FAQ even though it doesn't really have anything to do with Twisted specifically.  Heh I can't believe I forgot all that I learned in networking class... (2:09:44 PM) coworker: this is the expected behaviour (2:10:15 PM) coworker: the server has no way to know if someone dies or is just quiet (2:10:35 PM) coworker: unless ofcourse the server has some kind of ping/keepalive message (2:15:38 PM) claudiu: ah so if they have no communicatin (2:15:42 PM) claudiu: there's no way to tell that a TCP connection has died (2:15:47 PM) claudiu: i remember learning that now yes.. (2:16:23 PM) claudiu: but if i just make the server ping the client then it'll figure out soon enough from lack of ACKs that it's dead right? (2:16:45 PM) coworker: right
153,A,"General timing problem in network programming Suppose that there're two types of packet in the chatting server. Those two packets are used for getting the status of another user. One(call this as REQ-STAT RES-STAT) is request-and-response form and another one(NOT-STAT) is notification form packet. For instance if I send [REQ-STAT simpson] to the server and I may get [RES-STAT simpson online]. Or if the user SIMPSON's status has been changed I'll get the user's status like [NOT-STAT simpson offline]. Okay the server is running on multi-core machine and using multiple threads of course. If the server get [REQ-STAT simpson] packet the server will prepare packet [RES-STAT simpson online] - if the SIMPSON is online. At this point the SIMPSON logged out so the server send packet [NOT-STAT simpson offline] to the client. After the server sent [NOT-STAT simpson offline] packet [RES-STAT simpson online] packet is sent // the server lock during using user data structure but not for the whole process. ------[REQ-STAT simpson]------> simpson is online and prepare response packet <--[NOT-STAT simpson offline]-- simpson has been logged out and send not packet <--[RES-STAT simpson online]--- send response // the latest packet does not have the latest information !!! Can above scenario be happen if LOCK does not used for whole process? If so do I should lock that all? Any other solutions? Thanks. Well it appears that you can solve it by the way the receiving program treats the different packets. If the program asks if SIMPSON is online it expects an answer of yes or no. But it should treat that as: SIMPSON was or was not online around the time I sent my request but he could have logged out 1 millisecond after this packet was created so I'm not guaranteed he's online now I just know he was when I asked. Use this information for what it's worth. However when the program receives a ""SIMPSON signed off"" packet then you know that SIMPSON signed off at the time specified in the packet. Receiving this would override any pending request for current status because you were just told that he signed off. So ""abort"" the current status request if there is one. By abort I mean gracefully stop that thread or ignore the result for that request. Another option is to embed a high-resolution time stamp (which must be thread-safe) or a thread-safe client-specific sequential counter into every packet so the client can figure out the order of events independent of the order of receiving them. Standard thread-safe programming practices will allow you to do this and it sounds like you're knowledgeable about that."
154,A,"What kind of socket server protocol is efficient? When I was writing a simple server for a simple client <> server multiplayer game I thought of the following text-based protocol using a translation library. Basically each command had a certain meaning eg: 1 = character starts turning right 2 = character starts turning left 3 = character stops turning 4 = character starts moving forward 5 = character stops moving 6 = character teleports to x y So the client would simply broadcast the following to inform that the player is now moving forward and turning right: 4 1 Or to teleport to 100x200: 6#100#200 Where # is the parameter delimiter. The socket connection would be connected to the player identifier so that no identifier has to be broadcasted with the protocol to know what player the message belongs to. Of course all data would be validated server side but that is a different subject. Now this seems pretty efficient to me only 2 bytes to inform the server that I am moving forward and turning right. However most ""professional"" code snippets I saw seemed to be sending objects or xml commands. This seems to require a lot more server resources to me doesn't it? Is my unexperienced logic of why my text based protocol would be efficient flawed? Or what is the recommended protocol for real-time action multiplayer games? I want to setup a protocol that is as efficient as possible because I do not want to use multiple clusters/servers to cover excessive amounts of bandwidth for my 2D multiplayer game and to safe synchronization problems and hassle. The common way is to use a binary format not text not xml. So with only one byte you can represent one of 256 different commands. Also use UDP and not TCP. The game will be a lot more responsive with UDP in case of packet loss. In case of packet loss you can still extrapolate the movements. With each packet send a packet number so that the server knows when the command was sent. I highly recommend that you download the Quake source code where you can learn more about network programming in modern multiplayer games. It's really easy to read and understand. edit: I almost forgot.. Google's Protocol Buffers can be of great help when sending complex data structures. @Tom: I was terribly unclear. One byte can represent 256 different commands. So you can send one of 256 per byte. And yes with UDP you can by including a packet number replicate a desired feature of TCP (getting data in correct order). Packet loss may happen anytime. With TCP you get very high latency. With UDP you can sometimes get less latency but lost information instead which is usually good compromise in real-time online games. How would you send 256 commands in 1 byte? Some pseudocode would be highly appreciated. If I use UDP can I replicate TCP's feature to resend failed packages? With your binary format you still use a translation library as I am with my text format right?  I thought I would give my two cents and provide a practical application to what is being referred to as Binary Serialization. The concept is actually incredibly simple yet only seems complicated on the outside. You can actually send XMLs and have a server that processes the data within the XML to different functions within the server itself. You can also just send the server a single number that is stored within the server as a variable. After that it can process the rest of the data and choose the correct course of actions. As an example some rough code: private const MOVE_RIGHT:int = 0; private const MOVE_LEFT:int = 1; private const MOVE_UP:int = 2; private const MOVE_DOWN:int = 3; function processData(e:event.data) { switch (e) { case MOVE_RIGHT: //move the clients player to the right case MOVE_LEFT: //move the clients player to the left case MOVE_UP: //move the clients player to the up case MOVE_DOWN: //move the clients player to the down } } This would be a very simple example and would need to be modified but as you can see you merely just store the variables encoded with whole numbers that you transmit in strings of numbers. You can parse these and create headers of information to organize them into different sections of data that needs to be transmitted. Also it is better to do a UDP setup for games because just missing a packet should NOT halter the gaming experience but instead should be able to handle it client-side AND server-side.  You need to be aware of the latency involved in sending your data. ""Start turning""/""stop turning"" will be less effective if the time between the receipt of those packets is different than the time between sending them. I can't speak for all games but when I've worked on this sort of code we'd send orientation and position information across the wire. That way the receiver could do smoothing and extrapolation (figure out where the object should be ""now"" based on data that I have that is already known to be old). Different games will want to send different data but generally speaking you will need to figure out how to make the receiver's display of the data match the sender's so you'll need to send data that is resilient in the face of networking problems. Also many games use UDP for this sort of data transfer instead of TCP. UDP is unreliable so you may not get all of your packets. That means that ""stop moving now"" or ""start moving now"" may not be received in pairs. When coding on top of UDP then it's even more important to send ""this is the state right now"" every so often so that clients get ample opportunity to sync up. My point was that you generally don't want to deal in transition states. Text or not- doesn't really matter if you're not sending a lot of data but each character takes up a whole byte regardless of the fact that you're presumably not using the whole range of values. So it is rather wasteful of memory in honesty but if it works for what you're doing I wouldn't worry about it. Parsing text data is also pretty slow compared to just reading bytes off of the wire and using them without anything beyond endianness translation. Text is certainly going to be easier to debug though! I am going to completely restart my project from scratch. I do need a more definitive answer for example why people are sending serialized objects and how what the advantage is compared to text. Or maybe I should send commands in binary as bitc pointed out? Well if you've got something that works there's no need to throw it all away! I don't not for the amount of people I want to have online simultaneousely ;) ok- then you're going to want to send your data as ""raw"" data. I.e. instead of sending text for ""100"" send the bits that represent that. Keep in mind network byte ordering too especially if you want to support heterogenous platforms. What kind of protocol are you using Lo'oris? +1 reading this post I had an illumination for a somewhat related problem I just gave an example though a position state could be given with my ""system"" too. Eg 1#x#y would inform the client what a players real position is so that it could correct the display (smoothly). My question is whether a text based protocol with a translation library is more efficient than other methods.  However most ""professional"" code snippets I saw seemed to be sending objects or xml commands. This seems to require a lot more server resources to me doesn't it? Is my unexperienced logic of why my text based protocol would be efficient flawed? Or what is the recommended protocol for real-time action multiplayer games? Plain text is more expensive to send than a binary format containing the same information. For example if you only send 1 byte you can only send 10 different commands digits 0 to 9. A binary format can send as many different commands as there are different values you can fit into a byte ie. 256. As such although you are thinking of objects as being large in actual fact they are almost always smaller than the plain text representation of that same object. Often they are as small as is possible without compression (and you can always add compression anyway). The benefits of a plain text format are that they are easy to debug and understand. Unfortunately you lose those benefits if you put your own encoding in there (eg. reducing commands down to single digits instead of readable names). The downside is that the format is bigger and that you have to write your own parser. XML formats eliminate the second problem but they can't compete with a binary format for pure efficiency. You are probably overthinking this issue at this stage however. If you're only sending information about events such as the commands you mention above bandwidth will not be a concern. It's broadcasting information about the game state that can get expensive - but even that can be mitigated by being careful who you send it to and how frequently. I would recommend working with whatever format is easiest for now as this will be the least of your problems. Just make sure that your code is always in a state where you can change the message writing and reading routines later if you need. You'd have to find someone with more experience of those languages to give you the specifics. I often get the suggestion to start with whatever I got at the moment and change it later. I have actually completely restarted my project and before I start writing code this time I'm going to document every subject I need to know things about in this case it's what protocol to use for my networking. It seems like a binary format is the most efficient so that really is what I want. I am not experienced with sending raw binary data though could you maybe give me an example in java actionscript or even pseudocode? That'll finish this chapter in my quest and allow me to accept your answer. Could you give an example of a 1 byte binary command though? I've no idea how to do this. I'm afraid I am not proficient enough with Java or Actionscript to provide pseudocode. The java.io.DataOutput interface is close to what we're talking about however. And in Javascript and Actionscript I believe you can convert a byte value to a one-character string via fromCharCode I believe."
155,A,"TCP - Sending and Receiving TCP/IP Data I have a client and a server communicating to one another over a standard TCP connection. The server is configured to send data in consistently sized chunks of x bytes. The client is also configured to receive data in chunks of x bytes (i.e. the client expects that the call to the TCP stack to receive x bytes will be successful unless the connection with the remote end has failed). Is it OK to base my TCP based protocol on the assumption above? No. TCP is a stream based protocol. You cannot guarantee ""chunks"" unless you handle the chunking yourself at the application level. TCP as a protocol will not insure this for you.  Deja vu Will TCPStream read block until all data is received. I realize it looks a little different but at the core they are very similar questions. Don't make assumptions about packet sizes. Thanks for the dialog ribram. Several other articles have convinced me of your answer's correctness. I'm not sure that the answers in response to that question are complete. ""Proper form"" aside. TCP has strict rules governing reliability and flow control. If the sender sends 20 bytes in ""one hit"" surely the receiver side should not be allowed to read any of those 20 bytes until all have been received and reconstructed in the correct order? and; therefore a protocol contract based on pre-negotiated chunk size should be OK to assume should it not? No. It is a stream. I might write 100 bytes and that could translate into 100 1-byte reads. Or 2 50-byte reads. Or any other combination. It is not packet based. See [this](http://www.codeproject.com/KB/IP/socketmessageboundary.aspx) for additional discussion. That same article states the following: ""The easiest but most costly way to solve the TCP message problem is to create a protocol that always transmits messages of fixed size. By setting all messages to the same size the receiving TCP program can know without doubt when the entire message has been received from the remote device."" And so making assumptions on packet sizes is OK provided it is followed by both sides of the communication. Remember my protocol does not use variable length data chunks - it is always the same. What it means is that if a message is always size X then when the receiver has X it knows that it has a complete message. But it does not mean that the receiver will get X in a single read. The receiver may need to do X 1-byte reads to get the entire message. See the difference? Basically if the size is fixed the receiver just knows intrinsically when it has received a complete message without needing to include the message length in the message or use special data markers in the stream to indicate end of message. I have read nothing to indicate what you are saying to be true. If A sends 20 bytes and B receives all 20 bytes before it is made available in B's TCP buffer then calling recv(20 bytes) should never fail unless a communication problem has occurred? Sorry about being stubborn with this one I am attempting to locate any part of the spec which definitely answers this one way or the other at the same time. I'm not sure what else I can say. [#20](http://tangentsoft.net/wskfaq/articles/lame-list.html) on the list? I used to support a TCP/IP development kit and the assumption you are making was hand's down the #1 cause of buggy TCP applications. What makes it worse is that you might exchange thousands or even millions of messages without issue but it just means you're lucky to that point. At some point the network will hiccup and things will go awry if you ever assume TCP to be anything more than a byte stream. TCP simply has no mechanism to 'glue' bytes together."
156,A,"Using accept() and select() at the same time? I've got an event-driven network server program. This program accepts connections from other processes on other hosts. There may be many short-lived connections from different ports on the same remote IP. Currently I've got a while(1) loop which calls accept() and then spawns a thread to process the new connection. Each connection is closed after the message is read. On the remote end the connection is closed after a message is sent. I want to eliminate the overhead of setting up and tearing down connections by caching the open socket FDs. On the sender side this is easy - I just don't close the connections and keep them around. On the receiver side it's a bit harder. I know I can store the FD returned by accept() in a structure and listen for messages across all such sockets using poll() or select() but I want to simultaneously both listen for new connections via accept() and listen on all the cached connections. If I use two threads one on poll() and one on accept() then when the accept() call returns (a new connection is opened) I have to wake up the other thread waiting on the old set of connections. I know I can do this with a signal and pselect() but this whole mess seems like way too much work for something so simple. Is there a call or superior methodology that will let me simultaneously handle new connections being opened and data being sent on old connections? I'd put a listener in separate process(thread) not to mess things up. And run a worker process on another to handle existing sockets. There's no need for non-blocking listener really. And no thread overhead running 2 threads. It should work like that: you accept on your listener thread till it returns you a descriptor of client socket and pass it to worker which is doing all dirty read/write job on it. If you want to listen several ports and don't want to hold one process per listener I suggest you set your socket in O_NONBLOCK and do someth like: // loop through listeners here and poll'em for read when read is successful call accept get descriptor pass it to worker and continue listen while(1){ foreach( serverSocket in ServerSockets ){ if( serverSocket.Poll( 10 SelectRead ) ){ clientSocket = serverSocket.Accept(); // pass to worker here and release } } } That's spinning you've got there; no good. And one thread per socket is way too many threads. what's so bad about this spinning? :) it won't eat any CPU besides it allows you to listen many ports in one thread. This is bad you're busywaiting with a 10usec timeout on each listening socket one at a time this WILL eat CPU. It'd be better to use `select` on all the listening sockets at the same time which will block until at least one has a incoming connection AND then try to `accept` from each of them (making sure obviously that each is marked `O_NONBLOCK` and being prepared for `accept` to return `EAGAIN` or `EWOULDBLOCK` for those sockets without a connection).  Last time I checked you could just listen on a socket and then select or poll to see if a connection came in. If so accept it; it will not block (but you may want to really should set O_NONBLOCK just to be sure) Hmm that's a well-known race condition - the `accept(2)` will block if client drops connection attempt between two syscalls. You *need* the listening socket to be non-blocking. This is correct - you can add your listening file descriptor to the `readfds` in your `select()` call and `select()` will tell you the file descriptor is ""readable"" if it has a connection ready to `accept()`. @Nikolai is correct too - the listening socket should be nonblocking and the `accept()` call prepared to handle `EAGAIN`.  you could use listen then use select or poll then accept  if (listen (socket_fd Number_connection) <0 ) { perror(""listen""); return 1; } fd_set set; struct timeval timeout; int rv; FD_ZERO(&set); /* clear the set */ FD_SET(socket_fd &set); /* add our file descriptor to the set */ timeout.tv_sec = 20; timeout.tv_usec = 0; rv = select(socket_fd + 1 &set NULL NULL &timeout); if(rv == -1) { perror(""select""); /* an error accured */ return 1; } else if(rv == 0) { printf(""timeout occurred (20 second) \n""); /* a timeout occured */ return 1; } else client_socket_fd = accept (socket_fd(struct sockaddr *) &client_name &client_name_len); What if the fd_set also has fds on which data might be coming to read and not listening?"
157,A,"Lower than low level common bsd sockets How do you do low low level sockets in C example: actually sending a SYN. What you actually want is a raw socket ... you can completely control the headers and flags with the raw socket interface but programming them is much more challenging. Here's a great tutorial to get you started: http://mixter.void.ru/rawip.txt. given link is not valid anymore.  I suspect the nmap sources would be an excellent place to look.  You want to use raw sockets. In *nix you need to be root to be able to create raw sockets. I'm not sure if it's possible in Windows. it's possible and you have to be administrator(or system) to do it.  Raw sockets are your friend. There have been some links to useful information on this question. Also consult Chapter 25 ""Raw sockets"" of Steven's ""Unix Network Programming"" If you're attempting cross platform code you may find libpcap a useful alternative."
158,A,Is POSIX pselect available in Perl? Is POSIX pselect available in Perl? A CPAN module is fine also. Could you fake it with POSIX::sigprocmask and select? What sort of application do you have in mind? Nothing in particular at the moment I'm just curious. As far as I can tell (in February 2010) neither the POSIX module nor any other provides support for the pselect() function. Since February 2010 as Wumpus Q. Wumbley points out in a comment a module POSIX::pselect now exists at revision 0.03 on 2014-04-26 (but the link is the version-neutral permalink). Versions 0.01 and 0.02 were released in June 2011; 0.03 in August 2012. @Jonathan: Any idea why it's not provided? @Robert: Two guesses - maybe a combination. (1) It is too new - it is a relatively recent addition to POSIX and may not be available everywhere. (2) The signal masking semantics in pselect() don't fit particularly easily into the Perl framework. Time has passed and this answer has been wrong longer than it was right. [POSIX::pselect exists now](http://search.cpan.org/~gfuji/POSIX-pselect-0.02/lib/POSIX/pselect.pm) @WumpusQ.Wumbley: thanks for the information. I've updated my answer to reflect modern reality. I'm pretty sure I'd not revisited the question/answer since February 2010.
159,A,"How do i implement Location Based Service in my web application? except $_SERVER['REMOTE_ADDR'] .. In PHP I want to know how to implement Location Based service in my website where i can find the exact location of the user who is accessing my site.. I tried with $_SERVER['REMOTE_ADDR'] but its giving ISP IP .. which is a bit inaccurate.. Thanks in Advance. I recently implemented geolocation with GeoIP (a PEAR package). It is not terribly accurate but it gives country ok region/state ok and major nearest city almost always right. Once you install the package you have to download the latest data file from maxmind and it can be as easy to use as this: require_once ""Net/GeoIP.php""; $geoloc = Net_GeoIP::getInstance('/var/www/GeoIP.dat'); try{ $countryCode = $geoloc->lookupCountryCode($_SERVER['REMOTE_ADDR']); } catch(Exception $e) {}"
160,A,Tracerouting in UDP protocal I am using UDP network protocol to send message from various clients to a root server. The message from client to server may not be sent directly and may be sent via other clients. I want to know the clients via which the message is sent by looking at the message received at the root server. How to do this? traceroute is client-side and heuristic i.e. works only for stable connections. Since you are essentially constructing an overlay network the only ways to get information about the route is reconstructing the routing according to your routing algorithm (hard and probably infeasible in a distributed network) or having each relay add a note (typically consisting of the relay's name and the previous IP address) to the message.  UDP does not include this information. You'll need to include something in your protocol if you want to keep track of servers through which the message has passed. The traceroute program uses a trick to get bounced packets by setting the TTL to an increasing number. It starts with a TTL of 1 so that the first bounce comes from the closest server to the source. It then tries a TTL of 2 to get a bounce from the second server on the path and so on.
161,A,"how to detect a TCP socket disconnection (with c berkeley socket) I am using a loop to read message out from a c Berkeley socket but I am not able to detect when the socket is disconnected so I would accept a new connection. please help while(true) { bzero(buffer256); n = read(newsockfdbuffer255); printf(""%s\n""buffer); } The only way you can detect that a socket is connected is by writing to it. Getting a error on read()/recv() will indicate that the connection is broken but not getting an error when reading doesn't mean that the connection is up. You may be interested in reading this: http://lkml.indiana.edu/hypermail/linux/kernel/0106.1/1154.html In addition using TCP Keep Alive may help distinguish between inactive and broken connections (by sending something at regular intervals even if there's no data to be sent by the application). (EDIT: Removed incorrect sentence as pointed out by @Damon thanks.) Slight correction: _""Reading 0 bytes may also simply mean that the remote party didn't have anything to send without it being a problem necessarily""_ -- receiving 0 bytes means that the other end performed a clean shutdown of the connection. If the other end did not have anything to send you get EAGAIN or EWOULDBLOCK if the socket is non-blocking or it blocks until data arrives. One way of detecting that the connection is down would be registering EPOLLHUP and EPOLLRDHUP on an epoll. This is not 100% reliable but will report orderly shutdowns half-shutdowns and missing keepalives. Of course a _long long long time_ may pass before a broken connection is detected with keepalives. Alas that's the nature of an independent-packet based network. @Bruno The email you linked didn't explain why a successful read doesn't guarantee the connection is up. @Pacerier there's no difference between not getting anything because nothing was sent and not getting anything because the link is broken. (It's a general principle in fact. If you don't get any letter in the morning either no one sent you anything or the postal system doesn't work: you can only find out which is the cause by trying to send something by post more or less.) This is why you need to handle timeouts when using TCP connections. A connection that's closed abruptly will simply not send you anything to tell you its closing because it can't. This is also why most protocols implement delimiters or length headers when they write something so that the reading party can know what to expect (e.g. `Content-Length` header on chunked transfer encoding in HTTP). @Bruno I mean ""why a *successful* read doesn't guarantee the connection is up?"" If I received my letters *successfully* then the postal system is *up* right? @Pacerier sure a successful read with data will indeed tell you the connection is up the problem is with a successful read that reads nothing (it also depends on whether you're blocking on read: you could block forever if you don't time out when nothing is read). @Bruno sry I'm not quite sure what do you mean by ""a successful read that reads nothing""? How can a successful read read nothing? @Pacerier if you're not blocking or if there's a timeout (in which case you might attempt to read again): the connection may still be up without the remote party sending any data. This is quite common. @Bruno Thanks =) write to the socket may be wait long time to get a error due to the sending buffer. If your package is very small most of time it won't work @jean sure there may be a delay but that's the only way to detect a disconnection still. Flush the buffer if you need to know as soon as possible. (If you're waiting for read error you can wait for a really long time... and that wouldn't be reliable.) @jean with an async write sure you may have to wait. Using read to detect disconnection can only work for clean disconnections not for abrupt ones. It's a TCP thing nothing to do with Boost or other library. The answer you link to doesn't contradict this (""*If the connection has went through an orderly shutdown [...]* and *[...]as long as the FIN packet from the client has arrived*""). In general to handle that sort of problems you'd want your application to use and handle timeouts on read. (+ this question is about Berkeley sockets anyway). I don't understand: ""When nothing is meant to be sent you can't know whether you're meant to receive something"". I can simply do: while(1) asyncread(). when it return error I know disconnected. flush buffer should work but it is a feature of os driver it is inaccessible. I am using boost.asio on android I use async_write to get the disconnect error but I found more than 50% there is nothing happen after connection closed even waiting two minutes. So I found http://stackoverflow.com/a/2939077 and http://boost.2283326.n4.nabble.com/ASIO-Detecting-broken-connections-td2569211.html and something else all they say use read. So... @jean No you can't that's the point. Check the [TCP flow diagram](http://en.wikipedia.org/wiki/Transmission_Control_Protocol#Protocol_operation). If you've established a connection and you don't receive anything at all then you won't know whether (a) the cable was cut (whether or not the other side tried to close the connection by sending FIN) or (b) the other side simply didn't have anything to send. If the connection is established and if you don't know whether to expect to receive anything (you expect the connection to be up a priori) then you can't know by just reading. If it is a abrupt disconnection using write also helpless right? That needs heartbeat to detect. I let client send heartbeat package every 15 secs if there is no package arrive longer than 20 secs in server  it disconnect the connection. I use return value of write to detect the disconnect in client. But I found sometimes it not work even I want minutes(many package sent during this time). So I think you said:""detect that a socket is connected is by writing to it"" is problematic. OK I got you. I want know what happens write into a connection which is a abrupt disconnection? You said it should fail because the remote point is unreachable and device like router switch will detect this and let tcp know this and finally report back to sender? But it have not fail in my test. Nothing happen when write to a disconnected connection. Instead use read works. And I don't know what's your definition of abrupt disconnection I understand it by: unplug network wire remote machine losing power... It couldn't detect by tcp itself. @jean yes abrupt in that sense (complete packet loss without warning). When nothing is meant to be sent you can't know whether you're meant to receive something (hence the idea of hearbeat in the protocol as you suggested for protocols that support that). The only way to find out whether it's disconnected is to try to write. Trying to read something whether you don't know whether it's meant to have sent anything won't tell you anything either way. Your case is different since you're using an async write. I don't know boost.asio very well but try to flush the buffer immediately if you can. ""*It couldn't detect by tcp itself.*"": without TCP keepalives (which are a form of writing) no. Once the connection is established if you don't know whether the remote party is meant to have sent something and if you try to read and get nothing you'll just get nothing indeed. A connection that is abruptly terminated in a way its FIN doesn't go across will just look like an established connection. @jean ""*If it is a abrupt disconnection using write also helpless right?*"" No it's not helpless it should *fail* hence you can detect a disconnection. Your idea of expecting to receive a heartbeat every 15 sec is different there might be a longer delay without the connection being close. You're then making an arbitrary decision regarding what delay you find acceptable that's it (and this is often the right thing to do practically) but that doesn't actually tell you whether the socket was disconnected it just means you give up and assume it has."
162,A,"Problem on Transferring file through TCP with the use of file pointers i need to transfer a file of 8.3MB over TCP when i use file pointers to read and write to files then i send about 8.6MB data and also receives 8.6 MB of data by calculating the output of send and recv calls although my file size is 8.3 MB further the when i check the size of file separately by seeing its properties then it is around 3-5 MB (varies on every transfer) but when i use file descriptors in place of file pointers then i send and recv exactly 8.3 MB of data and file property size also shows 8.3 MB. So what is the problem in using file pointers and why is it removed in case of file descriptors.... but if i use file descriptors then i am not able to read the text file that i sent. Text editors shows some binary data in the file. i am not getting a bit of what is happening... please help and thanks in advance server.cpp #include ""server.h"" void server() { int fd = open(""out.txt""O_WRONLY); struct sockaddr_in sin; socklen_t addr_size; char buf[MAX_LINE]; int len; int s new_s; /* build address data structure */ bzero((char *) & sin sizeof (sin)); sin.sin_family = AF_INET; sin.sin_addr.s_addr = INADDR_ANY; sin.sin_port = htons(SERVER_PORT); printf(""File Descriptor : %d"" fd); /* setup passive open */ if ((s = socket(PF_INET SOCK_STREAM 0)) < 0) { perror(""simplex-talk: socket""); exit(1); } if ((bind(s (struct sockaddr *) & sin sizeof (sin))) < 0) { perror(""simplex-talk: bind""); exit(1); } listen(s MAX_PENDING); // wait for connection then receive and print text */ while (1) { if ((new_s = accept(s (struct sockaddr *) & sin &addr_size)) < 0) { perror(""simplex-talk: accept""); exit(1); } float total = 0; printf(""File Descriptor : %d"" fd); while (len = recv(new_s buf MAX_LINE 0) && strcmp(buf""close"")) { buf[len] = 0; total = total+len; //write(stdout buf len); write(fd buf len); //printf(""%fKB and %fMB\n""total/1024 total/(1024*1024)); } printf(""File Descriptor : %d"" fd); close(new_s); } } client.cpp #include ""client.h"" void client(int argc char** argv) { int fd = open(""/home/nikku/Desktop/data.txt""O_RDONLY); if (fd < 0 ) perror(""File not opened\n""); struct hostent *hp; struct sockaddr_in sin; char *host; char buf[MAX_LINE]; int s; int len; host = argv[1]; if (argc == 2) { host = argv[1]; } else { fprintf(stderr ""usage: simplex-talk host\n""); exit(1); } /* translate host name into peer’s IP address */ gethostname(host20); printf(""%s\n""host); hp = gethostbyname(host); if (!hp) { fprintf(stderr ""simplex-talk: unknown host: %s\n"" host); exit(1); } /* build address data structure */ bzero((char *) & sin sizeof (sin)); sin.sin_family = AF_INET; bcopy(hp->h_addr (char *) & sin.sin_addr hp->h_length); sin.sin_port = htons(SERVER_PORT); /* active open */ if ((s = socket(PF_INET SOCK_STREAM 0)) < 0) { perror(""simplex-talk: socket""); exit(1); } if (connect(s (struct sockaddr *) & sin sizeof (sin)) < 0) { perror(""simplex-talk: connect""); close(s); exit(1); } printf(""Connection Succeeded\n""); /* main loop: get and send lines of text */ float total = 0; while (read(fd buf MAX_LINE)) { //usleep(1000);; len = strlen(buf) + 1; total += send(s buf len 0); //printf(""%fKB and %fMB\n""total/1024 total/(1024*1024)); } send(s ""close"" 6 0); close(fd); } Just if i replace use of file descriptors with pointers and use fgets and fputs to read and write then my file transfer does not take properly. But if i use file descriptors then i am not able to read the text file that i sent. Text editors shows some binary data in the file. If there's aproblem with your code then no-one can help you with it unless they can see your code. Please post your code. while (read(fd buf MAX_LINE)) { //usleep(1000);; len = strlen(buf) + 1; total += send(s buf len 0); //printf(""%fKB and %fMB\n""total/1024 total/(1024*1024)); } There is an issue here you have no guarantee that read will read a zero byte so strlen(buf) is potentially dangerous. Also note that as you set len to strlen(buf) + 1 if you do encouter a zero byte you will send it across the socket but if you don't read a zero byte strlen will read beyond the end of the array and send 'junk' across the socket. It would be sensible to store the return value of read so that you know how many bytes were actually read from fd. while (len = recv(new_s buf MAX_LINE 0) && strcmp(buf""close"")) { buf[len] = 0; total = total+len; //write(stdout buf len); write(fd buf len); //printf(""%fKB and %fMB\n""total/1024 total/(1024*1024)); } Your receive side seems to assume that each call to recv won't include a zero byte as you manually terminate the buffer with a 0. Note that you don't actually have room to do this if recv actually receives MAX_LINE bytes as buf only consists of MAX_LINE elements. As your write is restricted by length in any case there is no need to do buf[len] = 0;. thanks!!! got the problem"
163,A,"How Do Sockets Work in C? I am a bit confused about socket programming in C. You create a socket bind it to an interface and an IP address and get it to listen. I found a couple of web resources on that and understood it fine. In particular I found an article Network programming under Unix systems to be very informative. What confuses me is the timing of data arriving on the socket. How can you tell when packets arrive and how big the packet is do you have to do all the heavy lifting yourself? My basic assumption here is that packets can be of variable length so once binary data starts appearing down the socket how do you begin to construct packets from that? Short answer is that you have to do all the heavy lifting yourself. You can be notified that there is data available to be read but you won't know how many bytes are available. In most IP protocols that use variable length packets there will be a header with a known fixed length prepended to the packet. This header will contain the length of the packet. You read the header get the length of the packet then read the packet. You repeat this pattern (read header then read packet) until communication is complete. When reading data from a socket you request a certain number of bytes. The read call may block until the requested number of bytes are read but it can return fewer bytes than what was requested. When this happens you simply retry the read requesting the remaining bytes. Here's a typical C function for reading a set number of bytes from a socket: /* buffer points to memory block that is bigger than the number of bytes to be read */ /* socket is open socket that is connected to a sender */ /* bytesToRead is the number of bytes expected from the sender */ /* bytesRead is a pointer to a integer variable that will hold the number of bytes */ /* actually received from the sender. */ /* The function returns either the number of bytes read */ /* 0 if the socket was closed by the sender and */ /* -1 if an error occurred while reading from the socket */ int readBytes(int socket char *buffer int bytesToRead int *bytesRead) { *bytesRead = 0; while(*bytesRead < bytesToRead) { int ret = read(socket buffer + *bytesRead bytesToRead - *bytesRead); if(ret <= 0) { /* either connection was closed or an error occurred */ return ret; } else { *bytesRead += ret; } } return *bytesRead; } Shouldn't the early return after the closed connection be ""return bytesRead;"" instead of ""return ret;""? I presume the readBytes function is meant to return the actual number of bytes read. I suppose you could define it to return a non-positive integer if there is an error but you could also detect that an error occurred by checking to see if readBytes returns a different number of bytes than requested. a negative value to flag an error state is common practice in C programming and should be adhered to as much as possible to prevent confusion by the user of the API. I modified my example function to return the number of bytes read even if an error occurred during the read. One point to keep in mind with sockets is that a read that returns zero indicates that the connection was closed and may not necessarily be an error.  When you do a read on the socket you tell it how many maximum bytes to read but if it doesn't have that many it gives you however many it's got. It's up to you to design the protocol so you know whether you've got a partial packet or not. For instance in the past when sending variable length binary data I would put an int at the beginning that said how many bytes to expect. I'd do a read requesting a number of bytes greater than the largest possible packet in my protocol and then I'd compare the first int against however many bytes I'd received and either process it or try more reads until I'd gotten the full packet depending.  So the answer to your question depends a fair bit on whether you are using UDP or TCP as your transport. For UDP life gets a lot simpler in that you can call recv/recvfrom/recvmsg with the packet size you need (you'd likely send fixed-length packets from the source anyway) and make the assumption that if data is available it's there in multiples of packet-length sizes. (I.E. You call recv* with the size of your sending side packet and you're set.) For TCP life gets a bit more interesting - for the purpose of this explanation I will assume that you already know how to use socket() bind() listen() and accept() - the latter being how you get the file descriptor (FD) of your newly made connection. There are two ways of doing the I/O for a socket - blocking in which you call read(fd buf N) and the read sits there and waits until you've read N bytes into buf - or non-blocking in which you have to check (using select() or poll()) whether the FD is readable and THEN do your read(). When dealing with TCP-based connections the OS doesn't pay attention to the packet sizes since it's considered a continual stream of data not seperate packet-sized chunks. If your application uses ""packets"" (packed or unpacked data structures that you're passing around) you ought to be able to call read() with the proper size argument and read an entire data structure off the socket at a time. The only caveat you have to deal with is to remember to properly byte-order any data that you're sending in case the source and destination system are of different byte endian-ness. This applies to both UDP and TCP. As far as *NIX socket programming is concerned I highly recommend W. Richard Stevens' ""Unix Network Programming Vol. 1"" (UNPv1) and ""Advanced Programming in an Unix Environment"" (APUE). The first is a tome regarding network-based programming regardless of the transport and the latter is a good all-around programming book as it applies to *NIX based programming. Also look for ""TCP/IP Illustrated"" Volumes 1 and 2.  Sockets operate at a higher level than raw packets - it's like a file you can read/write from. Also when you try to read from a socket the operating system will block (put on hold) your process until it has data to fulfill the request. This is only true if the socket is not set to be non-blocking."
164,A,"Find device address with libpcap I am trying to find address of devices in my computer. So far i managed to get list of devices(with pcap_findalldevs) but i can`t figure out how to get to those addresses. I saw this manpage - http://www.tcpdump.org/pcap3_man.html where is written something like this addresses a pointer to the first element of a list of addresses for the interface And then this Each element of the list of addresses is of type pcap_addr_t and has the following members: So I have this code pcap_if_t *alldevsp  *device; char *devname  **devs; int count = 1  n; if(pcap_findalldevs(&alldevsp errbuf)) { printf(""Error: %s""  errbuf); exit(1); } device = alldevsp; pcap_addr_t list; printf(""\nDevices:\n""); while(device != NULL) { printf(""%d. %s - %s"" count++  device->name  device->description); list = device->addresses[0]; printf(""address: %s\n""(char *) inet_ntoa(list.addr)); device = device->next; } Compilation is OK but when i try to run it i get this: Devices: 1. eth0 - (null)addres: 144.208.30.8 2. wlan0 - (null)addres: 128.213.30.8 Segmentation fault I can understand that segfault because third device is usb and it doesnt have address but those IP for eth0 and wlan0 are wrong they doesnt match. What am I doing wrong? How do you know the IPs are wrong? ifconfig? What if you run it a different way: http://stackoverflow.com/questions/4139405/how-to-know-ip-address-for-interfaces-in-c/4139893#4139893 I know they are wrong because of ifconfig :) Besides that they change every time I run a program From your link: Each element of the list of addresses is of type pcap_addr_t and has the following members: ... addr a pointer to a struct sockaddr containing an address ... Now what is a struct sockaddr? See here: http://www.retran.com/beej/sockaddr_inman.html So where you are doing this: printf(""address: %s\n""(char *) inet_ntoa(list.addr)); You should be doing something like this: printf(""address: %s\n"" inet_ntoa(((struct sockaddr_in*)list.addr)->sin_addr)); That is you need to extract the ""IP address"" (if indeed the family is AF_INET) else you are giving inet_ntoa the wrong type of argument. Yes that's what I was referring to when I said ""if indeed the family is AF_INET."" The family is of course just another field in the sockaddr. Looks like you can only retrieve the IP this way. How do you get the mac address? I'd try this approach: http://stackoverflow.com/questions/1779715/how-to-get-mac-address-of-your-machine-using-a-c-program to get the MAC address once you have the IP address. This of course assumes that the system configuration doesn't change between capture time and analysis time."
165,A,Twisted: source IP address for outbound connections I'm in the process of implementing a service -- written in Python with the Twisted framework running on Debian GNU/Linux -- that checks the availability of SIP servers. For this I use the OPTIONS method (a SIP protocol feature) as this seems to be a commonplace practice. In order to construct correct and RFC compliant headers I need to know the source IP address and the source port for the connection that is going to be established. [How] can this be done with Twisted? This is what I tried: I subclassed protocol.DatagramProtocol and within startProtocol(self) I used self.transport.getHost().host and self.transport.getHost().port. The latter is indeed the port that's going to be used whereas the former only yields 0.0.0.0. I guess that at this point Twisted doesn't [yet?] know which interface and as such which source IP address will be used. Does Twisted provide a facility that could help me with this or do I need to interface with the OS (routing) in a different way? Or did I just use self.transport.getHost().host incorrectly? For the sake of completeness I answer my own question: Make sure you use connect() on the transport before trying to determine the host's source IP address. The following excerpt shows the relevant part of a protocol implementation: class FooBarProtocol(protocol.DatagramProtocol): def startProtocol(self): self.transport.getHost().host # => 0.0.0.0 self.transport.connect(self.dstHost self.dstPort) self.transport.getHost().host # => 192.168.1.102  If you are using UDP then the endpoint is determined by either: calling bind() on the socket and explicitly giving it an address sending a packet If you want a few more details check this response. The problem is that I'm not that familiar with twisted. From what I can tell by a quick perusal of the source it looks like you might want to use a reactor like t.i.d.SelectReactor instead. This is what t.n.d.DNSDatagramProtocol does under the hood. If you take twisted out of the picture then the following snippet shows what is going on: >>> import socket >>> s = socket.socket(socket.AF_INET socket.SOCK_DGRAM 0) <socket._socketobject object at 0x10025d670> >>> s.getsockname() # this is an unbound or unnamed socket ('0.0.0.0' 0) >>> s.bind( ('0.0.0.0' 0) ) # 0.0.0.0 is INADDR_ANY 0 means pick a port >>> s.getsockname() # IP is still zero but it has picked a port ('0.0.0.0' 56814) Get the host name is a little trickier if you need to support multiple network interfaces or IPv4 and IPv6. If you can make the interface used configurable then pass it in as the first member of the tuple to socket.bind() and you are set. Now the hard part is doing this within the confines of the abstractions that twisted provides. Unfortunately I can't help a whole lot there. I would recommend looking for examples on how you can get access to the underlying socket or find a way to pass the socket information into the framework. Good luck. Your answer would be great if this question wasn't specifically about how to do it with Twisted but in this context it doesn't really help.  Did you see if that you want to do is possible with the SIP implementation that is part of Twisted? In any case how you set the source address and port for UDP in Twisted is quite similar to how you set them without Twisted. In Twisted reactor.listenUDP(port protocol interface) binds an UDP socket to a specific port and interface and handles the received datagrams to your protocol. Inside the protocol self.transport.write(msg addr) sends a datagram to addr using the address that the protocol is bound to as source address. Reading your question again I think the only part you were missing was passing interface to reactor.listenUDP(...). I used a bit of Twisted's SIP implementation to generate some of the required headers.
166,A,"Pthread-ed filetransfer application crash I am developing a file transfer application and am using pthreads on the receiver side for receiving multiple files. The function which is passed to pthreads calls the following function and at the end of this function I get a SIGABRT error and stack-smashing error appears on the terminal. Please help me find the bugs. If you need anymore code I'd be able to post the same. Thanks in advance. void recv_mesg(int new_sockid char *fname) { cout<<""New Thread created with ""<<new_sockid<<"" and ""<<fname<<endl; char buf[MAXLINE]; int fd; fd = open(fname O_WRONLY ); int len =0; while (len<1024) { int curr = recv(new_sockid buf 1024-len 0); //fprintf(stdout""Message from Client:\n""); len += curr; //write (fd buf curr); fputs(buf stderr); } int file_size = 0; sscanf(buf""%d""&file_size); if(file_size<=0) perror(""File Size < 0""); sprintf(buf""Yes""); send(new_sockidbufstrlen(buf)0); len = 0; while (len<file_size) { int curr = recv(new_sockid buf min(file_size-lenMAXLINE) 0); len += curr; write (fd buf curr); //fputs(buf stdout); //fflush(stdout); } len = 0; close(fd); close(new_sockid); } Post the calling code. What is the value of MAXLINE? If it's less than 1024 you're going to get stack smashing in your first while loop. Also you are not checking for errors or EOF in your recv() calls. If the remote peer closes the TCP connection before all the data has been sent your recv() calls will start returning 0 every time you call them and that will cause either of your while loops to start spinning forever. @Jeremy Friesner and if recv() returns -1 it would funny too. check the return code of system function especially recv() which could return 0 or -1 you forgot to increment the buffer pointer you're givin to recv() so each time it is called the content of the buffer is overwritten you forgot to ensure there's a NUL character at the end of buffer before calling sscanf() I'll check that and get back to you. Thanks for your help :)"
167,A,"Why no ""what's my ip"" well-known port/service? I know there are some web sites that provide this service but given that pretty much everyone lives behind NAT these days why isn't this standardized on a port and provided as a service to whomever wants to run it? It's at least as useful as an echo daytime or ""quote of the day"" server and as easy to implement. Or does one exist that I am aware of? Any proposals/RFCs in progress? EDIT: Thanks to everyone for a lively and thoughtful discussion. For those that might want to know I prefer http://www.whatsmyip.us/ over all the others as it has the least amount of shit on it. I think .org used to be plaintext but somebody ruined that. There's going to be no widespread fix. 'When/if IPv6 ever fully replaces IPv4 then the abundance of available addresses will make NAT and thus this issue a thing of the past. IPv6 will only replace NAT where it makes sense for each device to have a publicly-addressable IP address. In most situations that's not desirable for security reasons. I think you have it backwards. The abundance of NAT has made the need for IPv6 disappear.  It's standarised with the STUN protocol in RFC 5389 Of course vendors have to support standards to make them useful. Related threads on StackOverflow. They all mention STUN: Discovering public IP programatically Trying to get NAT’s external IPAddress with INATExternalIPAddressCallback in C# Obtaining Own External IP Address in POSIX C Interesting. I am only superficially aware of STUN so it will take a while to go through this. Offhand it seems like overkill for the basic situation outlined. Using a howitzer to get a gnat? (Sorry couldn't resist.) STUN is complicated because the world is complicated. There are many sort of NAT routers; each with different behaviour.  You really need more than to know what your IP address is ""right now"". You need to have it stay that way. More importantly it seems that the reason you need to know your IP address is that you need to tell some other machine how to connect to you. What's really needed then isn't a low-level hack but rather some protocol that configures all interested Network-layer devices to make sure that you can be reached and that the connection stays open (or that it can reconnect if broken). For instance it's possible for your router to lose its connection to your ISP. When it gets a new connection it could have a different IP address. You need to address that problem. In a Corporate environment (yes corporations do use P2P) any number of network reconfigurations may take place all without informing the peers about changes in IP address. This needs to be addressed as well. The overall problem is that you're depending on a Network-layer entity (IP Address) to remain useful for the upper layers. That's not the job of an IP address - it's only meant to get your datagrams to their destination. There's nothing inherent in knowing your IP address that says they'll still get to their destinations tomorrow by using that same address. Valid points. I guess I am coming from the opposite vantage point. Simple things can be put together to do complex things (e.g. 'cat' not so useful by itself but powerful when piped into other pgms). This is a very simple service. If and how it might fit into a larger protocol is the problem of someone with a bigger itch to scratch. It would be their problem to adequately deal with broken connections and such. But why even bother knowing that the Network Layer is in charge of these numbers and not you? But every layer is forced to deal with the fragility of those beneath it. IP has deal with various media idiosyncrasies like fragmentaion etc. I guess I don't see how this is terribly different (albeit simpler) than something like dynamic DNS a somewhat similar service created by some of the same factors (e.g. lack of static IPs ISP IP changes etc). Read my answer. I don't have a problem with creating a service to do this all properly. My only problem is if there is _no_ service purpose-built to solve the problem. In that case (the present case) people are depending on the Network layer to do their job for them which places constraints on the Network layer that do not belong there. I say decide what you would like to have in an ideal world then design a protocol to get it.  If such a peername service existed your NAT gateway could choose to modify the service's response replacing the external address with the internal one again. Also in case of a double-natted system: how would you get at the middle address? I don't think anyone ever really cares about the middle address. What end users typically care about is ""Hey I'm hosting a Ventrilo server and want my friends to connect to it - what address do they type in to get here?"" Why would NAT mess with the external address? Presumably it would be passed back in the payload and NAT really shouldn't be messing with that. NAT devices typically mess with the payload when they recognize a wire protocol that transmits IP addresses. The primary example is ftp which otherwise wouldn't work. That must be related to the ""protocol"" byte in IP Header or something similar no? NAT can't be messing with every protocol's payload or there would be chaos. No it's related to the port number. If they recognize and ftp control connection they inspect it. So if a new service would be registered (as the OP suggests) this very registration might defeat its purpose. It will not defeat it's purpose. STUN TURN ICE and other protocols are well defined and noone will mess with them at router level exactly because they should be left alone. You know.. obscurity is not protection etc. These are protocols that are designed to help connections from behind nat."
168,A,How to read an incoming tcp stream until a delimiter is found? How do you read an incoming tcp stream until a specific delimiter is found in C#? The only possible solution I have come up with is reading the incoming stream one byte at a time. A byte delimiter or a Text (char) delimiter? Reading the TCP socket and scanning for a delimiter are two different things. You can read all available data on a non-blocking socket into a byte array/string then scan the byte array for your delimiter. Do whatever else you need to do including perhaps saving data after the delimiter for the next read attempt. Its best to use some sort of buffer to add incoming data into so the socket operations don't exactly dictate handling of the data. I'll second that. Reading it it slower won't change what's coming down the pipe to you anyway it just makes for an inefficient read from the socket/stream. Yep. Reading from a TCP socket one byte at a time is very inefficient. Read the data from the socket into a byte array and then process the byte array.
169,A,"HttpURLConnection implementation I have read that HttpURLConnection supports persistent connections so that a connection can be reused for multiple requests. I tried it and the only way to send a second POST was by calling openConnection for a second time. Otherwise I got a IllegalStateException(""Already connected""); I used the following: try{ URL url = new URL(""http://someconection.com""); } catch(Exception e){} HttpURLConnection con = (HttpURLConnection) url.openConnection(); //set output input etc //send POST //Receive response //Read whole response //close input stream con.disconnect();//have also tested commenting this out con = (HttpURLConnection) url.openConnection(); //Send new POST The second request is send over the same TCP connection (verified it with wireshark) but I can not understand why (although this is what I want) since I have called disconnect. I checked the source code for the HttpURLConnection and the implementation does keep a keepalive cache of connections to the same destinations. My problem is that I can not see how the connection is placed back in the cache after I have send the first request. The disconnect closes the connection and without the disconnect still I can not see how the connection is placed back in the cache. I saw that the cache has a run method to go through over all idle connections (I am not sure how it is called) but I can not find how the connection is placed back in the cache. The only place that seems to happen is in the finished method of httpClient but this is not called for a POST with a response. Can anyone help me on this? EDIT My interest is what is the proper handling of an HttpUrlConnection object for tcp connection reuse. Should input/output stream be closed followed by a url.openConnection(); each time to send the new request (avoiding disconnect())? If yes I can not see how the connection is being reused when I call url.openConnection() for the second time since the connection has been removed from the cache for the first request and can not find how it is returned back. Is it possible that the connection is not returned back to the keepalive cache (bug?) but the OS has not released the tcp connection yet and on new connection the OS returns the buffered connection (not yet released) or something similar? EDIT2 The only related i found was from JDK_KeepAlive ...when the application calls close() on the InputStream returned by URLConnection.getInputStream() the JDK's HTTP protocol handler will try to clean up the connection and if successful put the connection into a connection cache for reuse by future HTTP requests. But I am not sure which handler is this. sun.net.www.protocol.http.Handler does not do any caching as I saw Thanks! How do you ""force use of HTTP1.0"" using the HttpUrlConnection of JDK? According to the section „Persistent Connections” of the Java 1.5 guide support for HTTP1.1 connections can be turned off or on using the java property http.keepAlive (default is true). Furthermore the java property http.maxConnections indicates the maximum number of (concurrent) connections per destination to be kept alive at any given time. Therefore a ""force use of HTTP1.0"" could be applied for the whole application at once by setting the java property http.keepAlive to false.  From the javadoc for HttpURLConnection (my emphasis): Each HttpURLConnection instance is used to make a single request but the underlying network connection to the HTTP server may be transparently shared by other instances. Calling the close() methods on the InputStream or OutputStream of an HttpURLConnection after a request may free network resources associated with this instance but has no effect on any shared persistent connection. Calling the disconnect() method may close the underlying socket if a persistent connection is otherwise idle at that time. I started looking the src code once I saw(using wireshark) that if I called disconnect() the TCP connection was still reused. In the src I could not see the connection placed back in the keepAlive cache once I closed the input stream of the first request. So there was no difference if I called disconnect() or not. This worried me that perhaps there is some kind of issue in the implementation unless I am missing something I have read this. My question is how the connection returns in the buffer to be shared by other instances. I can not find the code that returns the connection to the cache I'm not sure what your concern is. Are you seeing behavior that leads you to suspect a bug in the implementation?  I found that the connection is indeed cached when the InputStream is closed. Once the inputStream has been closed the underlying connection is buffered. The HttpURLConnection object is unusable for further requests though since the object is considered still ""connected"" i.e. its boolean connected is set to true and is not cleared once the connection is placed back in the buffer. So each time a new HttpUrlConnection should be instantiated for a new POST but the underlying TCP connection will be reused if it has not timed out. So EJP answer's was the correct description. May be the behavior I saw (reuse of the TCP connection) despite explicitly calling disconnect() was due to caching done by the OS? I do not know. I hope someone who knows can explain. Thanks. No I don't think OS can do this. There is a way to force closure by either forcing use of HTTP 1.0 (which does not support persistent connections) or using ""Connection: close"" header if you really do NOT want any sharing of underlying TCP connetion. @StaxMan:I know about the Connection header in HTTP.My confusion was due to the fact that even though I explicitly closed the underlying socket the connection was still reused. In my mind the behavior I was expecting was a 2(max) sec delay due to 3-way handshake and an HTTP connection over a new TCP connection. I did not see that and I thought that it was something related to OS caching Ok thanks for clarification. @Cratylus You didn't close the underlying socket. You can't. Only the underlying implementation can do that. Closing the input stream of the connection certainly didn't close that socket. I am seeing the exact same thing (reuse of TCP connection despite explicitly calling HttpURLConnection.disconnect()). Can anyone explain under what circumstances calling disconnect() would NOT cause the closure of the underlying TCP connection? @TomMcIntyre If the underlying connection had already been grabbed from the pool for use by another HttpURLconnection it would be highly improper for disconnect() to close it.  Should input/output stream be closed followed by a url.openConnection(); each time to send the new request (avoiding disconnect())? Yes. If yes I can not see how the connection is being reused when I call url.openConnection() for the second time since the connection has been removed from the cache for the first request and can not find how it is returned back. You are confusing the HttpURLConnection with the underlying Socket and its underlying TCP connection. They aren't the same. The HttpURLConnection instances are GC'd the underlying Socket is pooled unless you call disconnect(). I started looking the src code once I saw(using wireshark) that if I called disconnect() the TCP connection was still reused. There is a concurrent hashmap for the caching of url connections and I see that when a new HttpURlConnection is created if there is a connection in the pool it uses it (in the HttpURLConnection constructor). But I can not see how the connection is placed back in the keepAlive cache when I close the streams (and read whole response). I see that the connection is returned if the request is a HEAD (http.finished() is called) but not for a POST. You are right the serverSockets are pooled in the concurrentHashMap but once a request is about to be done the socket is removed from the ClientVector so that a new thread has to create a new connection. But this socket must return in the pool. This seems to happen only in putInKeepAliveCache();. But this is not called for a 200 OK with a response body. How is it possible to see the same behavior in the TCP connection using disconnect or not? No the *Sockets* are pooled. There are no ServerSockets in this context. And what you've found is that maybe not all of them are pooled. What do you mean by the HttpUrlConnection instances are GC'd ? @Pacerier GC = Garbage Collection. On Android at least the sockets are pooled even when disconnect() is called by default.  Hmmh. I may be missing something here (since this is an old question) but as far as I know there are 2 well-known ways to force closing of the underlying TCP connection: Force use of HTTP 1.0 (1.1 introduced persistent connections) -- this as indicated by the http request line Send 'Connection' header with value 'close'; this will force closing as well. How do you ""force use of HTTP1.0"" using the HttpUrlConnection of JDK? Hmmh. Good question -- I assumed there was a way but I could not find one with quick googling. So I would use the second solution; it's better either way and for this purpose works as well.  Abandoning streams will cause idle TCP connections. The response stream should be read completely. Another thing I overlooked initially and have seen overlooked in most answers on this topic is forgetting to deal with the error stream in case of exceptions. Code similar to this fixed one of my apps that wasn't releasing resources properly: HttpURLConnection connection = (HttpURLConnection)new URL(uri).openConnection(); InputStream stream = null; BufferedReader reader = null; try { stream = connection.getInputStream(); reader = new BufferedReader(new InputStreamReader(stream Charset.forName(""UTF-8""))); // do work on part of the input stream } catch (IOException e) { // read the error stream InputStream es = connection.getErrorStream(); if (es != null) { BufferedReader esReader = null; esReader = new BufferedReader(new InputStreamReader(es Charset.forName(""UTF-8""))); while (esReader.ready() && esReader.readLine() != null) { } if (esReader != null) esReader.close(); } // do something with the IOException } finally { // finish reading the input stream if it was not read completely in the try block then close if (reader != null) { while (reader.readLine() != null) { } reader.close(); } // Not sure if this is necessary closing the buffered reader may close the input stream? if (stream != null) { stream.close(); } // disconnect if (connection != null) { connection.disconnect(); } } The buffered reader isn't strictly necessary I chose it because my use case required reading one line at a time. See also: http://docs.oracle.com/javase/1.5.0/docs/guide/net/http-keepalive.html You don't need to read the stream to EOS or the error stream either. You just have to close. See your own citation. Closing the buffered reader will close the stream so you don't need that either. -1"
170,A,"TCP/IP protocol and network topology I am a newbie in network related aspects. I have few basic questions related to tcp/ip protocol and network If a network switch (in a LAN network) between two PC's running Client and server (that are communicating through async. sockets) is powered down. Can the client and server will be notified that the socket connection is no longer active. Client and server are running on Win XP OS and are coded using C#. Does network topology play a role in case of half open connection between socket client and socket server. For e.g. Will a disconnect status of either one or both be notified to other end and does it depend on network topology. Thanks in advance. A network element such as a router/hub/switch does not activly cause anything anything to happen on the TCP layer if it goes down. The operating system might notice that the physical layer is down and error out all sockets bound on that network card if it's a network element directly connected to the PCs that breaks - this will vary among operating systems/network cards and other things. Other than that in order to detect that the connection has been severed you'll have to send something and rely on the TCP timeout mechanisms to error out. This can be done implicittly by enabling TCP Keepalives on the connection. A disconnect on one side will only be noticed if those messages reach the other side if the network topology changes or sometinhg breaks in the middle of the connection in such a way that messages no longer reach the other end a disconnect won't be noticed. (NAT gateways are a big source of problems such as this they might time out a TCP connection they're tracking and you'll never know the connection is no longer valid unless you try to write something (or enable TCP keepalives) to the connection). Note that most networking APIs require that you Read from the connection to discovver that a the other end has closed the connection - assuming those ""close"" messages actually reach your side. +1 ""What he said."" Next time I'll try type faster."
171,A,"Request Local WebPage Hey guys I need to connect to the IP Address ""127.0.0.1:4000"" the problem is that I can't find a way to do that in C# WebRequest only supports URIs (to my knowledge) and I couldn't find a socket function to do it either. Any Help with this would be great Thanks Max An ip port combo as you write it is a pefectly valid URI Thanks for the help guys I found what I was Looking for! <code> /// <summary> /// Gets the contents of a page using IP address not host name /// </summary> /// <param name=""host"">The IP of the host</param> /// <param name=""port"">The Port to connect to</param> /// <param name=""path"">the path to the file request (with leading /)</param> /// <returns>Page Contents in string</returns> private string GetWebPage(string host int portstring path) { string getString = ""GET ""+path+"" HTTP/1.1\r\nHost: www.Me.mobi\r\nConnection: Close\r\n\r\n""; Encoding ASCII = Encoding.ASCII; Byte[] byteGetString = ASCII.GetBytes(getString); Byte[] receiveByte = new Byte[256]; Socket socket = null; String strPage = null; try { IPEndPoint ip = new IPEndPoint(IPAddress.Parse(host) port); socket = new Socket(ip.AddressFamily SocketType.Stream ProtocolType.Tcp); socket.Connect(ip); } catch (SocketException ex) { Console.WriteLine(""Source:"" + ex.Source); Console.WriteLine(""Message:"" + ex.Message); MessageBox.Show(""Message:"" + ex.Message); } socket.Send(byteGetString byteGetString.Length 0); Int32 bytes = socket.Receive(receiveByte receiveByte.Length 0); strPage = strPage + ASCII.GetString(receiveByte 0 bytes); while (bytes > 0) { bytes = socket.Receive(receiveByte receiveByte.Length 0); strPage = strPage + ASCII.GetString(receiveByte 0 bytes); } socket.Close(); return strPage; } Thanks again for the help couldn't have found it any other way.  You could use a TcpClient or raw Socket: using (var client = new TcpClient(""127.0.0.1"" 4000)) using (var stream = client.GetStream()) { using (var writer = new StreamWriter(stream)) { // Write something to the socket writer.Write(""HELLO""); } using (var reader = new StreamReader(stream)) { // Read the response until a \r\n string response = reader.ReadLine(); } } Remark: If it is a binary protocol you should directly write/read to the socket without using StreamWriter and StreamReader.  You can set the Proxy property of the HttpWebRequest that should do the trick. Pretty nice simple example here (In VB though but not hard to translate).. I tried the code you gave but it returned the same error I have been experiencing on the line: HttpWebRequest webReq = ((HttpWebRequest)(WebRequest.Create(url))); (where url = 127.0.0.1:4000)"
172,A,"Linux version of Windows ""nonpaged pool"" does such a thing exist? I have been working with an Windows application which reads from the 'nonpaged pool' to increase performance. In this case the nonpaged pool is the area of memory where the network drivers write data as they grab it off the wire. How does Linux handle memory which network drivers (or other drivers) which require high speed exclusive access to RAM and does the question 'how do I read directly from nonpaged pool?' even make sense when applied to Linux? Many thanks related question Do you mean a driver which uses non-paged pool? Windows does not provide access to non-paged pool to applications - it is a kernel only thing. The application only reads from non-paged pool occasionally not fast enough! Some networks like Infiniband support RDMA which requires being able to prevent paging for some of the pages in a process. See the mlock() mlockall() munlock() munlockall() functions. Other than that I don't think there is a concept of ""nonpaged pool"" per se. Generally kernel memory is AFAIK not pageable but all user memory except that locked with mlock() or such is."
173,A,How to do Socket communication in android? I am having a taskconsidering one emulator as server and i have to access a web service from the client emulator.I think this is not possible in androidif anyone knows about it please send some code snippets to do this. Android's java.net package is pretty much the same as Java's so you can use Sockets and ServerSockets for TCP communication or a DatagramSocket for UDP. However there is currently no support for SOAP webservices in the android SDK but you can use third party libraries or build your own. Have a look at the following questions: How to call a net web service from android Regarding connecting to a webserver from android I find it interesting that both questions I linked to were asked by a user with the exact same name (but different user id). Did you re-register?
174,A,Packet Details Markup Language I have problem in parsing XML containing gsm packets through my .Net application.I want to decode data obtained from parsed XML as wire shark Decode.Problem isThat my information regarding gsm packets and protocols is limited. I read somewhere that wire shark parse xml through PDML(Packet Details Markup Language) File which contains all the information of all types of protocols including gsm. I googled but not found that file. Any help and Suggestions will be appreciated You can use the Export->XML menu in Wireshark to generate a PDML file. PDML is an XML representation of your the captured data flow. For this to work you should have the GSM dissectors installed along with Wireshark. GSM call flows will give you an idea on what to expect from the decoded Wireshark output. Thanx I export pdml and it works
175,A,"Which header files are necessary to run this code snippet? It's from herebut fails when compiling: int main(int argc char **argv) { struct hostent { char *h_name; // main name char **h_aliases; // alternative names (aliases) int h_addrtype; // address type (usually AF_INET) int h_length; // length of address (in octets) char **h_addr_list; // alternate addresses (in Network Byte Order) }; #define h_addr h_addr_list[0] // First address of h_addr_list. struct hostent *info_stackoverflow; int i = 0; info_stackoverflow = gethostbyname( ""www.stackoverflow.com"" ); printf(""The IP address of %s is %s"" info_stackoverflow->h_name inet_ntoa( * ((struct in_addr *)info_stackoverflow->h_addr ))); /* aliases */ while( *(pc_ip->h_aliases + i) != NULL ) { printf(""\n\tAlias: %s"" *(pc_ip->h_aliases + i) ); i++; } } You need these three headers: #include <stdio.h> #include <netdb.h> #include <arpa/inet.h> You should get rid of your own definition of struct hostent. It's already defined for you in netdb.h and your definition will conflict. Tip: try ""man gethostbyname"" on almost any Unix system; the manual page for most C functions will tell you what header files to include. This still won't compile because pc_ip is undefined. You're missing part of your code snippet I guess. What libraries are also necessary? None these functions are in the standard C library.  #include <stdio.h> #include <winsock.h> Though struct hostent is already defined by Winsock so you will want to remove the definition of hostent from your code snippet. As dmazzoni has noted pc_ip is not declared in that code. It is being used like a pointer to a hostent structure so you could probably replace pc_ip with info_stackoverflow. When you link you will need to link against ws2_32.lib. At run-time you'll probably have problems until you add a call to WSAStartup at the beginning of your code and WSACleanup at the end before you return from main. Can you figure out how's `pc_ip` defined? Ohafter replace pc_ip with info_stackoverflowI got a error while linking: `error LNK2019: unresolved external symbol _inet_ntoa@4 referenced` It compiles OK nowbut get an Access violation at this line `inet_ntoa( * ((struct in_addr *)info_stackoverflow->h_addr )));` You probably don't have a valid data structure; `gethostbyname` is probably failing but your code isn't checking for failures. It is probably failing because you haven't added calls to `WSAStartup` and `WSACleanup`. I got a run-time error after adding WSAStartup and WSACleanup... That's not enough information. What's the error? It works for me. I just compiled it on Mac OS X and Linux with no problems. If you're still having trouble please post more information (like what error you're getting) otherwise how can we help?"
176,A,Approach to handle WiFi disconnects In my Android application it is noticed that when the device goes to sleep/standby WiFi is disconnected. When the device wakes up it gets reconnected. Before making a httpClient.execute(..) call to remote server we check if the device is connected to n/w. When the data transfer is being done and if the device goes to sleep then Android runtime will switch to another medium for connectivity(3GGPRS etc.). Is the switch from WiFi to alternate cellular service say 3G seamless? How do I wait for WiFi to become available again? Should I use Thread.sleep(delay) when the WiFi wakes up? I have seen broadcast actions when the WiFi state changes. In general what is the ideal approach to handling WiFi disconnects in a mobile app? Why not use WifiManager.WifiLock when the transfer is happening and release it when you have finished. I would suspect network connection switch would not be seamless. I don't know for sure. A BroadcastReceiver will let you know when Wifi connection state changes. Have a look at ConnectivityManager though because that will monitor Wifi and GPRS etc and it does do failover. Whether it is seamless though I don't know.
177,A,"unix select() call: how to combine fd_sets? I am writing an application in C for linux which uses 2 separate third-party libraries. Both libraries are asynchronous and use select(). They also provide an API which returns the file descriptors they wait on. My intention is to pass these into my own select() and then return control back to whichever library when their own fd values are set. I think I've got most of it written but I have trouble at the point where the select() parameters are concerned: Both libraries do not give individual file descriptors but pointers to their read and write fd_sets. I need to combine the returned fd_sets from these libraries into one fd_set for read one fd_set for write etc. Any suggestions on how I can combine 2 fd_sets into one resulting fd_set? Addendum Sorry! I should have been clearer.. these libraries only return the fd_sets... I don't know the number of FDs in each set so that I can do a for loop and set each FD individually.. is there a simple way of determining this given just an fd_set? *I am writing an application in C for linux which uses 2 separate third-party libraries.* should be *I am writing an application in C for Linux which uses two third-party libraries.* Are you sure that this strategy will work? Typically such libraries implement their own event loop and expect you to set up callbacks and transfer control to them exclusively. The good ones will allow you to add file descriptors of your own to their select lists. In a single-threaded program you can't easily have 2 exclusive control event loops never mind 3. The fd_set is actually a bitmap. Maybe you can combine them using binary or. typedef long int __fd_mask; typedef struct { __fd_mask __fds_bits[__FD_SETSIZE / __NFDBITS]; } fd_set; One might need to take special care about the size of the fd_set: it can be also dynamically allocated. Accessing bytes past the `(nfds+7)/8` (where `nfds` is the first select()'s argument) should be avoided. Unless of course one can confirm that the libraries are actually using the proper fd_set from sys/select.h. Except in the most performance-intensive applications I would recommend strongly against this approach. Looking inside opaque data structures and writing code that depends on their implementation is a very bad practice.  You can set up your own fd_set by looping over all possible file descriptors that each library could possibly have open and in each of their returned sets and set your's accordingly. It's a bit brute-force but the select interface is unfortunately quite primitive. For example fd_set *lib1_read_fds *lib2_read_fds; int fd; fdset my_fd_set; FD_CLR(&my_fd_set); for (fd = 0; fd < FD_SETSIZE; fd++) { if (FD_ISSET(fd lib1_read_fds) || FD_ISSET(fd lib2_read_fds)) { FD_SET(fd &my_fd_set); } } You need to use FD_ZERO not FD_CLR to zero the result set. My bad. Should be FD_ZERO of course.  Iterate through one set and check each fd up to FD_SETSIZE with FD_ISSET() and if it is set set the fd in the other set using FD_SET()  I don't think you can do that portably however if you rewrite your code to use poll() you can merge the sets it uses since they are just arrays of portably structs  Without relying on the internal implementation of fd_set the best you can do is a simple pair of loops fd_set combine_sets(fd_set* p_set1 int n1 fd_set* p_set2 int n2) { fd_set combined; int i; FD_ZERO(combined); for (i = 0; i < n1; ++i) { if (FD_ISSET(i p_set1)) { FD_SET(i &combined); } } for (i = 0; i < n2; ++i) { if (FD_ISSET(i p_set2)) { FD_SET(i &combined); } } return combined; } Where the n parameters are the largest descriptor number potentially in the corresponding set plus one (i.e. the nfds argument that you would pass to select if you were using just that set). Edit: Changed to take pointers as arguments since that's what you said you are getting from your libraries. Returning an fd_set seems like a really bad idea. It's been a while since I read POSIX but it may even allow fd_set to be an array type in which case this code would fail. Better to pass a pointer to an fd_set and let the function write into that...  C code which doesn't depend on the implementation of fd_set: void Fdset_Add(fd_set *Out fd_set const *In int InNfds) { for(i = 0; i < InNfds; i++) { if(i < InNfds && FD_ISSET(i In)) FD_SET(i Out); } } int Fdset_Merge(fd_set *Out fd_set const *In1 int NFds1 fd_set const *In2 int NFds2) { FD_ZERO(Out); Fdset_Add(Out In1 Nfds1); Fdset_Add(Out In2 Nfds2); return Nfds1 > Nfds2 ? Nfds1 : Nfds2; } int Fdset_Filter(fd_set const *Result int ResultNfds fd_set *ToFilter int NfdsToFilter) { int i; int Retval; Retval = 0; for(i = 0; i < ResultNfds; i++) { if(i < NfdsToFilter && FD_ISSET(i ToFilter)) { if(! FD_ISSET(i Result)) FD_CLR(i ToFilter); else Retval++; } } return Retval; } void Fdset_Split(fd_set const *Result int ResultNfds fd_set *In1 int Nfds1 int *Count1 fd_set *In2 int Nfds2 int *Count2) { *Count1 = Fdset_Filter(Result ResultNfds In1 Nfds1); *Count2 = Fdset_Filter(Result ResultNfds In1 Nfds2); } in FDSet_Merge() you only loop up to the minimum of NFds1 and NFds2. You need to loop up to the maximum of the two @JeremyP - thanks fixed. Shouldn't that be ""const fd_set *In"" and so on? @unwind - I try to put the `const` to the right of the type as it will always qualify the type directly to its left. Maintaining this style reinforces that understanding so that if I encounter something like `int * const * Something;` it's easier to remember which part is being marked read-only. Interesting thanks."
178,A,"gdb appears to ignore executable capabilities I am debugging a program that makes use of libnetfilter_queue. The documentation states that a userspace queue-handling application needs the CAP_NET_ADMIN capability to function. I have done this using the setcap utility as follows: $ sudo setcap cap_net_rawcap_net_admin=eip ./a.out I have verified that the capabilities are applied correctly as a) the program works and b) getcap returns the following output: $ getcap ./a.out ./a.out = cap_net_admincap_net_raw+eip However when I attempt to debug this program using gdb (e.g. $ gdb ./a.out) from the command line it fails on account of not having the correct permissions set. The debugging functionality of gdb works perfectly otherwise and debugs as per normal. I have even attempted to apply these capabilities to the gdb binary itself to no avail. I did this as it seemed (as documented by the manpages that the ""i"" flag might allowed the debugee to inherit the capability from the debugger. Is there something trivial I am missing or can this really not be done? GDB uses the ptrace subsystem. Does it have the CAP_SYS_PTRACE capability ? Does it work with any other binary? E.g. a hello world program ? @thkala: I edited the question to be more precise. `gdb` does work fine it can debug any program (including this one) otherwise. Would you mind mentioning the exact error message? There is no error message per se. I am using [this](http://www.netfilter.org/projects/libnetfilter_queue/doxygen/group__LibrarySetup.html) sample code provided by the developers and `nfq_unbind_pf()` is returning `-1` (and `errno` is set to `1`) indicating failure. For those who have the same problem you can bypass this one by executing gdb with sudo.  A while ago I did run into the same problem. My guess is that running the debugged program with the additional capabilities is a security issue. Your program has more privileges than the user that runs it. With a debugger a user can manipulate the execution of the program. So if the program runs under the debugger with the extra privileges then the user could use these privileges for other purposes than for which the program intended to use them. This would be a serious security hole because the user does not have the privileges in the first place. From linux execve manpage: ""If the set-user-ID bit is set on the program file pointed to by filename and the underlying file system is not mounted nosuid (the MS_NOSUID flag for mount(2)) and the calling process is not being ptraced then the effective user ID of the calling process is changed to that of the owner of the program file."" This probably also holds for file capabilities because like suid/sgid they provide privilege elevation. My guess is that this is handled by the kernel instead of gdb. You should probably look into the ptrace system call. That answer makes a lot of sense to me. I'll try looking into the `gdb` source to confirm this is the case but I'll mark your answer as accepted now anyway.  I run into same problem and at beginning I thought the same as above that maybe gdb is ignoring the executable's capability due to security reason. However reading source code and even using eclipse debugging gdb itself when it is debugging my ext2fs-prog which opens /dev/sda1 I realize that: gdb is no special as any other program. (Just like it is in the matrix even the agents themselves they obey the same physical law gravity etc except that they are all door-keepers.) gdb is not the parent process of debugged executable instead it is grand father. The true parent process of debugged executable is ""shell"" i.e. /bin/bash in my case. So the solution is very simple apart from adding cap_net_admincap_net_raw+eip to gdb you have also apply this to your shell. i.e. setcap cap_net_admincap_net_raw+eip /bin/bash The reason that you have also to do this to gdb is because gdb is parent process of /bin/bash before create debugged process. The true executable command line inside gdb is like following: /bin/bash exec /my/executable/program/path And this is parameter to vfork inside gdb."
179,A,Designing a simple network packet I'm learning socket programming (in python) and I was wondering what the best/typical way of encapsulating data is? My packets will be used to issue run stop configure etc. commands on the receiving side. Is it helpful to use JSON or just straight text? I suggest you use a fixed or mostly fixed format as this make things easier. By then using features such as the standard library's struct.Struct with its pack() and umpack() methods or possibly a slightly more featured pacakges such as Construct you should have much of the parsing work done for you ;-)  I suggest plain text to begin with - it is easier to debug. The format that your text takes depends on what you're doing how many commands arguments etc. Have you fleshed out how your commands will look? Once you figure out what that looks like it'll likely suggest a format all on its own. Are you using TCP or UDP? TCP is easy since it is a stream but if you're using UDP keep in mind the maximum size of UDP packets and thus how big your message can be.  If you're developing something as a learning exercise you might find it best to go with a structured text (ie. human readable and human writable) format. An example would be to use a fixed number of fields per command fixed width text fields and/or easily parsable field delimiters. Generally text is less efficient in terms of packet size but it does have the benefits that you can read it easily if you do a packet capture (eg. using wireshark) or if you want to use telnet to mimic a client. And if this is only a learning exercise then ease of debugging is a significant issue.  Take a look at how scapy (an awesome Python packet manipulation library) implements it. Looks like that have a handful of fields.
180,A,"Networking Framework for C++ (UDP or TCP)? I'm writing a threaded cross-platform application (Linux/Windows) using SDL and OpenGL and to do networking I was considering SDL Net2 because it sits on top of SDL_Net. However I've never done networking in C/C++ before so I'm unfamiliar with any available cross-platform technologies. Is there anyone with experience with SDL_Net or Net2 who would suggest a different library? Could you provide a bit more information on what type of application you are writing? Can you describe what you will be using the network layer to transfer? Un/reliable? Bandwidth considerations latency tolerance etc? As others already said boost::asio is certainly the good choice. Now that said that's really to make ""raw"" networking. IF you're making a video game or network-performance-like-video-game software maybe a more high-level and optimized for this case library like the heavily recommanded RakNet library would be a better choice. Another alternative in this same case if you want lower level UDP based library ENet might suit your needs.  There are a bunch of frameworks out there (e.g. Poco ACE). It depends on what you are looking for. If you want to create raw sockets at the OS layer and send byte streams across them then I would go with Boost.Asio as suggested by everyone else. If you are looking for sending XML documents over HTTP or something like that then investigate some of the other alternatives.  Boost.Asio (http://boost.org) or try out this one http://libunicomm.org based on Asio.  Try Boost.Asio perhaps.  boost::asio is your friend. It is cross platform async io library. Good for network. With a lot of samples on the boost site. http://www.boost.org/"
181,A,"Send data over the network C# I try to send a string over the network this is my code:  IPEndPoint serverEndPoint = new IPEndPoint(IPAddress.Parse(""127.0.0.1"") 25); TcpClient client = new TcpClient(serverEndPoint); Socket socket = client.Client; byte[] data = Encoding.ASCII.GetBytes(response); socket.Send(data data.Length SocketFlags.None); socket.Close(); client.Close(); When I run it I got System.Net.Sockets.SocketException Are you sure there's something listening on that port? Attempting to open a socket to a non-existent server would certainly result in a socket exception. I have temporarily downvoted you because you failed to present the message that accompanied the SocketException. If you would please re-test your application then present to us the contents of `SocketException.Message` as well as the Type and `Message` contents of any InnerExceptions. This helps us diagnose your issues. If you are using a connectionless protocol you must call Connect before calling Send or Send will throw a SocketException. If you are using a connection-oriented protocol you must either use Connect to establish a remote host connection or use Accept to accept an incoming connection. Refer Socket.Send Method (Byte[] Int32 SocketFlags) Assuming you are using a connectionless protocol the code should be like this string response = ""Hello""; IPAddress ipAddress = IPAddress.Parse(""127.0.0.1""); if (ipAddress != null) { IPEndPoint serverEndPoint = new IPEndPoint(ipAddress 25); byte[] receiveBuffer = new byte[100]; try { using (TcpClient client = new TcpClient(serverEndPoint)) { using (Socket socket = client.Client) { socket.Connect(serverEndPoint); byte[] data = Encoding.ASCII.GetBytes(response); socket.Send(data data.Length SocketFlags.None); socket.Receive(receiveBuffer); Console.WriteLine(Encoding.ASCII.GetString(receiveBuffer)); } } } catch (SocketException socketException) { Console.WriteLine(""Socket Exception : "" socketException.Message); throw; } } Next time try including the exception message to explain what actually went wrong."
182,A,File Descriptor Sharing between Parent and forked Children I am a beginner in network programming.. i follow stevens and implementing UDP server with reliability. i have a question. when a child is forked what happens to parent socket fd in child ...? i read that they get inherited that means we have two copies of socket fd in parent and child ?? do we need to close the inherited parent socket fd in child (stevens code doesnt do that) how does one close parent fds in child (using close () ??) but not the socket on which the client request arrived ( i should keep this open)? If i am being naive..please excuse me. Thanks for the help !! File descriptors are indeed 'inherited' when forking but only with respect to what socket they're connected to and closing the file descriptor will only close the socket if it's the last one associated with that socket (or file if we're dealing with files). What you usually do is you establish the socket and then you fork. In the parent process (the one where fork returned non-zero) you can go ahead and close the file descriptor using close(fd) if you don't you'll eventually run out of file descriptors in the parent process. This is for stream (e.g. TCP) sockets where you have one server socket listening for connections and one socket per established connection. However you're using UDP so there is in fact only one socket and if you intend to keep using it in the parent process you'll need to figure out how to share it between parent and child process. Both can continue using it but it'll be almost random who reads what and in what order stuff is sent. In this case you usually have some kind of multiplexing process which receives the packets and forwards them to the appropriate child (per some other mechanism such as pipes or other sockets) based on some message content (in TCP it's the source ip/port and destination ip/port tuple). As Matt pointed out using shutdown will in fact render the socket unusable (usually unwritable but you can specify this) for all involved. In TCP this could trigger the sending of a FIN-packet effectively initiating the tear-down of the connection but you're still able to receive data until the remote end acknowledges the FIN.
183,A,"How broadcast ping works? Hi Can someone tell me why when I send broadcast ICMP Request only router send me ICMP Reply? Even if I modify MAC destination of this ICMP Request (original is FF:FF:FF:FF:FF:FF) to MAC specific host (IP still broadcast that local network ...it still don't send me Reply. Why? From RFC1122: ""An ICMP Echo Request destined to an IP broadcast or IP multicast address MAY be silently discarded"" MS Windows usually discard broadcast ping. Check that your computers are really running MS Windows. Is there a way to bypass this? Modify some fields in whole ethernet packet or sth? @Saint_pl looks like there no way read here http://www.openg.info/entry/broadcast-ping-windows-lan @Saint_pl why do you need it? I write app that's looking for sniffer in my LAN. Of course I can test each host singly but I'd like to do this at one go."
184,A,10013 error (AccessDenied) on Silverlight Socet application I am writing silverlight 3 application which is working on network. It works like client-server application. There is WinForm application for server and silverlight application for client. I use TcpListener on server and connect from client to it with Socket. In local network it works fine but when I try to use it from internet it don't connect to server. I use IP address on local network and real IP with port number for internet version. I get error 10013 AccessDenied. Port number is correct and access policy exist. Firewall is turned of. Where is the problem? Thanks. The problem was that computer where hosted website hasn't real IP address and I couldn't connect to it from internet. I added Real IP address and everything works fine now.  Have you tried to check the general availability of the connection from your machine to the Internet server with telnet ? No I havn't. I don't know how to do that. Can you explain how to check?
185,A,"Difference between Internal IP Address and External IP Address Can anyone tell me what is difference between Internal IP Address and External IP Address? How to get both in any programming language like Java C# or Adobe AIR? For external IP address in Java look here: http://stackoverflow.com/questions/2939218/getting-the-external-ip-address-in-java What about internal IP? How to get it in Java? See Dan's answer Its C# I'd asked you for Java :) Internal IP address is the address from your network: IPHostEntry heserver = Dns.GetHostEntry(Dns.GetHostName()); IPAddress curAdd = heserver.AddressList[0]; curAdd.ToString(); Your external IP address is the address from your ISP string ip = new System.Net.WebClient() .DownloadString((""http://www.whatismyip.com/automation/n09230945.asp"")); Thanks a lot Dan Andrews. So how can we retrieve the External IP using C#? added the code to that too. Thanks Dan Andrews. And tell me also that how to get Computer's Name in C#??? string hostName = Dns.GetHostName();  You can use the following code (in java) to get the local IP address: public String getLocalIpAddress() { try { for (Enumeration en = NetworkInterface.getNetworkInterfaces(); en.hasMoreElements();) { NetworkInterface ni = en.nextElement(); for (Enumeration enumIpAddr = ni.getInetAddresses(); enumIpAddr.hasMoreElements();) { InetAddress inetAddress = enumIpAddr.nextElement(); if (!inetAddress.isLoopbackAddress()) { //ignore 127.0.0.1 return inetAddress.getHostAddress().toString(); } } } } catch (SocketException ex) { } return null; } Thanks a lot MByd. Please give me a code for geeting machine name in java too...."
186,A,Where can find examples and tutorials for Perl's Net::Pcap module? Could someone provide a good documentation / tutorial/ PDFs/ reference to book link about Net::Pcap in addtion to the module documentation and this Perl and Net::Pcap article on PerlMonks? See Chapter 2 materials for Programming the Network with Perl. See also an example included in the distribution.  If you just want a callback for every TCP/UDP/etc. packet matching a filter you might find Net::Pcap::Easy more convenient. You can man tcpdump for documentation on libpcap filter strings.
187,A,Non blocking file descriptors Unix Network Programming I want to ask what are the cases when do we need to use Non blocking flag on file/socket descriptors means instead we can always use select function call to determine the ready descriptor. This is reference to program in section 16.2 of Unix Network Programming V1. In that program why does the author sets non blocking flag on the 3 descriptors stdinstdout socket. He also says that EWOULDBLOCK never occurs. But he also says that the time decreases from 12.3 to 6.9 seconds. Not all of us have the book available. Please describe what programs' run times are being measured -- and on what kind of system. On Unix and he measuring the tranfer of a file basically a echo client. What kind of Unix and what hardware was used? I ask this because I suspect that the difference may have less to do with the programs' respective control flows than with the overhead of system calls. NO it has nothing to do with hardware/software. Its a general concept. I am just asking Why would I call read when that descriptor is not ready (I know by select()) so I wouldn't anyway block if that descriptor is ready ( returned by FD_ISSET). Files: Its good to open devices such as modems that need some time to initialize with a non blocking FD (aka O_NONBLOCK). This holds true not just for modems but many kinds of character devices that need to construct themselves in order to be suitable for use or where the device is likely to block prior to signaling itself as 'ready' for some other reason. Various QRNGs (Quantum Random Number Generators) also need this flag as well as various types of lighting controllers that must seek peers prior to signaling 'ready!'. Additionally as some user space file systems provide an ioctl() interface that is known to block for a few seconds .. you may or may not want to pass a non-blocking FD in that case depending on how detrimental sleep may be for the timing that you allowed. Socket: When you wish to use non-blocking I/O while a single threaded 'butler' tends to many guests. A common misconception here is thinking that block free means lock free by some mystical means. It doesn't. In fact either is usually exclusive of the other. I am asking difference between using select / setting Non blocking flag. Is there some simple application? U have mentioned complex applications.
188,A,"Managing Cisco programatically; Telnet vs SNMP? I was recently approached by a network-engineer co-worker who would like to offload his minor network admin duties to a junior-level helpdesk tech. The specific location in need of management acts as an ISP for tenants on its single-site property so there's a lot of small adjustments being made on a daily basis. I am thinking it would be helpful to write him a winform app to manage the 32 Cisco devices on-site. I'd like to initially provide functionality which could modify access control lists port VLAN assignments and bandwidth limitations per VLAN... adding more to the list as its deemed valuable. My initial thought was to emulate a telnet session with the network device; utilizing my network-engineer's familiarity with the command-line / IOS interaction. Minimal time would be required to learn Cisco IOS conventions myself. Though while searching for solutions it appears that most people favor SNMP. That or their specific circumstances pushed them in the direction of SNMP. I wanted to know if I've overlooked an obvious benefit of SNMP. Should I be using SNMP? Why or why not? SNMP has quite a significant CPU hit on the devices in question compared to telnet; I'd recommend telnet wherever possible. (As stated in a previous answer the IOS XR XML API would be nice but as far as I know IOS XR is only deployed on high-end carrier grade routers). In terms of existing configuration management systems two commercial players are HP Opsware and EMC Voyence. Both will probably do what you need. I'm not aware of many open source solutions that actually support deploying changes. (RANCID for example only does configuration monitoring not pre-staging and deploying config changes). If you are going to roll your own solution one thing I would recommend is sitting down with your network admin and coming up with a best-practice deployment model for the service he's providing (e.g. standardised ACL QoS queue and VLAN names; similar entries in ACLs that have the same function for different customers etc.). Ensure that all the existing deployed config complies with this BP before you start your design it will make the problem much more manageable. Best of luck.  Sidenote: before you reinvent the wheel writing another service provisioning system/network management system try looking for existing ones. I know quite a lot of commercial solutions of various degrees of flexibility/functionality but I am sure there are quite a lot opensource ones. You're probably right. But here curiosity has convinced me to make an honest effort.  I've found SNMP to be a pain for management. If you just need to grab a little data it's great; if you need to change things or use if heavily it can be very time consuming. In my case I'm comfortable with the CLI so a Telnet approach works well. I've written some Python scripts to perform administrative tasks on various pieces of network gear using Telnetlib  SNMP is great for getting information out of a Cisco device but is not very useful controlling the device. (although technically you can push a new config to a Cisco IOS device using a combination of SNMP and TFTP. But sending a whole new config is a pretty blunt instrument for controlling your router or switch). One of the other commenters mentioned the Cisco IOS XR XML API. It's important to note that the IOS XR XML API is only available on devices that run IOS XR. IOS XR is only used on a few of Cisco's high end carrier class devices so for 99% of all Cisco routers and switches the IOS XR XML API is not an option. Other possibilities are SSH or HTTP (many Cisco routers switches AP etc. have an optional web interface). But I'd recommend against either of those. To my knowledge the web interface isn't very consistent across devices and a rather surprising number of Cisco devices don't support SSH or at least don't support it in the base license. Telnet is really the only way to go unless you're only targeting a small range of device models. To give you something to compare against Cisco's own CiscoWorks network management software uses Telnet to connect to managed devices. Furthermore the Cisco web interface has historically been a ripe field for security issues and I fr one recommend disabling it by default unless you absolutely must have it enabled.  SNMP isn't bad but it may not be able to do everything you need it to do. Depending on the library you use and how it hides the details of interacting with SNMP you may have a hard time finding the correct parts of the MIB to change and even knowing what or how to change them to do what you want. One reason not to use SNMP is that you can do all the configuration you need using the IOS XR XML API. It could be a lot easier to bundle up the commands you want to send to the devices using that than to interact with SNMP.  I wouldn't use SNMP instead look at a little language called 'expect'. it makes for a very nice expect/response processor for these routers.  Another vote for expect. Also you don't want to allow configuration of your firewalls via either telnet or SNMP - ssh is the only way to go. The reason is that ssh encrypts its payload and will not expose the privileged management credentials to potential interception. If for some reason you cannot use ssh directly consider connecting up an ssh-enabled serial console server to the firewall's console port and configuring it that way.  Cisco has included menu options for helpdesk applications. Basically you telnet to the box and it presents a nice clean menu (press 1 2 3). For more info check this link: http://www.cisco.com/en/US/docs/ios/12_2/configfun/command/reference/frf001.html#wp1050026  I have done a reasonable amount of real world SNMP programming with Cisco switches and find Python on top of Net-SNMP to be quite reasonable. Here is an example via Google books of uploading a new Cisco configuration via Net-SNMP and Python: Cisco Switch Upload via Net-SNMP and Python. I should disclose I was the co-author of the book referenced in the link. Everyone's milage may vary but I personally do not like using expect and prefer to use SNMP because it was actually designed to be a ""Simple Network Management Protocol"". In a pinch expect is ok but it would not be my first choice. One of the reasons some companies use expect is that a developer just gets used to using expect. I wouldn't necessarily chock up bypassing SNMP just because there is an example of someone automating telnet or ssh. Try it out for self first. There can be some truly horrible things that happen with expect that may not be obvious as well. Because expect waits for input under the right conditions there be very subtle problems that are difficult to debug. This doesn't mean a very experienced developer can't develop reliable code with expect but it something to be aware of as well. One of the other things you may want to look at is an example of using the multiprocessing module to write non-blocking SNMP code. Because this is my first post to stackoverflow I cannot post more then one link but if you google for it you can find it or another one on using IPython and Net-SNMP. One thing to keep in mind when writing SNMP code is that it involves reading a lot of documentation and doing trial and error. In the case of Cisco the documentation is quite good though."
189,A,"C: socket connection timeout I have a simple program to check if a port is open but I want to shorten the timeout length on the socket connection because the default is far too long. I'm not sure how to do this though. Here's the code: #include <sys/socket.h> #include <sys/time.h> #include <sys/types.h> #include <arpa/inet.h> #include <netinet/in.h> #include <errno.h> #include <fcntl.h> #include <stdio.h> #include <netdb.h> #include <stdlib.h> #include <string.h> #include <unistd.h> int main(int argc char **argv) { u_short port; /* user specified port number */ char addr[1023]; /* will be a copy of the address entered by u */ struct sockaddr_in address; /* the libc network address data structure */ short int sock = -1; /* file descriptor for the network socket */ if (argc != 3) { fprintf(stderr ""Usage %s <port_num> <address>"" argv[0]); return EXIT_FAILURE; } address.sin_addr.s_addr = inet_addr(argv[2]); /* assign the address */ address.sin_port = htons(atoi(argv[2])); /* translate int2port num */ sock = socket(AF_INET SOCK_STREAM 0); if (connect(sock(struct sockaddr *)&addresssizeof(address)) == 0) { printf(""%i is open\n"" port); } close(sock); return 0; } What platform are you on? I'm using linux You added in your answer ""fcntl(sock F_SETFL O_NONBLOCK)"" Note that after this the next socket read becomes also nonblocking! This article might help: http://developerweb.net/viewtopic.php?id=3196 . Looks like you put the socket into non-blocking mode until you've connected and then put it back into blocking mode once the connection's established. Nice. This is a better suggestion than mine. This is a very good answer why did you make it community wiki? You should earn some reputation for suggesting the resource. The forum linked to seems to have changed their software so the link is dead now.  If you want to avoid changing an OS-wide setting for all applications as some are suggesting (it's kind of rude to other processes to alter global state and behaviors like that)... You can do the connect() in a different thread then have your original thread use poll() (POSIX) or WaitForMultipleObjects() (Windows) to wait for either (1) the connect thread to signal that it finished or (2) your timeout.  Set the socket non-blocking and use select() (which takes a timeout parameter). If a non-blocking socket is trying to connect then select() will indicate that the socket is writeable when the connect() finishes (either successfully or unsuccessfully). You then use getsockopt() to determine the outcome of the connect(): int main(int argc char **argv) { u_short port; /* user specified port number */ char *addr; /* will be a pointer to the address */ struct sockaddr_in address; /* the libc network address data structure */ short int sock = -1; /* file descriptor for the network socket */ fd_set fdset; struct timeval tv; if (argc != 3) { fprintf(stderr ""Usage %s <port_num> <address>\n"" argv[0]); return EXIT_FAILURE; } port = atoi(argv[1]); addr = argv[2]; address.sin_family = AF_INET; address.sin_addr.s_addr = inet_addr(addr); /* assign the address */ address.sin_port = htons(port); /* translate int2port num */ sock = socket(AF_INET SOCK_STREAM 0); fcntl(sock F_SETFL O_NONBLOCK); connect(sock (struct sockaddr *)&address sizeof(address)); FD_ZERO(&fdset); FD_SET(sock &fdset); tv.tv_sec = 10; /* 10 second timeout */ tv.tv_usec = 0; if (select(sock + 1 NULL &fdset NULL &tv) == 1) { int so_error; socklen_t len = sizeof so_error; getsockopt(sock SOL_SOCKET SO_ERROR &so_error &len); if (so_error == 0) { printf(""%s:%d is open\n"" addr port); } } close(sock); return 0; }"
190,A,Load and stress testing a network app A client has a piece of hardware - it doesn't really matter what it does. 150 of them can connect to a PC and up to 150 PCs can interface to the single server. These are his figures and I and not sure if I want to query them. At his premises he has a largish room with 150 h/w devices mounted on the walls - and testing consists of a lot of guys trying to push buttons all at once. He has seen the light and wants to have a software simulation. So how best to proceed? Automated script-based testing I would imagine. His guys code in C so I might do the tests in C++ and use CPPunit. Can a single test PC simulate 150x150 hardware deceives? S'pose it depends on how much traffic they are sending. Should I parameterize it to drive how many units to simulate what operations they should perform etc? Should I slap a GUI on the top to impress management? I could control the number of simulated units with a slider display system load on a histogram - you low pictures ... management ... Any suggestions for this sort of thing? Gotchas? Good URLs books? I know it might sound vague but I am sure that there are some general guidelines for this sort of thing. Be very clear what you are testing. Sounds like currently they test 150 devices 1 (or a few) PCs and one server. That's a long way from 150x150devices 150 PCs and 1 server. So is the goal to stress the server the PC? To determine behaviour under stress? To exercise particular paths? Sounds like you intend to build some software to run on PC (LoadProducer) that shall emulate the devices and fire the LoadProducer against the real PC. Hopeing to emulate people pressing buttons. I think that this potentially has a lot of value. As to bells and whistles my advice is first to get a simple command/file driven version going and then consider whether you can afford to build the UI. UIs are effort sinks if you don't get the function right first you'll allow your attentions to be diverted towards prettyness. I would not be too concerned about the number of devices you can emulate surely it's many if it's not 150 then presumably you just use several LoadProducers. If I really wanted to exercise the PC code then I might also want to emulate the server or create a test version of the server code that can do amusing things such as go really slow or send spurious answers. sorry for the delay. SO was having a lot of problems when I tried to +1 and post a reply earlier. Thanks - some good advice there. I will 1) test the PC with a dummy server and dummy devices 2) test the server with dummy PCs 3) test the system with real server and PCs and dummy devices. Do you have any advice as to how others do this in real life? Any recommended books or URLs?
191,A,How to limit user login to specific machine? I am developing web application in ASP.NET and C# in which a user should get login access from a specific machine. It will not allowed for the user to login from another machine. How to limit the user by a specific machine? Can I do this in ASP.NET C# or in a Java applet? Hello @bzlm  In our application there is Reseller & workers. workers can not access accounts from own PC. If workers leaved the job at that time Reseller doesnt know. and Reseller have credits to share amount. I want that workers should not access their login in the system from another machine.. Duplicate of [Web application where users can only register and access from one PC](http://stackoverflow.com/questions/6564930/web-application-where-users-can-only-register-and-access-from-one-pc). Abhishek this question is essentially identical to your earlier one. Either improve that one or ask a different question. @Michael that question is beyond improvement. :) @bzlm: Maybe. Posting an identical question isn't the right path though. @Michael let's help Abhishek remove the old question then. :) @Abhishek why do you want to do this? What does it matter to you whether a user accesses your web application from multiple machines? Rather than locking access down to a specific MAC address I'd recommend using certificate-based authentication. Have a trusted administrator install a certificate on the user's machine (in the local machine store) and ensure it's not marked as exportable. This article may help in terms of setup. http://support.microsoft.com/kb/315588 MAC addresses can be easily spoofed and only offer a degree of obfuscation. @Steve_Morgan Thanks. I will try and back to you. Hello @Steve_Morgan How I can do with certificate based authentication.?
192,A,"paket drop and splits in udp tunnel Currently I am working on video conferencing project.For this i m using pwnat for nat traversing. pwnat is based on udp tunneling.I m using the TCP connection for data transmission. My problem is that when i send a packet  it does not reach properly at its destination side . Sometime it drops the packet and many times it breaks ( split ) the packet into pieces. Please Help me .. How can i send and recieve a packet into single piece. So i can draw image properly and play sound. Any kind of help will be appriciated . Thanks in advance It would be my guess that tunneling TCP with UDP would defeat the point of using TCP and eliminate the possibility of any real sequential protocol but that is just a guess. TCP has no concept of packets. A TCP stream is a continuous stream of bytes - if you want a structure within that stream of bytes you have to impose it yourself by implementing some kind of framing mechanism. A simple one is the ""length prefix"" - when sending an application-level frame you first send the length of the frame then the data. Thanks for reply I don't know much about networking becoz m new in n/w programming.By review my log files.I found that before send data data length is 16384 bytes and at recieving end stream length is 11300 and next stream of 5084 bytes.But sometime it recieve full length data of 16384 bytes. this may be happened because pwnat is working on udp over tcp and it is based on the concept of udp tunneling . @SR Dusad: TCP **does not** have an concept of individual messages within a stream. If you send 16384 bytes you will eventually receieve those 16384 bytes - but you might recieve them in any number of chunks of any size. Your application has to handle that. Thanks caf : your answer help me a lot for understanding the concept"
193,A,"Web service time out errors in Delphi I have a client application that makes SOAP requests. I have set the timeout to 20 minutes. However sometimes I see the timeout error occurring after 10 seconds. I have the following in code: RIO.HTTPWebNode.ReceiveTimeout := 1200000 Do I need to set the ConnectTimeout and SendTimeOut? Currently they are set to the default values of 0. What difference would setting these make? I am using Delphi 2007. Looking further at the error message I see I get ""The operation timed out...."". So should I be setting my ReceiveTimeOut to zero since I really do not want any timeout at all? We are currently sending large amounts of data to the server should I just set ReceiveTimeOut to 0? However that does not still explain with I get ""occasionally"" a time out after 10 secs... CodeGear's SOAPHTTPTrans implementation sets timeouts globally not per session. Here's the relevant code from THTTPReqResp.Send: { Timeouts } if FConnectTimeout > 0 then Check(not InternetSetOption({Request}nil INTERNET_OPTION_CONNECT_TIMEOUT Pointer(@FConnectTimeout) SizeOf(FConnectTimeout))); if FSendTimeout > 0 then Check(not InternetSetOption({Request}nil INTERNET_OPTION_SEND_TIMEOUT Pointer(@FSendTimeout) SizeOf(FSendTimeout))); if FReceiveTimeout > 0 then Check(not InternetSetOption({Request}nil INTERNET_OPTION_RECEIVE_TIMEOUT Pointer(@FReceiveTimeout) SizeOf(FReceiveTimeout))); What I've had to do to is use the OnBeforePost handler to set the timeouts: transport.OnBeforePost := configureHttpRequest; procedure Tsomething.configureHttpRequest(const HTTPReqResp: THTTPReqResp; Data: Pointer); begin InternetSetOption(Data INTERNET_OPTION_CONNECT_TIMEOUT Pointer(@FconnectTimeoutMS) SizeOf(FconnectTimeoutMS)); InternetSetOption(Data INTERNET_OPTION_SEND_TIMEOUT Pointer(@FsendTimeoutMS) SizeOf(FsendTimeoutMS)); InternetSetOption(Data INTERNET_OPTION_RECEIVE_TIMEOUT Pointer(@FreceiveTimeoutMS) SizeOf(FreceiveTimeoutMS)); end; The MSDN documentation for these options is found at http://msdn.microsoft.com/en-us/library/aa385328%28VS.85%29.aspx Thank you so much glob. Any ideas why I would get a timed out operation even after a few seconds? no not 10 seconds. the default timeout varies with the installed IE version -- with IE7 it's 30 seconds. and as the default code sets the timeout globally you may hit problems should other code change the timeout. i always explicitly set the timeout as per the code in my answer."
194,A,Multi-thread serialization in C# Problem: I have large number of big messages to serialize and send over network. I would like to maximize performance so I'm thinking about creating multiple threads for message serialization and one thread for sending the data. Idea is to dynamically determine number of threads for serialization based on network performance. If data is sent fast and serialization is bottleneck add more threads to boost serialization. If network is slow use less threads and stop completely if send buffer is full. Any ideas how to do that? What would be the best algorithm to decide if more or less threads are needed? How to correctly concatenate serialization results from multiple threads? Please answer to any of this questions? Thanks You mention performance being a concern - it isn't really a direct answer to the threading stuff but I'd be very interested in knowing which serializer you're currently using... In particular I'd be looking at using protobuf-net (free - but disclosure: I wrote it) which is significantly faster (CPU) and smaller (IO/bandwidth) than any of the built in serializers. well for most common scenarios on the main .NET framework the trunk is stable. I can't promise a specific date as I have other (non-coding) commitments. But also: v1 works pretty robustly too... I'm using BinaryFormatter at the moment. I planned switching on protobuf. Any estimate when will V2 be production ready? You can chunk the data into packets and place the packets into a queue. The process that looks at network performance would look at the queue and decide how many packets to send. The downside to this implementation is that you will need to assemble the packets on the receiving end where they may not be received in the proper order.  It can be treated as a Producer/Consumer problem In Fx4 you can use a BlockingCollection. But frankly I would expect the (network) I/O to be the bottleneck not the serialization. You will have to measure.
195,A,"How to get the MAC address prior to connection? I have a situation where I ping a range of IPs in the network. Then I try to connect to the successful pings. My aim is to connect to specific equipment that has a specific MAC prefix. For example when I ping a range of 100 IPs I might get 20 replies. These replies include computers printers and possibly the hardware I am trying to connect. Currently what happens is that when I try to connect to anything other than the hardware I would like (i.e computer printer) I get a timeout connection. This is fine however it is not efficient. I would like to filter out the successful ping list by using the MAC address however I have not yet been able to find a solution that allows me to seek a MAC address prior to connecting the hardware. I have looked through most the MAC questions on here but none fit my needs. Any ideas?? possible duplicate of [How do I access ARP-protocol information through .NET?](http://stackoverflow.com/questions/1148778/how-do-i-access-arp-protocol-information-through-net) I tried the solution there however that only returns what ""arp -a"" in the command prompt returns. That is not suitable for me because the arp table only contains MAC addresses of IPs that the PC is cnnected to. I was able to find the solution here: http://pinvoke.net/default.aspx/iphlpapi/SendARP.html The following method returns the MAC internal static string GetMAC(string ip) { IPAddress dst = IPAddress.Parse(ip); // the destination IP address Note:Can Someone give the code to get the IP address of the server byte[] macAddr = new byte[6]; uint macAddrLen = (uint)macAddr.Length; if (SendARP((int)dst.Address 0 macAddr ref macAddrLen) != 0) throw new InvalidOperationException(""SendARP failed.""); string[] str = new string[(int)macAddrLen]; for (int i = 0; i < macAddrLen; i++) str[i] = macAddr[i].ToString(""x2""); return string.Join("":"" str); //Console.WriteLine(string.Join("":"" str)); }"
196,A,Java & Google Protocol Buffers: I need an example of how to use Google Protocol Buffers with client-server communication Looking for the best example you guys know of. The examples on the site are with files. But the library should be able to work over a Java.net.ServerSocket at least I hope. You can certainly write protocol buffers to an OutputStream and read them from an InputStream. The Protocol Buffers project itself hasn't specified an RPC protocol but third parties have done so. Alternatively if you're simply interested in transmitting a message you should be able to wrap that message in any protocol which allows you to either specify a byte array and retrieve it at the other end or which gives you a stream-based API. If you need an example using a specific RPC technique listed in the third party add-ons page I suggest you look at the home of that particular project. Hi Jon can you help my question: http://stackoverflow.com/questions/7606051/google-protocol-buffers-in-client-server-when-have-different-packet-type
197,A,"Rewriting Binary Streams using Java I have been studying Netty and Mina but am confused as to the best way to rewrite binary streams. For example I would like to create a proxy that will allow for replacement of XML and forward along. Examples appreciated. Could you please be more precise? Do you want to write a Java class that takes a stream provides a stream and in between manipulates the data? BTW: XML is plain text not binary data. I think you're thinking at too low of a level. XML is not so much ""binary"" as it is an abstraction on top of binary. If you want to replace snippets of XML as they come across your line you'll have to poke into the payload portion of the packets and look for patterns of XML.. a simple way is to use a regular expression after rebuilding the bytes into content temporarily. Once you have this search and you have matched what you want you can replace what you want to replace and re-send. The hard part of this is that you will likely need to cache some input before it leaves your machine so that you are able to find the beginning and end of what it is you are searching for. What makes this difficult is that often times you don't know what constitutes the ""beginning"" and the ""end"" of a data payload."
198,A,Network programming with C# I have a project to do which is packet monitoring. I want to capture each packet receive by specified network interface. I want to know what I should start with to do my project .... should I learn socket programming first or what? should I learn how to use winpcap under C# or just do my own function. please advice me from where I should start!!! I know there are many code in C# out there in the internet for this but I want to learn it by myself but I do not know from where to star! Any packet capturing framework will make you specify the interface that's being sniffed to that's taken care of. Then download wireshark and get a feel for what types of traffic are send across the net and how to filter them to narrow your results. Socket programming is generally for writing point-to-point application-level communications applications. You sould definitely learn the basics of socket programming first so you understand a little bit about what's going on. Next I would look into programming using WinPCap which is a packet capturing library. It's not built directly for .NET but there are I believe .NET wrappers or if nothing else you could call the functions by using COM Interop.  I recommend using Pcap.Net. It is a very active .NET wrapper for WinPcap wrapper with a vast packet interpretation library. It has great quality and performance.  I agree using WinPcap is probably a good idea. Specifically look at SharpPcap. it is actively maintained. In fact there was a release today. +1 for SharpPcap. I've used it developed it and like it a lot.
199,A,"How to get IP all hosts in LAN Hello I need list IP addresses of all connected hosts in my LAN. What is the simpliest way to do this? Define connected - you mean like active? Are they firewalled or can they be pinged? Yes active. They can be firewalled but not necessarily - I don't know because I'm user not admin. What's the environment this is in? Do you have a managed switch that does SNMP? If so you can just query the switch to see who's active. If its a little home router you're out of luck. Instead of arranging a wild ping-party you could possibly (I really don't know exactly!) use NetBIOS (request the netbios nametable somehow?) http://technet.microsoft.com/en-us/library/cc757216%28WS.10%29.aspx. Or why not ask your DHCP server? DHCP server will unlikely return ACTIVE. You only know recently active - still have to ping and you may miss some stuff statically configured. yeah the only way to be sure would be to search your network switch and then follow all the cables..  You would need to build an address range (e.g. 192.168.0.1 - 192.168.255.254) and ping each of those addresses. If a response is received then that host is active. Asynchronous Ping Tutorial: http://www.geekpedia.com/tutorial234_Asynchronous-Ping-using-Csharp.html However some newer operating systems block ping requests (ICMP). This would need to be disabled in the firewall on each computer for you to receive a response.  You could ping the adress range and note if a host responded. Of course this will require the host to respond to the ping packets. Pinging e.g 255 hosts is kind of time-consumming-I think so. Is there a faster method? a second is not time consuming. Just hit off all 255 pings at the same time not one after another waiting for timeouts. How to do this faster? Even if I ping 10 hosts it take several sec.  Ping to the global broadcast ip address (255.255.255.255) Normally we will get the reply from all the host connected to the LAN.. Or if you have any particular network for the lan ping to that networks broadcast id (i.e the last ip address in that range ). Then you can know all the ip address connected to the host in lan.  You'll have to do a ping sweep. There's a Ping class in the System.Net namespace. Example follows. Also this is only possible if your computers don't have firewalls running. If they've got a firewall enabled there's no way to determine this information short of doing SNMP queries on your switches. System.Net.NetworkInformation.Ping p = new System.Net.NetworkInformation.Ping(); System.Net.NetworkInformation.PingReply rep = p.Send(""192.168.1.1""); if (rep.Status == System.Net.NetworkInformation.IPStatus.Success) { //host is active } The other issue is to determine how large your network is. In most home situations your network mask will be 24 bits. This means that its set to 255.255.255.0. If your gateway is 192.168.1.1 this means that valid addresses on your network will be 192.168.1.1 to 192.168.1.254. Here's an IP Calculator to help. You'll have to loop through each address and ping the address using the Ping class and check the PingReply. If you're just looking for the information and aren't concerned with how you get it you can use NMap. The command would be as follows nmap -sP 192.168.1.0/24 EDIT: As far as speed goes since you're on a local network you can cut down the timeout interval considerably as your machines shouldn't take more than 100 milliseconds to reply. You can also use SendAsync to ping them all in parallel. The following program will ping 254 address in under half a second. using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Net.NetworkInformation; using System.Diagnostics; using System.Net; using System.Threading; using System.Net.Sockets; namespace ConsoleApplication1 { class Program { static CountdownEvent countdown; static int upCount = 0; static object lockObj = new object(); const bool resolveNames = true; static void Main(string[] args) { countdown = new CountdownEvent(1); Stopwatch sw = new Stopwatch(); sw.Start(); string ipBase = ""10.22.4.""; for (int i = 1; i < 255; i++) { string ip = ipBase + i.ToString(); Ping p = new Ping(); p.PingCompleted += new PingCompletedEventHandler(p_PingCompleted); countdown.AddCount(); p.SendAsync(ip 100 ip); } countdown.Signal(); countdown.Wait(); sw.Stop(); TimeSpan span = new TimeSpan(sw.ElapsedTicks); Console.WriteLine(""Took {0} milliseconds. {1} hosts active."" sw.ElapsedMilliseconds upCount); Console.ReadLine(); } static void p_PingCompleted(object sender PingCompletedEventArgs e) { string ip = (string)e.UserState; if (e.Reply != null && e.Reply.Status == IPStatus.Success) { if (resolveNames) { string name; try { IPHostEntry hostEntry = Dns.GetHostEntry(ip); name = hostEntry.HostName; } catch (SocketException ex) { name = ""?""; } Console.WriteLine(""{0} ({1}) is up: ({2} ms)"" ip name e.Reply.RoundtripTime); } else { Console.WriteLine(""{0} is up: ({1} ms)"" ip e.Reply.RoundtripTime); } lock(lockObj) { upCount++; } } else if (e.Reply == null) { Console.WriteLine(""Pinging {0} failed. (Null Reply object?)"" ip); } countdown.Signal(); } } } EDIT: After some use of it myself I modified the program to output a count of how many IPs responded. There's a const bool that if set to true will cause the program resolve the host names of the IPs. This significantly slows down the scan though. (under half a second to 16 seconds) Also found that if the IP address is incorrectly specified (made a typo myself) the reply object can be null so I handled that. @Tim do you know if Nmap has an API? Just for notice RFC1918 says that these ranges are all valid for LANs: 10.0.0.0 – 10.255.255.255 172.16.0.0 – 172.31.255.255 192.168.0.0 – 192.168.255.255 @Brad not sure about a full API but its designed in the spirit of a regular UNIX program so you can easily execute it via the command line and capture Standard Output. There are options to format the via XML as well as a GREPable format (IE use with regex). For info on how to execute a command and capture the output in c# see this answer. http://stackoverflow.com/questions/3258704/issues-about-files-in-use-get-the-name-of-another-process-that-use-file/3258779#3258779 @Saint_pl does this make sense? Do you need anything explained? There is no reason that multiple independent IP-subnets could share the same physical LAN so this kind of process is limited (and will not distinguish LANs linked by switches). And that's before considering VLANs. The real answer to the Q is ""in general there isn't such a method"". In practice pinging every address that is reachable without hitting the gateway (i.e. where this-IP & mask == other-IP & mask) will work unless the other nodes do not respond to pings. @TimCoker yes it works but if I'm pinging e.g. 255 hosts it takes several sec. Can I accelerate this? @Saint_pl see my edit about running async Its amazing code its more of art. Made slight changes according to my Prefs n now its running like Strom."
200,A,"Real-time Transport Protocol is not used in youtube? I came across reading about real time protocol in Wikipedia which mentions the following : ""RTP is used extensively in communication and entertainment systems that involve streaming media"" I was curious about this protocol and wanted to see this in wireshark. I thought youtube.com might be using RTP when running videos but was surprised to see that only TCP packets are being sent when a video is being played. Can someone please tell another free website which implements RTP so that I will be able to see it in wireshark. (I am actually wanting to explore network optimization opportunity in my server applications by using RTP since it is ok to loose a few packets) List of few sites: http://www.ehow.com/list_6774412_list-rtsp-sites.html  According to Computer Networks RTP is the payload of UDP (or TCP) as the book indicates. Here is a picture from the book: According to WireShark's wiki only RTP on UDP could be detected by WireShark. (Thanks to Ralf) Wireshark recognises RTP packets at least over UDP. IIRC the TCP wasn't implemented a couple of months ago but I'm not sure what the latest status is. @Ralf according to wiki.wireshark still not. it can sometimes at least v. 1.4.0. It was recognizing (sometimes!) RTP packets in RTSP interleaved session @Andy T really? Maybe you could post a new answer. this info is not relevant to the question  Youtube uses HTTP AFAIK. Also keep in mind that RTP can be sent over UDP as well as TCP. An RTSP server can be used to start an RTP media session. I don't know any public servers but another option would be to download the live555 RTSP server. There are also some example media files. Then all you need to do is build the media server application as well as the openRTSP client and use the client app to connect to the server for the stream. The client can request RTP over UDP TCP etc. Alternatively you could also use Darwin Streaming Server as an RTSP server."
201,A,"Scala Remote Actors - Pitfalls While writing Scala RemoteActor code I noticed some pitfalls: RemoteActor.classLoader = getClass().getClassLoader() has to be set in order to avoid ""java.lang.ClassNotFoundException"" link doesn't always work due to ""a race condition for which a NetKernel (the facility responsible for forwarding messages remotely) that backs a remote actor can close before the remote actor's proxy (more specifically proxy delegate) has had a chance to send a message remotely indicating the local exit."" (Stephan Tu) RemoteActor.select doesn't always return the same delegate (RemoteActor.select - result deterministic?) Sending a delegate over the network prevents the application to quit normally (RemoteActor unregister actor) Remote Actors won't terminate if RemoteActor.alive() and RemoteActor.register() are used outside act. (See the answer of Magnus) Are there any other pitfalls a programmer should be aware of? I'm currently up against the ""RemoteActor.classLoader = ..."" pitfall but even though I need this line *somewhere* I can't work out where! Can anyone point to an explanation of this? Here's another; you need to put your RemoteActor.alive() and RemoteActor.register() calls inside your act method when you define your actor or the actor won't terminate when you call exit(); see How do I kill a RemoteActor?"
202,A,"Set manually timeout for DataInputStream I am establishing standart TCP connection between two stations(AB) A is sending message B recieving and sending response back and then I close the connections. Stations B is ""blackbox"" I cant access change or do anything there. Sometimes there is situation when B didnt send response back and then I need to re-try the whole process again. I want to set timeout on the reciveing time of Station A (which wait for B for an answer). So basically when the waiting time is expired i will dispatch a retry. I didnt find a way how to set a timeout for DataInputStream. (only for the whole socket connection - which I dont want) some code:  /** * Method receives the Server Response */ public byte[] receive(DataInputStream is) throws Exception { logger.debug(TAG + "" Client Recieving...""); try { byte[] inputData = new byte[1024]; // here I want to set timeout for the ""receiving mode"" is.read(inputData); return inputData; } catch (Exception e) { throw new Exception(TAG + "" Couldnt receive data from modem: "" + e.getMessage()); } } Thanks ray. Consider using a non-blocking SocketChannel instead of a DataInputStream. Example: private static final long TIMEOUT = 500; /** * Method receives the Server Response */ public <C extends SelectableChannel & ReadableByteChannel>byte[] receive(C chan) throws IOException { logger.debug(TAG + "" Client Recieving...""); try { Selector sel = Selector.open(); SelectionKey key = chan.register(sel SelectionKey.OP_READ); ByteBuffer inputData = ByteBuffer.allocate(1024); long timeout = TIMEOUT; while (inputData.hasRemaining()) { if (timeout < 0L) { throw new IOException(String.format(""Timed out %d of %d bytes read"" inputData.position() inputData.limit())); } long startTime = System.nanoTime(); sel.select(timeout); long endTime = System.nanoTime(); timeout -= TimeUnit.NANOSECONDS.toMillis(endTime - startTime); if (sel.selectedKeys().contains(key)) { chan.read(inputData); } sel.selectedKeys().clear(); } return inputData.array(); } catch (Exception e) { throw new Exception(TAG + "" Couldnt receive data from modem: "" + e.getMessage()); } }  First check out socket.setSoTimeout(timeout) Second see this discussion: Is it possible to read from a Java InputStream with a timeout? read method is blocked. The only way to check whether stream contains data without blocking is using available() method. So you can try to call this method with certain delay and exit if nothing is available after n seconds. You can write your own input stream that wraps any other input stream implementing this logic. The reference above shows how to do this."
203,A,good/flexible software for visualizing a dynamic network simulation I would like to make a simulation of a constrained system indexed by time. This involves a network of agents/nodes that interact based on some logic/relationships. I would like to place the nodes on a grid 2D or 3D does not matter. I would like to have edges drawn between them and text beside them. I would like to give the logic for the tags on the nodes and give indication of the states. In the style of simjava: simjava But more based on mathematical simulations than software processes. Graphing tools would be useful etc. Any good recommendations? There is good software for visualizing data sets. Matlab's Simulink has a lot of complexity on non-discretec events and is not as cartoon-like which is good for testing principle rather than analytics. Unless I am wrong and Simulink provides this EDIT: A similar question asks about dynamic graphs stack overflow relevant question it is an issue that does not seems to have any concrete solution except for customized solutions. May be MathGL (cross-platform GPL plotting library) can meet yours requirement. Graph visualization is not the main goal of MathGL. However it have a primitives (rectangles lines marks curved text and so on) which allow one to make a graph. Also it can handle data in 3D. And it allows interaction -- it return the object Id at selected position (at mouse click).  hmm. Not sure if I'm on the right track or not but have you looked at graphviz? It'll render graphs (including auto-layout with various different algorithms). There are bindings from various languages e.g. pydot for python. If you need graph analysis algorithms (e.g. shortest path) there's also pygraphlib. There are alternatives to graphviz e.g. protovis / infovis. Both are javascript based provide force-directed layout and render in the browser. Apologies if I misunderstood question hope that helps. thanks for the links. The problem is visible if you look at simjava and then look for something equivalent in pydot which uses graphviz. Also look at the link to a relevant question I included in the question's EDIT. It is not obvious how to make dynamic simulations and visualize the changes 'online'/'realtime' as movements on the network... ok think I see sorry for misunderstanding your question.
204,A,"Unexplainable behavior in C. Odd vs Even number of strings So I have a problem that I can not figure out. I am writing some code in C. I kept winding up with issues where reading from the network would seemly randomly work. I finally traced it down to the number of strings in the code. I cant believe it but I have verified it pretty in depth. The code base is rather massive so I am not sure of the overall number of strings parity. However I know that if i add an odd number then the program works and if i add an even number it doesnt. Just to clarify when I say it doesnt work It does still build and execute but everytime I try to read anything over the network all i get is 0's. When its working I get the correct data. has anyone ever heard of anything like this? Or have any idea what could be causing this? I could see if the data portion of the program was getting too large and starting to impede on other code's space but the fact that its an odd/even thing completely confuses me. thanks EDIT (Adding more info): The platform is a custom designed device. the code base is redboot but its been altered significantly for the custom device. snipped for example: //This will work because its an odd number of strings. char* str1 = ""test""; char* str2 = ""test2""; char* str3 = ""test3""; int i = strlen(str1) + strlen(str2) + strlen(str3); ...................................... if i were to change the last line to int i = strlen(str1) + str(len2); so that str3 gets optimized out by the compiler then the code will no longer work. I have tested this many times with various lenghts of strings all result in the same odd/even behavior. (i is just sent to a debug log so that its not optimized out. nothing fancy is done with it). Edit2: The above code can be placed anywhere in the codebase and it causes the same problem. It doesnt matter if its been executed or not which leads me to believe its not a stack overflow. your EDIT: what are you doing with ""i""? I am convinced that it may be a buffer overflow that has nothing to do with str1 to str3 they only move the problem. Should those subsequent `str(lenX)` occurrences be `strlen(strX)` ? your statement about moving the code around is ambiguous. What happens if you put the string definitions in file scope? Also you didn't answer the second question that I asked in my reply. Try `int i = 0; i += strlen(str1); i += strlen(str2); i += strlen(str3);` and stuff like that to see how it changes it. Alternatively declare the variables you're assigning the string literals to as `const char *` rather than just `char *` (no clue if that would make any difference but the fact that it doesn't matter whether or not the code is actually executed makes me wonder if it could be a compile-time/compiler problem that doesn't manifest until later). Also have you tried using OTHER numbers of strings like 4 and 5 6 and 7 etc? After two hours and two edits there still isn't anywhere near enough information to make more than a wild guess at the problem. Voting to close. David what more information can i provide? The code base is redboot which imakes it HUGE. I've answerd all questions people ask. I realize its a long shot but I was hoping maybe somehow someone has seen a problem like this before. In reply to JAB yes i have tried up to 15 strings total (using all numbers below that) and it is always the same. odd works even doesnt. @Without me Its just Aweso: no you still didn't answer my question Check if `strlen` is being overridden by a macro of some sort and also check if `stdio.h` is `#include`d in the code files just in case. It's likely some error in your code not the number of lines. You should add plenty of logging and localize the problem then (if you still need that) ask a detailed question here. Please post a short yet compilable snipper piece of code that illustrate the problem. Narrowing the problem down could event help you see that solution. Agreed. This posted needs to isolate the problem. You don't say anything about what platform you're on? If you're on a desktop system the number or ""oddness"" of your strings shound have nothing to say. If you're on some more limited embedded system that may be another issue. Most likely it's as sharptooth points out a problem with the code. what do you mean exactly with ""number of strings""? you mean the number of string literals into the source code (like `char *s = ""hello""`)? And how do you verified it pretty in depth this? Where do you take the assertion from that it has to do with the parity of the number of strings? If I try to interpret what you say carefully this tells me that small changes in code let you trigger unexpected behavior. Smells like stack overflow. Do you allocate large arrays or strings on the stack and then do read and write to them? In that case try to allocate/deallocate these large buffers dynamically through malloc/free. i wantet to post the same answer. it is called a Heisenberg-Bug: add more debug code and the error moves to another point in the program. Peter: Not a ""Heisenberg-Bug"" but a ""Heisenbug"".  Here is a guess. Assume the platform is 32 bit. Perhaps the compiler aligns some of the data structutres of your program in memory on eight byte boundaries. You have a whole load of string pointers in your data segment and maybe some other stuff too. If there is an odd number of strings the next thing that needs an eight byte alignment has four bytes of padding in front of it. If there is an even number of strings there is no padding. Whatever piece of data that is just before that eight byte aligned object has an overflow bug that just destroys the contents of between one and four bytes after it. If there is padding after this thing nothing bad happens. If there is no padding the eight byte aligned object gets zapped.  I've not heard of a problem like this before. You sound like you're very frustrated and you say that your code base is rather massive. If solving the problem is important I would suggest that you try to reproduce the issue with a smaller amount of code. It may also help you get answers here if you post some samples of your code to illustrate the question.  if you have somthing like char buf[10]; long var; strcpy(buf ""ganz viel text""); you may or may not get an segmentation violation or strange behaviour with variable ""var"". if you put more debug text into your code the linker may reallocate the variables or the compiler may do other code optimization and reallocation space allocation in memory.  Random stab-in-the-dark time... A common misunderstanding when reading from network sockets is that a read() of 10 bytes will return the next 10 bytes. It won't. It will return UP TO 10 bytes and you may need to call read() multiple times to get all the data you require."
205,A,"Qt - webkit - how to extract images out of a web page I am trying to extract the images from the given web page. I have gone through ""Simple Selector Example"" and ""DOM Traversal Example"" . But they refer text-things not image. I have made a code to get a QNetWorkReply by sending QNetworkRequest through QNetworkManager. But I can't get how to bring images using it. Qt-Webkit enlightened ones are asked to shed a light on this. I guess you must select images then. Do you know basic HTML? @phresnel I know basic html QWebElement has tagName and attributeNames you want to search for an img tag with a src attribute. You can download URLs with QNetworkAccessManager. I used ""*"" which gave me what I wanted. Now I am having the elements which are having images. But can you say how to extract the image out of these QWebElement's. Well now you have the URL you can fetch it directly. Webkit may already have downloaded and cached it -- I don't know -- but Qt has *excellent* documentation. Happy digging. Hi now I got our point clearly. I got the image's web address by the attribute ""src"". Still I don't know how to get the image out of this web address. The address is something like this ""http://**/pic12.jpg"" . I checked your solution. But QWebFrame class can give the elements which are filtered by the selector ""CSS2 selector"". The function is ""QWebFrame::findAllElements ( const QString & selectorQuery ) const"" . But this standard ""CSS2"" has only attributes related things not tag related things. Can you say the way to find the elements as you said CSS attributes are not like HTML attributes. `` img is a HTML attribute."
206,A,"Select() + UDP resulting in too many open files I currently have a select() statement configured to keep track of two UDP sockers. I send perhaps 10 - 20 messages a second at one general data socket which is this interpreted as I expected. However once I hit around 1024 messages I get the notice: talker: socket: Too many open files talker: failed to bind socket This is logical to me since ulimit -n shows a max of 1024 open files for this user. However why are there all of these open files? With UDP there is no connection made so I do not believe I need to be closing a socket each time (although perhaps I'm wrong). Any ideas? Thanks in advance. where are you creating the sockets? wait I saw your comment about not needing to close the socket since it's UDP. You still need to close it. But you can reuse it in your select like you're doing. I think it is the code you are using to send the messages that is causing the problem not the select. @SB I've added the code for the socket creation. You think it is the sending of the messages which is the issue? Hmm @SB I checked my sending code --- I close the socket after relaying a message. Which side errors out? Client or server? Why is there more than one socket? I agree with the guys it's probably socket creation bug. @Nikolai -- this code simply allows a message to continuously be relayed. Once any of the relays reaches their max open file limit this error occurs. The sending portion is what appears to be erroring out (""talker""). However talker opens a port and then closes it every time. @Pavel There is more then one socket because I need to be able to easily distinguish between route updates and route messages. Route updates are processed route messages are relayed. @SB At what point would I be closing the socket? Within the select statement and then reopening it again? Show the code for the ""talker"" the problem is there. @Nikolai I've added the code for udpsender which is in charge of sending out packets. This is the ""talker"" component. Are you opening files in the ""talker"" to get data from are you closing them? @Nikolai: No talker just takes a message and relays it. I've filled in the routehelper.routeit component. This allows you to see the entire path here from start to finish. Don't worry about the routehelper.rccommand that isn't being triggered. And there it is. I've found it. Its within the systemIP() function I was using. It kept opening sockets to determine the systems IP but never closing them. My apologizes to everyone. Thanks for everyone's help! crazyscot's suggestion of lsof and /proc//fd helped me recognize where the sockets were coming from. I think in this case ""Too many open files"" really means you've hit the file descriptor limit; network sockets count towards this limit. Are you sure that there's nothing else - say in routehelper - that's creating further sockets? What platform are you running on? If Linux lsof or grobbling around in /proc/<pid>/fd - while it's running before it hits the limit - might illustrate where all the fds are going. Tip: Don't rely on socket_udp_inboundALL being numerically larger than socket_udp_inboundRC - it's better to explicitly compare their values at least once. Thank you -- lsof pointed me into the right direction. Full explanation below.  If you are on Linux do an strace(1) on the client to check for the socket(2) and open(2) vs close(2) system calls (try -e trace=socketopenclose option). This is the easiest way to balance the file descriptor count at this point. Thanks -- I was unaware of this option. lsof helped me this time but perhaps strace will be help next time (+1). Yes that's a great one to keep in your toolbox along with `lsof` and `tcpdump` :)"
207,A,"How to receive ip multicast using only one NIC I have server with multiple NICs running windows server 2003. My application receive multicast packets but now i need to receive multicast packets just from one single network interface. I think this can help but i have some questions. ip_mreq mreq; mreq.imr_multiaddr.s_addr = multicast group address mreq.imr_interface.s_addr= network interface address if (setsockopt( socet  IPPROTO_IP  IP_ADD_MEMBERSHIP  (const void *)&mreq  sizeof(mreq)) < 0) { std::cerr << ""setsockopt error"" << std::endl; } First of all what do i need to use as a network interface address? Local address of the network interface(same that ipconfig returns) or index of the network interface that i can get using GetAdaptersAddresses api function? Second is this actualy possible with windows or i just spend my time? Update I just read about WSAJoinLeaf function and i wonder that it is not possible to use only one NIC for multicasts. First: your choice. See http://msdn.microsoft.com/en-us/library/ms738695(VS.85).aspx Second: Yes this should definitely be possible. :)  Yes this works you can use GetAdaptersInfo for IPv4 only interfaces or GetAdaptersAddresses for both families. Note that mreq only specifies an interface by address not index the ip6_mreq and GROUP_REQ (family agnostic) versions prefer an index because IPv6 allows you to have multiple matching link-local addresses but be connected to different networks."
208,A,"Racket: Settings options on TCP sockets I need to send data over the wire using TCP as a transport layer and the library racket/tcp works fine for this purpose but I'd like to set some options for the sockets e. g. SO_RCVTIMEO and SO_SNDTIMEO. I've found this library vyzo/socket but I'm not sure it's the best (and only) option to modify sockets' parameters since it described as ""BSD/POSIX sockets library for mzscheme"" on the official site. Thanks. A quick grep of the Racket source (for setsockopt) suggests that there is no built-in interface that you can use to specify arbitrary socket options (all uses of setsockopt that I've found use fixed options only). So I think what you've got is probably the best option available."
209,A,"UDP socket protocol I am designing a UDP protocol which only requires there to be a single client and a single server however I would like the ability to support multiple clients or even better multiple clients on the same machine. When a client connects to the server the server receives the client's IP and outgoing UDP port along with the message. However I have not had any success in responding to the client via that outgoing port. I guess my question is about what the right way of implementing something like this is if data is to be sent both ways (client to server and server back to client). It appears to me now that a single socket will not be sufficient and that both the client and the server must maintain both a ""listening"" socket and a separate ""sending"" socket. The sending socket must also be repeatedly opened and closed if the recipient changes. Perhaps the server would have one sending socket per connected client. Since it seems like the outgoing UDP port isn't an actual useful value I have to keep track of the ports from within my protocol so that two clients on a particular machine can work out their individual listen ports relay that to the server and the server will simply send the data to the appropriate ports after explicitly being informed of what they are. Edit: Okay I don't want the ""Use TCP"" answer even though it may be a valid answer. I'll describe the application a little bit: I am controlling hardware remotely in real-time through the internet. I want there to be as little latency as possible and TCP's data integrity and in-order guarantees are simply not going to help. I just need somebody who is familiar with datagram based netcode to help me make sure that my approach is sane. Your first instinct was correct. With UDP you only require a single socket on the server side and a single socket on the client side. Both sides use that socket for both sending and receiving and in the server's case it can use that single socket to communicate with multiple peers. The server should save client's address that is provided by the recvfrom() call and use it as the destination in the sendto() call when replying to that client. The client should check that the address provided by the recvfrom() call matches the server that it expected to receive a response from (or alternatively connect() the socket which will filter out bogus replies). If this approach is not working then you have a bug in your code - but it is the right approach.  Since UDP is stateless and lossy are you sure it's the right choice? Is there a reason you've chosen UDP over TCP? EDIT: What do you use when you need reliable UDP? may be worth a read if you haven't seen it various UDP-based options are discussed there which I think may be of use to you if you decide you definitely need to go down the UDP route. Well the current application will actually probably be better served by using TCP however my questions are still relevant because I want to eventually implement this functionality in UDP for a multiplayer game. That would not necessarily scale on TCP. My advice is implement in TCP optimize later if you have to. Don't focus on the network layer it's an abstraction anyway. Use TCP because it's easy get your game built and then you'll know if you need UDP or not."
210,A,"Python tell when an ftp transfer sits on completion I have to download some files from an FTP server. Seems prosaic enough. However the way this server behaves is if the file is very large the connection will just hang when the download ostensibly completes. How can I handle this gracefully using ftplib in python? Sample python code: from ftplib import FTP ... ftp = FTP(host) ftp.login(login passwd) files=ftp.nlst() ftp.set_debuglevel(2) for fname in files: ret_status = ftp.retrbinary('RETR ' + fname open(fname 'wb').write) debug output from the above: *cmd* 'TYPE I' *put* 'TYPE I\r\n' *get* '200 Type set to I.\r\n' *resp* '200 Type set to I.' *cmd* 'PASV' *put* 'PASV\r\n' *get* '227 Entering Passive Mode (00001052).\r\n' *resp* '227 Entering Passive Mode (00001052).' *cmd* 'RETR some_file' *put* 'RETR some_file\r\n' *get* '125 Data connection already open; Transfer starting.\r\n' *resp* '125 Data connection already open; Transfer starting.' [just sits there indefinitely] This is what it looks like when I attempt the same download using curl -v: * About to connect() to some_server port 21 (#0) * Trying some_ip... connected * Connected to some_server (some_ip) port 21 (#0) < 220 Microsoft FTP Service > USER some_user < 331 Password required for some_user. > PASS some_password < 230 User some_user logged in. > PWD < 257 ""/some_dir"" is current directory. * Entry path is '/some_dir' > EPSV * Connect data stream passively < 500 'EPSV': command not understood * disabling EPSV usage > PASV < 227 Entering Passive Mode (000011116). * Trying some_ip... connected * Connecting to some_ip (some_ip) port 2932 > TYPE I < 200 Type set to I. > SIZE some_file < 213 229376897 > RETR some_file < 125 Data connection already open; Transfer starting. * Maxdownload = -1 * Getting file with size: 229376897 { [data not shown] % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 218M 100 218M 0 0 182k 0 0:20:28 0:20:28 --:--:-- 0* FTP response timeout * control connection looks dead 100 218M 100 218M 0 0 182k 0 0:20:29 0:20:29 --:--:-- 0* Connection #0 to host some_server left intact curl: (28) FTP response timeout * Closing connection #0 wget output is kind of interesting as well it notices the connection is dead then attempts to re-download the file which only confirms that it is already finished: --2009-07-09 11:32:23-- ftp://some_server/some_file => `some_file' Resolving some_server... 0.0.0.0 Connecting to some_server|0.0.0.0|:21... connected. Logging in as some_user ... Logged in! ==> SYST ... done. ==> PWD ... done. ==> TYPE I ... done. ==> CWD not needed. ==> SIZE some_file ... 229376897 ==> PASV ... done. ==> RETR some_file ... done. Length: 229376897 (219M) 100%[==========================================================>] 229376897 387K/s in 18m 54s 2009-07-09 11:51:17 (198 KB/s) - Control connection closed. Retrying. --2009-07-09 12:06:18-- ftp://some_server/some_file (try: 2) => `some_file' Connecting to some_server|0.0.0.0|:21... connected. Logging in as some_user ... Logged in! ==> SYST ... done. ==> PWD ... done. ==> TYPE I ... done. ==> CWD not needed. ==> SIZE some_file ... 229376897 ==> PASV ... done. ==> REST 229376897 ... done. ==> RETR some_file ... done. Length: 229376897 (219M) 0 (0) remaining 100%[+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++] 229376897 --.-K/s in 0s 2009-07-09 12:06:18 (0.00 B/s) - `some_file' saved [229376897] I think some debugging could be useful. Could you fold the class below into your code? (I didn't do it myself because I know this version works and didn't want to risk making an error. You should be able to just put the class at the top of your file and replace the body of the loop with what I've written after #LOOP BODY) class CounterFile(): def __init__(self file maxsize): self.file = file self.count = 0 self.maxsize = maxsize def write(self bytes): self.count += len(bytes) print ""total %d bytes / %d""%(self.count self.maxsize) if self.count == self.maxsize: print "" Should be complete"" self.file.write(bytes) from ftplib import FTP ftp = FTP('ftp.gimp.org') ftp.login('ftp' 'thouis@gmail.com') ftp.set_debuglevel(2) ftp.cwd('/pub/gimp/v2.6/') fname = 'gimp-2.6.2.tar.bz2' # LOOP BODY sz = ftp.size(fname) if sz is None: print ""Could not get size!"" sz = 0 ret_status = ftp.retrbinary('RETR ' + fname CounterFile(open(fname 'wb') sz).write) That's an old post but did you really wanted to include your login in this snipped? logging in as ""ftp"" is equivalent to logging in as ""anonymous"" and the password is then traditionally set to the email address. My email is fairly easy to deduce from my stackoverflow login name anyway.  I've never used ftplib but perhaps you could do: Get the name and size of the file you want. Start a new daemonic thread to download the file. In the main thread check every few seconds whether the file size on disk equals the target size. When it does wait a few seconds to give the connection a chance to close nicely and then exit the program."
211,A,"Multi-Client / Multi-Server Network View Implementing a huge Multi-Client / Multi-Server Network View Say we have just 4 clients with IDs 1  2  3  4 We have also 3 Servers with IDs A  B  C Initially Clients 1  2 chose Server A & Client 3 chose Server B & Client 4 chose Server C Here we are : [ 1  2 => A ]  [ 3 => B ]  [ 4 => C ] Suddenly  Server C broke down !! Question What's the best method to make clients Initially finding there suitable server to establish TCP/IP connection with it ? & Finding another Server if connected one was broken ? Note : Initially [ 1  2 => A ] as A is the closest Server BUT Client 4 MUST choose Server B even A was more closer !! Population limitations => Connection priority constrain .. Regards .. Done  Is it now clear ? It's unclear how or why 2 connects to A and not B and therefore why B connects to 3 and C connects to 4. Once that logic is understood perhaps the rest of the question can be understood. You might also want to explain what is ""closest"" to 3 if B breaks down - is it A or C? You should put server A/B/C behind a load-balancer and put the sessions if you have any in the database. If server B stops working the request will be routed to A and C. What do you mean by load-balancer as I got it a dispatcher ? What's its architecture ? Can I implements this point of view as a P2P based on DHT tables ?"
212,A,Sending the array of arbitrary length through a socket. Endianness I'm fighting with socket programming now and I've encountered a problem which I don't know how to solve in a portable way. The task is simple : I need to send the array of 16 bytes over the network receive it in a client application and parse it. I know there are functions like htonl htons and so one to use with uint16 and uint32. But what should I do with the chunks of data greater than that? Thank you. htons htonl etc. are for dealing with a single data item (e.g. an int) that's larger than one byte. An array of bytes where each one is used as a single data item itself (e.g. a string) doesn't need to be translated between host and network byte order at all. I'm a bit confused.Say my machine has little-endian byte order and I'm trying to send ipv6 address as a byte array in a single chunk over the network. As far as I understand we must revert the bytes (16th becomes the first) transfer them and on the client side (if it's also little-endian) revert the byte oreder. Am I right? An IPv6 address should be sent as an array in big-endian order. You should (normally) be able to treat it as basically opaque data -- e.g. you receive 16 bytes and copy them into a sockaddr_in6 exactly as you received them. The IP stack might decide to swap them around internally but all you should ever see externally would be in network order. Someday if you had a 128-bit machine and wanted to load one and manipulate it as a number then you'd have to worry about an `ntohlll` and `htonlll` (or whatever name got used).  Endianness is a property of multibyte variables such as 16- and 32-but integers; it has to do with whether the high-order or low-order byte goes first. If the client application is processing the array as individual bytes it doesn't have to worry about endianness as the order of the bits within the bytes is the same.  you say an array of 16 bytes. That doesnt really help. Endianness only matters fro things larger than a byte if its really raw bytes then just send them you will receive them just the same It its really a struct you want to send it  struct msg { int foo; int bar; ..... Then you need to work through the buffer pulling that values you want. When you send you must assemble a packet into a standard order  int off = 0; *(int*)&buff[off] = htonl(foo); off += sizeof(int); *(int*)&buff[off] = htonl(bar); ... when u receive  int foo = ntohl((int)buff[off]); off += sizeof(int); int bar = ntohl((int)buff[off]); .... edit : I see you want to send an IPv6 address they are always in network byte order - so you can just stream it raw  Bytes themselves don't have endianness any more in that any single byte transmitted by a computer will have the same value in a different receiving computer. Endianness only has relevance these days to multibyte data types such as ints. In your particular case it boils down to knowing what the receiver will do with your 16 bytes. If it will treat each of the 16 entries in the array as discrete single byte values then you can just send them without worrying about endiannes. If on the other hand the receiver will treat your 16 byte array as four 32 bit integers then you'll need to run each integer through hton() prior to sending. Does that help?
213,A,Twisted in Java What is the closest Java alternative to Twisted? I don't think there is anything quite like Twisted as far as Deferreds and complex callback chains go. For non-blocking IO I think NIO is the most commonly-used solution. In particular look at this section on Selectors.  Nio is really low level and supports Socket only and SSL if you dig hard enough on Google for samples. Apache Mina wraps the complexity and adds a few protocols but not as much as Twister.  I've implemented most of a Twisted reactor in my collection of AMP hacks on launchpad - but it needs a maintainer. This isn't a real answer but we really would like Twisted itself to run on Java via Jython. It's slow going but one determined person could probably do it in a couple of weeks. https://twistedmatrix.com/trac/ticket/3413  Like Stephane I would suggest you take a look at Mina. Its a framework for asynchronous network IO. It is built on top of NIO which was mentioned earlier and IMO hides away some of the complexity involved with SelectorsChannelsetc.. I've used Mina for a couple of projects and its pretty good but be warned I've found the documentation to be a little weak. And again like Stephane mentioned it doesn't have out of the box support for too many protocols.  See this framework from Spring.io: http://spring.io/blog/2013/05/13/reactor-a-foundation-for-asynchronous-applications-on-the-jvm  If there is still an interest in this I've just put up the first beta release of the 'Reaction' framework which supports the Twisted-style deferred callback model in Java. As well as working as a standard Java package it can also be used as an OSGi service. License is Apache version 2. More details on my website >here<.
214,A,building an sk_buff for egress device linux kernel Long story short I am trying to build a very bare bones UDP SKB just to get something onto the wire. The scenario is as follows: I have a kernel module loading that (among other things) overrides the memory location of the standard udp_sendmsg function in /net/ipv4/udp.c. From here I would like to construct an skb to the point where I can simply put it onto the wire. Normally udp_sendmsg simply does a little UDP bookkeeping tacks on a UDP header and sends it down to the IP layer for routing L3/L2 headers etc. Basically I am bringing some of that functionality up into the sendmsg function. At this time I am just allocating an skb:  skb = alloc_skb(1500 GFP_KERNEL); //skb has 1500 bytes of tail room only skb_reserve(skb 500); //now has head and tail but no data space data = skb_put(skb 500); //now we have an skb with 500 head 500 data sec 500 tail And then (after some route table seteup) I am trying to add a udp_hdr:  struct udphdr *uh; skb->transport_header = skb_push(skb sizeof(struct udphdr)); uh = udp_hdr(skb); uh->source = 5555; uh->dest = dport; uh->len = 18; uh->check = 0; and an ip_hdr (only the basics filled):  struct iphdr *iph; skb->network_header = skb_push(skb sizeof(struct iphdr)); iph = ip_hdr(skb); iph->version = 4; iph->ihl = 5; iph->tos = inet->tos; iph->tot_len = htons(skb->len); iph->protocol = IPPROTO_UDP; iph->saddr = saddr; iph->daddr = daddr; skb->dst = dst_clone(&rt->u.dst); Note: I got most of this stuff from this page but they are using an older kernel (pre 2.6.24) where the network and transport headers were unions and called nh and h respectively. The new way involves using skb->transport_header / skb->network_header and using these helper functions but apparently I am doing something wrong because I get a kernel oops when I try to invoke the udp_sendmsg Note: this ran without an oops and dumped junk to the wire when instead of:  skb->transport_header = skb_push(skb sizeof(struct udphdr)); I used: skb_reset_transport_header(skb); (and equivalent for network_header. But after reading the link above and looking at the source for the reset function in linux/sk_buff.h it didn't seem like it was doing what I wanted. Please also note that any assignment statement above with (in this context) undefined variables is simply because I didn't include the entire function. I realize this question might fall into a very specific domain but any guidance on correct usage of the newer skb construction would be greatly helpful. My buddy google is coming up pretty dry. The Oops Call Trace:  [<ffffffff813dbf98>] oops_end+0xb9/0xc1 [<ffffffff81030e21>] no_context+0x1f6/0x205 [<ffffffff81030fd3>] __bad_area_nosemaphore+0x1a3/0x1c9 [<ffffffff8101184e>] ? apic_timer_interrupt+0xe/0x20 [<ffffffff8103100c>] bad_area_nosemaphore+0x13/0x15 [<ffffffff813dd30a>] do_page_fault+0x125/0x222 [<ffffffff813db485>] page_fault+0x25/0x30 [<ffffffffa010924f>] ? udp_sendmsg_offload+0x1e3/0x250 [testmodule] [<ffffffffa010922e>] ? udp_sendmsg_offload+0x1c2/0x250 [testmodule] [<ffffffff81390a00>] inet_sendmsg+0x54/0x5d [<ffffffff8132f142>] __sock_sendmsg+0x61/0x6c [<ffffffff8132f8b9>] sock_sendmsg+0xcc/0xe5 edited post to reflect the oops Please show us the Oops. skb_push() returns a pointer to the transport header but skb->transport_header is a sk_buff_data_t which is supposed to be an offset - see how skb_transport_header() works. You also need to convert the header information into network byte order. The part where you set the UDP header should be: struct udphdr *uh; skb_push(skb sizeof(struct udphdr)); skb_reset_transport_header(skb); uh = udp_hdr(skb); uh->source = __constant_htons(5555); /* ... */ and similarly for the IP header: struct iphdr *iph; skb_push(skb sizeof(struct iphdr)); skb_reset_network_header(skb); iph = ip_hdr(skb); iph->version = 4; /* ... */ Yep I agree. I ran into that yesterday when looking at skbuff that the use of the skb->transport_header depends on NET_SKBUFF_DATA_USES_OFFSET which in turn depends on __WORDSIZE. Since I am on a 64 bit platform it would seem that the preprocessor is indeed using this method. Also thank you for pointing out the byte order After hacking that bit in I totally forgot PS this gets around the kernel oops (at least where it was happening before) but I am still having no luck with my goal. I suppose that is for another posting though.
215,A,recvfrom max buffer TCP/IP I am writing a small C program to understand sockets. What is the maximum length of data returned from recvfrom? recvfrom(raw packet_buffer buf_size ... ); what is the maximum buf_size in linux. Is there a constant related to this size_t? Thanks Why not have a look in the kernel source? You might have to chase around to find it but the answer is in there somewhere. I think that max is 65535 bytes. It does not depend upon MTU since it's handled by protocol stack by itself.. so basically you have a good abstraction prom the effective packets that are sent on network. The choice of 2^16 should be so because it's the max size of the TCP window (usually it's not 64kb anyway but smaller): so it's the maximum buffer the protocol allows for a TCP connection. No it's much larger than that and a function of how much memory the system has by default. On most systems these days it's a few megabytes.  This isn't really a direct answer ... somewhat oblique to the question. For TCP/IP even if you determine what the maximum size is on your system it would probably be best to implement the code to not rely on that. With stream-oriented sockets the excess data is not lost. So you can call the receive function again to retrieve the remaining data. That is not true with message-oriented (UDP) connections though.
216,A,"result too large c++ sockets I am getting an error an bind() with the value 34 (Result too large) can anyone help?  void Connect(string address unsigned short port){ memset(&server2 0 sizeof(server2)); server2.sin_family = AF_INET; server2.sin_addr.s_addr = inet_addr(address.c_str()); server2.sin_port = htons(port); desc2 = ::socket(AF_INET SOCK_STREAM IPPROTO_TCP); if(desc2 == -1) { cout << ""Error in Socket()"" << endl; } if((::bind(desc2(sockaddr*)&server2 sizeof(server2))) == -1) { cout << ""Error in Bind() "" << errno << endl; } if((::connect(desc2 (sockaddr*)&server2 sizeof(server2))) > 0) { cout << ""Error in Connect()"" << endl; } cout << ""YOU ARE CONNECTED TO "" << address << "" ON PORT "" << port << endl; } PS: I got this error 1 year ago toothe problem was simple i had write something bad when initializing the socket address where to connect but now again i have no clue where I made a mistake. its in dots notationif it is like a domainit works ....strange pointing out an error: connect returns 0 on success use `== 0`. I would also change -1 to `SOCKET_ERROR` i set both to SOCKET_ERROR. Btw do i really need to call bind when connectingor only when i am listening to connection(server part)? why are u binding and then connecting? `bind` is used with `listen` and `accept` for servers `connect` is used for clients that is why i ask myself.so i dont need it. well are you a server or a client? well i am proxy serverand in Accept() i call Connect() so basically i am a client and binding is for serverscorect? I don't know what caused your problem exactly to return that type of result. You said you were a proxy server so you are listening for incoming connections. Try this: server2.sin_addr.s_addr = 0; server2.sin_family = AF_INET; server2.sin_port = htons(port); desc2 = socket(AF_INET SOCK_STREAM IPPROTO_TCP); // Puts the socket in listening mode allowing 10 connection requests to queue. bind(desc2 (sockaddr*)&server2 sizeof(server2)); listen(desc2 10); // Accepts the first connection request. SOCKET accepted_socket = accept(desc2 NULL NULL); std::cout << ""connection accepted!\n""; You will probably want to learn how to program asynchronously. wellyes this is the server partthis works for mebut after thisi want to connect to the server that the cleint connected to me says.Like he says ""connect to www.google.com"" on port 80....and i must connect to google That will require you to have two separate sockets. One of those sockets is your connection between you and the client the other connection is between you and google. When your client tells you to ""connect to google on port 80"" you need to create another socket and connect to google... on port 80. Yes the code that i post is the socket beetween me and google(descriptor desc2 struct sockaddr server2). The socket beetwen the cleint and me is (descriptor desc sockaddr server). Btw after i get this to work it comes the part with the threading(i guess that means asynchronously programming)  I don't know why you're getting 'name too long' -- without seeing how server2 was declared and defined it's impossible to know for sure. But I do know that calling bind() and then connect() on the same socket with the same address will fail -- bind() is assigning the local address to the socket and connect() connects to the remote address. Giving the same address for both ends of the socket can only end badly. Almost no protocols require bind() before connect(). (The exceptions would involve the ""ports lower than 1024 can only be opened by root so we can trust this connection"" style of authentication which hasn't been used in ages. Think rlogin.) bind() makes most sense immediately before a listen() call. Clients will attempt to contact a server on a 'well-known port' and bind() is the mechanism you use to assign that name. ahthanks for explaining"
217,A,Retrieving the local IP address based on HttpConnection using Java Is there any way to determine the IP address of the socket used to create the HTTP connection when the class used to create the connection was HttpUrlConnection? Is it local IP or the IP of machine where the URL points to? Show working code... I'm looking for the local IP address. No sorry the HttpURLConnection is an abstract class. The actual connection is implemented in code provided by the JRE. As such there are no methods that will give you access to the actual Socket used.
218,A,"Generating all IP addresses in the IPv4 Range What would be a efficient way to generate all possible IP v4 addresses? other than iterating all bytes in a one giant nested for loop. In terms of ""efficiency"" I don't think there is a much better way than looping through all possible values. Do take note of two things: 1. There are a lot of addresses so it won't be that efficient. 2. Not all IP addresses are valid (and there are plenty of addresses which you probably don't intend to go over). For an example of what IP addresses are valid take note that all addresses between 224.0.0.0 and 239.255.255.255 are multicast addresses all addresses starting with 127.x.x.x are invalid etc.  All Possible? 0.0.0.0 to 255.255.255.255 which is 0 to 0xFFFFFFFF Yes all possible valid IP addresses from beginning to end. I see May I know the purpose of doing that? for checking valid IP? trying to replicate http://www.isi.edu/ant/address/index.html Ah I see to do a Assigned IP address map you will need some of data resources which I dont have in my ref links for now.  Not all IPv4 addresses are valid depending on what purpose they're intended for. See the section on reserved address blocks and the linked RFCs here: http://en.wikipedia.org/wiki/IPv4 So depending on what you want to do you might need to check for reserved addresses and leave them out.  You can start with an unsigned int/long (32-bit data type) initialized to zero and keep incrementing until you reach 0xffffffff. The increment operator is usually slightly more efficient than nested loops. Use bit masks and bit shift operators to pull out any given byte that you are interested in.  Edit: My previous answer would have gone from 128.0.0.0 to 255.255.255.255 to 0.0.0.0 to 127.255.255.255. Presumably you want to go from 0.0.0.0 to 255.255.255.255 so I've edited my solution to do that. int i = -1; do { i++; int b1 = (i >> 24) & 0xff; int b2 = (i >> 16) & 0xff; int b3 = (i >> 8) & 0xff; int b4 = (i ) & 0xff; //Now the IP is b1.b2.b3.b4 } while(i != -1); Note: if you're confused how this loop will ever end (i.e. how adding 1 to -1 enough times makes it -1 again) read up on two's complement. Basically adding one to Integer.MAX_VALUE results in Integer.MIN_VALUE and does not throw any kind of exception. Old answer. Still hits all IPs but probably not in the order you desire: for(long n = Integer.MIN_VALUE; n <= Integer.MAX_VALUE; n++) { int i = (int)n; int b1 = (i >> 24) & 0xff; int b2 = (i >> 16) & 0xff; int b3 = (i >> 8) & 0xff; int b4 = (i ) & 0xff; //Now the IP is b1.b2.b3.b4 } Please note: If the loop control variable was an int instead of a long this would be an infinite loop (since all ints are always <= Integer.MAX_VALUE). Is Integer unsigned in java? @S. Mark: no but it doesn't matter. the `& 0xff` will remove the sign bit. The loop could have been from 0 to -1 (since -1 == 0xffffffff). It would still work because `Integer.MAX_VALUE + 1 === Integer.MIN_VALUE` @S.Mark: i'm not sure if this is what you were getting at but he probably wanted to go from 0 to 0xffffffff. i've updated my answer to do that I see thanks I've got your point too."
219,A,"getting external IP from http://www.whatismyip.com/automation/n09230945.asp using Adobe AIR We can get external IP from How do i get my external IP from http://www.whatismyip.com/automation/n09230945.asp using Java C# or VB.NET. But I want to do that using Adobe AIR. How to make request to that link and get its string. Please help me asap. Thanks in advance. Here is a way to get external ip using java or C# from whatismyip.com http://stackoverflow.com/questions/5543738/difference-between-internal-ip-address-and-external-ip-address It will be like this: private function makeRequest():void { var loader:URLLoader = new URLLoader(); configureListeners(loader); var req:URLRequest=new URLRequest(""http://www.whatismyip.com/automation/n09230945.asp""); try { var header:URLRequestHeader=new URLRequestHeader(""content-type"" ""text/plain""); var header2:URLRequestHeader = new URLRequestHeader(""pragma"" ""no-cache""); req.requestHeaders.push(header); req.requestHeaders.push(header2); req.method = URLRequestMethod.POST; loader.dataFormat = URLLoaderDataFormat.TEXT; loader.load(req); } catch (error:Error) { trace(""Unable to load requested document.""); } } private function configureListeners(dispatcher:IEventDispatcher):void { dispatcher.addEventListener(Event.COMPLETE completeHandler); } private function completeHandler(event:Event):void { var loader:URLLoader = URLLoader(event.target); mx.controls.Alert.show("""" + loader.data); } Thanks All. Thanks for editing :)"
220,A,Python: How do I get the IP address from a FQDN If I have an FQDN e.g. www.google.com how do I get the corresponding IP address? The easiest way to do this is socket.gethostbyname().  You can use socket.getaddrinfo. That will give you the differents IP address associated with the name and can also give you the IPv6 address. From the documentation: >>> import socket >>> help(socket.getaddrinfo) Help on built-in function getaddrinfo in module _socket: getaddrinfo(...) getaddrinfo(host port [ family socktype proto flags]) -> list of (family socktype proto canonname sockaddr) Resolve host and port into addrinfo struct. >>> from pprint import pprint >>> pprint(socket.getaddrinfo('www.google.com' 80)) [(2 1 6 '' ('74.125.230.83' 80)) (2 2 17 '' ('74.125.230.83' 80)) (2 3 0 '' ('74.125.230.83' 80)) (2 1 6 '' ('74.125.230.80' 80)) (2 2 17 '' ('74.125.230.80' 80)) (2 3 0 '' ('74.125.230.80' 80)) (2 1 6 '' ('74.125.230.81' 80)) (2 2 17 '' ('74.125.230.81' 80)) (2 3 0 '' ('74.125.230.81' 80)) (2 1 6 '' ('74.125.230.84' 80)) (2 2 17 '' ('74.125.230.84' 80)) (2 3 0 '' ('74.125.230.84' 80)) (2 1 6 '' ('74.125.230.82' 80)) (2 2 17 '' ('74.125.230.82' 80)) (2 3 0 '' ('74.125.230.82' 80))] Note: gethostbyname is deprecated in C (and Python socket.gethostbyname is implemented with it) as it does not support IPv6 addresses and getaddrinfo is the recommended replacement.  use socket.gethostbyname(hostname) see http://docs.python.org/library/socket.html#socket.gethostbyname
221,A,"Determine if server is listening when using udp The setting: I want to write a point-to-point Connection class that when used does not differentiate between server and client. The first host which calls connect() shall become the server waiting for the client to connect and the second shall become the client that connects to the server. In order to do that the connect() method first needs to check for a listening server. a) The first time this happens no server is found and the party calling connect() starts listening on localhost and the configured port for an incoming connection. b) The second party calling connect() also checks the remote host on the given port recognizes the server and connects to it. This is not too hard using TCP since TcpClient.Connect() throws an exception when no connection could be established. Therefore I know when I'm the first. Since I use reliable LAN only I wanted to use UDP however. My problem: How can I determine whether an UDP server socket is waiting for incoming data. Ideally I would like to use the asynchronous network API directly afterwards. Instead of dealing with listening threads all by myself. I don't think you can check for a listening server short of sending a packet and waiting to see if you get a reply.  As Jon and Andrew said you can't see if a listener is open but you can implement a ping/pong protocol. Send a ping first time you connect if no pong back then set this up like a server. If you got pong back then that's your server.  With UDP the communication model is akin to a message in a bottle: you know you sent it but there's no way to know if anyone ever received it. You need to manually establish a communication protocol to determine if the remote party is listening (e.g. have them send a ""Yes I 'm here"" response). This would require both endpoints to accept UDP datagrams."
222,A,"Error while using libsoup I am writing a client using libsoup APIs. I have done the following. `session = soup_session_sync_new(); msg = soup_message_new(""GET""""http://www.google.com""); status = soup_session_send_message(sessionmsg);` However now i am getting the following error code in status. I print the reason phrase and it says the status= 4  msg->reason_phrase = Cannot connect to destination How do i resolve this issue? status = 4 means? I think you should look for information from libsoup. I believe the error should mean something. This is the return value from the api. Status is unsigned int. The problem is there is no proper documentation of error codes. I am suspecting its got to do something with network configuration. The code seems perfectly alright. Just try changing google.com to some other simpler site may be gnu.org and retry the code. I changed it to gnu.org. I get the same return value. I am running ubuntu on vmware. Does it have to do with my network configuration?  The problem was i use a proxy server to connect to internet. Hence i needed to set the session object property of SOUP_SESSION_PROXY_URI with the proxy-sever:port value. It works fine now. Time to test POST method now."
223,A,What's an example of a well designed network server? I'm interested in looking at architectures for extensible network serving applications. I'm not too interested in the protocol or the language used more in the elegance and extensibility of the design. Of course Apache comes to mind but I was wondering if anyone had any other examples that they find a pleasure to work with. EDIT: just to clarify I'm asking about a server application that implements a network protocol. Web development frameworks are not network servers in this sense. Protocols may include but are not limited to: FTP HTTP XMPP SNMP IMAP etc. Good network servers implement some sort of parallelism focus on scalability yet have good extensibility as well. Since no one else has mentioned it: IIS. It is a very extensible application for Windows (which is extremely extensible itself). Tack on ASP and other .NET components and you can't really get any more versatile than that.  Ejabberd is an XMPP server written in Erlang. The Erlang programming language's native message passing fits nicely with an instant-messaging server which allows Ejabberd to be very modular as well as exhibiting a lot of parallelism.  I'd take a look at OpenSSH it's well known for being practically impenetrable. This is mainly due to the OpenBSD group's intensive review process but I suspect it's also related to the architecture. For extensibility I would look a bit deeper to OpenSSL (the underlying protocol toolkit used for OpenSSH).  Figured I'd throw out Apache here to prevent everybody answering with it at the same time.  The Asterisk PBX is an open-source telephony server that implements a number of VoIP protocols including SIP. It's fairly modular and incredibly configurable even using it's own programming language of sorts AEL to describe IVR dialplan interactions.  Jetty has worked well for me. It is fast and works well under load. Of course performance will depend what kind of application you deploy on top of Jetty.  Django's web framework is a nice server. For that matter all web application servers are very extensible. This isn't really a network server in that it doesn't implement any network protocols. It appears to implement HTTP since it's a web server. Since your saying it doesn't implement HTTP your question is confusing.  I would say ASP.NET is a well designed server application. Programming model is extensible (allows you to hook into every event of the request lifecycle with user-authored modules). Also extremely scalabale and performant. The caching functionality alone is worth it. I can give you a generic link to ASP.NET but I'm sure you are reasonably familiar with it. http://www.asp.net/ asynchronous web pages are another great feauture  For writing network servers I like to work with POE. POE is a very interesting tool to build network servers but it's not in itself a network server.  nginx (see also the nginx wiki) is an HTTP server and mail proxy that has earned a reputation for scalability and resource efficiency. It uses an event-based architecture (supporting epoll kqueue etc.) to keep memory usage low even under sizable loads. This is awesome this sort of thing is exactly what I was looking for.  I would recommend to read these books on ACE: C++ Network Programming: Mastering Complexity Using ACE and Patterns C++ Network Programming: Systematic Reuse with ACE and Frameworks They contain a lot of very useful information about designing networked applications.
224,A,"Why do SocketChannel writes always complete for the full amount even on non-blocking sockets? Using the Sun Java VM 1.5 or 1.6 on Windows I connect a non-blocking socket. I then fill a ByteBuffer with a message to output and attempt to write() to the SocketChannel. I expect the write to complete only partially if the amount to be written is greater than the amount of space in the socket's TCP output buffer (this is what I expect intuitively it's also pretty much my understanding of the docs) but that's not what happens. The write() always seems to return reporting the full amount written even if it's several megabytes (the socket's SO_SNDBUF is 8KB much much less than my multi-megabyte output message). A problem here is that I can't test the code that handles the case where the output is partially written (registering an interest set of WRITE to a selector and doing a select() to wait until the remainder can be written) as that case never seems to happen. What am I not understanding? Update: I've tried the Sun VM 1.5 1.6 and JRockit VM 1.5 to make sure this network behaviour wasn't VM-specific. Where are you sending the data? Keep in mind that the network acts as a buffer that is at least equal in size to your SO_SNDBUF plus the receiver's SO_RCVBUF. Add this to the reading activity by the receiver as mentioned by Alexander and you can get a lot of data soaked up. Well yes but in TCP there can't be any data in flight that isn't also in the send buffer.  I managed to reproduce a situation that might be similar to yours. I think ironically enough your recipient is consuming the data faster than you're writing it. import java.io.InputStream; import java.net.ServerSocket; import java.net.Socket; public class MyServer { public static void main(String[] args) throws Exception { final ServerSocket ss = new ServerSocket(12345); final Socket cs = ss.accept(); System.out.println(""Accepted connection""); final InputStream in = cs.getInputStream(); final byte[] tmp = new byte[64 * 1024]; while (in.read(tmp) != -1); Thread.sleep(100000); } } import java.net.InetSocketAddress; import java.nio.ByteBuffer; import java.nio.channels.SocketChannel; public class MyNioClient { public static void main(String[] args) throws Exception { final SocketChannel s = SocketChannel.open(); s.configureBlocking(false); s.connect(new InetSocketAddress(""localhost"" 12345)); s.finishConnect(); final ByteBuffer buf = ByteBuffer.allocate(128 * 1024); for (int i = 0; i < 10; i++) { System.out.println(""to write: "" + buf.remaining() + "" written: "" + s.write(buf)); buf.position(0); } Thread.sleep(100000); } } If you run the above server and then make the above client attempt to write 10 chunks of 128 kB of data you'll see that every write operation writes the whole buffer without blocking. However if you modify the above server not to read anything from the connection you'll see that only the first write operation on the client will write 128 kB whereas all subsequent writes will return 0. Output when the server is reading from the connection: to write: 131072 written: 131072 to write: 131072 written: 131072 to write: 131072 written: 131072 ... Output when the server is not reading from the connection: to write: 131072 written: 131072 to write: 131072 written: 0 to write: 131072 written: 0 ...  I've been working with UDP in Java and have seen some really ""interesting"" and completely undocumented behavior in the Java NIO stuff in general. The best way to determine what is happening is to look at the source which comes with Java. I also would wager rather highly that you might find a better implementation of what you're looking for in any other JVM implementation such as IBM's but I can't guarantee that without look at them myself.  I can't find it documented anywhere but IIRC[1] send() is guaranteed to either a) send the supplied buffer completely or b) fail. It will never complete the send partially. [1] I've written multiple Winsock implementations (for Win 3.0 Win 95 Win NT etc) so this may be Winsock-specific (rather than generic sockets) behavior.  I'll make a big leap of faith and assume that the underlying network provider for Java is the same as for C...the O/S allocates more than just SO_SNDBUF for every socket. I bet if you put your send code in a for(1100000) loop you would eventually get a write that succeeds with a value smaller than requested. Thanks for the idea. I just did a few hundred multi-megabyte writes and not a single one of them returned with a value less than the full amount. Any chance you can watch the traffic with a sniffer and see how the packets are being sent? Might give you a clue if the sending library is even trying to break up the packets into smaller than MTU chunks.  You really should look at an NIO framework like MINA or Grizzly. I've used MINA with great success in an enterprise chat server. It is also used in the Openfire chat server. Grizzly is used in Sun's JavaEE implementation."
225,A,"problem with finding the hostname with ip addr using InetAddress class I have written the two program 1st whois.java to find the ip address of the given hostname import java.net.*; import java.io.*; public class whois{ public static void main(String args[]) throws IOException{ String hostName = args[0]; try{ InetAddress ipaddress = InetAddress.getByName(hostName); System.out.println(""IP address: "" + ipaddress.getHostAddress()); }catch(UnknownHostException e){ System.out.println(""Could not find IP address for: "" + hostName); } } } and other whois2.java which finds hostname for given ip import java.net.*; import java.io.*; class whois2{ public static void main(String args[]){ try{ String str[] = args[0].split(""\\.""); byte btArr[] = new byte[]{(byte)Integer.parseInt(str[0]) (byte)Integer.parseInt(str[1]) (byte)Integer.parseInt(str[2]) (byte)Integer.parseInt(str[3])}; InetAddress ipAddr = InetAddress.getByAddress(btArr); System.out.println(""Host name for this is : "" + ipAddr.getHostName()); }catch(UnknownHostException e){ System.out.println(""Unable to find the host for ip specified "" + args[0]); } } } and then i ran the program with jdk 1.6 and get following outputs: $java whois google.com IP address: 209.85.231.104 $java whois2 209.85.231.104 Host name for this is : maa03s01-in-f104.1e100.net why the host name is different not google.com? Thanks in advance The server responsible as defined by the DNS lookup for handling requests to a particular hostname need not have the same hostname as that of the original lookup. A more typical example would be that requests for foobar.com are handled by a server at an IP with IP having hostname www.foobar.com. Note also that the handling server may vary by region. So I get the same using the linux host tool: joel@bohr:~$ host google.com google.com has address 173.194.37.104 ... requests to google.com should be handled by the server at 173.194.37.104 joel@bohr:~$ host 173.194.37.104 104.37.194.173.in-addr.arpa domain name pointer lhr14s02-in-f104.1e100.net. ... the hostname of the server at 173.194.37.104 is lhr14s02-in-f104.1e100.net joel@bohr:~$ host lhr14s02-in-f104.1e100.net lhr14s02-in-f104.1e100.net has address 173.194.37.104 ... and sanity check the IP of lhr14s02-in-f104.1e100.net is indeed 173.194.37.104 nice didn't know of this cute little util!  whois(ip) resolves to the name of the registered server not to an entry on a domain name server. Same happens when we use whois services on the web: http://whois.domaintools.com/google.com resolves to IP 74.125.155.99 (from my location!) http://whois.domaintools.com/74.125.155.99 resolves the host px-in-f99.1e100.net (which again is different from your results) kk thanks andreas"
226,A,"c++ gethostbyaddr with user input I am writing c++ code for a telnet client. I am having problems getting the host address from the user input. struct in_addr peers; cin>>peers; peerserver = gethostbyaddr((const char*)peers4AF_INET); if (peerserver == NULL) exit(0); I am new to c++ can anyone suggest a better way of getting the host addr with user input. Thanks in advance. What you're looking for is gethostbyname not gethostbyaddr. gethostbyaddr assumes that you've already got the IP address. char peers[256]; cin >> peers; struct hostent *ent = gethostbyname(peers); printf(""%04x\n"" *(int *)(ent->h_addr)); ... and not forgetting that `getaddrinfo` (and now mostly obsolete `gethostby*` functions) may return several addresses/entries. Relation `name <-> address` is many to many. Better yet use an `std::string` and getaddrinfo (http://linux.die.net/man/3/getaddrinfo)"
227,A,UDP Broadcast or IP Multicast? If you had to implement a network broadcast would you use UDP broadcast or IP multicast in a new application? Why? What are the benefits and drawbacks you encountered with either method? (Especially concerning problems with VPNs different subnets various routers etc.) The way multicast behaves is almost but not entirely unlike every other type of network traffic. Your systems and network administrators won't thank you for forcing it on them. It's often voodoo and IMO best avoided if you can. You should spill the beans here: multicast forces the network node to interrupt the higher level IP stack and see if any application is interested in the received multicast packet. Every time on every node on every multicast send. So that's why net admins hate it.  Multicast has the drawback that it is not well supported by routers and NAT. If all of your clients are on the same network with only simple bridges multicast works great and avoids the overhead of broadcast addressing for machines that are not part of the group. If the routers support IGMP and properly propagate the TTL it can work on local networks. There have been attempts to do multicast across the Internet such as Mbone with various levels of success. Most of them use some sort of tunnel to get around bridges and non-compliant routers. One caveat for multicast packets however is if there are any WiFi connections the access point will use the slowest possible bit rate for the multicast packets and requires acks from all clients even those who are not part of the multicast group. There are also drawbacks for non-participating clients and battery life. The article about use of multicast over 802.11 is really interesting however it's from nearly ten years ago. Does anyone have a reference that explains whether this is still as much of an issue in practice for newer versions of the 802.11 family of standards?  The choice of which is most appropriate depends on the nature of the application and the configuration of the networks over which it's to be run - there's no hard or fast rules. I therefore wouldn't enforce either - I'd allow configuration of the broadcast address. If the user configures something in the 224.0.0.0/4 range then it's multicast otherwise it's broadcast.
228,A,"Getting the 'external' IP address in Java I'm not too sure how to go about getting the external IP address of the machine as a computer outside of a network would see it. My following IPAddress class only gets the local IP address of the machine. Any help would be appreciated. Thanks. public class IPAddress { private InetAddress thisIp; private String thisIpAddress; private void setIpAdd() { try { InetAddress thisIp = InetAddress.getLocalHost(); thisIpAddress = thisIp.getHostAddress().toString(); } catch(Exception e){} } protected String getIpAddress() { setIpAdd(); return thisIpAddress; } } You are aware that a machine can have many public addresses at once? They're really associated with a network interface not a machine. As @Donal Fellows wrote you have to query the network interface instead of the machine. This code from the javadocs worked for me: The following example program lists all the network interfaces and their addresses on a machine: import java.io.*; import java.net.*; import java.util.*; import static java.lang.System.out; public class ListNets { public static void main(String args[]) throws SocketException { Enumeration<NetworkInterface> nets = NetworkInterface.getNetworkInterfaces(); for (NetworkInterface netint : Collections.list(nets)) displayInterfaceInformation(netint); } static void displayInterfaceInformation(NetworkInterface netint) throws SocketException { out.printf(""Display name: %s\n"" netint.getDisplayName()); out.printf(""Name: %s\n"" netint.getName()); Enumeration<InetAddress> inetAddresses = netint.getInetAddresses(); for (InetAddress inetAddress : Collections.list(inetAddresses)) { out.printf(""InetAddress: %s\n"" inetAddress); } out.printf(""\n""); } } The following is sample output from the example program: Display name: TCP Loopback interface Name: lo InetAddress: /127.0.0.1 Display name: Wireless Network Connection Name: eth0 InetAddress: /192.0.2.0 From docs.oracle.com Thank You! While this solution won't work when the software is behind a NAT it is still very helpful for server software that you know won't be behind a NAT or as a fallback if the software cannot connect to another server to get it's own IP. In particular for software intended to be run on LANs disconnected from the internet it is the way to go.  The truth is 'you can't' in the sense that you posed the question. NAT happens outside of the protocol. There is not way for your machine's kernel to know how your NAT box is mapping from external to internal IP addressees. Other answers here offer tricks involving methods of talking to outside web sites. this should get many more upvotes for simply mentioning NAT  System.out.println(pageCrawling.getHtmlFromURL(""http://ipecho.net/plain""));  http://jstun.javawi.de/ will do it - provided your gateway device does STUN )most do)  It's not that easy since a machine inside a LAN usually doesn't care about the external IP of its router to the internet.. it simply doesn't need it! I would suggest you to exploit this by opening a site like http://www.whatismyip.com/ and getting the IP number by parsing the html results.. it shouldn't be that hard! You don't have to parse html. The first line of the webpage source of http://whatismyip.com/ is your external IP.  All this are still up and working smoothly! http://checkip.amazonaws.com/ http://icanhazip.com/ http://curlmyip.com/ http://www.trackip.net/ip Good luck!  If you are using JAVA based webapp and if you want to grab the client's (One who makes the request via a browser) external ip try deploying the app in a public domain and use request.getRemoteAddr() to read the external IP address.  I am not sure if you can grab that IP from code that runs on the local machine. You can however build code that runs on a website say in JSP and then use something that returns the IP of where the request came from: request.getRemoteAddr() Or simply use already-existing services that do this then parse the answer from the service to find out the IP. Edit: The other answers pointed out to the whatismyip.com website I found out that they have a link that you can scrap the IP from here is some Java code I put together to do it: import java.net.*; import java.io.*; URL whatismyip = new URL(""http://automation.whatismyip.com/n09230945.asp""); BufferedReader in = new BufferedReader(new InputStreamReader( whatismyip.openStream())); String ip = in.readLine(); //you get the IP as a String System.out.println(ip); this is fantastic! nice answer. Word of warning: they've adjusted the automation URL to http://automation.whatismyip.com/n09230945.asp I would suggest implementing at least two servers that serves your ip in case one of them is down at the moment (with correct error handling of course). My choise would be http://www.externalip.net (click the ""Need an API?"" link). Have been using that site for well over a year now haven't had any problems so far and it's a fast service that always returns a quick response. Awesome link; I will be using this as part of a proof of concept on a University networking assignment! Alternatively you can use AWS: http://checkip.amazonaws.com/ Another alternative would be if you have access to a web server or host account that runs PHP simply use the code @bakkal provided above and point it to your own PHP file that simply contains `echo $_SERVER['REMOTE_ADDR'];`. If this is the only or first output line of the script you're all set. Whatsmyip.com does not appear to support this type of automation any more.  Make a HttpURLConnection to some site like www.whatismyip.com and parse that :-)  One of the comments by @stivlo deserves to be an answer: You can use the Amazon service http://checkip.amazonaws.com import java.io.BufferedReader; import java.io.IOException; import java.io.InputStreamReader; import java.net.URL; public class IpChecker { public static String getIp() throws Exception { URL whatismyip = new URL(""http://checkip.amazonaws.com""); BufferedReader in = null; try { in = new BufferedReader(new InputStreamReader( whatismyip.openStream())); String ip = in.readLine(); return ip; } finally { if (in != null) { try { in.close(); } catch (IOException e) { e.printStackTrace(); } } } } } nice answerthis helped me thanks"
229,A,"how does creating too many sockets affect or even crash a router? Are there different limitations as to how many connections (sockets) that can be created and use it for data transfer? Does it matter if there is a presence of a home router or a commercial grade router? For example different users have different experience with Linksys D-link or Netgear routers. Some may be more easily down or ""get jammed"" especially if they are running network intensive programs (need to unplug the power cord to the router and plug in again). Is it due to the creations of too many sockets or otherwise using too much resource that the router can support? Routers that perform Network Address Translation need to maintain a mapping between the internal IP addresses/port numbers and external port numbers for each connection that's active. If the table for these connections becomes full this can cause problems such as crashing interruption of existing connections or failure to establish new connections. Routers that do not perform NAT - and all of the core Internet routers do no NAT - are generally not susceptible to this as they are not aware of what connections are passing through them and handle each packet individually.  Routers by themselves do not care about sockets unless they are state-ful routers such as a NAT gateway. If it is a stateful router then each connection will take some space in the RAM of the router and when the RAM runs out it has to drop old records (or it crashes and if you're lucky re-boots depending on what vendor it is...) A ""connection"" in IP terms is a TCP session (which is identified by source IP source port destination IP destination port and a sequence number agreement). UDP or ICMP and friends don't have ""connections"" per se but stateful routers/firewalls usually fake it by assuming that a source IP source port destination IP destination port tuple is a ""connection"" with some timeout. When no traffic is seen on that connection for some time it's assumed to die. That timeout may be set between 30 seconds and a day depending on the firewall/router. Yes in general commercial routers/firewalls are beefier and thus can track more connections when stateful. The best router you can get is often a $400 Linux or xBSD x86 box with a gig of RAM or two and a small flash disk to boot from... Residential routers are weak by almost all means of the word. If you're having trouble chances are that's your problem. so i wonder if it is the $60 Linksys wireless router (or D-Link or NetGear) how much RAM do they usually have? @Michelle: Usually between 4MB and 32MB. 16MB is now pretty typical for a modern ultra-low-end router. [List](http://wiki.openwrt.org/toh/start)"
230,A,"boost::asio: thread local asynchronous events I will be creating x amount of threads in my server-app. x will be the amount of cores on the machine and these threads will be (non-hyperthread) core-bound. Naturally with this scheme I would like to distribute incoming connections across the threads with the aim of ensuring that once a connection is assigned to a thread it will only be served out of that particular thread. How is this achieved in boost::asio ? I am thinking: a single socket bound to an address shared by multiple io_service's where each threads gets it's own io_service. Is this line of reasoning correct ? edit: looks like I am going to have to answer this question myself. Yes your reasoning is basically correct. You would create a thread per core an io_service instance per thread and call io_service.run() in each thread. However the question is whether you'd really do it that way. These are the problems I see: You can end up with very busy cores and idling cores depending on how the work is balanced across your connections. Micro-optimising for cache hits in a core might mean that you end up losing the ability to have an idle core do work when the ""optimal"" core is not ready. At socket speeds (ie: slow) how much of a win will you get from CPU cache hits? If one connection requires enough CPU to keep a core busy and you only up as many connections as cores then great. Otherwise the inability to move work around to deal with variance in workload might destroy any win you get from cache hits. And if you are doing lots of different work in each thread the cache isn't going to that hot anyway. If you're just doing I/O the cache win might not be that big regardless. Depends on your actual workload. My recommendation would be to have one io_service instance and call io_service.run() in a thread per core. If you get inadequate performance or have classes of connections where there is a lot of CPU per connection and you can get cache wins move those to specific io_service instances. This is a case where you should do profiling to see how much cache misses are costing you and where. Great answer and great reasoning. I will probably stick to your suggestion till I see a problem -- it's good to know that it's possible should I choose to micro-optimize.  If your server-app is supposed to run on a Windows machine then you should consider to use IO completion ports. It is able to limit the number of active threads to the number of cores. It distributes the IO events from a theoretically infinite number of sockets to the active threads. Scheduling is done by the OS. Here is a good example how to do it. (+1) IO completion ports kick ass -- However my current implementation uses IO completion ports and named pipes. I am rewriting the thing from scratch and portability is important. +1 here's a link to the latest version of that source code example that you link to: http://www.lenholgate.com/archives/000637.html  You can use a single io_service which is used by multiple threads and a strand to ensure that a connection is always handled by the same thread. Take a look at HTTP server 3 example. My question is a little more specific/advanced than the examples presented."
231,A,"Conventional way of accepting connections in a proactive web-server (using asynchronous IO) I'm currently exploring the aio on linux to use it for implementing a web-server. As I have already discovered there are not too many resources that cover in detail the usage of aio. The question is -- what is the conventional way of accepting the client connections in a single-threaded application that uses aio? (with minimum blocking possible) Ideally I can see aio_accept operation sending a notification (callback function call in my case) that the connection is established and the aio_read is called at once inside the handler. But there is no such call :( Thanks! I don't think aio works on pre-accept sockets. You probably want to select on them or perhaps get SIGIO when it is ready. [edit] Specifically the man page says: ""The device on which the file is opened must allow the seek operation. I.e. it is not possible to use any of the AIO operations on devices like terminals where an lseek call would lead to an error."" Sockets do not allow the seek operation so even post-accept sockets would seem out of luck."
232,A,"Are there some general Network programming best practices? I am implementing some networking stuff in our project. It has been decided that the communication is very important and we want to do it synchronously. So the client sends something the server acknowledges. Are there some general best practices for the interaction between the client and the server. For instance if there isn't an answer from the server should the client automatically retry? Should there be a timeout period before it retries? What happens if the acknowledgement fails? At what point do we break the connection and reconnect? Is there some material? I have done searches but nothing is really coming up. I am looking for best practices in general. I am implementing this in c# (probably with sockets) so if there is anything .Net specific then please let me know too. http://stackoverflow.com/questions/2368580/socket-protocol-fundamentals may be of interest. If both the client and the server are written in .NET/C# I would recommend WCF insted of raw sockets as it saves you a from a lot of plumbing code with serialization and deserialization synchronization of messages etc. That maybe doesn't really answer your question about best practices though ;-)  In a common think about network programming I think you should learn about : 1. Socket (of course). 2. Fork and Threading. 3. Locking process (use mutex or semaphore or others). Hope this help.. Hi thanks but not really. All of that stuff is fine. I was just looking for some general guidelines. For example uses of ACKs etc. Instead of learning about threading I'd learn about non-blocking IO (if working at a socket level that is).  The most important thing I found is that messages always should be stateless (read up on REST if this means nothing to you) For example if your application monitors the number of shipments over a network do not send incremental updates (+x) but always the new total. That makes perfect sense. Good advice. Luckily for us our messages are stateless. This is more by coincidence than by design though :) Good for you! I recently had to retrospectively change an application which had toggle-messages for all sort of things which eventually would always get confused. Stateless is a good general guideline but not *always* required. Some scenarios really demand non-stateless messages - it depends on the situation. However going stateless first is a great heuristic. And something like a ""toggle message"" is inherently bad. On the other hand as I pointed out in my answer something like ""increment"" is often exactly what you want as it will always do the correct thing on the server (vs. ""set to 5"" if you think the current value is 4)  The first thing to do is to characterize your specific network in terms of speed probability of lost messages nominal and peak traffic bottlenecks client and server MTBF ... Then and only then you decide what you need for your protocol. In many cases you don't need sophisticated error-handling mechanisms and can reliably implement a service with plain UDP. In few cases you will need to build something much more robust in order to maintain a consistent global state among several machines connected through a network that you cannot trust.  First rule of networking - you are sending messages you are not calling functions. If you approach networking that way and don't pretend that you can call functions remotely or have ""remote objects"" you'll be fine. You never have an actual ""thing"" on the other side of the network connection - what you have is basically a picture of that thing. Everything you get from the network is old data. You are never up to date. Because of this you need to make sure that your messages carry the correct semantics - for instance you may increment or decrement something by a value you should not set its value to the current value plus or minus another (as the current value may change by the time your message gets there)."
233,A,"Keeping multiple clients showing up-to-date information in Python? I am new to network programming but old to Python. I have a simple blotter program that is to be used on multiple client computers. The blotter works two ways it can show and update the information in the database. To avoid any blotters showing old data how can I make all blotters re-fetch information from the database as soon as another blotter has updated some data in the database? I would rather like to avoid complexity in setting up some client-server protocol. Is it possible to create some form of client-only protocol where a simple refresh message is passed along straight to the other blotters when they need to update their information? whats the current logic its python script ? Don't really get that question. What is frequency of changes? Is the dataset same for all blotters? Blotter records one parameter from data source or new dataset of multiple values? Could you elaborate nature of information? It's the same dataset for all blotters. The blotters could update an arbitrary amount of data in the database at any time. The frequency of changes is however not great minutes rather than seconds between changes. You could use triggers. When an information is updated send a signal (is up to you choose how maybe just an udp packet) to all blotters that will update their information consequentially. Postgresql could be scripted using python.  To receive messages (e.g. the UDP package suggested by the other poster or an http request) the clients would have to run a basic server on the client machine. You could use the Python xmlrpc module for example. However a local firewall may block the inward communication. The easiest solution if the number of clients is moderate will be to frequently poll the database for changes: add a column ""last modification time"" to your drawing table and have the clients check this field. This way the clients can find out whether they need to reload the drawing without wasting too many resources. Edit: The last modification field could either be actively updated by the client that made a change to the drawing or automatically updated by a database trigger. One question can I run a socket server (I assume that is what you are referring to) on each client (computer)? Won't I have to have one socket server and then many possible socket clients? Servers listen on a socket for incoming connections clients make these connections. Of course you can implement a single server in Python and connect to it from each client repeatedly to see if there is a message from another client but you can do the same with the database. To avoid the frequent connections you need a different architecture: your drawing application instances listen for incoming connections and update their drawing if they receive a message to do so. At start-up they have to subscribe to the news feed for example by adding their address to a database table. In your implementation also consider what happens if a client is down or unreachable and the messages cannot be delivered. Maybe that's why the other poster suggested UDP (instead of TCD). However note that UDP packages can get lost. I would go with the ""last modification time"" solution unless there are very good reasons against it. In this particular application sockets will be easy as the messages are very short. Have a look at http://wiki.python.org/moin/UdpCommunication  run the receiver loop in a separate thread (it blocks while waiting) and send a redraw event to your GUI instead of printing the message. Optionally answer the server that you got the message and have the server repeat the message every 20ms because UDP packages can get lost. (After a second without answer the server should then assume that the client was closed.) To avoid unnecessary redrawing because of the repeated messages you can still check a ""last modified"" field after receiving a UDP package. That is you now only ask the database whether a redraw is necessary if you received a UDP package. That's awesome. I managed to get one of those multicast examples to work. And you are right it's is definitely best to implement both ways i.e. a multicast refresh ping of sorts and then a last modified check. The last modified solution is probably the slickest but it also involves polling i.e. data isn't updated realtime and there will be a lot of unnecessary queries to the database. I don't mind the code overhead of the socket solution. Is it easiest done with sockets or should I look towards other frameworks such as Twisted? Are there any nice code examples out there?"
234,A,"Error parsing IP header I have a small function that tries to print the fragment offset of an IP header. ParseIpHeader(unsigned char *packet int len) { struct ethhdr *ethernet_header; struct iphdr *ip_header; /* First Check if the packet contains an IP header using the Ethernet header */ ethernet_header = (struct ethhdr *)packet; if(ntohs(ethernet_header->h_proto) == ETH_P_IP) { /* The IP header is after the Ethernet header */ if(len >= (sizeof(struct ethhdr) + sizeof(struct iphdr))) { ip_header = (struct iphdr*)(packet + sizeof(struct ethhdr)); /* print the Source and Destination IP address */ //printf(""Dest IP address: %s\n"" inet_ntoa(ip_header->daddr)); //printf(""Source IP address: %s\n"" inet_ntoa(ip_header->saddr)); printf(""protocol %d\n"" ip_header->protocol); printf(""Fragment off is %d\n"" ntohs(ip_header->frag_off)); } } My packets are TCP (the ip_header->protocol is always 6. the problem is that the frag_off is always 16384. I am sending a lot of data why the frag_off is always constant? Thanks. Also - maybe you would clarify the end goal of what you plan to achieve with the code / use case of it - maybe then I could expand on the answer. I am using recvfrom to get data from the wire. the max buffer of recvfrom is 65K. So some data are being received in the next packets and I want to merge them Fragment offset is shared with flags. You have the ""DF"" (don't fragment) bit set. Which gives you 16384 for the entire 16-bit field given the fragment offset of 0. Take a look at the http://www.ietf.org/rfc/rfc791.txt starting from page 10. EDIT: The DF bit in the TCP segments that you are receiving is set by the remote side to perform the Path MTU discovery - in a nutshell to try to avoid the fragmentation. In this case the sending side learns the biggest MTU that the overall path can handle and chops the TCP segments such that they did not exceed it after the encapsulation into IP. EDIT2: regarding the use of recvfrom() and TCP: TCP is a connection-oriented protocol and all of the segmentation/fragmentation details are already handled by it (fragmentation is obviously handled by the lower layer IP) - so you do not need to deal with it. Anything you write() on the sending side will be eventually read() on the other side - possibly not in the same chunks though - i.e. two 4K writes may result in a single 8K read sometimes and sometimes in two 4K reads - depending on the behaviour of the media inbetween concerning reordering/losses. IP Fragmentation and reassembly is handled transparently by the operating system so you do not need to worry about it same as about packets out of order etc. (you will just see the decreased performance as the effect on the application). One good read I could recommend is this one: UNIX network programming. Given Steven's involvement with the TCP it's a good book no matter which OS you use. EDIT3: And if you are doing something to be a ""man in the middle"" (assuming you have good and legitimate reasons for doing so :-) - then you can assess the upcoming work by looking at the prior art: chaosreader (one-script approach that works on pcap files but adaptable to something else) or LibNIDS - that does emulate the IP defragmentation and the TCP stream reassembly; and maybe just reuse them for your purposes. Thanks for the response. However I am queering the data from a packet. I am not setting the DF flag. if ((ntohs(ip_header->frag_off) & 0x3fff) == 0) this means I am reading the first packet. Since the flags occupy the three most significant bits the check above in fact checks that this packet is not fragmented at all (offset is zero and MF (more fragments) is not set). In any case this mask of 0x3fff would remove the DF that is set by the remote side (default on most of the systems to perform Path MTU discovery). Thanks Andrew. So what kind of code do I need to see if this is the first fragment or not? If you only need to know fragment offset then you'd mask away the 3 bits of flags by using the 0x1fff as the mask. Though would be interesting to know what you plan to do with this - merely knowing something is a first fragment might or might not be sufficient. I.e. if you assume that the TCP header always fits into the first fragment - in case of malicious/buggy implementations on the other side it will not be a correct assumption. OK. Assume that I have an echo server that sends lines of text. I am sniffing the wire. The first packet contains the text ""Hello ... "" and a few moments later comes a new packet with the text "" ... world"". How to I know that the second packet that comes after the first packet is the next packet? I want to concatenate them in order to create buf=""hello ... world"". So the question is whether you are sniffing the wire or you are the client on the other side using the sockets API ? I am actually using raw sockets. len = recvfrom(raw packet_buffer 5000 0 ...); how do i know that the package is completely received? uh. if you are using raw sockets then there is **a lot** more work besides the full IP reassembly TCP state machine handling etc. But take a look at my 3rd edit and see if that is something that might suit you - you can study the source of these to see what you're up for :-) @cateof: Sequence number tells you the offset of the data in the TCP stream. Get a copy of _TCP/IP Illustrated Volume 1_ [Google Books](http://books.google.com/books?id=-btNds68w84C&lpg=PA275&dq=tcp%2Fip%20illustrated%20volume%201&pg=PA275#v=onepage&q=tcp/ip%20illustrated%20volume%201&f=false) to learn about how TCP works."
235,A,virtual memory consumption of pthreads Hello I developed a multi-threaded TCP server application that allows 10 concurrent connections receives continuous requests from them after some processing requests responds them to clients. I'm running it on a TI OMAP l137 processor based board it runs Monta Vista Linux. Threads are created per client ie 10 threads and it's pre-threaded. it's physical memory usage is about %1.5 and CPU usage is about %2 according to ps top and meminfo. It's vm usage rises up to 80M where i have 48M (i reduced it from u-boot to reserve some mem for DSP). Any help is appreciated how can i reduce it??.(/proc/sys/vm/.. tricks doesn't help :) Thanks. You can try using a drop in garbage collecting replacement for malloc() and see if that solves your problem. If it does find the leaks and fix them then get rid of the garbage collector. Its 'interesting' to chase these kinds of problems on platforms that most heap analyzers and profilers (e.g. valgrind) don't fully (if at all) support. On another note given the constraints .. I'm assuming you have decreased the default thread stack size? I think the default is 8M you probably don't need that much. See pthread_attr_setstacksize() if you haven't adjusted it. Edit: You can check the default stack size with pthread_attr_getstacksize(). If it is at 8M you've already blown your ceiling during thread creation (10 threads as you mentioned). thanks! virtual memory usage is fine now it worked thank you. ulimit doesnt help i will try pthread_attr_setstacksize() i think it's at default values if it is not set by kernel configuration. @yet  then each thread probably has either a 4 or 8 MB stack. You can check with pthread_attr_getstacksize() to see the default.  Does it rise to that level and stay there? Or does it eventually run out of memory? If the former you simply need to figure out a way to have a smaller working set. If the latter you have a memory leak and need to fix it. It stays there and works good for a day time but after 48 hours it was killed by a magical force which i could not find any clue of. dmesg and the other logs are clean the process is just ended. I'd wager that 'magical force' was your memory leak causing your application to exhaust its heap. thanks btw if it's killed by OOM-Killer where can i find the logs? as i said it's like a magical hand that stops it. (i double checked the logins :) Which language? If C then check each malloc()'s return value and if NULL you should log it. If C++ then make sure you are using new (nothrow) then do the same. Otherwise put try...catch around each new and log the allocation failure in either case. it's ANSI C i ll check 'errno' but i think the thing you said leads to memory corruption and segmentation faults not a mem leakage.  Most VM is probably just for stacks. Of course it's virtual so it doesn't get commited if you don't use it. (I'm wondering if thread's default stack size has anything to do with ulimit -s) Apparently yes according to this other SO question yes ulimit verboses that it is unlimited i tried it this morning it did not help i ll try to set some thread specific limitation.
236,A,How to forward wireless packet to ethernet interface in windows? How to forward wireless packet to ethernet interface in windows? Please edit your question to tell us more about the context of what you want to do. Why do you want to forward packets? You can use built-in IP packet forwarding feature of Windows to forward packets from your Wireless NIC to your Wired (ie. Ethernet) NIC.
237,A,Different types in an array how to know what to cast them to when taking them out of array? Assume a generic List of type Packet holding custom classes extending Packet such as LoginPacket or ChatPacket. Let's say I put these in a list. When I take them out the most 'specific' type each will be is Packet. I want to cast these into their more specific types though back into their original LoginPacket or ChatPacket or back into whatever their original types were. Question: How can I do this? Reference: How to cast an object programmatically at runtime? So...it seems like if these custom classes share a common interface it would solve the casting problem? But if so my classes can't all share one interface. I need to build interfaces upon interfaces and classes upon classes. So...how would I do this? Sorry if this question isn't so accurate. Not sure how to articulate my exact issue. You can do something like this: if (myPacket is LoginPacket) { // Do LoginPacket stuff } else if (myPacket is ChatPacket) { // Do ChatPacket stuff } Once you know if its the right type you can cast it using the as keyword ProcessLoginPacket(myPacket as LoginPacket); Ay but what if there are many types and you have to do this conversion really often? Would this be a design issue? generally speaking its better to design the classes such that you don't really care what specific type they are (if you can). For example it would be better to define a method on the base class such as DoStuff() and each subclass would override and provide its own implentation of DoStuff() and then you would just call packet.DoStuff() and depending on the packet type it will call its own DoStuff() method What Daniel said. Amazing thanks.  You could have a whole long list of if/else or a switch if (item is ChatPacket) { // cast } else if (...) { // cast } else ... However that can become unwieldly. You may wish to revisit your class design. By having a List<Packet> you're effectively stating you do not care about the differences between the derived children at least in terms of their unique APIs. In other words if your design is class Packet { public void Foo() { } } class ChatPacket : Packet { public void Bar() { } } You're saying you only care about being able to access Foo(). If you do care about differences perhaps you can express those differences with polymorphism via abstract or virtual methods in the base and overriden behaviors in the children. Therefore you still have a collection of base classes but you still get the custom behaviors as specified by each child. class Packet { public virtual void Foo() { } } class ChatPacket : Packet { public override void Foo() { } } Here you call just call Foo(). No casting necessary. Great answer thanks.
238,A,"overriding non-NAPI network polling handler with a kernel module As those familiar with network device drivers are aware the interface between the kernel and driver has been changed to use NAPI (New API). In this paradigm a polling function is associated with a napi_struct which is a new structure that must be allocated by the device driver. At any rate I have an old driver and don't have the time/will to convert it to NAPI. Since the kernel has been modified to account for NAPI they use the default function ""process_backlog"" (/net/core/dev.c) to handle the pre-NAPI functionality. This function is still called within interrupt context on a per packet basis (as opposed to NAPI which polls the queue so it doesnt need interrupted each time). I need to override this function but my problem is that unlike the NAPI poll function that is assigned and created by the device driver the process_backlog is associated with the per-CPU input queue at boot-time. When I load the module I can override the method using  struct softnet_data *queue backup; queue = &__get_cpu_var(softnet_data); backup = *queue; queue->backlog.poll = my_process_backlog; The problem here is that this is only overriding the queue on the CPU that inits the module. Is there some way I can cycle through all the CPU's in a module context? I feel like there has to be some way to do this. Cheers That's a really bad idea. But if you wanted to set some other per cpu variable you could use per_cpu(name cpu). Update: why is it a bad idea? Because you're binary patching a part of the core networking code when your driver is loaded. That effects every network driver in the system not just yours. It's like having a kernel module that replaces the scheduler. If the rest of the kernel was written like that it would never work. Oh and you remembered to change back to the original process_backlog() when your module is removed right? If you really think you need to change process_backlog() then you should make the change in the source code and distribute a custom kernel. Or explain the change you need to the upstream networking community and get it accepted upstream. Or probably better still just convert your driver to NAPI it's not that hard. Can you expand on why this is bad? Note - I'm not trying to make something that's useful in a general way or work for all things. I'm making a specific driver for specific hardware for specific purposes."
239,A,How to access the same data using threads? How would I go about doing this in a client-server application? I want to basically let clients access the same data through separate threads and I want avoid concurrency thread issues so I'm asking for examples. Can you elaborate more about what you are trying to do? Same data with different threads in a client-server application is a little vague. To access the same piece of data you just need to ensure you're accessing the same instance of the class. So 2 threads hitting the same singleton or the same object obtained from some factory will both be getting the same piece of data. Obviously the exact scenario depends on your application. Concurrency issues will generally only arise when some piece of data is being modified. Say for example you had the following class: class Container { private List<?> data; public List<?> getData() { return this.data; } public void addData(Object data) { this.data.add(data); } } Then that would not necessarily be thread safe. To make it thread safe you need to lock on an object monitor. This could be done in one of two ways in this scenario: class Container { private List<?> data; public synchronized List<?> getData() { return this.data; } public synchronized void addData(Object data) { this.data.add(data); } } This solution would lock calls to getData and setData on the Container instance's lock. This is fine except if you were to add a third method to this class which didn't use data at all then it would also be blocked if any thread was inside getData or setData. To make it more specific you could do: class Container { private List<?> data; public List<?> getData() { synchronized (this.data) { return this.data; } } public void addData(Object data) { synchronized (this.data) { this.data.add(data); } } } This then uses the data variables object lock. The advantage of this is if you add a third method which doesn't use data then it won't be blocked by calls to getData or setData. That's a basic overview of Java's concurrency safety hope it helps. Note that the client-server nature of your problem doesn't really change much. If your threads are being created by for example thread per request via java servlets or something then this all applies in exactly the same way.
240,A,Creating a factory for which can support both real object & a NullObject I want to create a factory which will create a smack XMPPConnection. The factory should return the real XMPPConnection or a NullObject if the connection could not be made. However smack's XMPPConnection is a concrete class. It does not implement any interfaces so I can't use java dynamic proxy API to proxy the sucker. I could extend the XMPPConnection but that's not very elegant. Are there any other options ? Note that the factory must never return a null ! You would have to create your own interface that basically duplicates the XMPPConnection and create an Adapter that implements that interface and wrapps the real XMPPConnection class. That way you could have a NullXMPPConnection as well. That's a neat idea. Is it worth the effort ? I could just give up on the factory & do a new XMPPConnection every time. If you are only creating this Connection in one place it is probably not worth it but if you use it all over the place I'd say yes. You can also add loads of Design Patterns to your CV :) I count 3 places where I will need a XMPPConnection. Note that I am working in a GUICE environment. So the factory is helping me to inject the connections to different classes. I'd probably give it a go then unless it seems like too much work. Nope. It looks pretty easy. Working on it right now. Thanks again.
241,A,Creating RAW Sockets tutorial/explanation? So Im trying to further my understanding of sockets but I want to start out at the lowest level first (well in C not assembly lol) However most sites that I deal with use the SOCK_STREAM or SOCK_DGRAM. However I have read around on Beejs guide....but I don't know if that actually deals with RAW sockets or not. I'll obviously need to call SOCK_RAW in my call to sockets but theirs not really a WHOLE lot of information about it. And this is just for learning purposes i always try to understand the root of whats going on in abstraction. Thanks http://www.google.com/search?q=linux+raw+socket+sample+source Assuming you're using Linux I would look at the man pages for socket and packet as a start. From here and after reading a few other man pages to figure out how to actually send packets on the wire I'd install Wireshark and experiment with sending hand-crafted packets and using Wireshark to capture and analyze them. You can learn a lot about the different network layers this way and how you can tweak different values in various headers.
242,A,"Is it possible to make real-time network games in JavaScript Is it possible to make real-time network games using JavaScript? I've seen flash do it but I'm interested in making a multiplayer browser-based game that doesn't depend on any plugins. I've read that it is impossible to keep Ajax connections open for streaming communication and it isn't feasible to make several new Ajax connections per second to keep the client in sync with the server. It is. Look into a technology called Comet (like Ajax on steroids). Lift (a Scala web framework; twitter and others use it) has excellent Comet support build-in.  As I know sockets without plugins is possible only in HTML5. But you can use flash to accomplish this task. Since almost every browser supports flash now I think it is ok. Also there are some hacks which allow do the same thing without bunch of ajax calls. Try to find Long Polling. Hope it helps  Yes and it doesn't require any special libraries as certain people seem to imply. I question whether the author would want to set about this large task without using JavaScript libraries. Libraries and plugins are different. E.g JavaScript libraries can be downloaded at runtime in the page without requiring user action (so not a big deal); whereas plugins must be explicitly installed into the browser by the user. I question why would someone install Windows (tm) just to have Calculator functionality. You get my point... I understand calculator usage is overkill for Windows platform; however the author is proposing an opposite scenario: bringing something large that normally requires speed and resources into the ""meek"" browser platform. So you lost me on the analogy :) Well I didn't tell him *not to use* libraries I'm just saying that I did this kind of stuff before without any specialized software...it involves a persistent connection (a normal ajax call) and the server delaying the reply for ~25secs (or until a change in situation). Not such a big task. See here: http://stackoverflow.com/questions/3235973/php-code-to-convert-php-to-js (t'was a long story though...)  It looks like http://socket.io/ is good solution. I used socket.io in [groovebasin](https://github.com/superjoe30/groovebasin) and it definitely works great. Groovebasin is not a game but socket.io meets all the requirements.  WebSockets are the solution for realtime (low latency) networking with JavaScript in the browser. There are fallbacks for providing the WebSocket API with Flash. You can stick with JavaScript on the server and use something like http://RingoJs.org which has connectors for WebSockets. If you use those two you get at this: // SERVER websocket.addWebSocket(context ""/websocket"" function(socket) { socket.onmessage = function(m) { // message m recieved from server }; socket.send('my message to the client'); }); // CLIENT var ws = new WebSocket(""ws://localhost/websocket""); ws.onMessage(function(m) { // message m recieved from server // do something with it return; }); ws.send('message to server');"
243,A,"TCP Socket Communication Limit Is there any limit on the size of data that can be received by TCP client. With TCP socket communication server is sending more data but the client is only getting 4K and stopping. Come on now. Think about it. That would make the internet a really boring place wouldn't it? Could you post some of the client's code here No it should be fine. I suspect that your code to read from the client is flawed but it's hard to say without you actually showing it.  I'm guessing that you're doing exactly 1 Send and exactly 1 Receive. You need to do multiple reads there is no guarantee that a single read from the socket will contain everything. The Receive method will read as much data as is available up to the size of the buffer. But it will return when it has some data so your program can use it. +1 for probably being correct given a dearth of details to work with. The other less likely possibility is that they aren't doing a final flush after the last send and the Nagle algorithm is taking its sweet time to send the rest. Listen to this guy! It's a well known fact and that a `read()` and `write()` cannot guarantee reading and filling your entire buffer or writing out your entire buffer. You maybe have to either buffer things being sent in and out or in a synchronous application implement a `send_all()` and `receive_all()` functions.  There's no limit for data with TCP in theory BUT since we're limited by physical resources (i.e memory) implementors such as Microsoft Winsock utilize something called ""tcp window size"". That means that when you send something with the Winsock's send() function for example (and didn't set any options on the socket handler) the data will be first copied to the socket's temporary buffer. Only when the receiving side has acknowledged that he got that data Winsock will use this memory again. So you might flood this buffer by sending faster than it frees up and then - error! i think this is the main problem. are we supposed to change the registry keys to fix this problem ?  No limit TCP socket is a stream.  You may consider splitting your read/writes over multiple calls. I've definitely had some problems with TcpClient in the past. To fix that we use a wrapped stream class with the following read/write methods: public override int Read(byte[] buffer int offset int count) { int totalBytesRead = 0; int chunkBytesRead = 0; do { chunkBytesRead = _stream.Read(buffer offset + totalBytesRead Math.Min(__frameSize count - totalBytesRead)); totalBytesRead += chunkBytesRead; } while (totalBytesRead < count && chunkBytesRead > 0); return totalBytesRead; } public override void Write(byte[] buffer int offset int count) { int bytesSent = 0; do { int chunkSize = Math.Min(__frameSize count - bytesSent); _stream.Write(buffer offset + bytesSent chunkSize); bytesSent += chunkSize; } while (bytesSent < count); } //_stream is the wrapped stream //__frameSize is a constant we use 4096 since its easy to allocate."
244,A,"listen() without calling bind() I tried the following: int sockfd = socket(...); listen(sockfd 10); accept(sockfd ...); None of the calls failed and the program just started blocking as if I had called bind(). What will happen in this case? Will it just be impossible to ever receive a connection since it has no local address or port? Or was it implicitly assigned a local address and port and now it is listening on those? If so how can I retrieve what those are? As others mentioned the OS will assign a port if you don't bind() one. You can use the getsockname() call after listen() to see what address/port are assigned. Then if you communicated that address/port to a client it could connect. So it makes sense that this works. You could write a program where this was an interesting thing to do.  I suspect you will never get a connection. You can check if there is a new socket listening by doing the equivalent of ""netstat -an"" on your OS before and after running the program. I tried the same thing in C#: Socket socket = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); socket.Listen(1); socket.Accept(); I get an exception on the second line which informs me in a roundabout way that a Bind is required. It of course works fine with: Socket socket = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); socket.Bind(new IPEndPoint(IPAddress.Loopback 4567)); socket.Listen(1); socket.Accept(); I tested with a Udp socket as well and same thing Windows is not happy on the Listen call unless a binding has been done. Since the .Net socket calls are simply wrappers around Winsock this behaviour is likely to be the same for all Winsock derived libraries on Windows.  The calls are working but since you didn't bind the socket implicitly the operating system or system library provided a port and default binding for you (exactly the same as when you call connect(2) without calling bind(2) first). Also since you asked about the TCP stuff earlier I'm assuming you're talking about internet sockets here. Finding out what name the OS bound the socket to varies between operating systems so you will have to look for your specific OS but most operating systems provide a netstat or similar tool that you can use to query which applications are listening on which ports. As John mentions in a comment you can use getsockname(2) to later find a socket's name. Here is a short example: // ... // Create socket and set it to listen (we ignore error handling for brevity) int sock = socket(AF_INET SOCK_STREAM 0); listen(sock 10); // Sometime later we want to know what port and IP our socket is listening on socklen_t addr_size = sizeof(struct sockaddr_in); struck sockaddr_in addr; getsockname(sock (struct sockaddr *)&addr &addr_size); addr will now contain the port and IP address that your socket is listening to. The getsockname() call will give you the address/port that the OS assigned. Good point John... I will add an example code."
245,A,"Determine the IP Address of a Printer in C# I would like to determine the IP address of a printer using C# (.NET 2.0). I have only the printer share name as set up on the Windows OS in the format \\PC Name\Printer Name. The printer is a network printer and has a different IP address to the PC. Does anyone have any pointers? Thanks in advance for your help. Regards Andy. using of class WIN32_Printer is not enough here. It should be combined with Win32_TCPIPPrinterPort. Below is the code which should help: static void Main(string[] args) { var scope = new ManagementScope(@""\root\cimv2""); scope.Connect(); var searcher = new ManagementObjectSearcher(""SELECT * FROM Win32_Printer""); var results = searcher.Get(); Console.WriteLine(""Network printers list:""); foreach (var printer in results) { var portName = printer.Properties[""PortName""].Value; var searcher2 = new ManagementObjectSearcher(""SELECT * FROM Win32_TCPIPPrinterPort where Name LIKE '"" + portName + ""'""); var results2 = searcher2.Get(); foreach (var printer2 in results2) { Console.WriteLine(""Name:"" + printer.Properties[""Name""].Value); //Console.WriteLine(""PortName:"" + portName); Console.WriteLine(""PortNumber:"" + printer2.Properties[""PortNumber""].Value); Console.WriteLine(""HostAddress:"" + printer2.Properties[""HostAddress""].Value); } Console.WriteLine(); } Console.ReadLine(); }  Just adding an another solution here using .Net Framework 4.0 or higher Using System.Printing var server = new PrintServer(); var queues = server.GetPrintQueues(new[] { EnumeratedPrintQueueTypes.Local EnumeratedPrintQueueTypes.Connections }); foreach (var queue in queues) { string printerName = queue.Name; string printerPort = queue.QueuePort.Name; }  I know this is an old post but I had the same issue where I was able to get the Printer Port name but not the IP. In my case I couldn't rely on the Port Name being IP_[IP Address] but found how to get hold of the actual IP from the port name. Windows stores the information about ports in the registry under HKLM\SYSTEM\CurrentControlSet\Control\Print\Monitors\Standard TCP/IP Port\Ports\[port name] This key contains the values set up in the port configuration page including IP address and port number. A quick C# example to get the IP address using Microsoft.Win32; RegistryKey key = Registry.LocalMachine.OpenSubKey(@""System\CurrentControlSet\Control\Print\Monitors\Standard TCP/IP Port\Ports\"" + printerPortName RegistryKeyPermissionCheck.Default System.Security.AccessControl.RegistryRights.QueryValues); if (key != null) { String IP = (String)key.GetValue(""IPAddress"" String.Empty RegistryValueOptions.DoNotExpandEnvironmentNames); } Michael thanks for the additional information. +1  Based on the link http://stackoverflow.com/questions/296182/how-to-get-printer-info-in-c-net (Thanks Panos I was already looking at the link!) I have the following solution from Panos's answer: using System.Management; ... string printerName = ""YourPrinterName""; string query = string.Format(""SELECT * from Win32_Printer WHERE Name LIKE '%{0}'"" printerName); ManagementObjectSearcher searcher = new ManagementObjectSearcher(query); ManagementObjectCollection coll = searcher.Get(); foreach (ManagementObject printer in coll) { string portName = printer[""PortName""].ToString(); if(portName.StartsWith(""IP_"")) { Console.WriteLine(string.Format(""Printer IP Address: {0}"" portName.Substring(3))); } } Obviously this only works if the port name for the printer is given in the format ""IP_IPAddress"" which is I believe is the default.  Check this question: How to get Printer Info in C#.NET?. I think that you have to get the property PortName from the WMI properties. Panos Thanks for the pointer much appreciated.  Is this printer set up in a network which has Active Directory? Or is this on your own local network with just a switch and your printer plugged into it? If it is the former then you should be able to query for it based on the ""printer name"". This article show how to get c# .net to connect to the AD. But this does require some knowledge of AD servers in your network. This solution seems a bit long to me but may be a good starting point?"
246,A,"C++ asynchronous network programming I have some simple questions I have a client-server application and data sent on the wire. I'd like to be able to recover the data and handle it properly. struct T1 { int id; int foo; }; struct T2 { int id; char foo; int bar; }; Let's take these structures they are sent on the network preceded by an int which will tell if either T1 or T2 follows. If the client sends me T1 then sends me T2 do I have the assurance that I can read a full structure with asio::ip::tcp::socket.async_read() ? I'd like to set up a handler which will handle a single struct however what will happen if I'm unable to read everything in a single async_read() ? The asynchronous operation will continue until one of the following conditions is true: The supplied buffers are full. That is the bytes transferred is equal to the sum of the buffer sizes. An error occurred. Will it discard the data that cannot be read ? will it trigger another async_read ? And am I assured that an async_read will only get one ID+structure if my client sends me sequentially the ID+structure ? Or may the OS optimize things and put them both in the same paquet ? As you may have seen I'm a little confused I'd like to make the right decisions when designing a server/client application any help will be appreciated. Thank you. EDIT: Thanks to @timo for pointing out a mistake. async_read won't complete until an entire struct is read so you don't need to loop. Otherwise my answer is the same. Nothing will be lost regardless of how the TCP protocol breaks up or coalesces the data. If there isn't enough data in the TCP buffers to fill your input buffer the read will simply fetch whatever is available and report the number of bytes fetched. What happens next is up to you. You might have enough data in the fetched bytes to decide you don't want to continue so asio makes no assumptions. If you haven't read enough bytes to comprise a complete structure then initiate a further async_read repeating the process until you have enough bytes or something dies. I'm not sure what you mean by ""unable to read everything."" two possible meanings come to mind: The amount of data that's available to read doesn't fill a struct. In this case you simply do another read_async to wait for more data to arrive. The struct doesn't absorb all that data that has arrived. The TCP stack will simply buffer unread data until you get around to reading it. What you're descibing is the behavior of async_read_some. async_read won't finish until the buffer is full or error occured. Thank you. So if the amount of data that's available to read doesn't fill a struct and that I do another async_read I'll have to use some static buffer to keep track of the incomplete structure. @sirAA: No the point of @Timo's correction (and my amendment) is that async_read will wait till the entire struct is available before completing."
247,A,"Small http server using java? I have created the following test server using java:  import java.io.*; import java.net.*; class tcpServer{ public static void main(String args[]){ ServerSocket s = null; try{ s = new ServerSocket(7896); //right now the stream is open. while(true){ Socket clientSocket = s.accept(); Connection c = new Connection(clientSocket); //now the connection is established } }catch(IOException e){ System.out.println(""Unable to read: "" + e.getMessage()); } } } class Connection extends Thread{ Socket clientSocket; BufferedReader din; OutputStreamWriter outWriter; public Connection(Socket clientSocket){ try{ this.clientSocket = clientSocket; din = new BufferedReader(new InputStreamReader(clientSocket.getInputStream() ""ASCII"")); outWriter = new OutputStreamWriter(clientSocket.getOutputStream()); this.start(); }catch(IOException e){ System.out.println(""Connection: "" + e.getMessage()); } } public void run(){ try{ String line = null; while((line = din.readLine())!=null){ System.out.println(""Read"" + line); if(line.length()==0) break; } //here write the content type etc details: System.out.println(""Someone connected: "" + clientSocket); outWriter.write(""HTTP/1.1 200 OK\r\n""); outWriter.write(""Date: Tue 11 Jan 2011 13:09:20 GMT\r\n""); outWriter.write(""Expires: -1\r\n""); outWriter.write(""Cache-Control: private max-age=0\r\n""); outWriter.write(""Content-type: text/html\r\n""); outWriter.write(""Server: vinit\r\n""); outWriter.write(""X-XSS-Protection: 1; mode=block\r\n""); outWriter.write(""<html><head><title>Hello</title></head><body>Hello world from my server</body></html>\r\n""); }catch(EOFException e){ System.out.println(""EOF: "" + e.getMessage()); } catch(IOException e){ System.out.println(""IO at run: "" + e.getMessage()); }finally{ try{ outWriter.close(); clientSocket.close(); }catch(IOException e){ System.out.println(""Unable to close the socket""); } } } } Now i want this server to respond to my browser. that's why i gave url: http://localhost:7896 and as a result i receive at the server side: ReadGET / HTTP/1.1 ReadHost: localhost:7896 ReadConnection: keep-alive ReadCache-Control: max-age=0 ReadAccept: application/xmlapplication/xhtml+xmltext/html;q=0.9text/plain;q=0.8image/png*/*;q=0.5 ReadUser-Agent: Mozilla/5.0 (X11; U; Linux i686; en-US) AppleWebKit/534.10 (KHTML like Gecko) Chrome/8.0.552.224 Safari/534.10 ReadAccept-Encoding: gzipdeflatesdch ReadAccept-Language: en-USen;q=0.8 ReadAccept-Charset: ISO-8859-1utf-8;q=0.7*;q=0.3 ReadCookie: test_cookie=test cookie Read Someone connected: Socket[addr=/0:0:0:0:0:0:0:1port=36651localport=7896] And a blank white screen at my browser and source code also blank. In google chrome browser. So can anyone please tell me where i m wrong. actually i am new to this thing. so please correct me. Thanks in advance Ok this worked after adding one more \n after the headers. So thanks all for their support Hey but still one thing that seems me odd that addr of browser being 0:0:0:0:0:0:0:1?? what this means A pointer to you if you dont know already - ""How Tomcat Works"" by Budi Kurniawan  Paul Deck is a good start point. You almost certainly don't want to be using DataOutputStream on the response - and writeUTF certainly isn't going to do what you want. DataOutputStream is designed for binary protocols basically - and writeUTF writes a length-prefixed UTF-8 string whereas HTTP just wants CRLF-terminated lines of ASCII text. You want to write headers out a line at a time - so create an OutputStreamWriter around the socket output stream and write to that: writer.write(""HTTP/1.1 200 OK\r\n""); writer.write(""Date: Tue 11 Jan 2011 13:09:20 GMT\r\n""); etc. You may want to write your own writeLine method to write out a line including the CRLF at the end (don't use the system default line terminator) to make the code cleaner. Add a blank line between the headers and the body as well and then you should be in reasonable shape. EDIT: Two more changes: Firstly you should read the request from the client. For example change din to a BufferedReader and initialize it like this: din = new BufferedReader(new InputStreamReader(clientSocket.getInputStream() ""ASCII"")); then before you start to write the output read the request like this: String line; while ((line = din.readLine()) != null) { System.out.println(""Read "" + line); if (line.length() == 0) { break; } } EDIT: As noted in comments this wouldn't be appropriate for a full HTTP server as it wouldn't handle binary PUT/POST data well (it may read the data into its buffer meaning you couldn't then read it as binary data from the stream). It's fine for the test app though. Finally you should also either close the output writer or at least flush it - otherwise it may be buffering the data. After making those changes your code worked for me. ok thanks jon. i am just checking this out Hey jon i have done that as you told me but still getting the same error @codeomnitrix: You need to close the writer before you close the socket - either that or at least *flush* the writer. Will edit to mention that. @codeomnitrix: You also need to read the request at least from Chrome - I've edited the answer in include that. But note that the code in your question still doesn't have a blank line between the headers and the body. You need an *empty line* e.g. `outWriter.write(""\r\n"")`. ok trying jon. thanks for your support hey jon i have done that much and pretty good response as error gone away but still blank browser screen without any source code. And secondly would you please tell me what does that means by having close the writer earlier than socket. The `BufferedReader` is not appropriate for reading incoming HTTP requests as soon as there is a message entity which might contain binary data (i.e. file upload of an image using PUT/POST). Hey Partlycloudy ya true. but first i have to create this simpler one but its not working as i m getting blank html page at browser. so can you please tell me about that. and thanks for your response @PartlyCloudy: That's certainly true for a general HTTP server... but just to get this working it seemed like an easy approach :) Will edit the answer to make that clear. @Jon Skeet: I see your point that this was the right help suggested in this case. Just wanted to make clear that this is generally not the best solution however. @PartlyCloudy: Yup it was a helpful comment. Let me know if you feel the clarification isn't strong enough.  I'm not sure if you have can use writeUTF instead instead you may need to use writeBytes. Also you need to terminate each line with a '\n'. Hey i have made changes to my code but still getting the same error on google chrome. please check this Hey abdullah i have edited the code and now its working but i found a blank output at google chrome with page source also blank You need to terminate each header with \r\n. You don't need to line-terminate the body at all.  The HTTP protocol is ASCII based exept the body which depends on the Content-Type header. So no UTF-8 headers! Headers and body must be separated by an empty line. Why do you set your Transfert-Encoding to chuncked? Your body is not. hey can you please give me a single line as an example how to write the headers  Check this out it's already done for you: http://www.mcwalter.org/technology/java/httpd/tiny/index.html ya checking. thanks  If you're interested in learning the design and development of network servers like HTTP servers in Java you might also have a look at this repo: https://github.com/berb/java-web-server It's a small HTTP server in Java I started for educational purposes. Though it shouldn't be used in production or serious use cases yet. I'm still adding new features. It currently provides multi-threading static file handling Basic Authentication logging and a in-memory cache. EDIT An obvious error in your code is the missing \r\n between your Response Header and your HTML. Just append an additional \r\n to your last header. Additionally you must provide the content length unless you're using Chuncked Encoding: String out = ""<html><head><title>Hello</title></head><body>Hello world from my server</body></html>\r\n""; outWriter.write(""Content-Length: ""+out.getBytes().length+""\r\n\r\n""); outWriter.write(out); ok thanks i will check this out. and thanks for your response kk thanks partlycloudy that solved but what does the address or browser means which is 0:0:0:0:0:0:0:1 `0:0:0:0:0:0:0:1` is `localhost` when using IPv6. ok very thanks partlycloudy"
248,A,"Android HTTP Connection Can anybody tell my why this doesn't work in the Android emulator? From the browser I have access and the server is internal. All I can think of is that I'm missing some configuration on my app so it can access the network layer. try { InetAddress server = Inet4Address.getByName(""thehost""); //Doesn't work either //or InetAddress server2 = Inet4Address.getByAddress(new String(""192.168.1.30"").getBytes()); if(server.isReachable(5000)){ Log.d(TAG ""Ping!""); } Socket clientsocket = new Socket(server 8080); } catch (UnknownHostException e) { Log.e(TAG ""Server Not Found""); } catch (IOException e) { Log.e(TAG ""Couldn't open socket""); } Throws an UnknownHostException Thanks As far as configuration goes the only setting you should need to access the Internet from your application is the INTERNET permission enabled by adding the following line outside the Application tags within your application Manifest. <uses-permission android:name=""android.permission.INTERNET"" /> So the manifest would follow this general construction <manifest xmlns:android=""http://schemas.android.com/apk/res/android"" package=""com.example.android.apis""> <uses-permission android:name=""android.permission.INTERNET"" /> <application android:name=""MyApplication"" android:label=""@string/application_title"" android:icon=""@drawable/my_icon""> [ .. Your Activities go here ] </application> </manifest> That was it. Thank you very much. Excellent! I love it when it's the easy solutions. Wow! This thing has reached 3k views... that should give you lots of SO points Reto didn't work for me  It might still not work because of the timeout. Since you need root permissions to send an ICMP Package and the implemetation of isReachable will use the slow TCP version of ECHO. Chekcout the javaDoc."
249,A,"How can i convert NSData to const uint8_t?or getting error while passing a NSData over Wifi i have a array with two strings: NSArray *sendingArray = [[NSArray alloc]initWithObjects:@""CAT""@""DOG""nil]; and i converted the array to NSData: NSData *myData = [NSKeyedArchiver archivedDataWithRootObject:sendingArray]; now i need to convert my NSData(myData) to const uint8_t i am doing this code to send an array of values through Wifi network. i send successfully a string over wifi using the below code(my string is ""string"").  const uint8_t *message = (const uint8_t *)[string UTF8String]; [_outStream write:bytes maxLength:len]; i wrote the below code to send NSData object but it got error. code is: const uint8_t *bytes = (uint8_t)[myData bytes];//this line getting error. [_outStream write:bytes maxLength:len]; can i send my NSData(myData) through the above method without converting it to const uint8_t? if yesplease give me the code that i need to write. or can any one tell me the way to convert my NSData(myData) to const uint8_t You don't think it would be helpful for us to see the error you're getting?  const uint8_t *bytes = (const uint8_t*)[myData bytes]; // ^^ note the asterisk thank you .........."
250,A,"Running 2 threads simultaneously In the case of an IM client. I have made 2 separate threads to handle sending packets (by std io) and receiving packets. The question is how to make these 2 threads run simultaneously so that I can keep prompting for input while at the same time be ready to receive packets at any time? I have already tried setting a timer but the data is always lost receiving. In general a snippet of code that demonstrates your behaviour goes a long way toward getting a good quality answer. Well they won't run simultaneously unless you have a multiprocessor computer but that's not usually the issue. What will happen is that each thread will get a slice of time more or less alternatively. If you're losing I/O it's probably not the threading that's your real problem. can you tell us how you're reading this stuff?  Without more details it is hard to give a complete answer. Nevertheless here is the code for starting two threads: Thread thread1 = new Thread () { public void run () { // ... your code here } }; Thread thread2 = new Thread () { public void run () { // ... your code here } }; thread1.start(); thread2.start(); as in ur code is capable of creating a thread locally in a main my send and receive threads rely on sending and listening on UDP socket respectively. from what i can understand your sample codes seem to inject the thread in the code instantly. i have created a runnable class for them so will your approach be applicable to such? I don't know what ""inject the thread in the code instantly"" means. Perhaps you can post sample code to further explain what you mean? @lamsait: My code creates new threads which run independently. These are created wherever you execute them. I agree with Karlp that you seem to have a fundamental misunderstaning of threads. yes indeed i have to admit that in writing this IM service program 70% of the syntax methods and classes are never heard of before. Big thanks to both you and Karlp. That explained it pretty good :)  I think you might have missed something significant with either Threads Streams or both :-) You can start a new thread like this:  myThread.start(); The thread will be started and the run() method will be executed automatically by the jvm. If the threads run-method is reading from a Stream and it is the only one reading it will not ""miss"" anything in that stream."
251,A,"Advantages of knowing for a client how big the package sended by the server is I'm really new at network-programming so I hope this isn't a complete Newbie-question. I read a tutorial at the Qt-Homepage how to build a little server and I found this: QByteArray block; QDataStream out(&block QIODevice::WriteOnly); out << (quint16)0; out << ""...""; // just some text out.device()->seek(0); out << (quint16)(block.size() - sizeof(quint16)); At the start of our QByteArray we reserve space for a 16 bit integer that will contain the total size of the data block we are sending. [We continue by streaming in a random fortune.] Then we seek back to the beginning of the QByteArray and overwrite the reserved 16 bit integer value with the total size of the array. By doing this we provide a way for clients to verify how much data they can expect before reading the whole packet. So I want to know what are the advantages of this procedure? What can happen if you don't do that? Maybe you also could add a little example. Knowing the packet size before receiving it has a performance advantage. You can then allocated exactly the needed number of bytes from the heap or whatever buffer management you use and receive all by few (ideally one) calls to the 'network receive function'. If you don't know the size in advantage you have to call the 'network receive function' for very small portion of the message. Since the 'network receive function' (which may be recv() or whatever Qt offers to you) is a system call which also does TCP buffer handling and so on it should be assumed as being slow with a large per-call overhead. So you should call it as few as possible.  It is standard stuff. To the receiving program everything coming over the network is just a stream of bytes. The stream has no meaning beyond what the application imposes upon it exactly the same way a file has no meaning beyond how its records lines etc. are defined by the application(s). The only way the client and server can make sense of the stream is to establish a convention or protocol that they agree upon. So some common ways to accomplish this are by: have a delimiter that designates the end of a message (e.g. a carriage return) pass a length field as in your example which tells the receiver how much data comprises the next message. just establish a fixed convention (e.g. every message will be 20 bytes or type 'A' records will be one defined format type 'B' records another...) just treat it like a stream by having no convention at all (e.g. take whatever comes over the network and put it in a file w/o paying any attention to what it is) One advantage of the length byte method is that the receiver knows exactly how much data to expect. With some added sanity checks this can help eliminate things like buffer overflows and such in your application."
252,A,"C HTTP server - multithreading model? I'm currently writing an HTTP server in C so that I'll learn about C network programming and HTTP. I've implemented most of the simple stuff but I'm only handling one connection at a time. Currently I'm thinking about how to efficiently add multitasking to my project. Here are some of the options I thought about: Use one thread per connection. Simple but can't handle many connections. Use non-blocking API calls only and handle everything in one thread. Sounds interesting but using select()s and such excessively is said to be quite slow. Some other multithreading model e.g. something complex like lighttpd uses. (Probably) the best solution but (probably) too difficult to implement. Any thoughts on this? See also http://stackoverflow.com/questions/3981566/what-is-event-driven-web-server Threads I promise will be a PITA. Look at your platforms most efficient socket polling model - epoll (linux) kqueue (freebsd) WSAEventSelect (Windows). Perhaps combine with a thread pool handle N connections per thread. You could always start with select then replace with a more efficient model once it works. I'd look into I/O completion ports on Windows rather than `WSAEventSelect()`.  There is no single best model for writing multi-tasked network servers. Different platforms have different solutions for high performance (I/O completion ports epoll kqueues). Be careful about going for maximum portability: some features are mimicked on other platforms (i.e. select() is available on Windows) and yield very poor performance because they are simply mapped onto some other native model. Also there are other models not covered in your list. In particular the classic UNIX ""pre-fork"" model. In all cases use any form of asynchronous I/O when available. If it isn't look into non-blocking synchronous I/O. Design your HTTP library around asynchronous streaming of data but keep the I/O bit out of it. This is much harder than it sounds. It usually implies writing state machines for your protocol interpreter. That last bit is most important because it will allow you to experiment with different representations. It might even allow you to write a compact core for each platform local high-performance tools and swap this core from one platform to the other. +1 for the state machines paragraphs. The different alternatives are mainly about how to store state and how to context-switch (thread/process per connection store state implicitly on the stack and context-switch via the OS scheduler select() stores state explicitly and context-swicthes via select() libevent stores state explicitly and context-switches implicitly when calling the appropiate callback ...). @ninjalj: state machines are the way to go. They're usually pain to implement as they look nothing like ""synchronous logic"" people are used to writing. I've recently been looking into [Clojure](http://clojure.org/) because it has native support for asynchronous state machines.  Yea do the one that's interesting to you. When you're done with it if you're not utterly sick of the project benchmark it profile it and try one of the other techniques. Or even more interesting abandon the work take the learnings and move on to something completely different.  A simple solution might be having multiple processes: have one process accept connections and as soon as the connection is established fork and handle the connection in that child process. An interesting variant of this technique is used by SER/OpenSER/Kamailio SIP proxy: there's one main process that accepts the connections and multiple child worker processes connected via pipes. The parent sends the new filedescriptor through the socket. See this book excerpt at 17.4.2. Passing File Descriptors over UNIX Domain Sockets. The OpenSER/Kamailio SIP proxies are used for heavy-duty SIP processing where performance is a huge issue and they do very well with this technique (plus shared memory for information sharing). Multi-threading is probably easier to implement though.  You could use an event loop as in node.js: Source code of node (c c++ javascript) https://github.com/joyent/node Ryan Dahl (the creator of node) outlines the reasoning behind the design of node.js non-blocking io and the event loop as an alternative to multithreading in a webserver. http://www.yuiblog.com/blog/2010/05/20/video-dahl/ Douglas Crockford discusses the event loop in Scene 6: Loopage (Friday August 27 2010) http://www.yuiblog.com/blog/2010/08/30/yui-theater-douglas-crockford-crockford-on-javascript-scene-6-loopage-52-min/ An index of Douglas Crockford's above talk (if further background information is needed). Doesn't really apply to your question though. http://yuiblog.com/crockford/ Also another question may be helpful: http://stackoverflow.com/questions/409087/creating-a-web-server-in-pure-c"
253,A,"C# network programming and resource usage I've been doing a lot of research on how best to write ""correct"" network code in C#. I've seen a number of examples using the ""using"" statement of C# and I think this is a good approach however i've seen inconsistent use of it with various expressions. For instance suppose I have some code like this: TcpClient tcpClient = new TcpClient(""url.com"" 80); NetworkStream tcpStream = tcpClient.GetStream(); StreamReader tcpReader = new StreamReader(tcpStream); StreamWriter tcpWriter = new StreamWriter(tcpStream); Obviously this code is going to be very flaky. So i've seen some code that puts the using on the tcpClient which seems good. However doesn't NetworkStream also have resources that need cleaning up? What about StreamReader/Writer? Do I need to wrap all 4 statements in nested using statements? And if so what happens when the time has come to dispose? Won't StreamWriter close the stream and consequently the socket? Then what happens when StreamReader then NetworkStream then TcpClient each go through their disposals? Which brings up another question. WIth both StreamReader and StreamWriter composed of the same stream who owns it? Don't they both think they own it and will thus both try to destroy it? Or does the framework know that the stream has already been destroyed and just silently ignore it? It almost seems like the using statement is only necessary for the last statement in the chain but then what happens if an exception is thrown in GetStream()? I don't think it would properly clean up the socket then so it seems redundant usings are necessary to ensure this doesn't happen. Does anyone know of any good recent books on network programming with .net and prefeably c# that include chapters on exception handling and resource management? Or maybe any good articles online? All the books I can find are from the .NET 1.1 era (Network Programming for the Microsoft .NET Framework Network Programming in.NET etc..) so this seems like a topic that needs some good resources. EDIT: Please don't let Marc's very good comment stop anyone else from commenting on this :) I'd like to hear anyone elses book recommendations or opinions on resource management especially in regard to asynchronous usage. What about sockets? Is it OK to do: serverSocket = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); serverSocket.Connect(serverEndPoint m_NegotiationPort); . . . serverSocket.Close(); or better using (Socket serverSocket = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp) { . . . }  If an object supports IDisposable it's best to put it in a using {} block because the dispose method gets called automatically for you. This also makes for less code on your part. It is important to note the using a 'using' doesn't handle any exceptions. YOu still have to do that if you want to handle any errors. Once the using block goes out of scope so does your object. Old Style Code object obj; try { obj= new object(); //Do something with the object } catch { //Handle Exception } finally { if (obj != null) { obj.Dispose(); } } Newer Style Code try { using (object obj = new object()) { //Do something with the object } catch { //Handle Exception }  Generally objects should internally handle multiple Dispose() calls and only do the main code once; so a stream getting Dispose()d multiple times is not usually a problem. Personally I would use lots of using there; note that you don't need to indent/nest though (unless different levels have different life-times): using(TcpClient tcpClient = new TcpClient(""url.com"" 80)) using(NetworkStream tcpStream = tcpClient.GetStream()) using(StreamReader tcpReader = new StreamReader(tcpStream)) using(StreamWriter tcpWriter = new StreamWriter(tcpStream)) { ... } As you say this ensures that if an error happens during initialization everything is still cleaned up correctly. This also ensures that every level gets a chance (in the right order) to deal correctly with any buffered data etc. Re ownership; NetworkStream is actually an oddity in the first place... most streams are either input xor output. NetworkStream bends a few rules and shims two directions into one API; so this is an exception... normally the ownership would be clearer. Additionally many wrappers have a flag to determine whether they should close the wrapped stream. StreamReader doesn't but some do (such as GZipStream which has a leaveOpen ctor option). If you don't want to flow ownership this is an option - or use a non-closing stream intermediary - one is here (NonClosingStream or similar). Re books; I picked up a copy of ""TCP/IP Sockets in C#: Practical Guide for Programmers"" (here) - adequate but not great. Thanks for the feedback. This is largely what I suspected. I agree NetworkStream is an oddity. I'm not sure if it would have been any less problematic to create seperate read and write streams but it is what it is. Thanks for the book reccomendation. Also I didn't see a NonClosingStream implementation at the site you mentioned. It is there - MiscUtil.IO.NonClosingStreamWrapper"
254,A,need help on image transfer over network I am trying to build remote desktop application using java. For that what I have planned is to take screen shot of screen using Robot and convert it to byte array(bmp image) and send it. In server side read image into byte array and convert to image and display. Successive images are compared with previous image and only different bytes are sent(along with start index where difference starts and length of difference). In server side I only read difference bytes and construct image. Is this method suitable to use over internet(with moderate speed)?. Please specify any better way to do it if you feel this will not work on internet. This sounds similar (at a very high level) to the VNC protocol. Rather than trying to invent your own perhaps you could just adopt the protocol that it uses (RFB - remote frame buffers) or at the least look at it to get some ideas (using rectangles to denote areas of the screen that have changed sounds better than indices into a byte array). The documentation can be found here. Thanks for reply. But generally changes in screen does not change drastically always. Consider normal operation of editing text or moving mouse pointer.Generally small portion of image changes every time screen shot is taken. If this portion on is sent then there is great reduction in network traffic.Please let me know if I am wrong anywhere. You aren't wrong but I think indices into a byte array aren't the best way to do it. VNC addresses this by sending a series of rectangular diffs. Unless I'm not understanding properly I don't think sending indices of a bitmap image is going to be as good. Thank you.I'll go through VPN protocol and see how can it be adopted to my program.
255,A,"Select returning 0 on a closed SCTP socket This is related to the question: SCTP with Multihoming as a Drop In Replacement for TCP I have a simple echo client / concurrent server app that ran perfectly fine using TCP. I could pipe a file to stdin on the client and the client would receive all the data back call select which would return 1 indicating the socket was readable then the call to read would return 0 indicating EOF / FIN. The client would then exit. All is good. However the identical apps over SCTP cause problems. The only change made was from IPPROTO_TCP to IPPROTO_SCTP. The server forks echo's back the data the child exits and is reaped by the parent. The client receives all the data but afterwards select keeps returning 0 descriptors ready ( without the time out I added it would hang forever ). What in the world is going on? Here is what the code for the client looks like: #!/usr/bin/perl -w use strict; use Socket; # forward declaration sub logmsg; my $remote = shift || ""localhost""; my $port = 9877; ($port) = $port =~ /^(\d+)$/ or die ""invalid port""; my $iaddr = inet_aton($remote) || die ""no host: $remote""; my $paddr = sockaddr_in($port $iaddr); my $proto = getprotobyname('sctp'); my $sockfd; socket($sockfd PF_INET SOCK_STREAM $proto) || die ""socket: $!""; connect($sockfd $paddr) || die ""connect: $!""; str_cli($sockfd); exit(0); #----- subs down here ------# sub str_cli { my $sockfd = shift; my ($n $buff $stdineof $s); my $rin = ''; $stdineof = 0; while ( 1 ) { if ($stdineof == 0) { vec($rin fileno(STDIN) 1) = 1; } vec($rin fileno($sockfd) 1) = 1; my $nfound = select($rin undef undef 1.0); if ($nfound < 0) { next if $!{EINTR}; die ""select: $!""; } else { print ""\$nfound == $nfound\n""; } if (vec($rin fileno($sockfd) 1) == 1) { # socket readable print ""trying to read from sockfd\n""; $n = sysread($sockfd $buff 1024); if (!defined($n) || $n < 0) { # resume if sysread() returned because a signal was received next if $!{EINTR}; die ""sysread: $!""; } elsif($n == 0) { if ($stdineof == 1) { return; } # normal termination else { die ""str_cli: server terminated prematurely""; } } writen(*STDOUT $buff); } if (vec($rin fileno(STDIN) 1) == 1) { # stdin readable $n = sysread(STDIN $buff 1024); if (!defined($n) || $n < 0) { # resume if sysread() returned because a signal was received next if $!{EINTR}; die ""sysread: $!""; } elsif($n == 0) { $stdineof = 1; if (!defined($s = shutdown($sockfd SHUT_WR)) || $s == 0) { die(""shutdown: $!""); } vec($rin fileno(STDIN) 1) = 0; next; } writen($sockfd $buff); } } } sub writen { my ($connfd $buff) = @_; my $nleft = length($buff); my $total = 0; my $nwritten = 0; while ($nleft) { if (($nwritten = syswrite($connfd $buff $nleft $total)) <= 0) { # resume if syswrite() returned because a signal was received # 0 indicates an error in this context next if $!{EINTR}; die ""syswrite: $!""; } $nleft -= $nwritten; $total += $nwritten; } } sub logmsg { print ""$0 $$: @_ at "" scalar localtime ""\n"" } Remember this works perfectly over TCP. I'm on Ubuntu 9.04 with all the needed sctp packages installed. You are relying on TCP's half-closed state which is not available in SCTP. Using TCP shutdown($sockfd SHUT_WR) will send a packet with the FIN bit set closing the sending side of the connection but still allowing new data in the receiving direction. SCTP does not have such a half-closed state and this call will initiate the SCTP shutdown sequence in which the whole connection is closed. Further details can be found here. So is there a way in SCTP to tell a socket is closed other than application protocol level notifications? If you want to send an indication that the you are done sending but still want to keep the connection up to receive a response and do not want to do this at the application layer you can send your data in a separate SCTP stream. The receiver can then react to the closing of that stream and send back a response in a different stream. @mark4o: You'd need to use the sctp specific api calls to do that correct? Is there a way using the standard sockets api? Just curious what can be done when porting an existing TCP app in which you don't want to introduce the sctp specific api calls. If you don't want to use the sctp-specific calls like `sctp_sendmsg` (which takes the stream number as a parameter) you can do the same thing using `sendmsg`. This is technically a standard sockets api function however the ancillary data that you pass to `sendmsg` would be sctp-specific. That's what I thought. I was just looking back at the link you provided re: `shutdown`. My understanding is that it closes the association but not the socket and that's why select doesn't return any ready descriptors the way it does with TCP? I think that your select would return if the other side had initiated the shutdown sequence but in this case you are initiating it (and apparently it doesn't think that you need to be told about the shutdown that you initiated). With TCP each side shuts down its send direction independently but with SCTP there is only one shutdown sequence that applies to both directions. You could request the SCTP_ASSOC_CHANGE or SCTP_SHUTDOWN_EVENT notification but your server still won't be able to send data after you shut down the association."
256,A,"UDP Problem With Non-Reception I am new to Network Programming and I am having a problem with some code I've been testing as the basis of a LAN chat program. Server Code: public static void Main() { UdpClient publisher = new UdpClient(""230.0.0.1"" 8899); UdpClient subscriber = new UdpClient(""230.0.0.2"" 8800); IPAddress addr = IPAddress.Parse(""230.0.0.1""); subscriber.JoinMulticastGroup(addr); Console.WriteLine(""Running chat program at 230.0.0.1:8899""); while (true) { IPEndPoint ep = null; byte[] chats = subscriber.Receive(ref ep); string chatstring = Encoding.ASCII.GetString(chats); Console.WriteLine(chatstring); string msg = String.Format(chatstring); byte[] sdata = Encoding.ASCII.GetBytes(msg); publisher.Send(sdata sdata.Length); System.Threading.Thread.Sleep(500); } } And the client program: static void Main(string[] args) { UdpClient subscriber = new UdpClient(""230.0.0.1"" 8899); IPAddress addr = IPAddress.Parse(""230.0.0.1""); subscriber.JoinMulticastGroup(addr); IPEndPoint ep = null; Thread SendChats = new Thread(Send); SendChats.Start(); while (true) { byte[] receivedbytes = subscriber.Receive(ref ep); string receivedchats = Encoding.ASCII.GetString(receivedbytes); Console.WriteLine(receivedchats); Thread.Sleep(500); } } static void Send() { UdpClient publisher = new UdpClient(""230.0.0.2"" 8800); while (true) { string msg = Console.ReadLine(); byte[] sdata = Encoding.ASCII.GetBytes(msg); publisher.Send(sdata sdata.Length); Thread.Sleep(400); } } By my figuring the server program SHOULD be receiving data from the client but alas after a message is typed in and delivered there is never anything getting through. Am I missing something? Is your subscriber joinging the wrong multicast in the server? UdpClient subscriber = new UdpClient(""230.0.0.2"" 8800); IPAddress addr = IPAddress.Parse(""230.0.0.1""); subscriber.JoinMulticastGroup(addr); Possibly should be: IPAddress addr = IPAddress.Parse(""230.0.0.2""); Also - have you got any routers/switches between the client/server as they will prevent multicast traffic unless specifically configured to do so. No probs. Router / switch issues only come into play if your trying to send multicast packets across different LAN segments. If client and server are on the same LAN then this is irrelevant. Prevention by the router? When this is only functioning between two or more computers on LAN... AND TY for the catch. I removed the address in the subscriber and changed the addr variable."
257,A,"Where would I start to learn about programming something like an IM program? The IM program was just an example but where would I start to learn how to get input from another computer across the network whether that network is LAN or through the internet? EDIT:: The languages I'm familiar with are C++ and python. The platform I'm on is vista 64bit It also depends a good deal on your general competency as a programmer. I think network programming might be a little to difficult for somebody who's totally new to programming. That said - I found ""Foundations of Python Network Programming"" be a pretty good book to the basics (if Python is an option for you). Thanks I will look into that. Link to the book: http://www.amazon.com/Foundations-Python-Network-Programming-Goerzen/dp/1590593715 I'd probably suggest learning from the source code of an existing program. As far as chat oriented network programs go you cant get much simpler than Internet Relay Chat. For instance ircII is an open source text mode irc client for unix like systems.  That depends very much on which language and probably also which operating system you'll be programming for. Generally it will involve ""sockets"" though. Depending on what exactly you're trying to do there may be some higher-level system that you can use but any sort of network communication is fundamentally based on sockets. I added to my OP -- C++ and Python along with on vista 64-bit  sockets They have them for every language and every operating system.  There are 2 types of network programming Synchronys and Async. Sync programming is easier but less effecient Async is harder but much more efficient. In order to learn both models easily it is better to start in a language that has good library support for these 2 models C# and Java are great examples since they contain really cool high level frameworks to work with Sockets and threads. You can also check out the async programming model offered by CCR ( concurrency and coordination runtime ) which is really useful and make async programming really easy. Hope this help. If your language is C++ then I recommend you take a look at QT library from Trolltech it has a great framework to work with sockets and will make network programming really easy for you."
258,A,"Connecting two clients in a network If I have two clients which are aware of each other (ie they know which ports to use and their respective hostnames) can they connect to each other using sockets? I know that one could run as a server and one could run as a client but can they both be clients? Both clients are not behind a router or firewall and each have a unique IP. I'm also using Java so I'm thinking of using two Socket instances on both clients rather than a ServerSocket on one and a Socket on the other. Someone needs to listen for a connection somewhere. A standard Socket connection connects to a host and port on which there is an active listener (ServerSocket for standard Java socket). ServerSocket does by default block waiting for incoming connections. If you go with adding a ServerSocket connection on both clients you would want to have one thread listening for connections and another attempting to make an outgoing connections. There are some alternatives such as using NIO which would allow you to check for an incoming and outgoing connection without blocking your entire application. You can also look into Broadcasting however there are complexities with that solution as well. One more thought: Depending on your requirements you can always look into leveraging P2P frameworks such as JXTA.  You can create a ServerSocket accept a connect. This adds one line of code compared to just creating a socket so its not much code overhead. On the client side Socket s = new Socket(hostname port); on the server side ServerSocket ss = new ServerSocket(port); // can be reused. Socket s = ss.accept(); Since you only need a single connection it may not be any more complicated that this.  You can use the connection-less UDP protocol instead of TCP if reliable delivery of messages is not of concern. TCP is a connection-oriented protocol so one of the participant will have to initiate the connection while other listens for the connection.  No they only can do it by initializing a server socket at least in one of the sides. After one of the clients establishes the connection then you can implement a protocol to write/read from both sides. This is the mechanism to ""wait"" for connections client sockets can't receive a connection if it has not been init by server socket first. Idea why don't you integrate a small HTTP server in both clients so that you know you can stick to the HTTP protocol (i.e: Jetty)  Simply put - the answer is no you can't use both ends as clients. There is no command that lets two sockets connect to each other. The only way to create a connection with the sockets API is for one side to listen for connections and accept them and for the other to connect to it. The usual solution to the situation is to arbitrarily decide that one end is the server and the other is the client. You listen on the server end and connect on the client end.  No you need a ServerSocket to listen to a port and accept a connection."
259,A,"How to programatically create virtual IP Addresses in VB? What I want to do is use one PC to test an application on another on the same 19.2.168.X.X I would like to make it seem like SOAP requests are coming from a variety of different PCs just to make the applciations log file easier to read and I have been told that virtual IP Addresses are the way to go. So how do I defien a range and then use them one by one in VB? (I guss that this is effectively IP header spooging?) It says it all and yet says nothing at all. Why does this remind me of CSI? http://stackoverflow.com/questions/175545/worst-technobabble-youve-ever-heard/175584#175584 What have you come up with so far? ""What have you come up with so far?"" ... well I posted here :-) and then I rephrased the question which hopefully makes it clearer what I am trying to do. Not exactly sure what you want to do but I'd suggest first looking at the WMI class Win32_NetworkAdapterConfiguration to see if you can do what you want with it. If that class does not support it you might have to use INetCfg instead where I think you can change pretty much any network settings but it's a bit more difficult to use. Here's a codeproject article for using INetCfg from C# which might at least show you how to get started. +1 thanks. I will look into that You'd want the EnableStatic() method. It accepts multiple IP addresses.  If you just want to create a random IP address generate 4 random numbers from 0 to 255 and glue some dots between them. +1 O suppose I asked for that. Ok I have made the question clearer (I hope)"
260,A,"Set timeout for winsock recvfrom I'm trying to set up a blocking socket to timeout after 16 ms of trying to recvfrom() on a port. Platform is Windows. I've looked at tons of examples online and it seems really simple I just can't seem to get it to work. Any help would be appreciated! #include <winsock2.h> #include <string> #pragma comment(lib ""ws2_32.lib"") #define PORT_NUM 8001 int main(void) { std::string localIP; sockaddr_in localAddr; sockaddr_in remoteAddr; hostent* localhost; char buffer[1024]; WSADATA wsData; int result = WSAStartup(MAKEWORD(22) &wsData); // winsock version 2 localhost = gethostbyname(""""); localIP = inet_ntoa(*(in_addr*)*localhost->h_addr_list); localAddr.sin_family = AF_INET; localAddr.sin_port = htons(PORT_NUM); // Set Port Number localAddr.sin_addr.s_addr = inet_addr(localIP.c_str()); // Set IP Address int mHandle = WSASocket(AF_INET SOCK_DGRAM IPPROTO_UDP NULL 0 0); if(mHandle == INVALID_SOCKET) return 1; if(bind(mHandle (SOCKADDR*)&localAddr sizeof(localAddr)) == SOCKET_ERROR) return 1; timeval tv; tv.tv_sec = 0; tv.tv_usec = 1600; // Set Timeout for recv call if(setsockopt(mHandle SOL_SOCKET SO_RCVTIMEO reinterpret_cast<char*>(&tv) sizeof(timeval))) return 1; int length = sizeof(remoteAddr); // <-- Blocks here forever recvfrom(mHandle buffer 1024 0 (SOCKADDR*)&remoteAddr &length); return 0; } /* I've also tried passing the time like so: int ms = 16; if(setsockopt(mHandle SOL_SOCKET SO_RCVTIMEO reinterpret_cast<char*>(&ms) sizeof(int))) return 1; */ SO_RCVTIMEO isn't very portable - what platform are you using? You probably want to use the `select` function Hey thanks! Yeah I looked into select and got it to work! I looked into the select function and as laura said I should do and got it to work real easily! Thanks! fd_set fds ; int n ; struct timeval tv ; // Set up the file descriptor set. FD_ZERO(&fds) ; FD_SET(mHandle &fds) ; // Set up the struct timeval for the timeout. tv.tv_sec = 10 ; tv.tv_usec = 0 ; // Wait until timeout or data received. n = select ( mHandle &fds NULL NULL &tv ) ; if ( n == 0) { printf(""Timeout..\n""); return 0 ; } else if( n == -1 ) { printf(""Error..\n""); return 1; } int length = sizeof(remoteAddr); recvfrom(mHandle buffer 1024 0 (SOCKADDR*)&remoteAddr &length);  WINDOWS: Timeout value is a DWORD in milliseconds address passed to setsockopt() is const char * LINUX: Timeout value is a struct timeval address passed to setsockopt() is const void * Source: http://forums.codeguru.com/showthread.php?t=353217  I tried it by passing it like the folloing  int iTimeout = 1600; iRet = setsockopt( pSapManager->m_cSocket SOL_SOCKET SO_RCVTIMEO /* reinterpret_cast<char*>(&tv) sizeof(timeval) ); */ (const char *)&iTimeout sizeof(iTimeout) ); and run it!! Hi welcome to SO. Please try to explain your answer in words not just in code. It will be easier for everyone to learn that way. Also since you are answering an old question how does your answer differ from the other answers?  I'm guessing Windows from the WSASocket() call. If so you're passing the timeout incorrectly. MSDN says that SO_RCVTIMEO takes an int param that specifies the timeout in ms. I've also tried it by passing it like the following: int ms = 16; if(setsockopt(mHandle SOL_SOCKET SO_RCVTIMEO reinterpret_cast(&ms) sizeof(int))) return 1; I still have the same result.. Thanks this is just what I needed for a BSD socket system that does not accept the SO_RCVTIMEO It also says ""If a blocking receive call times out the connection is in an indeterminate state and should be closed. If the socket is created using the WSASocket function then the dwFlags parameter must have the WSA_FLAG_OVERLAPPED attribute set for the timeout to function properly. Otherwise the timeout never takes effect."" so I'm not sure if it is to be used regularly... Looking at this page: http://msdn.microsoft.com/en-gb/library/windows/desktop/ms740476(v=vs.85).aspx I don't see the mention of WSA_FLAG_OVERLAPPED (and it doesn't really make sense to me for an overlapped socket to have a recv timeout set via SO_RCVTIMEO"
261,A,"I need help implementing a ""remote control"" networking capability for a large java project I have the project running on one system and it looks like this: http://imgur.com/oKq1V Well I want to implement a ""remote control"" so I can control the whole thing from another machine. I'm not sure where to start... Any clues on how to make this easier? I appreciate any tips or advice. This project uses a lot of XML information etc. Would that make it harder to implement? Have you tried VNC or something similar? Hmm I'll look into VNC - at a glance I think it's too ""heavy"" of a solution. Provide a Web Service API to interact with the application? @mellamokb : Thanks a lot  yes a Web Service API will help. I would look at using JMX for control. It comes ""for free"" in the JVM it's pretty simple to use for the basic cases and you get a ""free"" console client that you can use (JConsole). That way you can get up and running easily and not have to write any client code if you don't necessarily want to. A custom client would likely be better in the long run but with JMX you can easily create control and statistic methods that you can poke at with JConsole. Thanks a lot - I'm reading about JMX now !  First thing is to understand how you control the application locally. Once that is separated extend it via some kind remote execution (RMI may be a quick option) web service may be more flexible. Thanks - great advice! I will do this!"
262,A,Is there a cross-platform network library for the iPhone? Is there a cross-platform network library for the iPhone? Is ACE available for the iPhone? I would like the networking library to be available for iPhoneOS OS X Windows (XP Vista and Win7 would be nice but at least XP) Android and Blackberry. Asking for a cross-platform library while only mentioning only one platform is not very helpful :-) you can see Qt (http://qt.nokia.com/). New port exists for iphone (http://qt.gitorious.org/+qt-iphone/qt/qt-iphone-clone) and android (http://www.kdedevelopers.org/node/4070)  The ACE networking library is available for iPhone.
263,A,"Advice on implementing a web server Has anyone got the experience to implement a web server? I got the following questions: Q1 - What major problems could be involved during the design and implementing a web server? Q2 - What major technologies could be used to solve the problems in Q1? Q3 - Are there any books related to this area? I know Apache is open source is there any book addrssing it? This could be a big problem. Any comments will be deeply appreciated be it general or detailed. Many thanks. No this is not a homework though it looks like one. I just want to know what's under the hood of a web server like famous IIS and Apache. Although this thread is more than a year old it still merits from the fact that no one mentioned about the not so recent innovations of nginx lighty and other web servers which scale much better than apache at higher concurrency and consume lesser resources. Contending ourselves with the laurels of past achievements would make us stagnant and backward looking rather it is always better to encourage future research albeit with some sincere warning and advise. Please do not discourage folks who would want to redo things hoping for better results just because it is a ""redo""; history is replete with examples of ""redoing"" the same old stuff resulted in discovering what we did wrong and identifying what needs to done for a better future. Thanks for your reply. I agree with you.  The best advice would be ""don't unless you have a really good reason to"". The architectural issues have been addressed for you within already available options such as Apache and IIS. Why duplicate code that you can already use -- if you need to do some hardcore ninja coding you could write your own plugin for you webservers of choice.  I worked on a simple one written in C for an university course. Our version implemented HTTP version 0.9 much simpler than 1.0 or 1.1. We started reading the specs (here you find rfc for HTTP/1.1). We had this book as reference for the course. It's a very good read! There you can find in detail how tcp and ip works. It builds the basis for programming network stuff. Another good reference book is ""Unix network programming"" (same author) or if you already have some background you might take a look at Beej's Guide to Network Programming . The experience for me was very enlightening on how a server works how to read specs and in general on unix programming. My suggestions: if you want to give a try at implementing one start with a small subset of the specs and use a high level programming language. As others said there's probably no need for yet another webserver but it's a good learning excercise. Thanks filippo. Your sharing is precious."
264,A,"How to implement a PPPoe server? I want to develop one PPPoe-like server so I can create a virtual connecting among server and client. Is there any opensource project for reference ？ Thanks pppd can do PPPoE (and about a billion other things). I'm not sure what you mean by ""virtual"" though. Both ends of PPPoE connection have to be on the same Ethernet LAN (strictly speaking there are ways around this but that involves solving the same problem I think you are trying to solve). I think you may be more interested in a VPN (Virtual Private Network). thank you very much! I think VPN is slow right ? one more question If one pc has a NAT IP(means the pc hava connected a NAT server) Does it can be connected the PPP server also ? @why: PPPoE works at a level below NAT. Any router will block PPPoE. Thanks for you good reply. So Any router will not block VPN if it is right can you give me a explain @Why: Usually when someone says ""router"" they are referring to an Internet Protocol Router which routes IP packets. PPPoE packets are not IP packets. IP was designed to be encapsulated in many different types of physical networks. PPP and Ethernet are two of the most popular network types that can carry IP. You can also encapsulate PPP on withing Ethernet (PPPoE) and Ethernet over IP (EoIP) and you can encapsulate IP packets within encrypted UPD packets (a type of IP packet) and have a VPN. @Why: VPNs generally are slower than Local Area Networks but you will likely have a similarly slow connection if you use something else because it will be doing mostly the same stuff. You could skip the encryption/decryption and get a little bit of speed but you may not even notice it. @why from what i understood normally your pppoe connection to ISP is after NAT and your LAN side is behind NAT so basically your data is exchanged encapsulated in PPP over Ethernet. The LAN Part is NAT'd but from ISP side your IP inside PPPoE is the same un-Natted one"
265,A,"Does more NICs on a server mean potential for more sustained concurrent I/O? If you're trying to build an application that needs to have the highest possible sustained network bandwidth for multiple and repetitive file transfers (not for streaming media) will having 2 or more NICs be beneficial? It can be beneficial but it won't necessarily be that way ""out of the box"". You need to make sure that both NICs actually get used - by separating your clients on different network segments by using round robin DNS by using channel bonding by using a load balancer etc. And on top of that you need to make sure your network infrastructure actually has sufficient bandwidth to allow more throughput. But the general principle is sound - you have less network bandwidth available on your server than disk I/O so the more network bandwidth you add the better up until it reaches or exceeds your disk I/O then it doesn't help you anymore.  Potentially yes. In practice it also depends on the network fabric and whether or not network I/O is a bottleneck for your application(s).  I think your answer will depend on your server and network architecture and unfortunately may change as they change. What you are essentially doing is trying to remove the 'current' bottleneck in your overall application or design which you have presumably identified as your current NIC (if you haven't actually confirmed this then I would stop and check this in case something else restricts throughput before you reach your NIC limit). Some general points on this type of performance optimization: It is worth checking if you have the option to upgrade the current NIC to a higher bandwidth interface - this may be a simpler solution for you if it avoids having to add load balancing hardware/software/configuration to your application. As pointed out above you need to make sure all the other elements in your network can handle this increased traffic - i.e. that you are not simply going to have congestion in your internet connection or in one of your routers Similarly it is worth checking what the next bottle neck will be once you have made this change if the traffic continues to increase. If adding a new NIC only gives you 5% more throughput before you need a new server anyway then it may be cheaper to look for a new server right away with better IO from new. the profile of your traffic and how it is predicted to evolve may influence your decision. If you have a regular daily peak which only exceeds your load slightly then a simple fix may serve you for a long time. If you have steadily growing traffic then a more fundamental look at your system architecture will probably be necessary. In line with the last point above it may be worth looking at the various Cloud offerings to see if any meet your requirements at a reasonable cost possibly even as temporary resource every day just to get you through your peak traffic times. And finally you should be aware that as soon as you settle on a solution and get it up and running someone else in your organization will change or upgrade the application to introduce a new and unexpected bottle-neck..."
266,A,"Understanding socket basics I've been reading up on basic network programming but am having a difficult time finding a straight-forward explanation for what exactly and socket is and how it relates to either the OSI or TCP/IP stack. Can someone explain to me what a socket is? Is it a programmer- or API-defined data structure or is it a hardware device on a network card? What layers of the mentioned network models deal with ""raw"" sockets? Transport layer? Network layer? In terms of the data they pass between them are socket text-based or binary? Is there an alternative to sockets-based network programming? Or do all networked applications use some form of socket? If I can get this much I should have a pretty clear understanding of everything else I'm reading. Thanks for any help! Short answers: Socket is an abstraction of an IP connection endpoint - so if you think of it as an API structure you are not very far off. Please do read http://en.wikipedia.org/wiki/Internet_socket Internet layer i.e. IP Protocol. In practice you usually use explicitly sockets that bind to a certain transport layer parameters (datagram/UDP or stream/TCP) Sockets send data in network byte order - whether it is text or binary depends on the upper layer protocol. Theoretically probably yes - but in practice all IP traffic is done using 'sockets' Sockets do not (generally) implement IPSec - you (usually) just use the 'ordinary' TCP or UDP socket to create the traffic and the lower levels of the network stack (possibly on a different network node) handle the ESP transform for IPSec. From an application programmer point of view there should be no difference between IPv4 and IPv6 sockets excect the addresses you need to use when opening the socket. Thanks Kimvais! So is it safe to say that a someone implementing a socket structure must allow it conform to IP protocol? ...Also if sockets are IP-level constructs then what is the distinction between a socket implementing IPv4 or IPSec?  A socket in C parlance is a data structure in kernel space corresponding to one end-point of a UDP or TCP session (I am using session very loosely when talking about UDP). It's normally associated with one single port number on the local side and seldom more than one ""well-known"" number on either side of the session. A ""raw socket"" is an end-point on more or less the physical transport. They're seldom used in applications programming but sometimes used for various diagnostic things (traceroute ping possibly others) and may required elevated privileges to open. Sockets are in their nature a binary octet-transport. It is not uncommon to treat sockets (TCP sockets at least) as being text-based streams. I have not yet seen a programming model that doesn't involve something like sockets if you dig deep enough but there have certainly been other models of doing networking. The ""/net/"" pseudo-filesystem where opening ""/net/127.0.0.0.1/tcp/80"" (or ""tcp/www"") would give you a file handle where writes end up on a web server on localhost is but one.  Socket is a software mechanism provided by the operating system. Like its name implies you can think of it like an ""electrical outlet"" or some electrical connector even though socket is not a physical device but a software mechanism. In real world when you have two electrical connectors you can connect them with a wire. In the same way in network programming you can create one socket on one computer and another socket on another computer and then connect those sockets. And when you write data to one of them you receive it on the other one. There are also a few different kinds of sockets. For example if you are programming a server software you want to have a listening socket which never sends or receives actual data but only listens for and accepts incoming connections and creates a new socket for each new connection. Who takes care of sending data from one end to the other?"
267,A,"What are the qualities and skill set you should have to be network programmer? I am new to programming. I have lots of interest in networking and want to be network programmer. Can someone please tell me ""what are the qualities and skill set you should have to be network programmer""? Thanks & Regards Rupesh could you define 'network-programmer'? Do you mean programming client-server applications or more hardware related things. If you can give me information about both the approaches or if you can give me some pointer about both the things that will be very helpful. - Thanks Rupesh For advance network programming I would suggest you to read ""Network Algorithmic by George Varghese"". It will tell you how hardware is designed and real life difficulties. Good Reference.. Thanx.  I am not sure I have ever hired a specific ""network programmer"" but I will give you my two cents worth. For many years I did what I call comms programming(wasn't just TCP/IP but many different flavours of network and serial communications programming). and I would say it is important that you have a deep understand of the stack you are programming for(in this cause mostly TCP/IP over ethernet). If you understand what a switch and router do know how multicasting works and have a good understanding of the ethernet protocol then you are one up on many. You may never have to apply this knowledge but understand how a dynamically routed and complex network work is important. Along with this is a good knowledge of multi-threaded and multiprocess programming. WIth the emphasis of buffer management between competing processes/threads. Inevitably a network aware client or server application will need to 1. respond to a network event(ie an incoming packet) and 2. delegate the processing of that data to another task/thread while the main thread goes back to servicing the network listener. So being good with threads and syncing buffers is vital. Hope that helps. Good answer.. Great piece of information for a newbie like me. Thanx  While I've never heard of a network programmer I do know of network analysts. These are the people that configure firewalls load balancers and other network equipment within companies and datacenters. Thus they do a lot of script work in setting up the rules for a firewall or understanding various algorithms for load-balancing and why one may prefer one approach over another for it. If that kind of work is what you mean then I'd think understanding network diagrams security rules various protocols for traffic and applications to track it like WireShark or Fiddler are some ideas to get the ball rolling. Another point is that this is sometimes under the umbrella of the system administrators that also manage the servers in smaller companies as they don't have the budget for each role so one person gets to manage the servers and network stuff."
268,A,"Encoding.ASCII.GetString() Problem VB.NET i got a problem when receiving data from networkstream that was converted from bytes to string.. the output would be the same but when i compare the the string generated from Encoding.ASCII.GetString() compared to the actual string the output is not the same.. this is my code.. Server: Dim tcp As New TcpClient tcp = findTCP(ip) Dim ns As NetworkStream = tcp.GetStream Dim b As [Byte]() = Encoding.ASCII.GetBytes(""a"") ns.Write(b 0 b.Length) Dim bb(tcp.ReceiveBufferSize) As Byte ns.Read(bb 0 tcp.ReceiveBufferSize) If Encoding.ASCII.GetString(bb) = ""b"" Then a = ""conn"" Else a = ""not"" End If Client: Dim ns As NetworkStream = socClient.GetStream Dim b(socClient.ReceiveBufferSize) As Byte ns.Read(b 0 socClient.ReceiveBufferSize) Dim bb As [Byte]() = Encoding.ASCII.GetBytes(""b"") ns.Write(bb 0 bb.Length) just for ping purposes.. :) my server pings to client.. is there any instance that when the string will be converted to bytes the string will change? :? Have you debugged the application to see what the actual values of the byte arrays / strings are? The string won't change by converting it to bytes and back again. mmm.. what im doing is msgbox Encoding.ASCII.GetString(bb) <-- output: b ||||||||| If Encoding.ASCII.GetString(bb) = ""b"" Then a = ""conn"" suppose to be.. You are ignoring the return value of the Read method which would tell you how many bytes it actually did read. Everything after that in the buffer is garbage. Also it would tell you if you have read all of the stream or not so you have no idea if you read all the data or how much data there is. Use the return value of the Read method to determine what you get and whether you need to call it again. Assuming that an array of 1000 bytes is enough to hold the stream this should work: Dim bb(999) As Byte Dim pos As Integer = 0 Dim len As Integer Do len = ns.Read(bb pos bb.Length - pos) pos += len Loop While len > 0 You also need to use that length when you decode the data or you will decode the entire buffer including the garbage after the actual data: Encoding.ASCII.GetString(bb 0 len)  Your byte arrays are declared far too large. This causes the strings to have extra appended characters which would mess with your string comparison logic. ReceivedBufferSize is the buffer size internally in the socket and will usually have a value ranging from 2.000-8000 bytes. You should instead use the Available property which indicates how many bytes that are currently received. Dim bb(tcp.Available) As Byte i changed it to Dim bb(10024) As Byte but still not the same string Try changing it to Dim bb(tcp.ReceiveBufferSize - 1) As Byte still not the same.. i even tried using trim.. anyway how can i see the whole data in byte array.. Set a break point on the line and then hover over the array variable. the length of byte is 8192.. i don't why.. it has only ""b"" character but having that length.. msgbox bb.length My bad you shouldn't use ReceiveBufferSize but Available. when using breakpoint on the line.. bb shows nothing until end if.."
269,A,"What is the Recommended Ping and URL Connection Timeouts? I am currently doing some network programming and had a couple questions concerning timeouts. Is there a recommended timeout in doing a ping? Also is there a recommended timeout in doing a URL connection? Edit: In my case with the ping I am just trying to see if a device is connected to the network. With the URL connection I am trying to open a URL and get the text from it. Thanks In general I set a timeout of 60 seconds for a request (This varies if you are streaming many MB of files via a request). There are two types of pings. Active pings where you actually ping a pingable component when requested and Passive pings where you ping a component in the background and just return a cached status when requested. In my application I still set these timeouts at 60s but if you think you want fail-fast feel free to set a smaller number.  This depends on the where you're going to connect to. To give an example: if you connect to another box in the same data center or even same rack there are only few jumps (routers switches firewalls ...) and connections should usually be established under a second - hence no need for a 30 second timeout (I'd set it to 5 seconds). If you connect to a box on another continent that's a totally different story. Packet loss crowded routes and connections may slow down the connection. A timeout of 30s or 60s sounds fair. Additionally you should consider if you're client really wants to wait for 60 seconds. To give another example if you connect to a web service in order to serve an HTTP request from a user. Waiting 60 seconds won't make much sense as the user will cancel/leave the request anyway. Furthermore such blocking service calls might lead to a lot of waiting threads filling up the thread pool of your server - not a good thing. In this case I'd set the timeout to 10 seconds and rather risk some ""service not available"" or similar page being thrown at the user as soon as the web service becomes slow.  How do you expect your network to behave ? That will dictate how you regard the connections to behave and when you'd expect a timeout. e.g. how many network jumps will your ping perform over ? How loaded are those devices in a normal scenario ? I apologize for the lack of knowledge. What do you mean by network jumps and device load?"
270,A,"iPhone SDK send string over wifi I'm trying to add a simple feature to my app it should simply send a pre-formatted string to another device very much like WiTap sample code does. It sounds like a very trivial thing to do but I cannot get it to work ;(. How can I modify WiTap to send a string instead of a single int? Any pointers to good tutorials would be great. I did look at SimpleNetworkStreams sample but it went way over my head as I'm only looking to send a string (NSString char[] don't have a preference) and not a file. Looked at this example too: http://stackoverflow.com/questions/694601/how-to-add-data-for-nsoutputstream but that did not fully help either. Am I supposed to just pass something like (const uint8_t *)[strBuffer UTF8String]; where strBuffer is a NSString for the buffer to write? I have since figured it out and decided to answer my own question here for the benefit of those who are in similar situation. Whenever I want to send any string I use this helper function I created: - (void) send:(NSString *)string { const uint8_t *message = (const uint8_t *)[string UTF8String]; if (_outStream && [_outStream hasSpaceAvailable]) if([_outStream write:message maxLength:strlen((char *)message)] == -1) NSLog(@""Failed sending data to peer""); } On the receiving side it looks like this: - (void) stream:(NSStream *)stream handleEvent:(NSStreamEvent)eventCode { switch(eventCode) { case NSStreamEventHasBytesAvailable: { if (stream == _inStream) { // read it in unsigned int len = 0; len = [_inStream read:buf maxLength:buffSize]; buf[len] = '\0'; if(!len) { if ([stream streamStatus] != NSStreamStatusAtEnd) NSLog(@""Failed reading data from peer""); } else { NSString *message = [NSString stringWithUTF8String:(char *)buf]; // here you do whatever you need with your received NSString *message } } } } The buffer is defined as: #define buffSize 60000 uint8_t buf[buffSize]; 60000 is fairly arbitrary you can change it to suit your needs. A few notes about the above. While it is safe to make the buffer for these strings fairly large you are never guaranteed to receive your string in one go. In the actual app you should carefully design a certain protocol that you can rely on to check if you received the whole string and combine strings received during subsequent NSStreamEvents if necessary."
271,A,"java network programming coordination of messaging I have 2 processes running in different machines which are communicating via TCP sockets. Both processes have code that acts both as a server and as a client. I.e. ProcessA has opened a server socket that binds at portX and ProcessB has opened a server socket bind at portY. ProcessA opens a client socket to connect with ProcessB and start sending messages as a client and receiving responses (over the same tcp connection of course). ProcessB once it receives a message and processes it it sends the response but also could send a message over the second tcp connection i.e. where ProcessB has opened a client socket to portX of ProcessA. So the flow of messages are over 2 different tcp connections. My problem is the following: Taking as granted that this ""architecture"" can not change and must stay as is: I have the problem that intermittently the messages send from ProcessB to ProcessA over the tcp connection that ProcessB has opened the client socket arrive at processA before the messages send as responses from ProcessB to ProcessA over the tcp connection that ProcessA has connected as a client socket. I.e. Both flows occur (1) ProcessA ---->(msg)----> ProcessB(PortY) (TCP1) ProcessB does processing ProcessB(portY)--->(response)----->ProcessA (TCP1) ProcessB--->(msg)----->ProcessA(portX) (TCP2) (2) ProcessA ---->(msg)----> ProcessB(PortY) (TCP1) ProcessB does processing ProcessB--->(msg)----->ProcessA(portX) (TCP2) ProcessB(portY)--->(response)----->ProcessA (TCP1) EDIT (after ejp request) How can I enforce/make sure that ProcessB does not send a msg over the connection that ProcessB has a client socket open to server portX of ProcessA before the message send as a reply from server portY of ProcessB arrives at processA? I.e. to have only flow (1) of the above. Note that processB is multithreaded and the processing is non-trivial. UPDATE: May be it is my misconception but when a process sends data over socket and control is returned to application this does not mean that the receiving side has received the data. So if a process sends data over 2 sockets is there a race condition by OS? UPDATE2 After answer I got from Vijay Mathew: If I did a locking as suggested is there a guarantee that OS (i.e. IP layer) will send the data in order? I.e. finish one transmission then send the next? Or I would they be multiplexed and have same issue? Thanks Your para beginning 'how can I ensure' doesn't mention port Y at all. So it doesn't agree with you've just said. Can you bring all this into agreement please? So I gather that B shouldn't send anything down port X while there is a pending reply on port Y. Is the reverse also true? @ejp:I made an edition.Is it better now?Yes you are right on your understanding that ""that B shouldn't send anything down port X while there is a pending reply on port Y"". The reverse is not a requirement so no. Can you clarify this? Do you mean that B shouldn't send the *next* request to A via portX until the *previous* reply has arrived from A via port X? so port Y is completely irrelevant here? @ejp:There are 2 tcp connections. B should not send msg to A via portX (opened in A) BEFORE the reply of previous message (that was send by A to B that was listening in portY) is received by A. Is this more clear? So portY is not irrelevant. It is the tcp connection that the first msg by A was send. The synchronisation problem may not be in the TCP protocol but in the thread handler choosing which thread to wake up when the messages arrive. I understand from the nature of your question that the PortX ""(Msg)"" is sent very quickly after after the PortY ""(Response)"". This means that the thread handler may occasionally have a choice as to which of the two listening threads it will wake. A simple but ugly and incomplete way to fix the problem is to insert a short sleep between the response and the next message. The sleep would have to be long enough to be confident that the other process will have woken up to the response before the next message is received. This way is incomplete because although you are increasing the changes of properly synchronising your processing issues like OS load and network congestion can always conspire to push your message right back up behind your response. And then you're back where you started. The other bit of ugliness is that the sleeping wastes time and will reduce your maximum throughput. Just less often. So... To completely resolve the issue you need some way for each socket listener to know whether the message it just received is the next one to be processed or whether there might be earlier messages that have to be processed first. Do this by sequentially numbering all messages sent by each process. Then the receiving process knows if anything is missing. You will have to think of a way for the listeners on each socket to confer between themselves to ensure that messages received are processed in order of transmission. There are a number of practical solutionsm but they all amount to the same thing at the abstract conceptual level. THREAD 1: A) ProcessA(PortX) thread receives a message and wakes. B) IF the sequence number indicates that there is a missing message THEN B1) synchronize on ProcessA(PortY) and wait (). B2) On waking back to B) C) {no message is missing} Process the message. D) Back to A) THREAD 2: A) ProcessA(PortY) receives a response and wakes. B) Process the response. C) notifyAll (). D) Back to A) The most generic practical solutions would probably involve a single socket listener instance adding all new messages to a PriorityQueue so the earliest-sent messages always go to the head of the queue. Then Thread 1 and Thread 2 could both wait on that instance until a message arrives that they can process. A simpler but less extensible solution would be to have each thread do it's own listening and waiting with the (response) handler notifying after processing. Good luck with it although after all this time it's probably solved already.  The obvious solution is: LockObject lock; ProcessA ---->(msg)----> ProcessB(PortY) // Processing the request and sending its response // makes a single transaction. synchronized (lock) { ProcessB does processing ProcessB(portY)--->(response)----->ProcessA (TCP1) } // While the processing code holds the lock B is not // allowed to send a request to A. synchronized (lock) { ProcessB--->(msg)----->ProcessA(portX) (TCP2) } May be it is my misconception but when processB hands over to OS the data to send over the socket does control return back to the application from OS after the data has been received by the other end? No so OS could buffer data ProcessB would move to the second lock and then pass the data over the second socket. So there would still be some race condition among the 2 send right?  The obvious question is why do you care? If you have operations that need to be synchronized at either end do so. Don't expect TCP to do it for you that's not what it's for. Synchronize how? I can not send message B before I have send a reply to previous message A.These 2 separate processes implement a single functional unit.Is this more clear? Synchronize why? I am not sure what you suggest.All I want to know if for example I followed Vijay Mathew suggestion I would still get a race condition as a result of the way data are transmitted.I.e. OS would buffer both data and send them multiplexed over the wire I am asking why in your application that would constitute a race condition in the first place. If you can't send message B before you've got an answer to message A don't. The simplest way to ensure that is to put them both in the same piece of sequential logic. What I don't understand is why two separate processes would care about each others state."
272,A,Sockets: I/O Error 32 Could someone please explain what an I/O Error 32 refers to in the context of a network socket? I have a multithreaded Socks5 server written using Poco SocketReactors and am getting this error when the server load reaches a certain point. The exception is thrown within my onReadable handlers at the same time across all threads which have connections associated with them. The only other thing I am doing within those threads is std::cout but I am not sure if this is a potential cause. You do not specify what platform you are on but my wild-ass guess is that this refers to errno 32 which as EPIPE on a POSIX system. The likeliest scenario for that is that you're trying to read from a socket when the remote side has closed the connection. I am on Linux x64. In my Google searches I did see an error 32 referred to but not specifically in the context of a socket. The error referred to a broken pipe so I guess this agrees with what you are saying.
273,A,"What is the best Networking implementation for my application? I am in the planning phases of a project for myself it is to be a single and multi-player card game. I would like to track statistics for each person such as world rankings etc... My problem is I do not know the best approach for the client - server architecture and programming. My original goal was to program everything in C# as I want to get proficient in that language. My original idea was to have a back-end database and a back end server run on some sort of hosting on the internet however that seems costly for such a small project that may or may not make any money. I have tried looking into cloud services however I am unfamiliar with the technology and I am not sure I can make them suit my needs especially since most like Google's cloud wants you to use their coding architecture from what I understand. Finally my last problem is that I would like an architecture that can be used for different languages so that I can port it from PC to IPhone Xbox etc... So does anyone have any advice on the best architecture and language to do this in? Am I worrying about architecture and back-end costs to much and should just concentrate on getting the game running any which way? C# would be a great language even for the iPhone and Xbox. With Microsoft's XNA framework you can write your client card application for PC Xbox and Zune without changing much code at all. If you're considering the iPhone then you should later consider MonoTouch which lets you write C# for the iPhone. You also have Silverlight which you can use to write a rich client for the web. So I would absolutely suggest C# whether it be just for learning or for putting this into production later on. As for using a server on the internet I would suggest simply installing SQL Server Express edition. That way you can run a server on your local machine separately alongside a client to save from having to pay for a host. Then when you're ready you can move it to web hosted SQL Server instance or just use MySQL or the like for free later on when you are ready.  I think the question of costs (e.g. of database hosting) is a business decision not a technical one. Are you trying to turn a profit? How many users will you have for your game and what will they pay (directly or indirectly via e.g. advertising). There are hosting solutions for any language/data storage you may pick. I doubt you need ""cloud"" solutions for this though a standard database (MSSQL MySQL Postgres etc.) should do just fine and one of these will come with almost any hosting plan that allows you to deploy your own code. Regarding technology C# is a perfectly fine choice for a web application and if learning it is a goal then by all means use it. You should definitely develop a working game on your own computer before it makes sense to pay for hosting (you'll still need to deploy and test the hosted version after it works locally). As for your last question about moving to other non-PC platforms I'm afraid you're mostly out of luck there. C#/.NET will help you deploy to Windows-based phones but for e.g. the iPhone you have to use Apple's SDK and environment. I have no idea about the XBox. The best advice I can give here is to abstract your layers (e.g. a MVC architecture) and maintain a clean and well-documented separation between them. That way you can re-implement say the view and controller on an iPhone but have that still utilize your centralized data model."
274,A,Java OutOfMemory issue while transfering byte array I am writing remote desktop application.So I am transferring Images from one machine to another machine as byte array through socket.After receiving byte array I convert it into image and draw on a panel. Code looks approximately like below imageBytes = //read from socket. InputStream in = new ByteArrayInputStream(imageBytes); BufferedImage bufferedImage = ImageIO.read(in); Image image = Toolkit.getDefaultToolkit().createImage(bufferedImage.getSource()); Image scaledImage = image.getScaledInstance(rmdPanel.getWidth()rmdPanel.getHeight() Image.SCALE_FAST); Graphics graphics = rmdPanel.getGraphics(); graphics.drawImage(scaledImage 0 0 rmdPanel.getWidth()rmdPanel.getHeight()rmdPanel); I also store imagebytes till next image comes(for comparision).Now I am getting java out of memory exception in this code(while receiving byte array).I have heap size of 128 mb-512 mb. Image bytes sent are maximum 3mb.Please help me resolve the issue.Thanks for your time. Can you post the code that reads the byte array from the socket? You said the exception occurs while receiving the byte array so that's probably where the problematic code is. Nothing stands out from the code you pasted. Are you closing your streams? What do you mean when you say you run with 128mb-512mb? What are your Xms Xmx values? Xms=128mb and Xmx=512mb I think ImageIO.read(...) does some sort of caching of images or their input streams so that may be causing you to run out of memory as you keep reading images.  imageBytes = //read from socket. InputStream in = new ByteArrayInputStream(imageBytes); BufferedImage bufferedImage = ImageIO.read(in); Why read the image bytes into an array? You don't need that. It is costing you at least one extra copy of the data maybe two if the ByteArrayInputStream copies the byte array. Just do ImageIO.read() straight from the socket. No I need it because if the image has not changed very much i'll send only changed bytes and reconstruct at server side using previous image.This way i'll reduce network traffic. @hnm: My point remains: you can use ImageIO.read() directly on the socket if the entire image is being shipped. If it isn't you can't use any of the code you showed. You can't 'send only the changed bytes' and expect ImageIO.read() to read them.  (you don't show the communication code so i'm just guessing) if you are using ObjectInputStream/ObjectOutputStream over the socket streams you need to be aware that they cache objects sent over the wire (to avoid resending the same data). sometimes this is a nice feature but it can cause problems if objects are held too long. you need to periodically call reset() on the ObjectOutputStream to clear this cache (in your case possibly after every image send). of course the surest way to solve this problem is to attach a memory profiler and see what's using all the memory (or analyze a heap dump). the jdk has a simple memory profiler in it (jhat) which will examine a heap dump (you can get your program to generate a heap dump when you get an OOM by adding the command line option -XX:+HeapDumpOnOutOfMemoryError). you can use jvisualvm (also in the jdk) to profile the app. i believe netbeans and various other free ides include profilers. or you can buy one. we personally use yourkit java profiler which is excellent. how to attach memory profiler?Thanx for reply
275,A,"client-server program problem sending message from client to server and viceversa using TCP I have a small program that sends and receives data from server and client and vice-versa. Everything works fine but I can't see received messages in both sides and it is always 0 bytes. It doesn't give any compile error but doesnt work the way I wanted to. Can you please have a look at this where I am doing wrong ? Thanks // client #include <stdio.h> #include <unistd.h> #include <stdlib.h> #include <string.h> #include <time.h> #include <errno.h> #include <sys/types.h> #include <sys/socket.h> #include <netdb.h> #include <arpa/inet.h> int main(int argc char* argv[]) { int sockfd; struct addrinfo hints *servinfo *p; int rv; int numbytes; char* hostname = ""localhost""; char* server = ""localhost""; memset(&hints 0 sizeof(hints)); hints.ai_family = AF_INET; hints.ai_socktype = SOCK_STREAM; if ((rv = getaddrinfo(hostname ""5000"" &hints &servinfo)) != 0) { fprintf(stderr ""getaddrinfo: %s\n"" gai_strerror(rv)); return 0; } // loop through all the results and make a socket for(p = servinfo; p != NULL; p = p->ai_next) { if ((sockfd = socket(p->ai_family p->ai_socktype p->ai_protocol)) == -1) { perror(""client: socket""); continue; } if (connect(sockfd p->ai_addr p->ai_addrlen) == -1) { close(sockfd); perror(""client: connect""); continue; } break; } if (p == NULL) { fprintf(stderr ""client: failed to bind socket\n""); return 2; } char message[] = ""hi""; if ((numbytes = send(sockfd message 2 0)) == -1) { perror(""client: send""); exit(1); } int numbytesRecv; char buf[100]; if ((numbytesRecv = recv(sockfd buf 99 0)) == -1) { perror(""client: recv""); exit(1); } freeaddrinfo(servinfo); printf(""client: sent %d bytes to %s\n"" numbytes server); printf(""client: received %d bytes from %s\n"" numbytesRecv server); buf[numbytesRecv] = '\0'; printf(""client: message received is %s\n""buf); close(sockfd); return 0; } // server #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <errno.h> #include <string.h> #include <sys/types.h> #include <sys/socket.h> #include <netinet/in.h> #include <arpa/inet.h> #include <netdb.h> #define MYPORT ""5000"" // the port users will be connecting to #define MAXBUFLEN 100 // get sockaddr IPv4 or IPv6: void *get_in_addr(struct sockaddr *sa) { if (sa->sa_family == AF_INET) { return &(((struct sockaddr_in*)sa)->sin_addr); } return &(((struct sockaddr_in6*)sa)->sin6_addr); } int main(void) { int sockfdTCP; struct addrinfo hintsTCP *servinfoTCP *pTCP; int rv; int numbytes; struct sockaddr_storage their_addr; char buf[MAXBUFLEN]; size_t addr_len; char s[INET6_ADDRSTRLEN]; memset(&hintsTCP 0 sizeof hintsTCP); hintsTCP.ai_family = AF_UNSPEC; hintsTCP.ai_socktype = SOCK_STREAM; hintsTCP.ai_flags = AI_PASSIVE; if ((rv = getaddrinfo(NULL MYPORT &hintsTCP &servinfoTCP)) != 0) { fprintf(stderr ""getaddrinfo: %s\n"" gai_strerror(rv)); return 1; } // loop through all the results and bind to the first we can for TCP socket for(pTCP = servinfoTCP; pTCP != NULL; pTCP = pTCP->ai_next) { if ((sockfdTCP = socket(pTCP->ai_family pTCP->ai_socktypepTCP->ai_protocol)) == -1) { perror(""listener: socket""); continue; } if (bind(sockfdTCP pTCP->ai_addr pTCP->ai_addrlen) == -1) { close(sockfdTCP); perror(""listener: bind""); continue; } break; } if (pTCP == NULL) { fprintf(stderr ""listener: failed to bind socket\n""); return 2; } freeaddrinfo(servinfoTCP); printf(""listener: waiting to recvfrom...\n""); addr_len = sizeof their_addr; const int BACKLOG = 10; if (listen(sockfdTCP BACKLOG) == -1) { perror(""listen""); exit(1); } socklen_t sin_size; int new_fd; sin_size = sizeof their_addr; new_fd = accept(sockfdTCP (struct sockaddr *)&their_addr &sin_size); printf(""I am here\n""); if (new_fd == -1) { perror(""accept""); } if ((numbytes = recv(new_fd buf MAXBUFLEN 0) == -1)) { perror(""recv""); exit(1); } char* msg = "" hi i am server""; int numbytesSend; if ((numbytesSend = send(new_fd msgstrlen(msg) 0) == -1)) { perror(""recv""); exit(1); } inet_ntop(their_addr.ss_familyget_in_addr((struct sockaddr *)&their_addr) s sizeof s); printf(""server: got connection from %s\n"" s); printf(""listener: packet is %d bytes long\n"" numbytes); buf[numbytes] = '\0'; printf(""listener: packet contains : %s\n"" buf); close(sockfdTCP); close(new_fd); return 0; } // output client ./clientTCP client: sent 2 bytes to localhost client: received 0 bytes from localhost client: message received is // output server ./serverTCP listener: waiting to recvfrom... I am here server: got connection from 127.0.0.1 listener: packet is 0 bytes long listener: packet contains : It's been a while since I've done raw sockets programming but I think there's something fishy with your recv() calls. IIRC recv() blocks until it reads the full size of the buffer or the connection is closed. Since neither end sends enough data for it to fill the buffer and the socket doesn't get closed until after the recv() calls I think one or both of your client/server should hang. Since they do not one of two things is happening. Either the sockets are configured for non-blocking mode and you need to configure them to block or something is closing the socket. I'm sorry I can't offer more detailed help than that but hopefully it will point you in the right direction. You may also want to investigate the shutdown() call to use on sockets so that you don't close them before all the data in the buffer is sent. Not sure if that has anything to do with your problem. recv() can return before it reads the full size of the buffer i.e. a short read occurs. However the return values of zero implies the socket was shutdown. That is a bit odd. I agree that some blocking should have occurred.  Your problem is this line in the server: if ((numbytes = recv(new_fd buf MAXBUFLEN 0) == -1)) It should be: if ((numbytes = recv(new_fd buf MAXBUFLEN 0)) == -1) This will allow numbytes to be properly assigned which allows buf[numbytes] = '\0'; to do what you intend and lets printf(""listener: packet contains : %s\n"" buf); print what you want. Same deal with if ((numbytesSend = send(new_fd msg strlen(msg) 0) == -1)) but the bug doesn't manifest itself in the example program. +1 - good catch +1 - Indeed a nice catch.  EDIT: I just tested on OS X. Your code works for me. Now I really think you must have something consuming your bytes. Kill your processes and do: $ netstat -na | grep 5000 To see if you have anything listening on port 5000. If so you need to kill the server. Also make sure the super server (inetd/xinetd) isn't spawning something to handle data on port 5000.  It looks like you're being bitten by Nagle's Algorithm. Try setting TCP_NODELAY to allow small packets to be sent right away. Note that for production code you should profile before deciding to disable the Nagle Algorithm. When I try to compile using your suggestion I get this compile error serverTCP.c: In function ‘main’: serverTCP.c:53: error: ‘TCP_NODELAY’ undeclared (first use in this function) serverTCP.c:53: error: (Each undeclared identifier is reported only once serverTCP.c:53: error: for each function it appears in.) Do I need to use any extra libraries other than lsocket. @seg.server.fault: You need to include  as described here: http://www.opengroup.org/onlinepubs/000095399/basedefs/netinet/tcp.h.html HTH!"
276,A,Why not using SO_REUSEADDR on Unix TCP/IP servers? I have not seen any important TCP/IP server not use SO_REUSEADDR : Apache HTTP Server SO_REUSEADDR usage nginx SO_REUSEADDR usage Very Secure FTPD SO_REUSEADDR usage exim SO_REUSEADDR usage Postfix SO_REUSEADDR usage OpenSSH SO_REUSEADDR usage Is there any use case for not using SO_REUSEADDR on TCP/IP servers ? I mean would making the OS always use SO_REUSEADDR break any server that does not use it? Do you know a TCP/IP server that not uses SO_REUSEADDR for a reason? (of course you may not want to use it on MSWindows as it allows to run two servers on the same port) If you want to quickly stop and restart the server you need it so... Why SO_REUSEADDR it is not the default for listening sockets? May be there should be SO_NOTREUSEADDR instead? @Vi. The problem is that there would not be any use case for using SO_NOTREUSEADDR. Do you know one? Well UNP (Stevens 2004) says: SO_REUSEADDR allows a listening server to start and bind its well-known port even if previously established connections exist that use this port as their local port. All TCP servers should specify this socket option to allow the server to be restarted
277,A,"client port number I was wondering how to using C find out what port a client is currently listening to. ie I want to know what the source port is not the destination port. Thanks in advance! Depending on what you want (which I'm too obtuse to guess right now) you want to call either getsockname() or getpeername().  Not sure what you mean. Clients don't listen servers do. I thought they listen to an open port number that they query the OS for. It seems to be the case... also for some reason torrent servers require you to send the port the client is listening on. @Cenoc: that's because a bittorrent client also acts as a ""server"" for other machines in the swarm to connect to (i.e. it receives incoming connections). P2P confuses the usual client-server division but when talking about a single TCP connection saying ""client"" implies whoever initiated the connection. Right but it still connects as a client so my question is how would I find that source port to send in that scenario? It will be buried in your bittorrent client's options somewhere. Options > Preferences > Connection in the mainline client. If you're asking how the bittorrent code knows what port to send then either it picks one for itself (in which case it knows what it picked) or else it can call `getsockname()` once the socket is bound. Well I meant how would I set it (the port number) using C or find out which port number is the source port without going into the raw. I'm not writing a bittorrent but I was wondering how this could be done using C... Sorry it's part of the `sockaddr_in` structure (""in"" meaning ""internet"") which is used wherever a Berkely-style sockets API mentions a `struct sockaddr*`. So normally when you call `bind` you specify the port then and that's what port will be listened on when you call `listen`. I dont know how to give you the answer I'll just give you a 1up. Thanks a lot though.  The socket address structure should be filled-in by the connect() system-call. Check it after a successful return from that call. Steve Jessop gave me the answer I was looking for Im not sure what to do with this question now...  If you're talking about how to scan for all open ports then you might be after a port scanner. There are many many many many source codes available but I definitely don't recommend you use one of them they are usually slow even if multithreading is enabled. Why? There's nmap : http://nmap.org/"
278,A,How far out of order should I expect UDP packets to be? Under normal circumstances what should I expect the worse case scenario to be for out of order UDP packets? I'm currently tagging each packet with two bytes (a 0 to 65535 number) to keep track of the order. Is this enough or too much? According to the RFC 3208 you can target the last two packets being out of order. Worst case would by definition be unlimited and such you are better off treating a third out of sequence packet as data loss. In all cases receivers SHOULD temper the initiation of NAK generation to account for simple mis-ordering introduced by the network. A possible mechanism to achieve this is to assume loss only after the reception of N packets with sequence numbers higher than those of the (assumed) lost packets. A possible value for N is 2. This method SHOULD be complemented with a timeout based mechanism that handles the loss of the last packet before a pause in the transmission of the data stream. http://tools.ietf.org/html/rfc3208  It should be plenty I have never seen more then 3-4 out of order UDP packets you could get away with a single byte to track it.
279,A,"How to use Tor control protocol in C#? I'm trying to send commands to the Tor control port programmatically to make it refresh the chain. I haven't been able to find any examples in C# and my solution's not working. The request times out. I have the service running and I can see it listening on the control port. public string Refresh() { TcpClient client = new TcpClient(""localhost"" 9051); string response = string.Empty; string authenticate = MakeTcpRequest(""AUTHENTICATE\r\n"" client); if (authenticate.Equals(""250"")) { response = MakeTcpRequest(""SIGNAL NEWNYM\r\n"" client); } client.Close(); return response; } public string MakeTcpRequest(string message TcpClient client) { client.ReceiveTimeout = 20000; client.SendTimeout = 20000; string proxyResponse = string.Empty; try { // Send message StreamWriter streamWriter = new StreamWriter(client.GetStream()); streamWriter.Write(message); streamWriter.Flush(); // Read response StreamReader streamReader = new StreamReader(client.GetStream()); proxyResponse = streamReader.ReadToEnd(); } catch (Exception ex) { // Ignore } return proxyResponse; } Can anyone spot what I'm doing wrong? Edit: Following Hans's suggestion which he has now deleted for some reason I tried to send ""AUTHENTICATE\n"" instead of just ""AUTHENTICATE"". Now I'm getting back an error from Tor: ""551 Invalid quoted string. You need to put the password in double quotes."" At least there's some progress. I then tried to send ""AUTHENTICATE \""\""\n"" like it wants to but it times out while waiting for a response. Edit: The command works fine in the Windows Telnet client. I don't even have to add the quotes. Can't figure out what's wrong. Maybe the double quotes aren't encoded correctly when they're sent? When I send the AUTHENTICATE command the StreamReader is reading the response to the end but there is no end because on success the stream is kept open. So I changed it to only read the first line of the response in this case. public static string MakeTcpRequest(string message TcpClient client bool readToEnd) { client.ReceiveTimeout = 20000; client.SendTimeout = 20000; string proxyResponse = string.Empty; try { // Send message using (StreamWriter streamWriter = new StreamWriter(client.GetStream())) { streamWriter.Write(message); streamWriter.Flush(); } // Read response using (StreamReader streamReader = new StreamReader(client.GetStream())) { proxyResponse = readToEnd ? streamReader.ReadToEnd() : streamReader.ReadLine(); } } catch (Exception ex) { throw ex; } return proxyResponse; }   public static void CheckIfBlocked(ref HtmlDocument htmlDoc string ypURL HtmlWeb hw) { if (htmlDoc.DocumentNode.InnerText.Contains(""FORBIDDEN ACCESS!"")) { Console.WriteLine(""Getting Blocked""); Utils.RefreshTor(); htmlDoc = hw.Load(ypURL ""127.0.0.1"" 8118 null null); if (htmlDoc.DocumentNode.InnerText.Contains(""FORBIDDEN ACCESS!"")) { Console.WriteLine(""Getting Blocked""); Utils.RefreshTor(); } } } public static void RefreshTor() { IPEndPoint ip = new IPEndPoint(IPAddress.Parse(""127.0.0.1"") 9051); Socket server = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); try { server.Connect(ip); } catch (SocketException e) { Console.WriteLine(""Unable to connect to server.""); RefreshTor(); return; } server.Send(Encoding.ASCII.GetBytes(""AUTHENTICATE \""butt\""\n"")); byte[] data = new byte[1024]; int receivedDataLength = server.Receive(data); string stringData = Encoding.ASCII.GetString(data 0 receivedDataLength); if (stringData.Contains(""250"")) { server.Send(Encoding.ASCII.GetBytes(""SIGNAL NEWNYM\r\n"")); data = new byte[1024]; receivedDataLength = server.Receive(data); stringData = Encoding.ASCII.GetString(data 0 receivedDataLength); if (!stringData.Contains(""250"")) { Console.WriteLine(""Unable to signal new user to server.""); server.Shutdown(SocketShutdown.Both); server.Close(); RefreshTor(); } } else { Console.WriteLine(""Unable to authenticate to server.""); server.Shutdown(SocketShutdown.Both); server.Close(); RefreshTor(); } server.Shutdown(SocketShutdown.Both); server.Close(); }  Probably you using Vidalia that generates a hashed password for control port access. You need to use Tor console app and configure a torrc file. No I wasn't using Vidalia and I solved it anyway."
280,A,"WSASend() with more than one buffer - could complete incomplete? Say I post the following WSASend call (Windows I/O completion ports without callback functions): void send_data() { WSABUF wsaBuff[2]; wsaBuff[0].len = 20; wsaBuff[1].len = 25; WSASend(sock &wsaBuff[0] 2 ......); } When I get the ""write_done"" notification from the completion port is it possible that wsaBuff[1] will be sent completely (25 bytes) yet wsaBuff[0] will be only partially sent (say 7 bytes)? As WSASend is the preferred way of doing overlapped socket IO it would not make any sense if it completed while incomplete - the completion notification/routine/event is the only way for the application to cleanup/recycle the used structures. Also: NO it's not possible a single WSASend call is still a single IO call regardless of the buffers used.  As I've said before in a reply to one of your other very similar questions the only time this is likely to fail is in low resource situations (non-paged pool or locked pages limit issues most likely) where you might get a partial completion and an error return of ENOBUFS. And again as I've said before in 10 years of IOCP development work I've never seen this as a problem in production only in situations where we have been stress testing a system to death (quite literally sometimes as non-paged pool exhaustion can sometimes cause badly behaved drivers to blue screen the box). I would suggest that you simply add some code to log the failure close the socket and that's it you've dealt with the possibility of the failure and can move on. I'd be surprised if your failure handling code is ever executed. But you can be confident that you'll know if it is and once you can reproduce the issue you can spend more time thinking about if you really need to handle it any better. Are you sure Len? Pay careful attention to this specific question - it's a single WSASend() call and not several thus I'd expect it to treat the WSABUF array as a single continuous buffer... and that's my question is actually all about. If you have set the socket's send buffer size to zero then YOUR buffers will be locked in memory and so could cause an ENOBUFS return if the locked pages limit is exceeded likewise neither of us are privy to the exact non-paged pool usage for this situation. The fact that you're using scatter/gather I/O is not IMHO actually that relevant. Also since the documentation to WSASend() doesn't say that this situation cannot cause the error that you're asking about you have to assume that it CAN occur and therefore IMHO my answer stands."
281,A,"Comparing strings in C - strcmp I'm having trouble comparing strings in C (with which I am fairly new to). I have socket on this server application waiting to accept data from a client. In this particular portion of my program I want to be able to execute a MySQL query based on the data received from the client. I want to be able to know when the received data has the value of ""newuser"" to initiate a simple registration procedure. Strcmp is returning a positive 1 value where I believe I should be getting a 0 because the values should be equal. Source Code: //setup socket //loop and select structure to handle multiple connections if ((nbytes = recv(i buf sizeof buf 0)) <= 0) { // got error or connection closed by client if (nbytes == 0) { // connection closed printf(""selectserver: socket %d hung up\n"" i); } else { perror(""recv""); } close(i); // bye! FD_CLR(i &master); // remove from master set } else { char check[] = ""newuser""; char fromUser[sizeof check]; strncpy(fromUserbuf sizeof check); printf(""length of fromUser: %d\n"" sizeof fromUser); printf(""length of check: %d\n"" sizeof check); printf(""message from user: %s\n"" fromUser); printf(""check = %s \n"" check); int diff = strcmp(fromUser check); printf(""compare fromUser to check: %d\n"" diff); if ( strcmp(fromUser check) == 0) { printf(""aha! new user""); } Output: length of fromUser: 8 length of check: 8 newuser from user: newuser check = newuser compare fromUser to check: I have a feeling I'm not handling the incoming buffer correctly or erroneously copying the buffer. You are not logging the fromUser value only its length. Replace with: char check[] = ""newuser\0""; double quotes produce null terminated string by themselves don't they? """" literals are implicitly NUL terminated. Adding another NUL explicitly does not help here.  I believe the problem here (one of the problems here) is that fromUser (due to the way it is created) is not null terminated.  Here's the sample code you gave in your question (with debugging code removed): //setup socket //loop and select structure to handle multiple connections if ((nbytes = recv(i buf sizeof buf 0)) <= 0) { [... exception handling here ...] } else { char check[] = ""newuser""; char fromUser[sizeof check]; strncpy(fromUserbuf sizeof check); if ( strcmp(fromUser check) == 0) { printf(""aha! new user""); } This code is wrong; you're potentially copying more bytes from buf[] than have been received. This will lead to you comparing against garbage (that might by chance happen to match your ""newuser"" string). And as other people have said you have a second bug due to not NUL terminating one of your strings. In this case I'd use memcmp(). This is like strcmp() but it takes a length parameter rather than expecting NUL-terminated strings. //setup socket //loop and select structure to handle multiple connections if ((nbytes = recv(i buf sizeof buf 0)) <= 0) { [... exception handling here ...] } else { static const char check[] = ""newuser""; const size_t check_len = sizeof(check) - 1; // exclude the NUL terminator if (nbytes >= check_len && memcmp(buf check check_len) == 0) { printf(""aha! new user""); } P.S. Not directly related but recv() can fail by returning -1 with errno==EINTR. This isn't an error condition you just need to try again. Normally this happens so rarely that people get away without checking it until they integrate with some other code that uses signals and suddenly their code randomly fails. In a select()-based app you should also be setting your sockets to non-blocking and then check for errno==EAGAIN and go back to the select() in that case. This can happen if the TCP/IP stack receives a corrupted packet - it thinks it has a packet so select() will tell you it's readable it's only when you try to read it that the TCP/IP stack does the checksum calculation and realises it has to throw away the data. It'll then either block (bad) or if it's set to nonblocking then it will return -1 with errno==EAGAIN.  This code seems off: if ((nbytes = recv(i buf sizeof buf 0)) <= 0) { // your stuff } else { const char *pCheck = ""newuser""; char *fromUser = new char[nbytes]; strncpy(fromUser buff nbytes); fromUser[nbytes] = '\0'; if(strcmp(fromUsercheck)==0) // blah delete [] fromUser; }  Two changes required: char fromUser[sizeof check] = {'\0'}; //Make all null characters strncpy(fromUserbuf sizeof check -1); //Last character is for null character.  strncpy copies at most - in this case - sizeof check bytes. If the nul byte is not in that range it is not copied. You probably are getting the word ""newuser"" as part of a longer sentence like ""newuser blah blah"" so you need to place that nul yourself strncpy(fromUser buf sizeof check); fromUser[sizeof check - 1] = '\0'; or use strlcpy if available. +1 for mentioning strlcpy  You miss '\0' char at the end of fromUser: ... strncpy(fromUserbuf sizeof check); fromUser[strlen(check)] = '\0'; Wouldn't that be sizeof(check) - 1 ?"
282,A,"Creating an IPEndPoint from a hostname I am using a third-party dll that requires an “IPEndPoint”. As the user can enter either an IP Address or a Host name I need to convert a Host name to an IP address before I can create an IPEndPoint. Is there any functions to do this in .net or am I going to have to write my own DNS lookup code ? System.Net.Dns.GetHostAddresses public static IPEndPoint GetIPEndPointFromHostName(string hostName int port bool throwIfMoreThanOneIP) { var addresses = System.Net.Dns.GetHostAddresses(hostName); if (addresses.Length == 0) { throw new ArgumentException( ""Unable to retrieve address from specified host name."" ""hostName"" ); } else if (throwIfMoreThanOneIP && addresses.Length > 1) { throw new ArgumentException( ""There is more that one IP address to the specified host."" ""hostName"" ); } return new IPEndPoint(addresses[0] port); // Port gets validated here. } How do you know `addresses[0]` is the most suitable address in the list of returned addresses? You don't know but the odds are very slim that it matters. it should be the first ip in the list as round robin DNS servers will serve it in a different order every time expecting clients to use the first one so use the first in the list ;).  You can use something like this: var addresses = Dns.GetHostAddresses(uri); Debug.Assert(addresses.Length > 0); var endPoint = new IPEndPoint(addresses[0] port);"
283,A,"How to maintain a persistant network-connection between two applications over a network? I was recently approached by my management with an interesting problem - where I am pretty sure I am telling my bosses the correct information but I really want to make sure I am telling them the correct stuff. I am being asked to develop some software that has this function: An application at one location is constantly processing real-time data every second and only generates data if the underlying data has changed in any way. On the event that the data has changed send the results to another box over a network Maintains a persistent connection between the both machines altering the remote box if for some reason the network connection went down From what I understand I imagine that I need to do some reading on doing some sort of TCP/IP socket-level stuff. That way if the connection is dropped the remote location will be aware that the data it has received may be stale. However management seems to be very convinced that this can be accomplished using SOAP. I was under the impression that SOAP is more or less a way for a client to initiate a procedure from a server and get some results via the HTTP protocol. Am I wrong in assuming this? I haven't been able to find much information on how SOAP might be able to solve a problem like this. I feel like a lot of people around my office are using SOAP as a buzzword and that has generated a bit of confusion over what SOAP actually is - and is capable of. Any thoughts on how to accomplish this task would be appreciated! I think SOAP is the wrong tool. SOAP is a spec for exchanging structured data. For your problem the simplest thing would be to write a program to just transfer data and figure out if the other end is alive. Sockets are a good way to go. There are lots of socket programming tutorials on the net. Pick your language and ask Mr. Google. Write a couple of demo programs to teach yourself how it works. Ask if you have more specific questions. For the problem you'll need a sender and a receiver. The sender sends data when it gets it the receiver waits for data and hands it off when it arrives. Get that working first. Next add in heartbeats; a message that says ""I'm alive"" sent periodically. Get that working next. You'll need to be determine the exact behavior you want -- should both sides send heartbeats to the other end the maximum time you are willing to wait for a heartbeat and what action you take should heartbeats stop arriving. The network connection can drop the other end can crash the other end can hang and perhaps there are other conditions you should think about (e.g. what if the real time data is nonsense?). Figure out how to handle each condition and code up the error handling. Test it out and serve with a side of documentation.  SOAP certainly won't tell you when the data source goes down though you could use ""heartbeats"" to add that. Probably you are right and they are just repeating a buzz word and don't actually know much about what SOAP is or does or have any real argument for why it ought to be used here."
284,A,"How do I list a computer's remotely shared files using C#? How do I list a computer's remotely shared files using C#? Could someone help me with this? [Update]: Done it by exe ""NET VIEW"" command in C# code finally. FYI: ""How *do you* list..."" I'd look at the Win32_Share WMI class. If you haven't used WMI from C# before this codeproject article should show you how to get started: Howto: (Almost) Everything in WMI via C# - Part 1: Registry This Technet article has a sample VBScript for enumerating shared: Enumerating Shared Folders Just change strComputer in that sample to be the computer you're interested in."
285,A,Do I really need two policy servers for two different silverlight applications? I have a client-server Silverlight application which is use Socets. I have server appliaction on may computer(Win Form application) and client applucation as web site(Silverlight application). I use policy server which open port 943. Everything works fine on this application. But now I need to write another client-server application. Server for that application olso use port 943 for policy connection. When I try to run this 2 server applications on the same compyeter an excepten is thrown which says that only one application can work on port 943. How can I solve this problem? Thanks. Use a different port for the second server. Alternatively use the REUSEADDR socket option but that's not a good idea. I can not use other port. The only port for policy server is 943. @Samvel: There is no other way around it - it's a limitation of TCP itself that you just have to live with.  Isn't the idea of a policy server to grant access to multiple applications?  The only thing you need to do is to write separate policy server application and run it before your main server application. That doesn't make any sense to me and I'm not sure how it got marked as correct.  Like Nikolai said this is a non sensical question. The answer is: Just run one policy server! I am going to edit the question to reflect this.
286,A,"Where's the const SOMAXCONN defined in c? I thought it was retrieve at compile time from /proc/sys/net/core/somaxconnbut after I modified it to 1024 by echo 1024 > /proc/sys/net/core/somaxconnthe SOMAXCONN is still 128 in my programe.  printf(""---------------set socket to listenmaxconn is %d--------------\r\n\r\n"" SOMAXCONN); // set socket to listen if (listen(sock_listen SOMAXCONN) != 0)... I've checked sys/socket.h but it's not there... Where/how can I change its value? It should be in `tcp.h`. See: http://stackoverflow.com/questions/1198564/programatically-evaluating-the-value-of-somaxconn-to-set-the-listen-backlog-para @Evan Mulawski just checkednot there... How about `/etc/sysctl.conf`? It should be under `kern.ipc.somaxconn`. You can use http://linux.die.net/man/8/sysctl to programmatically change the value. @Evan Mulawski I decide to hardcode the number in my programe `listen(sock_listen 1024)`.`sysctl` is a bash commandhow can I use it in my programe? On my system it's defined in bits/socket.h /* Maximum queue length specifiable by listen. */ #define SOMAXCONN 128 According to this you don't need to use SOMAXCONN. Just specify number you want and it will be limited to actual maximum. On my Ubuntu 12.04 it is in /usr/include/i386-linux-gnu/bits/socket.h. SOMAXCONN is defined by http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/sys_socket.h.html  Under Linux it seems to be here: $ find /usr/include -name \*.h -exec grep SOMAXCONN {} /dev/null \; /usr/include/bits/socket.h:#define SOMAXCONN 128 Under Mac OS X and BSD it seems to be here: $ find /usr/include -name \*.h -exec grep SOMAXCONN {} /dev/null \; /usr/include/sys/socket.h:#define SOMAXCONN 128 i.e. in both cases it's socket.h but they live in different locations on different operating systems it seems."
287,A,"How do I use the windows network API in Qt? I am going to put an network status icon in my GUI application. To get the network status notification I am trying to use the Windows API. For this I am thinking to use NetworkAvailabilityChangedEventHandler in my application. I am very new to programming with the Windows API and framework. Can anybody help me in the following things: Can the API NetworkAvailabilityChangedEventHandle only be used in C#? Can I use it in C++ (Qt)? Which header file must I include? (I checked in MSDN for this. but they are using namespace for this. All the examples are in C#. I am not able to understand how to implement it in my C++ code.) I will be grateful if somebody can give me a detailed code snippet for using this windows event handler including the .h file or namespace to be included. Where did you get the idea to use the NetworkAvailabilityChangedEventHandler delegate? That is explicitly not part of the Windows API but rather a delegate function used by the .NET Framework in conjunction with the NetworkChange.NetworkAvailabilityChanged event. That explains why all the examples on MSDN are in C#—because this is only intended to be used in applications targeting the .NET Framework. If you're writing unmanaged C++ using Qt then you're not using the .NET Framework and you can't take advantage of its functionality. The Windows API equivalent is the InternetGetConnectedState function which returns a value indicating whether or not the system is currently connected to the Internet. You'll find that its MSDN documentation is substantially friendlier towards unmanaged C++ developers because that's the primary intended audience. The information that you're seeking is given at the bottom: Header          Wininet.h Library           Wininet.lib DLL                 Wininet.dll You can find a list of all the WinINet functions here. @cody Gray: thanks a lot for quick response with solution. When i used the API 'GetSystemPowerStatus' i just included the windows.h file in my application and dint add any dll and lib file. But is it ok just to include the file wininet.h in my code? or i need to use the lib and dll? if yes could please let me know how i can use them in my qt app? Do i need to find the .lib and .dll and paste them in my qt app directory or provide the path? @Surjya: You'll need to include the header file and add `wininet.lib` as a dependency to your project. You do *not* need to copy the DLL file anywhere as it's included with a standard Windows installation. Your app should not have its own copy. It's difficult to provide more specific instructions on how to add a dependency without knowing which IDE/compiler you're using. In Visual Studio right-click on your project and open its Properties window. Then expand the ""Linker"" category and select ""Input"". Add `wininet.dll` to the ""Additional Dependencies"" line. @Surjya: See [this question](http://stackoverflow.com/questions/1114914/add-library-to-visual-studio-2008-c-project) for more details. @Cody Gray: I am using Qt-Creator IDE. I included the line ""LIB += wininet.lib"". But I am getting compilation error as ""No such file or directory : wininet.lib"". Where can I find the .lib file in my windows installation? @Surjya: I may not be the best person to ask about that. I downloaded Qt a few months ago just to see what all the hullabaloo was about and immediately uninstalled it after a few minutes upon discovering that it doesn't use native GUI controls from the underlying OS. So I've never actually used Qt-Creator and that `LIB += wininet.lib` looks mighty suspicious to me. It may very well be the way that Qt-Creator does things though. @Surjya: The only suggestion I can offer is to make sure that you have the Windows SDK installed. I don't know if Qt-Creator comes with it or not but you need it to develop programs targeting the Windows API. If you don't have it that would explain why you're missing the `.lib` files. You can download the latest version here: http://www.microsoft.com/downloads/en/details.aspx?FamilyID=6b6c21d2-2006-4afa-9702-529fa782d63b&displaylang=en This isn't _precisely_ the same thing as `NetworkChange.NetworkAvailabilityChanged` is a push notification while `InternetGetConnectedState` is a polling API... @bdonlan: You're right. I did say it was the equivalent not that it was exactly the same. `NetworkChange.NetworkAvailabilityChanged` actually polls `InternetGetConnectedState` under the hood and implements a push notification to the consumer. You get a lot of things for free with .NET that you don't get with the Windows API. That doesn't stop the asker from rolling his own polling function and to do it he'd use `InternetGetConnectedState`."
288,A,"gcc: wrong behaviour after new variable was added I am writing simple programs: server and client. You know I am just learning all these stuff. I added new variable (fileUp in server.c) and the client just crashed. I debugged it with gdb. The client can't read anything from the socket. Without that one variable works fine. I did compile these programs with both gcc and g++ with -Wall. No errors no warnings. Programs are as simple as they can be. I don't understand what is wrong. Any hint'll be appreciated. server.c #include <stdio.h> #include <fcntl.h> #include <stdlib.h> #include <sys/socket.h> #include <arpa/inet.h> #include <string.h> #include <unistd.h> int main(int argc char **argv) { struct sockaddr_in address client; int s = socket(AF_INET SOCK_STREAM 0); memset(&address 0 sizeof(address)); address.sin_family = AF_INET; address.sin_addr.s_addr = htonl(INADDR_ANY); #define PORT 54321 address.sin_port = htons(PORT); if(bind(s (struct sockaddr *)&address sizeof(address))<0) { perror(""nie udał się bind""); exit(-1); } if(listen(s 5)<0) { perror(""nie udał się listen""); exit(-1); } socklen_t client_len; int c = accept(s (struct sockaddr *)&client &client_len); int file = open(""../data"" O_RDONLY); if(file<0) { perror(""nie udało się otworzyć pliku""); exit(-1); } #define MAX 1024 char buf[MAX]; int n = read(file buf MAX); int fileUp = n; do { write(c buf MAX); buf[n-1] = '\0'; printf(""%d: %s\n"" n buf); /*fileUp += n; printf(""pobrano: %d\n"" fileUp);*/ n = read(file buf MAX); getchar(); } while(n != 0); close(c); close(s); return 0; } client.c #include <stdio.h> #include <fcntl.h> #include <stdlib.h> #include <sys/socket.h> #include <arpa/inet.h> #include <string.h> #include <unistd.h> int main(int argc char **argv) { struct sockaddr_in address; int s = socket(PF_INET SOCK_STREAM 0); memset(&address 0 sizeof(address)); address.sin_family = AF_INET; #define PORT 54321 address.sin_port = htons(PORT); if(inet_pton(AF_INET argv[1] &address.sin_addr) <=0) { perror(""podano nieprawidłowy adres""); exit(-1); } if(connect(s (struct sockaddr *)&address sizeof(address))<0) { perror(""nie można się połączyć""); exit(-1); } #define MAX 1024 char buf[MAX]; int n = read(s buf MAX); int fileDown = n; do { buf[n-1] = '\0'; printf(""%d: %s\n"" n buf); n = read(s buf MAX); fileDown += n; printf(""pobrano: %d\n"" fileDown); } while(n != 0); close(s); return 0; } Prime example why ""compiles without warnings and errors"" does not mean ""works"" let alone ""never fails"". Just curious: What university are you in? (In Poland I guess) Just so you'd know. It's spelled server not serwer. socklen_t client_len; should be socklen_t client_len = sizeof(client); The stack layout will change when you add your new variable - so the uninitialized value in client_len just happened to work before it doesn't after - most likely making your accept call fail and then you're trying to write to an invalid FD. You should of course also check the return value of accept ...and the return value of `write` as well. Now i feel stupid I have not been aware of this (need of initialization) since today :("
289,A,Objective C reachability of LAN devices I need write a program on Mac OS X to manage some number of projectors on the LAN (using IP address) and I'm trying to figure out how to monitor their connection status. Something like a simple ping just to make sure I can reach them. I've tried using the NSURLRequest requestWithURL call. It worked but the program stops responding for quite a while if any of the projectors are offline. I'm just looking to get the status and change the UI indication. Are there better ways to do this? Any help or point in the right direction would be great thanx. Try setting the timeoutInterval property on the NSURLRequest. Otherwise you'll have to run the ping connection in a background thread or else use use NSURLConnect and check the asynchronous callbacks delivered to the delegate. Thank you. NSURLConnect and asynchronous callbacks to a delegate works. Curious though how do one run a ping connection through code? If you mean a real ping using ICMP ECHO requests then the normal way to do it is to create a raw socket and craft the ICMP headers yourself. At least that's the way we used to do it.
290,A,Packet sniffing in c under window OS I want to sniff network packets without wincap library kindly give me some hints or direction so that I can make it possible. You mean winpcap library aye? Even Cygwin related TCPDump tools require it. I don't know of any that don't use it. Why do you need to avoid it? Only the window API which I have to use. It's an open source project so you can download the source and see what it's doing. Of course if you spend a lot of time looking at winpcap source to try to replicate what winpcap does you might as well just use it. http://www.winpcap.org/install/bin/WpcapSrc_4_1_2.zip You know libpcap exists for a reason: It does something that's necessary. You need to set up a system-level hook for TCP/IP events and the way global hooks work means you need to do this from a DLL. Having hooked those events you have to figure out the contents of the packets you get. Are you sure you want to re-invent this wheel? I found some introductory info on hooking Windows events here.  You could start to do kernel level programming and catch there the packets. This is for sure complicated but you'll learn much. Catching packets using netfilter hooks in the kernel is a pain. Catching packets in a huge sized non documented and closed kernel as the Microsoft one is shooting yourself in a foot. IMHO of course.
291,A,"BIND ERROR : Address already in use I am learning socket programming in c I wrote this simple program to accept connections on port 5072. i connect to it using telnet. This works fine the first time but when i try to run it again immediately it fails showing BIND : Address already in use but then again starts to work after a minute or so. What am i doing wromg? #include <stdio.h> #include <string.h> #include <sys/types.h> #include <sys/socket.h> #include <netdb.h> #include <arpa/inet.h> #include <stdlib.h> int main(){ //variables int listenfd clientfd; socklen_t clilen; struct sockaddr_in cliaddrservaddr; //getsocket if((listenfd = socket(AF_INETSOCK_STREAM0)) == -1){ perror(""SOCKET""); exit(0); } //prep the servaddr bzero(&servaddrsizeof servaddr); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = inet_addr (""127.0.0.1""); servaddr.sin_port = htons(5072); //bind if(bind(listenfd (struct sockaddr *) &servaddrsizeof servaddr)<0){ perror(""BIND""); exit(0); } //listen if(listen(listenfd20)<0){ perror(""LISTEN""); exit(0); } //accept int counter = 1; clilen = sizeof cliaddr; while(counter<3){ clientfd = accept(listenfd(struct sockaddr *) &cliaddr&clilen); if(clientfd == -1){ perror(""ACCEPT""); exit(0); } if(fork()==0){ close(listenfd); printf(""CONNECTION NO. %d\n""counter); close(clientfd); exit(0); } counter++; close(clientfd); } close(listenfd); } Thanks Please post the *smallest* code that shows the problem. sorry i didn't know whether there is a error in the bind call or the child process structure so i posted the whole thing. But will take care in future. Or simply wait a couple minutes. The TCP/IP stack holds onto the socket for twice the maximum segment lifetime after closeto prevent any stray packets from the first connection from showing up after a second one's established which will make it all confused. Is this answer really helpful? He is asking what he is doing wrong - besides abhishek already mentioned that waiting for a minute solves the problem...  You must setsockopt(SO_REUSEADDR) See this faq for explanation why."
292,A,"Why is Objective-C converting JSON values to a hash of ASCII character codes? We are building an iPhone chat application. When sending from the browser to the iPhone a JSON chat message:  {""content"":""Hi""} The iPhone receives:  {""content"":{""0"":72""1"":105""length"":2}} But we intend for it to receive the same exact message. To reproduce this issue first install node.js & redis. Then: Get the code: git clone git://github.com/acani/acani.git cd acani git submodule update --init Start Redis on the default port. From http://github.com/acani/acani-node: node acani-node-server.js # run node.js chat server # open index.html in a Google Chrome or Firefox and follow instructions. Open Lovers.xcodeproj located in http://github.com/acani/acani-chat/tree/master/Lovers2/ and change LoversAppDelegate.m to initially load the ChatViewController instead of the HomeViewController. homeViewController = [[HomeViewController alloc] init]; # comment out this line # change the next line to: navigationController = [[UINavigationController alloc] initWithRootViewController:[[ChatViewController alloc] init]]; # Then build & run. It's not the converting of the JSON string into an NSDictionary; that works. I know because I put an NSLog after this line: http://github.com/acani/acani-chat/blob/master/Lovers2/ZTWebSocket.m#L207 and it shows that the characters of the value of the JSON string have been translated to ASCII codes. Can you show the relevant Objective-C code where you actually parse the JSON? What JSON framework are you using? We figured it out. It wasn't the iPhone or Objective-C at all. The conversion error was happening on the node.js server. We forgot to put quotes around the string values of the JSON object and so the JSON.stringify() JavaScript function was converting the strings as shown above... except we were doing something like: {""content"":Hi}. When we changed it to: {""content"":""Hi""} it worked fine. Duhh...  My guess is you need to escape the string (JSON) being sent using stringByAddingPercentEscapesUsingEncoding and then unescape it on receipt. The first three numbers are 072 - in decimal that's 'H'. Which makes me think a "" might be getting lost due to transmission without encoding. There are other things against this theory but I think it is worth looking at. That makes sense. Thanks. I'll try that tonight and report back. I guess I figured the AsyncSocket library would take care of that..."
293,A,"Ruby - See if a port is open I need a quick way to find out if a given port is open with Ruby. I currently am fiddling around with this: require 'socket' def is_port_open?(ip port) begin TCPSocket.new(ip port) rescue Errno::ECONNREFUSED return false end return true end It works great if the port is open but the downside of this is that occasionally it will just sit and wait for 10-20 seconds and then eventually time out throwing a ETIMEOUT exception (if the port is closed). My question is thus: Can this code be amended to only wait for a second (and return false if we get nothing back by then) or is there a better way to check if a given port is open on a given host? Edit: Calling bash code is acceptable also as long as it works cross-platform (e.g. Mac OS X *nix and Cygwin) although I do prefer Ruby code. My slight variation to Chris Rice's answer. Still handles timing out on a single attempt but also allows multiple retries until you give up.  def is_port_open?(host port timeout sleep_period) begin Timeout::timeout(timeout) do begin s = TCPSocket.new(host port) s.close return true rescue Errno::ECONNREFUSED Errno::EHOSTUNREACH sleep(sleep_period) retry end end rescue Timeout::Error return false end end  Just for completeness the Bash would be something like this: $ netcat $HOST $PORT -w 1 -q 0 </dev/null && do_something -w 1 specifies a timeout of 1 second and -q 0 says that when connected close the connection as soon as stdin gives EOF (which /dev/null will do straight away). Bash also has its own built-in TCP/UDP services but they are a compile-time option and I don't have a Bash compiled with them :P They're pretty simple: just pretend /dev/{tcp}/HOST/PORT are files :) For future reference I found this as `nc` on my system rather than `netcat`  More Ruby idiomatic syntax: require 'socket' require 'timeout' def port_open?(ip port seconds=1) Timeout::timeout(seconds) do begin TCPSocket.new(ip port).close true rescue Errno::ECONNREFUSED Errno::EHOSTUNREACH false end end rescue Timeout::Error false end This gave a false positive for the inputs '192.0.2.0' 80 10 which should be invalid (according to http://stackoverflow.com/questions/10456044/what-is-a-good-invalid-ip-address-to-use-for-unit-tests). I got the same result with Ruby 1.9.3p448 and 2.0.0p195 both on Mac. In what situations does this method manage to return false? (I even tried writing to the socket before closing it but that still returned true!) Just tried '0.0.0.0' 80 1 and this also gave true. works fine for me! I think this function def is missing a begin statement before timeout or is that somehow optional? (`rescue Timeout::Error` should be in a `begin` block shouldn't it?) @nash Ruby permits a rescue clause to belong to the method itself. In effect the method is an implicit block.  I recently came up with this solution making use of the unix lsof command: def port_open?(port) !system(""lsof -i:#{port}"" out: '/dev/null') end  Something like the following might work: require 'socket' require 'timeout' def is_port_open?(ip port) begin Timeout::timeout(1) do begin s = TCPSocket.new(ip port) s.close return true rescue Errno::ECONNREFUSED Errno::EHOSTUNREACH return false end end rescue Timeout::Error end return false end Works like a charm! Thanks! I had some trouble with this blocking (I think). Basically the timeout wouldn't actually time out. Not sure why but the netcat solution worked well in its place. This answer has a solution that also works on windows: http://stackoverflow.com/a/3473208/362951 Shouldn't true/false be swapped? @Leopd I agree. Fixed."
294,A,"TCP HTTP and the Multi-Threading Sweet Spot I'm trying to understand the performance numbers I'm getting and how to determine the optimal number of threads. See the bottom of this post for my results I wrote an experimental multi-threaded web client in perl which downloads a page grabs the source for each image tag and downloads the image - discarding the data. It uses a non-blocking connect with an initial per file timeout of 10 seconds which doubles after each timeout and retry. It also caches IP addresses so each thread only has to do a DNS lookup once. The total amount of data downloaded is 2271122 bytes in 1316 files via 2.5Mbit connection from http://hubblesite.org/gallery/album/entire/npp/all/hires/true/ . The thumbnail images are hosted by a company which claims to specialize in low latency for high bandwidth applications. Wall times are: 1 Thread takes 4:48 -- 0 timeouts 2 Threads takes 2:38 -- 0 timeouts 5 Threads takes 2:22 -- 20 timeouts 10 Threads take 2:27 -- 40 timeouts 50 Threads take 2:27 -- 170 timeouts In the worst case ( 50 threads ) less than 2 seconds of CPU time are consumed by the client. avg file size 1.7k avg rtt 100 ms ( as measured by ping ) avg cli cpu/img 1 ms The fastest average download speed is 5 threads at about 15 KB / sec overall. The server actually does seem to have pretty low latency as it takes only 218 ms to get each image meaning it takes only 18 ms on average for the server to process each request: 0 cli sends syn 50 srv rcvs syn 50 srv sends syn + ack 100 cli conn established / cli sends get 150 srv recv's get 168 srv reads file sends data  calls close 218 cli recv HTTP headers + complete file in 2 segments MSS == 1448 I can see that the per file average download speed is low because of the small file sizes and the relatively high cost per file of the connection setup. What I don't understand is why I see virtually no improvement in performance beyond 2 threads. The server seems to be sufficiently fast but already starts timing out connections at 5 threads. The timeouts seem to start after about 900 - 1000 successful connections whether it's 5 or 50 threads which I assume is probably some kind of throttling threshold on the server but I would expect 10 threads to still be significantly faster than 2. Am I missing something here? EDIT-1 Just for comparisons sake I installed the DownThemAll Firefox extension and downloaded the images using it. I set it to 4 simultaneous connections with a 10 second timeout. DTM took about 3 minutes to download all the files + write them to disk and it also started experiencing timeouts after about 900 connections. I'm going to run tcpdump to try and get a better picture what's going on at the tcp protocol level. I also cleared Firefox's cache and hit reload. 40 Seconds to reload the page and all the images. That seemed way too fast - maybe Firefox kept them in a memory cache which wasn't cleared? So I opened Opera and it also took about 40 seconds. I assume they're so much faster because they must be using HTTP/1.1 pipelining? And the Answer Is!?? So after a little more testing and writing code to reuse the sockets via pipelining I found out some interesting info. When running at 5 threads the non-pipelined version retrieves the first 1026 images in 77 seconds but takes a further 65 seconds to retrieve the remaining 290 images. This pretty much confirms MattH's theory about my client getting hit by a SYN FLOOD event causing the server to stop responding to my connection attempts for a short period of time. However that is only part of the problem since 77 seconds is still very slow for 5 threads to get 1026 images; if you remove the SYN FLOOD issue it would still take about 99 seconds to retrieve all the files. So based on a little research and some tcpdump's it seems like the other part of the issue is latency and the connection setup overhead. Here's where we get back to the issue of finding the ""Sweet Spot"" or the optimal number of threads. I modified the client to implement HTTP/1.1 Pipelining and found that the optimal number of threads in this case is between 15 and 20. For example: 1 Thread takes 2:37 -- 0 timeouts 2 Threads takes 1:22 -- 0 timeouts 5 Threads takes 0:34 -- 0 timeouts 10 Threads take 0:20 -- 0 timeouts 11 Threads take 0:19 -- 0 timeouts 15 Threads take 0:16 -- 0 timeouts There are four factors which affect this; latency / rtt  maximum end-to-end bandwidth recv buffer size and the size of the image files being downloaded. See this site for a discussion on how receive buffer size and RTT latency affect available bandwidth. In addition to the above average file size affects the maximum per connection transfer rate. Every time you issue a GET request you create an empty gap in your transfer pipe which is the size of the connection RTT. For example if you're Maximum Possible Transfer Rate ( recv buff size / RTT ) is 2.5Mbit and your RTT is 100ms then every GET request incurs a minimum 32kB gap in your pipe. For a large average image size of 320kB that amounts to a 10% overhead per file effectively reducing your available bandwidth to 2.25Mbit. However for a small average file size of 3.2kB the overhead jumps to 1000% and available bandwidth is reduced to 232 kbit / second - about 29kB. So to find the optimal number of threads: Gap Size = MPTR * RTT MPTR / (MPTR / Gap Size + AVG file size) * AVG file size) For my above scenario this gives me an optimum thread count of 11 threads which is extremely close to my real world results. If the actual connection speed is slower than the theoretical MPTR then it should be used in the calculation instead. I fail to see what CPU time has to do with anything. @Eric Strom: Yeah I'm just having trouble finding sites that have 1000+ images on a single page with image tags and not flash or something weird like that. @Robert => you could see what kind of rates some of the batch image download addons for firefox can get from that site. I believe there are a few that you can set the number of concurrent downloads. @Robert If you download something like Google's start page instead of some images do the results differ? Are you sure your client isn't limiting itself to two concurrent HTTP sessions (causing client #3 to wait for client #1/#2 to finish)? This is after all what some HTTP RFCs suggests clients *should* do. And if not are you sure the *server* (or something along the way) doesn't do this? Ie. can you verify the simultaneousness of your sessions? @bzlm: I wrote the client from scratch I don't use any libraries. So I'm pretty close to 100% sure the connections are concurrent from the client side. I suppose I could you tcpdump to see when I recv SYN's from the server although I don't know if that's sufficient. @hobbs: How could Perl threading be at fault? Total cpu time for the process is under 2 seconds. If used right perl threads are NOT the cause of this. I have seen some servers with limits on concurrent access from the same IP. It may be that you are hitting a limit like that. You can check for this by trying to download content from some other sites. Please correct me this summary is incorrect: Your multi-threaded client will start a thread that connects to the server and issues just one HTTP GET then that thread closes. When you say 1 2 5 10 50 threads you're just referring to how many concurrent threads you allow each thread itself only handles one request Your client takes between 2 and 5 minutes to download over 1000 images Firefox and Opera will download an equivalent data set in 40 seconds I suggest that the server rate-limits http connections either by the webserver daemon itself a server-local firewall or most likely dedicated firewall. You are actually abusing the webservice by not re-using the HTTP Connections for more than one request and that the timeouts you experience are because your SYN FLOOD is being clamped. Firefox and Opera are probably using between 4 and 8 connections to download all of the files. If you redesign your code to re-use the connections you should achieve similar performance. @Robert: You don't have to be malicious in order to abuse something. You don't have to be a network analyst or a server administrator in order to put up a webpage with a thousand images on it. HTTP 1.1 spec says that implementations `SHOULD` implement persistence. Opening a thousand connections per minute to a webserver is inefficient (as you've discovered while asking this question) and still IMO abusive. Your assertion that ""hosting page with 1300 images"" implies ""the server MUST support 1300 simultaneous connection attempts from a single host"" won't hold with any server administrators. @MattH: +1 In general I suspect that you're correct about the timeouts being do to a firewall perceiving a `SYN FLOOD`. However I would disagree with your point about abuse for two reasons. The `HTTP/1.1` standard **does not** require clients to support persistent connections. Clients have the option of using the `Connection: close` header. Second the site lists all 1300+ thumbnails on a single web page. If you're not prepared to serve that many images to a single client over non-persistent connections then break it up into multiple pages. I will likely try persistent connections. @MattH: There is a thread pool of N threads each thread has exactly one TCP connection open at a time for N concurrent TCP connections. Pipelining is not currently used so each connection is used to download one file. You make a reasonable point. I'm still trying to understand more specifically what's happening and right now I'm actually suspecting that due to the small file sizes and the connection setup overhead that I'm suffering from tinygrams eating my bandwidth. You where most correct on your answer. Take a look at my summary of results if you're interested."
295,A,"Starting Timer problem from Asynchronous Server Socket I have got a Client/Server Application that using Asynchronous Socket.My problem is i cant start timer control on Client Side from Server side.I have got a method for sending data to client from server side and client got a method for handle this data and starts the timer.There is no problem at getting data and process it.But timer control is not working.I have got a button on client side thats starting timer with same code. So its working with a button on client side but not working if this command come from server.Whats the problem ?? Here is my codes ;  void MessageSend(string msj) { foreach (Client _client in connectedCompList) { //for sending data from server side to client side _client.clientSoket.Send(ConvertByteArray(msj)); } } private void btnStartExam_Click(object sender EventArgs e) { MessageSend(""/t/"" + """" + txtMinute.Text + """" + txtSecond.Text+""""); } void MessageControl(string message) { if (message.Length < 1) return; switch (message.Substring(0 3)) { case ""/e/"": txtAdayNo.Text = """"; txtVeri.Text = """"; txtAdSoyad.Text = """"; txtSinav.Text = """"; break; case ""/t/"": // starting exam after separate min and sec. string[] time = message.Split(''); minute = Convert.ToInt32(time[1]); second = Convert.ToInt32(time[2]); timer.Enabled = true; timer.Start(); break; default: break; } private void btnTest_Click(object sender EventArgs e) { // working with this event. timer.Enabled = true; timer.Start(); } If the timer is a Windows.Forms.Timer you'll need to call Control.Invoke to marshal the call that sets the timer properties and starts the timer back on the UI thread. You should also do this for all of your text box and other UI elements. I suspect this is your problem since socket communication typically occurs on a ThreadPool thread and not on the main (UI) thread. But i can control every windows forms control on client side Except Timer object.So there is no problem with other controls.Can you give me an example for me ? @Ibrahim: Have you set a break point to make sure you're actually getting into that call? i am using .NET Framework 2.0 so lambda not working. Is there .NET 2.0 Way ? @Ibrahim: Yes - Make it a separate method and use a delegate. See: http://msdn.microsoft.com/en-us/library/zyzhdc6b(v=VS.80).aspx Yeah. Its process codes but its not getting into Timer Tick event. Thank you so much. This fixed my problem >> this.Invoke(new ThreadStart(delegate() { timer.Enabled = true; timer.Start(); })); @Ibrahim: try changing it to: `this.Invoke(new Action( () => { timer.Enabled = true; timer.Start(); }));`"
296,A,"Is a socket in an error state ready for reading according to select()? (I'm using CPython v2.6 (on Linux Windows and OSX) but I don't think my question is specific to Python or my target systems.) If my application is blocked in a select([sock] [] [] None) call and the remote side is killed abruptly can I rely on my select call unblocking even though I'm not selecting for errors? I know that the proper way to do these things is to also check for errors in the socket (e.g. select([sock] [] [sock] None)) but the documentation I read for select says that the definition of error varies from system to system. The third set is not actually for errors even though it is sometimes called that. It is for exceptional conditions such as TCP urgent data. If there is a read error/remote disconnect that will show up in the readable set. I am not sure how much of this may be ""implementation dependent"" since there is a little fuzziness to this area of select() depending on how you read the man page but the systems I have worked on would return the socket as readable and then your read would return an error (this assumes TCP of course and the remote side has RST the connection.) The key part is ""descriptor is considered ready if it is possible to perform the corresponding I/O operation (e.g. read(2)) without blocking."" which if the read returned immediately with an error would be a true statement. I would recommend verifying this behavior on your target systems by writing a simple client server have the client connect and then select for read. Have the server accept sleep for a second and then exit itself. If what I have typically seen is standard to you the select will return with the socket as readable and your read will not block but will return an error of connection reset. Oh yes and just to make matters worse (from linux man pages) Under Linux select() may report a socket file descriptor as ""ready for reading"" while nevertheless a subsequent read blocks. This could for example happen when data has arrived but upon examination has wrong checksum and is discarded. There may be other circumstances in which a file descriptor is spuriously reported as ready. Thus it may be safer to use O_NONBLOCK on sockets that should not block."
297,A,"Why is my program sending 'Anonymous IP Packets' and how do I stop that? So we just got word today that one of our clients firewall is blocking our HTTP requests because ""The [software] is sending anonymous packets to our firewall (a Microsoft TMG firewall) so the firewall is dropping the packets as anonymous access is [not] allowed."" For our connection code we are using c++ with curl and we fallback to IEDownloadToFile if needed. I didn't write the original code nor am I really a network programmer so I came here for help. So my questions are: What are anonymous packets? What am I doing in curl that could cause anonymous packets? Where can I find more information about solving this problem? Thanks! What they mean is your app has to authenticate with the firewall. That link provides a wealth of information concerning the TMG product. Your client probably has this configuration: Require users to authenticate whenever they request Web access. Every Web session requires authentication. When using this method note the following: Anonymous Web access is disabled. Forefront TMG requests user credentials and validates them before it checks the request against the Firewall policy. If users fail to authenticate their access request is denied. This method is defined per network. Most non-interactive clients such as the Windows Update client cannot authenticate and are therefore denied access. So when the user opens their web browser and tries to access a web page they'll get a pop-up window asking for credentials because the firewall has intercepted their web request and sent its own authentication page. When the user authenticates the firewall passes web traffic. Your automated app does not authenticate with the firewall so the firewall drops packets and your traffic is classified as anonymous. Sorry I don't know the solution on how to make your application authenticate with the firewall. If your app goes to specific URLs the site operators could whitelist them. According to this page you should be getting error 407: proxy authentication required from curl. Try adding these options to the curl initialization but you still have the problem of asking the user for their network credentials interactively: CURLOPT_HTTPAUTH: add CURLAUTH_NTLM CURLOPT_PROXYAUTH: add CURLAUTH_NTLM set CURLOPT_FOLLOWLOCATION They are indeed using Microsoft TMG. So we will contact them and see what we can work out. Thank you for your help!  There is no such thing as an 'anonymous packet' in standard networking parlance. Your client's firewall is making up terms or there was a miscommunication somewhere along the line before the message got to you. Either way you're going to need to get clarification from your client or the firewall's vendor or documentation.  I agree with bdonlan. In the context of http requests ""anonymous packets"" is vague and ambiguous at best. Maybe they mean there is no referrer code? Or they require http-authentication? Or you need to establish a session key before being able to access the specific url you are requesting? You need actual technical details from your client."
298,A,"Catching exceptions caused by temporary network errors I have to download some URLs and it can happen that during the operations some exception may occur like java.net.NoRouteToHostException. I consider it a temporary error exception as the download can succeed if you retry it after some time. So I'd like to catch all those exceptions related to temporary errors and I've started with these: java.net.NoRouteToHostException java.net.SocketException Can you list other exceptions like these? java.net.SocketException is the parent of java.net.NoRouteToHostException so you could maybe catch only the parent. The others childs of SocketException are BindException ConnectException and PortUnreachableException. Maybe it's ok to catch only SocketException in your case... Added my comment as answer HttpRetryException is another. Maybe everything in java.net that does not extend SocketException? One quick (and very dirty) way could be try { .... } catch(Exception e) { if(e.getClass().getPackage().getName().equalsIgnoreCase(""java.net"")) { retry(); } }  java.net.SocketException is the parent of java.net.NoRouteToHostException so you could maybe catch only the parent. The others childs of SocketException are BindException ConnectException and PortUnreachableException. Maybe it's ok to catch only SocketExceptionin your case...  As @reef has already said java.net.NoRouteToHostException exntends java.net.SocketException so if you catch java.net.SocketException it is enough. But java.net.SocketException extends IOException and I think that this is what you really should to catch. All IO problems should throw this exception.  One pattern which is sometimes useful for dealing with transitory errors on retry-able operations is to define an error-callback interface and some classes that implement it; when an error occurs let the callback object know and let it decide whether an exception should be thrown or the action should be retried. Because the existing communications classes don't follow such a pattern it may be necessary to catch their exceptions and turn them into callbacks. Note that this approach allows much cleaner handling of many error-handling scenarios than would normal exception handling. Among other things it makes it easy to set policies for how many retries may be attempted how long they may take etc. and also provides a means of skipping future retries if something like a ""cancel"" request is received."
299,A,"select function in socket programming can anyone tell me the use and application of select function in socket programming in c? I think you should edit you question to be a bit more specific. Also if you put that many question marks at the end of your question people will tend not to take the question seriously. The select function allows you to implement a event driven design pattern when you have to deal with multiple event sources. Let's say you want to write a program that responds to events coming from several different event sources e.g. network (via sockets) user input (via stdin) other programs (via pipes) or any other event source that can be represented by a fd. You could start separate threads to handle each event source but you would have to manage the threads and deal with concurrency issues. The other option would be to use a mechanism where you can aggregate all the fds in a single entity (fdset) and then just call a function to wait on the fdset. The function would return whenever an event occurs on any of the fds. You could check which fd the event occurred on read that fd process the event and respond to it. After you have done that you would go back and sit in that wait function - till another event on some fd arrives. Select facility is such a mechanism. And the select function is the wait function. You can find the details on how to use it in any number of books and online resources. Right. `select()` is an event multiplexer. I like your explanation Ziffusion. I'm just starting out with select and had read a fair bit of documentation on the web and nothing comes close to this. Best explanation Thank you so much.  The select function allows you to check on several different sockets or pipes (or any file descriptors at all if you are not on windows) and do something based on whichever one is ready first. More specifically the arguments for the select function are split up into three groups: Reading: When any of the file descriptors in this category are ready for reading select will return them to you. Writing: When any of the file descriptors in this category are ready for writing select will return them to you. Exceptional: When any of the file descriptors in this category have an exceptional case -- that is they close uncleanly a connection breaks or they have some other error -- select will return them to you. The power of select is that individual file/socket/pipe functions are often blocking. Select allows you to monitor the activity of several different file descriptors without having to dedicate a thread of your program to each function call. In order for you to get a more specific answer you will probably have to mention what language you are programming in. I have tried to give as general an answer as possible on the conceptual level.  More detail would be good but I think you're referring to Java NIO's Selector.select() method. The simple answer to your question is that select() (in this context) will wait for a channel (that is one of the network connections that's being managed by this Selector object) has data available to read. When you have many connections open at once many/most will be dormant at any given moment. This method/class allows you to manage many connections without having a separate for each connection blocking on that connection. You can block with one thread for multiple connections and simply receive back whichever connection(s) are ""ready"" at the moment. Here's a great little tutorial that should make things clear: http://rox-xmlrpc.sourceforge.net/niotut/ Java's NIO in C? And it also selects writable channels ... And in C channels with errors.  This is what documentation is for select(2) - Linux man page select Function - Winsock Functions  select() is the low-tech way of polling sockets for new data to read or for an open TCP window to write. Unless there's some compelling reason not to you're probably better off using poll() or epoll_wait() if your platform has it for better performance."
300,A,"Multiple HTTP requests vs a single TCP connection on iOS I am developing an iPhone app that uses a web based API that I control. Would it be faster or more efficient to connect to a constantly open TCP port and make requests via the TCP API or make a new HTTP request for all the data that I want to fetch? I am imagining that the different would be negligible but I could be wrong. New data is fetched pretty much every time a new view is loaded so requests can happen fairly frequently. I think the difference would be minimal. It is also worth noting that NSURLConnection supports the HTTP Keep Alive connections by default so you could go straight HTTP and make sure your server is allowing the client to keep the connection alive. Ah that is reassuring. Thanks!  Having a ""real"" connection (""just like telnet does"") is tremendously better than fooling with HTTP. A simple example if you imagine oh one of those big famous multiuser games of course there's a ""real"" connection it's not just http calls. I hope this general answer helps you in some way! (Of course it's likely you have to actually write your own server and so on .. tremendously more work.) Well I wrote the web API and I put in TCP access while I was writing the web access and I haven't written anything on the iOS networking side yet. I guess I will just go with the TCP method."
301,A,"C# Game Network Library I am developing an online strategy game using .Net v2. Although the game is primarily strategic it does have some tactical elements that require reasonable network performance. I plan to use TCP packets for strategic data and UDP packets for tactical data. {EDIT} I forgot to mention that I am leaning away from WCF and .NET 3+ for a couple of reasons. First because I want to keep my download small and most of my customers already have .NET 2.0. Second because I would like to have the option of porting to Mac and Linux and am unsure of WCF availability in Mono. {/EDIT} I am looking for network library recommendations. I have found a few options such as GarageGames' Torque Network Library (C++) RakNet (C++) and the lidgren network library (C#): http://www.opentnl.org/ http://www.jenkinssoftware.com/ http://code.google.com/p/lidgren-network/ Does anyone have real-world experience with these or other libraries? I just stumbled on RakNetDotNet: http://code.google.com/p/raknetdotnet/ This might be what I'm looking for... http://code.google.com/p/lidgren-network-gen3/ Lidgren.Network is a networking library for .net framework which uses a single udp socket to deliver a simple API for connecting a client to a server reading and sending messages.  If you're new to game development I'd recommend XNA- it's easy to program with. The advantage of Torque however is it has asset creation tools which can also be invaluable. For a higher end game or FPS the Source engine is great.  Microsoft's own .NET based XNA allows you to create networked games on Windows and XBox 360. According to [wiki](http://en.wikipedia.org/wiki/Microsoft_XNA#License_Agreement): *""The Microsoft XNA Framework 2.0 EULA specifically prohibits the distribution of commercial networked games that connect to Xbox Live and/or Games for Windows Live in the absence of a specific agreement signed by both the developer and Microsoft.""*  I'd also like to note that ""Games for Windows"" which XNA uses on windows with its Live! networking APIs is now free ... which means that if you write an XNA game that uses the networking features your users do not have to have a gold membership :-) http://www.engadget.com/2008/07/22/games-for-windows-live-now-free/ According to [wiki](http://en.wikipedia.org/wiki/Microsoft_XNA#License_Agreement): *""The Microsoft XNA Framework 2.0 EULA specifically prohibits the distribution of commercial networked games that connect to Xbox Live and/or Games for Windows Live in the absence of a specific agreement signed by both the developer and Microsoft.""* - Is this no longer true? unfortunately it is still true to my knowledge :-( I really wish they'd clear up the story on this  Why limit yourself to .NET 2.0. .NET 3.0 (or 3.5) contains WCF and is a solid performant communications subsystem with good security. .NET 3.0 is just .NET 2.0 with additional libraries (WCF WF WPF).  An ancient question but I'll put this out there for anyone else who stumbles across this. We're using OpenTNL for our game Bitfighter and I am consistently surprised at how well it works. And it's free if you can live with GPL.  Although this question is rather old a somewhat higher level system developed specifically for games is APlay - among the supported platforms is also C#. There are evaluation versions and it is free for personal use. You define your game objects in an UML alike fashion and an online code generator creates assemblies containing your game objects. Sending state updates is then as simple as calling setter methods. Not the right thing if you want to cope with sockets by yourself. Please also note that I am a developer of APlay so this is a biased answer.  You could take a look at Entanglar ( http://entanglar.dunnchurchill.com ) if you are looking for something higher level. Entanglar provides full entity lifecycle and sync."
302,A,"How to send large data using C# UdpClient? I'm trying to send a large amount of data (more than 50 MB) using C# UdpClient. So at first I split the data into 65507 byte blocks and send them in a loop. for(int i = 0; i < packetCount; i++) myUdpClient.Send(blocks[i] block[i].Length remoteEndPoint); My problem is that only the first packets can be received. During sending the firsr packet the networkload increases rapidly to 100% and then the other packets cannot be received. I want to get as much data throughput as possible. I'm sorry for my English! Thanks for your help in advance. What happens if you do not split the data? Use Microsoft Network Monitor 3.3 to view data packets and confirm arrival. If I don't split the data the udpClient.Send call will fail because of the maximum udp-packet-size. I don't know about .Net implementation specifically it might be buffering your data but UDP datagram is normally limited by the link MTU which is 1500 on normal ethernet (subtract 20 bytes for IP header and 8 bytes of UDP header.) UDP is explicitly allowed to drop and reorder the datagrams and there's no flow control as in TCP. Exceeding the socket send buffer on the sender side will have the network stack ignore following send attempts until the buffer space is available again (you need to check the return value of the send() for that.) Edit: I would strongly recommend going with TCP for large file transfers. TCP gives you sequencing (you don't have to keep track of dropped and re-ordered packets.) It has advanced flow control (so fast sender does not overwhelm a slow receiver.) It also does Path MTU discovery (i.e. finds out optimal data packetization and avoids IP fragmentation.) Otherwise you would have to re-implement most of these features yourself. so the size for the data to use is 1472 bytes? the packet size of 65507 works fine in my LAN the send() call always return 65507 So what the receiver gets for the first 'packet'? 65507 bytes? yes he receive the whole 65507 byte at once Do you see IP fragments flowing on the wire? That's what that CPU spike is - fragmenting the 64K datagram into MTU-size packets. I'm sorry I can't understand. Please post the last message in more simple English! Thanks! The TCP/IP stack has to fragment the 64KB of data you give it. That shows as CPU/network load increase. Why do you think the data must be fragmented... udp support packets up to 64KB But ethernet doesn't. The MTU is 1500 bytes (unless you use jumbo frames on a gigabit link which still gives you around 9000) so the sending machine has to split the 64KB chunk into smaller pieces. You can easily confirm that with Wireshark or whatever Microsoft has for network sniffer. That's maximum that fits into single IP packet. Things like PPPoE or other tunneling protocols might shave off more bytes. Path MTU is the minimum of all the links the packet has to traverse. This is why UDP protocols like DNS for example set upper packet size limit (DNS switches to TCP after 512 bytes.)  For all those people saying to use TCP....are foolishly wrong. Although TCP is reliable and the window maintained by the kernel it's fairly ""set and forget"" protocol but when it comes to the guy wanting to use 100% of his throughput TCP will not do (it throttles too hard and the wait for an ACK automatically puts at least 50% trashed because of the RTT). To the original question you are sending UDP packets nonstop in that for-loop the window fills up and then any new data is dropped immediately and doesn't even attempt to go on the line. You are also splitting your data too large. I would recommend building your own throttle mechanism that starts off with 2k segments per second and slowly ramps up. Each ""segment"" contains a SEQ (sequence identifier for acknowledgements or ACK) and OFF (offset inside the file for this data set). As the data is being tagged let the server keep track of these tags. When the other side gets them it stores the SEQ numbers in an ACK list and any missing SEQ numbers are placed into a NACK timer list when the timer runs out (if they haven't been received) it moves to a NACK list. The receiver should send 5 or so ACKs from the ACK list along with up to 5 NACKs in a single transmission every couple seconds or so. If the sender receives these messages and there are any NACKs it should immediately throttle down and resend the missing fragment before continuing. The data that is ACKed can be freed from memory. Good luck! Disable Nagle's if you want high speed LAN communication. TCP is still often faster as most NICs provide hardware acceleration for segmentation.  Reliably - no you won't do it with UDP. As far as I understand this makes sense for sending to multiple computers at a time (broadcasting). In this case establish a TCP connection with each of them split the data into blocks give each block an ID send list of IDs to each computer with TCP connection broadcast data with UDP inform clients (via TCP) that data transmission is over than clients should ask to resend the dropped packets  I hate to say it but you need to sleep the thread. You are overloading your throughput. UDP is not very good for lossless data transfer. UDP is for when you don't mind dropping some packets. In addition to dropping some packets UDP doesn't even guarantee that they will be received in the original order which might also be a problem. i usually have up to 10 udp-packet depending on each other... so if one packets gets lost i drop the other 9 packets as well it isn't a problem if packets will be lost. This is the reason why I don't wanted to use tcp... I also thought with UDP I get better network throughput."
303,A,How do online port checkers work? For example http://www.utorrent.com/testport?port=12345 How does this work? Can the server side script attempt to open a socket? The server side script will try to open a connection on the specified port to the originating IP. If there is no response (the attempt will timeout) this would be an indication that the port is not open. @Downvoter - care to comment?  The server can try as @Oded said. But that doesn't ensure the receiver will respond.  There are many ways of accomplishing this through server-side scripting. As @Oded mentioned most server-side handlers are capable of initiating socket connections on arbitrary ports and most of those even have dedicated port-scanning packages/libraries (PHP has one in the PEAR repository Python's 'socket' module makes this type of tasks a breeze etc...) Keep in mind that on shared host platforms socket connections are typically disabled for security purposes. Another way that is also very easy to accomplish is to use a command-line port-scanner such as nmap from your server-side script. i.e in PHP you would do echo `nmap -p $port $ip`  Typically something like this happens: The URL request contains instructions about which port to access. The headers that your browser sends include information about where the request is originating from. Before responding to the request the server tries to open a port and checks if this is successful. It waits a while before timing out. The webpage is rendered dynamically based on the results of this test. The response is returned to you containing the results. Sometimes steps (2) and (3) will be replaced with an AJAX callback which allows the response to return sooner.
304,A,Framework/approach to use for 'alerter' type applet? A client has a lashed-together 'alerter' system based around batch files and 'NET SEND' commands. He edits the batch files (adding/removing users) as necessary. His requirements are getting a bit more complex now; he wants users to be able to drop in and out of the notifications during the day. I got to thinking about some kind of tray icon applet that registers with a server popping up 'toast' type alerts as necessary and allowing the user to opt-out of seeing notifications on an adhoc basis (ie don't bother me for the next hour don't bother me until I say otherwise start bothering me again from now on etc). The kind of information in each alert is simple; 'A new contract from store 1234 has arrived' 'Store 1234 have amended a contract' 'Store 1234 have deleted a contract' etc. Just fairly short basic text strings really. It's not particularly busy - there will be in the order of 5-20 alerts an hour the point being that in this new app they can be ignored (the Net Send approach he has now requires people to click the OK button of the message box whether or not they're interested in the alert at that point in time). Having said that there is some time sensitivity involved - looking every 5 minutes to see if there had been an alert would be no good though perhaps once a minute might be acceptable. The number of users interested in the alerts will vary during the day (and their workload) but it's between about 2 and a dozen - we're certainly not talking hundreds of users on the system. I've done Delphi socket apps where clients connect to a server on demand and exchange information but I've never done anything where the clients are 'listening-to' or polling a server. I don't know whether you would tackle this with Named Pipes TCP/IP or some kind of Windows Network tool. (The clients and the server are all Windows 2003/XP on the same internal network). I'm not really sure where to start. Is this the kind of thing that the RemObjects API would help with? Can anyone recommend a useful tutorial or example that I might reverse engineer for ideas? :-) This would be done in Delphi 2007 (or possibly 2009 with a bit of luck). This sounds to me like something where most of the work could be accomplished with an ordinary instant messenger. No custom software required. Don't want notifications for a while? Just log out or go to invisible mode. Thanks Rob. The events themselves are the result of files arriving on an FTP server (the contents of the files are processed by my server applet and it generates various events accordingly). If there are APIs for sending messages to instant messengers then my server could be modified to send out IMs. Well worth an investigation - though I don't know how the client will feel about me telling his staff to run IM! Thanks for the suggestion I'll have a look at this. Sound like you already have TCP/IP code lying around that you can refactor so that may be the best way. Another option would be to use a database for connection and message serving. I've used this for alerting in manufacturing environments where management wants instant info about production status.  Do I only get msgs if my computer is running? Or do I get a backlog on startup? If you don't have to memorize missed msgs a basic socket server-client demo is already close to what you want. Adding a simple auto-update mechanism could make deployment easier. Thanks Marco. Yes you only get messages that arrive while you're interested in getting them. No backlogs no reloading on startup etc. You're right I guess I could just have each client make a socket connection and keep it open... :-)  I understand all the recent events and talk about Web2.0 would give a feeling that there are better options but (on a company's LAN) there still is nothing better (in my humble opinion) than plain networking like UDP or TCP sockets. For a similar project to what you described I've used a straight-forward TCP server component on the server (first in a little-UI app for debugging later in an NT-service) and a TCP client with a sys-tray icon. The client keeps a connection open to the server (or to one of a list of servers to provide some load-balancing and fail-over). UDP doesn't handle connections but might be a better suit for short messages like you describe. You might still benefit from a central server the clients report to when starting or stopping. In both cases the server when receiving a message to broadcast would forward it to all the clients it knows about either by the open connections with TCP or by the client-addresses it got start-reports from over UDP. Okay cool. So really when the clients start-up and shut down they tell the server and the server sends a message to everyone on it's 'notify me' list when anything of note happens. The clients could either hop on/off the list as the user decides they don't want to be bothered or the client app itself could just not show messages to the user if the user is in 'don't disturb me' mode. I think I thought this would be harder than it probably is. Thanks for the suggestion! :-)  I would probably use a file based approach rather than a client server with sockets. Just put the messages into text files on a file server and read them from the clients. Nothing beats such a solution in simplicity. Of course it does not have the buzzword capability of a dotNET services consuming allways on WEB 3.14 community app written with smaragd on rails. ;-) That's interesting - I'd never thought of polling a file. I suppose that there's not that much of a hit on things if I've just got 20 or so machines watching a file on a server say every minute. Interesting idea!
305,A,What should the destination mac address be set to when sending packet outside of your local network? If I use arp and arping on machines in my local network I get the mac addresses from them. In the same way I can construct and send a ARP request and collect the response to these machines. This is used since I build raw packets completely from scratchy (to allow spoofing of every possible field including mac addresses if needed). But when I try arping or arp on external ip's and hosts such as google.com it doesn't get any reply. What should the destination mac address be set to when sending packets to targets outside my local network? I guess the router since that's what passes it on... am I correct? Is there a quick way in ANSI C to collect the mac address of the router in use by the computer? Or at least the IP so I can send a ARP request to it. Thanx in advance MAC operations are limited to machines directly connected within your subnet. So you should use the router's MAC address for packets intended for hosts outside your subnet. There are numerous ways to obtain the router's IP address. You can parse the configuration files on your local host if the interface is statically configured. You can see if your compute platform has an API that lets you access the interface configuration information directly. This would work in both static and dhcp cases. You can write socket code to send an ICMP message to an outside address then parse the incoming responses. They will be from the router. The stack will in this case find the router for you. Of course it ain't necessarily so that 1) the router for outgoing packets is the same as that for incoming packets or 2) the router doesn't change while your program is running or 3) the same router should be used for all destinations. Ideally you would consult the routing table each time. @caf: And is there a good and fast way to do that programaticly using ANSI C? @inquam: No - as pointed out in another comment ANSI C has no networking functions at all. You will have to use operating-system specific calls to read the routing table. The ICMP approach seems like a smart way. Feels like that to should work for both static and dhcp cases to right? Do that when the application starts up and store the gateway ip and arp it for the mac address. It will work for both static and DHCP. You won't even have to ARP it because the packet will forwarded from the router and its source MAC address will be what you are looking for. So very true... That was a very simple solution. Thanx!  It should be set to the gateway (assuming ethernet on that link...). Yes it's a normal ethernet network. I know the gateway address but if I were to install this application on another computer it would be nice not to have to manually enter the address of the gateway. Is there a quick function in ANSI C to retrieve the address of the default gateway? ANSI C has *no* functions dealing with networking. You'll have to use something OS specific. So what OS are you on ?
306,A,"Perl: Checking for the Existence of socket options This is a continuation of my previous question: In Perl how can I check for the existence of Socket options without generating warnings? If I run the following code I get the result I expect: #!/usr/bin/perl -w use strict; use diagnostics; use Socket qw(:all); my %opts; if ( defined( eval { SO_REUSEPORT } ) ) { $opts{'SO_REUSEPORT'} = {opt_level =>SOL_SOCKETopt_name=>SO_REUSEPORTopt_print=>\&sock_str_flag}; } else { print ""SO_REUSEPORT undefined\n""; $opts{'SO_REUSEPORT'} = {opt_level =>0opt_name=>0opt_print=>undef}; } =head # IPV6 options if ( defined( eval { IPV6_DONTFRAG } ) ) { $opts{'IPV6_DONTFRAG'} = {opt_level =>IPPROTO_IPV6opt_name=>IPV6_DONTFRAGopt_print=>\&sock_str_flag}; } else { print ""IPV6_DONTFRAG undefined\n""; $opts{'IPV6_DONTFRAG'} = {opt_level =>0opt_name=>0opt_print=>undef}; } =cut It outputs: anon@perl$ ./test.pl SO_REUSEPORT undefined But if I uncomment the block for IPV6_DONTFRAG I get: Bareword ""IPV6_DONTFRAG"" not allowed while ""strict subs"" in use at ./test.pl line 17. Bareword ""IPV6_DONTFRAG"" not allowed while ""strict subs"" in use at ./test.pl line 17. Why is one undefined bareword causing it to barf and the other not? And how can the error be propagating out of the eval { } block? Edit Apparently SO_REUSEPORT is exported by Socket.pm in some manner as it's in the @EXPORT array. So apparently it's defined but using it throws an error which the eval catches. That still doesn't explain what's going on with IPV6_DONTFRAG. I suppose I would need to define it myself and then just call getsockopt to check if it's supported... Why did someone mark this down? With regards to the IPV6_DONTFRAG bareword issue it looks like Perl checks for barewords at compile time not run time as documented here. Eval is a construct to swallow runtime errors so it won't help you here. It's like trying to handle a syntax error in C++ by sticking the offending code in a try/catch block.  I recommend writing it this way: if ( defined( &IPV6_DONTFRAG ) ) { $opts{'IPV6_DONTFRAG'} = {opt_level =>IPPROTO_IPV6opt_name=>&IPV6_DONTFRAGopt_print=>\&sock_str_flag}; } else { print ""IPV6_DONTFRAG undefined\n""; $opts{'IPV6_DONTFRAG'} = {opt_level =>0opt_name=>0opt_print=>undef}; } Note the added ampersand in the value for opt_name which evades constraints due to strict 'subs'. The documentation for defined explains: You may also use defined(&func) to check whether subroutine &func has ever been defined. The return value is unaffected by any forward declarations of &func. Note that a subroutine which is not defined may still be callable: its package may have an AUTOLOAD method that makes it spring into existence the first time that it is called—see perlsub. For example with SO_BROADCAST if (defined &SO_BROADCAST) { print ""SO_BROADCAST = "" SO_BROADCAST ""\n""; } the output on my machine is SO_BROADCAST = 6 So Socket.pm defines these as subroutines which return the correct constant value to pass to functions like `socket` and `getsockopt`? @Robert Yes that's the general idea. See http://perldoc.perl.org/perlsub.html#Constant-Functions I tried it and it doesn't work with IPV6_DONTFRAG. I still get the bareword / strict subs error. @Robert Did you use &IPV6_DONTFRAG (note the leading ampersand) as I did in my example? @gbacon: Yes I did. With `&SO_REUSEPORT` it worked fine but with `&IPV6_DONTFRAG` I still get the bareword / strict subs error. @Robert In *both* defined(&IPV6...) and opt_name=>&IPV6...? @gbacon: Take a look at this: http://perldoc.perl.org/constant.html#TECHNICAL-NOTES @gbacon: My bad. Forgot to put the `&` in both places. It works."
307,A,"following code showing segmentation fault why the following code shows segmentation fault? int CreateRawSocket(int protocol_to_sniff) { int rawsock; if((rawsock = socket(PF_PACKET SOCK_RAW htons(protocol_to_sniff)))== -1) { perror(""Error creating raw socket: ""); exit(-1); } return rawsock; } int BindRawSocketToInterface(char *device int rawsock int protocol) { struct sockaddr_ll sll; struct ifreq ifr; bzero(&sll sizeof(sll)); bzero(&ifrsizeof(ifr)); /* First Get the Interface Index */ char *t=(char*)ifr.ifr_name; strncpy(t device 1024); if((ioctl(rawsock SIOCGIFINDEX &ifr)) == -1) { printf(""Error getting Interface index !\n""); exit(-1); } /* Bind our raw socket to this interface */ sll.sll_family = AF_PACKET; sll.sll_ifindex = ifr.ifr_ifindex; sll.sll_protocol = htons(protocol); if((bind(rawsock (struct sockaddr *)&sll sizeof(sll)))== -1) { perror(""Error binding raw socket to interface\n""); exit(-1); } return 1; } void PrintPacketInHex(unsigned char *packet int len) { unsigned char *p = packet; printf(""\n\n---------Packet---Starts----\n\n""); while(len--) { printf(""%.2x "" *p); p++; } printf(""\n\n--------Packet---Ends-----\n\n""); } main(int argc char **argv) { int raw; unsigned char packet_buffer[2048]; int len; int packets_to_sniff; struct sockaddr_ll packet_info; int packet_info_size = sizeof(packet_info); /* create the raw socket */ raw = CreateRawSocket(ETH_P_IP); /* Bind socket to interface */ BindRawSocketToInterface(argv[1] raw ETH_P_IP); /* Get number of packets to sniff from user */ packets_to_sniff = atoi(argv[2]); /* Start Sniffing and print Hex of every packet */ while(packets_to_sniff--) { if((len = recvfrom(raw packet_buffer 2048 0 (struct sockaddr*)&packet_info &packet_info_size)) == -1) { perror(""Recv from returned -1: ""); exit(-1); } else { /* Packet has been received successfully !! */ PrintPacketInHex(packet_buffer len); } } return 0; } Where does it crash? On what address does it crash? What are the sizes of the buffers? I refuse to even look at this. Do some debugging and narrow it down. You're obviously not new to programming or you wouldn't be writing programs of this complexity. Once you've narrowed it down to the specific module and line that crashes post back. Argh my eyes. Too much whitespace!! You could always run it in valgrind too. There is a memory violation when you do not give enough command line arguments as you do not check argc.  The crash is caused by this line in your routine BindRawSocketToInterface: strncpy(t device 1024); Here you've asked strncpy to write 1024 bytes into char *t. Note that strncpy pads the destination string with the specified number of null bytes see man strncpy). But t points to an array which isn't nearly large enough namely ifr.ifr_name[IFNAMSIZ]. On my linux system IFNAMSIZ is only 16. So strncpy overflows and trashes memory that it shouldn't be touching. Changing the strncpy parameter to match the correct size of the array as follows fixes the crash: strncpy(t device IFNAMSIZ);"
308,A,"What is the best script language for n/w programming + web development I'm interested in developing web application and networking application. For that what is the best script language to learn. Which one is effective for this two. So for i don't know even a single syntax of any scripting language. Which is the best script for understanding maintainable effective and simple (may not). Please don't say what are all you know. Please tell me the best Hi Mohanavel you may get downvoted for this question because it is very high-level / subjective / might require extended debate / discussion. Most would probably recommend that you jump in and start coding. For example try using Python. Most would agree that Python is highly understandable maintainable effective simple and elegant even. Most *Python users* would agree that - and that is of course the problem with questions like this: people tend to like what they know. Assembly is the best! Thanks for guidance. I'll explore two/three and i'll find the right script for right place Mohanavel the question as it stands can not be answered until you define what you mean by ""best"". That aside when it comes to programming languages... Just pick one learn it find out what's crap in it then move onto another language. As a programmer you need to learn many languages anyway.  Ruby has the most simple syntax and with Ruby On Rails (which is a framework) you can get a web application working in a few minutes.  There are no bests. It is possible in all the scripting languages (Perl Python Ruby). Check the documentation at the respective sites and select for yourself the one that suits your way of thinking/coding. @Assembler epic fail for using l337speak but +1 for making me roflcoptor! ""no bests"".. apart from PHP! R0X0RZ!  Use python for your web applications (Django framework will do the trick for your) Then for socket programming use Python again (Twisted python package will be the library to use for networking functionality) Best of luck Why this answer is down voted. Answering will give clear idea.  You'll never find a consensus as to which language is best ever until the end of time. That being said Python seems like a good fit for what you're trying to do. I dispute that... When the end of time finally comes around we STILL won't have a consensus! We may however have a consensus once hell has frozen over though..."
309,A,"programming ftp: how to abort file transfer? i am doing small ftp client for recieving some big files from ftp. I have read in RFC that ABOR command is very problematic for servers. Almost all servers i see just continue to send data even after ABOR sent through control connection. Closing the data transfer can result (in 70% of tests) in closing control connection also. The server just sends FIN packet after my pushed ABOR packet. What is the best good method to stop recieving at some byte and not lose the control connection? FlashFXP doing this ok on all types of connection delays and servers. While investigating tcp traffic i found standard ftp rfc flow. But in my case still no success to abort transfer using this technique: 1) shutdown(passive_socket SD_BOTH) 2) closesocket(passive_socket); 3) send(control_socket""ABOR\r\n"") 4) recv(control_socket) - stalled here Thank you The ""ABOR\r\n"" command should be sent as out-of-band data. In case of send() - send(control_socket ""ABOR\r\n"" 6 MSG_OOB); Some time after you recv() code 426 Transfer aborted. Data connection closed. The following link is more helpful if you can't achieve success aborint transfer: http://www.developer.nokia.com/Community/Discussion/showthread.php?134079-Telnet-quot-Interrupt-Process-quot-(IP)-signal-amp-Telnet-quot-Synch-quot I've been reading about OOB data in [another question on SO](http://stackoverflow.com/questions/589928/socket-programming-how-do-i-handle-out-of-band-data/591104#591104) and they also mention this use case as a typical use of OOB data. However I'm confused as to why this is necessary (or even recommended) given that it seems to rely on a specific server architecture. If the server is properly listening on the control connection and processes `ABOR`command it can just shutdown the data connection socket or stop sending can't it? @André out of band data is descriebed : ""Urgent data"" notifies the receiving connection that the separate stream is more important than the main stream. Therefore it must first check the separate stream in order to process the main stream normally. But in real world it means (as i think) just sending data TCP packet with PSH set no matter of window size or other things. I confused too because this is only related to FTP protocol i found first. I understand how it works. However if you respect the control connection state from the client program you shouldn't have *anything* in the control connection's input buffer on the server side when `ABOR` reaches the server. If using OOB has an impact on the speed of interpretation of this command then I would understand that the data connection stops sending data faster. However using OOB doesn't explain how it will cause the `ABOR` command to be interpreted (VS not at all as in OP's case) on *any* given FTP server implementation. Maybe this is specific to the server implementation you are connecting to. Can you mention which FTP server you're using? I am using now 'FreeBSD vm1 7.4-PRERELEASE FreeBSD' /usr/libexec/ftpd. standard ftp server for unix/linux os. But i did not made other tests well so maybe you're right.The code i took about oob data is from /usr/src/usr.sbin/lukemftp (symlink from standard ftp client on unix/linux).  You should not lose your control connection when closing the data transfer connection they are two separate sockets. Maybe check your code to see why the control connection is being closed. I checked several times under freebsd the standalone ftpd server just sends FIN packet then when my client want to sends something to control socket the server sends RST. Another option is to have your code auto-reconnect on these disconnects. I have noticed that FileZilla does this. That is i am doing now but this is really takes long time. the code interfere with windows drivers and i can't do such delays. I am using this code to send ABOR command- sprintf(tb ""%c%c%c%c%cABOR\r\n"" 0xff 0xf4 0xff 0xfd 0x06); As descriebed in RFC ABOR command may require special processing (the codes ctrl-c) Well the problem is that when you have to deal with other servers which you can't control you kind of have to conform your code to their behavior. Maybe you should look into something other than FTP if that's an option. Yes i have planes about that but for yet that is my dilemma how to do this with FTP. If the goal is performance I would highly suggest not using FTP."
310,A,"why normal call to if_freenameindex would double free if_nameindex? I am learning socket programming under Linuxso I make a sample program to list all the network interfacehere is the code /* print the name of interface */ #include <sys/socket.h> #include <net/if.h> #include <stdio.h> int main(void) { struct if_nameindex *pif; pif = if_nameindex(); while (pif->if_index) { printf(""name: %s \t index: %d\n"" pif->if_name pif->if_index); pif++; } if_freenameindex(pif); printf(""after the first if_freenameindex call\n""); return 0; } run it and it returns  name: lo index: 1 name: eth0 index: 2 name: eth1 index: 3 name: eth2 index: 4 *** glibc detected *** ./if: double free or corruption (out): 0x0983b420 *** ======= Backtrace: ========= /lib/i686/cmov/libc.so.6[0xb7edb624] /lib/i686/cmov/libc.so.6(cfree+0x96)[0xb7edd826] /lib/i686/cmov/libc.so.6(if_freenameindex+0x40)[0xb7f6f9e0] ./if[0x80484b6] /lib/i686/cmov/libc.so.6(__libc_start_main+0xe5)[0xb7e83455] ./if[0x80483d1] ======= Memory map: ======== 08048000-08049000 r-xp 00000000 03:01 51169 /home/jcyang/src/net/gnu/if 08049000-0804a000 rw-p 00000000 03:01 51169 /home/jcyang/src/net/gnu/if 0983b000-0985c000 rw-p 0983b000 00:00 0 [heap] b7d00000-b7d21000 rw-p b7d00000 00:00 0 b7d21000-b7e00000 ---p b7d21000 00:00 0 b7e54000-b7e60000 r-xp 00000000 03:01 73587 /lib/libgcc_s.so.1 b7e60000-b7e61000 rw-p 0000b000 03:01 73587 /lib/libgcc_s.so.1 b7e6c000-b7e6d000 rw-p b7e6c000 00:00 0 b7e6d000-b7fc2000 r-xp 00000000 03:01 82774 /lib/i686/cmov/libc-2.7.so b7fc2000-b7fc3000 r--p 00155000 03:01 82774 /lib/i686/cmov/libc-2.7.so b7fc3000-b7fc5000 rw-p 00156000 03:01 82774 /lib/i686/cmov/libc-2.7.so b7fc5000-b7fc9000 rw-p b7fc5000 00:00 0 b7fd3000-b7fd5000 rw-p b7fd3000 00:00 0 b7fd5000-b7fd6000 r-xp b7fd5000 00:00 0 [vdso] b7fd6000-b7ff0000 r-xp 00000000 03:01 73586 /lib/ld-2.7.so b7ff0000-b7ff2000 rw-p 0001a000 03:01 73586 /lib/ld-2.7.so bffdc000-bfff1000 rw-p bffeb000 00:00 0 [stack] Aborted Acoording to the GNU C Library Reference Manaulwe should use if_freenameindex to free the earily returned if_nameindex.So whats wrong? thanks. +1 I made this exact same mistake. You should call if_freenameindex() on first pif not the final one. for example: struct if_nameindex *pif; struct if_nameindex *head; head = pif = if_nameindex(); while (pif->if_index) { printf(""name: %s \t index: %d\n"" pif->if_name pif->if_index); pif++; } if_freenameindex(head); ...."
311,A,Distributed message passing in D? I really like the message passing primitives that D implements. I have only seen examples of message passing within a program though. Is there support for distributing the messages over e.g. a network? Seems that this has been considered. From the Phobos documentation (found it through Jonathan M Davis answer) This is a low-level messaging API upon which more structured or restrictive APIs may be built. The general idea is that every messageable entity is represented by a common handle type (called a Cid in this implementation) which allows messages to be sent to in-process threads on-host processes and foreign-host processes using the same interface. This is an important aspect of scalability because it allows the components of a program to be spread across available resources with few to no changes to the actual implementation. Right now only in-process threads are supported and referenced by a more specialized handle called a Tid. It is effectively a subclass of Cid with additional features specific to in-process messaging.  The message passing functions are in std.concurrency which only deals with threads. So the type of message passing used to pass messages between threads is for threads only. There is no RMI or anything like that in Phobos. That's not to say that we'll never get something like that in Phobos (stuff is being added to Phobos all the time) but it doesn't exist right now. There is however the std.socket module which deals with talking to sockets which is obviously network-related. I haven't used it myself but it looks like it sends and receives void[]. So it's not as nice as sending immutable objects around like you do with std.concurrency but it does allow you to do network communication via sockets and presumably in a much nicer manner than if you were using C calls directly. That's great. If this works smoothly it really is a potential killer feature. I wonder how serialization will work... Currently std.concurrency only deals with threads but my understanding is that when Sean gets the time he will be implementing cross processing/network sharing. Threading was used to get the structure down and Sean just hasn't had time to finish it.
312,A,"Find through which network device user is connected to internet Using the code below i will get all network interfaces wich are enabled and functional on the machine. Private netIntrfc As NetworkInterface() = NetworkInterface.GetAllNetworkInterfaces() For i As Integer = 0 To netIntrfc.Length - 1 If netIntrfc(i).OperationalStatus = OperationalStatus.Up Then netDevicesList.Items.Add(netIntrfc(i).Name.ToString) End If Next But my problem is how to get the default one the one(ethernet adapter) through wich user is connected to internet? I need to change some settings of default(through wich user is connected to internet) adapter. settings i change through registry so i could sample add same settings for each network interface but that could cause problems and makes no point then :D can anyone help? Thank you! ;) EDITED: for now i have done like code below so if this can help someone other... :) but if someone have batter solution or more reliable then send please Dim u As UdpClient = New UdpClient(System.Net.Dns.GetHostName 1) Dim localAddr As IPAddress = CType(u.Client.LocalEndPoint IPEndPoint).Address Private netIntrfc As NetworkInterface() = NetworkInterface.GetAllNetworkInterfaces() For i As Integer = 0 To netIntrfc.Length - 1 If netIntrfc(i).OperationalStatus = OperationalStatus.Up Then For Each uni As NetworkInformation.UnicastIPAddressInformation In ipProps.UnicastAddresses If uni.Address.ToString = localAddr.ToString Then netDevicesList.Items.Add(""DEFAULT: "" & netIntrfc(i).Name.ToString) DEFDEVID = netIntrfc(i).Id.ToString End If Next netDevicesList.Items.Add(netIntrfc(i).Name.ToString) End If Next Thanks Thomas-Li and this post Will this give you some hints? http://stackoverflow.com/questions/359596/identifying-active-network-interface-in-net yep that helped me :) hope it will work ok i dont have much resources to test this out but as for me on my pc with 3 ethernet cards and one wireless it works ok :) thank you. you can post as answer so i accept you ;) Will this give you some hints? Identifying active network interface in .NET  i ported your code to c# i hope you don' t mind  static void Main(string[] args) { UdpClient u = new UdpClient(System.Net.Dns.GetHostName() 1); IPAddress localAddr = (u.Client.LocalEndPoint as IPEndPoint).Address; NetworkInterface[] netIntrfc = NetworkInterface.GetAllNetworkInterfaces(); for (int i = 0; i < netIntrfc.Length - 1; i++) { if (netIntrfc[i].OperationalStatus == OperationalStatus.Up) { IPInterfaceProperties ipProps = netIntrfc[i].GetIPProperties(); foreach (UnicastIPAddressInformation uni in ipProps.UnicastAddresses) { if (uni.Address.ToString() == localAddr.ToString()) { Console.WriteLine(""DEFAULT: "" + netIntrfc[i].Name.ToString()); Console.WriteLine(netIntrfc[i].Id.ToString()); } } } } }"
313,A,"Running Network Application on port Server side give me error how can i solve it? if i run Server App. Exception occurs: on Dinle.Start() System.Net.SocketException - Only one usage of each socket address (protocol/network address/port) is normally permitted How can i solve this error? Server.cs using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Linq; using System.Text; using System.Windows.Forms; using System.IO; using System.Net; using System.Net.Sockets; using System.Threading; namespace Server { public partial class Server : Form { Thread kanal; public Server() { InitializeComponent(); try { kanal = new Thread(new ThreadStart(Dinle)); kanal.Start(); kanal.Priority = ThreadPriority.Normal; this.Text = ""Kanla Çalıştı""; } catch (Exception ex) { this.Text = ""kanal çalışmadı""; MessageBox.Show(""hata:"" + ex.ToString()); kanal.Abort(); throw; } } private void Server_Load(object sender EventArgs e) { Dinle(); } private void btn_Listen_Click(object sender EventArgs e) { Dinle(); } void Dinle() { // IPAddress localAddr = IPAddress.Parse(""localhost""); // TcpListener server = new TcpListener(port); // server = new TcpListener(localAddr port); //TcpListener Dinle = new TcpListener(localAddr51124); TcpListener Dinle = new TcpListener(51124); try { while (true) { Dinle.Start(); Exception is occured. Socket Baglanti = Dinle.AcceptSocket(); if (!Baglanti.Connected) { MessageBox.Show(""Baglanti Yok""); } else { TcpClient tcpClient = Dinle.AcceptTcpClient(); if (tcpClient.ReceiveBufferSize > 0) { byte[] Dizi = new byte[250000]; Baglanti.Receive(Dizi Dizi.Length 0); string Yol; saveFileDialog1.Title = ""Dosyayi kaydet""; saveFileDialog1.ShowDialog(); Yol = saveFileDialog1.FileName; FileStream Dosya = new FileStream(Yol FileMode.Create); Dosya.Write(Dizi 0 Dizi.Length - 20); Dosya.Close(); listBox1.Items.Add(""dosya indirildi""); listBox1.Items.Add(""Dosya Boyutu="" + Dizi.Length.ToString()); listBox1.Items.Add(""İndirilme Tarihi="" + DateTime.Now); listBox1.Items.Add(""--------------------------------""); } } } } catch (Exception ex) { MessageBox.Show(""hata:"" + ex.ToString()); } } } } TcpListener.Start is being called multiple times. 1- Called when you start your thread in the Server constructor 2- Via the call to Dinle in the Server_Load event handler 3- Again if you click the button in the btn_Listen_Click event handler I do not claim to have a complete grasp of what you are trying to do but I think this can be simplified. First you should create and start the listener once lets say when the code starts running. After that you can enter into a loop that calls AcceptTcpClient to accept connection and handle the communication. You also seem to be mixing Socket and TcpClient which should not be needed. Take a look at the following like for a basic example of using TcpListener and TcpClient. http://msdn.microsoft.com/en-us/library/system.net.sockets.tcplistener.aspx How can i rearrange my codes? @Phsika did you take a look at the sample that is on the page that I linked to? That sample provides a reasonable starting point for structuring your code."
314,A,"C code to get the ip-address I like to know how to get the ip-address of the local machine using C code ? If there are multiple Interfaces then I should be able to display the ip-address of each interface. NOTE: Do not use any commands like ifconfig within C code to retrieve the ip-address. Homework? What have you got so far? You must note that in production code using `ifconfig` is not the worst solution. Don't worry this is not my homework puzzle .... I am getting myself more involved into some serious programming in C as a result trying to fix some missing links in my application .... With the inputs from Michael Foukarakis I am able to show the ipaddress for various interfaces on the same machine #include <arpa/inet.h> #include <sys/socket.h> #include <netdb.h> #include <ifaddrs.h> #include <stdio.h> #include <stdlib.h> #include <unistd.h> int main(int argc char *argv[]) { struct ifaddrs *ifaddr *ifa; int family s; char host[NI_MAXHOST]; if (getifaddrs(&ifaddr) == -1) { perror(""getifaddrs""); exit(EXIT_FAILURE); } for (ifa = ifaddr; ifa != NULL; ifa = ifa->ifa_next) { family = ifa->ifa_addr->sa_family; if (family == AF_INET) { s = getnameinfo(ifa->ifa_addr sizeof(struct sockaddr_in) host NI_MAXHOST NULL 0 NI_NUMERICHOST); if (s != 0) { printf(""getnameinfo() failed: %s\n"" gai_strerror(s)); exit(EXIT_FAILURE); } printf(""<Interface>: %s \t <Address> %s\n"" ifa->ifa_name host); } } return 0; }  get known all interfaces from ""/proc/net/dev"" note cannot get all interfaces using ioctl only. #define PROC_NETDEV ""/proc/net/dev"" fp = fopen(PROC_NETDEV ""r""); while (NULL != fgets(buf sizeof buf fp)) { s = strchr(buf ':'); *s = '\0'; s = buf; //filter all space ' ' here got one interface name here continue for others } fclose(fp); then get address using ioctl() struct ifreq ifr; struct ifreq ifr_copy; struct sockaddr_in *sin; for each interface name { strncpy(ifr.ifr_name ifi->name sizeof(ifr.ifr_name) - 1); ifr_copy = ifr; ioctl(fd SIOCGIFFLAGS &ifr_copy); ifi->flags = ifr_copy.ifr_flags; ioctl(fd SIOCGIFADDR &ifr_copy); sin = (struct sockaddr_in*)&ifr_copy.ifr_addr; ifi->addr = allocating address memory here bzero(ifi->addr sizeof *ifi->addr); *(struct sockaddr_in*)ifi->addr = *sin; here also you could get netmask and hwaddr } see other posts for how to open the ""fd""  #include <stdio.h> #include <string.h> #include <sys/types.h> #include <sys/socket.h> #include <sys/ioctl.h> #include <netinet/in.h> #include <net/if.h> int main() { int fd; struct ifreq ifr; fd = socket(AF_INET SOCK_DGRAM 0); ifr.ifr_addr.sa_family = AF_INET; snprintf(ifr.ifr_name IFNAMSIZ ""eth0""); ioctl(fd SIOCGIFADDR &ifr); /* and more importantly */ printf(""%s\n"" inet_ntoa(((struct sockaddr_in *)&ifr.ifr_addr)->sin_addr)); close(fd); } If you want to enumerate all the interfaces have a look at the getifaddrs() function - if you're on Linux."
315,A,"Is this a scaleable named pipe server implementation? Looking at this example of named pipes using Overlapped I/O from MSDN I am wondering how scaleable it is? If you spin up a dozen clients and have them hit the server 10x/sec with each batch of 10 being immediately after the other before sleeping for a full second it seems that eventually some of the instances of the client are starved. Server implementation: http://msdn.microsoft.com/en-us/library/aa365603%28VS.85%29.aspx Client implementation (assuming call is made 10x/sec and there are a dozen instances). http://msdn.microsoft.com/en-us/library/aa365603%28VS.85%29.aspx The fact that the web page points out that: The pipe server creates a fixed number of pipe instances. and Although the example shows simultaneous operations on different pipe instances it avoids simultaneous operations on a single pipe instance by using the event object in the OVERLAPPED structure. Because the same event object is used for read write and connect operations for each instance there is no way to know which operation's completion caused the event to be set to the signaled state for simultaneous operations using the same pipe instance you can probably safely assume that it's not as scalable as it could be; it's an API usage example after all; demonstration of functionality is usually the most important design constraint for such code. If you need 12 clients making 10 connections per second then I'd personally have the server able to handle MORE than just 12 clients to allow for the period when the server is preparing for a new client to connect... Personally I'd switch to using sockets but that's just me (and I'm skewed that way because I've done lots of high performance socket's work and so have all the code)... Well putting sockets aside since I think the focus here is the Windows asynchronous I/O constructs being used I guess the question is what's a better way to get asynchronous I/O for named pipes with low CPU util. Would doing simultaneous I/O on a single pipe instance really be better performance? Would it beat simply upping the number of instances? I presume that is what you suggested doing above when you said you'd have more than 12 instances if 12 clients were needed? I'd use IO Completion Ports if possible as that will give you the most scalable I/O at the expense of some more complexity (you'd also be allowing multiple read/write operations which would mean that you'd be managing multiple 'per operation' OVERLAPPED structures and this would give higher read/write performance). However perhaps it would be better for you to describe the actual problem in more detail; ""starved"" is a little vague. Are these clients failing to connect? or failing to send/recv? What problem are you trying to solve here?"
316,A,"socket identifier equals -1 Somehow someway the socket is -1. It shouldn't be because my client program connects and send information but is stopped by SIGPIPE signal. When I debug the server I see that accept returns -1. I have got no idea why. If you no answer please. client.cpp using namespace std; using namespace msg; using namespace dataExchange; int main() { Connection con(""127.0.0.1"" 123456); Annoucement ann; ann.set_typ(Annoucement::AKCJA); con.send(ann); } server.cpp using namespace std; using namespace msg; using namespace dataExchange; int main() { Server server(123456); while(1) { Connection con = server.accept(); Annoucement ann = con.receive(); cout << ann.typ() << endl; } } connect.cpp using namespace msg; using namespace google::protobuf; using namespace google::protobuf::io; namespace dataExchange { class SocketMaintenance { protected: sockaddr_in addr; int s; public: SocketMaintenance(const unsigned int port) { memset(&addr 0 sizeof(addr)); addr.sin_family = AF_INET; addr.sin_port = htons(port); s = socket(AF_INET SOCK_STREAM 0); } SocketMaintenance(const sockaddr_in Addr const int Socket) { addr = Addr; s = Socket; } SocketMaintenance(const SocketMaintenance& source) { addr = source.addr; s = source.s; } ~SocketMaintenance() { close(s); } void write(const char* buffer const int n) { ::write(s buffer n); } }; class Connection : public SocketMaintenance { FileOutputStream* raw_output; FileInputStream* raw_input; char buffer[1024]; public: Connection(const char* IP const unsigned int port) : SocketMaintenance(port) { if(inet_pton(AF_INET IP &addr.sin_addr)<=0) { throw WrongAddress(); } if(connect(s (sockaddr*)&addr sizeof(addr))<0) { throw ConnectFailed(); } raw_output = new FileOutputStream(s); raw_input = new FileInputStream(s); } Connection(const SocketMaintenance& source) : SocketMaintenance(source) {} ~Connection() { delete raw_output; } void send(const Message& msg) throw(EmptyMessage) { CodedOutputStream* coded_output = new CodedOutputStream(raw_output); int n = msg.ByteSize(); if(n<=0) throw EmptyMessage(); coded_output->WriteVarint32(n); delete coded_output; raw_output->Flush(); msg.SerializeToArray(buffer n); SocketMaintenance::write(buffer n); } Annoucement receive() throw() { CodedInputStream* coded_input = new CodedInputStream(raw_input); google::protobuf::uint32 n; coded_input->ReadVarint32(&n); char *b; int m; coded_input->GetDirectBufferPointer((const void**)&b &m); Annoucement ann; ann.ParseFromArray(b n); return ann; } }; class Server : public SocketMaintenance { public: Server(const unsigned int port) : SocketMaintenance(port) { addr.sin_addr.s_addr = htonl(INADDR_ANY); if(bind(s (struct sockaddr *)&addr sizeof(addr))<0) { throw BindFailed(); } if(listen(s 5)<0) { throw ListenFailed(); } } Connection accept() throw() { sockaddr_in client; socklen_t client_len; int client_socket = ::accept(s (sockaddr*)&client &client_len); return Connection(SocketMaintenance(client client_socket)); } }; } I have tested this code before I rearranged it into classes. Then it worked flawlessly. One glaring issue I see is that your port is invalid. sockaddr_in::sin_port is a uint16_t whose largest value can possibly be 65536. Best case you're truncating your port on the call to htons which should result in 57920. In regards to why the accept is failing -1 indicates an error occured and you should check the value of errno to see why. From man 2 accept: RETURN VALUE On success accept() returns a non-negative integer that is a descriptor for the accepted socket. On error -1 is returned and errno is set appropriately. errno is equal 22 - invalid argument. Socket argument was equal 5. Don't know what is wrong :/ socklen_t client_len = sizeof(client); that is ok. Thx for info. I'm a noob.  At a guess somewhere in there you are passing a socket class object by value and its destructor is closing the socket prematurely. I couldn't be bothered wading through the code to find out if that is the case but some observations: don't allocate things with new unless you absolutely have to don't implement copy constructors if all they do is what the default copy constructor should do DO prevent copying of socket objects by declaring copy constructors and assignment operators as private and then not implementing them don't ""rearrange"" things into classes - either start out writing classes from the get go or just write procedural code I suppose it has nothing to do because when I comment the close(s) in destructor problem remains."
317,A,"On the Design of zero-copy Memory allocators used in high volume fast-path code Disclaimer: please do not bother reading this long post unless you are an embedded programmer or a kernel developer or a high performance system software programmer. It is about memory allocators and might not be of any interest to you. I am building a library for high-volume high-performance same machine IPC using OS specific pipes (named pipes on windows and UNIX domain sockets on UNIX). The library receives messages and inject them into a processing engine and this engine could be in the same process in C++ or something like Erlang or Lua or perhaps moved off to a central machine. The messages at the moment come from within a web-server and is part of the resolution pipeline of a HTTP request and should scale with the web-server -- i.e. I cannot write an inefficient system because a web-servers worker threads depend on the speed with which they can offload these messages. If a web-server is capable of handling 10k requests of static pages it should not be massively hampered by this IPC mechanism. The data comes in as typed messages and I believe they will have their own size statistics which could be leveraged. Doug lea's allocator creates linked lists of buckets by sizes if I remember correctly perhaps this is the way I should go - it is the standard of a general allocator. I want zero-copy semantics so till the message is done processing it should not by copied around. On a side note: this perhaps takes Googles protocol buffers out of the picture (can't be sure at the moment) as the wire format is different from the in-memory object format. I will investigate further -- my current implementation uses my own IPC format; perhaps someone has some opinions of how protocol buffers does things does it use zero-copy internally ? I want to use this allocator in all areas of the application. My study of network stack software is urging me towards a processor-page-aligned-pool based allocator; my application certainly has the same access semantics. I would copy all messages into a page and only recycle the pages once all element references -- that are allocated to the pool -- are inactive. To slightly formalize and help you understand what I am thinking about consider the following data structure: typedef struct _page { int active_elements; // page is returned to empty pool at zero int free_bytes; // used to sort the pages void * next_free_contigious; // pointer to where next allocation may occur _page * next_page; // linked list next page pointer } page; struct pageAllocator { page * ll_empty; // completely empty pages page * ll_active; // sorted by highest_size_available could be binary tree // continue reading for rationale. }; Some points are : The messages won't linger arround. So pages will become free fairly soon. I need to write bullet-proof code to free the pages. I do not know if page->next_free_contigious needs to be word alligned. I assume that under pressure the amount of pages allocated will be stable. Should I be returning pages to the OS or should I just keep them ? Maximum pressure would only allocate as much as it needs once maximum pressure has been established. Should I allocate messages to the first element in the size sorted list (first being the highest) or should I find the best matching page. In this case I would have to use a binary tree of some sort. Am I re-implementing doug's algorithm here ? I want suggestions from people who have personally dealt with such things alternate allocator suggestions would be welcome. Off the shelf libraries would be great (they have to be ANSI-C or a c-pre-processor meta-code implementation). The APR is a popular library with a hierarchical pool based strategy but I am not dealing with sessions at this low-level so I don't need a hierarchy. I don't think I am pre-optimizing because I am convinced I need it. My current implementation of this system jumps to 30% CPU during web-server CPU saturation and I know this is because of my naive way of programming the application using new and a lot of copy constructing in C++. I will of-course benchmark my application before I proceed (I am too busy atm to re-build a test environment) but any discussion here will be useful regardless of the outcome. --- edit 1 --- This implementation was inspired from some comments from Markr check answer below for some discussion. A bit more thinking lead to the following. Ok I am thinking about a circular buffer now (say 1mb^x). Composed of equal sized elements (perhaps some multiple of 128-bytes). The allocations happen incrementally till the end (with a allocatex-head(A) marking where allocations may occur from and a free-marker(F) marking contigious free regions(0) being returned to the pool. Consider the following ASCII illustration: |000000F101110001111A00000000| 0=free 1= occupied. Some specific are: The free-marker delimits free contiguous regions. In the illustration F will only move two forward when the immediately following chunk is returned. Once the allocation reaches the end or there isn't enough space at the end to perform a contiguous allocation the Allocate-head moves to the beginning; most of the buffer should be free by this point. once a message is moved to its destination the chunks are immediately returned to the buffer. On same machine delivery the messages will be delivered in short order (some blocking) or immediately if it just requires a thread-free function call. Comments would be nice and I would still prefer some self-contained portable library if anyone knows of any that does something similar. The GLIBC slice allocator looked nice but I believe the above implementation is simpler and probably faster and active regions on the buffer will always have good locality. @Vainstah: I think you're right to stick to C. There's nothing C++ can do that C can't do. (BTW MSVC has a nice pause button for getting stackshots.) If you happen to be using a recent gcc version you could have a look at C++0x move constructors. Could save some copying. I have toyed with the idea of C++ for this specific part of the application --i.e. the message switch. But I see a good implementation being very re-usable and its easier to interface with engines like erlang and Lua by sticking with ANSI-C. I also require the code to compile under MSVC. How do you ""know this is because of my naive way of programming the application using new and a lot of copy constructing in C++"" - is that what a profiler has said? Because the application regressions are quite simple. With thousands of messages coming in; the windows allocator has to have gynormous fragmented search lists of small chunks of memory. All the application does as it currently stands - is verify and multiplex high volumes of messages coming in into SQL batches and uploads them. I have benchmarked the app and afaik it was the allocation calls that caused most resource usage. I will ofcourse redo these benchmarks at some point. You may be right that you need it you might also be wrong. If you are right a simple way to prove it is to take stackshots. That could also prove that the problem is something else. Stackshots :D I must pursue that technique thank you. **pstack** is one option. This is an explanation of how/why it works: http://stackoverflow.com/questions/375913/what-can-i-use-to-profile-c-code-in-linux/378024#378024 ... If you're right maybe you can recycle used fixed-size objects in your own pools. That will be streets faster than new and delete. After reading others' comments it sounds like using a fixed pool of objects and allocating ownership of them may be a better idea than trying to mess with allocators. Presumably you have a large number of threads so any allocator would need to be doing quite a bit of locking; a large number of small allocs/ frees sounds undesirable. If you think you've got enough memory to keep reasonable sized pools per thread you can just allocate a fixed pool per thread which contains enough IPC objects to handle all the requests that thread will have outstanding at one time and allocate them per-thread from this pool with no locking whatsoever marking them for reuse once those requests are complete. Once you move the other processes off-box the problem becomes completely different as zero-copy is no longer an option (the data clearly need to be copied at least once to reach another machine) so I think you'll have other issues. Currently I use number_of_cpu * cpu-bound threads for the single named channel. I was thinking of having a tunable amount of threads and now that you mention it - I will remove any shared state even for threads multiplexing the same channel. So you are suggesting a pool of pages do you agree with my strategy of filling a page linearly and not reclaiming space by delivered messages and returning only after the page has is entirely free ? Secondly If I use Erlang I have to format the message into an Erlang friendly format (assumption); this is similar to forwarding message off-machine. I suppose one way of dealing with the input-format and the next-stage-format (NSF) of the message is to deterministically pre-allocate the size of the NSF in the same buffer. Another would be to make Erlang understand my IDL and just make a call to deliver the message; I think this method might be the best I can deliver the message straight into an Erlang amnesia table (which I can pre-generate from my IDL). In terms of size if this thing is living on a sparc with 32 cores and 32 web servers; I can just give each thread 32 * 0.5or 1-mb to be on the safe side. That should be perfectly justifiable on such a powerful machine. I'm afraid I don't really quite understand any of the above (I know nothing about Erlang) but I hope my comments were helpful :)  Answer: Think about WHAT you want to reach. Do you want zero-copy semantics when processing the messages? Or do you want a fancy new uberfast memory allocator? A ""zero copy memory allocator"" is non-sense! Writing a general purpose memory allocator is difficult (most probably the default allocator is perfect for your purposes). If you need an idea for a good allocator for your purposes then please provide some more details. The zero-copy thing: it depends a lot of other things in your application but for example I assume a piece of code that reads a stream from a named pipe and another piece of code that parses this stream and interprets this as messages. First reading from IPC pipe: have a list of buffers: read() from the FIFO into buffers[current_write_buffer_idx].data+write_pos a maximum of (buffer_size-write_pos) bytes. Add number of read bytes to write_pos and so on. If a buffer is full then check your list of buffers whether you find an unused one (keep a usage flag per buffer). If no unused one is found then allocate a new buffer. Set next_buffer_index on the old buffer (used for the ""parsing"" later..). You end up with: size_t write_pos read_pos; struct Buffer { void* data; bool is_used; size_t number_bytes_written; size_t next_buffer_index; }; Buffer *buffers; size_t number_buffers; size_t current_write_buffer_idx; size_t current_read_buffer_idx; Second parsing the stream: find the next byte at buffers[current_read_buffer_index].data+read_pos. You can read bytes from that buffer up to buffer.number_bytes_written bytes if number of written bytes equals buffer_size (fixed) then the buffer is full and you have to continue reading from the buffer at next_buffer_index... And so on. Set is_used true whenever you write to a new buffer set it to false whenever you have done parsing the buffer. What ""reading"" exactly means depends a lot of how your message are coded! So long... to make it (almost) zero-copy you have to provide more details. When messages are fixed size its too easy.. But assuming your messages are prefixed with type and/or size information and the data of the message is some kind of struct then you could do the following: read type and/or size information from stream and.. if the message is not yet completely received (read_pos number_bytes_written ...) then skip ... if the message (the size is known now) fits into a single buffer just use the pointer into this buffer (struct MsgXYZ*)( buffers[current_read_buffer_idx].data + start_of_message_index ) and pass it to whatever code does the job if the message does spawn the buffers create a temporary buffer and copy the parts of the message together pass pointer to the message to code.. after that free the temporary buffer (its a rare case anyway) Well I omitted a lot of details here ;) Think twice if zero copy really makes sense to your app. Zero copy when reading from a socket/pipe/... is IMHO not possible maybe through shared memory of some kind of mmap but not with pipes. Hi Its getting late and I promise I will read the entire post tommorow. Named pipes in windows and Unix Dom sockets both allow message semantics; this means that the recv() equavalent function (at least in windows) can be used to return the size of the incoming message.  Perhaps you are looking for something like the GLib Slice Allocator? It is designed for high-speed allocation and deallocation of similarly-sized objects. It also claims to have excellent performance in a multi-threaded environment. The one aspect of the API that might cause trouble for you is that you must give the size of your object during deallocation as well as allocation. I use GLib on Windows it works quite well. the characteristics are nice letting the code dictate how the sizing is done and delaying the return to OS very nice for fix sized data structures. I wonder if I would benifit from creating 128-byte intervals for the messages as they would be dynamic and this would create static sized intervals. I will look at how many dependencies the header has and if it works well (or has support for) windows."
318,A,"Resolve mac address by host name I've wrote several perl scripts during my internship and I would like to simplify the use of them. The scripts asks in arg a mac address and returns which switch is connected speed...etc. Instead of giving a mac address I would like to give a host name of a computer. So how can I resolve the hostname to mac address ? Thanks bye. Edit -> Solution could be : bash command or perl module or something powerfull like that... how many different subnets will you need to poll for mac-addresses? Keep in mind that the solution I offered below is only good for the subnet that your server is connected to What's the scope of this script? How large is the network hosts and switches? You're accessing switch information so are you a network administrator? @MattH : This script localize a mac address in the company. Network is about 20 cisco switch (2960 mainly) and about 500 computers maybe more. I'm not admin I'm here for my internship I ask infos to switchs in SNMP (with Perl). To simplify script for others I wrote CGI to display in a browser and I would like to enter PC name instead of mac address. Do the hosts pick up their IPs via DHCP? If so there may already be an IP/MAC database at your company. Do you have access to the router(s) ARP table via SNMP too? @MattH @Mike Pennington : Ok guys I see. The network admin gave me an export of the dhcp (.csv). So I can resolve host name -> mac (-> ip). Thx guys. The ethers file on a UNIX system maps Ethernet address to IP-number (or hostname). If your /etc/ethers is properly maintained you can look it up in there. Thx but mine is not properly maintained empty file. That's properly maintained then.  Does this help? [mpenning@Bucksnort ~]$ arp -an ? (4.121.8.3) at 08:00:27:f5:5b:6b [ether] on eth0 ? (4.121.8.4) at 08:00:27:f5:5b:6b [ether] on eth0 ? (4.121.8.1) at 00:1b:53:6b:c9:c4 [ether] on eth0 [mpenning@Bucksnort ~]$ In python... #!/usr/bin/env python import subprocess import re def parse_arpline(line hosts): match = re.search(r'\((\S+?)\)\s+at\s+(\S+)' line) if match is not None: ipaddr = match.group(1) mac = match.group(2) hosts.append((ipaddr mac)) return hosts SUBNET = '192.168.1.0/24' # Insert your subnet here subprocess.Popen([r""nmap""""-sP"" SUBNET]stdout=subprocess.PIPE).communicate() p = subprocess.Popen([r""arp""""-an""]stdout=subprocess.PIPE).communicate()[0].split('\n') hosts = [] ii = 0 for line in p: hosts = parse_arpline(line hosts) ii +=1 # Iterate and do something with the hosts list print hosts in perl... my $SUBNET = '192.168.1.0/24'; # Insert your subnet here `nmap -sP $SUBNET`; my $p = `arp -an`; for my $line (split('\n' $p)) { $line=~/\((\S+?)\)\s+at\s+(\S+)/; $ipaddr = $1; $mac = $2; # do something with with each mac and ip address } the `arp` cache will only be populated with hosts for which the local host has interacted with (and cached the response of an ARP lookup). If the host(s) you are interested in are not in the cache you will need to generate an ARP request. @jmtd good point... he will need to run `nmap -sP ` before running the arp Thanks guys cool for your help srry Mike didn't want to be nasty. @eouti thank you no problem. Good luck with your project @eouti what about the `arp` command doesn't help for 500 computers? @eouti you have a bizarre manner of interacting with people who try to help you"
319,A,"Project on SMS Server Hello I have taken a project 'SMS server'---which will do A. Auto SMS generator for clients like on a specific event like transaction on user account or regular interval like everyday once for reservation status B. Response receiver for polling through mobile phone. i was searched a ozeki server but it is not which i want. I want to implement my own SMS server. I am so confused. So i have no idea how can i implement this project using C#. Thanks..!! :) I successfully implemented an SMS server a long time ago... it is very complicated. Some of the points you need to consider are: You need to find and learn the PDU format which is the structure of SMS messages on the network You need to open a special SMPP account with a cell phone provider. This can be quite complicated and expensive. You need dedicated VPN hardware to connect to the cell phone provider's network. Software VPN's won't work because the provider probably won't allow Split Tunnelling. I used a PIX 501. THEN you can start working on the message tracking account management disaster mitigation etc. etc. If I started again I would almost definately find an online SMS gateway provider that has Webservice APIs or something similar.  Good luck implementing your ""own"" SMS gateway. Your best bet is to use an API driven utility model SMS gateway like Twilio.com. They already have REST API's ready to integrate into your application. Your other options are writing something in C# to interface with a serial port on the computer that would be connected to a GSM modem. The GSM modem acts like a phone on the cell network but accepts modem AT commands and can deliver SMS messages to other phones. You can also use something like Kannel (open source SMPP gateway) to link up to a 3rd party SMS gateway that talks SMPP like Mblox.com. However you will be paying a lot for something like this. My guess is you want what Twilio does already."
320,A,"Choose one of many Internet connections for an application I have a computer with a few different internet connections. LAN WLAN WiFi or 3G. All of these are active and the machine can use any of them. Now I want to tell my application to use one of the available connections. For example I want to tell my application to use only WiFi while other software might use something else. In my c# application I use classes like HttpWebRequest and HttpWebResponse. Is this even possible? if you are connected to internet  you can access internet  why you need to tell your application @Saurabh Maybe one route is faster than the other one. this is pretty outside my scope of knowledge but couldn't you do something with a proxy here? @Saurabh Being able to determine the routes your packets take has usages. Maybe one network is hardware encrypted another is high latency and yet another has limited usage such as the 3G(* assumed it was a misspelling) There are reasons to use each one depending on the scenario. See this MSDN blog entry on ServicePoint.BindIPEndPointDelegate. It explains how to explicitly create the local endpoint that will be used by an HttpWebRequest. For example to bind to 192.168.16.100: WebRequest webReq = WebRequest.Create(myUri); HttpWebRequest httpRequest = (HttpWebRequest)webReq; httpRequest.ServicePoint.BindIPEndPointDelegate = new BindIPEndPoint(BindIPEndPointCallback); ... private static IPEndPoint BindIPEndPointCallback(ServicePoint servicePoint IPEndPoint remoteEndPoint int retryCount) { if (remoteEndPoint.AddressFamily == AddressFamily.InterNetwork) { return new IPEndPoint(IPAddress.Parse(""192.168.16.100"") 0); } // Just use the default endpoint. return null; } always going on retry  is there any settings to turn ON in Server  No it is not possible using any C# classes - they all operate at too high of a level that its at the TCP/IP level (Even sockets are too high of a level). The Windows TCP/IP stack uses the routing table and interface metrics to determine which interface to send the packets out. You can see which interface Windows TCP/IP stack has determined has the best route (which is the one it will use to send the packet on) by calling GetBestInterfaceEx(). At a former employer we tried to do something similar separating traffic onto different NIC's by binding a socket to the IP address of the card we wanted to send the traffic on. However Windows would send the traffic out the NIC it saw had the best route (a NIC with a different IP address than the source). That took a bit of head scratching before we figured out what was going on. So short of doing something (perhaps) with raw sockets or libpcap (which would be ridicules you would have to build your own TCP/IP and HTTP handlers) you cannot do that on Windows. This isn't correct. You can certainly limit TCP / UDP traffic to a particular Network Interface using standard WinSock. So it is certainly possible on Windows - and as @Simon Svensson showed you can bind a socket in .NET to an address as well. Granted your packets still may not route properly - but you can still control their initial source.  This is somewhat advanced functionality which is abstracted away by both HttpWebRequest WebRequest WebClient and the like. You can however do this using TcpClient (using the constructor taking a local endpoint) or using sockets and calling Socket.Bind. Use the Bind method if you need to use a specific local endpoint. You must call Bind before you can call the Listen method. You do not need to call Bind before using the Connect method unless you need to use a specific local endpoint. Bind to a local endpoint for the interface you want to use. If your local machine have ip address 192.168.0.10 for the WiFi address then using that a local endpoint will force sockets to use that interface. Default is unbound (really 0.0.0.0) which tells the network stack to resolve the interface automatically which you want to circumvent. Here's some example code based on Andrew's comment. Note that specifying 0 as local endpoint port means that it is dynamic. using System.Net; using System.Net.Sockets; public static class ConsoleApp { public static void Main() { { // 192.168.20.54 is my local network with internet accessibility var localEndPoint = new IPEndPoint(IPAddress.Parse(""192.168.20.54"") port: 0); var tcpClient = new TcpClient(localEndPoint); // No exception thrown. tcpClient.Connect(""stackoverflow.com"" 80); } { // 192.168.2.49 is my vpn having no default gateway and unable to forward // packages to anything that is outside of 192.168.2.x var localEndPoint = new IPEndPoint(IPAddress.Parse(""192.168.2.49"") port: 0); var tcpClient = new TcpClient(localEndPoint); // SocketException: A socket operation was attempted to an unreachable network 64.34.119.12:80 tcpClient.Connect(""stackoverflow.com"" 80); } } } +1 seems that i was just wrong and there exists a possibility. So my answer is false and thous is deleted now. This would of been better with code to show. Here is an example of me using my VirtualBox endpoint instead of my NIC. TcpClient client = new TcpClient(); client.Client.Bind(new IPEndPoint( IPAddress.Parse (""192.168.56.1"") 32800)); client.Connect(""google.com"" 80); +1 still"
321,A,"Clustering vs Replication There are n clients the main concern is that is most of them is online (the more the better) as much time. There is only one server for reasons of budget and power consumption. I have seen this problem from many angles most exposed in this discussion Strategies for Java ORM with Unreliable Network and Low Bandwidth then summarized my options. Clustering. Using terracotta and using a second server (passive) installed on a node. Replication / Synchronization. My original idea: Allow nodes to be offline during network failures and then restart operations. What do you recommend? PS if there is something wrong in my reasoning please tell me No this is not easy. Are you talking about clustering/replicating the server where hibernate is running or clustering/replicating the database itself? The article you refer to seems to be about dealing with unreliable connectivity to the database and unreliability of the database. You only have one server you say so where are you replicating? You say ""clients"" -- clients of what? If they are clients to your *one* server and it goes down they will ALL experience the downtime (and all the clustering / replication in the world won't help). If in fact they're configured as a cluster or something please don't call them ""clients"" -- they're servers too. Sorry if I was unclear. That's the current configuration I want make some changes. In fact today if the only server goes down all system goes down that's the problem. So I'm thinking that I can implement a cluster solution or a replication solution with a local database in every node. Thanks for your interes djna and Chris! Also the system has low workload the full uncompressed mysql database is only 1.5 gb with 18 months data. It is incredibly unclear what the actual question here is. Is it just ""what do you recommend for clustering/replication/failover options?"" Thanks for those thoughts. The application is a simple point of sale. After thinking and analyzing your answer I will try to store users products and sales in DSO's (using some boxes as servers and nodes at the same time). At one time the server with the database is available pour the sales according to the producer-consumer pattern. And an apology for my short communication skills I'm still learning English!.  I think you are saying that you have a single machine on it you have both an application using Hibernate and a MySql database. If you lose that machine your users cannot work. You are asking whether some additional resilience is possible and believe that you have identified two solutions. You don't give much detail about either solution so I'm not going to try to second guess what you have in mind. You also don't say much about about the nature of the application. If (for example) it was entirely read-only no database updates then replication is comparatively easy. If all writes are additive and cannot conflict (for example some kind of opinion poll vote yes/no) then again replication with perhaps a bit of queueing is comparatively easy. However a traditional application with users updating shared data where a consistent view is important (for example we don't want to sell the last available hotel room twice) then replicating gets tricky. One approach is to separate the database to a highly resilient tier products such as Oracle RAC have very clever resilient characteristics and I guess you pay for cleverness. Once you have a resilient database then clustering your application becomes much easier. I often see lots of cheap boxes running the app in parallel because the definitive ""truth"" is managed by the DB. However even this is tricky unless the app was designed up-front for replication. You find all manner of subtle assumptions are made on the basis of the App is some sense being able to rely on it's previous view of truth. Products like Terracotta may well (I've never tried so I don't know) help to implement a resilient design but unless the design is well thought through it may have business flaws. I interpret your idea as being to run multiple parallel copies of the app and db and the deal with synchronisation issues. Only you know whether this makes sense for your business requirements. You would be opening up the possibility of inconsistency especially in the times when failures have occurred and resynchronisation has not completed. Oracle RAC seems to be a good alternative!"
322,A,IPv6 address problem in `ifreq` struct I am working with the following code: struct sockaddr_in6 *sin; struct ifreq ifr; sin = (struct sockaddr_in6 *)&ifr.ifr_addr; ifr_addr is of type struct sockaddr which for convenience is reported here under: struct sockaddr { sa_family_t sa_family; /* address family AF_xxx */ char sa_data[14]; /* 14 bytes of protocol address */ }; How am I supposed to put an IPv6 address (via sin->sin6_addr.s6_addr) in the ifr_addr field given there are only 14 bytes available? Where am I going wrong? TIA Chris Fortunately struct ifreq is declared to contain the addresses in a union at the end of the structure: struct ifreq { union { char ifrn_name[16]; } ifr_ifrn; union { struct sockaddr ifru_addr; struct sockaddr ifru_dstaddr; struct sockaddr ifru_broadaddr; struct sockaddr ifru_netmask; struct sockaddr ifru_hwaddr; short int ifru_flags; int ifru_ivalue; int ifru_mtu; struct ifmap ifru_map; char ifru_slave[16]; char ifru_newname[16]; __caddr_t ifru_data; } ifr_ifru; }; Note that the above was the result of gcc -E and so all the preprocessor stuff has been expanded. Also note that ifr_addr is #defined to be ifr_ifru.ifru_addr. This means that technically you can allocate bytes off the end of the structure to hold the address. In my opinion this is extremely sloppy. The union should be declared to hold a value of type sockaddr_storage along with everything else. Then the structure size will be big enough to hold any address. If you wanted to declare your own structure of the right size that you could use pretty easily just do this: union my_ifreq { struct ifreq sys_ifreq; struct { char name_pad[IFNAMSIZ]; struct sockaddr_storage addr; } padding; }; Then you will have a structure that always has enough storage that you can stuff any valid address in sys_ifreq.ifr_addr and have it work. Great explanation. Many thanks! I didn't even spot the `ifru_data` field. However I'm still confused on how to use it. I mean what I can do is `sin = (struct sockaddr_in6 *)&ifr.padding.addr;`. But when I pass around the structure how does the code know that there is an address at that location? @Jir: Just do this: `sin = (struct sockaddr_in6 *)(&ifr.sys_ifreq.ifr_addr)`. The `padding` part is just to make sure the union occupies enough space and should not actually be used. Presumably you will set the `sin->sin6_family` field to `AF_INET6` and this will be enough to let the system know that there's a full IPv6 address there. If you notice the family field is the same at the beginning of all the different sockaddr types. Now I get the whole picture! And that's why you defined `my_ifreq` as a union and not simply as a struct! Thanks for thoroughly explaining the details once again!  The platforms that use struct sockaddr inside struct ifreq also have an IPv6 friendly version struct lifreq that uses struct sockaddr_storage instead for example Solaris other platforms such as Linux and AIX already use the longer structure. Hi Steve! I'm coding for Linux kernel 2.6.26 but I have found no trace of `struct lifreq` unfortunately. @Jir see my example usage of `ifreq` for IPv6 here: http://code.google.com/p/openpgm/source/browse/trunk/openpgm/pgm/getifaddrs.c
323,A,"how to use VxWorks etherOutputHookAdd I'm stumped trying to get etherOutputHookAdd() to work. Its counterpart etherInputHookAdd() seems to work fine. The OS version in question is VxWorks 5.4 . The hook code looks like so (the code I intend to actually run is more complicated but this serves for an example.) int anCounter; STATUS etherHook(struct ifnet *pif char *buf int size) { anCounter += 1; return FALSE; } I can hook up etherInputHookAdd from the vxworks shell like so etherInputHookAdd etherHook""fei""0 This returns 0 (STATUS OK) after which examination of the 'anCounter' variable will indicate activity as expected. However no such luck with the output direction. I've tried both of these command lines etherOutputHookAdd etherHook""fei""0 etherOutputHookAdd etherHook Both of these return OK but the hook routine doesn't seem to be getting called at all. My best hypotheses are (1) I'm missing an initialization step or calling it wrong (2) the etherOutputHookAdd implementation is just a stub (3) you just can't call it from the shell or (4) maybe my nic driver implementation is buggy. Any ideas that solve the central problem - how do I see what's being sent off my board - are welcome. The following VxWorks network drivers support both the input-hook and output-hook routines: if_cpm - Motorola MC68EN360 QUICC network interface driver if_eex - Intel EtherExpress 16 if_ei - Intel 82596 ethernet driver if_elc - SMC 8013WC Ethernet driver if_elt - 3Com 3C509 Ethernet driver if_ene - Novell/Eagle NE2000 network driver if_fn - Fujitsu MB86960 NICE Ethernet driver if_ln - Advanced Micro Devices Am7990 LANCE Ethernet driver if_sm - shared memory backplane network interface driver if_sn - National Semiconductor DP83932B SONIC Ethernet driver if_ultra - SMC Elite Ultra Ethernet network interface driver if_gn - generic MUX interface layer The following drivers support only the input-hook routines: if_nic - National Semiconductor SNIC Chip (for HKV30) if_sl - Serial Line IP (SLIP) network interface driver The following drivers support only the output-hook routines: if_ulip - network interface driver for User Level IP (VxSim) The following drivers do not support either the input-hook or output-hook routines: if_loop - software loopback network interface driver  To those few who might stumble this way .. It was the horrible 'hypothesis 4'! It turns out that in order for etherOutputHookAdd() to work correctly it is incumbent on the NIC device driver writer to include a call to the function pointed to by etherOutputHookRtn. All etherOutputHookAdd() does is add your proffered packet handler to a list so that when a NIC driver calls etherOutputHookRtn you get a copy of what's being transmitted. Sadly there are many drivers where for whatever reason this was simply not done. So in cases such as this one there are only two courses of action. find a patch for your driver or patch it yourself change tactics entirely e.g. try using etherInputHookAdd() on the other side."
324,A,"extract IP from a buffer of bytes If I have a buffer containing this bytes: 510175126-94-51080 How can I extract the 75126-94-51 part from it(which is the ip) and print the correct IP ? Thanks in advanced. for(int i = 0; i < bytes_recv; i++) { cout << static_cast<int>(temp[i]); } cout << endl; edit: this is my output: 5 1 0 1 75 126 -94 -51 0 80 5 1 0 1 91 -53 99 45 0 80 What language?? Is the string always 18 characters long? What if the last part is 12221-94-51? it is ambiguous. What do you mean by ""correct IP""? Is `75126-94-51` enough or you want `75.126.94.51`? You should supply more test cases. SOrry i forgot to say i want in c/c++ . @Simone: i just now that the first 4 bytes represent something elsethe last 2bytes(080) should be the portso the rest is the IP(4 bytes)...so yesit would differe... @Simon: im testing a proxy serverand the client asys this: opera.exe - 75.126.162.205:80 open so i guess the ip it should send me would be that 75.126.162.205 thats way i say corect ip @vBx do you have dots in your buffer? If not you cannot tell if `12221-94-51` is `122.21.94.51` or `12.221.94.51` unless you know they can only come from certain IP classes. I dont think somy buffer its exactly like 510175126-94-51080 witch contains 10 bytes 1 byte(version) + 1 byte(type of connection) + 1 byte(reserved byte) + 1 byte(type of address) + 4 bytes(ip) + 2 bytes(port)  exactly in this order. The other option to what @Simone is saying is to have fixed field sizes i.e. 075126094051 or 122021094051. But you need to do one or the other in any case if you're writing the buffer. @vBx: You should rewrite your question. From your description of 10 bytes you should write this like: char buffer[10] = {5 1 0 1 75 126 162 205 80 00}. @vBx Please rewrite your question telling us how you got from your 10 bytes as you explained in the above comment to a string like ""510175126-94-51080"". Otherwise it is way too ambiguous. I am using protocol socks5 it always send me 10 bytes of data(if the address is of type IPv4. when i do bytes_recv = ::recv(...) it equals 10 bytesand when i print the buffer it gives me that...i donnt know why... @vBx How did you print the buffer? That string doesn't look at all as any printable representation or serialization of a 10-byte buffer. Also please edit your question to clarify with the extra information you are giving in the comments. Make the question complete. @Juliano: i print it using static_cast(buffer[i]) @vBx ok iteractively I see. Please do not hide such information from your question. It is not our job to determine how you got that number all of these comments specially the last must have been part of the question. The hyphens are not hyphens they are minus signs. You are printing everything mended. You should print spaces between each number or even better print it in unsigned hexadecimal. It is impossible to determine if ""51080"" is ""5 10 80"" ""51 0 80"" ""5 108 0"" or something completely different. In Python: input = '510175126-94-51080' output = input[4:-3] print output Yes in python its easy :) i forgot to say in c/c++ @vBx: The above is easily translated to std::string methods in C++; the problem is whether the [assumptions](http://stackoverflow.com/questions/4887061/extract-ip-from-a-buffer-of-bytes#comment-5436189) are valid. @Fred Nurk: yesi guess your writei will give it a shoot. using string i get this when i print the entire buffer: 51076-69123-78114-127-75 strange  If you have the following buffer: buffer[10] = {5 1 0 1 75 126 162 205 80 00}; And you want it in C/C++. I would define a struct like this: #pragma pack(1) struct buffer_s { uint8_t version; uint8_t reserve_1; uint8_t atype; uint32_t ip; uint16_t port; }; #pragma pack() Then put it in a union: union buffer_u { char buffer[10]; struct buffer_s data; }; Now you can declare union buffer_u buf and read bytes into buf.buffer. Then for the ip and port fields use ntohl and ntohs to convert network byte order to host byte order. @Juliano: I suppose. The alternative is casting the pointer to a char type. The end result is the same. Unions are not supposed to work like that. You can only access the last member that you have written to. If you replace ""union"" with ""struct"" your application must still work (binary incompatible but still work).  How are you getting this buffer? Is it part of a struct sockaddr_in? Are you showing a decimal dump of bytes starting at the lowest address of the buffer? What you want to do is get a pointer to the base (memory) address of the IP addr part and use ntohl() to translate it to host byte order and likewise use ntohs to translate the port addr. edit Suppose you have stuct sockaddr_in foo; char str[16]; // do some stuff that initializes foo e.g. a call to accept() returns You can extract the IP address in dotted-quad form with inet_ntop(AF_INET &(foo.sin_addr) str sizeof(str)); I dont know how to do how you say. I am using sockaddr_in to initialize the socketand the buffer is of type char*. Can you help ? i used inet_ntoa because i windwos i dont have inet_ntop so i did this: struct sockaddr_in server; cout << inet_ntoa(server.sin_addr); but it only gives me 0.0.0.0 @vBX - can you post a code sample that shows what you are trying to do? By which I mean show your struct `sockaddr_in` getting initialized and what you try to do with it after that?  A lot of information is missing from the question. So you are reading the server reply from the SOCKS protocol. First the buffer should not and will not have a fixed size of 10 bytes. It has 10 bytes if your address is IPv4 (which coincidently was exhausted a few days ago time to think about IPv6). If the origin has a IPv6 address the size of the server response is different. From RFC 1928 section 6 the server reply has this format: +----+-----+-------+------+----------+----------+ |VER | REP | RSV | ATYP | BND.ADDR | BND.PORT | +----+-----+-------+------+----------+----------+ | 1 | 1 | X'00' | 1 | Variable | 2 | +----+-----+-------+------+----------+----------+ Note that the address field has variable size. For IPv4 specifically ATYP == 0x01 and BND.ADDR has size 4 which makes 1+1+1+1+4+2 = 10 bytes. But you should consider that other sizes are possible specially if ATYP == 0x03 which makes BND.ADDR truly variable in length. So answering your question considering that you have these bytes in a char buffer[] array (or pointer) you must first check the type of the address then extract it like this: #include <arpa/inet.h> switch (buffer[3]) { case 0x01: { /* IPv4 address */ char result[INET_ADDRSTRLEN]; inet_ntop(AF_INET (void*)(&buffer[4]) result sizeof result); std::cout << ""IPv4: "" << result << ""\n""; break; } case 0x04: { /* IPv6 address */ char result[INET6_ADDRSTRLEN]; inet_ntop(AF_INET6 (void*)(&buffer[4]) result sizeof result); std::cout << ""IPv6: "" << result << ""\n""; break; } default: std::cout << ""Unsupported format.\n""; break; } Can i use inet_ntoa ? for windowsi guess inet_ntop is only for windows ? ahhwait there is an equivalent in windows inet_ntoa() is obsolete and doesn't support IPv6. inet_ntop() is the new standard it should be available for all POSIX operating systems and also Windows. For Windows see: http://msdn.microsoft.com/en-us/library/cc805843%28v=vs.85%29.aspx Yesthat is what I am trying to usebut i am having problem finding the header it is using...its not winsock2.h... Neither Ws2tcpip.h how it says on msdn website oki got it to workit shows the IP perfect;y. Thanks a lot Juliano"
325,A,How does _XPG4_2 and other defines work on Solaris? On Solaris in order to get the msg_control field in struct msghdr and have IPV6_TCLASS I seem to need to define _XPG4_2 and __EXTENSIONS__. It seems to work if I just define these to 1 before including anything: #if defined (__SVR4) && defined (__sun) # define _XPG4_2 1 # define __EXTENSIONS__ 1 #endif Should I do it this way? Do I need to define them in all source files or bad things may happen? Is there a list of these things somewhere? This is related to this question. man -k XPG4 reveals that there is a standards(5) man page which lists the feature test macros and library linking info for various standards including the following: X/Open CAE To build or compile an application that conforms to one of the X/Open CAE specifications use the following guidelines. Applications need not set the POSIX feature test macros if they require both CAE and POSIX functionality.  SUS (XPG4v2) The application must define _XOPEN_SOURCE with a value other than 500 (preferably 1) and set _XOPEN_SOURCE_EXTENDED=1. Grepping through /usr/include for _XOPEN_SOURCE turns more information in /usr/include/sys/feature_tests.h: application writers wishing to use any functions specified as X/Open UNIX Extension must define _XOPEN_SOURCE and _XOPEN_SOURCE_EXTENDED=1. The Sun internal macro _XPG4_2 should not be used in its place as unexpected results may occur. So defining _XPG4_2 yourself is not the way to do it. If any structure definitions depend on these macros you would definitely be better off defining them in all translation units. The easiest way to do that is to specify them on the compiler command line: cc -D_XOPEN_SOURCE=1 -D_XOPEN_SOURCE_EXTENDED=1 If you're using make you should be able to do this by adding the -D parameters to the CFLAGS variable: CFLAGS += -D_XOPEN_SOURCE=1 -D_XOPEN_SOURCE_EXTENDED=1
326,A,How does a HTTP response find its way to the correct browser window? If you have two browser windows open and you use each to navigate to a different website then how does the software know which HTTP response belongs to which browser instance? Update It seems that the distinction is made by the inbound TCP port numbers. But what about network messages that don't involve TCP/UDP? For example if you open two terminal applications and use both send a ping message to the same remote server how does the reply find its way to its terminal instance? Typically each browser instance creates its own socket to communicate with the server. Though the outbound port of all the sockets is the same (usually TCP 80 or 443) their inbound ports are different. Thus there are no conflicts when the server responds to the requests since the responses are sent to different inbound ports. Tools like ping use ICMP packets which provide their own way to uniquely identify the calling application (a unique identifier and a sequence number).  They're usually associated with different TCP connections which between them have used different ports on the client end. This means that the TCP stack at the client end knows the different and passes them via the sockets API the client used in an easily distinguishable way. (Typically different file descriptors) The exception to this is pipelining where multiple http request can be sent over one connection as an optimisation. Requests sent like this are received in the order they were sent however making it trivial to match them to the requests.
327,A,"POSTing to the web in Clarion I'm developing a RESTful API for a client. The problem is he's using a rather obscure language called Clarion. It's proprietary and closed and the docs are not freely available online. Whenever we discuss passing data from his code to mine and back again he starts talking about ""ftp file uploads"" and direct server-to-server SQL. Needless to say these ideas bring back visions of the bad old days. I have done some googling and I can't find any evidence that this language is capable of creating HTTP Post requests at all let alone using SSL encryption to protect them from prying eyes. I'm looking for advice specific enough that I can guide him through implementing his end of the bargain. I specifically want to avoid trying to pass XML requests as files via FTP or by writing them to the disk and calling some script. It should go without saying but I'm also not interested in running proprietary clarion server code or DLLs on my server. Is Clarion capable of generating POST requests? Is XML hard to generate in Clarion? Is there a simpler/easier to use format my client may have more sucess with? None of the data is more complex than key/value pairs. I'm coding in python but I can deserialize any reasonable data format if there's some way to get the data to my server. I came in very late to this post for I only had Stack Overflow account set today. However I would like to comment on Bruces answer. Bruce runs a 3rd party Clarion add on maker company and will always suggest the use of their products. Altough they're really fine an work very well I can't help pointing that there are standard open tools for about anything that needs to be done. For example the programmer could use ""curl"" http://curl.haxx.se/ to communicate with a web server from a program. Not only a Clarion program but any program. Aside from that Clarion does have access to all the Windows API and it is just a matter of writing the code so sockets http mci and whatever are at any programmer's reach. Need to send e-mail from a program that apparently doesn't have access to smtp functions? use ""Blat""! - blat.net Want to download some file from a web site? wget - gnu.org/software/wget These are all command line interfaces. And I suggest the ones who don't know what ""interface"" means to go get a look at The Free Dictionary - tfd.com/interface Regards Thanks for your answer but as I specified in the question I'm looking for a NATIVE solution. Anything which makes deployment more complex is out (curl extra command line tools etc.). Also I'm not sure why you're insinuating that I don't know what the word interface means.  Information wants to be free. The language may be proprietary and closed but the documentation is published online: http://www.softvelocity.com/clarion/pdf/LanguageReferenceManual.pdf Looks like a Windows 3.1 vintage report generating language which has the ability to talk DDE/OLE (!) but seems to have no external communication features other than that. So no Clarion cannot do POST requests (except via a third party custom control / DDE conversation). Using the file system might be a safe way to proceed: it keeps the client in familiar territory and is the easiest to debug. However if two way communication is required you might need to blow the dust off the manuals and go the DDE route. It really depends on the exact requirements (e.g. is the program batch or interactive?) but page 935 (Appendix A) in the 1158 page manual is where to start looking! Information may want to be free but that manual is for version 6 not 7 and is about 6 years old now. I presume version 7 has more features but I don't have the docs for it. One small point here just to correct some possible misconceptions. Clarion is a business orientated language with strong functionality WRT reading and writing data so more than a report generating language. The language has it's roots in DOS circa 1984 with the Windows version debuting circa 1994. Because it is a native compiled language it has access the whole Windows Win32 API and is not limited to the functions in the doc you read. So native Clarion can do POSTS simply by using the Winsock API or indeed the WinINet DLL.  I feel your pain. Communicating between systems can be a major pain. Good news though is that Clarion can do TCP/IP and XML (with a little help) so there's nothing that should hold your Clarion colleague back. In the interests of full disclosure I should point out that I'm biased here - I'm about to recommend that the Clarion guy use tools I created - nevertheless there are thousands of Clarion programmers out there using them and they provide the answer to your question so please forgive me. Ignore if you like. In Clarion there are a couple of tools that make TCP/IP communications easy and that enable the use of SSL. The one I make is called NetTalk (http://www.capesoft.com/accessories/netsp.htm). There is also XML support inside the Clarion box although it's unnecessarily cumbersome so there are at least 2 xml products he can use - iqXML (which is free) and xFiles (http://www.capesoft.com/accessories/xfilessp.htm) which is designed to be super fast. Using NetTalk & xFiles together it's trivial to create SOAP servers or clients. (Or plain HTTP servers and clients as you prefer.) There are a LOT of folk doing just this so there's absolutely no excuse for using shared files or FTP'ing requests around. I recommend you gently point your Clarion friend in the right direction. If you'd like to run this question past other Clarion developers then try http://faq.clarionmag.com/ (which is using the StackOverflow engine.) There are also lots of programmers active on the NNTP protocol (news) at news.softvelocity.com (comp.lang.clarion and others). Cheers Bruce Thank you Bruce! this is exactly the answer I needed but (unfortunately) can no longer accept. I guess an upvote will have to do. I'll definitely be passing this info along and appreciate the pointers to more active Clarion communities.  Instead of trying to accomplish more in this obscure language I'd go with the approach that you hinted upon: using the file system as a hand-over mechanism. Have his code output files to a given folder; then have a daemon written in a ""normal"" language monitor that folder regularly (cron job etc). When a new file shows up upload it through HTTPS / other ""normal"" means to your other server to do the task. This approach follows the ""localize the crap"" philosophy - if you can't get rid of crap at least make sure that it's ""borders"" are well defined. This really isn't an option here. I don't have control over the install process and making it more complex than it already is just isn't an option."
328,A,Single vs. Dual Sockets In Network Programming The question is about network programming more precisely servers. Let's suppose there is a server which handles a lot of connections and thus has a listening socket. There obviously exists a single instance of such a socket that's clear. Now I've seen designs which use (a)one socket for every connection both for incoming and outgoing data and (b)two sockets one for the incoming data one for the outgoing data. What makes one or the other design more preferrable? What are the possible reasons / usecases for these two designs? The programs I was referring to were actually instant messengers (two of them) but this theoretically applies to any multiple-connection server (any server then.) Hope this question is not too generic I don't know a lot about network programming at the moment so asking this. Rapid Googling didn't help either. TCP and UDP Sockets are full duplex. There is no reason whatsoever to use separate sockets for input from and output to the same client. It is just wastefully using up kernel resources at double the rate. OK so as far as I can see that design (with dual sockets) was flawed since the very beginning right? Or could they have done like that because of connection recovery in the case of connection loss? Still thanks for the explanation it has shown me what direction I should investigate further as far as TCP sockets are concerned (duplex is the keyword here). @Semen Semenych: Using two sockets where one would suffice doesn't help connection recovery in any way that I can see. It only introduces the additional complication that now one of the connections may be down and the other one still up.
329,A,Why does TCP wait for three duplicate ACK before fast retransmit? Why does TCP wait for three duplicate ACK before fast retransmit? RFC 2001 says Since TCP does not know whether a duplicate ACK is caused by a lost segment or just a reordering of segments it waits for a small number of duplicate ACKs to be received. It is assumed that if there is just a reordering of the segments there will be only one or two duplicate ACKs before the reordered segment is processed which will then generate a new ACK. If three or more duplicate ACKs are received in a row it is a strong indication that a segment has been lost. TCP then performs a retransmission of what appears to be the missing segment without waiting for a retransmission timer to expire. The reasoning for not doing the retransmit until the third duplicate seems to be that until that point it's more likely to just be out-of-order delivery and the retransmit isn't really needed.
330,A,LENGTH Field in IEEE 802.11b I am simulating the IEEE802.11b PHY Model. I am building the header of the Packet in the Physical Layer. As per the Literature The PLCP LENGTH field shall be an unsigned 16-bit integer that indicates the number of microseconds to transmit the PPDU. If I assume the packet size to be 1024Bytes what should be the value of the Length field(16 bit wide) The calculation of the LENGTH field depends on the number of bytes to send as well as on the data rate (5.5 or 11 Mbps). The basic idea of the calculation is:  Bytes * 8 LENGTH = Time (µs) = ---------------- Data rate (Mbps) However you need to read Section 18.2.3.5 Long PLCP LENGTH field in the 802.11b-1999 Standard pages 15-17. It has the complete details of how to calculate this value along with several examples. It unambiguously explains how to properly round the data as well as when the length extension bit in the SERVICE field should be set. I will not reproduce the text of the section here since it looks like IEEE might be strict about enforcing their copyright. However if you don't have the standard already I suggest you download it now from the link above -- it's free! If you have any questions about interpreting the standard don't hesitate to ask. Thanks a Lot for your help. I will have a look. But I got an idea how to set the LENGTH field. Just another question regarding the LENGTH field. There was no mention about how to calculate LENGTH for 1Mbps and 2Mbps for 802.11b. If the rate is 1Mbps or 2Mbps ( not 5.5 or 11 Mbps) how do we calculate the LENGTH ? As far as I can tell 5.5 and 11 Mbps are the only allowed data rates for 802.11b. See the table on http://en.wikipedia.org/wiki/IEEE_802.11 Thanks I was wrong. I had an impression that even 1Mbps and 2Mbps is part of 802.11b Standard.
331,A,Can boost::asio only receive full UDP datagrams? I am working on a UDP server built with boost::asio and I started from the tutorial customizing to my needs. When I call socket.receive_from(boost::asio::buffer(buf) remote 0 error); it fills my buffer with data from the packet but if my understanding is correct it drops any data that won't fit in the buffer. Subsequent calls to receive_from will receive the next datagram available so it looks to me like there is some loss of data without even a notice. Am I understanding this the wrong way? I tried reading over and over the boost::asio documentation but I didn't manage to find clues as to how I am supposed to do this the right way. What I'd like to do is reading a certain amount of data so that I can process it; if reading an entire datagram is the only way I can manage that but then how can I be sure not to lose the data I am receiving? What buffer size should I use to be sure? Is there any way to tell that my buffer is too small and I'm losing information? I have to assume that I may be receiving huge datagrams by design. Use getsockopt with the SO_NREAD option. From the Mac OS X manpage: SO_NREAD returns the amount of data in the input buffer that is available to be received. For data-gram oriented sockets SO_NREAD returns the size of the first packet -- this differs from the ioctl() command FIONREAD that returns the total amount of data available. There is an available() method in the boost::asio::ip::udp::socket class thanks for the suggestion.  This is not specific to boost; it's just how datagram sockets work. You have to specify the buffer size and if the packet doesn't fit into the buffer then it will be truncated and there is no way to recover the lost information. For example the SNMP protocol specifies that: An implementation of this protocol need not accept messages whose length exceeds 484 octets. However it is recommended that implementations support larger datagrams whenever feasible. In short: you have to take it into account when designing your communication protocol that datagrams may be lost or they may be truncated beyond some specified size. That is what I thought and I just needed confirmation from someone with more experience on this issue. I talked with the project manager and he gave me an estimate of the maximum size of the datagrams although there is no fixed protocol.  For IPv4 the datagram size field in the UDP header is 16 bits giving a maximum size of 65535 bytes; when you subtract 8 bytes for the header you end up with a maximum of 65527 bytes of data. (Note that this would require fragmentation of the enclosing IPv4 datagram regardless of the underlying interface MTU due to the 16-bit IPv4 packet/fragment length field.) I just use a 64 KiB buffer because it's a nice round number. You'll want to keep in mind that on the transmitting side you may need to explicitly enable fragmentation if you want to send datagrams larger than will fit in the interface MTU. From my Ubuntu 12.04 UDP(7) manpage:  By default Linux UDP does path MTU (Maximum Transmission Unit) discov‐ ery. This means the kernel will keep track of the MTU to a specific target IP address and return EMSGSIZE when a UDP packet write exceeds it. When this happens the application should decrease the packet size. Path MTU discovery can be also turned off using the IP_MTU_DIS‐ COVER socket option or the /proc/sys/net/ipv4/ip_no_pmtu_disc file; see ip(7) for details. When turned off UDP will fragment outgoing UDP packets that exceed the interface MTU. However disabling it is not recommended for performance and reliability reasons.
332,A,"send data struct over network between C/C++ server C/C++ and C# clients My client will communicate with clients using a struct/class called annoucement. I suppose I will write the server in C++. There will be many different classes that inherit annoucement. My problem is to send these classes over network to a client I thought maybe I should use iovec. However I don't know if there is anything corresponding to iovec in C#. The idea is: I would send a struct with writev a client would use readv and read the data from the socket like it would be an annoucement class check uint16 variable in the received struct to deduce the exact type and read the rest of data with apprioprite iovec/readv. C# can send whole objects as an array but how to interpret it in C/C++? I want both C/C++ and C# clients to work with the same protocol but that is the first time I designed the protocol and I'm not sure all my ideas will work together. Any hint'd be appreciated. use a fifo buffer. if ya dont know what that is let me know ill roll you one up no problem. 2. use sockets dont use writev. in C# use tcpClient or what ever you deem necessary. 3. in C/C++ you can use struts to byte align your data so that when you send it over the wire all is well cast from the start of your filled out strut to a pointer  and use that pointer in the call to send with the actual data size of the struct as the Len of your data(you'll have to cast here as a send call expects char* data types no biggie though because a byte is an unsigned char anyways. Back in C# you'll not have such luxury as you had in C/C++ i suggest using a StringBuilder class to gather your data into it's buffer and if you know your data size it is indeed byte aligned from the wire you'll have to (because it's Unicode in C# only no ASCII 1st convert the string to the ASCII data type (this all depends on what utf codepage your machine is set on and judging from your question id say your not on the same one i am:) tool for that job is (i think) System.Text.Encoding.GetEncoding(""CodePageString"").GetBytes(--insert stringbuilders.ToCharArray function here); and save it all in a byte buffer which you can set up from finding the lenth of the recieved data either off the socket call or from the StringBuilder then you can look at your data in a biniary format just remember though always use htons and ntohs (host-to-network order and network-to-host order) (not gonna go into little endian and big endian). Follow all of that and you'll nave 0 hitches ;)  The question is a bit chaotic but I have a feeling that you might be interested in Protocol Buffers for efficiently serializing data and transferring it over the network.  There are a number of options when it comes to serialization. This FAQ at parashift.com discusses serialization/deserialization in detail: http://www.parashift.com/c++-faq-lite/serialization.html.  Don't try to define your protocol in terms of structs. Define the format of an announcement i.e. what bytes in what order constitute an announcement and then define how multiple announcements are transmitted in sequence. Once you've got this wire format defined you can implement parsers and writers in each programming language using the most appropriate programming language constructs libraries and frameworks available in each. A good example for this is protobuf: format specification C++ Java Python implementation .NET implementation."
333,A,"How to transfer a file in a client/server Java application I am currently working on a homework assignment and I am thoroughly stuck. I am on the last question and I just can not figure out how to go about accomplishing the last task. Below is the tasks I had to complete: The client should save the file in the ""client"" subdirectory of the home directory. Test your program. Be sure it works with binary files not just text files. Be sure it works when both programs are on the same machine as well as when they are separated over the network. Thus far when I start the server it asks what port I want to use. Then I start the client and it asks what IP and port to use. The the server immediately sends a list of the files in the home directory ""server"" folder. I then respond with the client with the file number I wish to download. This is where I get stuck. I can't seem to find any information about how to do this. So as you can see in my code posted below I am trying to use a FileInputReader to convert the file to an array of bytes. Then I am sending that to the client. I am then trying to FileOutputReader the recieved array of bytes to a file. But I can't seem to find the correct methods to do that or even if I am doing that correctly. CLIENT  int i = 0; while(i < 1000){ String modifiedSentence = inFromServer.readLine(); System.out.println(""From Server: "" + modifiedSentence); i++; } while(j < 1000) { int byteString = inFromServer.read(); ArrayList<byte[]> bytes = new ArrayList<byte[]>(); bytes.add(byteString); } Integer byteInt = new Integer(byteString); FileOutputStream fo = new FileOutputStream(System.getProperty(""user.home"")+ ""/client/text.txt""); fo.write(byteInt.byteValue()); } } SERVER  byte[] bytes = new byte[1024]; FileInputStream fi = new FileInputStream(file.toString() + fileArray[userChoiceInt]); fi.read(bytes 0 1024); outToClient.write(bytes 0 1024); } } } If anyone could offer any advice or the correct classes or methods to use I would appreciate it. Thank you in advance. Yea you are correct in that I can not use any third party libraries. :( Since this is homework are you allowed to use 3rd party libraries? You're basically reinventing the FTP there exist useful FTP libraries in Java. If you can use an IO library for this I would suggest Netty or Mina. There are some netty examples here: http://jboss.org/netty/documentation.html Even if you cannot use a library these may be helpful for learning how things are done.  You can take a look at this tutorial by Sun (Oracle). That should give you a basic understanding of sockets. What I do seem to notice however in the client side you iterate a specific amount of times (1000) which is not a good idea since generally you do not know the size of the file to be sent the tutorial should show this and how to make the appropriate changes. Also you keep creating the structures within the loop so you loose any information that you have received besides creating new datastructures each and every time which is inefficient. What you have to do is to move the initialization of the structures from outside the loop. Also in the case of the modifiedSentence variable you might want to change it from string to StringBuilder. Wow yea that is terrible. The fact that I am continually instantiating a new ArrayList each time really defeats the purpose. Thanks for pointing that out. As for the while(i < 1000) I was using while(true) but then it would never leave the loop. I also tried the method call on the socket.ready(); but that didn't seem to work. Thank you for your advice I am looking at the tutorial you posted right now. You should either read until the socket returns -1 (if you are reading bytes) or null (if you are reading strings)  Without spoiling the whole thing here's some hints. This can be easily accomplish by using Socket (Server & Client). Using byte[] for transfering the file(s) will ensure that your program will work with both ascii and binary file(s). Another approach would be to use the build in Remote Method Invocation (RMI). I haven't transfered file using this approach but I'm sure it's feasible. And in case you didn't know getting the user home directory is accomplished with the following call: System.getProperty( ""user.home"" );  You probably should not ask how to do homework in the class on Web sites like this. It is not appropriate. Your server does look mostly good. Your program will only read files up to 1024 bytes though. You should look at java.io.File in more detail. There is a length method you can use to find the length of the file so you know how much to send."
334,A,What do we need to build a multiplayer iPhone app? I'm doing some work right now for a small startup who wants to build a multiplayer game for the App Store. The basic idea is that users of the game can look to see if anyone else is online and enter a 2 player turn based game. I have no experience with this kind of thing so I'm not responsible for that part but we need to know the right kinds of questions to ask potential hires. Does apple provide a framework for what we're trying to achieve? What kind of experience would you expect somebody capable of setting this up and maintaining it to have? I know we'll need an SQL-server to manage accounts and stats and such but beyond that I'm not too sure. How would you get started tackling this? I'd start by looking at Apple's Game Center which was added in iOS 4: https://developer.apple.com/devcenter/ios/gamecenter/ (You will need an Apple ID to see that page). Game Center provides much of the framework you're looking for. (Note: it's less than a year old so you may find limited experience with your interviewees).  Apple provides Game Center for turn-based multi-player gaming. you can read about it here: http://developer.apple.com/library/ios/#documentation/NetworkingInternet/Conceptual/GameKit_Guide/GameCenterOverview/GameCenterOverview.html that is great news. I wasn't aware of that thanks. i hate reinventing the wheel. ;) I agree! Good luck with your project.
335,A,Activate a network user I have created a user in a local network and grant some permissions to this user. now i want to do something like visual studio doing it means i want to activate this user in my program so every users that run my program have the permissions of this new user not him\her own permissions.for example users do not have permission to access to a folder but when run the program can open that folder because of new users permission. please tell mehow can i do that? There is another answered question here on the same topic: Need to Impersonate user forAccessing Network resource Asp.Net Account is this code working in windows app on local network?
336,A,Netstat for a single connection? On Linux is there any way to programmatically get stats for single TCP connection? The stats I am looking for are the sort that are printed out by netstat -s but for a single connection rather than in the aggregate across all connections. To give some examples: bytes in/out retransmits lost packets and so on. I can run the code within the process that owns the socket and it can be given the socket file descriptor. The code that sends/receives data is out of reach though so for example there's no way to wrap recv()/send() to count bytes in/out. I'll accept answers in any language but C or Java are particularly relevant hence the tags. The information nos refers to is available from C with: #include <linux/tcp.h> ... struct tcp_info info; socklen_t optlen; getsockopt(sd IPPROTO_TCP TCP_INFO &info &optlen) Unfortunately as this is Linux specific it is not exposed through the Java Socket API. If there is a way to obtain the raw file descriptor from the socket you might be able to implement this as a native method. I do not see a way to get to the descriptor. However it might be possible with your own SocketImplFactory and SocketImpl. It's probably worth noting that the TCP(7) manual page says this re TCP_INFO: This option should not be used in code intended to be portable. Thanks @Richard Fearn I've corrected it. `getsockopt`'s 5th parameter should be a pointer to a `socklen_t` holding the size of `info`.  Most of the statistics you see with netstat -s is not kept track of on a per connection basis only overall counters exists. What you can do is pull out the information in /proc/net/tcp First readlink() on /proc/self/fd you want to parse the inode number from that symlink and match it against a line with the same inode number in /proc/net/tcp  which will contain some rudimentary info about that socket/connection. That fil is though not very well documented so expect to spend some time on google and reading the linux kernel source code to interpret them.
337,A,"Software to receive printjobs and forward them to printers in a LAN I want to create a software which windows will detect it as a installed printer driver and list that software under ""Devices and Printers"" Just like the ImagePrinter sowftware you can access it through following link. http://sourceforge.net/projects/imageprinter/ when this Image Printer is installed on a computer windows lists it under Printers and Devices so we can right click and share it in our network. LAN users can add this as a remote printer and send print jobs to it which it will in turn convert those print jobs to image format (jpg etc.) and save in a specified directory. What I need is to get those print jobs from other PCs on LAN like ImagePrinter and send them to real printers shared in the LAN as sending from this own machine. (like the computer running my software sending those print jobs) so they could be printed. to do this i need to get two things.. 1- creating the software in a way windows will detect it as a printer (so it could be shared easily and receive print jobs) 2- send print jobs to installed remote printers added to the machine running my software just like machine's own print jobs.. I want to do them in c# (because my rest of the application code is in C# anyway if it can be combined together then the programming language is not a matter.) Please give me instructions or even some topics to read on.. coz I don't have any idea how to build it. Only little confident because ImagePrinter is a similar software.. so the task is possible. Thanks in advance. This question is a duplicate of http://stackoverflow.com/questions/3311565/print-job-accepting-and-routing-software -- I should have noticed before. I found some information about *Microsoft Universal Printer Driver* does anyone know any information about this? How to use it is it possible with this driver? are there better options? Following is the MSDN link on it http://msdn.microsoft.com/en-us/library/ff556567%28v=VS.85%29.aspx thanks This can be done combining four ingredients in the right way: a print queue setup with a PostScript printer driver shared on the LAN; Ghostscript (scroll down to get gs871w{3264}.exe) to convert PostScript to image; Redmon (download redmon17.zip) to serve as the 'printer port monitor'; a DOS batch file to do exactly what you want; The printqueue will be using the 'Red-irector Port Mon-itor' to channel the incoming PostScript jobs to a program/application/batchscript of your choice. What's left to do is your job: write a simple program/application/batchscript which does three things: take the incoming PostScript as its input call a Ghostscript commandline to convert input to the %imageformat% of your choice and finally send the %imageformat% as jobs to a printer of your choice. Here is a document that describes some of the basic need-to-know things regarding RedMon: http://pages.cs.wisc.edu/~ghost/redmon/en/redmon.htm Here are a few additional hints: If you are a newbie to Ghostscript you'll probably have the biggest problem with constructing a commandline that would do what you neeed. Here are some examples. The first one would convert data arriving from standard input (stdin - at the end of the command) to single-page black+white TIFF G4 with a resolution of 600dpi where each page is a separate file named page_001.tif page_002.tif etc.: gswin32c ^ -dBATCH ^ -dNOPAUSE ^ -dSAFER ^ -sDEVICE=tiffg4 ^ -r600x600 ^ -sOutputFile=c:/path/to/output/page_%03d.tif ^ - ### <-- note this! Here is a Ghostscript commandline which would generate the same output but this time as one single multi-page TIFF G4: gswin32c ^ -dBATCH ^ -dNOPAUSE ^ -dSAFER ^ -sDEVICE=tiffg4 ^ -r600x600 ^ -sOutputFile=c:/path/to/output/multi_page_g4.tif ^ - ### <-- note this! You don't want black+white G4 TIFF but colored TIFF 32-bit CMYK? OK use a different output device for Ghostscript: gswin32c ^ -dBATCH ^ -dNOPAUSE ^ -dSAFER ^ -sDEVICE=tiff32nc^ -r600x600 ^ -sOutputFile=c:/path/to/output/multi_page_color.tif ^ - ### <-- note this! You want JPEG? Sorry there is no such thing as multi-page JPEG. But single-page no problem: set outputname=some-uniq-name && ^ gswin32c ^ -dBATCH ^ -dNOPAUSE ^ -dSAFER ^ -sDEVICE=jpeg ^ -dJPEGQ=95 ^ -r600x600 ^ -sOutputFile=c:/path/to/output/%outputname%-page_%03d.jpeg ^ - ### <-- note this! Thank you very much pipitas this is whole lot of information to me. Your answer gave a very good detailed solution. Right now I'm following your details and reading more on the ghostscript and Redmon. I have few problems they might sound inappropriate because I don't know about the print job handling processes. Without converting to an image format using ghostscript is there anyway to directly accept and forward the print job as its to another computer with a real printer attached? Thanks again! Yes that is possible. -- This question is a duplicate of ""Print Job Accepting and Routing Software"" [http://stackoverflow.com/questions/3311565/print-job-accepting-and-routing-software] btw. (and I didn't really notice before). @Zerone: see my second answer to your duplicated question ""Print Job Accepting and routing Software"" (at http://stackoverflow.com/questions/3311565/print-job-accepting-and-routing-software/3421860#3421860 )"
338,A,"C++ SMTP that allows file attachments I am looking to create an SMTP that allows for file attachments however I am finding it difficult to locate a tutorial. I have found one close thing which is C++ SMTP Example  however this fails to compile in VS2005/2010 because of the include files that are used. I would like to roll my own and not adapt to some library such as Curl or Boost. Does anyone have any suggestions on how to do this or some small sample code with good documentation that will compile in Visual Studio? Do you mean SMTP *client* ? Yes I did my bad Are there any other suggestions by anyone? Building on my answer to a previous question using the Microsoft ATL classes which are included in the paid versions of Visual Studio. CSMTPConnection smtp; if (!smtp.Connect(m_strEmailServer)) return false; // start generating the email message; remember to call CoInitialize somewhere in the app before this CMimeMessage msg; msg.SetSubject(m_strSubject); msg.SetSender(m_strSender); // repeat the following as necessary msg.AddRecipient(strSingleRecipient); msg.AddText(m_strBody); // add an attachment msg.AttachFile(m_strAttachmentPath m_strAttachmentName _T(""application/octet-stream"")); if (!smtp.SendMessage(msg)) return false; return true; The MIME type supplied in the AttachFile call will depend on the type of attachment. While very interesting does that require MAPI? I would rather not require the use of the default email program. I misread that post you linked but it seems that requires ATL/non-express version of Visual Studio. I would like for this to work with express versions of Visual Studio also.  There are a couple of ways to do attachments. You can use UUEncoded or MIME formatting. The UUEncoded is much simpler if you want to roll your own."
339,A,"When does a UDP sendto() block? While using the default (blocking) behavior on an UDP socket in which case will a call to sendto() block? I'm interested essentially in the Linux behavior. For TCP I understand that congestion control makes the send() call blocking if the sending window is full but what about UDP? Does it even block sometimes or just let packets getting discarded at lower layers? This can happen if you filled up your socket buffer but it is highly operating system dependent. Since UDP does not provide any guarantee your operating system can decide to do whatever it wants when your socket buffer is full: block or drop. You can try to increase SO_SNDBUF for temporary relief. This can even depend on the fine tuning of your system for instance it can also depend on the size of the TX ring in the driver of your network interface. There are a few discussions about this in the iperf mailing list but you really want to discuss this with the developers of your operating system. OK but now the question is pushed just one step further: under which conditions will Linux let the socket buffer fill up? (as opposed to dropping packets out of it). This is unfortunately a very difficult question. Ok following your link I found linux-specific explanation on [this thread](http://www.mail-archive.com/iperf-users@lists.sourceforge.net/msg00113.html). So short simplified answer : _Linux do block sendto on a full sending buffer_.  This may be because your operating system is attempting to perform an ARP request in order to get the hardware address of the remote host. Basically whenever a packet goes out the header requires the IP address of the remote host and the MAC address of the remote host. 192.168.1.34 and AB:32:24:64:F3:21. Your ""block"" behavior could be that ARP is working. I've heard in older versions of Windows (2k I think) that the 1st packet would sometimes get discarded if the request is taking too long and you're sending out too much data. A service pack probably fixed that since then. Ok good answering element interesting ; but what I have interest in is mostly the blocking (or absence of blocking) related to congestion/full buffer. ARP is only used for locating hosts on the same subnet - usually the router. And the results are cached for subsequent sends. When an IP packet goes out it requires the IP address of the remote host and the MAC address of the **next hop**..."
340,A,"More TCP and POSIX sockets listen() and accept() semantics Situation: The server calls listen() (but not accept()!). The client sends a SYN to the server. The server gets the SYN and then sends a SYN/ACK back to the client. However the client now hangs up / dies so it never sends an ACK back to the server. The connection is in the SYN_SENT state. Now another client sends a SYN gets a SYN/ACK back from the server and sends back an ACK. This connection is now in the ESTABLISHED state. Now the server finally calls accept(). What happens? Does accept() block on the first faulty connection until some kind of timeout occurs? Does it check the queue for any ESTABLISHED connections and return those first? When the server gets the SYN and sends the SYN/ACK back the connection (from a server perspective) is in the SYN_RCVD state. It is the client who is in the SYN_SENT state which is not relevant for the discussion. no both after the server sends a SYN/ACK it too is in SYN_SENT. oh sorry - it seems like you're right. i might have to fix my code - thanks! Well what you're describing here is a typical syn-flood attack ( http://en.wikipedia.org/wiki/SYN_flood ) when executed more than once. When looking for example at: http://lkml.indiana.edu/hypermail/linux/kernel/0307.0/1258.html there are two seperate queues one syn queue and one established queue. Apparently it the first connection will remain in the syn queue (since it's in the SYN_RCVD state) the second connection will be in the established queue where the accept() will get it from. A netstat should still show the first in the SYN_RCVD state. Note: see also my comment it is the client who will be in the SYN_SENT state the server (which we are discussing) will be in the SYN_RCVD state.  You should note that in some implementations the half open connection (the one in the SYN_RCVD state) may not even be recorded on the server. Implementations may use SYN cookies in which they encode all of the information they need to complete establishing the connection into the sequence number of the SYN+ACK packet. When the ACK packet is returned with the sequence number incremented they can decrement it and get the information back. This can help protect against SYN floods by not allocating any resources on the server for these half-open connections; thus no matter how many extra SYN packets a client sends the server will not run out of resources. Note that SCTP implements a 4-way handshake with cookies built into the protocol to protect against SYN floods while allowing more information to be stored in the cookie and thus not having to limit the protocol features supported because the size of the cookie is too small (in TCP you only get 32 bits of sequence number to store all of the information). So to answer your question the user-space accept() will only ever see fully established connections and will have no notion of the half-open connections that are purely an implementation detail of the TCP stack.  You have to remember that listen() accept() et al are not under the hood protocol debugging tools. From the accept man page: ""accept - accept a connection on a socket"". Incomplete connections aren't reported nor should they be. The application doesn't need to worry about setup and teardown of sockets or retransmissions or fragment reassembly or ... If you are writing a network application covering the things that you should be concerned about is more than enough work. If you have a working application but are trying to figure out problems then use a nice network debugging tool tools for inspecting the state of your OS etc. Do NOT try to put this in your applications. If you're trying to write a debugging tool then you can't accomplish what you want by using application level TCP/IP calls. You'll need to drop down at least one level. I'm implementing listen() and accept() myself for a project so I have to know how they behave."
341,A,How to program an audio/video application on network? I want to make (for fun challenge) a videoconference application I have some ideas about this: 1) taking the audio/video streams (I don't know what an audio/video stream is) 2) pass this to a server that lets communicate the clients. I can figure out how to write a server(there are a lot of books and documentation about this) but I really don't know how to interact with the webcam and with the audio/video in general. I want some links book suggestions about the basics of digital audio/video expecially on programming. Please help me!!! I want to make it run on a Linux platform. Linux makes video grabbing really nice. As long as you have a driver that outputs the video stream to the /dev/video/v* channels. All you have to do is open up a control connection to the device [an exercise for the OP] and then read in the channel like a file [given the parameters set by the control connection. Audio should be the same way but don't quote me on it. BTW: Video streaming from a server is a very complex issue. You have to develop or use an existing protocol. You have to be very aware of networking delays and adjust the information sent (resize or recompress) to the client based on the link size between the client and the server.
342,A,Does BitTorrent support partial transfers? Can the BitTorrent protocol specify wanting the first 3% or first 5% of the file first? If not would adding such support be an improvement to the protocol? Update: so i guess if it is part of the protocol why the many clients out there do not use it? After 10 minutes 10% of the file is done but usually you cannot even preview 1% of the content... (depends on luck) Yes this is supported by the torrent protocol ktorrent for example uses it to provide preview chunks for different file types (e.g. mp3 avi or even pdf).  After 10 minutes 10% of the file is done but usually you cannot even preview 1% of the content... (depends on luck) This is because the client doesn't download the file in order (so that the transfer is more efficient when you've got lot's of peers each one has a different part of the file)  Azureus has an option to get the first and last blocks of a file first. But it's not 100% reliable since you can't control which peer will start to send data frist. See here for the protocol spec specifically: request: The request message is fixed length and is used to request a block. The payload contains the following information: So yes you can ask for a specific block (even a part of it). That should make it possible to request exactly the first 3% or 5% of a file independent of the block size. so it is actually part of the protocol? Yes it is. See my edit for details.  BitTorrent's purpose as a protocol is not for streaming media its purpose is to make a best effort at keeping all data of interest reliably available at all times. Using it for media streaming purposes is actually counterproductive to BitTorrent's goal. If it's not obvious to you why this is the case I suggest picking up Computer Networks by Tanenbaum before you go any further.  The Bittorrent protocol is designed around scarcity: the rarest chunks are distributed first to decrease the likeliness of no longer having all chunks in the swarm of peers. Because that would make it impossible to get the entire file. i see... what if there are hundreds of seeders? in that case the scarcity isn't that's crucial? It is: what if many peers exit the swarm at once (seeders or not)? By downloading the rarest chunks first the chances of not having all chunks in the swarm are reduced to the smallest possible minimum.
343,A,How can I perform network IO at the very end of a process' lifetime? I'm developing a DLL in C++ which needs to write some data via a (previously established) TCP/IP connection using the write() call. To be precise the DLL should send a little 'Process 12345 is terminating at 2007-09-27 15:30:42 value of i is 131' message over the wire when the process goes down. Unfortunately all the ways I know for detecting that the process is ending are apparently too late for any network calls to succeed. In particular I tried the following approaches and the write() call returned -1 in every case: Calling write() from the destructor of a global object. Calling write() from a callback function registered using atexit(). Calling write() from DllMain (in case the reason argument is DLL_PROCESS_DETACH). I know that this is not a safe thing to do but I'm getting a bit desperate. :-) I'm aware that a DLL can't detect any process shutdown (it might have been unloaded long before the process terminates) but since the shutdown data which the DLL needs to send depends on other code in the DLL that's acceptable. I'm basically looking for the latest moment at which I can safely perform network IO. Does anybody know how to do this? Where is Winsock being shut down using WSACleanup? You need to make sure that your I/O completes before this happens. You should be able to work out if this is happening by placing a breakpoint on the Win32 call in Winsock2.dll. Unload of DLLs is displayed in the output in the debug window. I verified that at least none of *my* code calls WSACleanup() before the write() call is performed. My suspicion is that some other code (maybe in some other DLL which is unloaded before my own DLL is unloaded) tears down all of the socket API before I get to flush out my data. Maybe the WinSock DLL itself is being unloaded before I get unloaded I don't know. @Frerich - see edit  I suggest taking option 3. Just do your DLL loading/unloading properly and you're fine. Calling write() should work I can't explain why it's not in your case. Is it possible that the call fails for a different reason that is unrelated? Does it work if you call your DLL function manually from the host app? @user198397 - the problem with this approach is that there is no guarantee that WInsock has not already been shutdown and/or detached from the process. Order of DLL release is not in the control of OP. @frerich needs first to identify a reliable point at which to make his sockets calls. http://www.microsoft.com/whdc/driver/kernel/DLL_bestprac.mspx documents a number of things that should not be done (or are destined to fail) from `DllMain`. This is because when Windows is shutting down a process DLLs can be unloaded in any order so it may be the case where `Ws2_32.dll` was unloaded before your DLL. If I read the `DllMain` documentation right only (some) functions from `kernel32.dll` (those which don't load other DLLs) can be safely called from within `DllMain`.  Why? Just close the socket. If that's the only close in the program which by your description it must be that tells the other end that this end is exiting and you can send the process ID information at the beginning instead of the end. You shouldn't do anything time-consuming or potentially blocking in an exit hook or static destructor. You're right. However I was unfortunately a bit imprecise: I need to log a bit more information than just the PID. I extended my question accordingly.  Consider monitoring the process from a separate watchdog process. Determining If a Process Has Exited: http://msdn.microsoft.com/en-us/library/y111seb2(v=VS.71).aspx Tutorial: Managing a Windows Process: http://msdn.microsoft.com/en-us/library/s9tkk4a3(v=VS.71).aspx Using a watchdog process is a good idea. I'll think about it a bit. Unfortunately the pages which your links point to only talk about process management of foreign processes (not the current one).  Consider to use Windows Job Objects. You main program (monitoring program which will use for example send()) can start child process suspended place it into a Job and then resume. Then it will run in the job object. You can register notification via SetInformationJobObject with JobObjectAssociateCompletionPortInformation. Then you will be notified if in the job will be created some child process and if some process inside of job will be ended. So you will be able to send all what you need from the monitoring process. If you debug a program in Visual Studio it uses also job objects to have control under your process and all child processes which you start. I successfully use the technique in C++ and in C#. So if you will have some problem with implementation I could post you a code example.
344,A,Capturing network status change event I am trying to get events when the internet connection is reestablished after it is lost. It is for a data transfer software that I am developing. If I lose the network during data transfer I would like to be notified when it is back and continue the transfer automatically. I can of course create a separate thread and check the network once in a while with a timer but maybe there is a better option out there. I am developing for windows mainly in C++ (not .net). I can also use wxwidgets (I use it for GUI) but I doubt it offers any related functionality. You might want to check out the System Event Notification Server (SENS) API http://msdn.microsoft.com/en-us/library/cc185680(VS.85).aspx I have not actually used it but it seems like it supplies the events your looking for. EDIT: WMI appears to have all the information you need about various network connectivity and state changes. It also has an asynchronous event model that can be used to get notifications. The trick is i suppose generating the proper WMI query to get the information you want. This blog looks like the right type of query and this MSDN explains how to handle the events asynchronously. Related: http://stackoverflow.com/questions/1085045/check-whether-internet-connection-is-available-with-c Looks like it is only for mobile platforms. Gotta look more. Edited answer to include suggestion on using WMI  This MSDN link gives a very detailed example of how to capture events on WMI with COM. The example doesn't actually capture network events - but I believe that if you plug the right query in it would work. (lots of code here so I'm not copying it into the answer) http://msdn.microsoft.com/en-us/library/aa390425%28v=vs.85%29.aspx And - with this built-in tool you can perform WMI queries without having to run the code around them: (run from command line) wbemtest.exe  I don't know which protocol you use and whether you can control the destination but in that case the destination can poll for a retry. The destination knows best what it has received so it can give the received number of bytes as offset for the retransmission. Thanks but I am able to continue transfer from where it was left successfully. But I would like to do that as soon as network / internet is available. So I should detect the network availability in an event based manner.
345,A,Faster way to communicate using TcpClient? I'm writing a client/server application in C# and it's going great. For now everything works and it's all pretty robust. My problem is that I run into some delays when sending packets across the connection. On the client side I'm doing this: NetworkStream ns = tcpClient.GetStream(); // Send packet byte[] sizePacket = BitConverter.GetBytes(request.Length); byte[] requestWithHeader = new byte[sizePacket.Length + request.Length]; sizePacket.CopyTo(requestWithHeader 0); request.CopyTo(requestWithHeader sizePacket.Length); ns.Write(requestWithHeader 0 requestWithHeader.Length); // Receive response ns.Read(sizePacket 0 sizePacket.Length); int responseLength = BitConverter.ToInt32(sizePacket 0); byte[] response = new byte[responseLength]; int bytesReceived = 0; while (bytesReceived < responseLength) { int bytesRead = ns.Read(response bytesReceived responseLength - bytesReceived); bytesReceived += bytesRead; } (Left out some exception catching etc.) The server does the opposite i.e. it blocks on NetworkStream.Read() until it has a whole request then processes it and sends a response using Write(). The raw speed of Write()/Read() isn't a problem (i.e. sending large packets is fast) but sending several small packets one after another without closing the connection can be terribly slow (delays of 50-100 ms). What's strange is that these delays show up on LAN connections with typical ping times <1 ms but they do not occur if the server is running on localhost even though the ping time would effectively be the same (at least the difference should not be on the order of 100 ms). That would make sense to me if I were reopening the connection on every packet causing lots of handshaking but I'm not. It's just as if the server going into a wait state throws it out of sync with the client and then it stumbles a bit as it reestablishes what is essentially a lost connection. So am I doing it wrong? Is there a way to keep the connection between TcpServer and TcpClient synchronised so the server is always ready to receive data? (And vice versa: sometimes processing the request from the client takes a few ms and then the client doesn't seem to be ready to receive the response from the server until it's had a few moments to wake up after blocking on Read().) Just out of curiosity did you try using a StreamReader? Any particular reason why your not using [WCF](http://msdn.microsoft.com/en-us/netframework/aa663324)? This framework has everything your trying to do and more built in. On the contrary IMO; WCF is hugely bloated and most people only use a tiny fraction of it. Raw sockets are often fine and generally significantly more efficient. I had a quick look at WCF and while I can see how it might be useful in a huge mesh of interconnected enterprise applications I'm really only after the ability to send simple byte arrays between two computers on a network. .NET has a bunch of very nice features (serialization DeflateStream XML stuff etc.) that extend this making it all more than adequate for my purposes. Anyeay **I FIXED IT NOW** see below. Jay: I tried StreamReader BTW no difference. But thx. @ReturningTarzan: I moved your update to your answer and +1'd both the question and the answer. Nice detective work. It turns out my server and client were not completely symmetrical after all. I had noticed but I didn't think it mattered at all. Apparently it's a huge deal. Specifically the server did this: ns.Write(sizePacket 0 sizePacket.Length); ns.Write(response 0 response.Length); Which I changed into this: // ... concatenate sizePacket and response into one array same as client code above ns.Write(responseWithHeader 0 responseWithHeader.Length); And now the delay is completely gone or at least it's no longer measurable in milliseconds. So that's something like a 100x speedup right there. \o/ It's still odd because it's writing exactly the same data to the socket as before so I guess the socket receives some secret metadata during the write operation which is then somehow communicated to the remote socket which may interpret it as an opportunity to take a nap. Either that or the first write puts the socket into receive mode causing it to trip up when it's then asked to send again before it has received anything. I suppose the implication would be that all of this example code you find lying around which shows how to write to and read from sockets in fixed-size chunks (often preceded by a single int describing the size of the packet to follow same as my first version) is failing to mention that there's a very heavy performance penalty for doing so. Just to explain the delays I believe you hit Nagles Algorithm in Windows sockets: Small Packets will be put in a Buffer to improve Network overall Performance. Each single Packet might see a 200ms Delay before reaching the network peer: http://support.microsoft.com/kb/214397/en-us http://msdn.microsoft.com/en-us/library/system.net.sockets.tcpclient.nodelay.aspx It's not really specific to Microsoft (apart from their own choice of algorithm). Any production TCP stack will favor sending large packets over small ones to reduce overhead. So when you write a small amount of data to the TCP stream it will be buffered in the hopes that some other writes will occur. A good TCP stack algorithm strives to balance low latency (by sending immediately) and high bandwidth (by using the largest packet size possible). Most stacks offer a way to either manually flush the buffer on command or for better performance specify no delay on a specific socket. Well thanks for that it was quite informative. I wish Microsoft were better at linking to excellent articles like that from MSDN. Or maybe I just need to know where to look. Either way I feel smarter now. Still a little odd that I was measuring ~100 ms delays nothing like the 200 ms timeout for the Winsock buffer but maybe I was measuring it wrong.
346,A,Detection of connection and speed of Cellular Network vs. 802.11 and Ethernet? C# .Net I am working on a program which will be used on mobile devices (Full Windows install) that may have 3G connectivity. If the particular device does have 3G connectivity we would like to take advantage of it. However the program should be able to prioritize communication based on what type of connection and speed are currently available. I am able to get a list of the network interfaces using System.Net.NetworkInformation.NetworkInterface but is there any good way to see if the interface is cell data wireless or regular ethernet as well as the available speed (particularly in the case of cell data)? This will list all of the network connections that are currently up. It should be a place to start. var interfaces = NetworkInterface.GetAllNetworkInterfaces() .Where(n => n.OperationalStatus == OperationalStatus.Up) .Select(n => new { Nic = n Speed = n.Speed }); Should throw in NetworkInterfaceType it should tell him if its cellular or not. http://msdn.microsoft.com/en-us/library/system.net.networkinformation.networkinterface.networkinterfacetype.aspx I am already able to get this info. Problem being that A) the speed seems to be the maximum speed for the interface and B) the InterfaceType for a cell device would be...? Ethernet is obvious but I don't know what type cell would appear as. @Totty: What device are you using? The network interface may only provide info that it has. You may have to enjoy the world of WMI or pInvoke to get more information. @six: I just showed those two values for an example. I figured he would be able to poke around and get whatever he wants. The device is a Panasonic CF-U1 with a Sierra Wireless MC5725 card. However this card may be replaced with other cards based on the customer's wireless carrier. What does that device report back under GetAllNetworkInterfaces()?
347,A,how to write a shell script to input wireless connection password from my password file? in ubuntu does it possible to write a script to read passwords from a txt file (one password per line) to try to connect to a wireless connection? assume i know the ESSID and MAC and the connection on my computer is eth1. what command can i use to input password and how to know the status? Thanks. To connect to a wlan net: iwconfig wlan0 essid NETWORK_ID key WIRELESS_KEY If the key isn't hex but ascii: iwconfig wlan0 essid NETWORK_ID key s:WIRELESS_KEY  In bash or whatever shell scripting : - to read passwords from a txt file: yes it's possible (while read..) - to try to connect to a wireless connection: yes it's possible (bring up eth n )
348,A,Does sendto() dst_addr arg matters if used on a raw socket with IP_HDRINCL set? The question is almost all in the title. I was wondering given that: - I use a raw socket (on GNU/Linux); - the option IP_HDRINCL is set so that I craft the IP headers by myself. As the dest IP addr is provided in the crafted IP header does the dst_addr argument still plays a role or is it totally useless & only here cause that's how the function prototype is ? No it does not matter. What you enter in the headers is where the packet would go.  The destination address is used to route the packet - it'll be the key that's used for a routing table lookup to determine the next hop address to send it to. It should usually be the same as the destination address you set in the header.
349,A,"Design for fastest page download I have a file with millions of URLs/IPs and have to write a program to download the pages really fast. The connection rate should be at least 6000/s and file download speed at least 2000 with avg. 15kb file size. The network bandwidth is 1 Gbps. My approach so far has been: Creating 600 socket threads with each having 60 sockets and using WSAEventSelect to wait for data to read. As soon as a file download is complete add that memory address(of the downloaded file) to a pipeline( a simple vector ) and fire another request. When the total download is more than 50Mb among all socket threads write all the files downloaded to the disk and free the memory. So far this approach has been not very successful with the rate at which I could hit not shooting beyond 2900 connections/s and downloaded data rate even less. Can somebody suggest an alternative approach which could give me better stats. Also I am working windows server 2008 machine with 8 Gig of memory. Also do we need to hack the kernel so as we could use more threads and memory. Currently I can create a max. of 1500 threads and memory usage not going beyond 2 gigs [ which technically should be much more as this is a 64-bit machine ]. And IOCP is out of question as I have no experience in that so far and have to fix this application today. Thanks Guys! If you need to address more than 2 GB either compile it as 64-bit (but your code may not be ready for it) or link the 32-bit app with /LARGEADDRESSAWARE (this should give you 4GB for a 32-bit app on a 64-bit system) I don't see any performance gain by using extra sockets. For a single CPU processor it has to ""share"" code execution between the various sockets dividing the performance. Same is true with too many threads. For serious performance handling you will need extra hardware support. You will need to convert the incoming (serial) data into multiple buffers of data (parallel). This will not necessarily boost your performance. However if you could download one page per physical connection that may boost your performance. Most of the bottleneck (IMHO) is receiving data packets and analyzing their destinations. The more of these analysts the faster your performance; although you may have performance hits when one or more directors wants to use the same memory area (two directors are downloading the same page). If you can have the hardware support download an entire page uninterrupted by a CPU that is the fastest performance you will see. ""That's just my opinion I could be wrong."" -- Dennis Miller.  First and foremost you need to figure out what is limiting your application. Are you CPU-bound IO-bound memory-bound network-bound ...? Is there locking contention between your threads? etc... Its impossible to say from your description. You will need to run your app in a profiler to get an idea where the bottlenecks are."
350,A,"Detecting end-user connection speed problems in Apache for Windows Our company provides web-based management software (servicedesk helpdesk timesheet etc) for our clients. One of them have been causing a great headache for some months complaining about the connection speed with our servers. In our individual tests the connection and response speeds are always great. Some information about this specific client : They have about 300 PC's on their local network all using the same bandwith/server for internet access. They dont allow us to ping their server so we cant establish a trace route. They claim every other site (google blogs news etc) are always responding fast. We know for a fact they have no intention to mislead us and know this to be true. They might have up to 100 PC's simulateneously logged in our software at any given time. They have a need to increase that amount up to 300 so this is a major issue. They are helpfull and colaborative in this issue we are trying to resolve for a long time. Some information about our server and software : We have been able to allocate more then 400 users at a single time without major speed losses for other clients. We have gone extensive lengths to make good use of data caching and opcode caching in the software itself and we did notice the improvement (from fast to faster) There are no database CPU or memory bottlenecks or leaks. Other clients are able to access the server just fine. We have little to no knowledge on how to do some analyzing on specific end-user problems (Apache running under Windows server) and this is where I could use a lot of help. Anything that might be related to Apache configuration would also be helpfull. While all signs points to it being an internal problem in this specific client network we are dedicating this effort to solve that too if that is the case but do not have capable or instructed professionals to deal with network problems (they do however while their main argument is that 'all other sites are fast only yours is slow') you might want to have a look at the tools from google ""page speed family"": http://code.google.com/speed/page-speed/docs/overview.html your customer should maybe run the page speed extension for you. maybe then you can find out what is the problem: http://code.google.com/speed/page-speed/docs/extension.html"
351,A,"Communicating to Applications on LAN I want to code an app with recreational purposes. This should be a Desktop app that detects itself running on other computers on the same LAN and communicates with them. By communication I mean that I should be able to pass anything from on to another. Please note that although I'm not asking for code (that would beat my purpose of course) I think some small snippets wouldn't hurt. In any case what I really want is the ""recommended procedure"" I mean what Microsoft recommends for this environment and documentation :) If this is your first ever network application you should start with sockets so you can understand and appreciate the issues involved in building an app like this from scratch. Effectively what you are trying to build is a peer-to-peer application. There are many things you'll want to think about and research here like: Peer discovery -- detecting other instances on the network and figuring out if they're still online or not Messaging -- designing an on the wire protocol for your app Security -- things like encryption preventing replay attacks etc. Eventually (once you've understood the underlying concepts) you will want to defer all of this to a framework rather than developing everything from scratch. I think WCF peer-to-peer will fit your needs.  Socket programming? Make them listen on a specific port and whenever application starts it should connect to that socket and the they are both connected. I am not sure how should application search for other application instances running on LAN. That is the idea :) but how can I listen to ports and connect to them?  As Taz said socket programming is the best choice but you can use other solutions like WCF (not recommended normally). About finding other instances on the LAN I could say you need some scanning algorithm. Most worms have such an algorithm for spreading. You should consider many networking issues like open ports on target LAN and firewall restrictions. Another point could using a TCP port over 5000 (up to 65535 but don't use famous ports) since ports under 5000 are subject to be used by operating system. Also to need your own protocol over TCP/IP for messaging/commanding and data transfer between peers. At the end it's a good idea to have a hand from a networking/security consultant in this kind of projects since most problems you encounter are not about programming but networking. I'm only coding this application to precisely learn all those issues you mention... I have no intention to build an application to deploy whatsoever. I'm just looking to widen my knowledge on network communications subject of which I know basically nothing... Good. So don't worry about networking issues. First you need a test environment. Simply use multiple virtual machines(Microsoft virtual machine is good choice) and start by writing to console applications communicating over `TCP/IP` and writing results on console one as server and another as client. Then you can write more complex ones (both server/client peers)"
352,A,"Ruby vs Perl in network programming Whats the difference between Ruby and Perl when it comes to networking. Which is better to use and why? @Nakilon: Later Max learned about Moose and other modern Perl features and decided that Perl is still better. If you already know one but not the other use the one you know. If you know neither choose a different language that you do know. If you know both you should be able to answer this question yourself. I want to learn one of this language and trying to find out which is better. Network programming is kinda important for me so i am trying to decide based on that. Perl is better designed for text processing and hard for later support and big projects. Ruby is rather universal and better for fast developing. Your question seems to be related to fast developing so... Networking is pretty minor in the grand scheme of things - try out both languages and see which you like better. What boat is better for floating? @mikerobi *...for floating through ice.* *... for floating in mountains.* I don't see a problem to make answers here. I think this is a really good question. it is only potential answers that are the problem Sad... question was closed not because it's impossible to answer it but because it's possible but potential answerer was not so fast as active closers ) I mean I think it's possible to make a full argumented nice answer but question should be closed because holywars and bad answers would be much more popular. @Nakilon - YOU may be a poor software developer and thus can't develope well designed Perl code. That's not Perl's fault. My systems in Perl are easy for later support are large projects and I develop fast. OMG just like I said... good quality can exist but it will not... sad. Note: Nakilon's first comment is a load of rubbish. Nakilon's comment reminds me of http://www.defectiveperlprogramming.com/?p=107 @justintime exactly. It's good but not for a broad audience. Too risky. We can now see it in comments. @DVK a hero read this http://www.codinghorror.com/blog/2007/12/nobody-cares-what-your-code-looks-like.html @justintime - the problem is not the question. It's the fact that such questions bring out a lot less thoughtful fact based answers and a lot more ""I can't feel good about MY languages of choice without baselessly bashing someone else's"" like Nakilon's comment above. What a marasm are you talking... 42 years old but looks like nervous scholar. Shame on you troll. @Nakilon: I read the codinghorror blog. So what? If you read the comments also there's a differing opinion. And Max changed his mind: http://use.perl.org/~chromatic/journal/33191 @runrig - that link is 404 now but I believe you meant to link to Max's blog post he made later called ""Ooh I made Coding Horror"" and was a rebuttal of sorts to BOTH his original assessment of Perl's suitability and to Jeff's blog. http://avatraxiom.livejournal.com/70947.html @Nakilon - specifically to quote from Max's follow-up post: **""Since the ""problems of Perl"" post I've had the opportunity to start a brand-new project in Perl ... also because I wanted to see if using all the modern techniques of Perl development could lead to well-architected code from the start...** ... (to be continued in next commen) ... ***I found that Moose is the best object system in any language I've ever used***. I find it weird that when I go to write in other languages again I'll probably think ""Gee I wish this had Moose."" I used to think ""I'm so glad this language isn't Perl"" (except when dealing with regular expressions which are so much easier to use in Perl than in other languages). But with Moose I just wish everything had Moose. Both languages are very similar and if you're just starting out then Ruby is probably easier to learn than Perl. But if have a specific area you are targeting then it probably comes down to which has the better libraries in that area. And in questions about breadth and quality of libraries Perl wins almost all battles because of the CPAN. Take a few minutes and search for the types of libraries you're interested in and compare that with what you'll find on Ruby Gems"
353,A,"unable to read tcp/ip headers i am getting this error:‘struct iphdr’ has no member named ‘ip_ttl’ same for other members too but not for protocol feild what is the solution to it?and y does it happen? PS:I saw this on various forums but couldn't get why is it able to access ipHeader->protocol and not others Could you post some code? Also isn't the field just 'ttl' and not 'ip_ttl'? Assuming you're using Linux try taking a look at /usr/include/linux/ip.h. That header file defines the structure: struct iphdr { #if defined(__LITTLE_ENDIAN_BITFIELD) __u8 ihl:4 version:4; #elif defined (__BIG_ENDIAN_BITFIELD) __u8 version:4 ihl:4; #else #error ""Please fix <asm/byteorder.h>"" #endif __u8 tos; __be16 tot_len; __be16 id; __be16 frag_off; __u8 ttl; __u8 protocol; __sum16 check; __be32 saddr; __be32 daddr; /*The options start here. */ }; As you can see the name of the field is ttl not ip_ttl. `ip_ttl` is from the BSD-compatible IP header declaration found in `/usr/include/netinet/ip.h`."
354,A,"Debugger message I do webservice requests with a UISegmentedControl when I change segmentedItem and request fast the application crashes with this message: [Session started at 2011-05-12 10:58:50 +0200.] Terminating in response to SpringBoard's termination. [Session started at 2011-05-12 11:06:31 +0200.] GNU gdb 6.3.50-20050815 (Apple version gdb-1516) (Fri Feb 11 06:19:43 UTC 2011) Copyright 2004 Free Software Foundation Inc. GDB is free software covered by the GNU General Public License and you are welcome to change it and/or distribute copies of it under certain conditions. Type ""show copying"" to see the conditions. There is absolutely no warranty for GDB. Type ""show warranty"" for details. This GDB was configured as ""--host=i386-apple-darwin --target=arm-apple-darwin"".tty /dev/ttys001 Loading program into debugger… Program loaded. target remote-mobile /tmp/.XcodeGDBRemote-239-58 Switching to remote-macosx protocol mem 0x1000 0x3fffffff cache mem 0x40000000 0xffffffff none mem 0x00000000 0x0fff none run Running… [Switching to thread 11779] [Switching to thread 11779] sharedlibrary apply-load-rules all continue Program received signal: “SIGKILL”. warning: Unable to read symbols for /Developer/Platforms/iPhoneOS.platform/DeviceSupport/4.3.3 (8J2)/Symbols/Developer/usr/lib/libXcodeDebuggerSupport.dylib (file not found). kill quit The Debugger has exited with status 0.(gdb) Does anybody have an idea of how can I fix this? Is this a problem because I'm not using NSOperationQueue? And if so any suggestions how I can fix this would be very welcomed. I can not find any memory leaks when running. Next time I ran it this message got logged: Program received signal: “EXC_BAD_ACCESS”. warning: Unable to read symbols for /Developer/Platforms/iPhoneOS.platform/DeviceSupport/4.3.3 (8J2)/Symbols/Developer/usr/lib/libXcodeDebuggerSupport.dylib (file not found). (gdb) Here is the code for starting the connection: Class header ServiceGetChildren: #import <Foundation/Foundation.h> @class Authentication; @class AttendanceReportViewController; @interface ServiceGetChildren : NSObject { Authentication *authentication; AttendanceReportViewController *attendanceReportViewController; NSString *username; NSString *password; NSMutableString *authenticationString; NSString *encodedLoginData; NSMutableData *responseData; NSMutableArray *childrensArray; } @property (nonatomic retain) NSString *username; @property (nonatomic retain) NSString *password; @property (nonatomic retain) NSMutableString *authenticationString; @property (nonatomic retain) NSString *encodedLoginData; @property (nonatomic retain) NSMutableData *responseData; @property (nonatomic retain) NSMutableArray *childrensArray; - (void)startService:(NSURL *)url :(NSString *)method withParent:(UIViewController *)controller; @end Class handling the request: #import ""ServiceGetChildren.h"" #import ""JSON.h"" #import ""Base64.h"" #import ""AttendanceReportViewController.h"" #import ""Authentication.h"" @implementation ServiceGetChildren @synthesize appDelegate; @synthesize username; @synthesize password; @synthesize authenticationString; @synthesize encodedLoginData; @synthesize responseData; @synthesize childrensArray; - (id) init { if ((self = [super init])) { } return self; } - (void)startService:(NSURL *)url :(NSString *)method withParent:(UIViewController *)controller { username = appDelegate.username; password = appDelegate.password; attendanceReportViewController = (AttendanceReportViewController *)controller; Authentication *auth = [[Authentication alloc] init]; authenticationString = (NSMutableString*)[@"""" stringByAppendingFormat:@""%@:%@"" username password]; encodedLoginData = [auth encodedAuthentication:authenticationString]; [auth release]; // Setup up the request with the url NSMutableURLRequest *request = [NSMutableURLRequest requestWithURL: url cachePolicy: NSURLRequestReloadIgnoringCacheData timeoutInterval: 20.0]; [request setHTTPMethod:method]; [request setValue:[NSString stringWithFormat:@""Basic %@"" encodedLoginData] forHTTPHeaderField:@""Authorization""]; NSURLConnection *connection = [[NSURLConnection alloc]initWithRequest:request delegate:self]; // Display the network indicator when the connection request started [UIApplication sharedApplication].networkActivityIndicatorVisible = YES; // Check that the NSURLConnection was successful and then initialize the responseData if(connection) { NSLog(@""Connection made""); responseData = [[NSMutableData data] retain]; } else { NSLog(@""Connection could not be made""); } } - (void)connection:(NSURLConnection *)connection didReceiveResponse:(NSURLResponse *)response { [responseData setLength:0]; } - (void)connection:(NSURLConnection *)connection didReceiveData:(NSData *)data { [responseData appendData:data]; } - (void)connectionDidFinishLoading:(NSURLConnection *)connection { NSLog(@""Connection finished loading.""); // Dismiss the network indicator when connection finished loading [UIApplication sharedApplication].networkActivityIndicatorVisible = NO; // Parse the responseData of json objects retrieved from the service SBJSON *parser = [[SBJSON alloc] init]; NSString *jsonString = [[NSString alloc] initWithData:responseData encoding:NSUTF8StringEncoding]; NSDictionary *jsonData = [parser objectWithString:jsonString error:nil]; NSMutableArray *array = [jsonData objectForKey:@""Children""]; childrensArray = [NSMutableArray arrayWithArray:array]; // Callback to AttendanceReportViewController that the responseData finished loading [attendanceReportViewController loadChildren]; [connection release]; [responseData release]; [jsonString release]; [parser release]; } - (void)connection:(NSURLConnection *)connection didFailWithError:(NSError *)error { NSLog(@""Connection failure.""); NSLog(@""ERROR%@"" error); // Dismiss the network indicator when connection failure occurred [UIApplication sharedApplication].networkActivityIndicatorVisible = NO; [connection release]; // Inform the user that the server was unavailable } - (void) dealloc { [attendanceReportViewController release]; [super dealloc]; } Class header for AttendanceReportViewController: #import <UIKit/UIKit.h> @class ServiceGetGroups; @class ServiceGetChildren; @class DetailViewController; @interface AttendanceReportViewController : UIViewController <UITableViewDataSource UITableViewDelegate> { ServiceGetGroups *serviceGetGroups; ServiceGetChildren *serviceGetChildren; DetailViewController *detailViewController; UISegmentedControl *segmentedControl; IBOutlet UIToolbar *groupsToolbar; NSMutableArray *btnArray; NSInteger index; IBOutlet UITableView *theTableView; IBOutlet UILabel *attendanceLabel; IBOutlet UILabel *abscentLabel; IBOutlet UILabel *totalLabel; NSMutableDictionary *selectRequestDictionary; NSMutableDictionary *selectNameDictionary; NSMutableArray *groupsArray; NSMutableArray *childrensArray; NSDictionary *childrensDictionary; NSURL *url; } @property (nonatomic retain) ServiceGetGroups *serviceGetGroups; @property (nonatomic retain) ServiceGetChildren *serviceGetChildren; @property (nonatomic retain) DetailViewController *detailViewController; @property (nonatomic retain) IBOutlet UIToolbar *groupsToolbar; @property (nonatomic retain) IBOutlet UITableView *theTableView; @property (nonatomic retain) IBOutlet UILabel *attendanceLabel; @property (nonatomic retain) IBOutlet UILabel *abscentLabel; @property (nonatomic retain) IBOutlet UILabel *totalLabel; @property (nonatomic retain) UISegmentedControl *segmentedControl; @property (nonatomic) NSInteger index; @property (nonatomic retain) NSMutableArray *btnArray; @property (nonatomic retain) NSMutableDictionary *selectRequestDictionary; @property (nonatomic retain) NSMutableDictionary *selectNameDictionary; @property (nonatomic retain) NSMutableArray *groupsArray; @property (nonatomic retain) NSMutableArray *childrensArray; @property (nonatomic retain) NSDictionary *childrensDictionary; @property (nonatomic retain) NSURL *url; - (void)setupSegmentedControl; - (void)requestGroups; - (void)requestChildren:(NSNumber *)groupId; - (void)loadGroups; - (void)loadChildren; @end Class that the Uses the UISegmentedControl: #import ""JSON.h"" #import ""AttendanceReportViewController.h"" #import ""CustomCellViewController.h"" #import ""DetailViewController.h"" #import ""ServiceGetGroups.h"" #import ""ServiceGetChildren.h"" #import ""Group.h"" #import ""Child.h"" @implementation AttendanceReportViewController @synthesize serviceGetGroups; @synthesize serviceGetChildren; @synthesize detailViewController; @synthesize segmentedControl; @synthesize groupsToolbar; @synthesize index; @synthesize btnArray; @synthesize theTableView; @synthesize attendanceLabel; @synthesize abscentLabel; @synthesize totalLabel; @synthesize selectRequestDictionary; @synthesize selectNameDictionary; @synthesize groupsArray; @synthesize childrensArray; @synthesize childrensDictionary; @synthesize url; #pragma mark - #pragma mark View lifecycle // The designated initializer. Override if you create the controller programmatically and want to perform customization that is not appropriate for viewDidLoad. - (id)initWithNibName:(NSString *)nibNameOrNil bundle:(NSBundle *)nibBundleOrNil { self = [super initWithNibName:nibNameOrNil bundle:nibBundleOrNil]; if (self) { // Custom initialization. } return self; } - (void)requestGroups { NSURL *groupsURL = [NSURL URLWithString:@""http://services/groups""]; [serviceGetGroups startService:groupsURL :@""GET"" withParent:self]; } - (void)requestChildren:(NSNumber *)groupId { NSLog(@""%@"" groupId); NSString *baseURL = @""http://services/group/""; NSString *method = [NSString stringWithFormat:@""%@%@"" groupId @""/children""]; NSString *theURL = [NSString stringWithFormat:@""%@%@"" baseURL method]; self.url = [NSURL URLWithString:theURL]; NSLog(@""%@"" self.url); [serviceGetChildren startService:self.url :@""GET"" withParent:self]; } - (void)loadGroups { // Retrieve a array with dictionaries of groups from ServiceGetGroups self.groupsArray = [[serviceGetGroups.groupsArray copy] autorelease]; // The array to hold segmentedItems for the segmentedControl representing groups buttons btnArray = [NSMutableArray arrayWithObjects: @""Alla"" nil]; for (NSDictionary *groupDict in groupsArray) { // Add each groups name to the btnArray as segmentedItems for the segmentedControl [btnArray addObject:[groupDict objectForKey:@""Name""]]; } // Create a new NSMutableDictionary with group names as keys and group id´s as values // used for reference to segementedControl items to make request to serviceGetChildren self.selectRequestDictionary = [NSMutableDictionary dictionary]; for (NSDictionary *dict in groupsArray) { [selectRequestDictionary setObject:[dict objectForKey:@""Id""] forKey:[dict objectForKey:@""Name""]]; } // Create a new NSMutableDictionary with group id´s as keys and group names as values // used for retrieving a groupName from a passed id self.selectNameDictionary = [NSMutableDictionary dictionary]; for (NSDictionary *dict in groupsArray) { [selectNameDictionary setObject:[dict objectForKey:@""Name""] forKey:[dict objectForKey:@""Id""]]; } [self setupSegmentedControl]; } - (void)setupSegmentedControl { // Setup the UISegmentedControl as groups buttons segmentedControl = nil; segmentedControl = [[UISegmentedControl alloc] initWithItems:btnArray]; segmentedControl.tintColor = [UIColor darkGrayColor]; segmentedControl.selectedSegmentIndex = 0; segmentedControl.autoresizingMask = UIViewAutoresizingFlexibleWidth; segmentedControl.segmentedControlStyle = UISegmentedControlStyleBar; segmentedControl.frame = CGRectMake(0 0 300 30); // Setup the target and actions for the segmentedControl [segmentedControl addTarget:self action:@selector(selectGroup:) forControlEvents:UIControlEventValueChanged]; // Add the UISegmentedControl as a UIBarButtonItem subview to the UIToolbar UIBarButtonItem *segmentedItem = [[[UIBarButtonItem alloc] initWithCustomView:segmentedControl] autorelease]; UIBarButtonItem *flexSpace = [[UIBarButtonItem alloc] initWithBarButtonSystemItem:UIBarButtonSystemItemFlexibleSpace target:nil action:nil]; NSArray *groupsButtons = [NSArray arrayWithObjects:flexSpace segmentedItem flexSpace nil]; [groupsToolbar setItems:groupsButtons]; [flexSpace release]; } - (void)loadChildren { // Retrieve a array with dictionaries of children from ServiceGetChildren self.childrensArray = [[serviceGetChildren.childrensArray copy] autorelease]; // TODO create seperate method - Setup compilation bar values int total = [childrensArray count]; totalLabel.text = [NSString stringWithFormat:@""%d"" total]; [theTableView reloadData]; } // Handles UIControlEventValueChanged for UISegmentedControl retreives the name and id for a selected group - (void)selectGroup:(UISegmentedControl *)theSegmentedControl { NSString *selectedGroup = [segmentedControl titleForSegmentAtIndex: [segmentedControl selectedSegmentIndex]]; NSNumber *selectedId = [selectRequestDictionary objectForKey:selectedGroup]; // Persist the selectedSegmentIndex when view switches to detaildView index = [segmentedControl selectedSegmentIndex]; // Request children based on the selected groupId [self requestChildren:selectedId]; } - (void)viewDidLoad { [self requestGroups]; [super viewDidLoad]; // Uncomment the following line to display an Edit button in the navigation bar for this view controller. //self.navigationItem.rightBarButtonItem = self.editButtonItem; } #pragma mark - #pragma mark Table view data source - (NSInteger)tableView:(UITableView *)tableView numberOfRowsInSection:(NSInteger)section { // Return the number of rows in the section. return [childrensArray count]; } // Customize the appearance of table view cells. - (UITableViewCell *)tableView:(UITableView *)tableView cellForRowAtIndexPath:(NSIndexPath *)indexPath { static NSString *CellIdentifier = @""CustomCell""; // Load from nib CustomCellViewController *cell = (CustomCellViewController *)[tableView dequeueReusableCellWithIdentifier:CellIdentifier]; if (cell == nil) { NSArray *topLevelObjects = [[NSBundle mainBundle] loadNibNamed:@""CustomCellView"" owner:nil options:nil]; for (id currentObject in topLevelObjects) { if ([currentObject isKindOfClass:[UITableViewCell class]]) { cell = (CustomCellViewController *) currentObject; break; } } } // Set up a children dictionary for easy retrieving specific values to display in the UITableView childrensDictionary = [childrensArray objectAtIndex:indexPath.row]; NSNumber *idForName = [childrensDictionary valueForKey:@""GroupId""]; NSString *name = [NSString stringWithFormat:@""%@ %@"" [childrensDictionary valueForKey:@""Firstname""] [childrensDictionary valueForKey:@""Surname""]]; NSString *group = [NSString stringWithFormat:@""%@"" [selectNameDictionary objectForKey:idForName]]; cell.childNameLabel.text = name; cell.groupNameLabel.text = group; cell.scheduleLabel.text = @""Schema""; cell.deviationLabel.text = @""Avvikelse""; return cell; } #pragma mark - #pragma mark Table view delegate - (void)tableView:(UITableView *)tableView didSelectRowAtIndexPath:(NSIndexPath *)indexPath { // Navigation logic may go here. Create and push the detailedViewController. if (detailViewController == nil) { DetailViewController *_detailViewcontroller = [[DetailViewController alloc] initWithNibName:@""DetailView"" bundle:nil]; self.detailViewController = _detailViewcontroller; [_detailViewcontroller release]; } childrensDictionary = [childrensArray objectAtIndex:indexPath.row]; NSNumber *idForName = [childrensDictionary valueForKey:@""GroupId""]; NSString *group = [NSString stringWithFormat:@""%@"" [selectNameDictionary objectForKey:idForName]]; [self.detailViewController initWithDetailsSelected:childrensDictionary:group]; [self.navigationController pushViewController:detailViewController animated:YES]; } #pragma mark - #pragma mark Memory management - (void)didReceiveMemoryWarning { // Releases the view if it doesn't have a superview. [super didReceiveMemoryWarning]; // Relinquish ownership any cached data images etc. that aren't in use. } - (void)viewDidUnload { // Relinquish ownership of anything that can be recreated in viewDidLoad or on demand. // For example: self.myOutlet = nil; segmentedControl = nil; } - (void)dealloc { [segmentedControl release]; [groupsArray release]; [childrensArray release]; [detailViewController release]; [super dealloc]; } @end I cannot post the picture from allocation tool but each time I push a SegmentedItem 13 categories gets increased by 1mb. Looks all good I think the bigger picture is the problem. You call this method when you switch the segment right? In which class is this code? You should reset your previous connection if it's already doing something. See you are releasing the connection only when it fails or finishes. Maybe you can outline the class a lil bit more? OK there we have a clue. I think you should post code in which you create your connection and the part where setLength: is sent. How can i find the part where setLength: is sent? Can you set a breakpoint at `[responseData appendData:data];` and step over it and see if it crashes there? @Nick thanks alot that you take some time to help me I have struggled with this so long now. I have now put the class that uses the connection above. Alright can you post the class header where `- (void)startService:(NSURL *)url :(NSString *)method withParent:(UIViewController *)controller` is inside? Sure posted above. It doesn´t crash but gets hit every time i push a segmentedItem. variable data has summary 136bytes first push then second push the varible is red and value is 2000bytes. If I step over it Do you think I have to look into threading? And pass the request somehow on individual threads? No give me a second :) It does hang when I set breakpoint at **[responseData setLength:0];** and UI gets unresponsive. Ok thanks a lot :) posted the other header file to. Here you go have a look at the answer and try to alter your code. Hope it'll work. However give me some comment on the answer if anything doesn't work out. [Enable NSZombie](http://cocoa-nut.de/?p=16) for your active executable and see if the console output shows something new. Try to get the EXC_BAD_ACCESS again. **responseData = [[NSMutableData data] retain];** is causing the error message **-[NSConcreteMutableData setLength:]: message sent to deallocated instance 0xa5c8f90** If move it to the init method it hangs on first change request. Any idea how I can deal with this? OK here we go. First of all I could not test the code so I need your feedback. The main problem with this class is you only have one instance which you reuse every time you hit the segmented control. This leads to creation of new instances of the NSURLConnection. So let's correct a bit in the header. I changed the NSString properties to copy. Have a look at this Q&A to know why however this is not important for the improvement just wanted to let you know. I've removed Authentication *authentication; looked like you don't use it. Added NSURLConnection as property so we keep a reference on it. Header file @class Authentication; @class AttendanceReportViewController; @interface ServiceGetChildren : NSObject { AttendanceReportViewController *attendanceReportViewController; NSString *username; NSString *password; NSMutableString *authenticationString; NSString *encodedLoginData; NSMutableData *responseData; NSMutableArray *childrensArray; NSURLConnection *connection; } @property (nonatomic copy) NSString *username; @property (nonatomic copy) NSString *password; @property (nonatomic retain) NSMutableString *authenticationString; @property (nonatomic copy) NSString *encodedLoginData; @property (nonatomic retain) NSMutableData *responseData; @property (nonatomic retain) NSMutableArray *childrensArray; @property (nonatomic retain) NSURLConnection *connection - (void)startService:(NSURL *)url :(NSString *)method withParent:(UIViewController *)controller; @end Next comes the Implementation file. I only altered the startService method and dealloc. If you declare properties use them :) Have a look at this Q&A for some explanations on synthesized properties. Always release what you own so I added release code in the dealloc. Cancel the previous request!!! If there is none the message is sent to nil which causes no harm. Implementation file - (void)startService:(NSURL *)url:(NSString *)method withParent:(UIViewController *)controller { self.username = appDelegate.username; self.password = appDelegate.password; [connection cancel]; attendanceReportViewController = (AttendanceReportViewController *)controller; Authentication *auth = [[Authentication alloc] init]; authenticationString = (NSMutableString *)[@"""" stringByAppendingFormat:@""%@:%@"" username password]; self.encodedLoginData = [auth encodedAuthentication:authenticationString]; [auth release]; // Setup up the request with the url NSMutableURLRequest *request = [NSMutableURLRequest requestWithURL:url cachePolicy:NSURLRequestReloadIgnoringCacheData timeoutInterval:20.0]; [request setHTTPMethod:method]; [request setValue:[NSString stringWithFormat:@""Basic %@"" encodedLoginData] forHTTPHeaderField:@""Authorization""]; self.connection = [[[NSURLConnection alloc] initWithRequest:request delegate:self] autorelease]; // Display the network indicator when the connection request started [UIApplication sharedApplication].networkActivityIndicatorVisible = YES; // Check that the NSURLConnection was successful and then initialize the responseData if (connection) { NSLog(@""Connection made""); self.responseData = [NSMutableData data]; } else { NSLog(@""Connection could not be made""); } } ... - (void) dealloc { [UIApplication sharedApplication].networkActivityIndicatorVisible = NO; [connection cancel]; [connection release]; [appDelegate release]; [responseData release]; [username release]; [password release]; [authenticationString release]; [encodedLoginData release]; [responseData release]; [childrensArray release]; [super dealloc]; } Hope this helps for the next steps. However I think we will have to work a bit on the solution until it's final. I do still think there is some other performance issues left for me to look into when I run allocation instrument I see that everytime I push a segmentedItem: Malloc 1.00kb Malloc 4.0kb Malloc 8.0kb Malloc 2.00kb Malloc 6.00kb Malloc 96bytes Malloc 5.50kb CFString CFString(store) CGColorTransform CGColorTransFormCache all increments for every click from between 1.19mb to 4.36mb. Wich raises the overall allocated bytes to 46mb quite fast. So I guess I have some more memory management to look into. I have no clue what it could be. But this time could it be in the ViewController perhaps. I dont want bother you to much because of taking your time already but perhaps anyone that sees this could give me a hint. The problem I also have is that the first segementedControl segment is a ""Everyone"" that is it needs to make all the request as each of the other segments do individual. This segment will be selected as default. For this issue do I have to look at NSOperationQueue? All suggestions are welcomed. You are welcome however I am off for today. I'll get back to you tomorrow and try to find a solution. @Silversnail Yes release the connection and the responseData in the dealloc only. Set it to nil in the `startService` method. Setting the property with `self.XXX = YYY` does two things: it releases the old value(if there is one) and retains the new one. Setting it to nil is a good way to release the current reference and assign nil to it. @Nick Great I will keep looking into this problem tonight and post it here if I make any progress. Thanks =) Very good glad to hear that. @Nick ok I get how it works now...thanks for all your help! Ok if I remove **[connection release];** from connectionDidFinishedLaunching makes the autorelease ok. And also by removing **[responseData release];** from same method. So the it doesn´t leak memoery anymore. Found a problem that this **self.connection = [[[NSURLConnection alloc] initWithRequest:request delegate:self] autorelease];** and this **self.responseData = [NSMutableData data];** terminates with following output. **-[NSURLConnection cancel]: message sent to deallocated instance 0x4d4faf0 and **-[NSConcreteMutableData release]: message sent to deallocated instance 0x4d57a50** If I remove autorelease on the connection and keep retain on responseData the app runs fine except for a lot of memory leaks related to CFNetwork and NSURLConnection. Do I set both to nil in the method instead? Like **self.connection = nil;** and **self.responseData = nil;** @Nick Thanks so much! I can not get it to crash anymore! I could not thank you enough for taking the time to help me out!"
355,A,"Does anyone know of any problems with using WCF to expose a SOAP interface for non .NET clients? Does anyone know of any problems with using WCF to expose a SOAP interface for non .NET clients? For example incompatibilities with other SOAP libraries? This is so that the SOAP interface can be exposed for third parties to integrate with our software. Some of the problem areas I've encountered with WCF: It generates WSDL that is split across multiple URLs. That is one part of the schema is at one URL another is at a different URL etc. The ""main"" WSDL URL (the one with just ""?WSDL"" after the service name) references the others via xsd:import elements. Many SOAP clients (eg pre-.NET Delphi) have enormous difficulty with this idiom. So you really have to ""flatten"" your WSDL in order to achieve interoperability in practice. One solution is given here. WCF doesn't generate XML namespaces the same way as say ASMX web services. WCF has a tendency to place any service or data contract into a namespace of its own choosing. Again some SOAP clients have difficulty with this. You can increase you interoperability level by adding an explicit namespace to your ServiceContract and DataContract attributes. Many SOAP clients won't handle faults as nicely as WCF clients. For example the proxy generation code won't create client-side objects for the faults declared in the WSDL. The faults will still be transmitted to the client of course but the client then has to do more work to figure out what kind of fault it was.  versions of the WS-* standards stack can also be an interoperability issue - for example the version of WS-Addressing (2003) supported by some java implementations eg Oracle BPEL is not supported by WCF which supports the later draft and 1.0 versions but not the earlier 2003 one  Generally everything works fine. It will obviously depend on the client you're using - not everyone implement SOAP properly. P.S. Could you please rephrase your question if you hope for more specific answer? It is a general question: I don't know what problems there could be but I would like to see if anyone has encountered any. Obviously I would like to know about any problems before I start coding! :) Hopefully clearer now :) again could you please be more specific?"
356,A,measuring response time through packets? good day! im developing an app in c++ and winpcap that will list all the URL accessed in the browser with its corresponding response time.. currently i can now track or monitor all accessed url through capturing and analyzing packets.. is there any way of measuring the response time of a web page to load from the request made to the server's response?.. any easy way? thanks.. You will have to keep track of the individual TCP connections between the browser and the server - that's just keeping track of the source/destination IP and port numbes in the packets you capture. Then you'll have to parse the HTTP in the captured packets and correlate HTTP requests with its response and take the time difference(which you'll get from the timestamps from pcap). This is non-trivial in the cases the HTTP request/responses spans several packets and certainly non-trivial if you also want to account for lost packets and retransmission.
357,A,"Game network physics collision How to simulating two client-controlled vehicles colliding (sensibly) in a typical client/server setup for a network game? I did read this eminent blog post on how to do distributed network physics in general (without traditional client prediction) but this question is specifically on how to handle collisions of owned objects. Example Say client A is 20 ms ahead of server client B 300 ms ahead of server (counting both latency and maximum jitter). This means that when the two vehicles collide both clients will see the other as 320 ms behind - in the opposite direction of the velocity of the other vehicle. Head-to-head on a Swedish highway means a difference of 16 meters/17.5 yards! What not to try It is virtually impossible to extrapolate the positions since I also have very complex vehicles with joints and bodies all over which in turn have linear and angular positions velocities and accelerations not to mention states from user input. @Jonas: This is an awesome question. I can't wait to see what people come up with. Play some Mario Kart Wii online and you'll get a feel for how lag affects its physics/collisions. Wow I have always wondered this. Out of all the questions I have favorited on SO this is definitely the best. +1 I wrote a 3d networked game for in my final year in college so I can sympathise (20 years ago and much simpler physics). The only solution I can think of involves time travel so I'm no good to you pal. I'd be interested in any solutions you find though. Sorry to answer with ""What not to try"" but I've never heard of a solution that doesn't involve predicting the outcome on client side. Consider a simplified example: Client A is stationary and watching client B's vehicle approach a cliff. Client B's vehicle is capable of reducing speed to 0 instantly and does so at the last possible moment before going over the cliff. If Client A is attempting to show Client B's state in real time Client A has no choice but to predict that Client B fell off the cliff. You see this a lot in MMORPGs designed such that a player's character is capable of stopping immediately when running full-speed. Otherwise Client A could just show Client B's state as the state updates come in but this isn't viable as Client A needs to be able to interact with Client B in real time in your scenario (I assume). Could you try simplifying the collision models so that extrapolation is possible for real time prediction? Maybe make your ""joints and bodies all over"" have processor-less-intensive physical models like a few cubes or spheres. I'm not too familiar with how to improve the efficiency of collision detection but I assume it's done by detecting collisions of models that are less complex than the visual models. Good idea but unfortunately very hard to do. All my models are already at a minimum an excavator will for instance have only three joins for the arm boom and hoe.  Perhaps the best thing that you can do is not so show the actual collision real time but give the illusion that things are happening in real time. Since the client is behind the server (lag) and the server needs to show the result of the collision perhaps what you can do client side is to show a flash or explosion or some other graphic to distract the user and buy enough time on the server side to calculate the result of the collision.. When you are finished with the prediction you ship it back to the client side for presentation. Isn't that solution too naive? What if the collision never happens on the server? People watch racing secretly hoping that they'll see a crash. - Let them eat cake! Voting up for the cool idea. Perhaps this could work by pretending that predicted collisions that never happened were actually collisions that were not severe enough to cause any type of feedback to the colliding objects.  Ross has a good point. You could simplify the model you use to detect collisions by abstracting it to some simpler volume (i.e. the rough boxy outline of the vehicle). Then you can do the predictions based on the simple volume and the detailed calculations on the exact volumes while you have the user distracted by the ""explosion"". It may not be perfect but would allow you to speed up your collision detection.  Regarding ""What not to try"". You are assuming that you need to predict perfectly but you are never going to find a perfect solution in a game with complex physics. An approximation is probably the best you can do (for example most commercial physics engines can cast a shape into the physics scene and return the first point of collision). For example I implemented some critical parts of the network physics for Mercenaries 2 under the guidance of Glenn (the blog poster you mentioned). It was impossible to push all of the necessary physics state across the wire for even a single rigid body. Havok physics gradually generates contact points each frame so the current ""contact manifold"" is a necessary part of the physics state to keep the simulation deterministic. It's also way too much data. Instead we sent over the desired transform and velocities and used forces and torques to gently push bodies into place. Errors are inevitable so you need a good error correction scheme. A more specific proposal (which has been suggested but worth repeating): Keep an alternate physics scene of network-relevant bodies with simplified shapes and vehicle physics model. Use it to simulate from the remote player's last known frame to the current local frame and report the approximate point of any high velocity collision. This would allow a cheap ""pretty good"" prediction which can be combined with error correction techniques. What I ended up doing was simulating the physical bodies as close as possible to the remote ends' but ""pushing"" (or rather ""sliding"") the graphical representation. Similar soluation but I think I got better determinism in this way.  What I eventually ended up doing was simply skipping prediction alltogether and simply doing this: Client has very much say about its own position Server (almost) only says anything about the owning client's position when a ""high energy"" collision has happened with another dynamic object (i.e. not static environment). Client takes meshoffset=meshpos-physpos when receiving a positional update from the server and then sets meshpos=physpos+meshoffset each frame and gradually decreases meshoffset. It looks quite good most of the time (in low latency situation) I don't even have to slerp my quaternions to get smooth transitions. Skipping prediction probably gives high-latency clients an awful experiance but I don't have time to dwell on this if I'm ever going to ship this indie game. Once in a while it's nice to create a half-ass solution that works good enough but best. ;) Edit: I eventually ended up adding the ""ownership"" feature that Glen Fiedler (the blogger mentioned in the question) implemented for Mercenaries 2: each client gets ownership of (dynamic) objects that they collide with for a while. This was necessary since the penetration otherwise becomes deep in high latency and high speed situations. That soluation works just as great as you'd think when you see the GDC video presentation can definitely recommend it! So the client can tell the server whatever it wants about its position? I don't see how that could be abused at all. Adding a slack contraint that checks that the client deviation isn't too big requires five minutes of coding. That will probably be enough for a long time to come.  I don't know of a perfect solution and I have a feeling that one does not exist. Even if you could accurately predict the future position of the vehicle you would be unable to predict the way the user will operate the controls. So the problem comes down to minimizing the negative effects of client/server lag. With that in mind I would approach this from the position of the principle of least astonishment (paraphrased from Wikipedia): In user interface design the principle of least astonishment (or surprise) states that when two elements of an interface conflict or are ambiguous the behaviour should be that which will least surprise the human user at the time the conflict arises. In your example each user sees two vehicles. Their own and that of another player. The user expects their own vehicle to behave exactly the way they control it so we are unable to play with that aspect of the simulation. However the user can not know exactly how the other user is controlling their vehicle and I would use this ambiguity to hide the lag from the user. Here is the basic idea: The server has to make the decision about an impending collision. The collision detection algorithm doesn't have to be 100% perfect it just has to be close enough to avoid obvious inconsistencies. Once the server has determined that two vehicles will collide it sends each of the two users a message indicating that a collision is imminent. On client A the position of vehicle B is adjusted (realistically) to guarantee that the collision occurs. On client B the position of vehicle A is adjusted (realistically) to guarantee that the collision occurs. During the aftermath of the collision the position of each vehicle can be adjusted as necessary so that the end result is in keeping with the rest of the game. This part is exactly what MedicineMan proposed in his answer. In this way each user is still in complete control of their own vehicle. When the collision occurs it will not be unexpected. Each user will see the other vehicle move towards them and they will still have the feeling of a real-time simulation. The nice thing is that this method reacts well in low-lag conditions. If both clients have low-latency connections to the server the amount of adjustment will be small. The end result will of course get worse as the lag increases but that is unavoidable. If someone is playing a fast-paced action game over a connection with several seconds worth of lag they simply aren't going to get the full exeperience. Hmm. Like my previous comment. My dumb luck led me to a similar suggestion but not nearly so eloquent. restated: ""Let them eat cake!"" Exactly! Your idea of hiding the post-collision calculations in the aftermath is very good. Step 5 should be dedicated to you :) The crux of my suggestion is to take small liberties with the apparent position of the other vehicle *before* the collision takes place. I hope I made that distinction clear in my answer. One problem with step 1 is that the server will have to extrapolate 600 ms ahead of current server time to get position of client B. To say the least 0.6 seconds is looong time in a fast-paced racing game. Plus this starts feeling weird when you take into consideration a 3rd vehicle ramming your 2 colliding (but at different locations on the players' screens) vehicles. This might work head-on collisions but is much too naive since most collisions are just minor bounces off one another. Consider for instance the truck having a jeep on top of its trailer. Or two race cars driving along side each other. Server simulation diverge too quickly. @AmigableClarkKant: depends on solution of course but roundtrip for 300 ms client is 600 ms so either use 2x.3 s or 1x.6 s extrapolation. The solution I went with keeps all clients lag+jitter ahead of server so no extrapolation is necessary as that is not feasible in games with advanced physics simulations. @JonasByström why 0.6 seconds not 0.3?  In addition to predicting on the client side where the other user might be and sending the collision information and how you handled it to the server what most mmo's do to deal with lag is they have the server run ""in the past"" as it were. Basically they buffer the recent inputs but only react to what happened .1sec in the past. This lets you ""peek into the future"" when you need to (ie when a collision is about to happen in your time frame you can look at the buffered input to see what will happen and decide if the collision is real). Of course this adds an extra layer of complexity to your program as you have to consider what data to send to your clients and how they should react to it. For example you could send the entire ""future"" buffer to the clients and let them see which possible collisions will actually happen and which won't. Everything runs ""in the past"" but with information about some of the immediate future events. It really doesn't have to be too far into the past certainly not far enough to really notice just enough to give you an uniform buffer to help predict collisions (and other things). This makes the server run even more behind the client. Or do you mean that both clients and server run ""in the past"" so that the normal pipe can be overridden by the more current but inaccurate ""approximation pipe""?"
358,A,What is a good way to connect multiple clients to the server? As there are several ways of connecting multiclients to the server such as: fork select threads etc. I would be glad if you could describe which is better to connect multiple clients to the server? Could you be more specific? Thanks ! See again the question I modified it ... One simple advice avoid use fork as it create a heavy weight process use threads instead that are much more lighter and shares the same memory space of the processes the created then. It depends on OS. On UNIX fork() and processes are efficient. Also address space shared between threads can cause very difficult to produce bugs. To sum up both processes and threads have pros and cons. thanks ! I know fork() is not supported on windows efficiently  if your program is running on windows. Avoid threads and processes completely and use the asynchronous model of socket programming. It's by far the most efficient (and easiest way) to do network communications. Thanks! Could you pls explain how to asynchronous sockets on windows and why threads are useless ...  I know in C that fd_select offered this polling of sockets ability. Not sure but i'm guessing that C# has a much nicer way of doing it. thanks! could you please explain what do you mean by << polling of sockets ability ..  Chapter 30 of Unix Network Programming introduce the alternatives of client/server design. The UNP book is the best start of network programming.  Take a look at the C10K page for a great overview and comparison of I/O frameworks and strategies. thanks for link. it is interesting Excellent webpage!  There are at least two options. Create a listen socket. Wait for a connection. Spawn a thread to handle the connection. or Create a listen socket and wait on it with select(). If the listen socket fd becomes active make a new connection and add the new fd to the select call. If a connection fd becomes active handle it. If you understand threads the first alternative might be a cleaner approach. The second approach needs some fd management (e.g. listen or connection) but can be done in a single thread. what do you mean by this <<the first alternative might be a cleaner approach>>? can you explain pls? If you understand threads and synchronization between threads the first approach can be simpler because you can use normal blocking reads and writes to the connection socket. The second approach requires that you use non-blocking reads/writes and handle partial data packets if you want to be able to handle all the clients simultaneously. I understand thread and work with them on different OSs. Sorry I think you are out of the topic ... the question is not about socket but is about connecting multiple clients to the server Sorry aybe I didn't make myself clear. I was describing two ways to have a server deal with multiple clients. I've used them both in the past. what do you mean by this <>? can you explain pls?  If you're using windows then the most efficient way to handle lots of clients is to build your server around asynchronous I/O and I/O completion ports. These allow you to service lots (10's of thousands) of clients with a small number (4?) threads in an efficient manner. Some people find the asynchronous nature of the I/O completion port model to be a little difficult to understand but I have a free C++ source code framework that you could download from here to get you started. If you're using managed code then all of the asynchronous socket APIs in the .Net framework use I/O completion ports 'under the hood'. The I/O completion port model scales well because it's an operating system primitive that was built from the ground up to be efficient; it knows if the threads associated with it are running or waiting and so can manage the number of threads that it allows to process 'completions' at the same time. It also uses the threads in a fifo order so that if there is little work to do then there are fewer wasted context switches as only the minimum number of threads will be used (rather than scheduling work items across all threads and causing all of their stacks to remain paged into memory). In contrast your own thread pool must be less efficient as you don't have the same level of knowledge of the threads as the IOCP has. Select often suffers from a limitation on the number of sockets that it can support which means that for true scalability you need to build some complex code on top of your select to be able to manage more than this number of clients. Using a thread per connection model is much less efficient as not only are new threads are started and stopped regularly but you have less ability to control the number of threads in the process at any one time - threads are relatively heavy-weight resources when you factor in their stack space etc. Once you have more threads than CPU cores you will be losing time in context switches (which IOCP often avoids completely). Finally forking a new process per connection is even more heavy-weight than the thread per connection model and personally I don't see any reason to consider this a valid option on a modern OS. Thanks a lot for the reply  Design some heartbeat loop which will go over all of your sockets and respond to their respective events. I recommend the server to be passive (clients will connect to it instead of server connecting to the clients) as it is normally done on the web. Therefore it will also have a listener socket. Heartbeat loop is a bad idea. As it implies a forced sleep between checks (maybe you have something else in mind in which case ignore this). There is already an API that waits for data on a set of sockets. Check out 'http://linux.die.net/man/2/select' And epoll on Linux kqueue on BSDs/MacOSX etc. :) @martin you wait on a set of sockets but then you go over all list and see which got events. this is out of the topic. It is not about socket  but about handling multiple clients
359,A,"Using TCP for real-time commands: Nagle arithmetic causes huge delays what should I do? I'm writing a socket server and flash game client. The game requires real-time commands such as movement and turning. It is important for these commands to be sent by the server to the client as soon as possible because the other clients will otherwise desynchronise a lot with the moving/turning client. This is an example of the problem caused by the Nagle arithmetic: Note: see the command table below if you wish to understand what these commands mean. First one is the ship I moved (moved forward + right forward was received but right not) The client sending commands: 84796: Sending data: 2#4 84796: Sending data: 2#2 84904: Sending data: 2#3 84904: Sending data: 2#0 86187: Sending data: 2#4 86188: Sending data: 2#2 86374: Sending data: 2#3 86404: Sending data: 2#0 The client receiving commands: 79244: Raw receive: 3#3#4$ 79244: New command: 3#3#4 79398: Raw receive: 3#3#2$3#3#3$3#3#0$ 79399: New command: 3#3#2 79399: New command: 3#3#3 79399: New command: 3#3#0 80635: Raw receive: 3#3#4$ 80635: New command: 3#3#4 80908: Raw receive: 3#3#2$3#3#3$3#3#0$ 80908: New command: 3#3#2 80908: New command: 3#3#3 80908: New command: 3#3#0 ""moment"" is a strange term that doesn't mean what I am trying to say but here it seems the amount of time in milliseconds after the previous command move forward send by client A (moment: 0) received by client B (moment: 0) turn right send by client A (moment: 0) received by client B (moment: 155) stop moving send by client A (moment: 108) received by client B (moment: 0) stop turning send by client A (moment: 0) received by client B (moment: 0) move forward send by client A (moment: 1283) received by client B (moment: 1236) turn right send by client A (moment: 1) received by client B (moment: 273) stop movement send by client A (moment: 186) received by client B (moment: 0) stop turning send by client A (moment: 30) received by client B (moment: 0) This is the command table corresponding with the commands: Client-> Server 2# (movement info) 0) now not turning 1) now turning left 2) now turning right 3) now not moving 4) now moving forward Server-> Client 3# (movement info) [shipId]# 0) now not turning 1) now turning left 2) now turning right 3) now not moving 4) now moving forward So what you can see is that the commands are totally desynched because of ""Nagle"". This causes the stop movement command to be received by other clients at the same time as the start movement command causing that player to not move at all. This is why I need these commands to be send in real-time as fast as possible by the TCP server. An easy fix would be to simply disable Nagle. However I have googled (note that his suggestion about tcp message partial is implemented in my system but has nothing to do with timing) a bit and noticed that people absolutely not recommend disabling Nagle. Is it true that I should not disable the Nagle arithmetic for this cause and should instead look for an other solution? Why (not)? Thanks in advance. - Tom If it needs to be as fast as possible you should use UDP. However note that TCP provides guarantee of delivery and order whereas UDP provides none. That point would seem to be a critically important point in this application so you would have to layer your own acknowledgement/retry command ordering mechanism on top. If you want to stick with TCP TCP__NODELAY will help. As your game advances you may end up requiring a stream of data that TCP will be well suited to without TCP_NODELAY. RFC1006 formalizes a mechanism for sending packets over a TCP stream. The OP did *not* say he needed it to be as fast as possible. He merely stated he wanted data sent immediately which is significantly different. All other things being equal turning of nagling is much better than jumping straight to UDP. That's confusing two people having the same name in one thread.  You want to use TCP_NODELAY to turn off the nagle algorithm for that socket. There are settings in modern OSes to disable it on a system-wide basis and that IS frowned upon but there's not reason you shouldn't do it with a socket that you KNOW you need low latency on. setsockopt(TCP_NODELAY)  If your goal is to have the ships end up doing the same thing on everyone's screen you're going to have to send position updates. You can't assume that just because client A did: Turn Left Wait 100ms Stop turning that Client B will see that exact same 100ms delay. The Nagle buffering just makes this more extreme - but there will always be jitter and varying delays. You really need to send something like: (I am at position X1 Y1 with heading Z1) Turn Left (I am at position X2 Y2 with heading Z2) Stop Turning So that the clients are continually resynchronising. I should have provided more background information. I am doing this so don't worry. The main issue I brought up in this thread has been solved. Thanks.  Note that your switch router ISP's swithcgear.... etc may all also nagle their black little guts out. The dirty little beasts can read packets so they take liberties and rewrite them sometimes.. Take a look at RTP ( which runs over UDP) for a more network -survivable approach.  Note that Windows implements delayed ACK as well as Nagle. This can give you an extra 200ms of delay on individual TCP messages. As far as I know you can't change this on Windows except via a registry key (and a hotpatch to enable the registry key in some cases)."
360,A,"Waiting for data via select not working I'm currently working on a project which involves multiple clients connected to a server and waiting for data. I'm using select and monitoring the connection for incoming data. However the client just continues to print nothing acting as if select has discovered incoming data. Perhaps I'm attacking this wrong? For the first piece of data the server does send it is displayed correctly. However the server then disconnects and the client continues to spew blank lines.  FD_ZERO(&readnet); FD_SET(sockfd &readnet); while(1){ rv = select(socketdescrip &readnet NULL NULL &timeout); if (rv == -1) { perror(""select""); // error occurred in select() } else if (rv == 0) { printf(""Connection timeout! No data after 10 seconds.\n""); } else { // one or both of the descriptors have data if (FD_ISSET(sockfd &readnet)) { numbytes = recv(sockfd buf sizeof buf 0); printf(""Data Received\n""); buf[numbytes] = '\0'; printf(""client: received '%s'\n""buf); sleep(10); } } } Essentially the problem is that select is returning 1 rather then 0 to the rv value. I don't understand why it would do this if there has been no new data received. it's because ""EOF"" or ""server closed connection"" is a readable event indicated by `recv()` returning zero bytes on the socket as `@Mark Wilkins` describes below. If I remember correctly you need to initialise set of fds before each call to select() because select() corrupts it. So move FD_ZERO() and FD_SET() inside the loop just before select(). Unfortunately the same issue will still occur (I tried that earlier -- my apologizes for not mentioning in my initial question)  Is this true when talking about your code? select(highest_file_descriptor+1 &readnet NULL NULL &timeout); In your simple example (with FD_ZERO and FD_SET moved inside the while(1) loop as qrdl said) it should look like this: select(sockfd+1 &readnet NULL NULL &timeout); Also - please note that when recv returns 0 bytes read it means that connection was closed - no more data! Your code is also buggy - when something bad happens on recv (it returns <0 when this happens) you will have serious trouble because something like buf[-1] may lead to unpredictable results. Please handle this case properly. While I respect the fact that you try to use the low-level BSD sockets API I must say that I find it awfully inefficient. That's why I recommend to you if possible to use ACE which is a very efficient and productive framework which has a lot of things already implemented when it comes to network programming (ACE_Reactor for example is something that makes it easier to do what you're trying to achieve here).  acting as if select has discovered incoming data. Perhaps I'm attacking this wrong? In addition to what was said before I'd like to note that select()/poll() do tell you not when ""data are there"" but rather that next corresponding system call will not block. That's it. As was said above recv() doesn't block and properly returns 0 what means EOF connection was closed by the other side. Though on most *nix systems in the case only first call of recv() would return 0 following calls would return -1. When using async I/O rigorous error checking is a must! And personally I would strongly suggest to use poll() instead. Unlike select() it doesn't destroy its arguments and works fine with high numbered socket descriptors.  When server closes the connection it will send a packet taking FIN flag to client side to announce that it no longer sends data. The packet is processed by TCP/IP stack at the client side and has no data for application level. The application level is notified to trigger select because something happened on the monitored file descriptor and recv() return 0 bytes because no data sent by server.  I think you need to check the result of recv. If it returns zero I believe it means the server has closed the socket. Also (depending on the implementation) you may need to pass socketdescrip+1 to select. If the server has closed the socket why would select return a positive value? Wouldn't a socket closure result in a timeout from the client's perspective? @BSchlinker: On a close (at least with stream based sockets) I believe a signal is sent from the socket being closed to the other end resulting in select returning 1. The recv call returning zero indicates the other end was closed. My guess (it's a guess) on why select would continue to return >0 is that the ""signal"" still exists indicating the other end is closed. The recv apparently does not remove that signal (I'm misusing the term signal in this case a bit). Thanks for the explanation Mark -- I understand the overall gist now. This is correct. The reason is that `select` returning the socket as ""readable"" just means ""a `read()` / `recv()` will not block"" which is the case when the connection has been closed by the other end."
361,A,"Library issue in the project I am getting very strange errors and i am not able to understand what these errors actually mean when build my project in visual studio it gives me the following errors can anybody tells me what actually these error means i though that there are some configuration issues i am doing socket programming and network programming. Here are the bunch of error your help weill be highly appreciated.... C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\Microsoft.CppBuild.targets(9905): warning MSB8012: TargetPath(E:\Study\FWIF\demola\ext-libs\libcommoncpp2-1.6.0\w32\Debug\ccgnu2.dll) does not match the Linker's OutputFile property value (E:\Study\FWIF\demola\ext-libs\libcommoncpp2-1.6.0\w32\Debug\CapeCommon14.dll). This may cause your project to build incorrectly. To correct this please make sure that $(OutDir) $(TargetName) and $(TargetExt) property values match the value specified in %(Link.OutputFile). 1>C:\Program Files (x86)\MSBuild\Microsoft.Cpp\v4.0\Microsoft.CppBuild.targets(9925): warning MSB8012: TargetName(ccgnu2) does not match the Linker's OutputFile property value (CapeCommon14). This may cause your project to build incorrectly. To correct this please make sure that $(OutDir) $(TargetName) and $(TargetExt) property values match the value specified in %(Link.OutputFile). 1> Creating library E:\Study\FWIF\demola\ext-libs\libcommoncpp2-1.6.0\w32\Debug\CapeCommon14.lib and object E:\Study\FWIF\demola\ext-libs\libcommoncpp2-1.6.0\w32\Debug\CapeCommon14.exp 1>socket.obj : error LNK2019: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct sockaddr const *)const "" (?isMember@IPV4Cidr@ost@@QBE_NPBUsockaddr@@@Z) referenced in function ""public: bool __thiscall ost::IPV4Cidr::operator==(struct sockaddr const *)const "" (??8IPV4Cidr@ost@@QBE_NPBUsockaddr@@@Z) 1>in6addr.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct sockaddr const *)const "" (?isMember@IPV4Cidr@ost@@QBE_NPBUsockaddr@@@Z) 1>inaddr.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct sockaddr const *)const "" (?isMember@IPV4Cidr@ost@@QBE_NPBUsockaddr@@@Z) 1>peer.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct sockaddr const *)const "" (?isMember@IPV4Cidr@ost@@QBE_NPBUsockaddr@@@Z) 1>simplesocket.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct sockaddr const *)const "" (?isMember@IPV4Cidr@ost@@QBE_NPBUsockaddr@@@Z) 1>socket.obj : error LNK2019: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct in_addr const &)const "" (?isMember@IPV4Cidr@ost@@QBE_NABUin_addr@@@Z) referenced in function ""public: bool __thiscall ost::IPV4Cidr::operator==(struct in_addr const &)const "" (??8IPV4Cidr@ost@@QBE_NABUin_addr@@@Z) 1>in6addr.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct in_addr const &)const "" (?isMember@IPV4Cidr@ost@@QBE_NABUin_addr@@@Z) 1>inaddr.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct in_addr const &)const "" (?isMember@IPV4Cidr@ost@@QBE_NABUin_addr@@@Z) 1>peer.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct in_addr const &)const "" (?isMember@IPV4Cidr@ost@@QBE_NABUin_addr@@@Z) 1>simplesocket.obj : error LNK2001: unresolved external symbol ""public: bool __thiscall ost::IPV4Cidr::isMember(struct in_addr const &)const "" (?isMember@IPV4Cidr@ost@@QBE_NABUin_addr@@@Z) 1>E:\Study\FWIF\demola\ext-libs\libcommoncpp2-1.6.0\w32\Debug\CapeCommon14.dll : fatal error LNK1120: 2 unresolved externals 2>------ Build started: Project: buffer Configuration: Debug Win32 ------ 2> buffer.cpp 2>e:\study\fwif\demola\ext-libs\libcommoncpp2-1.6.0\demo\buffer.cpp(41): fatal error C1083: Cannot open include file: 'cc++/buffer.h': No such file or directory ========== Build: 0 succeeded 2 failed 0 up-to-date 0 skipped ========== I formatted the build errors by selecting and hitting the code icon (101010). Same goes for code included in questions. Easier to read now. For the errors in project 1 it looks like you have to add a library to the build to resolve the function call ost::IPV4Cidr::isMember(struct sockaddr const *)const. Possibly you are compiling with the wrong calling conventions for a library that you already included. For the compile error in project buffer you need to add the include path for that file cc++/buffer.h to the project."
362,A,Is it possible to get speed of wifi of iPhone device programmatically Is it possible to find out the speed of internet connectivity via wifi or 3G connection in iPhone or iPad device. We can find the device connected to 3G or wifi programmatically that I know and already implemented in the project. Now I need to display the information about the internet speed of the wifi as well as 3G connection of iPhone device depend upon the connection it uses. Can anyone help me to find out solution for this problem. You need to download/upload an file to get the speed. And you can display the speed after that by knowing the size of the file and the amount of time that took to download/upload the file. Thanks for your answer. Here we need to get the speed for the statical purpose only. But the approach you suggested is something that we need to wait for download/upload and then only we can know the answer. If you have any other solution please let me know. You can do this in backThread with an small file like 4k.
363,A,"Error on MulticastSocket.joinGroup() I'm trying to execute a simple example of Multicast sockets on Java.  MulticastSocket s = new MulticastSocket(6789); InetAddress group = InetAddress.getByName(""230.1.1.1""); s.joinGroup(group); This code generates the error: IP_ADD_MEMBERSHIP failed (out of hardware filters?) Any idea? Please show the stack trace rather than your interpretation of it. Some Windows machines can experience this when the DHCP Media Sense feature is enabled (it is by default). To address this you may need to disable HDHCP media sensing as described here: http://support.microsoft.com/kb/239924. It may also happen if the network interface does not support multicast. VPN interfaces are notorious for this. Also try to disable TCP/IP filtering: Local Area Connection Properties > Internet Protocol > Properties > Advanced > Options > Properties > Disable ""Enable TCP/IP Filtering""."
364,A,"blocking sockets and select i'm playing around with sockets and I'm having some doubts about using blocking sockets with select.Let's assume i'm only tracking potential readers. Am I right in thinking that select will go through the first socket in the list and if some data is available it will return and if not it will block until select's timeout expires? When will the other sockets in the read list be checked by select? Let me illustrate with a python example for simplicity: servsock = socket.socket(socket.AF_INET socket.SOCK_STREAM) servsock.bind(("""" 2500)) servsock.listen(15) servsock.setblocking(1) readlist = [servsock] while 1: (sread swrite sexc) = select.select(readlist [] [] ); for sock in sread: #received a connect to the server socket if sock == servsock: (newsock address) = servsock.accept() newsock.setblocking(1) print ""I got a connection from "" address readlist.append(newsock) newsock.send(""you're connected to the select server"") else: recv_msg = sock.recv(5) if recv_msg == """": (host port) = sock.getpeername() print ""Client %s:%s closed the connection"" % (hostport) sock.close() readlist.remove(sock) else: (host port) = sock.getpeername() print ""client %s:%s sent: %s ""% (hostportrecv_msg) Since they're blocking sockets will select always block when testing to see if the socket in the list has data to read? The select() C API returns a list of all sockets that are currently readable. Assuming python's wrapper does the same sread will contain multiple elements if multiple sockets are readable. A blocking socket will only block if there is no data present so after a select() call indicated there is data you can assume that one call to recv will not block. so in other words select works the same way for blocking and non-blocking sockets? Yes it only tells you whether data is available not whether reads would block. That reads generally do not block if data is available even if it means returning less data than was asked for is a property of `recv()`.  select() will block until one of the sockets has data. It doesn't check only the first one it checks everything in your readlist variable. In for sock in sread: it iterates through all the sockets that have data to be read."
365,A,"Displaying UIActivityIndicator while performing a SYNCHRONOUS download I am downloading XML to populate an array used to build UITableView. Until I'm informed otherwise I believe I have to completely download the array before I can display it in the table (also it's text only and extremely small so it downloads in a reasonable time on the slowest possible connection). It does take about 3-5 seconds at it's slowest so it would be nice to display the activity indicator in the status bar while it downloads. I make the call to... [[UIApplication sharedApplication] setNetworkActivityIndicatorVisible:YES]; ...before I do ANYTHING (then turn it off once I've done EVERYTHING) but it just pops and quits in about the minimum amount of milliseconds that make it visible to the human eye. Any suggestions as to why I'm having this experience? Thanks Z@K! The easy answer for me was GCD Grand Central Dispatch. I barely had to modify my code at all... My code started as this... self.table_array = [self.webQuery downloadAndParseXMLForTable]; [(UITableView *)self.view reloadData]; *webQuery is a custom object that downloads and parses xml data from the web. **downloadAndParseXMLForTable is a custom method that synchronously downloads and parses an XML file then returns an (NSArray *) object to support a table view. The modified code below shows the ONLY changes I had to do to adopt GCD and keep my UI responsive. dispatch_queue_t table_download_queue = dispatch_queue_create(""com.yourcompany.queuename"" NULL); dispatch_async(table_download_queue ^{ self.table_array = [self.webQuery downloadAndParseXMLForTable]; dispatch_async(dispatch_get_main_queue() ^{ [(UITableView *)self.view reloadData]; }); }); dispatch_release(table_download_queue); That's it! I hope this helps others in my predicament... Cheers Z@K! WARNING: At the WWDC 2010 it was mentioned that GCD cannot currently support SECURE transmissions. I don't remember the details but the speaker Quinn was very adamant about it. I believe the process he suggested required NSOperation...  Synchronous downloads are performed on the calling thread and blocks the thread until it is complete which is probably done on the same thread as your UI. As the download is blocking the thread till its complete you either won't see the activity indicator or it will display and not move till the download is complete. You will have to put the synchronous download on a separate thread or use NSURLConnection:initWithRequest (which is multithreaded) to be able to have the App respond as expected. If I put the download on a separate thread how would I make my table view wait for the completed download so it can/will display accurate information? Whole heap of ways if you are using synchronous downloads have a call back to your main thread once the download is complete to remove the activity indicator and either [tableview reloadData] or actually load the UITable at that instance using initWithFrame. The easiest way and I think the way most people do it using NSURLConnection:initWithRequest and then in the connectionDidFinishLoading:connection delegate method hide the activityIndicator and reload the table. Can you expand on how would you choose to implement the callback? I like the idea of downloading the data asynchronously I just don't know how to make the table behave ""normally"" without blocking the main thread. The answer http://stackoverflow.com/questions/2124421/are-there-complete-examples-that-make-use-of-all-the-nsurlconnection-delegate-met will work for you. basically -(void)load: is where you initiate your activity indicator and connectionDidFinishLoading: is where you remove it and load / reload your table view. -(void)load is a custom one perhaps place whats in it in your viewDidLoad method all the rest including connectionDidFinishLoading are delegate methods and the names must remain the same. My program is currently utilizing NSURLConnection and I have it able to run a synchronous request or an asynchronous request based on an indicator. The example you linked only works when you have ""self"" as the delegate for the connection. I'm not set to run this way I have a dedicated object that handles the download request for several tableviews so basically connectionDidFinishLoading: has no reference to the calling table view and cannot ask it to reload. I suppose the easiest thing to do is to pass it a reference but that seems sloppy to me. Do you have any thoughts on this? You can pass the delegate of the calling class to the object that calls the NSURLConnection. You can then have the delegate methods in the calling class rather than the other object or you can respond to two different objects depending on the response of NSURLConnection but this is getting complex. It would be easier just to create a separate thread and call the synchronous resource and then respond back to the main thread. Your code and the required answer is becoming a bit too complex for a simple question answer thread though. Either way your synchronous request is blocking your UI"
366,A,"Possible methods to send the output of a PHP-invoked .exe program (that runs as a separate process not in PHP) back to the iPhone client My iPhone client app uploads a data to the server which runs on PHP. There is a code to invoke a .exe program on the server side on PHP. The .exe program will take the uploaded data and run on a process on its own. That means the PHP execution will end without waiting for the .exe program to finish. After the .exe program finished processing the uploaded data and have an output I want this output to be sent back to the iPhone. Normally if we call the .exe program to be run inside the php without making it a seperate process we have to wait for the program to finish and we can send the output back to the iPhone client. By running the .exe program as a seperate process it is impossible to send the data back via PHP that invokes the .exe program. The question is if we have the .exe program running on a seperate process rather than on the PHP script what are the possible methods to send the output back to the iPhone client? You're going to have a hard time reconnecting with the iPhone once you've severed the connection. It may be out of coverage it may have changed IP address ...... Your best bet is to have the iPhone reconnect back to the server and poll for it's information.  I would say this is a perfect case for an AJAX snippet frequently polling data from say a text file the .exe writes its status in. The upload script you call could return a unique identifier of some sort to the uploading client. Using that identifier the client would poll the exe's status (e.g. ""does the output file xyz already exist?"") until it gets positive feedback.  You could do this by using Apple's Push Notification service but that's probably overkill unless you think the data processing is going to take a long time and/or you want to update the app icon when the processing is done even if the app isn't running. Do you expect the user to just be patiently waiting for the result or are they going to fire off the data and check back later? If it's only going to take a couple of seconds you could just have the iPhone app poll for the result after waiting a little while (while displaying a progress indicator).  That's a need problem you've outlined. Let me explain a couple of ideas. First of all if you terminate the initial upload request the only resonable way to check for it being done is to poll every few seconds from the iPhone. Send a request to ""get-update.php"" every 5 seconds to see if you have data. By using $_SESSION you should be able to store a token that will identify the data when it has finished processing. Regarding the actually process you may be able to accomplish that in a number of ways. One is to do a fairly standard double-fork detaching the child process from the parent so it will continue after the parent exits. Another (recommended) would be to author a backend server process that would watch your database for requests fetch them process them and update the database. So when the inital upload script actually uploads the data have PHP put it in the database store the record ID in $_SESSION and return to the user. The back end process will notice that there is a record to process read the data call the executable and update the database with the result. The get-update.php script will read $_SESSION for the record id and check the database if the data has been processed (or what the status is). If you do not have the ability to run a background process and you are constrained to using PHP you could do the double-fork magic and fork of another PHP process to do the database read / exe / database update. Feel free to comment with questions. You need (a) a good way to pass the data to the program and (b) a good way to get the data back."
367,A,"How to simulate network failure for test purposes (in C#)? I'm building what could be called the DAL for a new app. Unfortunately network connectivity to the database is a real problem. I'd like to be able to temporarily block network access within the scope of my test so that I can ensure my DAL behaves as expected under those circumstances. UPDATE: There are many manual ways to disable the network but it sure would be nice if I could enable/disable within the test itself. Might want to check out this question on network tools to simulate slow connections: http://stackoverflow.com/questions/1094760/network-tools-that-simulate-slow-network-connection For the time being I'm just ""disabling"" the network by setting a bogus static IP as follows: using System.Management; class NetworkController { public static void Disable() { SetIP(""192.168.0.4"" ""255.255.255.0""); } public static void Enable() { SetDHCP(); } private static void SetIP(string ip_address string subnet_mask) { ManagementClass objMC = new ManagementClass(""Win32_NetworkAdapterConfiguration""); ManagementObjectCollection objMOC = objMC.GetInstances(); foreach (ManagementObject objMO in objMOC) { if ((bool)objMO(""IPEnabled"")) { try { ManagementBaseObject setIP = default(ManagementBaseObject); ManagementBaseObject newIP = objMO.GetMethodParameters(""EnableStatic""); newIP(""IPAddress"") = new string[] { ip_address }; newIP(""SubnetMask"") = new string[] { subnet_mask }; setIP = objMO.InvokeMethod(""EnableStatic"" newIP null); } catch (Exception generatedExceptionName) { throw; } } } } private static void SetDHCP() { ManagementClass mc = new ManagementClass(""Win32_NetworkAdapterConfiguration""); ManagementObjectCollection moc = mc.GetInstances(); foreach (ManagementObject mo in moc) { // Make sure this is a IP enabled device. Not something like memory card or VM Ware if ((bool)mo(""IPEnabled"")) { ManagementBaseObject newDNS = mo.GetMethodParameters(""SetDNSServerSearchOrder""); newDNS(""DNSServerSearchOrder"") = null; ManagementBaseObject enableDHCP = mo.InvokeMethod(""EnableDHCP"" null null); ManagementBaseObject setDNS = mo.InvokeMethod(""SetDNSServerSearchOrder"" newDNS null); } } } } how do you reset it?  Look for a WAN simulator that will allow you to restrict bandwidth (and cut it off completely) I always find it interesting to see how the user experience changes when my apps are run in a bandwidth restricted environment. Look here for some information.  Write a wrapper to the network class connectivity class you're using (e.g. WebClient) with an on-off switch :) Either that or block your application in the firewall. This is a really interesting proposition. Do you know of any way to handle this non-intrusively? (i.e. without modifying the code?) Any such wrapper for Win32 networking API?  Probably not helpful for simulating ""real"" network issues but you could just point your DB connection string to a non-existent machine while within the scope of your test. I agree that a simple enable/disable doesn't get you very far in dealing with ""real"" network issues but it's a starting point for testing what happens to the app when connectivity is down.  There is a tool you can use for simulating High Latency and Low Bandwidth in Testing of Database Applications as explained in this blog entry.  Try blocking the connection with a firewall midway through the session maybe? I like the wrapper idea as well but thats kind of abstracting the problem and you prolly might not get exact real world behavior. Also inserting the wrapper layer and then removing it may be more trouble than its worth. Edit: Run a script that turns the Network adapter on/off randomly or at set intervals?  Depends on what particular network problem you wish to simulate. For most folks it's as simple as ""server unreachable"" in which case you'd just try to connect to a non existent server. Be careful though because you want something that is routable but does not answer. Trying to connect to dkjdsjk.com will fail immediately (DNS lookup) but trying to connect to www.google.com:1433 will (probably) time out due to a firewall - which is how your app will behave when your DB server is down.  Just found an alternative that allows to directly close TCP connections: http://lamahashim.blogspot.ch/2010/03/disabling-network-using-c.html It is based on Windows IP Helper API (uses DllImport): http://msdn.microsoft.com/en-us/library/windows/desktop/aa366073(v=vs.85).aspx  If you are trying a complete network outage for your application unplugging the network cable will work. Sometimes you might have a data access layer with multiple data sources (on different machines) in which case you can simulate an exception in your tests with a Mock Framework like Rhino Mocks. Here is some pseudo-code that you may have in your test void TestUserDBFailure() { // ***** THIS IS PSEUDO-CODE ******* //setting up the stage - retrieval of the user info create an exception Expect.Call(_userRepository.GetUser(null)) .IgnoreArguments() .Return(new Exception()); // Call that uses the getuser function see how it reacts User selectedUser = _dataLoader.GetUserData(""testuser"" ""password""); }  Use mock objects to create configurable destructible versions of the real thing--in this case the database."
368,A,"Extract IP from connection that listen and accept in socket programming in Linux in c From this code I open a socket (server) and listen for incoming request when someone request accept this and save file descriptor (FD) in newsockfd how to extract IP for requester ? int sockfd newsockfd portno clilen; portno = 8090; clilen = 0; pthread_t serverIn; struct sockaddr_in serv_addr cli_addr; sockfd = socket(AF_INET SOCK_STREAM 0); if (sockfd < 0) { perror(""ERROR opening socket""); } bzero((char *) & serv_addr sizeof (serv_addr)); serv_addr.sin_family = AF_INET; serv_addr.sin_port = htons(portno); serv_addr.sin_addr.s_addr = INADDR_ANY; if (bind(sockfd (struct sockaddr *) & serv_addr sizeof (serv_addr)) < 0) { perror(""ERROR on binding""); } listen(sockfd 5); clilen = sizeof (cli_addr); newsockfd = accept(sockfd (struct sockaddr *) & cli_addr &clilen); You already have it in cli_addr don't you? @zneak I need IP for client that connect to me yes that is what `accept()` is giving back to you in `cli_addr`! You only need to format it - look at answers below. The API is described in the manual pages. You can either browse them from the console starting with man socket and follow references to man getpeername or use Konqueror which renders it nicely with links if you ask for #socket address. In my case on Kubuntu it was necessary to install manpages-dev package.  You can follow this example : #include <sys/types.h> #include <sys/socket.h> #include <netinet/in.h> #include <arpa/inet.h> #include <stdio.h> { int s; struct sockaddr_in peer; int peer_len; . . . /* We must put the length in a variable. */ peer_len = sizeof(peer); /* Ask getpeername to fill in peer's socket address. */ if (getpeername(s &peer &peer_len) == -1) { perror(""getpeername() failed""); return -1; } /* Print it. The IP address is often zero because */ /* sockets are seldom bound to a specific local */ /* interface. */ printf(""Peer's IP address is: %s\n"" inet_ntoa(peer.sin_addr)); printf(""Peer's port is: %d\n"" (int) ntohs(peer.sin_port)); . . . }  I think getpeername() is not needed - the client address is already filled into cli_addr by the accept() call. You only need to use inet_ntop() getnameinfo() or gethostbyaddr() to print or get more information.  Your cli_addr already contains the IP address and port of the connected client after accept() returns successfully in the same format as your serv_addr variable. Use inet_ntop to convert IP to a string. I need IP for client that connect to me that use getpeername() Hmm you're confused somewhere. accept() returns client IP and client port in its second argument. You **don't need** to call getpeername() since that gives you same exact information. Upvoted. I think the confusion stems from the fact that sockaddr contains the IP as 4 bytes while the OP needs it in text. Completely agree - there's no need to use `getpeername()` in this case after the `accept()` `cli_addr.sin_addr` and `cli_addr.sin_port` contain the connecting peer's address and port in the same format that `getpeername()` returns.  getpeername() See the helpful description of how to use it over at the indispensable Beej's Guide to Network Programming. No need to call `getpeername()` as `accept()` fills in the client address into its second argument. Beej's is a really nice intro. (Apologize about mistakenly voting above comment as noise.)"
369,A,Socket progamming in C on linux Hii all Am interested in socket programming and i learned basics of socket programming .Am doing it C on linux .Now i would like to work on some project related to socket programing .Could any body suggest from where i get help means online socket related projects Thanks and Regards Amit Singh Tomar. start writing a simple app that's the best way you will learn. Here are some tutorial sites: http://www.tenouk.com/cnlinuxsockettutorials.html http://www.linuxhowtos.org/C_C++/socket.htm http://beej.us/guide/bgnet/ but there are many more... Thanks trojanfoe but links simply provides tutorials but i would like to do some real life projects related to socket prograaming ... Oh see apologies. I would have a look through http://code.google.com and search for what you want.  In case you see C++ as an option you could take a look at the network examples of the Qt framework. If you want to stick to C you could take a look at the socket support of GLib. Thats fine Taneli but Taneli where i get some real-life project on Socket programming since am good in C so i would like it to do in C. anyway Thanks for your kind response....
370,A,"Sending large chunks of data over Boost TCP? I have to send mesh data via TCP from one computer to another... These meshes can be rather large. I'm having a tough time thinking about what the best way to send them over TCP will be as I don't know much about network programming. Here is my basic class structure that I need to fit into buffers to be sent via TCP: class PrimitiveCollection { std::vector<Primitive*> primitives; }; class Primitive { PRIMTYPES primType; // PRIMTYPES is just an enum with values for fan strip etc... unsigned int numVertices; std::vector<Vertex*> vertices; }; class Vertex { float X; float Y; float Z; float XNormal; float ZNormal; }; I'm using the Boost library and their TCP stuff... it is fairly easy to use. You can just fill a buffer and send it off via TCP. However of course this buffer can only be so big and I could have up to 2 megabytes of data to send. So what would be the best way to get the above class structure into the buffers needed and sent over the network? I would need to deserialize on the recieving end also. Any guidance in this would be much appreciated. EDIT: I realize after reading this again that this really is a more general problem that is not specific to Boost... Its more of a problem of chunking the data and sending it. However I'm still interested to see if Boost has anything that can abstract this away somewhat. Well regardless of what API you use one simple way is to send first a small 4 byte hedaer describing how large the dataset would be and then just keep sending data until you finish. There's not really a limit. You could also compress the data before send it depending on what your computational needs are. A lot depends on the particular problem. In addition to how to chunk and deliver the data another issue you should consider is platform differences. If the two computers are the same architecture and the code running on both sides is the same version of the same compiler then you should probably be able to just dump the raw memory structure across the network and have it work on the other side. If everything isn't the same though you can run into problems with endianness structure padding field alignment etc. In general it's good to define a network format for the data separately from your in-memory representation. That format can be binary in which case numeric values should be converted to standard forms (mainly changing endianness to ""network order"" which is big-endian) or it can be textual. Many network protocols opt for text because it eliminates a lot of formatting issues and because it makes debugging easier. Personally I really like JSON. It's not too verbose there are good libraries available for every programming language and it's really easy for humans to read and understand. One of the key issues to consider when defining your network protocol is how the receiver knows when it has received all of the data. There are two basic approaches. First you can send an explicit size at the beginning of the message then the receiver knows to keep reading until it's gotten that many bytes. The other is to use some sort of an end-of-message delimiter. The latter has the advantage that you don't have to know in advance how many bytes you're sending but the disadvantage that you have to figure out how to make sure the the end-of-message delimiter can't appear in the message. Once you decide how the data should be structured as it's flowing across the network then you should figure out a way to convert the internal representation to that format ideally in a ""streaming"" way so you can loop through your data structure converting each piece of it to network format and writing it to the network socket. On the receiving side you just reverse the process decoding the network format to the appropriate in-memory format. My recommendation for your case is to use JSON. 2 MB is not a lot of data so the overhead of generating and parsing won't be large and you can easily represent your data structure directly in JSON. The resulting text will be self-delimiting human-readable easy to stream and easy to parse back into memory on the destination side. Wow lot of information in here. I was going to upvote you for the comment to separate the protocol serialization from the in-memory representation but... JSON for what amounts to a big blob of scientific data? No way man. For 2 MB there's no point in doing anything else. So it expands to 5 or 6 MB so what? You're still talking a tiny fraction of a second to transfer it. JSON is easier and safer than mucking around with binary formats. In many cases piping it through a compressor/decompressor will result in a smaller transfer size than a binary format anyway. If we were talking about large volumes of data I'd give a very different answer but for this make it easy.  Have you tried it with Boost's TCP? I don't see why 2MB would be an issue to transfer. I'm assuming we're talking about a LAN running at 100mbps or 1gbps a computer with plenty of RAM and don't have to have > 20ms response times? If your goal is to just get all 2MB from one computer to another just send it TCP will handle chunking it up for you. I have a TCP latency checking tool that I wrote with Boost that tries to send buffers of various sizes I routinely check up to 20MB and those seem to get through without problems. I guess what I'm trying to say is don't spend your time developing a solution unless you know you have a problem :-) --------- Solution Implementation -------- Now that I've had a few minutes on my hands I went through and made a quick implementation of what you were talking about: http://boost.teeks99.com/temp/DataChunker.zip There are three big parts: The serializer/deserializer boost has its own but its not much better than rolling your own so I did. Sender - Connects to the receiver over TCP and sends the data Receiver - Waits for connections from the sender and unpacks the data it receives. I've included the .exe(s) in the zip run Sender.exe/Receiver.exe --help to see the options or just look at main. More detailed explanation: Open two command prompts and go to DataChunker\Debug in both of them. Run Receiver.exe in one of the Run Sender.exe in the other one (possible on a different computer in which case add --remote-host=IP.ADD.RE.SS after the executable name if you want to try sending more than once and --num-sends=10 to send ten times). Looking at the code you can see what's going on creating the receiver and sender ends of the TCP socket in the respecitve main() functions. The sender creates a new PrimitiveCollection and fills it in with some example data then serializes and sends it...the receiver deserializes the data into a new PrimitiveCollection at which point the primitive collection could be used by someone else but I just wrote to the console that it was done. Yeah I guess I'm trying to use the Boost TCP stuff as efficiently as possible... there don't seem to be many examples using it effectively. the download is broken - may you could add a new link  Without anything fancy from what I remember in my network class: Send a message to the receiver asking what size data chunks it can handle Take a minimum of that and your own sending capabilities then reply saying: What size you'll be sending how many you'll be sending After you get that just send each chunk. You'll want to wait for an ""Ok"" reply so you know you're not wasting time sending to a client that's not there. This is also a good time for the client to send a ""I'm canceling"" message instead of ""Ok"". Send until all packets have been replied with an ""Ok"" The data is transfered. This works because TCP guarantees in-order delivery. UDP would require packet numbers (for ordering). Compression is the same except you're sending compressed data. (Data is data it all depends on how you interpret it). Just make sure you communicate how the data is compressed :) As for examples all I could dig up was this page and this old question. I think what you're doing would work well in tandem with Boost.Serialization.  I would like to add one more point to consider - setting TCP socket buffer size in order to increase socket performance to some extent. There is an utility Iperf that let test speed of exchange over the TCP socket. I ran on Windows a few tests in a 100 Mbs LAN. With the 8Kb default TCP window size the speed is 89 Mbits/sec and with 64Kb TCP window size the speed is 94 Mbits/sec."
371,A,"Decomposition of an IP header I have to do a sniffer as an assignment for the security course. I am using C and the pcap library. I got everything working well (since I got a code from the internet and changed it). But I have some questions about the code. u_int ip_len = (ih->ver_ihl & 0xf) * 4; ih is of type ip_header and its currently pointing the to IP header in the packet. ver_ihl gives the version of the IP. I can't figure out what is: & 0xf) * 4; +1 for being honest and thorough :) & is the bitwise and operator in this case you're anding ver_ihl with 0xf which has the effect of clearing all the bits other than the least signifcant 4 0xff & 0x0f = 0x0f ver_ihl is defined as first 4 bits = version + second 4 = Internet header length. The and operation removes the version data leaving the length data by itself. The length is recorded as count of 32 bit words so the *4 turns ip_len into the count of bytes in the header In response to your comment: bitwise and ands the corresponding bits in the operands. When you and anything with 0 it becomes 0 and anything with 1 stays the same. 0xf = 0x0f = binary 0000 1111 So when you and 0x0f with anything the first 4 bits are set to 0 (as you are anding them against 0) and the last 4 bits remain as in the other operand (as you are anding them against 1). This is a common technique called bit masking. http://en.wikipedia.org/wiki/Bitwise_operation#AND i got the idea (ver_ihl contains the Version (4 bits) + Internet header length (4 bits)) but i still didn't get how this operation is removing the version data. when you are anding with 0xf does this mean that your anding only the fist 4 bits?  The ver_ihl field contains two 4-bit integers packed as the low and high nybble. The length is specified as a number of 32-bit words. So if you have a Version 4 IP frame with 20 bytes of header you'd have a ver_ihl field value of 69. Then you'd have this:  01000101 & 00001111 -------- 00000101 So yes the ""&0xf"" masks out the low 4 bits.  Reading from RFC 791 that defines IPv4: A summary of the contents of the internet header follows: 0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ The first 8 bits of the IP header are a combination of the version and the IHL field. IHL: 4 bits Internet Header Length is the length of the internet header in 32 bit words and thus points to the beginning of the data. Note that the minimum value for a correct header is 5. What the code you have is doing is taking the first 8 bits there and chopping out the IHL portion then converting it to the number of bytes. The bitwise AND by 0xF will isolate the IHL field and the multiply by 4 is there because there are 4 bytes in a 32-bit word."
372,A,Low Throughput on Windows Named Pipe Over WAN I'm having problems with low performance using a Windows named pipe. The throughput drops off rapidly as the network latency increases. There is a roughly linear relationship between messages sent per second and round trip time. It seems that the client must ack each message before the server will send the next one. This leads to very poor performance I can only send 5 (~100 byte) messages per second over a link with an RTT of 200 ms. The pipe is asynchronous using multiple overlapped write operations (and multiple overlapped reads at the client end) but this is not improving throughput. Is it possible to send messages in parallel over a named pipe? The pipe is created using PIPE_TYPE_MESSAGE would PIPE_READMODE_BYTE work better? Is there any other way I can improve performance? This is a deployed solution so I can't simply replace the pipe with a socket connection (I've read that Windows named pipe aren't recommended for use over a WAN and I'm wondering if this is why). I'd be grateful for any help with this matter. Yes I understand that thanks. However I was suprised to find that each write event requires an ack from the client side before the pipe will send any more data and I'm trying to find a way around this. Overlapped I/O just means that WriteFile returns control to your app immediately. It doesn't affect the actual WAN traffic. I've implemented a work around introducing a small (~1ms) fixed delay to buffer up as much data as possible before writing to the pipe. Over a network link with a RTT of 200ms I can send ten times as much data in about a third of the time. I send a message down the pipe when it first connects so the client can determine the comms mode supported by the server and send data accordingly.  I would imagine that some of the WAN optimisation gear out there would be able to boost performance as one of the things they do is understand protocols and reduce their chattiness. Given the latency of many WAN links this alone can boost throughput and reduce timeouts.  We found that Named Pipes had poor performance from Windows XP onwards. I don't have a solution for you. But I am concurring with the notion of Named Pipes being useless from XP onwards. We changed our software (in terms of IPC) completely because of it. Is your comms factored into a separate DLL? Perhaps you could replace the DLL with an interface that looks the same but behaves differently? Your answer in that link mentions you replaced them with your own shared memory data transport implementation. Did you find the pipes were performing poorly over a network or were you just using them for local IPC? Principally for local IPC. I can't remember the details it was years ago. I think we did briefly look at network stuff as well. But if its poor locally its probably going to be worse over the network. We did some searches to find if others had problems with named pipes and sure enough they did and no one had a solution which is why we changed tack completely. The shared memory solution has been great but it has rather tied our software to working on one machine rather than across a network :-( which fortunately for us most of our customers want to work on one machine. Interesting but unfortunately that doesn't help me with the problem. We have a deployed solution consisting of 46 servers over 3 continents using named pipes for IPC.
373,A,"Cancel UDP recvfrom in C on Unix I'm just starting to learn how network programming in C works and I've written a small program that sends messages to and from a UNIX terminal. I'm using pthreads in my program one of which essentially just waits on recvfrom() to receive a message. However I want to be able to close all threads properly if the users chooses to quit the program. The way I have it set up right now a different thread just cancels the thread waiting on recvfrom but I'm worried this might not be a good idea since I'm leaving sockets unclosed and I'm not freeing all the memory I allocated. Is there a way to cancel a recvfrom() call or some way to run a certain routine upon cancelling a pthread? Thanks. Another approach would be to maintain a list of open connection matched with thread id's. Then whenever you cancel a thread go through the list to find it's socket and call close on the socket.  If you use pthread_kill to send a signal to the thread recvfrom() should return -1 with errno set to EINTR - you can then check for the ""application is now exiting"" condition and finish up gracefully."
374,A,"Question about getaddrinfo() function in socket programming Below is a sample invoking of getaddrinfo() status = getaddrinfo(""www.example.net""""1234"" &hints &server_info); After that the server_info will point to a linked list with all kinds of address information. I have the following questions: Since I have clearly specified the host name and port number the only address infos I can think of are IPv4 and IPv6 addresses. So the length of the linked list should be 2. Is there any other kind of address info besides them? Thanks. You want to disconnect the physical configuration of machines with names in your mind. DNS merely maps a name to a set of addresses. A lot of hosts will only have one interface. Many hosts will have multiple (called multi-homing). DNS doesn't care about what the configuration of the machine or machines that the addresses it maps a name to is. As simple examples often a server will have interfaces on multiple networks with different addresses that will all map to the same name. Sometimes when collapsing services from different machines onto one different names will map to the same address. So don't assume any sort of 1:1 mapping between names and machines much less interfaces.  The name could resolve to more than one IPv4 or IPv6 address there is nothing to say that only one IPv4 address will be returned for example (try it with ""www.google.com"" for example you'll likely get more than one IPv4 address). But in any case I think the basic premise of your question is wrong. Even if there was no possibility to return more than one IPv4 and one IPv6 address today the function is documented to return an arbitrarily-long linked list of addrinfo objects. Therefore even if your code worked today in every situation there is no guarantee that it would continue to work tomorrow. If the function is documented to return an arbitrarily-long linked list then you need to be able to handle that."
375,A,"Application level checksumming as the tcp checksumming might be too weak? This Paper (When the CRC and TCP checksum disagree) suggests that since the TCP checksumming algorithm is rather weak there would occur an undetected error every 16 million to 10 billion packets using TCP. Are there any application developers out there who protect the data against such kind of errors by adding checksums at the application level? Are there any patterns available to protect against such errors while doing EJB remote method invocation (Java EE 5)? Or does Java already checksum serialized objects automatically (additionally to the underlying network protocol)? Enterprise software has been running on computers doing not only memory ECC but also doing error checking within the CPU at the registers etc (SPARC and others). Bit errors at storage systems (hard drives cables ...) can be prevented by using Solaris ZFS. I was never afraid of network bit errors because of TCP - until I saw that article. It might not be that much work to implement application level checksumming for some very few client server remote interfaces. But what about distributed enterprise software that runs on many machines in a single datacenter. There can be a really huge number of remote interfaces. Is every Enterprise Software vendor like SAP Oracle and others just ignoring this kind of problem? What about banks? What about stock exchange software? Follow up: Thank you very much for all your answers! So it seems that it is pretty uncommon to check against undetected network data corruption - but they do seem to exist. Couldn't I solve this problem simply by configuring the Java EE Application Servers (or EJB deployment descriptors) to use RMI over TLS with the TLS configured to use MD5 or SHA1 and by configuring the Java SE clients to do the same? Would this be a way to get reliable transparent checksumming (although by overkill) so that I would not have to implement this at application level? Or am I completely confused network-stack wise? I am convinced that every application that cares about data integrity should use a secure hash. Most however do not. People simply ignore the problem. Although I have frequently seen data corruption over the years - even that which gets by checksums - the most memorable in fact involved a stock trading system. A bad router was corrupting data such that it usually got past the TCP checksum. It was flipping the same bit off and on. And of course no one is alerted for the packets that in fact failed the TCP checksum. The application had no additional checks for data integrity. The messages were things like stock orders and trades. The consequences of corrupting the data are as serious as it sounds. Luckily the corruption caused the messages to be invalid enough to result in the trading system completely crashing. The consequences of some lost business were nowhere near as severe as the potential consequences of executing bogus transactions. We identified the problem with luck - someone's SSH session between two of the servers involved failed with a strange error message. Obviously SSH must ensure data integrity. After this incident the company did nothing to mitigate the risk of data corruption while in flight or in storage. The same code remains in production and in fact additional code has gone into production that assumes the environment around it will never corrupt data. This actually is the correct decision for all the individuals involved. A developer who prevents a problem that was caused by some other part of the system (e.g. bad memory bad hard drive controller bad router) is not likely to gain anything. The extra code creates the risk of adding a bug or being blamed for a bug that isn't actually related. If a problem does occur later it will be someone else's fault. For management it's like spending time on security. The odds of an incident are low but the ""wasted"" effort is visible. For example notice how end-to-end data integrity checking has been compared to premature optimization already here. So far as things changing since that paper was written - all that has changed is we have greater data rates more complexity to systems and faster CPUs to make a cryptographic hash less costly. More chances for corruption and less cost to preventing it. The real issue is whether it is better in your environment to detect/prevent problems or to ignore them. Remember that by detecting a problem it may become your responsibility. And if you spend time preventing problems that management does not recognize is a problem it can make you look like you are wasting time.  I've worked on trading systems for IBs and I can assure you there is no extra checksumming going on - most apps use naked sockets. Given the current problems in the financial sector I think bad TCP/IP checksums should be the least of your worries. @altCognito please do not edit my answers so as to alter their meaning. thank you. This is a case of Premature Optimization http://c2.com/cgi/wiki?PrematureOptimization (old answer removed I'm spooked)  Well that paper is from 2000 so it's from a LONG time ago (man am I old) and on a pretty limited set of traces. So take their figures with a huge grain of salt. That said it would be interesting to see if this is still the case. However I suspect things have changed though some classes of errors may still well exist such as hardware faults. More useful than checksums if you really need the extra application-level assurance would be a SHA-N hash of the data or MD5 etc. document is from 2000 but TCP protocol with its checksum is even more old - from seventies of previous century- and the error still there. So I wouldn't expect it to disappear just by becoming too ""ancient"""
376,A,Handling Password Authentication over a Network I'm writing a game which requires users to log in to their accounts in order to be able to play. What's the best way of transmitting passwords from client to server and storing them? I'm using Python and Twisted if that's of any relevance. If you want plug'n'play solution use py-bcrypt for storing passwords (http://www.mindrot.org/projects/py-bcrypt/) and SSL/TLS to protect them in transit.  The best way is to authenticate via SSL/TLS. The best way of storing passwords is to store them hashed with some complex hash like sha1(sha1(password)+salt) with salt. +1 but i'd recommend 1 round of sha256. sha1 has a problem with collisions and a collision in the inner sha1() would not be fixed by the outer salt and sha1(). (disclaimer no one has actually generated a collision for sha1 but its possible to do better than brute force. )
377,A,no route.h on the iPhone SDK I am trying to build some code that is originally target at OSX/BSD/Linux for the iPhone. It uses struct rt_msghdr from route.h but as it turns out this header is not available in the iPhone SDK. Looks like the function tries to find the available bind addresses as a list of struct addrinfo. Does anyone have a suggestion how to proceed here? Copy the header into the project and change the includes to be local that often works for me. Surprisingly (at least to me) that did indeed work. Still odd it's not included in the SDK. But thanks to the pointer! At least I got it compiling now. I noticed that the header was present in the Simulator SDK but not in the iPhone SDK (3.2+).
378,A,Implementing a simple server / client concept in C++ I'm searching for a library / framework / codesnippet that helps me build a really simple server / client. Both should be portable (linux/win/mac). A picture says more then a thousand words so: The server should be able to seperate diffrent clients. What I'm looking for is only a hint on how to do the transfere. I've little experience with languages other then c++ and it was mostly plain easy to create a server that stores all the clients connected in an array and interacts with each one of them once you got pointed in the right direction. What I looked into for C++ was RakNet and Boost.Asio. I don't want to use RakNet because I might sell the application at some point (you never know) and Boost.Asio seems way to complex for me to understand and way to low-level. All I really need is a simple send/recieve functionallity. The json will be handled by me (I already looked into jsoncpp). One message will have a maximum length of 2048 bytes. Any suggestions / hints / help on what to look into / what to use? I'm really looking for something easy and not so much low-level to use as I don't need the low-level functionallity. Thanks in advance Robin. Although a picture with text doesn't say much more than those words. IS this homework? No it isn't. I want to develop a small game with one of my friends (just for the purpose of learning stuff about opengl) and we though it would be cool if we could add network support. As cross-platform and easy-to-use are both requirements I would recommend Qt. It's pretty easy reliable and cross-platform. http://qt.nokia.com/products/developer-tools/ It comes with lots of samples including 13 networking samples (take a look at the threaded server tutorial which I believe it's just what you're after). Yes but its kinda bloat to use QT in this case imho and the licencing would be a problem if you are planing to use it commercially. LGPL trickier than one might think. @zhengtonic - You can stick with LGPL. The commercial license is needed only if you plan to modify Qt's source and keep the changes closed source otherwise you can create you own commercial closed-source application with Qt's LGPL license. As far as the bloat goes I respectfully disagree Qt is modularized so you should only need libqtcore and libqtnetwork. This is really a small price to pay for cross-platform. It's not like you'll be adding a virtual machine or anything like it it will still be a native application  I enjoyed Beej's guide to network programming.  Have you done Network programming before? Its pretty straight forward in C. And what you want can be implemented in a few hours. I used this manual below to finish my assignments back when i was a student. http://shoe.bocks.com/net/ Just read this and you are good to go. Its really not very hard. Good Luck! I'm looking for a cross platform solution. @Robin: I still suggest you read a well-written non-portable solution to get familiar with network programming. Network sockets have nearly identical interfaces on different platforms allowing you to port **clients** easily. Scalable network server architectures however vary **a lot** from one platform to the other. What Andre says they are almost the same. Porting it to various platforms it pain free at this level. I don't know exactly what you want to develop but if you don't mind looking into java its even easier there and its platform independent.
379,A,"How would you test an SSL connection? I'm experimenting with OpenSSL on my network application and I want to test if the data sent is encrypted and can't be seen by eavesdropper. What tools can you use to check? Could this be done programmatically so it could be placed in a unit test? check out wire shark http://www.wireshark.org/ and tcp dump http://en.wikipedia.org/wiki/Tcpdump Not sure about integrating these into unit tests. They will let you look at a very low level whats going on at the network level. Perhaps for the unit test determine what the stream looks like unencrypted and make sure the encrypted stream is not similar To test if the stream is encrypted compute the distribution of byte values. A properly encrypted stream will have an even distribution since it looks like random noise while an unencrypted stream won't. This works surprisingly well with a stddev of >=3 in my experience. Interesting idea must try it some time  openssl has an s_client which is a quick and dirty generic client that you can use to test the server connection. It'll show the server certificate and negotiated encryption scheme.  Franci Penov made an answer to one of my questions ""Log Post Parameters sent to a website"" suggesting I take a look at Fiddler: http://www.fiddler2.com/fiddler2/ I tried it and it works beautifully if you're interested in viewing HTTP requests. :)  For a quick check you can use Wireshark (formerly known as Ethereal) to see if your data is transmitted in plain-text or not.  As mentioned before http://www.wireshark.org/ you can also use cain & able to redirect the traffic to a 3rd machine and anylze the protocol from there.  Yeah - Wire Shark (http://www.wireshark.org/) is pretty cool (filters reports stats). As to testing you could do it as a part of integration tests (there are some command line options in wireshark)"
380,A,"IP address problem I built a TicTacToe game application (I'm using TCP protocol) that consist of server and client (that run two times to represent the 2 opponents). I have a problem and I can't find the solution for it that is why I'm asking this question. the problem is : when the client try to connect the server on my machine it never connect because of the IP address(obtained from the support tab from the local area connection in my windows) of my machine isn't correct. I tried to obtain my IP address from websites that told you your IP but I doesn't work. I have a problem to determine my IP address that any machine can connect to me from anywhere. p.s. I'm using router to connect the internet. thanks. You need to forward the TCP port you are using to your computer (In your router's settings) First I'd check to make sure any local settings and whatnot are correct by running the server and clients on the same machine and using localhost or 127.0.0.1 directly as the ip address. If this doesn't work you have a problem in either your code or your local firewall which you'll have to fix before you look into router stuff. Local firewall is easy enough to check just disable all your firewalls completely to see if you can connect to localhost. If you cannot connect with firewalls completely disabled it's your code that you need to fix first. If that does work then play around with allowing your (server) application through the firewall to accept traffic. Client application opening a connection won't (shouldn't) have any firewall issues on its own as that's solicited traffic that firewalls usually don't interfere with. If/when you can connect to localhost you can try connecting through the internet through your router. Most likely your problem is your router not forwarding the port that you're opening to your computer. I've only ever implemented port forwarding manually myself and you should probably look into what this is and why it's needed yourself before messing with things. Basically you want the client to connect to your public ip (what you find from whatsmyip.org or wherever) and your router to forward to your LAN ip (what you find in the support tab of local area network config). Step by step instructions for this are router-specific but are usually straight-forward (there's usually a 'firewall' and/or 'advanced' tab or something in the router management page). You could eventually try to implement upnp which doesn't seem too complicated from a quick google search but it's probably too complex for the stage of learning you're probably in.   string procCall = ""http://project-intercept.com/ip.php""; WebClient wc = new WebClient(); UTF8Encoding utf8 = new UTF8Encoding(); string requestHtml = """"; requestHtml = utf8.GetString(wc.DownloadData(procCall)); return requestHtml; You can either use project-intercept.com/ip.php (witch is my anti-cheats project site) or you can use whatsmyip.org/ip.php (i think is the url). Basically any URL thats echo's just the ip. If you want to host the php yourself create a php file and add this echo $_SERVER['HTTP_CLIENT_IP']; or echo $_SERVER['REMOTE_ADDR']; Not Working too. I know 100% that that code snippet works. Make sure your firewall isn't blocking your application same for the router. If you need a full example on how to use it let me know. Remember to use System.Net and System.Web what is ""Settings.Log.Write(""GetMyIP(): My IP is: "" + requestHtml);"" from where you got ""Setting"" ? Sorry Settings is a custom Namespace that I was using in that particular project. Please discard that."
381,A,Access to TCP statistics/information per socket possible? (C/C++) I require some informations like the amount of resend packages/packet-loss occurred for a specific TCP-Socket I created. Does somebody know a way how to access or request such informations directly from my C/C++ program? Maybe something Linux specific? Or do I need (as a workaround) to capture and analyze my own traffic? Thanks in advance! By using getsockopt() to get or setsockopt() to set TCP socket options you can use TCP_INFO option on linux machines in order to get information about a socket. This option should be avoided if you want the code to be portable. What you will get back is a struct tcp_info from the kernel that contains information such as retransmissions lost packets states etc. Sadly this isn't easy to find and tcp_info seems to be not very well documented. However after some trial & error it seems to deliver what I need. Thanks!
382,A,How to build a server that accepts many thousands of persistent network connections? I am about build a server on linux (I get to pick programming language) that accepts many TCP/IP socket persistent connections from a desktop software. How can this be done cheaply and efficiently? A machine cannot have more than 60000 ports so if I have to support 600k connections then I will need 10 linux boxes? Since the computation needed for each connection is extremely small (idle 95% of the time) one linux box can already handle 600k I don't want to waste money and resource running 10 boxes just to get around port limitations. Any ideas? Since you're about to write a Unix network server daemon: lighttpd is one of these and you should look at the source. The `main()` function in src/server.c is a terse list of all the standard things a network daemon is expected to do: write a pid file respond to SIGHUP double-fork to get away from a controlling tty etc. http://www.lighttpd.net/ If you are serving 600k clients 10 boxes is NOT a waste of money as you will probably want more than that anyway (in practice). Talk to your operations team who should probably know something about capacity management if they are planning to operate this application. If you are running a server you only need one port to bind to - not one port for each client. Doesn't it create a server port upon accepting the connection? It creates server socket which is pair of client(ip:port)-server(ip:port) You do NOT need a port for each connection. An HTTP server will just use one port (typically 80). Port 80 is a *great* example of this; +1.  A connection is identified by the following tuple: [Server IP;Server Port; Client IP; Client Port] hence more than 60000 ports are available per machine. Erlang effectively handles 10's of 1000s of connections. Have a look at this comparison between Apache and Erlang Yaws. So according to that tuple there isn't really a limit on the server side outside of memory limitations? I got it wrong then. @erotsppa: that's what I like about SO: you can always learn new things! Have fun!  You should be able to do this easily with any framework which is decent. We use Twisted which is good (Assuming you understand python and asynchronous programming) if a little weird to understand. Asynchronous programming is pretty much a given because you will probably NOT want to start 600k threads.  Since any relatively widely used language has a reasonably efficient socket library I would go for a language that: Is efficient in whatever the processing you need to do with the data from the desktops You are familiar enough (read; most familiar) with to avoid the most common pitfalls / design patterns
383,A,"What more socket APIs are available? What are the differences between each of these Socket API? Everyone referred to it as Socket Programming or Network Programming in C and we started using it by using by including sys/socket.h & netinet/in.h. We thought it was 100% true. But question raised in my mind when I saw this book Internetworking With TCP/IP Volume III: Client-Server Programming and Applications which was available in 4 different versions Linux/POSIX Sockets AT&T TLI (Transport Layer Interface) Sockets BSD (Berkeley) sockets Window Sockets I'm confused. This clearly shows that there is no standard for Socket API. also I'm surprised to see sys/socket.h & netinet/in.h which are part of POSIX C library in the http://en.wikipedia.org/wiki/Berkeley_sockets . I'm more confused now. Why isn't there a standard for this? What more socket APIs are available? What are the differences between each of these Socket API? When People say Just ""Network Programming in C"" / ""Socket Programming"" what exactly they are referring to? Links for any further information? Why isn't there a standard for this? The de facto standard is BSD sockets upon which the Linux POSIX and Windows sockets APIs are based. What more socket APIs are available? Nothing that's still widely used. Before BSD sockets and its derivatives took over the world there were many. Most of the ones that remain are probably in the embedded world and even those are going away as mainstream OSes continue to swallow more and more of the embedded market. This battle was pretty much fought and over by the mid 90's. BSD sockets won. What are the differences between each of these Socket API? There are minor differences among the BSD Linux and POSIX variants nothing more serious than any other differences among Unixy operating systems. The reason they have a Linux/POSIX version of the book probably has more to do with marketing than anything technical. It answers a question the publisher probably saw a lot ""Why do I need a BSD book I'm running Linux not BSD!"" Or more commonly these days: ""What's BSD?"" From a 10000 foot view Winsock is very different from BSD sockets but because it's a fairly strict superset of BSD sockets you can still move your knowledge over. Most of the differences are pure extensions to BSD sockets mostly to do with the differences in the Windows kernel architecture and the way Windows programs are typically built. For instance the first really big extension was asynchronous sockets which makes it much easier to use sockets in a single-threaded Windows GUI program than using pure BSD sockets. Later extensions support special features available in the NT derived kernels that have no simple analog in Unixy systems like event objects and overlapped I/O. For what it's worth there are extensions to plain old BSD sockets in some Unixy systems too like the aio_*() stuff in Solaris and other systems. If your program has to be source compatible with many systems you either ignore these differences and program to the common base shared by all these systems or you build some kind of translation layer that lets you use platform features transparently. Apache does the latter for instance making use of the fastest networking features on each platform while the core web server code doesn't care exactly how the networking gets done. Many other programs choose the portable path since they're not performance critical and saving programmer time is therefore more important. When People say Just ""Network Programming in C"" / ""Socket Programming"" what exactly they are referring to? BSD sockets or some variant. Links for any further information? The Winsock Programmer's FAQ. Specifically you might want to look at its resources section and the FAQ article BSD Sockets Compatibility. (Disclaimer: I'm the FAQ's maintainer.)  The Linux BSD and Windows sockets APIs (at least; I don't know about the AT&T TLI API) are pretty similar to each other. The Windows API for example is/was originally based on the BSD API. Yep 4 different names doesn't actually mean 4 completely different implementations. At least with the 3 you mention it's pretty much the same thing with a few platform-specific quirks.  The TLI interface is actually aligned to OSI. Novell were also shipping it at one time. It is obsolete. The Linux and Winsock APIs are based on the BSD API closely in the case of Linux less so in the case of Winsock. There are also small implementation differences in both cases.  Why isn't there a standard for this? There are. You named several. Anyone can publish a ""standard"" there doesn't need to be just one nor is there some controlling authority with the power to confer ""The One True Way"". In terms of networking the real standards are the network protocols themselves. Everyone must inter-operate with them or nothing works together. How any given platform implements the APIs to accomplish that goal can vary. What are the differences between each of these Socket API? Linux/POSIX/BSD are all similar. Windows is a little different but since it still has to support the underlying protocols the functionality ends up being pretty much the same. Honestly I am not sure who uses AT&T TLI anymore if they ever did. What more socket APIs are available? There are probably plenty but it is a question of who uses them anymore. IBM used to have a bunch and so did most every other major manufacturer back in the day. But I think these for have for the most part been retired to the history dust bin since most everyone uses TCP. When People say Just ""Network Programming in C"" / ""Socket Programming"" what exactly they are referring to? Nothing precise. Technically the C language doesn't know anything about network programming. However many vendors (e.g. Unix Linux Windows etc) provide C libraries to perform networking operations hence the many APIs you noted above."
384,A,"What are some good involved books or resources regarding game network programming such as client side prediction using UDP/TCP? Possible Duplicate: Best Game network programming articles and books Hi! I'm looking to start some game programming in my free time. I am pretty well versed in general programming but I would like to read about some methods and algorithms for such things as sending game updates from server effectively but still tolerant towards dropped datagrams and client side prediction etcetera. Preferably algorithms useful as general-purpose game-oriented network/gamestate synching and such. I already have pretty solid knowledge about streams/datagrams so it should be geared towards teaching networking in a game programming context. There is this post: Best Game network programming articles and books But it is a few years old now. Feel free to close if you want to though. @James You are right I will have to adapt everything to my specific case but if you look at the table of contents of the book Jeff linked to you see that he covers a few areas such as dead reckoning and prediction. I know I can't simply rip his algorithms and use them but I'd very much like to be armed with that knowledge when I start developing so that I can take into account issues that my forefathers has encountered and solved... But you are right in that any information about networking will be intimately bound to whatever domain I am developing in... @warren I did check those lists and the gamedev one was worthless had nada on network programming. Jeffs answer on this thread seems much superior to most of those (aswell as more current). @james Yes of course game architecture in a networking sense. One of those answers said ""serialize data send it reconstruct on other side"" which is so obvious I don't know what to say. I want to know how to construct gameplay diffs while still tolerant of dropped packages client side prediction synching algorithms etc which is far mode advanced than simple serialization... @Max - But what your are talking about is basically game architecture. For example if I send 5 position packets/sec and one is dropped it may be that it won't be noticed that one position update wasn't done. If someone is moving in a straight line you can predict they will continue if there is no obstruction but this depends heavily on the game so for mario it is easier to predict than for a single-person shooter. Ultimately these are simple serialization but based on game architecture you will determine which optimizations may be helpful or what changes can you make to optimize. Well this is a list question and it seems almost an exact dupe and the lists pointed to in the other question seem quite good really. network programming hasn't change din the last 2 years. voting to close. @James Well that is true the game code must have certain specific properties to fit into a networking model and vice versa but alot of the mechanics must be similar whether we are talking about 2D or 3D - I mean one thing would (perhaps) be to serialize differences in space (2D or 3D) in a way such that it is very tolerant to incomplete information and that could probably be described in a game-independent way? I don't mean to argue so if I come off as obstinate that's not my intention I'm simply trying to figure out WHAT I actually have to learn :P @Max - Write a game with just the basic mechanics then experiment. Intentionally drop a packet perhaps every 5th packet. See what happens. As you add optimizations some will help some won't. As time goes on you will get better at knowing which optimizations will work with your design and you will design to allow optimizations that you want added. For example a space game over a network with high latency may entail not allowing rapid turns so your game mechanics would build that in. How to serialize depends on what you are sending as it won't just be positional data for example. @Max - You may want to explain why the books in the link you mentioned aren't not good enough. What are you looking for that that link is lacking? It seems you are looking more at game architecture/design btw. @Warren game network programming has changed in the last two years even thought the underlying protocols and such haven't. Also the other question's answer isn't all that great. I wouldn't vote to close this one. . . I don't know of a good book but Joe Hega (currently (network)programmer at Bioware and I think he did the networking for Fable 3 Dirt 2 and Age of Conan) wrote a nice series of tutorials on writing a network library for games: http://www.joehegarty.com/topics/building-a-network-library-series/ He also links to a Gaffer on Games article: http://gafferongames.com/networking-for-game-programmers/ Thanks! Joe Hegas series doesn't seem to be nearly completed yet (he simply mentioned how to use winsock) but hasn't yet said anything about game-specific algorithms to reduce perceived latency. That Gaffer on Games article seems great though thanks!  The best and ONLY good book on this is here: http://www.amazon.com/Algorithms-Networking-Computer-Games-Jouni/dp/0470018127/ref=sr_1_1?ie=UTF8&qid=1304979863&sr=8-1 Gaffer on Games is great as well. Also check out open source network libraries such as RakNet: http://www.jenkinssoftware.com/ for some implementation ideas. There are some great articles out there as well. Deterministic Lockstep: http://www.gamasutra.com/view/feature/3094/1500_archers_on_a_288_network_.php Source Networking Engine techniques: http://developer.valvesoftware.com/wiki/Source_Multiplayer_Networking Halo: Reach networking: http://downloads.bungie.net/presentations/David_Aldridge_Programming_Gameplay_Networking_Halo_final_pub_without_video.pptx (this is from GDC this year) Quake networking model overview: http://trac.bookofhook.com/bookofhook/trac.cgi/wiki/Quake3Networking Implementation details: http://www.tilion.org.uk/Games/Quake_3/Network_Protocol This ought to get you started. Ask if you have any SPECIFIC questions and I'll try to answer them I know a lot about this stuff. Thanks Jeff! I think this is exactly what I was looking for... I know the basics of network programming pretty intimately but it seems a huge leap to invent my own game network engine - Kind of like knowing basic arithmetics and then trying to reinvent linear algebra. If you want a prebuilt networking engine checkout RakNet (http://www.jenkinssoftware.com/) it's what Unity recommends for people to use (I think) and it's pretty complete. I don't like everything about it but it would be just fine for many games and has a pretty good set of features."
385,A,"why cannot we use process id insted of taking the port we are binding why cannot we use process id insted of taking the port we are binding in socket programming. in socket programming we create socket and get a socket descriptor and we bind to a specific port .for multiple connection why are we not using the process id as all the connection are also a process returning the processs id? i guess you like to ask a lot of questions but no to select answers? First multiple connections can exist per process. Second socket API is does not depend on any OS process API.  It's an interesting idea but I think it would raise a few problems: How would you know what process ID you wanted to connect to? What if you wanted to listen on more than one ""port"" inside the same process? You only have one process ID. IPv4 and IPV6 allocate 16 its for port IDs but process IDs usually are 32-bit (or bigger) values so they wouldn't fit There are many programs that don't have a networking aspect and don't want one. Would automatically instantiating a network communications path to them be a potential security problem? One trick you can do (especially with UDP multicast or broadcast) is have several programs listen on the same port (via SO_REUSEPORT) so that when anyone sends out a UDP packet to that port all of the programs receive it. That trick would be difficult or impossible if programs had to use their (unique) process ID numbers as port numbers.  Because TCP has port numbers in the specification but it doesn't have process IDs. Why would you want to use a processID that you can't control when you can control the port number? How would a process listen on multiple ports?"
386,A,"transfer int over network I am new to network programming so I have a question: is it safe to send an integer (let's assume 16 bit integer because 32 has issue with ""endianess"") in my case between C++ and C# and how do that? In my program I cast int to char (I know is is lower than 255) and send it. However I would like to send 32 int. I tried conduct that with protocol buffers. I used WriteVarint32 in C++ and ProtoReader.ReadUInt32 in C# with no success (exception in C#). I thought it will work after I read this. Would you be so kind and suggest me the right way to send an integer between C++ and C# applications? If you explained me how to do that with protocol buffers (protobuf-net in case of C#) I would be very pleased. @David varint encoding works around endianness (it is implicit) So why would 16bit no have issues with endianness and 32bit have? 16bits will be different in bigendian/littleendian machines. I suggest that you decide an approach (for example protocol buffers) and try it and ask about the issues that you get. Basically it's going to depend on the network libraries that you use and how compatible they are at either end. At the lowest level the data is being transferred as stream or block of bytes. When these bytes are converted back to any more complex structure above a byte (even an int) then the encoding obviously matters and needs to match at both ends. Easiest thing is just to try it out and see. It sounds like you might be interested in packing the data as tightly as possible (from your comment on converting ints to chars) in which case maybe you should just write a set of low level packing routines at either end. That way you have no worries over encoding and you can squeeze every last bit out of the packet.  If you are writing with protobuf-net the WithLengthPrefix methods will do this for you. I'd be more than happy to help with this (I'm the author). Btw the reason it is throwing an exception is that a varint value by itself is not normally valid in a protobuf stream. The system will override this in the case of WithLengthPrefix if you pass 0 as the field number (or pass a positive field-number for a more pure protobuf stream; this is optional though). As it happens there is a method on ProtoReader to read an isolated varint (without worrying about protobuf stream semantics) it would be trivial for me to add such to ProtoWriter if that would be useful. Btw - in terms of PrefixStyle Base128 is the same as varint. Some folks prefer 32-bit prefixes to marry up with other systems - hence a few others are provided. I want to use WithLengthPrefix. My problem is I don't know how should I use C++ Protobuf API to construct the prefix C# protobuf-net can understand. 'Base128 is the same as varint' - I tried to send varint with size. C# aroused System.IO.EndOfStreamException. The prefix is an idea present only in protobuf-net. C++ implementation considers nothing even similiar to that concept. @lord - indeed. There is only a brief mention of strategies for this (in the docs) which basically say ""add your own length"". Re the EOF - I re-deployed the dlls following on from a v2 sample I added in an earlier discussion on this (with you I believe) - check you have the current release. Can you give a brief few words as to your strategy i.e. ""my message via C++ is 27 bytes so I've written [TODO: the bytes] as a prefix then the 27 bytes"" - then I should be able to see what you have. message Test { required int32 id = 1; } That is the message I want to send. Now I set the id (in C++): Test test; test.set_id(1234); I send Varint32 coded_output->WriteVarint32(test.ByteSize()); C# application should receive exactly these bytes (in hex) 03 08 d2 09. 03 is the size the rest is the message. Currently I struggle with sending the response back from C# to C++. I believe I will finish that soon. @lord - as it happens individual primitives (`int` etc) are treated by protobuf-net as a field on a message at field 1 so *the minumum* to get that is `int i = Serializer.DeserializeWithLengthPrefix(source PrefixStyle.Base128 0)` or `Serializer.SerializeWithLengthPrefix(ms 1234 PrefixStyle.Base128 0);` (needs the r404 dll though due to a bug in the earlier beta). You can of course have a full object with an int property etc - that would work fine too.  There is no endianess issues with the varint encoding it's always little endian by design. It stores the value seven bits at a time starting with the least significant seven bits and uses the eighth bit as stop bit when there is no more one bits left. You use the Read7BitEncodedInt method in C# to read a varint encoded integer. @lord-didger: It's in the `BinaryReader` class. If you are using some other way to read the data you can easily adapt the code for it it's just a few lines of code. You can find the source code here: http://www.koders.com/csharp/fid4083672E4B426989ECED8C0C517B0327B94BC1CB.aspx I'm not sure but it seems Read7BitEncodedInt is not available in MonoDevelop I use. Any other idea?"
387,A,Is there another way to match the MAC and IP Addresses in VB.NET 2008 besides using WMI? I would like to match multiple network card MAC's with the corresponding IP Addresses to be displayed. Is there a better (or just another) way to enumerate them besides using System.Management or out-and-out WMI? You could P/Invoke GetAdaptersAddresses in the IP Helper API. This returns you a list of IP_ADAPTER_ADDRESSES structures that include the IP Addresses and MAC info (in PhysicalAddress) for each adapter. Non-trivial marshalling - fortunately PInvoke.Net has it down. That's some great stuff! Will it work with WinXP through Win7? I worry that there will be changes in the DLL. Per the MSDN page for the function it's good for WinXP and WinSrv 2003 on up. All the best.
388,A,"How does sending tinygrams cause network congestion? I've read advice in many places to the effect that sending a lot of small packets will lead to network congestion. I've even experienced this with a recent multi-threaded tcp app I wrote. However I don't know if I understand the exact mechanism by which this occurs. My initial guess is that if the MTU of the physical transmission media is fixed and you send a bunch of small packets then each packet may potential take up an entire transmission frame on the physical media. For example my understanding is that even though Ethernet supports variable frames most equipment uses a fixed Ethernet frame of 1500 bytes. At 100 Mbit a 1500 byte frame ""goes by"" on the wire every 0.12 milliseconds. If I transmit a 1 byte message ( plus tcp & ip headers ) every 0.12 milliseconds I will effectively saturate the 100Mb Ethernet connection with 8333 bytes of user data. Is this a correct understanding of how tinygrams cause network congestion? Do I have all my terminology correct? Some channels don't have this problem - ATM for example has fixed-sized cells with a fixed-size payload (thus a fixed overhead) that are queued without gaps on a switched (collision-free) medium. I wasn't assuming Ethernet. From your examples 802.11g and cable modems are half-duplex and prone to expensive collisions. Most switched Ethernet networks are full-duplex. Small packets also increase the risk of a collision over half-duplex links. @Matthew: Thanks however the Ethernet is just an example of the more general problem I'm looking at - it could be 802.11g or a cable modem link or dsl or ATM whatever. Let me add that you still have to transmit your IP and TCP headers over that ATM link - the tinygram issue is still present just not at the link layer. BDK has about half the answer (+1 for him). A large part of the problem is that every message comes with 40 bytes of overhead. Its actually a little worse than that though. Another issue is that there is actually minimum packet size specified by IP. (This is not MTU. MTU is a *M*aximum before it will start fragmenting. Different issue entirely) The minimum is pretty small (I think 46 bytes including your 24 byte TCP header) but if you don't use that much it still sends that much. Another issue is protocol overhead. Each packet sent by TCP causes an ACK packet to be sent back by the recipient as part of the protocol. The result is that is you do something silly like send one TCP packet every time the user hits a key you could easily end up with a tremendous amount of wasted overhead data floating around.  A TCP Packet transmitted over a link will have something like 40 bytes of header information. Therefore If you break a transmission into 100 1 byte packets each packet sent will have 40 bytes so about 98% of the resources used for transmission are overhead. If instead you send it as one 100 byte packet the total transmitted data is only 140 bytes so only 28% overhead. In both cases you've transmitted 100 bytes of payload over the network but in one you used 140 bytes of network resources to accomplish it and in the other you've used 4000 bytes. In addition it take more resources on the intermediate routers to correctly route 100 41 byte payloads than 1 40 byte payloads. Routing 1 byte packets is pretty much the worst case scenerio for the routers performancewise so they will generally exhibit their worst case performance under this situation. In addition especially with TCP as performance degrades due to small packets the machines can try do do things to compensate (like retransmit) that will actually make things worse hence the use of Nagles algorithm to try to avoid this.  In wired ethernet at least there is no ""synchronous clock"" that times the beginning of every frame. There is a minimum frame size but it's more like 64 bytes instead of 1500. There are also minimum gaps between frames but that might only apply to shared-access networks (ATM and modern ethernet is switched not shared-access). It is the maximum size that is limited to 1500 bytes on virtually all ethernet equipment. But the smaller your packets get the higher the ratio of framing headers to data. Eventually you are spending 40-50 bytes of overhead for a single byte. And more for its acknowledgement. If you could just hold for a moment and collect another byte to send in that packet you have doubled your network efficiency. (this is the reason for Nagle's Algorithm) There is a tradeoff on a channel with errors because the longer frame you send the more likely it experience an error and will have to be retransmitted. Newer wireless standards load up the frame with forward error correction bits to avoid retransmissions. The classic example of ""tinygrams"" is 10000 users all sitting on a campus network typing into their terminal session. Every keystroke produces a single packet (and acknowledgement).... At a typing rate of 4 keystrokes per second That's 80000 packets per second just to move 40 kbytes per second. On a ""classic"" 10mbit shared-medium ethernet this is impossible to achive because you can only send 27k minimum sized packets in one second - excluding the effect of collisions:  96 bits inter-frame gap + 64 bits preamble + 112 bits ethernet header + 32 bits trailer ----------------------------- = 304 bits overhead per ethernet frame. + 8 bits of data (this doesn't even include IP or TCP headers!!!) ---------------------------- = 368 bits per tinygram 10000000 bits/s ÷ 368 bits/packet = 27172 Packets/second. Perhaps a better way to state this is that an ethernet that is maxed out moving tinygrams can only move 216kbits/s across a 10mbit/s medium for an efficiency of 2.16% If the files are merely a few bytes each that's certainly possible.. The 802.11 WLAN frames are different as well so the overhead calculation above doesn't hold. Also the stations have more ""smarts"" than the classic ethernet station and coordinate with the AP to reduce collisions... Microsoft has a very nice overview at http://technet.microsoft.com/en-us/library/cc757419%28WS.10%29.aspx (look under ""802.11 MAC Frame"") So my 54Mbit 802.11g effectively would have a bandwidth of approximately 1 mbit ( not including tcp/ip headers ) when flooded with tinygrams. So if I'm downloading lots of small files and setting up / tearing down a TCP connection for every file I assume that this would have a similar effect? Do you happen to know what protocol is used on cable networks which use direct DHCP connections? Is there such a thing as Ethernet over cable or do they use something like ATM? The DOCSIS standards define this MAC layer. Cable modems transmit full Ethernet frames with additional overhead so the tinygram issue is present. Each packet on the wire contains at least 6 and as many as 246 additional bytes up-front for DOCSIS purposes. Cable modems use a time- or code-divided uplink channel - so you can't crowd out your neighbor and only have the single headend talking on the downstream - so there are few collisions. But with the cable modem being a transparent IP bridge and the whole system being engineered for traffic control it could be reassembling short packets."
389,A,"How reasonable/possible/difficult is it to write a tsocks clone in Haskell I'm a reasonably competent programmer who knows haskell but who hasn't used it in any major projects. I know enough about c and systems and network programming that I believe I can pick apart tsocks from the source code. I don't have any experience with the low-level systems interfaces haskell provides. I'm looking for any advice people can offer me on the topic including ""Don't do it; you'll hate yourself for it"" provided there is an explanation. (Posting as a separate answer since this is advice unrelated to the FFI) You probably know this stuff but in case its useful for anyone... Read up on the Network.Socket module Search the Haskell Wiki for pages that might help you (like Applications and libraries/Network) Check out System Programming in Haskell and other chapters from RWH Ignore the people that say ""Haskell is terrible for I/O"" - protip: you can just scare them away by saying fancy words like ""endofunctor""  This may not be exactly the answer you were looking for but instead of re-writing it in Haskell you could just use the Foreign Function Interface to wrap the already-existing C implementation in Haskell types. Note one of the few major changes in Haskell 2010 was to officially include the FFI as a language feature. Link: Haskell 2010 FFI I'm actually not interested in tsocks as it exists so FFI won't help me. I'm planning to modify its functionality and looking for a project to practice Haskell on and wondering if this one is suitable. @Alex It may not be the easiest for practice but if it is something you are interested in it will almost certainly turn into a valuable programming experience. You could even upload it to hackage.  I really wouldn't do this except as an experiment. I'm a Haskell guy but not a deep systems guy so there's a caveat there. But nonetheless I see the following on the tsocks page: tsocks is based on the 'shared library interceptor' concept. Through use of the LD_PRELOAD environment variable or the /etc/ld.so.preload file tsocks is automatically loaded into the process space of every executed program. From there it overrides the normal connect() function by providing its own. Thus when an application calls connect() to establish a TCP connection it instead passes control to tsocks. tsocks determines if the connection needs to be made via a SOCKS server (by checking /etc/tsocks.conf) and negotiates the connection if so (through use of the real connect() function ) It is possible to call Haskell from C and vice-versa. And its relatively easy in fact. For shared libraries see this: http://www.haskell.org/ghc/docs/6.12.1/html/users_guide/using-shared-libs.html. But when you invoke Haskell from C you need to A) link in the runtime and B) invoke the runtime. So that works when the C knows that its calling Haskell. But its relatively trickier when the C doesn't know that it's calling Haskell and so you'd need to wrap the Haskell shared library with a C library that invoked and managed the runtime transparently to the program that is preloading the haskell-tsocks library to intercept its normal connect functions. So I'm sure this can be done -- but it sounds rather painful and complicated and somewhat expensive in terms of having to link the whole ghc runtime in for this one feature. And frankly I imagine the code you'd be writing (I haven't inspected the tsocks code itself yet) would largely be FFI calls anyway. So a Haskell implementation of some element of socks -- a proxy a client etc. sounds interesting and potentially useful. But the exact preload magic that tsocks does sounds like a perhaps poor fit. Bear in mind that there are Haskell hackers that are much better at this stuff than me more knowledgeable and more experienced. So they might say otherwise."
390,A,"How getservbyname can get the server port information if it is run on a client machine? The the following client program trys to connect to a server and finds the current time and date on that server. /* Start with the usual includes and declarations. */ #include <sys/socket.h> #include <netinet/in.h> #include <netdb.h> #include <stdio.h> #include <unistd.h> #include <stdlib.h> int main(int argc char *argv[]) { char *host; int sockfd; int len result; struct sockaddr_in address; struct hostent *hostinfo; struct servent *servinfo; char buffer[128]; if(argc == 1) host = ""localhost""; else host = argv[1]; /* Find the host address and report an error if none is found. */ hostinfo = gethostbyname(host); if(!hostinfo) { fprintf(stderr ""no host: %s\n"" host); exit(1); } /* Check that the daytime service exists on the host. */ servinfo = getservbyname(""daytime"" ""tcp""); if(!servinfo) { fprintf(stderr""no daytime service\n""); exit(1); } printf(""daytime port is %d\n"" ntohs(servinfo -> s_port)); /* Create a socket. */ sockfd = socket(AF_INET SOCK_STREAM 0); /* Construct the address for use with connect... */ address.sin_family = AF_INET; address.sin_port = servinfo -> s_port; address.sin_addr = *(struct in_addr *)*hostinfo -> h_addr_list; len = sizeof(address); /* ...then connect and get the information. */ result = connect(sockfd (struct sockaddr *)&address len); if(result == -1) { perror(""oops: getdate""); exit(1); } result = read(sockfd buffer sizeof(buffer)); buffer[result] = '\0'; printf(""read %d bytes: %s"" result buffer); close(sockfd); exit(0); } Question: We run the above program on a client machine how the function getservbyname can get the server information without a reference to the server machine in the parameter list? It examines /etc/services for an entry with the given service name and protocol. $ grep ""^daytime\s.*/tcp"" /etc/services daytime 13/tcp This is the part that I feel confused. Assume that I provide a hostname in the comand line for the server machine name. This client in fact doesn't acqure the service of datetime from that provided server machine instead it gets the info local from the file /etc/services. In other words we make assumption that both server and client share the same service information. Is that correct? See [RFC 1700: ""Assigned Numbers""](http://www.ietf.org/rfc/rfc1700.txt). @q0987 These numbers are standard: http://www.iana.org/assignments/port-numbers  getservbyname simply looks in /etc/services to find the ""daytime"" service using the ""tcp"" protocol. It's just a convenience to save you from parsing that file. Edit (clarification) Each of these protocols has a friendly name (""daytime"" ""http"" etc) and a useful name (the port number - 13 80 etc). /etc/services holds this mapping nothing more. Do you mean the daytime service is same as server and client machine? @q0987 I mean it's just a **standard** port number (and a L4 protocol)."
391,A,"How to perform a latency test in iPhone I would like to perform a latency test with respect to different urls in my iPhone app an approach would be to make an async request for an url and measure the time duration based upon when the control reaches in connectionDidFinish delegate is this the correct approach or some alternatives available in iPhone to measure latency time? I am not sure how you define latency. Look at the wiki article for some clarification. I'd profile for the response time to get a picture of the network lag very interesting in mobile connections. I am using this approach in one of the apps I've developed: - (NSURLRequest *)connection:(NSURLConnection *)connection willSendRequest:(NSURLRequest *)request redirectResponse:(NSURLResponse *)redirectResponse { ... self.reqStart = [NSDate date]; self.url = [[request URL] absoluteString]; NSLog(@""--> Requesting : %@"" url); ... } - (void)connection:(NSURLConnection *)connection didReceiveResponse:(NSURLResponse *)aResponse { NSLog(@""<-- Response time %fs : %@"" [[NSDate date] timeIntervalSinceDate:reqStart] url); ... } - (void)connectionDidFinishLoading:(NSURLConnection *)connection { NSLog(@""<== Request time %fs : %@"" [[NSDate date] timeIntervalSinceDate:reqStart] url); ... } Throttling bandwidth for simulator testing could be very helpful.  Share your Mac's Airport to your iPhone (bridge it to Ethernet). Then use tcpdump on the Mac to capture the network traffic using Wireshark to view the pcap this will give you the best view of network latency. could you please share some sample code"
392,A,Examples of OpenGL programs with networking code? I'm wondering if anyone has/knows of any examples of networking code integrated with OpenGL. Basically I need to send position coordinates of something over a network to my OpenGL display... which would then draw the object at the correct position. The problem I'm having is integrating my UDP code with the game. I basically have a loop which constantly updates/draws. I originally thought that I could have the UDP code in a separate thread and just update the shared coordinates. However if my draw functions read position before the network code is done writing the new position then I have a race condition and invalid data... however I can't make the Update or Draw loops block... I'm not sure if I should be using Synchronous or Asynchronous networking code here. So any tips on how I can solve this problem? Thanks Sounds like the best real world use case for STM yet. The network updates state in a txn the drawing code draws in a txn everything is always in a consistent state. Run network in separate thread set a flag (atomic write) when new data has arrived and check that in update/draw and synchronize only then. Then you can use a mutex for the shared data. The sync part is likely to be very efficient vs. the rest of your code so it shouldn't affect your draw loop much. I like this for the simplicity and it may be what I go with :)  I assume your game is real-time since you wrote that you use UDP and you haven't said otherwise. The general solution to this problem is: To implement buffering of states from the network which trades latency for stability. To decouple the simulation from the rendering loop. To interpolate between two or more states instead of treating the states as absolute. To implement client-prediction. Buffering Buffering gives you increased stability because the simulation will always lag behind the data you recieve. That way you will never run out of data to use for the simulation on the receiving end. The natural consequence of this is a small increase in latency. With a good heuristic (right-sized buffer) the simulation will never catch up with the buffer i.e the synchronization will never block unless the communication channel itself becomes unstable over a longer time period. That could be sudden change in bandwidth available packet loss severely increased latency etc. You can also use a synchronization primitive which blocks on a timeout say 1 ms. When a timeout occurs you can use extrapolation instead of interpolation (see below) discard updating the missing states all together and only update the stuff handled by the client prediction (if any) or refuse to update anything until you get contact with the remote side again. Simulation-loop decoupling Your simulation should sample at a constant frequency. By decoupling simulation from rendering you can render at a higher frequency than your simulation does for sampling. For example your simulation can run at 30 fps but you can still render as fast as your graphics card allows say 300 fps. This means that you need to send less data over the network but without sacrificing animation quality. Save bandwidth interpolate between samples Smoothly interpolate between two samples instead of treating the data as absolute. And example could be player positions in a game. If you simple move or snap a player position using a vector directly you would need some insanely high sampling frequency which would flood your network. By using linear or cubic interpolation between two or three points respectively you get a smooth motion but with very few sample points. This of course requires a point to interpolate to so it will increase the latency with one delta time i.e your time step. Summary EDIT: Ok so how are these concepts directly related to your problem? Your problem is a design issue; you simply just feed the data directly to your simulation and rendering as they arrive on the wire. Using a mutex whose wait() function has a timeout fixes your blocking issue but not the data starvation. Some latency is better than frequent timeouts. This is why I recommend implementing buffering and decrease the bandwidth usage to a sane level. Only send as much data you need to get a result which is good enough. Responsive-wise and aesthetic-wise. For excellent insight on game network theory visit the articles on Glenn Fiedler's blog : Gaffer on games : networking for game-programmers Gaffer on games : game-physics OMG I can't thank you enough... I was actually headed in the buffer direction before I started rethinking it again. You're welcome :-)  Mads' suggestions are very good so I'll just add this. Along the lines of interpolation it can be very valuable to do use client-side prediction. Synchronize position and velocity (and optionally acceleration) rather than just position. As an additional improvement when an update arrives that indicates that the current position is in error interpolate to the new correct position over a few frames rather than snapping directly to it. That way the client can continue to predict object positions even if server updates are infrequent. If your objects move smoothly then this can work very well. This is the method that most mutliplayer games use. Updates can be made only a few times per second (to conserve bandwidth) and variable network lag can be handled.  For an example of real-world game that does this you could of course browse through some suitable Quake sourcecode. Quake 3 Arena uses OpenGL and uses UDP networking for its multiplayer.
393,A,"Bluetooth service problem I need to create a custom bluetooth service and I have to develop it using c++. I read a lot of examples but I didn't success in publishing a new service with a custom UUID. I need to specify a UUID in order to be able to connect to the service from an android app. This is what i wrote: GUID service_UUID = { /* 00000003-0000-1000-8000-00805F9B34FB */ 0x00000003 0x0000 0x1000 {0x80 0x00 0x00 0x80 0x5F 0x9B 0x34 0xFB} }; SOCKET s s2; SOCKADDR_BTH sab if (WSAStartup(MAKEWORD(2 2) &wsd) != 0) return 1; printf(""installing a new service\n""); s = socket(AF_BTH SOCK_STREAM BTHPROTO_RFCOMM); if (s == INVALID_SOCKET) { printf (""Socket creation failed error %d\n"" WSAGetLastError()); return 1; } memset (&sab 0 sizeof(sab)); sab.addressFamily = AF_BTH; sab.port = BT_PORT_ANY; sab.serviceClassId = service_UUID; if (0 != bind(s (SOCKADDR *) &sab sizeof(sab))) { printf (""bind() failed with error code %d\n"" WSAGetLastError()); closesocket (s); return 1; } int result=sizeof(sab); getsockname(s(SOCKADDR *) &sab &result ); printSOCKADDR_BTH(sab); if(listen (s 5) == 0) printf(""listen() is OK! Listening for connection... :)\n""); else printf(""listen() failed with error code %d\n"" WSAGetLastError()); printf(""waiting connection""); for ( ; ; ) { int ilen = sizeof(sab2); s2 = accept (s (SOCKADDR *)&sab2 &ilen); printf (""accepted""); } if(closesocket(s) == 0) printf(""closesocket() pretty fine!\n""); if(WSACleanup () == 0) printf(""WSACleanup() is OK!\n""); return 0; When i print the SOCKADDR_BTH structure retrieved with get getsockname i get an UUID that is not the mine. Furthermore if i use the UUID read from getsockname to connect the Android application the connection fails with this exception: java.io.IOException: Service discovery failed Could you help me?? Thanks! ""Discovery Failed"" Means the phone sent out a broadcast message asking for a connection with the specified bluetooth MAC address but nobody with that address responded. Double check the Bluetooth MAC address of the remote device. if i'm not wrong this is the only way i can establish a bluetooth connection through android http://developer.android.com/reference/android/bluetooth/BluetoothDevice.html#createRfcommSocketToServiceRecord%28java.util.UUID%29 How could I use MAC address? BTDevice = BTAdapter.getRemoteDevice(PeerMAC); BTSocket = BTDevice.createRfcommSocketToServiceRecord(UUID); this is what i do..but UUID is not a mac address..if i'm not wrong.. Your statement doesn't make sense. Of course the UUID is not a MAC address. If that is what you're already doing then you need to double check the bluetooth MAC that you are using. You cannot make a bluetooth connection without knowing the peer device's MAC as shown in my code snippet above. sorry..i didn't realize the first statement!i will try that.Thanks!  You have to create the SDP record and publish it with WSASetService before bind and wait for connections. Follow this tutorial and you'll be able to receive connections. Hi..yes..i saw that tutorial but(QUOTE):""The format of the SDP information that’s published is so complex that Windows CE provides a special COM control to construct and deconstruct SDP records...As a shortcut many Bluetooth applications compose a generic record either hand-assembling the record or using an example tool named BthNsCreate (available in the Windows CE SDK) that’s provided in the Platform Builder."" I have already asked about BthNsCreate http://stackoverflow.com/questions/2763966/how-to-get-an-sdp-record-for-bluetooth-service Any suggestions? You are using BT_PORT_ANY in the structure so that's why you get a UUID different from yours I think. The documentation of SOCKADDR_BTH structure says that the serviceClassId is ignored if you're using bind. In my application(server) I set the channel(the port parameter) to 0 and the last byte of SDP to 0; At the offset 8 of the record you have the UUID of your service and the last byte the channel. Publishing the record the UUID and channel are binded and you can look for the channel in the client using your UUID. In the client app I don't set the channel only the UUID of the service and the client make the SDP search to find the channel."
394,A,"Let two UDP-servers listen on the same port? I have a client which sends data via UDP-broadcast. (To let's say 127.0.0.255:12345) Now I want to have multiple servers listening to this data. To do so on a local machine they need to share the port 12345 for listening. My question is if that is possible if there are any disadvantages and if there could be problems with this approach. There is one alternative which unfortunately brings with a lot of overhead: Implement some kind of registration-process. On startup each server tells the client its port. The client then sends the messages to each port (having to send the data multiple times some kind of handshaking needs to be implemented...) Do you know any better alternative? If that matters: I'm using C++ with Boost::Asio. The software should be portable (mainly Linux and Windows). I've tried to use the following method from boost::asio::udp::socket `set_option(udp::socket::reuse_address(true));` with no success... There is almost no documentation about that does anyone has a hint about that? See also http://stackoverflow.com/questions/14388706/ You will have to bind the socket in both processes with the SO_REUSEPORT option. If you don't specify this option in the first process binding in the second will fail. Likewise if you specify this option in the first but not the second binding in the second will fail. This option effectively specifies both a request (""I want to bind to this port even if it's already bound by another process"") and a permission (""other processes may bind to this port too""). See section 4.12 of this document for more information. @MOnsDaR: What do you mean ""not possible?"" From that document: *""The SO_REUSEPORT flag allows multiple processes to bind to the same address provided all of them use the SO_REUSEPORT option.""* It doesn't show anything of the sort. It shows that it *is* possible and it shows *how.* I marked this answer as ""correct"" because it shows that it is not possible to listen at 1 port with serveral server at the same time. The linked url supports this answer. If another answer is posted which shows that it is possible or which has a more detailled explanation i'll mark the new answer as accepted instead of this. I probably ran into problems with Boost::Asio. It seems as if REUSEPORT is not added there and there is no possibility to activate it. Sorry for my confusion according to the document it should be possible.  Several sources explain that you should use SO_REUSEADDR on windows. But none mention that it is possible to receive UDP message with and without binding the socket. The code below binds the socket to a local listen_endpoint that is essential because without that you can and will still receive your UDP messages but by default your will have exclusive ownership of the port. However if you set reuse_address(true) on the socket (or on the acceptor when using TCP) and bind the socket afterwards it will enable multiple applications or multiple instances of your own application to do it again and everyone will receive all messages. // Create the socket so that multiple may be bound to the same address. boost::asio::ip::udp::endpoint listen_endpoint( listen_address multicast_port); // == important part == socket_.open(listen_endpoint.protocol()); socket_.set_option(boost::asio::ip::udp::socket::reuse_address(true)); socket_.bind(listen_endpoint); // == important part == boost::array<char 2000> recvBuffer; socket_.async_receive_from(boost::asio::buffer(recvBuffer) m_remote_endpoint boost::bind(&SocketReader::ReceiveUDPMessage this boost::asio::placeholders::error boost::asio::placeholders::bytes_transferred)  This answer is referenced to the answer of cdhowie who linked a document which states that SO_REUSEPORT would have the effect I'm trying to achieve. I've researched how and if this option is implemented and focused mainly on Boost::Asio and Linux. Boost::Asio does only set this option if the OS is equal to BSD or MacOSX. The code for that is contained in the file boost/asio/detail/reactive_socket_service.hpp (Boost Version 1.40 in newer versions the code has been moved into other files). I've wondered why Asio does not define this option for platforms like Linux and Windows. There are several references discussing that this is not implemented in Linux: http://kerneltrap.org/mailarchive/linux-netdev/2008/8/7/2851754 http://kerneltrap.org/mailarchive/linux-kernel/2010/6/23/4586155 There also is a patch which should add this functionality to the kernel: http://kerneltrap.org/mailarchive/linux-netdev/2010/4/19/6274993 I don't know if this option is existing for Windows but by defining portable as an attribute for software which runs on Linux too this means that SO_REUSEPORT is OS specific and there is no portable solution for my question. In one of the discussions I've linked it is recommended for UDP to implement a master-listener which then provides the incoming data to multiple slave-listeners. I will mark this answer as accepted (though feeling kind of bad by accepting my own answer) because it points out why the approach of using SO_REUSEPORT will fail when trying to use it with portable software."
395,A,"Artificially create a connection timeout error I've had a bug in our software that occurs when I receive a connection timeout. These errors are very rare (usually when my connection gets dropped by our internal network). How can I generate this kind of effect artificially so I can test our software? If it matters the app is written in C++/MFC using CAsyncSocket classes. Edit: I've tried using a non-existant host and I get the socket error: WSAEINVAL (10022) Invalid argument My next attempt was to use Alexander's suggestion of connecting to a different port e.g. 81 (on my own server though). That worked great. Exactly the same as a dropped connection (60 second wait then error). Thank you! Connect to a non-routable IP address such as 10.255.255.1. This worked for me. Idem and I guess this is a better answer since google.com:81 might be reachable one day. ... and because sending random packets to other people's servers from your unit tests is rude. This won't always work. For instance with Python's `urllib` this will return a 'No route to host' exception. FYI May be it isn't the most universal but the simplest solution! In Java you get ""Connection timed out""  There are a couple of tactics I've used in the past to simulate networking issues; Pull out the network cable Switch off the switch (ideally with the switch that the computer is plugged into still being powered so the machine maintains it's ""network connection"") between your machine and the ""target"" machine Run firewall software on the target machine that silently drops received data One of these ideas might give you some means of artifically generating the scenario you need  I had issues along the same lines you do. In order to test the software behavior I just unplugged the network cable at the appropriate time. I had to set a break-point right before I wanted to unplug the cable. If I were doing it again I'd put a switch (a normally closed momentary push button one) in a network cable. If the physical disconnect causes a different behavior you could connect your computer to a cheap hub and put the switch I mentioned above between your hub and the main network. -- EDIT -- In many cases you'll need the network connection working until you get to a certain point in your program THEN you'll want to disconnect using one of the many suggestions offered.  How about a software solution: Install SSH server on the application server. Then use socket tunnel to create a link between your local port and the remote port on the application server. You can use ssh client tools to do so. Have your client application connect to your mapped local port instead. Then you can break the socket tunnel at will to simulate the connection timeout.  I would like to point everybody's attention to pathod With a config (taken from their examples) of 200:b@100:dr you'll get a connection that randomly drops.  Connect to an existing host but to a port that is blocked by the firewall that simply drops TCP SYN packets. For example www.google.com:81. emu's response below works better. This answer is simple and works just like @emu's answer below. I understand this answer didn't intend to suggest using google.com:81 but the point here is to use a different port that is blocked. So you can always use :.  You might install Microsoft Loopback driver that will create a separate interface for you. Then you can connect on it to some service of yours (your own host). Then in Network Connections you can disable/enable such interface...  Depending on what firewall software you have installed/available you should be able to block the outgoing port and depending on how your firewall is setup it should just drop the connection request packet. No connection request no connection timeout ensues. This would probably work better if it was implemented at a router level (they tend to drop packets instead of sending resets or whatever the equivalent is for the situation) but there's bound to be a software package that'd do the trick too.  The easiest thing would be to drop your connection using CurrPorts. However in order to unit test your exception handling code perhaps you should consider abstracting your network connection code and write a stub mock or decorator which throws exceptions on demand. You will then be able to test the application error-handling logic without having to actually use the network. With CurrPorts I seem to only be able to close the connection (which causes the next `recv()` to fail immediately) but could not find a way to simulate a timeout (i.e. no more data is transferred but the connection stays open).  I like to use CurrPorts to manipulate network connections when debugging. Sometimes just unplugging the network cable will do just as fine (as posted by others). With CurrPorts I seem to only be able to close the connection (which causes the next `recv()` to fail immediately) but could not find a way to simulate a timeout (i.e. no more data is transferred but the connection stays open).  You can use the Python REPL to simulate a timeout while receiving data (i.e. after a connection has been established successfully). Nothing but a standard Python installation is needed. Python 2.7.4 (default Apr 6 2013 19:54:46) [MSC v.1500 32 bit (Intel)] on win32 Type ""help"" ""copyright"" ""credits"" or ""license"" for more information. >>> import socket >>> s = socket.socket(socket.AF_INET socket.SOCK_STREAM) >>> s.bind(('localhost' 9000)) >>> (clientsocket address) = s.accept() Now it waits for an incoming connection. Connect whatever you want to test to localhost:9000. When you do Python will accept the connection and accept() will return it. Unless you send any data through the clientsocket the caller's socket should time out during the next recv().  You can try to connect to one of well-known Web sites on a port that may not be available from outside - 200 for example. Most of firewalls work in DROP mode and it will simulate a timeout for you.  Despite it isn't completely clear which one the OP wants to test: there's a difference between attempting a connection to a non-existent host/port and a timeout of an already established connection. I would go with Rob and wait until the connection is working and then pull the cable. Or - for convenience - have a virtual machine working as the test server (with bridged networking) and just deactivating the virtual network interface once the connection is established.  Plug in your network cable into a switch which has no other connection/cables. That should work imho."
396,A,"What does the ""s"" mean in the structure? Here's a simple question. What's the meaning of the leading letter ""s"" in the sin_family sin_port sin_addr and sin_zero? struct sockaddr_in { short int sin_family; // Address family AF_INET unsigned short int sin_port; // Port number struct in_addr sin_addr; // Internet address unsigned char sin_zero[8]; // Same size as struct sockaddr }; Thanks. probably for *s*ockaddr? This comes from Berkeley back when LSD was still legal. So very obvious in their naming choices :/ All kidding aside this dates back to very early K&R C where structure members didn't have their own namespace. Which required you to come up with distinct names for the structure members that wouldn't collide with identifiers in the global namespace. Painful. Prefixing the names with an abbreviation of the structure name was the common approach. Thus ""sockaddr_in"" becomes ""sin"". Thanks it is convincing. BTW what is LSD? A synthetic drug that enables sustained unpowered flight. http://en.wikipedia.org/wiki/Lsd +1 for the B^HLSD reference.. no actually for citing the historical reason and getting it right. Thanks guys now I know too much. :)  ""sin"" stands for ""Socket INternet"" in this context.  sin is repeating the name of the sockaddr_in structure i.e. Socket INternet."
397,A,"What is the BSD (or portable) way to get ToS byte (like IP_RECVTOS from linux)? What is the right (portable stable) way to get the ToS byte of a received packet? I'm doing UDP with recvmsg() and on linux I can get the ToS if I setsockopt() IP_RECVTOS/IPV6_RECVTCLASS but IP_RECVTOS doesn't seem to be available on my BSD systems. What is the right way to do this? I primarily want this to work on the BSDs and Solaris. Edit: To clarify: I currently use recvmsg() where I get the TTL and TOS in the msg_control field on Linux but in order to get TTL and TOS I need to setsockopt()-enable IP_RECVTTL and IP_RECVTOS. And since Solaris and BSD (working with FreeBSD at the moment) don't have IP_RECVTOS from what I can see I don't get TOS when looping over the CMSG data. I tried enabling IP_RECVOPTS and IP_RECVRETOPTS but I still don't get any IP_TOS type CMSG. Edit 2: I want ToS to be able to verify (as much as possible) that it wasn't overwritten in transit. If for example a VoIP app all of a sudden notices that it's not getting EF tagged packets then something is wrong and there should be an alarm. (and no I'm not expecting EF to be respected or preserved over the public internet) I want TTL basically just because I can. Hypothetically this could be used to trigger ""something changed in the network between me and the other side"" alerts which can be useful to know if somethings stops working at the same time. I searched and searched but to no avail. Looks like the only ""portable"" way is to use Raw Sockets. Can I ask why do you require ToS or TTL at UDP level? The ""proper"" and standard solution is probably to use cmsg(3). You'll find a complete description in Stevens' ""Unix network programming"" book a must-read. Google Code Search found me this example of use. That's what I'm doing. But I only get cmsgs for what I have set up setsockopt for. This is the way I get the TTL by setting IP_RECVTTL and then getting it in my msg_control from recvmsg() but only linux seems to even *have* IP_RECVTOS which seems to be required for the kernel to send ToS info in cmsg. From the zorp changelog: lib/packsock.c: fix Solaris compilation by enabling IP_RECVTOS only if ZORPLIB_ENABLE_TOS is defined. Implying that it won't work on solaris.  I was thinking if you can create two sockets. One socket of type DGRAM used exclusively for sending One Raw socket used exclusively for receiving. Since you are using UDP you can call a bind + recvFrom on the Raw Sock Fd and then manually unpack the IP header to determine the TOS or TTL. When you want to send use the DGRAM sockFd so you dont have to bother to actually create the UDP & IP packet yourself. There may be issues like the kernel may pass the received buffer to both sockets or to the UDP socket instead of Raw socket or just to the Raw socket. If that is the case (or if it is implementation dependent) then we are back to square one. However you can try calling bind on the Raw socket and see if it helps. I am aware this maybe a hack but searching on the net for a setsockopt for BSD returned nothing. EDIT: I wrote a sample program It kind of achieves the objective. The code below creates two sockets (one raw & one udp). The udp socket is bound on the actual port I am expecting to receive data whereas the raw socket is bound on Port 0. I tested this on Linux and like I expected any data for port 2905 is received by both the sockets. I am however able to retrieve the TTL & TOS values. Dont downvote for the quality of the code. I am just experimenting whether it will work. Further EDIT: Disabled the receive by UDP socket. I have further enhanced the code to disable the receive by the UDP packet. Using setsockopt I set the UDP's socket receive buffer to 0. This ensures the kernel does not pass the packet to the UDP socket. IMHOYou can now use the UDP socket exclusively for sending and the raw socket for reading. This should work for you in BSD and Solaris also. #include<stdio.h> #include<stdlib.h> #include<sys/types.h> #include<sys/socket.h> #include<netinet/in.h> #include<netinet/ip.h> #include<arpa/inet.h> #include<string.h> #include ""protHeaders.x"" #include ""gen.h"" int main(void) { S32 rawSockFd; S32 udpSockFd; struct sockaddr_in rsin; struct sockaddr_in usin; S32 one = 1; const S32* val = &one; struct timeval tv; fd_set rfds; S32 maxFd; S16 ret; S8 rawBuffer[2048]; S8 udpBuffer[2048]; struct sockaddr udpFromrawFrom; socklen_t rLenuLen; memset(rawBuffer0sizeof(rawBuffer)); memset(udpBuffer0sizeof(udpBuffer)); memset(udpFrom0sizeof(udpFrom)); memset(rawFrom0sizeof(rawFrom)); if ((rawSockFd = socket(PF_INETSOCK_RAWIPPROTO_UDP)) < 0) { perror(""socket:create""); RETVALUE(RFAILED); } /* doing the IP_HDRINCL call */ if (setsockopt(rawSockFdIPPROTO_IPIP_HDRINCLvalsizeof(one)) < 0) { perror(""Server:setsockopt""); RETVALUE(RFAILED); } rsin.sin_family = AF_INET; rsin.sin_addr.s_addr = htonl(INADDR_ANY); rsin.sin_port = htons(0); usin.sin_family = AF_INET; usin.sin_addr.s_addr = htons(INADDR_ANY); usin.sin_port = htons(2905); if(bind(rawSockFd(struct sockaddr *)&rsin sizeof(rsin)) < 0 ) { perror(""Server: bind failed""); RETVALUE(RFAILED); } if ((udpSockFd = socket(PF_INETSOCK_DGRAMIPPROTO_UDP)) < 0) { perror(""socket:create""); RETVALUE(RFAILED); } if(bind(udpSockFd(struct sockaddr *)&usin sizeof(usin)) < 0 ) { perror(""Server: bind failed on udpsocket""); RETVALUE(RFAILED); } /*set upd socket receive buffer to 0 */ one = 0; if (setsockopt(udpSockFdSOL_SOCKETSO_RCVBUF(char *)&onesizeof(one)) < 0) { perror(""Server:setsockopt on udpsocket failed""); RETVALUE(RFAILED); } tv.tv_sec = 0; tv.tv_usec = 0; maxFd = (rawSockFd > udpSockFd)? rawSockFd:udpSockFd; while(1) { FD_ZERO(&rfds); FD_SET(rawSockFd&rfds); FD_SET(udpSockFd&rfds); ret = select(maxFd+1&rfds00&tv); if ( ret == -1) { perror(""Select Failed""); RETVALUE(RFAILED); } if(FD_ISSET(rawSockFd&rfds)) { printf(""Raw Socked Received Message\n""); if(recvfrom(rawSockFdrawBuffersizeof(rawBuffer)0&rawFrom&rLen) == -1) { perror(""Raw socket recvfrom failed""); RETVALUE(RFAILED); } /*print the tos */ printf(""TOS:%x\n""*(rawBuffer+1)); printf(""TTL:%x\n""*(rawBuffer+8)); } if(FD_ISSET(udpSockFd&rfds)) { printf(""UDP Socked Received Message\n""); if(recvfrom(udpSockFdudpBuffersizeof(udpBuffer)0&udpFrom&uLen) == -1) { perror(""Udp socket recvfrom failed""); RETVALUE(RFAILED); } printf(""%s\n""udpBuffer); } } RETVALUE(ROK); } Awesome Aditya! The only downside I see is that it requires root privileges for the raw socket but for OSs that don't have IP_RECVTOS this is probably the best answer. I will give it a try. Thomas another thing. With a raw socket on port 0 you will receive all UDP data for all ports destined to your node. When you do a recvfrom you can either look at the ""from"" address to determine whether it is for your application (inefficient) or you can call *connect* with the IP:port of the remote side that you are expecting to send/receive data to. In case there are multiple *remote* sides you can call *connect* multiple times too. Thanks for the appreciation. Hmm... will that work though? I would have to bind() both sockets to the same address:port. Its worth a try especially. Let me try writing some code and see how it spans out. Hey Thomas I have posted a sample program. Like I said I am able to retrieve the TTL & TOS without doing any sockopt system call. However the received datagram is being received by both sockets. If you think this is something you can work with we can think of something to avoid that too. +1 for providing code.  My understanding is that firstly BSD does not support IP_RECVTOS like functionality and secondly BSD raw sockets do not support the reception of UDP nor TCP packets. However there are two other ways of doing this firstly by using the /dev/bpf interface - either directly or via libpcap. Or secondly by using DIVERT sockets which allow for diversion of specified traffic flows to userland. Has anyone actually tested the code above on a BSD box? (it may work on Solaris...) On Linux this approach will work but as mentioned it is also possible (and more convenient) to use setsockopt() with IP_TOS on the outgoing socket to set the outgoing TOS byte and setsockopt() with IP_RECVTOS on the incoming socket and use recvmsg() to retrieve the TOS byte.  Unfortuneatly this sort of thing usually varies across different *ixs. On Solaris you want to use getsockopt with IP_TOS; I don't know about BSD. See man 7 ip for details. But isn't IP\_TOS just used to get and set (with getsockopt() and setsockopt()) the TOS for the *outgoing* TOS. Are you saying I can use it on a packet by packet bases to get *incoming* TOS bytes?"
398,A,"Trying to packetize TCP with non-blocking IO is hard! Am I doing something wrong? Oh how I wish TCP was packet-based like UDP is! [see comments] But alas that's not the case so I'm trying to implement my own packet layer. Here's the chain of events so far (ignoring writing packets) Oh and my Packets are very simply structured: two unsigned bytes for length and then byte[length] data. (I can't imagine if they were any more complex I'd be up to my ears in if statements!) Server is in an infinite loop accepting connections and adding them to a list of Connections. PacketGatherer (another thread) uses a Selector to figure out which Connection.SocketChannels are ready for reading. It loops over the results and tells each Connection to read(). Each Connection has a partial IncomingPacket and a list of Packets which have been fully read and are waiting to be processed. On read(): Tell the partial IncomingPacket to read more data. (IncomingPacket.readData below) If it's done reading (IncomingPacket.complete()) make a Packet from it and stick the Packet into the list waiting to be processed and then replace it with a new IncomingPacket. There are a couple problems with this. First only one packet is being read at a time. If the IncomingPacket needs only one more byte then only one byte is read this pass. This can of course be fixed with a loop but it starts to get sorta complicated and I wonder if there is a better overall way. Second the logic in IncomingPacket is a little bit crazy to be able to read the two bytes for the length and then read the actual data. Here is the code boiled down for quick & easy reading: int readBytes; // number of total bytes read so far byte length1 length2; // each byte in an unsigned short int (see getLength()) public int getLength() { // will be inaccurate if readBytes < 2 return (int)(length1 << 8 | length2); } public void readData(SocketChannel c) { if (readBytes < 2) { // we don't yet know the length of the actual data ByteBuffer lengthBuffer = ByteBuffer.allocate(2 - readBytes); numBytesRead = c.read(lengthBuffer); if(readBytes == 0) { if(numBytesRead >= 1) length1 = lengthBuffer.get(); if(numBytesRead == 2) length2 = lengthBuffer.get(); } else if(readBytes == 1) { if(numBytesRead == 1) length2 = lengthBuffer.get(); } readBytes += numBytesRead; } if(readBytes >= 2) { // then we know we have the entire length variable // lazily-instantiate data buffers based on getLength() // read into data buffers increment readBytes // (does not read more than the amount of this packet so it does not // need to handle overflow into the next packet's data) } } public boolean complete() { return (readBytes > 2 && readBytes == getLength()+2); } Basically I need feedback on my code and overall process. Please suggest any improvements. Even overhauling my entire system would be okay if you have suggestions for how better to implement the whole thing. Book recommendations are welcome too; I love books. I just get the feeling that something isn't quite right. Here's the general solution I came up with thanks to Juliano's answer: (feel free to comment if you have any questions) public void fillWriteBuffer() { while(!writePackets.isEmpty() && writeBuf.remaining() >= writePackets.peek().size()) { Packet p = writePackets.poll(); assert p != null; p.writeTo(writeBuf); } } public void fillReadPackets() { do { if(readBuf.position() < 1+2) { // haven't yet received the length break; } short packetLength = readBuf.getShort(1); if(readBuf.limit() >= 1+2 + packetLength) { // we have a complete packet! readBuf.flip(); byte packetType = readBuf.get(); packetLength = readBuf.getShort(); byte[] packetData = new byte[packetLength]; readBuf.get(packetData); Packet p = new Packet(packetType packetData); readPackets.add(p); readBuf.compact(); } else { // not a complete packet break; } } while(true); } Wait what? TCP is just as packet-based as UDP... @tzaman at the IP level everything is transmitted in packets. But whereas with UDP you send and receive ""datagram packets"" TCP is stream-based; it tries to imitate a stream of data as if you are reading/writing a file. Do you know that there is a [Stream Control Transmission Protocol](http://en.wikipedia.org/wiki/Stream_Control_Transmission_Protocol)? And you can implement it over UDP if your system doesn't support it. I'd refrain from reinventing the wheel. ""It loops over the results and tells each Connection to read()"" this happens in the same thread? Basically trying to understand what this means ""First only one packet is being read at a time"" @Klerk yes the ""PacketGatherer"" thread does selector.select() and then loops on the results of that select operation. By one packet at a time I mean that a Connection tells the current packet in progress to read its data and if the packet is complete adds it to the queue. It doesn't try to read a second packet if data is still remaining. I can implement it to do so but I feel it would only make things messier; I want to clean up this existing code and hopefully find a more elegant way to read in an arbitrary number of packets in one go. @Juliano I'd not heard of SCTP before. It sounds really cool! Looks like it's not natively supported in Java but planned for version 7. For learning purposes I think I'll continue with my TCP packetization but perhaps in Java 7 I'll look into replacing my network system with SCTP. Ignoring client disconnects and server shutdown for now here's more or less traditional structure of a socket server: Selector handles sockets: polls open sockets if it's the server socket create new Connection object for each active client socket find the Connection call it with event (read or write) Connection (one per socket) handles I/O on one socket: Communicates to Protocol via two queues input and output keeps two buffers one for reading one for writing and respective offsets on read event: read all available input bytes look for message boundaries put whole messages onto Protocol input queue call Protocol on write event: write the buffer or if it's empty take message form output queue into buffer start writing it Protocol (one per connection) handles application protocol exchange on one connection: take message from input queue parse application portion of the message do the server work (here's where the state machine is - some messages are appropriate in one state while not in the other) generate response message put it onto output queue That's it. Everything could be in a single thread. The key here is separation of responsibilities. Hope this helps.  I think you're approaching the issue from a slightly wrong direction. Instead of thinking of packets think of a data structure. That's what you're sending. Effectively yes it's an application layer packet but just think of it as a data object. Then at the lowest level write a routine which will read off the wire and output data objects. That will give you the abstraction layer I think you're looking for. OP wants to read *packets* from a *stream* which can be off by an arbitrary number of bytes. You are completely ignoring that part. Actually I thought what he was doing looks exactly like a standard use of tcp. It's guaranteeing the delivery but you have to reassemble your data on the far side. Same type of thing I've done many times when reading raw tcp data.  Can't you just read whatever number of bytes that are ready to be read and feed all incoming bytes into a packet parsing state machine? That would mean treating the incoming (TCP) data stream like any other incoming data stream (via serial line or USB a pipe or whatever...) So you would have some Selector determining from which connection(s) there are incoming bytes to be read and how many. Then you would (for each connection) read the available bytes and then feed those bytes into a (connection specific) state machine instance (the reading and feeding could be done from the same class though). This packet parsing state machine class would then spit out finished packets from time to time and hand those over to whoever will handle those complete and parsed packets. For an example packet format like  2 magic header bytes to mark the start 2 bytes of payload size (n) n bytes of payload data 2 bytes of checksum the state machine would have states like (try an enum Java has those now I gather) wait_for_magic_byte_0 wait_for_magic_byte_1 wait_for_length_byte_0 wait_for_length_byte_1 wait_for_payload_byte (with a payload_offset variable counting) wait_for_chksum_byte_0 wait_for_chksum_byte_1 and on each incoming byte you can switch the state accordingly. If the incoming byte does not properly advance the state machine discard the byte by resetting the state machine to wait_for_magic_byte_0. I'm sure I 'can' given the design of such a thing. Might you elaborate on this possibly give the setup of such a ""packet parsing state machine""? I admit that I do find it odd that an IncomingPacket is reading into itself; typically this is where a factory pattern or something comes in so maybe that's what you are proposing? In any case I would love for this answer to be more elaborate! Thanks for the elaboration. I probably need to read more on ""state machines"" in the context of programming. My existing code seems to pretty much follow what you've said in a series of if statements but I'm not sure how to transform that into a state machine without basically becoming a set of more complicated if statements. Hmm and I just realized you are proposing parsing the stream one byte at a time. I wonder what the performance hit of that would be; trying not to venture into premature optimization of course.  Probably this is not the answer you are looking for but someone would have to say it: You are probably overengineering the solution for a very simple problem. You do not have packets before they arrive completely not even IncomingPackets. You have just a stream of bytes without defined meaning. The usual the simple solution is to keep the incoming data in a buffer (it can be a simple byte[] array but a proper elastic and circular buffer is recommended if performance is an issue). After each read you check the contents of the buffer to see if you can extract an entire packet from there. If you can you construct your Packet discard the correct number of bytes from the beginning of the buffer and repeat. If or when you cannot extract an entire packet you keep those incoming bytes there until the next time you read something from the socket successfully. While you are at it if you are doing datagram-based communication over a stream channel I would recommend you to include a magic number at the beginning of each ""packet"" so that you can test that both ends of the connection are still synchronized. They may get out of sync if for some reason (a bug) one of them reads or writes the wrong number of bytes to/from the stream. D'oh! I think you might be right this maybe be a better approach. Definitely something to consider. It also gains the benefit of not needing to parse the stream byte-by-byte like ndim is suggesting which seems overkill for this. Whether parsing byte by byte as the data comes in or examining the buffer content byte by byte... you will still need to take a look at each and every byte. @ndim That's not true. My current implementation uses a temporary ByteBuffer of 2 bytes to get the length and after the length is received a ByteBuffer is created (backed by a byte[]) of the exact length. Then each time data is available SocketChannel.read() is called with the ByteBuffer and the integer return value (# chars read) is used to keep track of how many total bytes have been read. This is in contrast to your approach of a state engine that accepts only one byte at a time so I'd need to loop through the data one byte at a time for no particular reason. @ndim Well I guess it's partially true in the sense that at some point yes I will be looking at the data when higher level code extracts meaningful data from the packets and processes it. But this will happen regardless of the stream-to-packet system I use. And I'd like to stick with my current approach which allows SocketChannel.read() to fill the ByteBuffer instead of repeatedly calling something like Socket.getInputStream().read() [which gets one byte]. I've implemented something similar to what Juliano is suggesting and it works. The great thing about TCP of course is that it's reliable. If you lose the connection you can throw away the buffer but if you get a whole ""packet"" you know it's good. It worked amazing and I feel like my code is very clean now. Thanks a lot for clearing my mind when I was overcomplicating the solution! :) (see my edit at the bottom of my question for the methods I wrote given your advice) You are welcome!"
399,A,"Optimizing a LAN server for a game I'm the network programmer on a school game project. We want to have up to 16 players at once on a LAN. I am using the Server-Client model and am creating a new thread per client that joins. However a lot of CPU time is wasted just checking on each thread if the non-blocking port has received anything from the client. I've been reading ""Network Programming for Microsoft Windows"" by Anthony Johns and Jim Ohlund. They mention two different models for server-client apps. 1.) Using overlapped IO socket options Pass the overlapped struct and a WorkerRoutine to WSARecv Call WSAWaitForMultipleEvents() or SleepEX() to set thread to alertable. Handle Received data in the WorkerRoutines. 2.) Using overlapped IO socket options Create Io Completion Port Create ServerWorkerThreads (however many CUPs you have) Associate Completion port with Socket. Call GetQueuedCompletionStatus in the ServerWorkerThread and handle received data. I wanted to know which method would best fit my circumstance. The book says the Completion Port model is great for thousands of clients but that makes me think its made for a big server not for a small LAN game. The WorkerRoutines/Event system seems simpler. Post the code that you feel is wasting time someone may be able to suggest a more optimised solution. Otherwise there's not really a programming question here. Have you read about the C10K problem? It relates to a few of your issues. http://www.kegel.com/c10k.html You should stop using non-blocking ports. They're really most useful for concurrency and you have threads for that. Just make each thread in the team wait on the port and when it unblocks it'll have something to do so you don't waste time spinning. If you have other things to do (like update game state) do that with other threads that don't do any network I/O at all. And make sure to use synchronization primitives (semaphores) to keep from getting out of sync with yourself. That's the super multi-threaded way to do it and you probably should use it. The other way is to have a lock-step advancing state machine that updates all clients round-robin. This is rather useless if you want to play across the internet since varying pings will make your game run at varying speeds. So when my port is blocking does it waste CPU cycles or does it sleep? The thread that is blocking goes to sleep until data arrives."
400,A,"Accessing TCP/IP routing information from a .NET application I need to check if a TCP/IP route already exists from my application and add the route if it doesn't. Now I'm running the route add <destination network ip address> MASK <mask> <gateway ip address> with a Process.Start() and that's fine for me. However as I'm elevating the route command with a ""runas"" verb I need to check first if the route is already configured in order to avoid running the elevated command again. Does anybody know how to get the TCP/IP routing information using .NET Framework? (I don't want to run route print with a Process.Start() and parse the output) Thank you in advance!! I think you have to rely on IpHlpApi.Dll Here is a tutorial on how to use it with P/Invoke Thanks for the link I think it's the solution but my question is why that information is not available to the .NET framework directly? I think it is possible to use System.Management to access WMI. Also I guess a pure .NET solution would only be a wrapper around the IpHlpApi.dll so they didn't bother Yeah you're right but having a wrapper is better than nothing :P thank you very much for your answer."
401,A,"Few question about client application to be networked I have made a registration program. Making use of mysql database. Can I still use the traditional programming for client applications. If I want to network it. Do I have to modify the codes a little in order to make it work? -Please enlighten me I'm just a beginner. I don't have any idea on how this works. Do I have to install wampserver only in one computer? I'm using visual studio 2008. Win forms to be exact. A bit more detail needed here.... Is the IT you want to network the database OR the application? Given the correct connectString and a LAN your application could be installed on many machines simultaneously and all operate correctly. I'm thinking of the database only. There's really no difference between using a mySQL database and a SQL Server or Oracle database for Windows Forms programming. To connect to your mySQL database during development you will need to download and install the MySQL .Net Connector on your development PC. Once you've referenced the MySql.Data DLL in your WinForms project you can then use the MySql database classes to retrieve data: string MyConString = ""SERVER=localhost;"" + ""DATABASE=mydatabase;"" + ""UID=testuser;"" + ""PASSWORD=testpassword;""; MySqlConnection connection = new MySqlConnection(MyConString); MySqlCommand command = connection.CreateCommand(); MySqlDataReader Reader; command.CommandText = ""select * from databaseTable""; connection.Open(); Reader = command.ExecuteReader(); etc.. You will need to install mySQL server on a database server somewhere - one way to do this is indeed using wampServer (although it's a bit heavier than you really need - you definitely don't need Apache or PHP although I guess phpMyAdmin is a good way to manage your mySQL database). Either way you'll probably just want one central database on a single server so you'd just install wamp there. can I install wampserver even if the computer is not a server? yes of course!"
402,A,"Server-client Java distributed application I have to design a distributed application composed by one server (developed in Java) and one or more remote GUI clients (Swing application with windows). As stated before the clients are Swing GUI application that can connect to the server in order to receive and send data. The communication is bidirectional (Server <=> Clients). Data sent over the network is mainly composed by my domain logic objects. Two brief examples: a client calls the server in order to receive data to populate a table inside a window; the server calls client in order to send data to refresh a specific widget (like a button). The amount of data transmitted between server and clients and the frequency of the network calls are not particularly high. Which technology do you suggest me for the server-clients communication? I've in mind one technology suitable for me but I would like to know your opinions. Thanks a lot. I believe sockets should do the trick. They are flexible and not especially hard to code/maintain. Most entry level programmer should also be able to maintain them. They are also fast and adapt to any kind of environment. Unless your server is going to be off-site or you expect to have firewall issues. In that case web services are the way to go since your basic communication happens through port 80. If you want to send domain objects I agree that RMI is a better option. I must admit that my first thought is never RMI but with Java clients and servers it may be a better option. By off-site I meant server is somewhere on the internet while your clients are in a network. Looks like that is not expected to happen. Thanks for your comment. Probably (as suggested by msparer and you) I will adopt RMI for server-client communication. Thanks for your answer. I think socket is an easy way for network communication too. But what about you effort for writing and reading domain objects on sockets? It can be negligible? What do you mean for ""off-site""? I think that server will be placed in a dedicated LAN and accessible from any remote host where my Swing application is installed (software authentication/login will be required).  The first technology that came to my mind was RMI - suitable if you're communicating between java client and java server. But you may get difficulties if you want do switch the client technology to - say - a webinterface. Thanks for your answer. In fact the technology I thinked about is RMI. I don't think that client will be changed in a web interface in the future (Swing technology will be enough). So other than this point do you think that there can be other disadvantages adopting RMI? to be honest I haven't used RMI for a long time. ""Long"" in a sense of 2 years. What I'm using now a lot is SOAP with Apache CXF - which might not be the best choice if you're sending huge Object graphs. The stuff I did with RMI however worked pretty well (I used to sync data between a PDA and a workstation back then). In fact data sent over network for server-clients communication is composed by domain objects. Sometimes these objects can be ""simple"" (for example only one object with two integers and a string) or ""complex"" (a list of many objects sent by the server to the clients). Clearly the implementation of these objects is visible to the servers and to the clients.  I would second msparer's suggestion of RMI except I would just use EJB3 (which uses RMI as the communication protocol). EJB3 are very easy and even if you don't use the other feaures EJB gives you (e.g. security) you can still leverage Container Managed Transactions (CMT). It really does make development easy. As for the server->client communication you would probably want to use JMS. Again using EJB3 this is pretty e3asy to do with annotations. The clients will subscribe to the message service and receive update notifications from the server. And yes I am currently working on an application that does this very thing. Unfortunately we are using EJB2.1. Still it is my opinion that this is where EJBs really shine. Using EJBs in a web app is frequently overkill but in a distributed client/server app they work very well.  You can try using ICE http://www.zeroc.com for establishing server-client connection.  I would go with RMI but implement the whole architecture using Spring framework. This way it is independent of technology used and can be switched to other ways of communication (such as HTTP or other ) with almost no coding. UPDATE: And Spring will allow you to have none of RMI specific code."
403,A,"Downloading ZIP file from FTP and copying to folder within website Finding some problems copying a zip file from an FTP location. It is just copying and empty file so I think there is something wrong with my use of StreamReader or StreamWriter. Here is the code: //read through directory details response string line = reader.ReadLine(); while (line != null) { if (line.EndsWith(""zip"")) //""d"" = dir don't need ""."" or "".."" dirs { FtpWebRequest downloadRequest = (FtpWebRequest)FtpWebRequest.Create(""ftp://"" + ftpHost + line); //new Uri(""ftp://"" + ftpServerIP + DestinationFolder + fileInf.Name)); downloadRequest.Credentials = new NetworkCredential(ConfigurationManager.AppSettings[""FilesUser""] ConfigurationManager.AppSettings[""FilesPass""]); downloadRequest.KeepAlive = false; downloadRequest.UseBinary = true; downloadRequest.Method = WebRequestMethods.Ftp.DownloadFile; string folderToWrite = HttpContext.Current.Server.MapPath(""~/Routing/RoutingFiles/""); string folderToSave = HttpContext.Current.Server.MapPath(""~/Routing/""); StreamReader downloadRequestReader = new StreamReader(downloadRequest.GetResponse().GetResponseStream()); DirectoryInfo downloadDirectory = new DirectoryInfo(folderToWrite); FileInfo file = new FileInfo(Path.Combine(downloadDirectory.FullName line)); if (!file.Exists) { StreamWriter writer = new StreamWriter(Path.Combine(folderToWrite line) false); writer.Write(downloadRequestReader.ReadToEnd()); using (var downloadResponseStream = response.GetResponseStream()) { } } } } By the time it gets to the bottom of that section the file has been copied but is empty so I don't think I'm reading the stream correctly for a zip file. Anyone any ideas? I've seen talk of FileStream being better for downloading Zip files but I couldn't get that to work either. Thanks. Here is an example that downloads a file from an ftp. try { FtpWebRequest request = (FtpWebRequest)WebRequest.Create(ftpAddr + ""test.zip""); request.Credentials = new NetworkCredential(userName password); request.UseBinary = true; // Use binary to ensure correct dlv! request.Method = WebRequestMethods.Ftp.DownloadFile; FtpWebResponse response = (FtpWebResponse)request.GetResponse(); Stream responseStream = response.GetResponseStream(); FileStream writer = new FileStream(""test.zip"" FileMode.Create); long length = response.ContentLength; int bufferSize = 2048; int readCount; byte[] buffer = new byte[2048]; readCount = responseStream.Read(buffer 0 bufferSize); while (readCount > 0) { writer.Write(buffer 0 readCount); readCount = responseStream.Read(buffer 0 bufferSize); } responseStream.Close(); response.Close(); writer.Close(); } catch (Exception e) { Console.WriteLine(e.ToString()); } Edit I'm sorry for the error in previous code. When correcting my previous code I found the following resource useful: example Hi Teletha - making progress but is now bringing up an error saying ""ZipEntry::ReadDirEntry(): Bad signature "" when I try and go into the Zip file to loop through the contents. When I try and open the downloaded zip file just in explorer it brings up an error saying ""The compressed folder... is invalid"". Do you not need a different download method for Zip files? You are correct. It worked for normal files and I assumed it would also work for zip files. I'm sorry. Looking at a solution now. @e-on It should work now. I'm sorry for previous error. Ah fantastic - that's the one. Many thanks :)"
404,A,Realtime multiplayer game over mobile network? I just discovered that my phone is behind a symmetric udp firewall which simply means that udp traffic is not possible. My 3g provider is t-mobile which is one of the biggest in my country. I suspect that many more mobile 3g providers have udp limitations in place. I'm planning on developing a realtime game. udb seemed to be the right solution for quick movement but i can't use udp because then i lose flexibility (play anywhere many places with no wifi) and a big audience (3g providers which block udp). Mobile networks like umts and 3g have high latency so tcp (with resending lost packets and queuing) is not ideal solution. Is there a alternative? Or should drop the idea of a realtime game over 3g? It is very hard to achieve real time over the network with satisfying latency for your game logic (especially if you are making a game like first person shooter you can read about overcoming some latency issues here: networked physics). When talking about mobile network well it gets even harder: Of course you can make your life easier and cover only WIFI connection BUT you will lose all the users that want to play over 3G when no FREE WIFI connection is available (or they simply stuck with 3G connection because they don't know how to change to WIFI). If you chose to cover 3G as well welcome to the operators hell: no UDP no sockets no none-standard ports no long timeouts and hello disconnections hello strange headers and hello weird proxies now multiply it by the number of different operators over the world and Voila' you cover all their issues. I am not trying to scare you just remember these things during your implementation: 1. No sockets or usage of none-standard ports - sockets are not allowed by operators from obvious reasons they don't want you to take their resources because during that time they could serve some other paying customer ;) If your game logic allows it try implementing the protocol without sockets. 2. Test your game with a couple of friends over the seas or use crowd testing services that can do it for you. You can calibrate the latency through your own protocols. 3. Distributing your servers will greatly help with the latency issue. 4. Make sure you are not sending a lot of data over the network be smart be gentle with the device's battery. 5. Compress your data! For full disclosure: I am working at Skiller and we provide multiplayer SDK for Android developers (among others) with free tools like social layer user management revenue generation etc... We saw a lot of issues with multiplayer over 3G and we compensate it with the algorithms we wrote on the client and server side to make developer's lives easier. If you want to try us out: www.skiller-games.com
405,A,Is it possible to upload data as a background process in j2me? Even with a poor network connection? Specifically I've written code which launches a separate thread (from the UI) that attempts to upload a file via HTTP POST. I've found however that if the connection is bad the processor gets stuck on outputstream.close() or httpconnection.getheaderfield() or any read/write which forces data over the network. This causes not only the thread to get stuck but steals the entire processor so even the user interface becomes unresponsive. I've tried lowering the priority of the thread to no avail. My theory is that there is no easy way of avoiding this behavior which is why all the j2me tutorial instruct developers to create a ‘sending data over the network…’ screen instead of just sending everything in a background thread. If someone can prove me wrong that would be fantastic. Thanks! what phone are you using ? I assume the code works fine in the wireless toolkit... It pretty much depends on how you write the code and where you run it. On CLDC the concept of threading is pretty limited and if any thread is doing some long lasting operation other threads might be (and usualy are) blocked by it as well. You should take that into account when designing your application. Oh right I should have clarified: I'm working in MIDP 2.0 + CLDC 1.1.  One important aspect is you need to have a generic UI or screen that can be displayed when the network call in background fails. It is pretty much a must on any mobile app J2ME or otherwise. As Honza said it depends on the design there are so many things that can be done like pre-fetching data on app startup or pre-fetching data based on the screen that is loaded (i.e navigation path) or having a default data set built in into the app etc. Another thing that you can try is a built-in timer mechanism that retries data download after certain amount of time and aborting after say 5 tries or 1-2 minutes and displaying generic screen or error message. Certain handsets in J2ME allow detection of airplane mode if possible you can detect that and promptly display an appropriate screen. Also one design that has worked for me is synchronizing UI and networking threads so that they dont lock up each other (take this bit of advice with heavy dose of salt as I have had quite a few interesting bugs on some samsung and sanyo handsets because of this) All in all no good answer for you but different strategies. Useful strategies indeed. Prefetching does not work in my case since all UI is already saved on phone. (I want to fill a form stored on-device and submit via HTTP POST.) But I think your first comment is saying I must have a static 'sending...' screen while transmitting which sounds right. Thanks!  You can divide your file data into chunks and then upload with multiple retries on failure. This depends on your application strategy . If your priority is to upload a bulk data with out failure. You need to have assemble the chunks on server to build back your data . This may have the overhead for making connections but the chance is high for your data will get uploaded . If you are not uploading files concurrently this will work with ease .
406,A,"How do browsers handle a multiple IP response for a single hostname from DNS? I want to know how this is handles or if there is a standard? Browsers cache DNS Responses for a few minutes and typically attempt a connection with the first IP address returned in the DNS response. The same IP is used until the cache expires. Internet Explorer caches DNS lookups for 30 minutes by default as specified by the DnsCacheTimeout registry setting. Firefox caches DNS lookups for 1 minute controlled by the network.dnsCacheExpiration configuration setting. From: Yahoo Dev Network: Best Practices for Speeding Up Your Web Site Therefore for multiple IP addresses to be used for load-balancing purposes the DNS server must change the order of the addresses supplied in the response choosing the order randomly or in a sequential ""round robin"" fashion. In fact this is usually the default behaviour of DNS servers when they respond to hostnames with multiple A records. There is no standard procedure for deciding which address will be used by the requesting application - a few resolvers attempt to re-order the list to give priority to numerically ""closer"" networks. Some desktop clients do try alternate addresses after a connection timeout of 30-45 seconds. From: Wikipedia: Round robin DNS So what happens when a browser has the entry cached and the server goes down/becomes unreachable. does it mean a requery on the dns to get the new entry. Also do the plugins also follow the browser(like the flash plugin)? or do they have their own thing going on? @Ankur: As for plugins they have their own thing. Most plugins even have provide a full TCP socket API so they can do lookups themselves and in general have nothing to do with the browser except for being plugged into the document. As for your question I believe yes it will remain down until the cache expires but I'm not 100% sure on that... I'm checking if I can find some authoritative answer...  Generally they iterate through the responses and use the first one they can connect to. Er... nop. They try one and if they can't connect to it they fail. Which one is a mistery though because that depends on implementation of the creator of the software."
407,A,read sniffing data over tcp i'm developing application that is listening to the data coming to the pc and store it in a db when i'm trying to use any sniffing software it decode the data and i can read it... but in my code ....i cant read it at all it come in a format like that 1822262151622341817118815518211616121520941131921572041519912321413018224510453482062312258624219217426213385792952422362282081777270129716688629114817282188771708157542505055171418651781981425595109572128317191993018793431541418175198551682143218916536118562071014546919618158204181231187237183188160147127165111798312311810419822146114761993113815821216617541542372062129733198212250147199288115346102031191275215728146245198190171121209115149107193226253199151253205183146112072202559697791491441131572351381412278441552554817712614110121823714822712523618924690185291182071331471286244143181469018522814822821118012620321315924832238219115405615512392145202385512115735771691111055935782371281492476567165158924021493139815144225143762294713291762001113814720516216041120169912317914878167571392103510118386589521910621319622274158971538465206168139190127867123282255271781242497522124211517622131122113236255230254211206911242051832545515823012124925217318223920523316923122925514321122343602492471242........ can any one tell me what kind of data is that and any code to solve it out?? Could you be a bit more specific on what kind of tool/library/etc. you use to sniff this data ? And how are you printing/converting this data you get hold of ? i tried this one System.Text.Encoding.ASCII.GetString(tcpHeader.Data) and I'm able to get the data...but when i run it and wait for the data(it's in XML format) i got it in many packets the it's hard to reread it again... what should i do? do i have to collect the data from every packets then add it all to get the whole message? or is there is a library to do that in the .net To see what a real packet sniffer looks like check out WireShark. There are many different protocols over TCP and many of them are binary. Those that aren't may be using unicode characters which are two-byte characters so an ascii display of them would be meaningless. Anyway the data you're displaying is pretty meaningless. It looks like decimal data are you concatenating a bunch of decimal representations of the binary stream interpreted as byte or integer values? That would explain it. You should start by running the stream through System.TextEncoding.ASCII.Decode You'll probably see some recognizable strings. Then try System.TextEncoding.Unicode.Decode etc.  No we cannot. And the reason is simple we don't know what application you are sniffing. That stream of data could mean anything. But I suggest you print the data in hexadecimal. Maybe the data would make more sense.
408,A,"Pushing data to nodeJS from OSX application I've got an OSX application written in c++ (in particular built with OpenFrameworks) and a server written using nodejs. The application tracks an object wandering around a room and I want to send information about the object's position to the server on a relatively constant (at least 3 times a second) basis. From what I can tell I either need a protocol with minimal amounts of setup overhead or the ability to open up a persistant connection that I can push to. In case you can't tell - I'm not a network programming guru! Does anyone know of libraries on either the node.js side and the C++ side that can accomplish this without a 5 million second latency? From the openFrameworks side you have quite a bunch of possibilities to send data: (Given current OF 007) look at the following examples if they fit your needs: <OF-folder>/apps/addonsExamples/networkTcpClientExample <OF-folder>/apps/addonsExamples/networkUdpSenderExample <OF-folder>/apps/addonsExamples/oscSenderExample For node.js just define a basic socket or something like that to handle the request Here is an example of a simple TCP server which listens on port 1337 and echoes whatever you send it: var net = require('net'); var server = net.createServer(function (socket) { socket.write(""Echo server\r\n""); socket.pipe(socket); }); server.listen(1337 ""127.0.0.1""); (pulled from the nods.js frontpage http://nodejs.org/)"
409,A,"Why do I need a DataHandler? What is the main reason of using a javax.activation.DataHandler? Is it to facilitate the transfer of objects that do not implement Serializable over the network? I.e. for instance I have seen the conversion of files from local file systems to bytes and then create a DataHandler with these bytes and transfer the DataHandler over the network. Is this the reason that one would use DataHandler? You have a lot of questions going on there. Can you be more specific? @Lord:I updated my question.Is this more specific or extra editing is needed? Yes it looks better now. @Martin: If you already go around editing titles please make it really better not only change capitalization. ""Question on ..."" is never a good title. I'll lead off with the start of the description from the API entry for DataHandler: The DataHandler class provides a consistent interface to data available in many different sources and formats. It manages simple stream to string conversions and related operations using DataContentHandlers. Admittedly that's not the clearest description. DataHandler has to do with XML and SOAP which you can see from the the use tab of its API page. Like you I've used it to represent data about an uploaded file as it's being sent from one web service component to another for processing. The Transferable interface that DataHandler implements is not exactly referring to ""transfer"" of the kind serialization deals with. It's about transfer of information between separate components in a program or separate programs not saving an object for later use. See the API entry for Transferable for more. You'll notice that it links to the Drag 'n' Drop Java Tutorial which has little to do with DataHandler but does illustrate a use of Transferable.  There are also performance considerations i.e. using a javax.activation.DataHandler for a SOAP attachment will improve performance. e.g. as mentioned by Oracle ""...Improved Performance: Informal tests have shown that using DataHandler wrappers doubles throughput for image/gif MIME types and multiplies throughput by approximately 1.5 for text/xml or java.awt.Image for image/* types...."" this is from LINK Other references A discussion of Base64 encoding vs. binary attachment types MTOM and DataHandler LINK Apache CXF on MTOM LINK"
410,A,How to build ACE 6.0(network library) as static? ACE supplied solution file for Visual Studio and there were solution files for static and dynamic liking.(ACE.sln and ACE_static.sln) After they release 6.0 there was no static.sln anymore. I can't understand. Why did they remove solution file for static? Is there a reason? I'm putting predefined values for static build by myself. It's very annoying me. Is there other convenient way? Any particular reason you don't want to use the dll ? @DumbCoder I don't want to let end-users know that we use ace. Yes it's not a big deal most of users even don't care about that. But I prefer hiding way. Another reason. When we deploy files to client machines by autoupdaters if a dll is using it's annoying autoupdaters. I've just posted the same question on the comp.soft-sys.ace newsgroup where I got some help from one of the ACE developers: ACE 6.0 Static Builds Basically there's a Perl tool called MPC that's distributed with ACE that they use to compile all the build files (I haven't gotten it to work yet but then that's probably just something I'm doing wrong :) Anyway if you open up the .sln files in Notepad you'll actually see the MPC command line used to build it near the top in the comments. oh thank you! Turns out I needed to run the mwc.pl script from within the directory that contains the ACE.mwc file! Knew it was something simple :) Anyway it works fine now...
411,A,"How sockets relate to common networking APIs This morning I asked a sockets-related question but this is a different beast and so I decided making a separate post altogether. I now have a good idea of what sockets are and the purpose(s) they serve. I'm trying to understand their relation to Java NIO Java RMI and so-called networking APIs like Netty or MINA. Are sockets the basis of all Java-based networking and is NIO the foundation of Java-based networking (i.e. is it the API that provides the socket structures for higher frameworks like Netty or MINA)? If not then what is the foundation of Java's networking capabilities? Can one use sockets (which as I understand it or network-layer constructs) to send (byte-wise) messages compliant with higher protocols such as TCP or UDP HTTP FTP etc? If so I would imagine that development teams might for example make their own HttpProtocol library or SSLProtocol library yes? My understanding is that NIO is the basis of all Java networking and that RMI Netty MINA etc. all extend the NIO framework into their own implementations. Is this correct? If not how do these frameworks relate to each other? Thanks again for all your help! Sockets are the Java basic building blocks for networking. They only allow you to do basic operations: opening a connection (TCP or UDP) writing or reading bytes and closing the connection. They also handle failure cases by throwing exceptions. In the OSI model this is the 4th layer (transport). Higher level APIs are built on top of sockets they allow to do more interesting tasks such as HTTP connections or SSL communication. Java NIO is the new version of the first Java IO API. As far as networking is concerned it essentially brings a new API for non blocking sockets. Higher level APIs are built on top of Java IO and/or Java NIO. Thanks Guillaume! So just to clarify: Java NIO/IO are the standard Java sockets frameworks and other frameworks like Netty or MINA build upon that framework by providing implementations of higher-layer protocols yes? Does the same apply for Java RMI (does RMI extend NIO like Netty/MINA?). Thanks! Right Java NIO/IO implement sockets while other libraries implement higher level protocols on top of these sockets. RMI is a package for communicating with remote objects. RMI uses URL protocols so most of the time it will use sockets for communication. Thanks! I guess I'm confused with RMI because I'm seeing APIs like NIO Netty MINA etc. that are all sockets-based and which implement specific protocols... what protocols does RMI implement and how does this differ than normal client/server communication using say Netty? RMI provides the wiring for calling methods on objects located on a remote server. You could look at it like a protocol on the same level as HTTP SOAP JMS... Guillaume last question! So is it fair to summarize that RMI is an RPC framework that heavily uses sockets (and perhaps other non-sockets technologies) to serialize objects between client and server and because the communication is between 2 programs (the client and the server) then this serialized transmission could be viewed as an ""application layer"" protocol? Thanks again for all your help so far! That's a good summary of what RMI does! RMI doesn't use URL protocols whatever they are.  Are sockets the basis of all Java-based networking For IP protocols yes. There might be special libraries such as JavaComm for serial port communication and JAIN for various telecom protocols that's not socket based and is NIO the foundation of Java-based networking Not really sockets are. see the previous answer. NIO builds on and augments sockets though as well as other things. The most important part is probably that NIO allows you to listen for events on several sockets in a single thread and do non-blocking IO. Netty and Mina takes advantage of the NIO apis. NIO stands for New I/O and was a new API for doing IO introduced in Java 1.4. It does a lot of other stuff than socket IO though. Arguably NIO will allow you to do socket IO more efficently at the cost of more complexity. Can one use sockets (which as I understand it or network-layer constructs) to send (byte-wise) messages compliant with higher protocols such as TCP or UDP No. Sockets gives you access to transport protocols such as TCP or UDP (actually with Java it's only TCP and UDP in the future probably SCTP as well). You can build stuff that works on top of TCP/UDP not implement TCP/UDP yourself(unless you want to implement an IP stack on top of TCP or UDP - which is what several VPN or tunelling protocols does) HTTP FTP etc? Yes these commonly work on top of TCP you can implement HTTP FTP and other protocols that that runs above TCP/UDP by using sockets in java - that's what sockets are for. nos - thank you for the excellent answer. One question though: if sockets are the basis of Java networking then where do they come from? They must be external to Java perhaps provided by the OS? sure files/sockets are provided by the OS. Are sockets not serial port-based? What's different than a socket and this JavaComm or JAIN? Wouldn't they all at some basic level bind to a port and listen for I/O? ""Serial Port"" sounds like a generic term but in this context it means ""async serial port probably RS-232"". Sockets are TCP/IP or UPD/IP and are most commonly Ethernet based. At least for the LAN. @Pam Sockets are pretty specific they generally refer to a specific style of API(namely BSD Sockets http://en.wikipedia.org/wiki/Berkeley_sockets). That style of interfacing to some form of network communication has been carried through to most operating systems and programming languages. A native C socket API might give you access to a whole range of low level protocols mostly TCP/IP based but also others e.g. bluetooth appletalk X.25 etc but even the native APIs have exceptions e.g. serial programming. In java  sockets are for IP/UDP and IP/TCP only."
412,A,Get status of client computer Hardware Status Hi I have been given the assignment to check that if it is possible to check for hardware status (Mouse keyboard etc etc i.e conflicted devices )of each client System. Clients may be connected via LAN or WIFI. I know this can be done on single computer like my application should be installed on a computer and i can get all the data via WMI but how to get this data remotely Is installing your application on each of the client computers an option? You can use WMI to connect (and get information from) to remote computers but it does involve some configuration. If these are machines under your control then you can do it this way. See here more more details http://msdn.microsoft.com/en-us/library/aa389290(v=vs.85).aspx Thanks Shiv it helped WMI is explicitly meant to be used remotely check out this and see if it helps.
413,A,"Rationale behind ACKs and SEQs? I am not sure if people find this obvious but I have two questions: During the 3-way handshake why is ACK = SEQ + 1 i.e. why am I ACKing for the next byte that I am expecting from the sender? After the handshake my ACK = SEQ + len. Why is this different from the handshake? Why not just ACK for the next byte I am expecting as well (the same as during the handshake)? I know I must've missed out a basic point somewhere. Can someone clarify this? This is because the first byte of sequence number space corresponds to the SYN flag not to a data byte. (The FIN flag at the end also consumes a byte of sequence number space itself.)  Both the SYN and FIN flags cause the sequence number of the stream to increment by one. Thus SYN (seq x) --------------> <--- SYNACK (ack x+1 seq y) ACK (seq x+1 ack y+1) ---> Is your three way handshake. It's done that way because SYNs and FINs require acknowledgement of receipt. That way everyone can be on the same page during the lifetime of the connection. Theoretically any packet in part of the TWHS could have payload but if either of the packets with the SYN flag set have payload the opposite side needs to acknowledge both data AND the flag.  During the handshake you're synchronizing. The sequence number is the known data. Once you've synced the data length is the known data as well as a useful pseudo-random verifier. Sender knows how much he sent and if you reply he assumes you got it. This is easier than reply with say a checksum or hash of the data and is usually sufficient. I see... I think I understood your point about the handshake. But again why not just reply with ACK = SYN just to say ""Alright.. I got your SYN numbered this""? And secondly my second question concerned the same thing... I wasn't really suggesting we add a checksum or a hash... From your reply it seems to be that there is a notion of security involved somewhere but I'm still trying to figure out where... no - it's just a verification. Once sender/receiver have synced they need a way to positively confirm receipt of the message. The AOK is that receiver got as many bytes as sender sent. Something know to both parties. It's not security. It's more like a parity bit though that is an inaccurate analogy. Why impose a messsage counter on the protocol? It adds no value only overhead."
414,A,"'web browser' program works in Windows but not Linux I am not going to post my code it is for a school project and I don't want to risk having my code copied. That being said I don't expect an concrete answer but any thoughts would be appreciated. Anyway my assignment was as follows: I am suppose to simulate a web browser in the terminal. The user provides an IP address file path and port number. I then have to create the get request send it to the server and parse the file's content length from the response. Finally I am to use the content length to count the bytes as I read in the file so I know when the read is complete. All of this data is to be written to an html file. The weird part is I started working on this on my home machine (Linux) and was able to parse the content length and queue to the beginning of the html file and start reading. After after each read I subtract the number of bytes I read from the content length and when I zero out I haven't read the entire file. However if I run this on one of the school machines (Windows using cygwin) it reads the entire file. I've debugged using print statments and confirmed that I am parsing the content length correctly and the countdown is working also. It has me scratching my head. Any ideas? Thanks Check line endings. Chances are that the stdio library that you are using on Windows does translations on any CR-LF combinations in the data. Check your compiler's manual for the open function and make sure that the mode is binary. This is a classical porting problem which requires different code to be compiled using ifdefs. Check if your Windows compiler supports a constant like WIN32 which you can use like this #if defined(WIN32) /* do this */ #else /* do that */ #endif  How do you determine that you haven't read the entire file? Are you relying on EOF? If so are you aware of HTTP/1.1 connection persistence? @Pinsickle: Don't forget to watch out for `read()` returning `-1` indicating an error rather than a number of bytes read. @caf good point thanks No EOF would be the easy way but we are not allowed to use it. We have to parse the file length from the ""Content-Length"" header. It is suppose to tell the file length in bytes. Then you have to queue up to the beginning of the html file. Then you basicly keep track of the number of bytes read. Just to remove extra variables from the problem what is the server you're using for testing? I can't check right now (my computer with my code and wireshark is at home) but I'm 99% sure it is Apache Just making sure that it's not a homebrew server. When you say bytes read do you mean `int bytes_read = read(fd buff 256);` or `read(fd buff 256); int bytes_read = 256;`? yeah the professor has us using a max buffer size of 80. However on the final read I may have to read less than 80 bytes. Say I have a 85 bytes to read. I read 80 bytes leaving me with five. Then on the final read I would read 5. Yes but how do you increment your total_bytes_read variable? Do you do `total_bytes_read += read(fd buff 80);` The reason I ask is that the read function will read what's available and return the byte count unless the fd was opened with special flags. Oh wow that could be my problem. I am just assuming that it reads the correct amount each time. I was just subtracting 80 each time but the final. I think you fixed my problem! You are awesome."
415,A,"Sending structure using recvfrom() and sendto() I am using C language which is a common platform for both the server and the client. I have a structure of a particular type which I want to send to the client from the server. For e.g. SERVER CODE  //necessary declarations struct hostent *hp; hp=gethostbyname(""www.google.com""); sendto(sockDes&hpsizeof(hp)0(struct sockaddr *)&cli_addrsizeof(cli_addr)); CLIENT CODE struct hostent *hp; msg=recvfrom(mysock&hpsizeof(hp)0(struct sockaddr)&serv_addr&size_len); So  basically I want to send a structure from the server to the client.But from the above pieces of code I am getting segmentation faults  and I am not sure whther such structure transfer is feasible IS THERE ANY WAY OUT? It more or less is. I wouldn't call it portable in any way though. First you have to malloc() in order to allocate memory for the struct hostent. You can transfer a struct through sendto()/recvfrom() but since struct hostent contains pointers the members of this struct that are pointers have no meaning as soon as they are transfered to the other end. Good catch! I was looking for segfault causes and not even thinking about what exactly he was trying to transfer. Ok..then it is insignificant to send the structure hostent.It would be better if I send the contents of the structure string by string to the server. That's correct and that's usually the case. You really don't want to be transmitting anything other than characters or integers over a network unless you are very careful about it. And when you transmit integers be sure to convert them to/from network byte order using the htonl/htons/ntohl/ntohs family of functions.  I'd remove the & in front of hp  By the way if I consider the very generic question in your post i.e. ""I want to send a structure from the server to the client"" I would recommend looking at the following post on SO: Passing a structure through Sockets in C  You should correct the code as shown below: struct hostent *p_hp = gethostbyname(""www.google.com""); struct sockaddr_in cli_addr; sendto(sockDesp_hpsizeof(hostent)0(SOCKADDR *)&cli_addrsizeof(cli_addr)); struct hostent hp; struct sockaddr_in serv_addr; int size_serv_addr = sizeof(serv_addr); msg=recvfrom(mysock&hpsizeof(hostent)0(SOCKADDR *)&serv_addr&size_serv_addr); It would always help to read the documentation on socket functions.  In general you want to avoid transferring raw structures over networks unless they have specifically been designed for use as network transport or unless you are absolutely sure that both ends will always be exactly the same architecture. Differences in padding and byte ordering may cause the data to appear garbled at the other end. The particular type of struct you are trying to transfer just will not work being sent raw (See the answer by Petros) The problem in your send code is that hp is a pointer and you are treating it like it is the data it points to. sizeof(hp) returns the size of a pointer not the size of the pointed-to hostent struct. Also you are creating a pointer-to-pointer in the second argument of sendto when you do not mean to. The sending code should read: struct hostent *hp; hp=gethostbyname(""www.google.com""); sendto(sockDeshpsizeof(*hp)0(struct sockaddr *)&cli_addrsizeof(cli_addr)); You also have a problem in the receive code. You are supposed to allocate space for recvfrom to store the data it received. It does not allocate it for you. Change your receive code to one of these two: // Store on the stack struct hostent h; msg=recvfrom(mysock&hsizeof(h)0(struct sockaddr)&serv_addr&size_len); or // Store on the heap be sure to free() when done struct hostent * hp = malloc(sizeof(struct hostent)); msg=recvfrom(mysockhpsizeof(*hp)0(struct sockaddr)&serv_addr&size_len);  If hp is a pointer why are you taking its address?"
416,A,"Callback observers and asynch sockets in Python I'm still a neophyte Python programmer and I'm trying to do something that is a bit over my head. What I've done is create a simple IRC bot using asyncore (and asynchronous sockets module). The client runs in a continuous loop listening to the conversation in the channel. What I would like to do (I think?) is implement an observer pattern so I can respond to events. I imagine it would look somthing like this: class MyBot(object): def __init__(self): bot = MyIRCClient(server='whatever' channel='#whatever') bot.observe(event='join' handler='log_join') bot.connect() # Bot is now listening continously in a loop def log_join(self e): print e + ' joined the channel.' I'm basing this design around what I know of observers used in the various Javascript frameworks. I don't know if the same technique can or should be applied here. Any suggestions? While Observer is not a particularly popular DP (design pattern) in Python it's not a totally ""alien"" one either so if you're familiar with it go right ahead. However the normal way to call observe would be with handler=self.log_join a callback that's actually a callable not with a string value forcing the bot to perform introspection to find out what it actually has to call when the event occurs (and not even giving it a self to refer to the object it's supposed to perform introspection on -- shudder!). Callback is a perfectly reasonable and popular DP in Python but that's because passing around first-class callables (functions bound methods classes instances of classes with a __call__ method etc etc) is so wonderfully easy (pretty trivial actually;-)."
417,A,What component do I need to monitor my internet traffic on my PC? I would like to be able to see and monitor my internet data (http/emule/email) on my own PC using Windows XP. I am thinking of something like WireShark but I would like to control it programmatically. I would be using C or C++. How can I do this? Have a look at the code of the tool trafficWatcher. It uses WinPCap to distinguish between LAN and internet traffic. The sourcecode is available here.  winpcap is probably the most well known choice but you could also write a Layered Service Provider. There's not a whole lot of documentation but a good place to start is the article on msdn: http://www.microsoft.com/msj/0599/LayeredService/LayeredService.aspx This has some advantages vs layer 2 packet parsing but also some disadvantages. You'll need to evaluate where in the stack you want to live. edit: Obviously when I say pcap -- I really mean any similar approach. Obviously winpcap was not the first driver/library combo to provide this sort of information.  WireShark uses winpcap to do it's thing. Winpcap comes with a C interface. is it the only method ?
418,A,question related to sockets in network programming with C There is a system call known as socket(); it creates a socket on the listening server. What I want to understand is a server creates an IP+port combination. Let us say telnet uses port 23. Now when the client machines do connections then the port on which the server is listening then the connection there is not on port 23 in fact it is on a different port.My confusion is on the server side also does the same thing happen. For example I write a server to listen to port 23 then the connections which will be done on server side with different clients how are they differentiated because all of them will be on same port.So how can you make so many connections on the same server port.If some one uses telnet (23) or ftp (21) or ssh (22) then many people can still login to the same service port on the server i.e. more than one connection for ssh by different users where as ssh is listening only at port 22.So what exactly does the socket do or how is a socket created? UPDATE I got what is explained.Depending upon the IP+port combination from the client machine where the connection originated rest of the things by the server side can be handled and I think this information probably is used by socket file descriptors. What I see is in connect() system call which we use as follows connect(sockfd(struct sockaddr *)&client_addresssize_t); we do pass on struct sockaddr * at the client end which has unique IP+port combination I think when server gets this in accept then things proceed. What I want to know further is the argument at server side accept(server_sockfd(struct sockaddr *)&client_address(size_t *)sizeof (struct sockaddr )); Does it get that same client_address which from client side was passed on using connect() system call? If yes then the socket_descriptors for the same server listening to many clients are different.What I want to know is how does the data structure at the server side is maintained when it accepts a request from the client. When you have a listening socket the specified port works on the accept phase. Once the acception is done the communication works on a different port estabilished by the accept. Have a look here. That is wrong the new socket created by `accept` keeps using the same port as the listening socket.  TCP connections are identified by 4 parameters: local IP local port remote IP remote port So even if two connections share the same local IP and port the OS can differentiate between then because the remote IP and/or port are going to be different.  I will give you pseudo code from Qt to explain the matter. All TCP servers work in same way. You first create a listening TCP server socket. When an incoming connection request arrives the operating system creates a new socket (your OS uses a different port than listening socket) and associates this new socket with the remote client for you. The TCP server socket continues to accept new connections and you resume your communication with the remote peer via the newly created socket. tcpServer.listen(LocalHost PORT); connect(&tcpServer SIGNAL(newConnection())this SLOT(NewConnection())); //Callback that is called when server accepts a new connection void NewConnection() { //Here you will have a new socket for communication with the client //The tcpServer will continue listening to given port QTcpSocket* connection = tcpServer.NextPendingConnection(); //Use your new connection to communicate with the remote client... } I hope this helps I did not down vote your question some one else hadI had plus +1 voted. More over I am not a Qt guy so no idea. If something is wrong with what I have written please explain ! You should at least show the courtesy of entering a comment!  The unique combination that identifies the connection is: Source address and port Destination address and port In your example the destination address and port are the same for many connections but each comes from a unique combination of source address and port. Here is a brief tcpdump session of me connecting from my desktop to my server via FTP (port 21): 22:55:50.160704 IP 172.17.42.19.64619 > 172.17.42.1.21: S 2284409007:2284409007(0) win 8192 <mss 1460nopnopsackOK> 22:55:50.160735 IP 172.17.42.1.21 > 172.17.42.19.64619: S 1222495721:1222495721(0) ack 2284409008 win 65535 <mss 1460sackOKeol> 22:55:50.160827 IP 172.17.42.19.64619 > 172.17.42.1.21: . ack 1 win 8192 22:55:50.162991 IP 172.17.42.1.21 > 172.17.42.19.64619: P 1:61(60) ack 1 win 65535 22:55:50.369860 IP 172.17.42.19.64619 > 172.17.42.1.21: . ack 61 win 8132 22:55:56.288779 IP 172.17.42.19.64620 > 172.17.42.1.21: S 3841819536:3841819536(0) win 8192 <mss 1460nopnopsackOK> 22:55:56.288811 IP 172.17.42.1.21 > 172.17.42.19.64620: S 454286057:454286057(0) ack 3841819537 win 65535 <mss 1460sackOKeol> 22:55:56.288923 IP 172.17.42.19.64620 > 172.17.42.1.21: . ack 1 win 8192 22:55:56.290224 IP 172.17.42.1.21 > 172.17.42.19.64620: P 1:61(60) ack 1 win 65535 22:55:56.488239 IP 172.17.42.19.64620 > 172.17.42.1.21: . ack 61 win 8132 22:56:03.301421 IP 172.17.42.19.64619 > 172.17.42.1.21: P 1:12(11) ack 61 win 8132 22:56:03.306994 IP 172.17.42.1.21 > 172.17.42.19.64619: P 61:94(33) ack 12 win 65535 22:56:03.510663 IP 172.17.42.19.64619 > 172.17.42.1.21: . ack 94 win 8099 22:56:06.525348 IP 172.17.42.19.64620 > 172.17.42.1.21: P 1:12(11) ack 61 win 8132 22:56:06.526332 IP 172.17.42.1.21 > 172.17.42.19.64620: P 61:94(33) ack 12 win 65535 22:56:06.726857 IP 172.17.42.19.64620 > 172.17.42.1.21: . ack 94 win 8099 You can see the initial connection is 172.17.42.19.64619 <-> 172.17.42.1.21. Port 64619 is what the Windows 7 box happened to select as the source port when it made the outgoing connection. The two lines with S are the SYN packets going back and forth to establish the connection. Then I start the next connection and Windows just uses the next available port 64620. The connection 172.17.42.19.64620 <-> 172.17.42.1.21 forms a new unique tuple of the items I listed at the top. Only the client's port is different but that's enough. Each packet that arrives at the server to port 21 can be distinguished by the source port. Each packet from port 21 on the server arriving at the client can be distinguished by the destination port. so how does it differentiate the connections? @Registered: I added an example. If that's not enough try to explain what you understand so we know what you're thinking. thanks a lot for your answer I digged further into things and have update the question.
419,A,"How to get netmask? I know how to get from ifconfig. (linux) But is there another way? Can found it in socket. What is your target OS? linux is my target os You need to use IO#ioctl. This is totally non-portable. On my linux box this code words: require 'socket' sock = Socket.new(Socket::AF_INET Socket::SOCK_DGRAM0) buf = [""eth0""""""].pack('a16h16') sock.ioctl(0x891b buf) netmask = ""#{buf[20]}.#{buf[21]}.#{buf[22]}.#{buf[23]}"" #=> ""255.255.255.240"" Ioctl differs considerably between systems and I had to look through a few system header files to get the right sizes for the [].pack the location of the address in buf and the numeric value for SIOCGIFBRDADDR (the first argument to ioctl). If these values don't work for you I can give you more information on how to find them. Edited to rename the variable - it really is the netmask not the broadcast ;)"
420,A,"Ignoring your own UDP broadcasts in Java In my program I'm sending out UDP broadcasts and reacting to them. I need a way to ignore the UDP broadcasts that I send out but react to ones that aren't from my machine. I did try using: if (NetworkInterface.getByInetAddress(packet.getAddress()) != null) but this generated IOExceptions in some cases (java.net.SocketException: no network interface is bound to such an IP address) Anyone any ideas? Also: getInetAddress() on my socket throws a NullPointerException The ""industrial strength"" solution to this of course would be to generate yourself a random UUID for each server and to include this ID (it could be an int or maybe a long) in each packet. If that ID matches your own you can drop it. I'm not enamored of this solution because it wastes a number of bytes in each datagram packet. But it's simple and effective.  This is not exactly an answer to your question. But for handling UDP broadcasts you should have a look at JGroups: JGroups is a toolkit for reliable multicast communication. (Note that this doesn't necessarily mean IP Multicast JGroups can also use transports such as TCP). It can be used to create groups of processes whose members can send messages to each other. The main features include Group creation and deletion. Group members can be spread across LANs or WANs Joining and leaving of groups Membership detection and notification about joined/left/crashed members Detection and removal of crashed members Sending and receiving of member-to-group messages (point-to-multipoint) Sending and receiving of member-to-member messages (point-to-point)  I think there's a little discrepancy between the javadoc and the actual implementation of NetworkInterface getByInetAddress(). The javadoc seems to suggest that getByInetAddress would return null if no match was found yet the implementation either returns a match either throws a SocketException. JavaDoc public static NetworkInterface getByInetAddress(InetAddress addr) throws SocketException Returns: A NetworkInterface or null if there is no network interface with the specified IP address. Implementation  public static NetworkInterface getByInetAddress (InetAddress addr) throws SocketException { if (networkInterfaces == null) networkInterfaces = getRealNetworkInterfaces (); for (Enumeration interfaces = networkInterfaces.elements (); interfaces.hasMoreElements (); ) { NetworkInterface tmp = (NetworkInterface) interfaces.nextElement (); for (Enumeration addresses = tmp.inetAddresses.elements (); addresses.hasMoreElements (); ) { if (addr.equals ((InetAddress) addresses.nextElement ())) return tmp; } } throw new SocketException ( ""no network interface is bound to such an IP address""); } I suggest to either catch the exception and treat it as an answer from a 3rd party either re-implement it using the getNetworkInterfaces() method. Ah that makes more sense thanks. I'm only really interested in when the broadcast isn't from my machine so I'm not sure catching it is my best option. Would it be silly to just reimplement getByInetAddress as a method in my class and have it not throw an exception? I'm all for improving readability so reimplementing might be the better option.  Can't you just do an equals() between packet.getAddress() an an Address object for your own host? The `DatagramSocket` is bound to 0.0.0.0 but the packet originates from a LAN IP (192.168.1.103 in this case) - that's why I was trying to see if the packet's source address was in `NetworkInterface.getByInetAddres()` Bummer! I've also had a hard time getting a meaningful address for my own host. How about sending yourself a test message at initialization time? Sounds kludgy but I'm almost out of ideas."
421,A,Is it possible to implement ping on windows phone 7? To get the impression of networking capabilities in WP7 I was going to build a simple ping app that would display the result of ICMP ping request to a certain host. However not only the System.Net.NetworkInformation.Ping class is missing System.Net.Sockets namespace is missing as well. After a short research I found out that there are only two ways of communication in WP7: WebClient class that works with http(s) requests and WCF-client that works with SOA services. Does that mean that I can't ping hosts directly from the phone? The only possible solution I see is implementing a separate WCF service on a dedicated server that will do the pinging for the phone which looks like an overkill and has certain flaws. You won't be able to do this until sockets come to the WP7 platform as there isn't going to be any other way to send an ICMP packet. The phones may not respond to pings back from a server either - we'll have to see on that. If they don't I doubt there would be anyway to change this on unhacked devices. Pinging to a target host from a server on behalf of a phone isn't going to be too helpful either since unfortunately it won't reflect the latency between phone and target host in any way. Thank you! That's helpful. Pinging to a target host from a server will however answer a question whether a target host is down. But this is unfortunately the only question answered by this kind of ping. That's true for hosts accepting pings.
422,A,Write program like netstat I would like to write an application like a netstat - to show all the active connections and the open ports respectively. The problem is that I don't know how to do it - I've done some network programming but in general it was some simple server-client packet and simple TCP and UDP packets sending/receiving. I looked at the netstat code but it looks really complex. I am also reading the Unix Network Programming book and waiting for my copies of TCP/IP Illustrated (the 1 and 2 vol). Any advice guys ? Anyone's got experience with stuff like this ? If does can someone help me a bit in here ? The important things to do etc. etc. ? I'll appreciate every helpful answer. Thank you in advance. Welcome to networking! IMHO it's one of the most enjoyable areas of system programming. You can do this but realize that implementation will be operating system specific. This is because every OS exposes its counters and statistics differently. For example on Linux you can get most of the statistics by parsing the files in /proc/net. Dumping the contents of /proc/net/dev gives you the list of interfaces (along with some stats). # cat /proc/net/dev Inter-| Receive | Transmit face |bytes packets errs drop fifo frame compressed multicast|bytes packets errs drop fifo colls carrier compressed lo:1417676206 2810305 0 0 0 0 0 0 1417676206 2810305 0 0 0 0 0 0 eth0:3780840146 96049486 0 0 0 0 0 0 2202685287 17436558 0 0 0 0 0 0 Or parsing /proc/net/netstat will give you protocol (TCP/IP) statistics: # cat /proc/net/netstat TcpExt: SyncookiesSent SyncookiesRecv SyncookiesFailed EmbryonicRsts PruneCalled RcvPruned OfoPruned OutOfWindowIcmps LockDroppedIcmps ArpFilter TW TWRecycled TWKilled PAWSPassive PAWSActive PAWSEstab DelayedACKs DelayedACKLocked DelayedACKLost ListenOverflows ListenDrops TCPPrequeued TCPDirectCopyFromBacklog TCPDirectCopyFromPrequeue TCPPrequeueDropped TCPHPHits TCPHPHitsToUser TCPPureAcks TCPHPAcks TCPRenoRecovery TCPSackRecovery TCPSACKReneging TCPFACKReorder TCPSACKReorder TCPRenoReorder TCPTSReorder TCPFullUndo TCPPartialUndo TCPDSACKUndo TCPLossUndo TCPLoss TCPLostRetransmit TCPRenoFailures TCPSackFailures TCPLossFailures TCPFastRetrans TCPForwardRetrans TCPSlowStartRetrans TCPTimeouts TCPRenoRecoveryFail TCPSackRecoveryFail TCPSchedulerFailed TCPRcvCollapsed TCPDSACKOldSent TCPDSACKOfoSent TCPDSACKRecv TCPDSACKOfoRecv TCPAbortOnSyn TCPAbortOnData TCPAbortOnClose TCPAbortOnMemory TCPAbortOnTimeout TCPAbortOnLinger TCPAbortFailed TCPMemoryPressures TCPSACKDiscard TCPDSACKIgnoredOld TCPDSACKIgnoredNoUndo TCPSpuriousRTOs TCPMD5NotFound TCPMD5Unexpected TcpExt: 0 0 7053 2480 0 0 0 0 0 0 136514 0 0 0 0 4732 1291978 48 10938 0 0 726917 0 14734 0 3505285 23 3285967 4288783 963 56625 82 294 218 85 153 80 817 739 4861 176075 7864 246 3859 1647 285964 13690 54668 25710 599 4775 0 0 10792 21 12994 1492 0 305 27 0 946 0 0 0 1178 5175 3188 353 0 0 IpExt: InNoRoutes InTruncatedPkts InMcastPkts OutMcastPkts InBcastPkts OutBcastPkts IpExt: 0 0 372 14 269538 0 Or /proc/net/udp gives you UDP connection information: # cat /proc/net/udp sl local_address rem_address st tx_queue rx_queue tr tm->when retrnsmt uid timeout inode ref pointer drops 20: 0100007F:1194 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 6788392 2 f6701b80 0 20: 4CE85CD0:1194 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 6788384 2 f6703200 0 20: 4DE85CD0:1194 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 6788379 2 f6701e00 0 39: 00000000:0AA7 00000000:0000 07 00000000:00000000 00:00000000 00000000 108 0 6790562 2 d2890f00 0 40: 00000000:11A8 00000000:0000 07 00000000:00000000 00:00000000 00000000 108 0 6790539 2 d2892580 0 68: 00000000:13C4 00000000:0000 07 00000000:00000000 00:00000000 00000000 108 0 6790505 2 d2892d00 0 87: 00000000:87D7 00000000:0000 07 00000000:00000000 00:00000000 00000000 106 0 6957 2 f67b0f00 0 89: 00000000:11D9 00000000:0000 07 00000000:00000000 00:00000000 00000000 108 0 6790508 2 d2890500 0 105: 00000000:14E9 00000000:0000 07 00000000:00000000 00:00000000 00000000 106 0 6956 2 f67b2580 0 116: 0100007F:01F4 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 6788388 2 f6701900 0 116: 4CE85CD0:01F4 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 6788382 2 f6701180 0 116: 4DE85CD0:01F4 00000000:0000 07 00000000:00000000 00:00000000 00000000 0 0 6788378 2 f6702f80 0 120: 0100007F:88F8 0100007F:88F8 01 00000000:00000000 00:00000000 00000000 107 0 6790576 2 f67b0a00 0 Hope this helps.
423,A,"multiple outstanding requests I don't really understand the expression ""multiple outstanding requests"". could you please give me an example? what is an outstanding request? what is the oposite of an ""outstanding request""? An outstanding request is one which has not been served yet. For instance an application could make 30 concurrent requests to different web servers. 10 of them may come back with a response while the other 20 have not been serviced. Therefore those 20 are outstanding since they are waiting for a response. The opposite would be one that is served or completed I guess. You can use an analogy of a restaurant. If you ask a waiter for water and a spoon and he only returns you a water then your request for a spoon is outstanding. It isn't complete until he gives you your spoon or tells you that he doesn't have one to give you. Which brings us to the next question why would you ask a waiter for water and a spoon? And as no vessel was specified how is the water served?"
424,A,"Check connection open or closed ?(in C in Linux) In socket programming in Linux I need to write data in socket but I don't know socket is open or close . how to I know that socket is open and close without read ? printf(""befor read%d\n"" n); bzero(buffer MAX_SIZE_BUFFER); n = read(sockfd buffer MAX_SIZE_BUFFER - 1); printf(""after read%d\n"" n); if (n <= 0) { break; } printf(""befor write%d s: %d \n"" n  sockfd); n = write(newsockfd buffer n); if (n <= 0) { break; } I read from sockfd and I sure this connection is open . When to write buffer in newsockfd I don't know newsockfd is open or close how to check newsockfd is closed ? I know problem . in middle of writing connection closed . for example write 1024 data in 500 connection closed and program closed. how to avoid this ? I use send() instead write() that handle no signal : bzero(buffer MAX_SIZE_BUFFER); n = read(sockfd buffer MAX_SIZE_BUFFER - 1); printf(""after read%d\n"" n); if (n <= 0) { break; } n2 = send(newsockfd buffer n MSG_NOSIGNAL); if (n2 == -1) { close(sockfd); close(newsockfd); return; } if (n2 != n) { break; } This will work and the other alternative is to use `sigaction` to ignore `SIGPIPE` (set its handler to `SIG_IGN`).  Socket programming can be rather tricky because you often don't know an error has occurred until much later. For instance if the machine you are writing to shuts down abnormally the write call may succeed ( because you were able to write to your internal OS buffers ) only to fail during the close call. Unless you have an application-layer way of verifying the socket is active ( i.e. sending a message and requiring a response within some time period ) you have no way of knowing. If you are using a standard protocol something may already exist to handle errors. So the short answer is that you need to check for error returns from pretty much every call that touches the socket ( read write close etc... ). Unfortunately this is the right answer. I tried a few ways to find out if keep-alive HTTP connections are still valid but it's not reliable and without race conditions.  The way to check if you can write to a socket is surprisingly to try and write to it :-) If the socket has been closed you will get a -1 return code from write and you can examine errno to see what the problem was. If the socket is still valid but you just can't write any data at the moment write will return 0. The read call also behaves in a similar fashion returning -1 if there's a problem. Basically for write: if you get back a -1 there's been a problem and you should check errno to see if it's recoverable or fatal. If you get back a 0 then you can't write anything at the moment (may be a network backlog or some other problem but definitely not (yet) fatal). If you get a value less that what you wanted then some of the data has been sent. Adjust your pointers so you can try send the rest in the next cycle. Do not assume a positive return value means the entire block has been sent. If you get back the same number as the number of bytes you tried to send the entire buffer has been accepted for delivery. If you get back more than what you asked to be sent send an email off to the kernel developers with some acerbic comment. Linus et al will love that :-) Update: As caf has pointed out in the comments I forgot to take into account the signal handling. You have to ignore the broken pipe signal or write will fail internally by raising that signal. You can do this by inserting: struct sigaction new_actn old_actn; new_actn.sa_handler = SIG_IGN; sigemptyset (&new_actn.sa_mask); new_actn.sa_flags = 0; sigaction (SIGPIPE &new_actn &old_actn); before starting to use the socket functions. You can then use: sigaction (SIGPIPE &old_actn NULL); to restore the previous signal handling. pax you are forgetting that by default trying to `write` to socket that has been closed by the other end will result in your process recieving a `SIGPIPE` signal the default action of which is to terminate the process. My mistake. Fixed now (hopefully). in the next line see if(n<=0) but in write function some error occured and close program @SjB do *not* assume problems if `write` returns 0. This can happen if there's a temporary network backlog. If the backlog turns into a real problem the session will be shutdown and you will eventually get -1. Until then just keep trying. I know problem . in middle of writing connection closed . for example write 1024 data in 500 connection closed and program closed"
425,A,How to determine IP used by client connecting to INADDR_ANY listener socket in C I have a network server application written in C the listener is bound using INADDR_ANY so it can accept connections via any of the IP addresses of the host on which it is installed. I need to determine which of the server's IP addresses the client used when establishing its connection - actually I just need to know whether they connected via the loopback address 127.0.0.1 or not. Partial code sample as follows (I can post the whole thing if it helps): static struct sockaddr_in serverAddress; serverAddress.sin_family = AF_INET; serverAddress.sin_addr.s_addr = INADDR_ANY; serverAddress.sin_port = htons(port); bind(listener (struct sockaddr *) &serverAddress sizeof(serverAddress)); listen(listener CONNECTION_BACKLOG); SOCKET socketfd; static struct sockaddr_in clientAddress; ... socketfd = accept(listener (struct sockaddr *) &clientAddress &length); The solution to my specific problem (thanks to zildjohn01) in case anyone needs it is shown below: int isLocalConnection(int socket){ struct sockaddr_in sa; int sa_len = sizeof(sa); if (getsockname(socket &sa &sa_len) == -1) { return 0; } // Local access means any IP in the 127.x.x.x range return (sa.sin_addr.s_addr & 0xff) == 127; } From your code socketfd = accept(listener (struct sockaddr *) &clientAddress &length); Analyse the returned clientAddress. This is what you need. The OP is looking for the locally-bound address not the remote address. @zildjohn01: When using loopback both client and remote will be loopback addresses.  You can use the getsockname function. The getsockname() function retrieves the locally-bound name of the specified socket Call this on the socket returned by `accept()` of course.
426,A,Way to get client download speed I have to tell to the user in a web application(using struts) the download speed. What is the best way? For the moment I simply have a .jsp filled with java scripts and I calculate the time it takes to the user to load this .jsp(I make this action 3 times and take the average speed). It worked ok when I had slow connection speed(under 1mb) but for bigger speed connection the values where wrong(to small; and if I put somebody from another country to test it-the connection speed results are even smaller). The jsp size (taken with YSlow add-on from Mozilla) was little:70kb (gzipped) so I raised it to 260kb and still doesn't show the right value. I have to mention that I cannot download a file to the user and that is because I have to do a little test of compatibility of the application with the user networkbrowser..etc and the application means a lot of java script on the client side and the average size of a jsp file is 70kb(gzipped)<-that's why I tried at start with this size. Could you please give me an advice I'll appreciate Luisa http://speedof.me/api ? You think the best way to do it is use client side (javascript for example). If you want more specific data you need to do some downloads in the loop (100k 1M 5M). http://ditio.net/2008/08/06/detect-connection-speed-with-javascript/ Downloads in a loop..how to do that?I download 100k then 1M..when I stop? Look at source code of this page http://www.pcpitstop.com/internet/BwDownTest.asp
427,A,Audio streaming from iPhone to a server? How do I stream live audio recording from iPhone to a server? Do I really have to use RTP? Any other suggestions? Thanks. While I look at the following projects: http://www.iphone-recorder.com/about.html http://blog.pjsip.org/2009/02/19/native-iphone-sip-client-based-on-pjsip-available-on-app-store-open-source-and-not-tied-to-any-provider/ http://code.google.com/p/siphon/ It seems to be possible that audio can be streamed. Am I missing something?
428,A,"Which network interface on local host is connected to host B Let's say my computer A is connected to multiple networks. On one of these networks is another host B. Is it possible through C# from host A to determine how host B would reach host A? The solution must handle both IPv4 and IPv6. It doesn't need to handle NAT. One could say I want to figure out which one of host A's network interfaces is connected to host B. your catch is How to detemine which network interface (ip address) will be used to send a packet to a specific ip address? you can find pinvoke for GetBestAddress . i am unable to find some pinvoke example for GetBestAddressEx that supports IPv6  It is never possible to tell how another host will reach you because it is up to that host's route table. However usually routes are symmetric which means a remote host will reach you the same way you will reach that remote host. You can examine the route table. On Windows that command would be ""route -n"". This is probably exposed via some API too. If host A and B are already connected that connection will show up in netstat. If you control the code that has the connected socket then you just need to get the peer. Sorry I don't have the C# code to do that.  You might trying looking at http://msdn.microsoft.com/en-us/library/system.net.networkinformation.tcpconnectioninformation.aspx as well."
429,A,"which IP Helper API interface to use? •Managing Network Adapters Using GetAdaptersInfo •Managing Interfaces Using GetInterfaceInfo •Managing IP Addresses Using GetIpAddrTable These are the three methods listed on the msdn IP Helper API. Why would I prefer one method over another? Im trying to A. get as much information about the state of the networking adapters (to diagnose a problem) and then B. be able to fix the problem (via changing a DNS server gateway static ip dhcp server ...) What are you trying to do? They are mainly resulted from terminology differences see http://msdn.microsoft.com/en-us/library/aa365798%28v=vs.85%29.aspx for the explanation regarding ""adapter"" and ""interface"". In short an adapter is a data-link level abstraction while an interface being IP-level abstraction. It will depend on the situation that you want to solve to determine what API to use. For example link speed or tunnel type can only be acquired from adapters."
430,A,"How to make connect fail when the server is not ready? I am writing a library for local IPC which should act both as server and client. The strategy is to fail and try again if the server is not ready. For this test I have create two process which are communicating both as server and client. But I have have a problem with my current implementation because the client code does not fail when the server are busy. The problem is best illustrated with a snippet from strace: execve(""./RemoteSubjects"" [""./RemoteSubjects""] [/* 72 vars */]) = 0 // Client A starts 28069 .000105 socket(PF_FILE SOCK_STREAM 0) = 3 28069 .000031 fcntl(3 F_GETFL) = 0x2 (flags O_RDWR) 28069 .000006 fcntl(3 F_SETFL O_RDWR|O_NONBLOCK) = 0 28069 .000047 unlink(""/tmp/A.ipc"") = 0 28069 .000007 bind(3 {sa_family=AF_FILE path=""/tmp/A.ipc""} 110) = 0 28069 .000007 listen(3 0 ) = 0 // Client B starts 28070 .000031 socket(PF_FILE SOCK_STREAM 0) = 3 28070 .000012 fcntl(3 F_GETFL) = 0x2 (flags O_RDWR) 28070 .000005 fcntl(3 F_SETFL O_RDWR|O_NONBLOCK) = 0 28070 .000030 unlink(""/tmp/B.ipc"") = 0 28070 .000034 bind(3 {sa_family=AF_FILE path=""/tmp/B.ipc""} 110) = 0 28070 .000027 listen(3 0) = 0 // Client A creates a socket which it intend to use for communication with B 28069 .000023 socket(PF_FILE SOCK_STREAM 0) = 4 28069 .000016 fcntl(4 F_GETFL) = 0x2 (flags O_RDWR) 28069 .000024 fcntl(4 F_SETFL O_RDWR|O_NONBLOCK) = 0 // Client B creates a socket which it intend to use for communication with A 28070 .000038 socket(PF_FILE SOCK_STREAM 0) = 4 28070 .000016 fcntl(4 F_GETFL <unfinished ...> // Client A connects to B 28069 .000004 connect(4 {sa_family=AF_FILE path=""/tmp/B.ipc""} 110 <unfinished ...> // Client B creates a socket which it intend to use for communication with A 28070 .000011 <... fcntl resumed> ) = 0x2 (flags O_RDWR) 28070 .000007 fcntl(4 F_SETFL O_RDWR|O_NONBLOCK) = 0 // Client B connects to A 28070 .000015 connect(4 {sa_family=AF_FILE path=""/tmp/A.ipc""} 110 <unfinished ...> // Client A's connect returned 28069 .000010 <... connect resumed> ) = 0 // Here is what I do not understand... Why does connect returns 0 when the backlog // is set to 0 and B has not accepted? What I would like to happen is that the connect should fail until I catch the server in its event loop waiting for poll to report activity on file descriptor 3. I should properly set a timeout (SO_RCVTIMEO/SO_SNDTIMEO i think) allowing connect to block for 10ms to increase changes for the server to accept the connect. But how should I fix this and make sure connect fails if accept is not called? By calling listen() you indicate your willingness to allow incoming connections and an accept queue is created. The kernel takes care of connection establishment behind the scenes putting new connections on the accept queue. The accept() function just takes the next fully completed connection off the accept queue and does not cause anything to be sent. As for the backlog argument POSIX states that this is a hint: The backlog argument provides a hint to the implementation which the implementation shall use to limit the number of outstanding connections in the socket's listen queue. It also says: A backlog argument of 0 may allow the socket to accept connections in which case the length of the listen queue may be set to an implementation-defined minimum value. If you want the connect() to fail do not call listen() until you are ready for it to succeed. And after calling accept() close the listening socket if you don't want any more connections to succeed (any additional connections sitting in the accept queue will be aborted). Alternatively have the application send some data over the socket once it accepts the connection (if it doesn't already do so); if the other side does not receive anything within a certain timeout period it could close the connection."
431,A,Are there applications where the number of network ports is not enough? In TCP/IP the port number is specified by a 16-bit field yielding a total of 65536 port numbers. However the lower range (don't really know how far it goes) is reserved for the system and cannot be utilized by the application. Assuming that 60000 port numbers are available it should be more than plenty for most nework application. However MMORPG games often have tens of thousands of concurrently connected users at a time. This got me wondering: Are there situations where a network application can run out of ports? How can this limitation be worked around? The lower range goes to 1023 on many UNIX-like systems at least. You don't need one port per connection. A connection is uniquely identified by a tuple of (host address host port remote address remote port). It's likely your host IP address is the same for each connection but you can still service 100000 clients on a single machine with just one port. (In theory: you'll run into problems unrelated to ports before that.) Ah kind of suspected this but just wanted to make sure. Thanks!  The canonical starter resource for this problem is Dan Kegels C10K page from 1999. The lower range you refer to is probably the range below 1024 on most Unix like systems. This range is reserved for privileged applications. An application running as a normal user can not start listening to ports below 1024. An upper range is often used by the OS for return ports and NAT when creating connections. In short because of how TCP works ports can run out if a lot of connections are made and then closed. The limitation can be mitigated to some extent by using long-lived connections one for each client. In HTTP this means using HTTP 1.1 and keep-alive.
432,A,"Does epoll() do its job in O(1)? Wikipedia says unlike the older system calls which operate at O(n) epoll operates in O(1) [2]). http://en.wikipedia.org/wiki/Epoll However the source code at fs/eventpoll.c on Linux-2.6.38 seems it is implemented with an RB tree for searching which has O(logN) /* * Search the file inside the eventpoll tree. The RB tree operations * are protected by the ""mtx"" mutex and ep_find() must be called with * ""mtx"" held. */ static struct epitem *ep_find(struct eventpoll *ep struct file *file int fd) { In fact I couldn't see any man page saying the complexity of epoll() is O(1). Why is it known as O(1)? @sarnold: I made no assumption that `epoll` is scanning anything. What I meant was that if you're waiting on events that can happen at anytime then `O(1)` operation is not guaranteed by definition. Either that or everyone talking about the `epoll` API meant something else by `O(1)`. @Insilico hehe good point. `O(asleep)`. :) I would vote this up because you really did your homework but I'm maxed out on daily votes :P what homework?? @ddoman: What Keoki Zee meant was that you did prior research on your own before asking this question. A lot of new (and old!) users of Stack Overflow can't even be bothered to do that. Research for doing research :) aha sorry and thanks :) I am pretty much ESL. Well google seems to [agree](http://www.google.com/search?q=%22epoll+operates+in+O%281%29%22&ie=utf-8&oe=utf-8&aq=t&rls=org.mozilla:en-US:official&client=firefox-a#sclient=psy&hl=en&client=firefox-a&hs=RCb&rls=org.mozilla:en-US%3Aofficial&source=hp&q=%22epoll+operates+in+O%281%29%22&aq=f&aqi=&aql=&oq=&pbx=1&bav=on.2or.r_gc.r_pw.&fp=27b93cf21cbb40b9&biw=1339&bih=795) There's no way `epoll` executes in `O(1)` as the number of file descriptors it needs to watch isn't constant. There has to be some kind of repetition construct. Would someone explain this `epoll()` function people keep talking about? I can't find any function with that name. `epoll` is a whole family of functions not all of which have the same complexity. @Insilico you're making the assumption that `epoll()` must _scan_ the file descriptors to find ready ones. `epoll()` can also _wait_ on the descriptors to become ready... It is somewhat inaccurate to say that `epoll` operates at O(1). `epoll` operates at O(log(n)) in respect to _adding and removing descriptors_ at O(n) in respect of _ready descriptors_ and at O(1) in respect of _monitored descriptors_ -- which is the whole point of `epoll`. Adding/removing descriptors is a rare thing waiting for readiness is frequent and you usually have _many more_ descriptors in your set than are actually ready. Insofar while `epoll` does not really operate at O(1) it does so where it matters. The common case is fast the uncommon case is not. This makes sens once you look for ep_find. I only spent a few minutes with it and I see ep_find is only called by epoll_ctl. So indeed when you add the descriptors (EPOLL_CTL_ADD) that costly operation is performed. BUT when doing the real work (epoll_wait) it isn't. You only add the descriptors in the beginning. In conclusion it's not enough to ask the complexity of epoll since there is no epoll system call. You want the individual complexities of epoll_ctl epoll_wait etc. Other stuff There are other reasons to avoid select and use epoll. When using select you don't know how many descriptors need attention. So you must keep track of the biggest and loop to it. rc = select(...); /* check rc */ for (s = 0; s <= maxfd; s++) { if (FD_ISSET(s)) { /* ... */ } } Now with epoll it's a lot cleaner: nfds = epoll_wait(epollfd events MAX_EVENTS -1); /* check nfds */ for (n = 0; n < nfds; ++n) { /* events[n].data.fd needs attention */ } select returns the number of ready fd's too in the worst case you need to loop through them all though (if `maxfd` has an event). @nos And how would you use that number to replace the loop ? Somewhere you need to have a list of all your file descripors. I often use a linked list if I'm using select. `int handled = 0; while(client) { if(FD_ISSET(client->fdread_set) {handle(client); handled++; } if(handled == rc) break; client = clients->next;}`  no need to check from `0 to rc` only check the FDs you actually are monitoring and no need to check more fds when you handled all the events select reported. Yes you're right you can *shorten* the loop. But as you correctly pointed in your comment you still have to process all your list (in the unfortunate case a descriptor near ""the end"" needs attention). And if that list is large and the descriptors are the first and the last.."
433,A,WPF startup application execute after internet connection connected? I'm developing a windows application using WPF.The program is running in the startup and should wait untill the internet connection is connected to be executed. Normally internet connection will get some time to connect. Therefore now im running a thread to ping(like this) with the server in space of 3 seconds to watch the connection status.  public bool CheckConnection() { try { //////////////check connction System.Net.Sockets.TcpClient clnt = new System.Net.Sockets.TcpClient(UserConfig.configlist[2] Convert.ToInt32(UserConfig.configlist[3])); clnt.Close(); return true; } catch (Exception) { return false; } } If the status is true program will be executed. Is there any efficient way of doing this. Any ideas please?????? You might find this question useful http://stackoverflow.com/questions/843810/c-fastest-way-to-test-internet-connection Thank a lot.... There is a very useful NetworkManager class over on CP that will allow you to check for network connection status using the NetConnectionStatus enum... You could start a timer to check for network connection every couple of seconds with if (NetworkManager.NetConnectionStatus != NetConnectionStatus.Connected){} and wait to execute your network dependent code until the network status changes to Connected. Thanks i'm a new comer.I didn't know that tick was requited.thanks again for your answer. No problem welcome it is not required but that's how the system works... if someone gives you a good answer you give them thumbs up...
434,A,"Why do some switches have uplink ports? So this appears on the surface to be a network admin (serverfault) question but I'm looking for a lower level answer from a network hacker type. I was pretty much oblivious to how networks actually work in real life until I started my summer internship. Then by way of having no other option (internship is at a pretty networking-centric place and I have to put together testbeds for testing [among other things] networks) I became familiar with them. For one thing the fact that there's no ""This goes out to the internet!"" port on commercial switches was kind of surprising until you reasoned about how it works (starts out like a hub til it 'learns' where ips are in terms of the physical port i guess?). And after this home-crafted self-discovery (or possibly error in thinking) I'm back at the extended stay hotel and looking at my cheap little home switch and it has an uplink port. Now my question to you Network hackers (in the good way) is why? We need superuser.com... Uplink ports can be thought of special ports for inter-switch connections. Sometimes they may have a higher speed (1G instead of 100M for example). Or they are interchangeable (laid out as modules). Some have multiple uplink ports (I had one with two) so you may have redundancy or multiple switched connected this way with the same logic (where is the mac address (on wthich other switch)?). This is sometimes true of higher-end gear but I've never seen it on a SOHO switch.  The ""uplink"" port on your SOHO switch is internally crossed over. It relieves you of having to use a crossover cable to connect two switches. That is the only difference. BTW: There isn't a ""this goes out to the internet"" port on SOHO switches either. You're confusing switches and routers/gateways. This confusion may be encouraged by manufacturers putting the two logically separate devices in one piece of hardware e.g. a router with a 4-port switch. While we're at it a wireless router w/ 4 port switch is actually logically three separate devices (router switch and access point). BTW #2: A switch (well except for layer 3 switches which arguably are only switches to the marketing department) actually learns where MAC addresses are. It neither knows about nor cares about IP. Awesome thanks"
435,A,"Creating a web server in pure C Im doing a little project on my university which now involves in creating a webserver only using C. I know a little about HTTP 1.1 and I've created a webserver in C# before. However I'd like to see how you would either begin with socket programming and using multi threads in C or how you would otherwise create a webserver in C. There's not really a lot of requirements I'd just like to get started with a little socket listening to port 80 sending out some respons text to the webserver / telnet-client. And maybe some code-structure advice would be of help too. The biggest requirement is probably that its for Linux. I've got a lot of help on this subject in both this thread and others regarding small parts of the webserver. I will put all information togehter as soon as possible and make a nice ""How to"" on this subject. If there is ever anyone who want to do the same thing :) To get some hints as to how a real HTTP server is written in C you can look at the lighttpd source. lighttpd (pronounced ""lighty"") is a web server designed to be secure fast standards-compliant and flexible while being optimized for speed-critical environments. Lighttpd is written in C. Lighttpd is used by some of the biggest websites including sites such as YouTube Wikipedia and meebo.  I'd start with this good socket tutorial and follow the HTTP 1.1 rfc spec.  The fact that it is a web server you want to build in C is less important than the fact that you want to do network socket programming in C. A web server is just a special case where you happen to be returning a certain type of content based on a request the low-level plumbing is more important. A good starting guide for Berkeley socket programming can be found here: http://www.uwo.ca/its/doc/courses/notes/socket/ it explains all the C structs and how to use them and is a great primer for network programming in C or other low-level languages. Yes you got an interesting point however that is why i did state that i know some about the protocol. I might just have been a little miss-leading. But you bring a nice resource to the table thank you. Do you also have any comments on architectural points when coding sockets in C ? It depends on what your goals are for the project if it's just to get a very simple web server up and running in C then I'd stick with using Unix processes forking off a new process to handle each incoming request than bother with threads. If you need performance use an existing program instead. +1 for the forking. Im not looking for re-inventing the wheel here this is for a purpose of learning how to program on unix and how to manage sockets. And by speaking of threads i mean it in a more abstract manner which means ""multi-tasking"". :) Unfortunately the link is broken. Did some search but wasn't successful. Anyone has the new URL? @Manfred better late than never: [An Introduction to Socket Programming](http://ods.com.ua/win/eng/program/socket/socketprg.html)  See this question/answer here for a bunch more. There's several ways to architect multithreaded socket servers but one I've found to be successful/scalable is to use two thread pools. Have 1 or more threads listening for incoming connections and have them pass those connection into a queue of work items. You then have a thread pool with a bunch of threads that process the queue. As soon as a thread is done processing one request it goes back to the queue and gets the next request. This sets things up as a classic producer/consumer deal which is generally a solved problem in multithreaded libraries. Having the connection accepted immediately lets the client know that the server is not down even when it's overloaded and you can keep an eye on the length of the work queue to know when you need to add new hardware. You could also do the same thing with a pool of processes using fork insteads of threads. Here's a paper describing a pipelined multithreaded web-server but it also outlines a variety of other standard architectures. I know basics of sockets and as i stated i know how HTTP 1.1 works. What i do need which that thread doesnt supply is guidance in the matter of Structure Webserver specific structure and guidance and good tutorials on how to create a socket app that uses multi-threads. Ah - I see what you're getting at.  Here is a list of small webservers some of them are written in C Tiny web servers And here is the code for a very small one I posted the code here because the website is unavailable. /* Copyright (C) 2007 Cosmin Gorgovan <cosmin@linux-geek.org> This program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation version 2 of the License. This program is distributed in the hope that it will be useful but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program; if not write to the Free Software Foundation Inc. 51 Franklin Street Fifth Floor Boston MA 02110-1301 USA. */ /* qshttpd is a lightweight http server. It was tested only under Linux. It is quite fast when handling small files actually about 6 times faster then Apache. I think it is useful to serve static content from your site. Home page: www.linux-geek.org/qshttpd/ */ /* Version 0.3.0 - alpha software See qshttpd.conf for a configuration example. */ /* TODO: logging virtual hosts */ #include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <errno.h> #include <string.h> #include <sys/types.h> #include <sys/socket.h> #include <netinet/in.h> #include <arpa/inet.h> #include <sys/wait.h> #include <signal.h> #include <pwd.h> #include <grp.h> #define BACKLOG 10 //Variables used in get_conf(). char conf[5] dir[500] port[10] charset[200] user[100] group[100]; int portf; //Sockets stuff int sockfd new_fd; struct sockaddr_in their_addr; socklen_t sin_size; struct sigaction sa; //Other global variables int buffer_counter; char * buffer; FILE *openfile; void read_chunk() { fread (buffer11048576openfile); buffer_counter++; } void sigchld_handler(int s) { while(waitpid(-1 NULL WNOHANG) > 0); } //Chroot and change user and group to nobody. Got this function from Simple HTTPD 1.0. void drop_privileges() { struct passwd *pwd; struct group *grp; if ((pwd = getpwnam(user)) == 0) { fprintf(stderr ""User not found in /etc/passwd\n""); exit(1); } if ((grp = getgrnam(group)) == 0) { fprintf(stderr ""Group not found in /etc/group\n""); exit(1); } if (chdir(dir) != 0) { fprintf(stderr ""chdir(...) failed\n""); exit(1); } if (chroot(dir) != 0) { fprintf(stderr ""chroot(...) failed\n""); exit(1); } if (setgid(grp->gr_gid) != 0) { fprintf(stderr ""setgid(...) failed\n""); exit(1); } if (setuid(pwd->pw_uid) != 0) { fprintf(stderr ""setuid(...) failed\n""); exit(1); } } void get_conf() { FILE *conffile; conffile = fopen (""/etc/qshttpd.conf"" ""r""); while (fgets (conf  6 conffile)) { if (strcmp (conf ""ROOT="") == 0){ fgets (dir 500 conffile); strtok(dir ""\n""); } if (strcmp (conf ""PORT="") == 0){ fgets (port 10 conffile); portf=atoi(port); } if (strcmp (conf ""CHAR="") == 0){ fgets (charset 200 conffile); strtok(charset ""\n""); } if (strcmp (conf ""USER="") == 0){ fgets (user 100 conffile); strtok(user ""\n""); } if (strcmp (conf ""GRUP="") == 0){ fgets (group 100 conffile); strtok(group ""\n""); } } fclose (conffile); } void create_and_bind() { int yes=1; struct sockaddr_in my_addr; if ((sockfd = socket(PF_INET SOCK_STREAM 0)) == -1) { perror(""socket""); exit(1); } if (setsockopt(sockfdSOL_SOCKETSO_REUSEADDR&yessizeof(int)) == -1) { perror(""setsockopt""); exit(1); } my_addr.sin_family = AF_INET; my_addr.sin_port = htons(portf); my_addr.sin_addr.s_addr = INADDR_ANY; memset(&(my_addr.sin_zero) '\0' 8); if (bind(sockfd (struct sockaddr *)&my_addr sizeof(struct sockaddr)) == -1) { perror(""bind""); exit(1); } drop_privileges(); if (listen(sockfd BACKLOG) == -1) { perror(""listen""); exit(1); } sa.sa_handler = sigchld_handler; sigemptyset(&sa.sa_mask); sa.sa_flags = SA_RESTART; if (sigaction(SIGCHLD &sa NULL) == -1) { perror(""sigaction""); exit(1); } } int main(void) { char in[3000] sent[500] code[50] file[200] mime[100] moved[200] length[100] auth[200] auth_dir[500] start[100] end[100]; char *result=NULL *hostname *hostnamef *lines *ext=NULL *extf *auth_dirf=NULL *authf=NULL *rangetmp; int buffer_chunks; long filesize range=0; get_conf(); create_and_bind(); //Important stuff happens here. while(1) { sin_size = sizeof(struct sockaddr_in); if ((new_fd = accept(sockfd (struct sockaddr *)&their_addr &sin_size)) == -1) { perror(""accept""); continue; } if (!fork()) { close(sockfd); if (read(new_fd in 3000) == -1) { perror(""recive""); } else { lines = strtok(in ""\n\r""); do { hostname = strtok(NULL ""\n\r""); if (hostname[0] == 'R' && hostname[1] == 'a' && hostname[2] == 'n' && hostname[3] == 'g' && hostname[4] == 'e') { rangetmp = hostname; strcpy(code ""206 Partial Content""); } } while (hostname[0] != 'H' || hostname[1] != 'o' || hostname[2] != 's' || hostname[3] != 't'); hostnamef = strtok(hostname "" ""); hostnamef = strtok(NULL "" ""); result = strtok(lines "" ""); result = strtok(NULL "" ""); if (strcmp(code ""206 Partial Content"") == 0 ) { rangetmp = strtok(strpbrk(rangetmp ""="") ""=-""); range = atoi(rangetmp); } strcpy(file result); if (opendir(file)){ if (file[strlen(file)-1] == '/'){ strcat(file ""/index.html""); openfile=fopen (file ""r""); if (openfile){ strcpy(code ""200 OK""); } else { //Here should be some kind of directory listing strcpy(file ""/404.html""); openfile = fopen (file ""r""); strcpy(code ""404 Not Found""); } } else { strcpy(code ""301 Moved Permanently""); strcpy(moved ""Location: http://""); strcat(moved hostnamef); strcat(moved result); strcat(moved ""/""); } } else { openfile=fopen (file ""rb""); if (openfile){ if (strlen(code) < 1) { strcpy (code ""200 OK""); } } else { strcpy(file ""/404.html""); openfile = fopen (file ""r""); strcpy(code ""404 Not Found""); } } } if (strcmp(code ""301 Moved Permanently"") != 0){ fseek (openfile  0  SEEK_END); filesize = ftell (openfile); rewind (openfile); if (range > 0) { sprintf(end ""%d"" filesize); filesize = filesize - range; sprintf(start ""%d"" range); fseek (openfile  range  SEEK_SET); } buffer_chunks = filesize/1048576; if(filesize%1048576 > 0){ buffer_chunks++; } sprintf(length ""%d"" filesize); buffer_counter = 0; buffer = (char*) malloc (sizeof(char)*1048576); } if (strcmp(code ""404 Not Found"") != 0 && strcmp(code ""301 Moved Permanently"") !=0){ ext = strtok(file "".""); while(ext != NULL){ ext = strtok(NULL "".""); if (ext != NULL){ extf = ext; } } } else { extf=""html""; } /* Maybe I should read mime types from a file. At least for now add here what you need.*/ if (strcmp(extf ""html"") == 0){ strcpy (mime ""text/html""); } else if(strcmp(extf ""jpg"") == 0){ strcpy (mime ""image/jpeg""); } else if(strcmp(extf ""gif"") == 0){ strcpy (mime ""image/gif""); } else if(strcmp(extf ""css"") == 0){ strcpy (mime ""text/css""); } else { strcpy(mime ""application/octet-stream""); } strcpy(sent ""HTTP/1.1 ""); strcat(sent code); strcat(sent ""\nServer: qshttpd 0.3.0\n""); if(strcmp(code ""301 Moved Permanently"") == 0){ strcat(sent moved); strcat(sent ""\n""); } strcat(sent ""Content-Length: ""); if(strcmp(code ""301 Moved Permanently"") != 0){ strcat(sent length); } else { strcat(sent ""0""); } if(strcmp(code ""206 Partial Content"") == 0) { strcat(sent ""\nContent-Range: bytes ""); strcat(sent start); strcat(sent ""-""); strcat(sent end); strcat(sent ""/""); strcat(sent end); } strcat(sent ""\nConnection: close\nContent-Type: ""); strcat(sent mime); strcat(sent ""; charset=""); strcat(sent charset); strcat(sent ""\n\n""); write(new_fd sent strlen(sent)); while (buffer_counter < buffer_chunks) { read_chunk(); write(new_fd buffer 1048576); } close(new_fd); exit(0); } close(new_fd); } return 0; } I don't like that code. But thanks for the example im looking for something more commented more structured and more explaining i guess. It is pretty clear... added to favorites thanks! Segmentation fault when i change the fprintf to printf because of errors with %d any ideas  It's strange that Apache was not mentioned here. It's might be a little bit to complex for your purpose but it's definitely one of the most impressive pieces of C code that you can learn from including socket programming and more advanced topics. BTW i'm not 100% sure but as far as know Apache is single threaded application and include built in scheduler for efficiency. That's not a tutorial really :/ It is very well documented and one of the best web servers... So as i stated it's might be a bit complex but if you want to learn best practice this is the place.  When I had a similar assignment in a network programming course I used Beej's Guide to Network Programming. I only had to write a simple server that handled GET and POST requests but Beej's Guide got me all the way through the project. I like that very much thank you! You're welcome. Good luck with your project. Well something to check out over the weekend! thanks a ton It's been a while but I dug up the code that I used for the project and have started to clean it up for anyone interested it is available on GitHub: https://github.com/fekberg/GoHttp  If you're targeting linux I'd go with forking instead of threads to keep it simple.  Advanced Programming in the Unix Environment comes in handy for a lot of the syscalls on linux. Already got that. That's more generaly though than i wanted.."
436,A,What's the difference between streams and datagrams in network programming? What's the difference between sockets (stream) vs sockets (datagrams)? Why use one over the other? Stream Socket: Dedicated & point-to-point channel between server and client. Use TCP protocol for data transmission. Reliable and Lossless. Data sent/received in the similar order. Long time for recovering lost/mistaken data Datagram Socket: No dedicated & point-to-point channel between server and client. Use UDP for data transmission. Not 100% reliable and may lose data. Data sent/received order might not be the same Don't care or rapid recovering lost/mistaken data  A long time ago I read a great analogy for explaining the difference between the two. I don't remember where I read it so unfortunately I can't credit the author for the idea but I've also added a lot of my own knowledge to the core analogy anyway. So here goes: A stream socket is like a phone call -- one side places the call the other answers you say hello to each other (SYN/ACK in TCP) and then you exchange information. Once you are done you say goodbye (FIN/ACK in TCP). If one side doesn't hear a goodbye they will usually call the other back since this is an unexpected event; usually the client will reconnect to the server. There is a guarantee that data will not arrive in a different order than you sent it and there is a reasonable guarantee that data will not be damaged. A datagram socket is like passing a note in class. Consider the case where you are not directly next to the person you are passing the note to; the note will travel from person to person. It may not reach its destination and it may be modified by the time it gets there. If you pass two notes to the same person they may arrive in an order you didn't intend since the route the notes take through the classroom may not be the same one person might not pass a note as fast as another etc. So you use a stream socket when having information in order and intact is important. File transfer protocols are a good example here. You don't want to download some file with its contents randomly shuffled around and damaged! You'd use a datagram socket when order is less important than timely delivery (think VoIP or game protocols) when you don't want the higher overhead of a stream (this is why DNS is primarily a datagram protocol so that servers can respond to many many requests at once very quickly) or when you don't care too much if the data ever reaches its destination. To expand on the VoIP/game case such protocols include their own data-ordering mechanism. But if one packet is damaged or lost you don't want to wait on the stream protocol (usually TCP) to issue a re-send request -- you need to recover quickly. TCP can take up to some number of minutes to recover and for realtime protocols like gaming or VoIP even three seconds may be unacceptable! Using a datagram protocol like UDP allows the software to recover from such an event extremely quickly by simply ignoring the lost data or re-requesting it sooner than TCP would. VoIP is a good candidate for simply ignoring the lost data -- one party would just hear a short gap similar to what happens when talking to someone on a cell phone when they have poor reception. Gaming protocols are often a little more complex but the actions taken will usually be to either ignore the missing data (if subsequently-received data supercedes the data that was lost) re-request the missing data or request a complete state update to ensure that the client's state is in sync with the server's. +1 for the great explanation. So excellent explanation! Nice explanation with example. Simply superb for including the detail of SYNACK.
437,A,"Is there a Java client that supports Telnet RFC2217 (to communicate with COM ports over a network connection)? This should be simple very simple but I'm having a hard time with it. Problem I'm looking for an open source project in java that will communicate using the RFC2217 protocol. I find no shortage of Java RFC2217 Terminal Servers but I need a client. If all these people are writing servers someone has to have written a client! Right? I just can't find one in java. Context We have a piece of hardware (SeaLINK+16 Ultra) that's physically connected to a dozen serial devices and accepts TCP network connections to control them. This device is networked with a server that has virtual COM ports that our application uses to communicate with those dozen devices. Basically our server ultimately sends network data to this SeaLINK device and this device converts the TCP traffic to COM data and transmits it via serial to its connected serial devices. The server and the SeaLINK device use the protocol specified in RFC2217 in order to communicate. The commands that need to be sent to these serial devices are very simple (Cisco IOS). Short strings like ""enable"" ""write memory"" ""reload"" etc. The issue is the virtual COM ports enabled on our server require drivers that are a HUGE hassle to install in Linux and are not cross-platform. Plus our entire application is written in Java so if we could just find a java networking package that can ""speak RFC2217"" we could solve our problems in no time and our application wouldn't need to be bundled with drivers. Summary All I need to do is ""wrap"" these commands into TCP packets that comply with RFC2217. There should be a java client out there somewhere that you provide it a command string and it opens a socket and transmits your characters in a manner compiant with RFC2217. Meaning I wouldn't have to deal with installing drivers or using virtual COM ports locally. All I'd have to do is run code along the lines of: RFC2217Client magicJavaClient; magicJavaClient.setServer(""192.168.40.5""); magicJavaClient.setPort(4162); magicJavaClient.connect(); magicJavaClient.send(""enable""); magicJavaClient.send(""write memory""); magicJavaClient.close(); Heck I'd even settle for something that created virtual com ports locally--as long as it's all java. Something like this COM Redirector is exactly what I need but it's not in Java. Thanks in advance for any and all suggestions! Update NVTCom seems to be the type of java-based RFC2217 client I'm looking for but it's so poorly documented that it's almost unusable. Are there any other clients out there? We would even consider a commercial solution. http://www.viara.eu/en/nvtcom.htm Thanks! Looks like it might do exactly what I need. It's very poorly documented but the source code looks okay. I'll have to wait until I'm back in the office next week to really test this out. Thanks. Gave this a try yesterday and it didn't work. It complained that the server was not RFC2217 compliant which is almost certainly not true. Both the documentation and error-handling are poor so the only way to decipher the problem is to step through the code. It's not clear what the problem is. Are there other java clients? @gmale It is the only one I know of. I think I saw some commercial ones. You might want to go that route at least you get support. @gmale - You could always use Serial Port Redirector and then use Javacomm or an equivalent serial port library for Java to talk to the serial port. @Romain: that's pretty much what we're doing--we're using RxTx to open and manage serial connections. The problem with Serial Port Redirector and programs like it is that they are Windows applications and we use only Unix based systems (RedHat and OS X). I've been working with Tibbo--which is a linux-based serial port ""emulator"" but I'm having a lot of problems installing the drivers because I'm not a Linux guru and our application runs on a virtual machine which complicates everything (I'm actually opening another question related to the issues I'm having with that effort) @gmale I have done a lot of interfacing with serial devices but only on Windows using the old JavaComm API (which is no longer supported by Oracle they say to use RxTx). The app I wrote used deicated machines just to talk to the serial devices on an Intranet. The machines then used EJB technology to talk to the server. Also used the JavaComm library to receive data from Serial scanners and to write to Serial Port based label printers. It should be pretty straight forward to talk to a Serial Port using RxTx on Linux. @gmale look at this - might give you more oprions http://stackoverflow.com/questions/52187/virtual-serial-port-for-linux Also you could use a USB to Serial port converter and then program as if for a Serial port. The OS sees it as a Serial POrt even though it is hooked into the box via USB.  Everybody talks about the weather but nobody does anything about it. -- Mark Twain I found myself in the same boat looking for a Java RFC 2217 client library and finding nothing useful. So I decided to take a stab at it. Check out my new jvser project and let me know if you find any bugs or would like to join the effort."
438,A,"How to write a simple Bittorrent application? How to write a simple bittorrent application. Something like a ""hello world"" using a bittorrent library I mean a simplest of the application to understand the working of bittorrent. I would prefer a python or a C/C++ implementation but it can be any language. Platform is not an issues either but i would prefer Linux. Recommendations for the library to follow I have downloaded the source code for one (i think official bittorrent) from - http://sourceforge.net/projects/bittorrent/develop. But I see a lot of other libraries at http://en.wikipedia.org/wiki/Comparison_of_BitTorrent_clients#Libraries. I would appreciate recommendations on this. How to test an application if all you have is one laptop. Read the spec (http://wiki.theory.org/BitTorrentSpecification) write code :-) You should try libtorrent (rasterbar). http://libtorrent.org If you want to write your client in python on linux install it with: sudo apt-get install python-libtorrent A very simple example of python code to use it to download a torrent: import libtorrent as lt import time import sys ses = lt.session() ses.listen_on(6881 6891) info = lt.torrent_info(sys.argv[1]) h = ses.add_torrent({'ti': info 'save_path': './'}) print 'starting' h.name() while (not h.is_seed()): s = h.status() state_str = ['queued' 'checking' 'downloading metadata' \ 'downloading' 'finished' 'seeding' 'allocating' 'checking fastresume'] print '\r%.2f%% complete (down: %.1f kb/s up: %.1f kB/s peers: %d) %s' % \ (s.progress * 100 s.download_rate / 1000 s.upload_rate / 1000 \ s.num_peers state_str[s.state]) sys.stdout.flush() time.sleep(1) print h.name() 'complete' Wow. They really achieved their goal of ""easy to use"". This is an excellent implementation! 6881 is the listen port. If binding to that listen port fails libtorrent will try to increment it by one and try again. If it keeps failing until it reaches 6891 it will stop trying and just fail. errors are reported as alerts. Can you tell me what is this doing? `ses.listen_on(6881 6891)` and why those values?"
439,A,How many simultaneous (concurrent) network connections does iphone support? I can't seem to find this anywhere but I'm sure there must be an answer... How many simultaneous HTTP connections can an iphone app service? In other words if I build and run a bunch of NSURLConnections asynchronously how many can be in flight before it starts queuing them? Also is there a method to check how many threads are available and/or in use programmatically? I'm trying to write an intelligent pre-fetching engine and these limits (if any) would be useful information. I'm not sure there is a limit--if there is it's probably the same as OSX. The iPhone doesn't start queuing NSURConnections. That's totally your responsibility. The iPhone will start all your asynchronous NSURLConnections in parallel. The only thing that will stop it is your memory :) I recently implemented a connection pool (my solution was based on NSOperationQueue) just because of that. Now I can control parallel connections and my app doesn't crash when loading hundreds of avatar images. About your second question. I actually don't know about a way to get the list of current threads. I had a look at NSThread API but no sign of an API for that.. I agree but I guess if there's not a hard limit the answer I'm looking for is how to determine what the practical limitations are. Sure we can arbitrarily set the limit to 2 or 3 or infinity but I'd like to develop a more intelligent management system. I think the post I referenced in my answer comes closest to that.  So the reason I assumed there may be a simultaneous connection limit at the mobile OS level is because many mobile browsers enforce one. There are techniques for speeding up loading speed of the mobile version of your website by ensuring that there are as few additional content fetches as possible. Images are the main culprit but css files javascript files etc all require an additional connection. In mobile setting up and tearing down a connection is much more expensive than a single connection with more data and one technique for improving page load performance is embedding your CSS and javascript in the page itself. It sounds like this limitation may not exist in the OS itself at least on the iphone platform. As far as an answer to this question I found this post that discusses the practical limitations of simultaneous connections. It's not a hard limit but I think it answers the point of the question. http://stackoverflow.com/questions/1940903/background-threads-consuming-100-cpu-on-iphone-3gs-causes-latent-main-thread
440,A,"Copying Files over an Intermittent Network Connection I am looking for a robust way to copy files over a Windows network share that is tolerant of intermittent connectivity. The application is often used on wireless mobile workstations in large hospitals and I'm assuming connectivity can be lost either momentarily or for several minutes at a time. The files involved are typically about 200KB - 500KB in size. The application is written in VB6 (ugh) but we frequently end up using Windows DLL calls. Thanks! I agree with Robocopy as a solution...thats why the utility is called ""Robust File Copy"" I've used Robocopy for this with excellent results. By default it will retry every 30 seconds until the file gets across. And by default a million retries. That should be plenty for your intermittent connection. It also does restartable transfers and you can even throttle transfers with a gap between packets assuing you don't want to use all the bandwidth as other programs are using the same connection (/IPG switch)?.  You could use Microsoft SyncToy (free). http://www.microsoft.com/Downloads/details.aspx?familyid=C26EFA36-98E0-4EE9-A7C5-98D0592D8C52&displaylang=en  Try using BITS (Background Intelligent Transfer Service). It's the infrastructure that Windows Update uses is accessible via the Win32 API and is built specifically to address this. It's usually used for application updates but should work well in any file moving situation. http://www.codeproject.com/KB/IP/bitsman.aspx  How about simply sending a hash after or before you send the file and comparing that with the file you received? That should at least make sure you have a correct file. If you want to go all out you could do the same process but for small parts of the file. Then when you have all pieces join them on the receiving end. That's exactly what BITS does.  I've used Robocopy for this with excellent results. By default it will retry every 30 seconds until the file gets across. Thanks for the RoboCopy and CopyFileEx suggestions. CopyFileEx in particular looks promising. Currently the client application is using [SHFileOperation](http://msdn.microsoft.com/en-us/library/bb762164(VS.85).aspx) to do the actual file copy.  SMS if it's available works.  I'm unclear as to what your actual problem is so I'll throw out a few thoughts. Do you want restartable copies (with such small file sizes that doesn't seem like it'd be that big of a deal)? If so look at CopyFileEx with COPYFILERESTARTABLE Do you want verifiable copies? Sounds like you already have that by verifying hashes. Do you want better performance? It's going to be tough as it sounds like you can't run anything on the server. Otherwise TransmitFile may help. Do you just want a fire and forget operation? I suppose shelling out to robocopy or TeraCopy or something would work - but it seems a bit hacky to me. Do you want to know when the network comes back? IsNetworkAlive has your answer. Based on what I know so far I think the following pseudo-code would be my approach: sourceFile = Compress(""*.*""); destFile = ""X:\files.zip""; int copyFlags = COPYFILEFAILIFEXISTS | COPYFILERESTARTABLE; while (CopyFileEx(sourceFile destFile null null false copyFlags) == 0) { do { // optionally increment a failed counter to break out at some point Sleep(1000); while (!IsNetworkAlive(NETWORKALIVELAN)); } Compressing the files first saves you the tracking of which files you've successfully copied and which you need to restart. It should also make the copy go faster (smaller total file size and larger single file size) at the expense of some CPU power on both sides. A simple batch file can decompress it on the server side.  Hm seems rsync does it and does not need server/daemon/install I thought it does - just $ rsync src dst."
441,A,How to implement server-sided applications that can process user's inputs? There are some web-based services such as converting .flv files into .mp3 files .doc files into .pdf files etc. These are the servers that take files from the user and apply respected applications (such as conversion). After that the converted file may be available to the requested user. I understand a little bit about writing a text and stores it into a server with javascript and php. What are the required tools and languages in order to achieve the above? Can some sort of C++ programs on the server side be invoked by some means? Or do I need completely different tools and methods? Please provide me some keywords for this as I do not know what to google for. You could write a simple php page that takes a user input via POST or GET that calls a server side executable and then once the executable has done its thing returns the results to the user as a http response.  Well for PDFs this can be done using entirely PHP. See http://www.php.net/manual/en/book.pdf.php for the PDF functions and http://uk.php.net/manual/en/pdf.examples-basic.php for some examples of usage.
442,A,"Unable to create a ServerSocket in eclipse (java) i am very new to programming in java however have a lot of experience in .NET (c# & vb.net). I am trying to create a new instance of a serversocket class in eclipse IDE and when i type the following code it is giving me an ""Unhandled exception type IOException"" and i havent even tried to run the code yet!! I dont understand how my code is exceptioning before runtime or what i can do to fix it. Can someone help me? Offending code: ServerSocket server = new ServerSocket(1234 5 InetAddress.getLocalHost()); If it is giving ""Unhandled exception type IOException"" in java file while editing it means that you need to enclose this statement in try-catch block try{ ServerSocket server = new ServerSocket(1234 5 InetAddress.getLocalHost()); }catch(IOException ex){ e.printStackTrace(); } thanks viv.. is this normal for java coding? to handle exceptions that may occur before they actually do? It is a good practice to do but some where it wants that an exception of a particular kind have to be handled. Generally you should enclose all your code within try-catch it's a nice practice.  This is a feature of the Java language called Checked Exceptions. Basically there are Exceptions in code that you call that can be determined at compile time. The Java language designers thought it prudent to force you to handle them. In this case the ServerSocket class constructor in its method signature declares that it throws an IOException. There are two ways to make the compile error go away. You can wrap the code in a try/catch. try { ServerSocket server = new ServerSocket(1234 5 InetAddress.getLocalHost()); } catch (IOException e) { // handle exception } Or you can pass responsibility on to the calling method. For example suppose you called the ServerSocket constructor inside a method called createSocket(). You would declare your method like so public ServerSocket createSocket() throws IOException { ServerSocket server = new ServerSocket(1234 5 InetAddress.getLocalHost()); return server; } In this case you're just moving responsibility up the call chain but sometimes that makes sense. This is so common in the Java language that Eclipse offers the two options above as Quick Fixes."
443,A,writing a network discovery tool in Objective-C I want to embark on a project for a CS class. Can anyone provide insight on how to write a tool that will map out a network and state device info IP info open ports etc. I will be using OS X. A conceptual overview is a design. There are [guidelines for asking homework questions](http://meta.stackexchange.com/questions/10811/how-to-ask-and-answer-homework-questions) on StackOverflow that you should read. You really need to ask specific questions about an existing implementation rather than ask the community to design your solution for you. @brian - I dont want a solution designed. Why would you presume this? I am looking for conceptual overview of how this is done. There's a few things you can use to discover the nodes on a network. SNMP will help detect a few different network devices. Broadcasting an SNMP get request for sysName.0 will give you results from anything that responds to SNMP (even if they do not have a value for the OID). Some may be printers some may be servers some may be NAS etc. sysDescr.0 can also be used although for many network devices (in my experience) it returns the same value as sysName.0. Mac OS X comes with the NetSNMP libraries. Once you have received responses from SNMP agents you can then send extra SNMP requests to further determine details about the device. There are human readable files that explain these in detail in /usr/share/snmp/mibs. You can use a broadcast ping (your application will require root privileges to give you raw access to the Internet Protocol for constructing a broadcast ping packet or a ping packet in general). Just broadcast a few packets and wait for replies. The source code to BSD's ping utility used in Mac OS X can be found at Apple's website. The ping executable is usually installed with owner root and with the setuid bit meaning that the ping executable is run as root even when invoked by a non-root user (this is why non-root users can use ping). You'll notice though that in ping's source code it only creates the socket as root and then immediately drops its root privileges. Zeroconf/Bonjour will also help. Have a look at Core Foundation's CFNetServices or Foundation's NSNetServices. Mac systems can be configured to publish themselves very easily. There are also Windows and Linux implementations but of course these need to actually be running on the network devices before you can detect them with your program. In order to determine what ports are open you can simply run a loop over the desired port range and attempt to make a connection. Keep in mind though that many servers consider this an attack and will drop packets and perhaps even permanently blacklist your IP. Once you have made a successful TCP connection you can look up the port number in the /etc/services file to determine the name of the service. This can be done using the POSIX function getnameinfo. this is very constructive for helping me get starting in laying out functionality and requirements gathering. I appreciate it.
444,A,how to make Multiplayer Game I already knew the basic of android programming especially the one at game. now I want to try multiplayer game. but I dont have any background about multiplayer game. so I want have a number of question : What is the basic of multiplayer programming? how is the communication method for multiplayer? should it bluetooth? or maybe client-server ? or maybe peer to peer? if the game that I want to make required 2 android handset near each other? what should I know in order to understand this multiplayer? is there any basic tutorial about how to make multiplayer game? can an android handset acted as a Server? I don't plan to make any sophisticated game yet. may be just multiplayer tic tac toe is alright for me. the most important is that I understand the basic of multiplayer game programming. :) You need fast birection connection.Name is sockets  websockets IO steam ... Please do not use peertopeer  you must have server when you can catch and save info about users and game. Alternative multyplatform solution for multiplayer is html5 teh. Today you can render 3d on webapplication and use websockets. For first time you can use Chat app becouse chat is basic for any communitation. dataSend -> send usefull data about player position  rotation ... ondatareceived -> handling data for remote player www.zlatnaspirala.netfast.org  this is first person 3d web site  very fast and you can easy implement websockets for multiplayer. You can use phpwebsocket (fancy websocket version - find on net 100% work just start in console php c:\PATH\server.php) or use one of the best websockets jwebsocket . Sorry on my poor english!  I would give this series of articles a read: http://gafferongames.com/networking-for-game-programmers/ It's mostly in C/C++ but you can translate it to Java. Learn about UDP sockets in Java for example: http://download.oracle.com/javase/tutorial/networking/datagrams/index.html This should be enough to get you going. I would probably do client-server setup. You can do P2P but it's harder from what I've heard. I updated number 2 question. :)  its not mass multiplayer just 2 player game played near each other. anyway. great article there. :) The articles I linked to aren't only for MMO games just for anything over a network. If you wanted to have the game only for handsets near each other I am not the person to ask :P . I think it would be bluetooth but don't take my word for it. I'm sure somebody will come post with the answer.  Looking up the APIs for android would defiantly help. client server would be the easiest to set up. Or something that i am doing in a game i am making is to have a web server and your application hit the web server for updates and whatnot. This is working really well with a turned based game I am making. Might take more capital up front ie. Hosting your own web server but since android supports SQLlite you are good to go. :)  There are couple of ways to do multiplayer game: Multiplayer on the same device: Make multiplayer logic for your game and allow multi touch controller for both players on the same screen. It can be turn based or simultaneous. For this game you do not have any dependency but the players should be near each other. Bluetooth game: This the next stage of multiplayer games. A little bit trickier but can be done. The controller need to get and synchronize the game between two devices that are near each other. A short review of the Bluetooth android API and you are good to go. Score comparison: This is not a real multiplayer but you can upload scores of the users and compare with other users. You can do it yourself with server side that will store all the scores for each user or use existing services that allow score comparison like Skiller SDK or scorelop SDK. Real multiplayer games: This is the best one from my opinion. Everybody can play with everybody else in real time (as far as latency allows :D). This one is pretty difficult if you want to do all by yourself. Here you will need a strong server side and a lot of server logic. But again you can use existing services that handle the server side for you. I went with the Skiller multiplayer SDK. Good support and monetization features. Whatever multiplayer implementation you choose give your game to your friends first so that they could review it and tell you what can be improved. It will help you A LOT!!! Good luck. Three of your four answers promote the Skiller SDK. If you're associated with them please make sure you properly disclose this. what is Skiller SDK? Nope Brad :) just new to this forum and because i found many solutions for my questions here i felt like contributing with something of my own. For some time i am trying to add competition aspect to my game and although there are some solutions available i did not find a proper multiplayer tools without having to build my own server side logic skiller is the solution i found for this but if you have other solutions i will be more than happy to check and post my opinion here. By the way my additional recommendation would be andengine. Very easy to integrate and build a 2d physics game.
445,A,Is it possible to execute net_enable_timestamp() from user space in linux? I'd like to enable timestamping received network packets. I found that function net_enable_timestamp() should do this form me but I can't find it neither in CentOS nor in ArchLinux. But when I execute: # cat /proc/kallsyms | grep net_enable_timestamp c126a330 T net_enable_timestamp In kernel source from kernel.org it's located in linux/netdevice.h but not in both distros mentioned above. Is it possible to execute it? How? What's wrong with enabling SO_TIMESTAMP on the socket and accessing the timestamp of the last message via ancilliary data? Alternatively you can call ioctl(socket SIOCGSTAMP) as described in socket(7) Edit: Looking at the kernel source code it appears that net_enable_timestamp() actually gets called indirectly when you do setsockopt(sock SO_TIMESTAMP ... So my answer answers your question :) yes! you are right! I don't know why I ommited .c files while digging through kernel source and trying to solve my problem.
446,A,Using recvfrom() with raw sockets : general doubt I have created a raw socket which takes all IPv4 packets from data link layer (with data link layer header removed). And for reading the packets I use recvfrom. My doubt is: Suppose due to some scheduling done by OS my process was asleep for 1 sec. When it woke upit did recvfrom (with number of bytes to be received say 1000) on this raw socket (with the intention of receiving only one IPv4 packet and say the size of this packet is 380 bytes). And suppose many network applications were also running simultaneously during this time so all IPv4 packets must have been queued in the receive buffer of this socket. So now recvfrom will return all 1000 bytes (with other IPv4 packets from 381th byte onwards) bcoz it has enough data in its buffer to return. Although my program was meant to understand only one IPv4 packet So how to prevent this thing? Should I read byte by byte and parse each byte but it is very inefficient. What OS are you using? can you please post your code !!!! Raw sockets operate at the packet layer there is no concept of data streams. You might be interested in recvmmsg() if you want to read multiple packets in one system call. Recent Linux kernels only there is no equivalent send side implementation.  IIRC recvfrom() will only return one packet at a time even if there are more in the queue.
447,A,"How to check if internet is active and it works? Possible Duplicate: Easiest way to determine whether iPhone internet connection is available? Hi guys I'm developing an iPhone app. It works totally with internet. When I run my app how can I check if internet is active and it works correctly? And if it's not I would to show a popup. Thanks. Be aware that Reachability is (in my opinion) a misnomer. Reachability including reachabilityWithHostName (as in Apple's example) only indicates whether the device has an available internet connection with which to reach the outside world. IMPORTANTLY: it does not indicate whether the specified host is actually up and running. To find out whether the remote host is actually up and running you'll need to initiate a connection to the remote host and handle a timeout in the case that it is not.  Another vote for Apple's reachability example code here. Note it's important to get this right - apparently it's grounds for app rejection if you provide a misleading error message relating to networking. For example if you say you could not reach myhost.com when in fact it's because there is no network connection.  There's no such thing as ""Internet"". There exists large number of computers and other hardware linked together. Now if you need a particular host you can ping it (also in code don't know if iPhone lets you do this though). If you need to ensure that particular service on the particular host is up and running your only option is to send a request and check the response. ""Internet"" or ""the Internet"" is a something that exists and even thought some host may not be reachable you are still connected to the Internet. You can find more info in [the usual place](http://en.wikipedia.org/wiki/Internet)  Check out this blog post. Also see Apple's Reachability sample code here. All of the above should be enough to get you rolling."
448,A,tcp server question How does a server knows what kind of packets his receiving ? (strcutarray...) ? PS: i know dumb question What do you mean with *what kind of packets*? Do you mean the protocol? If so at which layer? There are no dumb questions only dumb downvoters. (Actually that isn't quite true; there *are* dumb questions but this most definitely isn't one.) It doesn't - its just a bunch of bits/bytes. The application using application level protocols decides how to interpret those bits/bytes. Just like memory is a bunch of bits/bytes - a pointer to a struct can be forced to point anywhere and the struct can used to read the memory but the data may be nonsensical. Your application logic ensures (hopefully) that the struct pointer is only applied to memory that contains valid struct data. Similarly your network application must decide how to interpret what the bits in the packets or TCP stream mean. An application may use a well-known protocol to decide how to interpret those bytes. For example the HTTP protocol indicates what the client should transmit and the server knows to interpret that data from the client according to how the HTTP spec. Regardless of what the client sends (e.g. if a gaming client accidentally sent a binary stream to the HTTP server) the HTTP server will nevertheless try to interpret the bits as a HTTP client request. Thank youi understand now. @vBx - Happy to help  TCP servers receive a stream of bytes. Any interpretation beyond this is up to the application logic.
449,A,C# UDP Listener Thread -- Don't Burn CPU with while(true)? With the following code is there a better way to set up a UDP listen than a while(true) with Thread.Sleep(10)?  public void Start() { socket.Bind(ip); while (true) { data = new byte[1024]; receivedDataLength = socket.ReceiveFrom(data ref Remote); raw = Encoding.ASCII.GetString(data 0 receivedDataLength); row = new LogRow(raw); //Console.WriteLine(row.ClientIp); row_queue.Enqueue(row); Thread.Sleep(10); } } ReceiveFrom is blocking. Thread will be suspended until data arrives. Unless Socket is in non-blocking mode. You should add that because `ReceiveFrom` is blocking there's no need for the `Sleep`. Ah okay it must be my insert thread that is burning the CPU. Thanks I will drop the `sleep`. @Kyle Brandt♦ yes `Spleep` is excessive. Run Application in debug and when burning starts pause debug and check each non-sleaping thread.
450,A,"In Perl how can I check for the existence of Socket options without generating warnings? I'm checking for the existence and default values of various socket options using Perl. #!/usr/bin/perl -w use strict; use Socket; if (defined(SO_BROADCAST)) { print(""SO_BROADCAST defined\n""); } if (defined(SO_REUSEPORT)) { print(""SO_REUSEPORT defined\n""); } When I run this it outputs: SO_BROADCAST defined Your vendor has not defined Socket macro SO_REUSEPORT used at ./checkopts.pl line 9 Is there a way to do this without generating warnings in the output? That message is coming from AUTOLOAD in Socket.pm. When it finds a constant that isn't supported it croaks. You can catch that with an eval:  use Socket; if( defined eval { SO_REUSEPORT } ) { ...; }  Ask whether the sub has been defined not whether the expression's value is defined: if (defined &SO_REUSEPORT) { ... } The documentation for defined explains: You may also use defined(&func) to check whether subroutine &func has ever been defined. The return value is unaffected by any forward declarations of &func. Note that a subroutine which is not defined may still be callable: its package may have an AUTOLOAD method that makes it spring into existence the first time that it is called—see perlsub. If the sub is exported into your namespace it has to be defined. Why do you check for a subroutine define? Are these actually defined as subroutines in Socket.pm and not as constants? @Robert The constants *are* subs! See http://perldoc.perl.org/perlsub.html#Constant-Functions You're answer is better than the one I originally picked."
451,A,How to know when bytes are to send/recv from client server c++ My question is when you have a proxy server and you need to send/recv with client and send/recv with remote server how do you know at what end there is data to be send/recv so I can call the apropiate functions. I need to recv/send bytes from a website to a client(via proxy) and from client to server (via proxy)but I dont know in what order they are commingI saw that is diferent for most sites. My curent implementation is this: 1) receive from client 2) send to server //infinite loop here 3) receive from server 4) send to client // until bytes from server is 0 This just works for a few sitesand doesnt load them completyonly 15-20 KB. Any suggestions ? Thanks. receive from client send to server while(true) { if(recvfrom server is not zero) send to client if(recvfrom client is not zero) send to server } I think this might work..  Use select() to wait for data to be available from either side then read it and pass it over.  Your task will be forwarding data from client to server and back. Since client and server can transmit data simultaneously approach when you read everything from client than pass it to server and vice versa won't work: consider situation when you waiting for client to start transmission and client wants to data from server before starting its own transmission. So there are following ways to make it work: Build two explicit pipes -- one from client to server another from server to client. This will require 4 threads -- one reading from client and passing data to other writing to server and vice versa. This have drawback of having 4 threads per client which limits number of simultaneous clients your proxy can support. Employ select(2) functionality (or similar sys call from Windows API) + nonblocking sockets. This will tell you when there's data to be read or written. Non-blocking sockets are needed if you want to serve several clients per one thread so thread won't become blocked in read/write syscall. There's select sample in man page and lots of info on internet. One famous page dedicated to servers development is here. Thank you very much
452,A,"Threads for peer to peer communication I am trying to make a multi-player network game. Each player is represented by a rectangle on the screen. I am using OpenGL for the graphics and also the user input (commands like MOVE-LEFT MOVE-RIGHT etc ) will be handled by it (or GLUT or sumthing). I have the following architecture for the game. There are 4 players(nodes) in the game. Each player is sending and receiving data using UDP. Each player can send data to any other player. Data is required to be sent by a player if there is any input from the corresponding user. (For example MOVE-LEFT command etc). Whenever a player (say p1) receives any data from any other player(say p2) (like new position of the player p2 on the screen) the player p1 's screen should be updated immediately. I am thinking on the following lines : Create one thread for handling graphics. Create 2 more threads  1 each for receiving and sending data using UDP. Whenever the graphics thread gets input for 'myposition' from the user it updates the shared global variable 'myposition'. The network-send thread which is waiting on this variable gets activated and tells every other player about its new position. Similarly whenever 'position' updates are received from any other player 'i' the network-receive thread updates the global variable player[i].position. The graphics thread will now redraw the scene with the updated positions. Is this design correct. If yes How good is this design and how can i improve it Network Game Programming is a monster of a topic so it is hard to say ""yes this is how you design a network architecture"". It is completely dependent on your game requirements. What sort of volume of packets do you plan on sending? Will there be a packet sent every frame that indicates Player A is holding the left key? Is this traffic confined to a LAN? For something as simple as synchronizing movement between amongst 4 clients putting send receive and rendering in separate threads seems like overkill. Perhaps as a starting point you should begin with a more simple design and make the move to multithreading when you feel that your packets are not going out fast enough. For example you may wish to have a game loop like the following (all within the same thread): while (running): readUpdSocketForIncomingPackets(); updateGameObjects(); renderGameObjects(); sendPacketsToPeers(); At the start of each frame you can read your udp socket for incoming packets and update positions (and whatever else you are sending to your peers) then draw. As game input is processed packets are created and accumulated in a packet queue. Doing it this way allows you to perform optimizations such as cramming/merging multiple messages into one packet removing duplicate messages (e.g. only send the latest position update) etc. So at the end of each game loop the final queue of packets are processed and send to the peers. But again this is a big topic and I've glossing over a lot of details. Take a look at gaffer's blog: http://gafferongames.com/networking-for-game-programmers/what-every-programmer-needs-to-know-about-game-networking/ He's got some great articles that address some fundamentals of network game programming."
453,A,"Network output from server can't be read by client So I have a simple socket server and a socket. I run the socket server successfully. The client socket connects and sends a string - this works. I want the server to write back different information based on this string. I can check what the string is and get an OutputStream to the client but whenever I write to it and flush the InputStream client-side is NEVER in a ready state and will never get a message back... I just don't see what I'm doing wrong. All the code is at http://pastebin.com/u/omegazero NetworkAgent.java is the client SimbadAgent.java is the server and UserAgent.java is the actual implementation of said server (the server is abstract for other reasons). Compile everything then run UserAgent followed by NetworkAgent and you will see what happens. Executed your code (after commenting the reference to StringQueue in SimbadAgent) and I got the following output. wrote get_cmd Input shutdown? false iS()iI()iM()iB()iA()iD()i ()iB()iO()iO()iY()iA()NETWORKAGENT: Response to ""get_cmd"": ""SIMBAD BOOYA"" I tried this in Ubuntu 11.04 Wait... really? I'll have to test it on a different OS... maybe it's my Debian setup or something weird. Thanks. It was actually because I had to switch them over to PrintWriter and BufferedReader if anyone reads this eventually. Java is fickle about character streams apparently.."
454,A,What applications can make use of the Kerberos Protocol? Does anyone know what sort of applications make use of the Kerberos protocol? Is the Kerberos protocol still used? or is it considered old hat? Cheers Joel Windows 2000 and later use Kerberos as their default authentication method. —Wikipedia: Kerberos (protocol)
455,A,In Ruby language how to post data to a web page that need authentication? I use e-texteditor as my primary editor in which you can execute dynamic languages directly such as Ruby Python Perl etc. Now I want to write blog post in this editor and post it to my blog directly. Though the blog need authentication(username and password). How can I post data to this kinda of pages? Thanks. You should be able to do this with mechanize. It emulates a real broswer and is quite powerful. About the only major feature lacking is a javascript engine so it won't work if the blog uses javascript in its authentication.
456,A,Is connecting a TcpClient object with host string and port number slower than using an IPEndpoint? TcpClient has one Connect method which takes a hostname as a string and a port. TcpClient.Connect(String hostname Int32 port) There is another variant which takes an IPEndPoint. TcpClient.Connect(IPEndPoint endpoint) I assume that the first variant above has to use a DNS lookup to get the IP address from the hostname before establishing the TCP connection while the second does not need to do so. I don't know if the first one is optimized to check whether the hostname just contains the direct IP address or if it does a DNS lookup each time. So my question is Is the second variant faster when I already know the IP address? P.S. I am developing for Windows Mobile (6.1) and testing it over a router in my local network. When the router is connected to the open internet the connections happen fast and I get a quick response from the server. But when I am only using my local network (i.e. router is disconnected from the open internet) connections seem to take forever. Don't know if the open internet somehow speeds up DNS resolution. Not an answer but if you've got a complete version of Visual Studio (Ultimate or Pro/Testing editions IIRC) you can simulate network setups and monitor traffic through the testing tools It happens just the way you assumed the first overload (hostname port) internally performs a DNS lookup and then calls the second overload (IPEndPoint). If you want to see what exactly happens you could download .NET Reflector (http://www.red-gate.com/products/dotnet-development/reflector/) and inspect the TcpClient class. By reading the background information it looks like the device still tries to use the DNS it can no longer connect to hitting the timeout. I don't know enough about networking to back that assumption up though.
457,A,"List the IP Address of all computers connected to a single LAN I am writing a program where you connect for various reasons to other computers in a LAN. However rather than having to input the IP address for multiple computers (a pain in the butt) I was wondering if there is a way to list the IP addresses of all the computers in a LAN. I have researched all day and as of yet have found nothing suitable. Is this because nothing of this sort exists? Thank you in advance. EDIT: It would seem that with the many views this post is getting I should post my actual solution. In general the naming conventions for computers IP addresses on a LAN are the same. example being 192.168.2.* * being replaced with any valid number. My program detects the IP address displays it to the user then asks for the first 3 blocks of IP. It then sequentially scans up to 200 in the given IP naming convention by pinging and waiting for a response. No response no computer. It can do everything you can do with an IP once it knows it has a computer behind it. You could get the subnet and loop through the available addresses to ping them. But that wouldn't help for the ones that are just turned off. If you had access you could look at DHCP lease files `ARPing` as Jason described is the only cross-platform and firewall-defensible solution. BlueRaja's WNetEnumResource() approach is reasonable if you're limited to Windows boxen. The scanning/nmap approaches have too many dragons with host-based firewalls. There is not a magic bullet/API call. The iphelper API's SendARP() is likely your best bet: http://msdn.microsoft.com/en-us/library/aa366358%28VS.85%29.aspx Why not 254? (255 in larger subnets) Any host discovery tool can help you here. In particular Nmap will certainly give you this information although it might be overkill in this situation. Google for ""ping scan"" and you should get some useful results. (+1) use nmap via a process call and parse the results.  1) Read the subnet mask and calculate all the IP addresses in the subnet mask you are in. Then you can either user ICMP ping (standard ping) or ARP ping to list all the valid IP addresses. ARP Ping is much reliable in a subnet setting. 2) You can nmap to list all the hosts nmap -nsP 192.168.10.1/254 | grep ^Host  You're not really going to find anything more reliable than pinging or arpinging addresses on the same subset. I implemented this for a certain piece of software back in the day on my first internship and last time I checked (to be fair it was several years ago) that is what they were still using for this functionality. I take that to mean that they haven't found anything better. It is not hard to find the source code for these and translate them to C#. ping arping. Alternatively you just shell out to a command prompt and execute ping and then parse the results. Actually `System.Net.NetworkInformation.Ping` should do the trick. No need to translate anything from C.  See WNetOpenEnum() and WNetEnumResource() here.  Can you just look at the IP and subnet mask on the network adapter and ping every address? Whichever ones respond can be queried to see if it has whatever you need to connect."
458,A,How can I send raw IP packets with Perl under Windows? Is there any Perl module which has the capability to send raw packets on Windows? I know there is Net::RawIP but it seems that it does not work on Windows. I think I remember reading something about Microsoft removing the ability to send raw packets. @Brad Correct after Steve Gibson made a huge stink about it MS disabled raw socket support in XP SP2. The original page is no longer available but you can find many references to it: http://www.google.com/search?q=http%3A%2F%2Fwww.GRC.com%2Fdos%2Fwinxp.htm On XPSP2/3 you need to use something like WinPCap it can create raw ethernet packets for you  Have you looked at Net::Write?
459,A,"Virtual Network Connection I can see that lot's of programs like openvpn and Teamviewer for their VPN Connection creat a virtual network connection on windows. I want to create one for myself for testing purposes. Is it possible to create one programmatically or so? Question is very clear how to do so programmatically so I would say not to close this. I don't think this belongs to superuser.com this is a programming matter for it's driver... Are you looking for a ""ready made solution"" **or** an answer as to how to **code** one up? Actually both .... MSDN Ras Dial function This might be a step in the right direction? From the page: The Remote Access Service (RAS) supports Virtual Private Network (VPN) connections in addition to conventional remote access connections that use Point-to-Point Protocol (PPP). In a VPN connection the VPN packets are encapsulated in IP packets and sent across an IP network such as the Internet. Therefore access to an IP network is a requirement in order to establish a VPN connection. If the client computer has an always-on connection to an IP network for example a connection to an IP LAN the client can establish the VPN connection using a single call to the RasDial function. I need answer not direction :p Calling the Rasdial function the parameters that are specified in the link will make windows dial a VPN for you?  It sounds like you're looking for the Tap-Win32 driver. It's the driver OpenVPN on Windows uses to create the virtual interface you are seeing and in fact part of the OpenVPN package. This subsystem is also available on many *nixes. The interface to this TAP driver is roughly the same on all OSes. You open a special file and write raw Ethernet frames to this file. The driver then inserts these frames into the virtual interface. Conversely any packets that are transmitted on the virtual interface can be read from the special file as raw Ethernet frames. Most implementations also have a TUN mode which operates at layer 3 instead of layer 2. So you will be reading raw IP IPv6 etc. packets instead of Ethernet frames. I have no experience with this on Windows so I'm going by quick skimming of source code here. OpenVPN goes through most of these steps in tun.c function open_tun. You'll find multiple definitions of this function but they're #ifdef'd for different OSes (so search for CreateFile). The basic way this seems to operate on Windows is: Before any application operating a TAP interface is started one or more virtual interfaces are pre-created (by the installer?). These interfaces start out disconnected. Your application starts and does a special CreateFile call on ""\\.\Global\GUID.tap"". Where GUID is replaced by the GUID that describes the specific virtual interface. Virtual interfaces can be iterated in the registry key which is defined as ADAPTER_KEY in ""tap-win32\common.h"" in the OpenVPN source code. Your application may perform some DeviceIoControl calls. OpenVPN uses this a bunch of times to get the driver version get MTU set TUN mode and other misc things. At this point the interface is probably showing up as connected in Windows and you might even be reading DHCP requests you're receiving from Windows itself already. OpenVPN goes through a large amount of hoopla to configure the interface using other parts of Windows networking APIs but this is not specific to the TAP driver. So while the API is really just a special file and thus fairly simple there's a lot to actually managing the interface. But if you're just in it for testing this may well be enough. You can then manually configure your test interface in Windows."
460,A,"binding a port number to a TCP socket (outgoin) to send packets I know that it is not easy to bind a port number to TCP socket that you would use to send data (because systems usually bind a random port to sockets). But I read on one article that by using some low level networking methods you can bind a port number to a TCP socket then use it to send data ? Does anyone has an idea about how that could be done ? I am using c programming language similar: http://stackoverflow.com/questions/204169/how-to-create-a-tcp-socket-connect-using-c-to-a-predefined-port The reasons for wanting to do this at all are usually spurious. I want to do a port prediction (because my NAT box don't use port preservation) You can simply call bind() before connect() in a client in the same way that you would call bind() before listen() in a server. There is nothing more complicated to it than that. then will I able to listen through same port(in this case 22222) also? I think NO. but please confirm.  Actually it is quite easy. Simply use the bind function as you would for a server. then will I able to listen through same port also? I think NO. but please confirm.  Bind it before connecting. s = socket(AF_INET SOCK_STREAM 0); /* ... */ memset(&client_addr 0 sizeof(client_addr)); client_addr.sin_family = AF_INET; client_addr.sin_port = htons(22222); if (bind(s (struct sockaddr *) &client_addr sizeof(client_addr)) < 0) { perror(""bind""); exit(1); } connect(s (struct sockaddr *) &server_addr sizeof(server_addr)); then will I able to listen through same port(in this case 22222) also? I think NO. but please confirm."
461,A,"Socket Programming in Java I am in the process of implementing a protocol based on an RFC written in my lab. I intend to use Java to run the simulations. I don't think I can use object serialization to pass around messages because I want the messages to be interoperable with other systems implemented in other languages which I think is not possible using serialization. What feature can I use in Java to be able to talk to nodes implemented in a different language? Also there are about 50 different types of messages that can be sent and received each having a different structure. Ex: hello bye register etc. Each message contains some information that is needed to be processed. I plan to implement each message type as a class in Java. What is the cleanest way possible to figure out at the receiving node's end what type of message was sent by the sending node? Ex: How would I as a receiver know that a node who sent me a message just now wants to register with me? I'll be grateful if I could get suggestions on some good design patterns. ""I don't think I can use object serialization to pass around messages"". Hate to break this to you but [CORBA works this way for several languages](http://en.wikipedia.org/wiki/Serialization#Programming_language_support). If you need something else there's XML. The link in my previous is incorrect. That page does not list the CORBA langauge mappings. Mappings exists for Ada C C++ Lisp Ruby Smalltalk Java COBOL PL/I and Python to my knowledge although this [OMG page](http://www.omg.org/technology/documents/recent/corba_language.htm) does not list all. As you noted serializing Java objects is a convenient means of communicating between two Java processes but does not work for non-Java processes. For that type of communication I would recommend using either XML or JSON. Both of these are essentially plain text formats that conform to a specification. There are libraries available in most languages for converting native types to/from XML or JSON. As for the second part of your question both the sending and receiving systems have to agree on a common message format / specification. For example the following xml could represent the intent to register to both the sender and receiver: <Command>REGISTER</Command> Won't transferring large numbers cause considerable overhead when transported as plain text instead of binary form? @Anand no why would you think that? Yes. As with many aspects of programming there are trade-offs. XML and JSON are more verbose than say a custom binary format. However they come with the benefit of being easier to maintain because they are both human readable and are more flexible in terms of permitting changes to the specification. @Johan: 2147483647 occupies 4 bytes in binary form and 10 bytes in text form. Over double the overhead. In general the performance hit is negligible but the final call will depend on the needs of your application. Also because XML and JSON are text they have high compression ratios if bandwidth is a concern. @btreat: This is a low level system that I intend to implement. Something like implementing a protocol for routers. I need to be able to keep the overhead to a bare-minimum while making it interoperable. With binary you also need to be concerned about big-endian little-endian considerations between sender and receiver. @Anand - the original post didn't explicitly specify what information was needing to be transmitted for all message types. However since you were considering java object serialization (a fairly verbose format) I assumed your question was more around interoperability rather than minimizing bandwidth consumption. @anand that's what you call overhead? Consider that a TCP packet contains a 20-60 bytes header. @btreat: You said something about compression. In what way can I compress my data so as to make the other system aware of it? By say sending my compression algorithm before sending the message? Like one would send an encryption algorithm before the actual communication. How are protocols such as Bittorrent implemented? @btreat: Consider that I am implementing a P2P protocol that can be implemented by anyone in this world by looking at the RFC. Something very much like Bittorrent an open protocol. I cannot consider Apache Thrift/Google Protocol Buffers as that would no more leave my protocol an open one. How should I approach this problem? I would only recommend compression if you are sending very large (100K+) sized xml. It sounds like that may not be the case here. As @Johan has highlighted you don't even need to really worry about your payload message size until it reaches a certain threshold. @anand - if your goal is to implement something like a BitTorrent defining a protocol to pass XML or JSON would be a fine place to start.  If you wish to do raw socket communication you could make it as easy or as complicated as you wish. The communication format could be as easy as sending a number representing a message type or for instance an XML. However there are many systems out there that does this for you already. Examples of such are e.g. Corba or Thrift which is available to many programming languages.  I would suggest that you look at Thrift and Protocol Buffers for this.  Personally I feel that Protocol buffers will be the best choice."
462,A,"java.net.SocketException: Network is unreachable: connect I am trying to download a xml text file from a web server using this method: static void download (String url  String fileName) throws IOException{ FileWriter xmlWriter; xmlWriter = new FileWriter(fileName); System.out.println(""URL to download is : "" + url); // here Exception is thrown///////////////////////////////// BufferedReader inputTxtReader = new BufferedReader (new BufferedReader(new InputStreamReader(addURL.openStream()))); //////////////////////////////////////////////////////// String str ; String fileInStr = """"; str = inputTxtReader.readLine(); while (!(str == null) ){///&& !(str.equals(""</tv>"")) fileInStr += (str + ""\r\n""); str = inputTxtReader.readLine(); } xmlWriter.write(fileInStr); xmlWriter.flush(); xmlWriter.close(); System.out.println(""File Downloaded""); } Sometimes this exception is thrown (where I specified is code): java.net.SocketException: Network is unreachable: connect at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333) at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182) at java.net.Socket.connect(Socket.java:518) at java.net.Socket.connect(Socket.java:468) at sun.net.NetworkClient.doConnect(NetworkClient.java:157) at sun.net.www.http.HttpClient.openServer(HttpClient.java:389) at sun.net.www.http.HttpClient.openServer(HttpClient.java:516) at sun.net.www.http.HttpClient.<init>(HttpClient.java:233) at sun.net.www.http.HttpClient.New(HttpClient.java:306) at sun.net.www.http.HttpClient.New(HttpClient.java:318) at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:788) at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:729) at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:654) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:977) at java.net.URL.openStream(URL.java:1009) at MessagePanel.download(MessagePanel.java:640) at WelcomThread.run(MainBody2.java:891) Please guide me Thank you all. `!(str == null)` just looks confusing you should write `str != null`. Where and how are you initializing the addURL variable? ""Network is unreachable"" means just that. You're not connected to a network. It's something outside of your program. Could be a bad OS setting NIC router etc. That too but is it me or is there no definition for `addURL` anywhere in there? @TC1: ha good catch!  You are facing a connection breakdown. Does this happen in 3G WiFi or ""plain"" connection on a computer? Anyway you must assume that the connection may be lost from time to time when writing your app. For example with mobiles this happens frequently in the tube in basements etc. With PC apps this is less frequent but occurs sometimes. A retry can be a good solution. And a clean error message that explains the network is not available at this moment too. I added a thread that as request time outs it tries again. this this was effective. because in my network we broadcast several media contents; this problems occur. thank you for help"
463,A,"CFHTTPStream segmentation faul during NSRunLoop I'm trying to assemble a toy application to interact with http servers via CoreFoundation. I'm not using NSURLConnection because I want to be able to specify a proxy different from the one set in the OSX System Preferences. My problem is that I wasn't able to find any working example and the Apple documentation do not provide a working snippet of code. Anyhow this is the code I pulled together: #import <CoreFoundation/CoreFoundation.h> #import <CoreServices/CoreServices.h> #import <Foundation/Foundation.h> void myCallBack (CFReadStreamRef stream CFStreamEventType eventType void *clientCallBackInfo){ NSLog(@""Something happened""); BOOL *done = ((BOOL *)clientCallBackInfo); *done = TRUE; } int main(int argc char *argv[]){ NSAutoreleasePool *pool = [[NSAutoreleasePool alloc] init]; CFStringRef bodyString = CFSTR(""""); // Usually used for POST data CFStringRef url = CFSTR(""http://www.apple.com""); CFURLRef myURL = CFURLCreateWithString(kCFAllocatorDefault url NULL); CFStringRef requestMethod = CFSTR(""GET""); CFHTTPMessageRef myRequest = CFHTTPMessageCreateRequest(kCFAllocatorDefault requestMethod myURL kCFHTTPVersion1_1); if (myRequest){ NSLog(@""Request created: %@ %@"" CFHTTPMessageCopyRequestMethod(myRequest) CFURLGetString(CFHTTPMessageCopyRequestURL(myRequest))); } CFDataRef bodyData = CFStringCreateExternalRepresentation(kCFAllocatorDefault bodyString kCFStringEncodingASCII 0); CFHTTPMessageSetBody(myRequest bodyData); NSLog(@""HTTPMessage body set""); CFDataRef mySerializedRequest = CFHTTPMessageCopySerializedMessage(myRequest); NSLog(@""Serialized string generated""); CFStringRef mySerializedString = CFStringCreateFromExternalRepresentation(kCFAllocatorDefault mySerializedRequest kCFStringEncodingASCII); NSLog(@""Serialized request: %@"" mySerializedString); CFReadStreamRef myReadStream = CFReadStreamCreateForHTTPRequest(kCFAllocatorDefault myRequest); BOOL done = FALSE; CFReadStreamOpen(myReadStream); CFOptionFlags registeredEvents = kCFStreamEventHasBytesAvailable | kCFStreamEventErrorOccurred | kCFStreamEventEndEncountered; NSLog(@""Setting read stream client""); if (CFReadStreamSetClient(myReadStream registeredEvents myCallBack (void *)&done)){ CFReadStreamScheduleWithRunLoop(myReadStream CFRunLoopGetCurrent() kCFRunLoopCommonModes); while (!done){ NSLog(@""Wait""); [[NSRunLoop currentRunLoop] run]; } }else{ NSLog(@""Error setting readstream client""); } CFRelease(myRequest); CFRelease(myURL); CFRelease(url); CFRelease(mySerializedRequest); myRequest = NULL; mySerializedRequest = NULL; [pool drain]; } The code compiles but it throws a segfault: Reading symbols for shared libraries .+++++...................... done 2010-09-06 21:36:34.470 a.out[27973:a0f] Request created: GET http://www.apple.com 2010-09-06 21:36:34.473 a.out[27973:a0f] HTTPMessage body set 2010-09-06 21:36:34.474 a.out[27973:a0f] Serialized string generated 2010-09-06 21:36:34.474 a.out[27973:a0f] Serialized request: GET / HTTP/1.1 2010-09-06 21:36:34.478 a.out[27973:a0f] Setting read stream client 2010-09-06 21:36:34.479 a.out[27973:a0f] Wait Program received signal EXC_BAD_ACCESS Could not access memory. Reason: 13 at address: 0x0000000000000000 0x00007fff83b7b83d in _signalEventSync () (gdb) bt #0 0x00007fff83b7b83d in _signalEventSync () #1 0x00007fff83b7b7f4 in _cfstream_solo_signalEventSync () #2 0x00007fff83b7b734 in _CFStreamSignalEvent () #3 0x00007fff8391c3a1 in HTTPReadStream::streamEvent () #4 0x00007fff83b7b883 in _signalEventSync () #5 0x00007fff83b7c5d9 in _cfstream_shared_signalEventSync () #6 0x00007fff83b1be91 in __CFRunLoopDoSources0 () #7 0x00007fff83b1a089 in __CFRunLoopRun () #8 0x00007fff83b1984f in CFRunLoopRunSpecific () #9 0x00007fff8300fa18 in -[NSRunLoop(NSRunLoop) runMode:beforeDate:] () #10 0x00007fff8300f8f7 in -[NSRunLoop(NSRunLoop) run] () #11 0x0000000100000c94 in main (argc=1 argv=0x7fff5fbff5b8) at test.m:56 I don't understand where the error is. Anyone can show me the light or point me to a clear example on how to manage CFHTTPStreams? Thanks Andrea The problem was that I didn't read carefully the documentation of CFReadStreamSetClient. The third parameter has to be a reference to a CFStreamClientContext whereas I was passing a reference to an arbitrary pointer which instead has to be included into the context. Fixing this resolved the segmentation fault and now the test ca is working as expected."
464,A,"How to determine an incoming connection is from local machine I have a SocketServer accepting incoming connections. For security reasons I should only allow local connections (connections from the machine on which server is running). How can I determine if an incoming connection is from another machine? Is the following code safe for this? Socket socket = someServerSocket.accept(); String remoteAddress = socket .getInetAddress().getHostAddress(); if (!fromThisMachine(remoteAddress)) { // Not from this machine. } while fromThisMachine() method is like this: public boolean fromThisMachine(String remoteAddress) { try { Enumeration<NetworkInterface> interfaces = NetworkInterface.getNetworkInterfaces(); while (interfaces.hasMoreElements()) { NetworkInterface networkInterface = interfaces.nextElement(); Enumeration<InetAddress> addresses = networkInterface.getInetAddresses(); while (addresses.hasMoreElements()) { InetAddress inetAddress = addresses.nextElement(); String hostName = inetAddress.getHostName(); String hostAddr = inetAddress.getHostAddress(); if (hostName.equals(remoteAddress) || hostAddr.equals(remoteAddress)) { return true; } } } } catch (Exception e) { e.printStackTrace(); return false; } log(""Unauthorized request to server from: "" + remoteAddress); return false; } Thanks Mohsen Thanks skaffman. The following code worked with a little manipulation (hard-coding 127.0.0.1). int port = ..... SocketAddress socketAddress = new InetSocketAddress(""127.0.0.1"" port); ServerSocket serverSocket = new ServerSocket(); serverSocket.bind(socketAddress); serverSocket.accept(); If I read local address from InetAddress.getLocalHost() other network users on the same subnet are still able to see my server. Mohsen. D'oh yes I forgot that `InetAddess.getLocalHost()` doesn't give you 127.0.0.1 it gives the external IP address of that local host. My bad. Yes I've found InetAddress.getLocalHost() doesn't really work as expected too. Be warned that the ""I'm listening on 127.0.0.1 so only local users can connect to me"" assumption sometimes fails - eg if you have a proxy running on your machine that can be coerced into connecting to 127.0.0.1 on behalf of a remote user.  If you want to limit connections from the localhost then specify that when you open the ServerSocket. If you only listen on localhost then you'll only get connections from localhost.  int port = ..... SocketAddress socketAddress = new InetSocketAddress(""127.0.0.1"" port); ServerSocket serverSocket = new ServerSocket(); serverSocket.bind(socketAddress); serverSocket.accept(); edited to use hard-coded 127.0.0.1 It doesn't work. See: I'm on 192.168.0.1 and someone else on 192.168.0.2. I want to deny his access to my server socket listening on 192.168.0.1:port but this solution doesn't work. It only caused that connecting to 127.0.0.1:port doesn't work from my own machine.  InetAddress.getByName( null ) always returns the loopback address. See the javadoc  int port = ..... SocketAddress socketAddress = new InetSocketAddress( InetAddress.getByName( null ) port); ServerSocket serverSocket = new ServerSocket(); serverSocket.bind(socketAddress); serverSocket.accept(); Nice one. Wish I'd known this a while back. I got terribly confused. This is one of the worst examples of feature creep. Providing `InetAddress.getLoopbackAddress()` would be so much cleaner. I discovered this simply by incident. The address string in my application got nulled under some condition and instead of NPE my application still chugged alone. This has taught be a valueable lesson to read Java API carefully and with full attention. @AlexanderPogrebnyak silly question... what about [InetAddress.isLoopbackAddress()](http://docs.oracle.com/javase/8/docs/api/java/net/InetAddress.html#isLoopbackAddress--)? @Gili. The OPs original question was about accepting connections only from the localhost. `isLoopbackAddress` will not help you obtain the localhost IP address. @AlexanderPogrebnyak good point. Thanks :)"
465,A,"Confusion in htons- little endian/ big endian When I send a integer variable from one process to other through socket and then printing the value at received end the value is still the same without using ntohl/htonl then where do I need to use these functions other than initializing socket structures. I understand litte/big endian. But why do we need to convert port and IP nos to host/network byte order when value remains the same. Please explain in detail how the integer is tranferred over network? It remaimns the same because on your architecture the network order is the same as the native order. If you never anticipate compiling your code for another architecture you could omit the hton/ntoh calls. Your code would then not be portable. This is not correct. He could well be using an architecture where the host order is different from the network order. Your code will 'work' without htonX/ntohX whenever the architecture of both endpoints have the same endianess. You are correct -- I intended to say something like that but wasn't very clear in my answer.  If you want your program to be portable then any time you send an integer greater than 1 byte in size over the network you must first convert it to network byte order using htons or htonl and the receiving computer must convert it to host byte order using ntohs or ntohl. In your case the reason the value is still the same is probably because the sending computer and the receiving computer are of the same endianness. In other words the sending computer and the receiving computer you're working with are both little endian (or big endian whatever the case may be.) But if you want your program to be portable you can't rely on this to always be the case. One day for example the sending computer may be an Intel x86 and the receiving may be a Sun SPARC and then your program will fail if you don't use htons. I wrote client and server and did not use htonl and ntohl. Say when I sent 6 server received 6 value. Then what is the need of these functions? The need of these functions is in the event that the client is running on a little-endian computer and the server is running on a big-endian or vice versa. What about non-integral data types such as structs? Do they have an endianess?  If you want to send data from an x86 or amd64 machine to a machine with a PowerPC processor in binary format you'll quickly see that your data encounters the ""NUXI problem"" as the different processors treat the integers differently and appear to swap the bytes. (They don't actually swap the bytes -- they just work with them in a different order.) When working on x86 or amd64 the least-significant byte comes first in memory (this way you can perform addition from lower to higher memory addresses). The PowerPC puts the most-significant byte first in memory (this way you can sort numbers based on the bytes that come earlier in memory -- a string sort and an integer sort can be exactly the same.)"
466,A,"Building a Network Appliance Prototype Using a standard PC with Linux and Two NIC's I am willing to build a prototype of network appliance. This appliance is suppose to transparently manipulate Ethernet packets. It suppose to have two network interface cards having one card connected to the outside leg (i.e. eth0) and the other to the inside leg (i.e. eth1). In a typical network layout as in the attached image it will be placed between the router and the LAN's switch. My plans are to write a software that hooks at the kernel driver level and do whatever I need to do to incoming and outgoing packets. For instance an ""outgoing"" packet (at eth1) would be manipulated and passed over to the other NIC (eth0) which then should be transported over to the next hope My questions are: Is this doable? Those NIC's will have no IP address is that should be a problem? Thanks in advance for your answers. (And no there is no such device yet in the market so please ""why reinvent the wheel"" style of answers are irrelevant) This is more of a ServerFault question what do you mean? have you read this to the end? this is a software question not an hardware at all. AFAIK Line encryption units work in a similar way (2 ports an embedded pc and some custom silicon) so it can be done in a mix of hardware and software but I don't know enough detail to post an answer. -2 wonder why. perhaps if a question is not related to ruby/c#/jquery one might risks to be getting down votes. as long as the NICs are set to promiscous mode they catch packets on the network without the need of an IP address set on them. I know it can be done as there are a lot of companies that produce the same type of equipment (I.E: Juniper Networks Cisco F5 Fortinet ect.) well they also need to be bridged.  I'd suggest libipq which seems to do just what you want: Netfilter provides a mechanism for passing packets out of the stack for queueing to userspace then receiving these packets back into the kernel with a verdict specifying what to do with the packets (such as ACCEPT or DROP). These packets may also be modified in userspace prior to reinjection back into the kernel. In doing intermediate filtering/modification be aware that you're probably going to be introducing some significant latency into the message stream especially if you're passing data in and out of user-space. Is this going to be in a high-bandwidth network? You may want to think about fast hardware and kernel level packet mangling. thanks allot will take a look at this as well searching for this at the firewall areas was a smart move! if I would be able to build a prototype fast enough using it I would prefer it over scapy which is very slow comparing to libipq  Apparently it can be done. I am actually trying to build a prototype of it using scapy"
467,A,Passing a Entity over a network? I have been studying Java networking for a while. I am using ObjectInputStream and ObjectOutputStream for the I/O between sockets. Is this possible to transfer a Entity or Model from server to client and vise versa? How can I implement this? Am I suppose to implement the Entity or Model to Serializable? Your response is highly appreciated. You definitely need to look at one of these two libraries Google gson: http://code.google.com/p/google-gson/ Converts Java object to JSON and back. advantage is that the object can be consumed or generated by Javascript. I have also used this for Java-Java RPC but it gives you flexibility if you want to target browsers later Google protocol buffers: http://code.google.com/apis/protocolbuffers/ This is what google uses for RPC. Implementations for Java C Python. If you need performance and the smallest size this is the one to go with (The trade off is you can't look at the data easily to debug problems like you can with gson which generates plaint text JSON).  I am not sure what sort of special thing you mean to denote by capital-E Entity and capital-M Model; these terms don't have any fixed privileged meaning in Java (although they might with respect to a certain API or framework.) In general if by these you just mean some specific Java objects then yes you can send any sort of objects this way and yes they would be required to implement Serializable. The only limitations would be if these objects contained members whose values wouldn't make sense on the other end of the pipe -- like file paths etc. Note that if you send one object you'll end up sending every other object it holds a non-transient reference to as well. I apologize for the capitalization. I'll see try this.  First of all... why sending an object through I/O stream? What's wrong with XML? However you can always send/receive an object through I/O stream as long as the sender can serialize the object and the receiver can deserialize the object. Hope it helps Sending a XML file over the network? XML has a large overhead. RMI or Hessian provide much better performance with Hessian you can also easily switch to XML on demand (Burlap).
468,A,"What's the purpose of using sendto/recvfrom instead of connect/send/recv with UDP sockets? I can grasp the concept of TCP vs UDP but still I don't why there are 2 ways to send UDP packets and with that I still don't understand if this is absolutely necessary to bind() and accept()... accept() is for TCP. It has nothing to do with UDP. connect() in UDP doesn't do anything to the other end it just conditions the local API to know who you are sending to and receiving from. If you don't already know that or don't care or want to send to multiple destinations with the same socket you don't use connect() you use sendto() instead. Similarly for receiving. Consider a UDP server for example. It would call recvfrom() so it would get the source address information process the request create the response and send it to that address via sendto(). No connect() involved anywhere ergo not possible to use either send() or recv(). It is only necessary to bind() a server because the clients need a fixed port number to send to. A client needn't bind() at all: an automatic bind() will take place on the first send/sendto/recv/recvfrom using a system-assigned local port number. what about listen() ? and about the bind() method does it mean I receive everything that land on that port ? @gokoon 1. listen() is for TCP. It has nothing to do with UDP. 2. What exactly does 'what about bind()' mean as a question? 3. Yes you receive everything addressed to that UDP port if you aren't connected. @EJP This http://linux.die.net/man/7/udp says that when connect() is called UDP uses read/write! So read/write or send/recv ? @entropy No it doesn't. It says ""When connect(2) is called on the socket the default destination address is set and datagrams can now be sent using send(2) or write(2)"". Similarly in that circumstance you can use either *read()* or *recv()*. @EJP: After calling `connect()` what happens to datagrams from peers other than the default destination? Are they discarded? Can you `bind()` multiple sockets to the same local port and `connect()` each to a different peer then will they all receive data coming from their specified peer? @BenVoight Datagrams from the wrong source are discarded. I guess you could do that multiple bind() thing.  It is important to understand that TCP is ""Connection Oriented"" and UDP is ""Connection-less"" protocol: TCP: You need to connect first prior to sending/receiving data to/from a remote host. UDP: No connection is required. You can send/receive data to/from any host. Therefore you will need to use sendto() on UDP socket in order to specify the destination. Similarly you would want to use recvfrom() to know where the UDP data was received from. You can actually use connect() on UDP socket as an option. In that case you can use send()/recv() on the UDP socket to send data to the address specified with the connect() and to receive data only from the address. (The connect() on UDP socket merely sets the default peer address and you can call connect() on UDP socket as many times as you want and the connect() on UDP socket of course does not perform any handshake for connection.) Hope this helps."
469,A,"How to initialize struct in6_addr? I do know one method to do this const struct in6_addr naddr6 = { { 0x3f 0xfe 0x05 0x01 0x00 0x08 0x00 0x00 0x02 0x60 0x97 0xff 0xfe 0x40 0xef 0xab }}; but could not with this const struct in6_addr naddr6 = { { { 0x3ffe0501 0x00080000 0x026097ff 0xfe40efab } } }; and it seems that I could either 1/2/3 paris of brackets.Why? thanks. Because one need to indicate which form of the address it is addressing (shown using C99): const struct in6_addr naddr6 = { { .u6_addr32 = { 0x3ffe0501 0x00080000 0x026097ff 0xfe40efab } } }; The first bracket pair is for the in6_addr struct the second for the union: struct in6_addr { union { uint8_t u6_addr8[16]; uint16_t u6_addr16[8]; uint32_t u6_addr32[4]; } in6_u; };  The portable way to do this is like so: struct in6_addr s6 = { }; if (!IN6_IS_ADDR_UNSPECIFIED(&s6)) inet_pton(AF_INET6 ""2001:db8::1"" &s6);"
470,A,Can anybody tell me How bonjour works on IOS I created an application using bonjour and i am able to send files from one device to another. But the question is: i am not able to discover the devices on the LAN without running both the applications on the device. Do i need to run the application using bonjour to get it detected using bonjour. Yes you do. Running the application registers the appropriate entries into the iOS multicast DNS service. Once you shut the app down I expect it removes itself from the multicast DNS registry (which it is correct to do because it is no longer available) so you can't find it from other devices. EDIT: (Very roughly) Bonjour is multicast DNS. The Bonjour service runs a multicast DNS server. When your application starts up it communicates with the local multicast DNS server and creates a number of entries that identify the service it is making available the ports it is available on and other relevant attributes. It also registers itself as interested in learning about any other network device that is running the service. The local multicast DNS server makes announcements that signal to any one else listening on the network that a new service is available. Your app (on a different machine) is notified by the Bonjour service that another client has appeared and that is more or less how the magic is done. Longer multicast DNS writeups are all around: Google is your friend. Thanks for your reply Do you know how exactly it works
471,A,IPV6 usage study I have a pretty overview on the TCP/IPv4 stack. I am planning to get into the next version of IP which is IPV6 which is for the future I believe. I really need some valuable inputs from anyone as to which site or link or maybe I can do something practically by using some kind of simulators to learn something like atleast some UDP6 stack or TCP 6 stack. Any valuable inputs? Why do you need simulators?. Just activate IPv6 on your machine. All operating systems have IPv6 support for many years. I hope this might be of some help to you - http://ctieware.eng.monash.edu.au/twiki/bin/view/Simulation/IPv6Suite cheers  Although I am a bit late to reply but as I found this question so generally I am replying here. The best simulation software available at the time of writing this answer is GNS3 and you can download it from GNS3.net for free. It simulates real routers IOS images and you can create real routing packets environment and learn a lot even for IPv6 routing protocols. I literally created my CCIE LAB environment through this fantastic software and you can also connect those GNS3 routers to your physical switches. I also wrote some articles in my blog related to IPv6 and you may read few of the articles below Understanding IPv6 Addressing IPv6 configuration  See the following ipv6 site - it has links to specifications ipv6 enabled applications and ipv6 stack implementations: http://www.ipv6.org/ GNR: if it solves your question then accept the answer. SupertuxThanks a lot
472,A,Closing and Opening Frames in wxPython I'm working on writing a very simple client/server application as an excuse to start learning network/gui programming in python. At the moment I'm stuck on transitioning from my login frame to the main frame of the application. The login frame is a subclass of wx.Frame and basically I just want to close it and open the main frame when it receives confirmation from the server: def handleServerSignon(self msg): if msg.getCode() == net.message.HANDLE_IN_USE: self.status.SetStatusText('Handle already in use') elif msg.getCode() == net.message.SIGNON_SUCCESSFUL: self.Close() mainWindow = wx.Frame(None wx.ID_ANY 'main window' wx.DefaultPosition \ (640 480)) mainWindow.Show(True) I can't even get this to give a consistent error message though... sometimes it works sometimes it crashes with the following: python: ../../src/xcb_io.c:242: process_responses: Assertion `(((long) (dpy->last_request_read) - (long) (dpy->request)) <= 0)' failed. Any help is very much appreciated! Walker I've not done any wxpython in a while but what happens if you create and show the main window before you close the login window? Same thing it works every once in a while and crashes 3/4 or so of the time. Is this network code run on a separate thread or the GUI thread? You can only make wx calls on the main GUI thread. mainWindow is a local variable of handleServerSignon. This is a guess but I think it may be garbage collected as soon as the handleServerSignon method returns.  I would make your main frame appear then show a modal login dialog over top instead. If you don't want to do that I suggest you create two separate Frames and have your application listen for a close event on the login frame. Handle the login in that event handler then have it show the main window. Basically you don't want to be instantiating the main window in your event handler since once you leave the function scope is lost the the garbage collector will try to remove your frame. Lastly you should consider calling getCode() once and caching the result for your comparisons. Since your if statement and elif statement both call getCode() it very well may be yielding different results. Thanks that's definitely a much cleaner solution. Appreciate it!
473,A,File/Directory synchronization across network I need to synchronize few directories/files within the cluster. Say if a file content changes in one node I need to propagate the change to other nodes so that the file content are same atany point of time.Same applies when some files/directories are deleted. DRBD is not a option so is there any library which can do this for me. This may be better asked on ServerFault where this infrastructure setup is more common. I'd consider using rsync :) A handy tool for syncing between remote hosts. I was under impression that it is used to take backups (my bad) can it do mirroring kind of stuff in a cluster? It is usually used for backups but that does not diminish its value as a mirroring tool. I think it would work well enough. It's quite lightweight tool not comparable (in terms of power) to distributed Filesystems. Syncing more than two machines would be tricky. In that case let me give it a try. What problems do you see in the case of more than 2 machines? With the addition of a new machine you need to add it to the replication scripts on all the machines: _script maintainance is a problem_ replicating all-to-all seems a waste of time and bandwidth but also is the easiest way to use rsync. Perhaps you could describe your scenerio? Are you facing multiple machines addition over time? well number of machines will be constant and will not change dynamically. But my problem is the directory's content can change in all the nodes so if all of them call rsync then can there be a collision or corruption of data. Ok in this case rsync won't do you good. It may corrupt your data unless figuring out some clever way of syncing. But this would be too much of reinventing a wheel. Try some clustered FS. Well we ended up with rsync and we changed the design so that only one node writes to the disk and syncs it with others  Have you considered NFS? SMB? If the updates don't have to be immediate you could consider rsync  You need to use a distributed filesystem (GlusterFS comes to mind) that can guarantee the synchronization and locking depending on cluster's usage of the files. Otherwise you may want to consider a centralized storage served via NFS for the simplicity. Beyond that but still centralized would be a SAN filesystem like GFS but be aware that setup requires more to it like fencing.
474,A,network traffic of application Is it possible to measure the network traffic of applications running in iphone ? can anyone suggest me the road map or example kind of thing please ? Using proxy is that possible ? Yup you can use the Paros proxy which was built for web application security assessments. Tutorial can be found here: Sniff Your iPhone's Network Traffic. i want to monitor network traffic of iphone within iphone only
475,A,"List-Array Error? I was working on a project which required pulling down and parsing a .html page from a server then parsing it for content. I searched a string for two values as a unit test then saved each of them to a List then compared them to a manually created String[]. The code is below: SiteGrabber.java: //some imports import java.util.ArrayList; import java.util.List; import java.util.Scanner; import javax.swing.JOptionPane; public class SiteGrabber { //constructor and java.net stuff public List<String> getWords(String content){ int prev = 0; List<String> res = new ArrayList<String>(); String tar = ""<tr> <td></td><td><li>""; int tarlen = tar.length(); while(content.indexOf(tar prev) != -1){ int contentind = content.indexOf(""</li>"" prev); if(contentind != -1){ res.add( content.substring( content.indexOf(tar prev) + tarlen content.indexOf(""</li>"" content.indexOf(tar prev)))); prev = contentind + 5; } else{break;} } return res; } } SiteGrabberTest.java: import java.util.List; import org.junit.Test; import junit.framework.Assert; public class SiteGrabberTest { String htsTest=""<tr> <td>List of scrambled words:&nbsp;&nbsp;&nbsp;</td> <td><li>nielle</li></td> </tr><tr> <td></td><td><li>ierneb</li></td> </tr>""; //I want the text between the </td><td><li>...</li> tags. //2 working tests that show that it sets the List.size() to 0 on a dummy string //and that it records the right number of results in the List on a valid input. @Test public void ValidContentTest(){ SiteGrabber myGrabber = new SiteGrabber(); List<String> mylst = myGrabber.getWords(htsTest); String[] expected = new String[] {""nielle"" ""ierneb""}; Assert.assertEquals(""wrong size"" expected.length mylst.size()); for (int i = 0; i < expected.length; i++) { Assert.assertEquals(""wrong word"" expected[i] mylst.get(i)); //breaks on 1st iteration saying it expects ""nielle"" and got //"">ierneb"" implying some sort of off-by-one error. } } } And the problem is....? Please explain what your question is. Don't make people read compile and run your code just to figure out what's wrong. You forgot to tell about the error itself. You know an error/exception tells in detail about the root cause of the problem. You also know once the root cause of the problem is understood the solution is obvious :) Sorry. The problem is that JUnit fails on the third test ValidContentTest(). It returns '>ierneb' rather than nielle. I'll try to abridge some of the unneeded parts and add JUnit error messages. Change: content.indexOf(tar prev) + tarlen to: content.indexOf(tar prev) + tarlen + 1 That would make sense. I initially overcompensated with tarlen - 1 and removing the - 1 helped. Thanks!  I suspect that here... content.substring( content.indexOf(tar prev) + tarlen content.indexOf(""</li>"" content.indexOf(tar prev)))); You'll find your off-by-one problem... more specifically content.indexOf(""</li>"" content.indexOf(tar prev)) ... as content.indexOf(tar prev) returns an index (which would range from 0 to n-1) and you're trying to use it as a length (which should range from 1 to n). Sound about right? Try putting a '+1' in there... content.indexOf(""</li>"" content.indexOf(tar prev)+1) Also your technique won't work for all HTML documents. You should either use a proper HTML parsing library/tool or actually parse the HTML element by element. This was for a challenge on hackthissite.org and the data was in a predefined format but I see what you mean about needing a general parsing system. This is for one particular problem rather than a framework."
476,A,Preventing TIME_WAIT using .NET 'Async' API I have a problem I've developed a Client and Server wrapper for my personal use but unfortunately due to insufficient knowledge in network programming I have TIME_WAIT problems during connect on the client. My client tries to make multiple connections to the same host within short period of time now I have found out that the main reason for that is because I'm trying to reuse the socket and it goes to TIME_WAIT state because I'm closing the connection without graceful shutdown. I would like to know the correct pattern to close connection using .NET sockets in case I'm using 'Async' APIs intensively i.e. functions like ConnectAsync AcceptAsync SendAsync ReceiveAsync DisconnectAsync (DisconnectAsync - reuses socket) If you using tcp socket it has some default in windows see http://support.microsoft.com/kb/158474 for details and if your clients are not thin client I suggest you use WCF over tcp instead of this. I have found out that it is impossible to prevent TIME_WAIT. Either server or client will have the problem any way depending only on who initiates a closure of the connection first. If it's the client who closes the connection there will be no TIME_WAIT on server. If it's the server who closes first than there will be no TIME_WAIT on client. So the only option that is left to do is using SO_REUSEADDR but in this case it is still impossible to use the reused address for contacting previously disconnected host Excellent observation!  You can use SO_REUSEADDR on the socket to get around this. See Socket.SetSocketOption for details it's the ReuseAddress option you need to set. By the way you don't really mean reuse the socket do you? once you get an error you have to close it and open a new one. Using SO_REUSEADDR is not an option http://www.unixguide.net/network/socketfaq/4.5.shtml http://msdn.microsoft.com/en-us/library/ms740621(VS.85).aspx @Lu4 Why? I don't see anything in either link saying that it should be avoided only that the implications need to be understood. Eg it's crucial if you want to implement P2P TCP connections from behind a NAT.
477,A,"check that outgoing network is open on port 443 with vb6 in Visual Basic 6 How can i check that communication to port 443 is opened for network requests? I must NOT use any 3rd party controls / activeX / OCX so I'm looking for ways to send / receive an https or just make a connection to a server which is serving on 443 using windows (2k/2k3/xp/7/vista) API calls. There is a known server I can check against. (or for that matter https://google.com/accounts) Thanks. There are a couple of ways that I can think of. One being to use WinInet specifically the InternetOpenUrl function. Another way is to use WinHttp. You can use the function WinHttpOpenRequest. You can also use it as a COM object. Here are a few examples: http://www.ex-designz.net/apidetail.asp?api_id=72 http://msdn.microsoft.com/en-us/library/Aa384072  I've done similar for ping in the past perhaps you can modify it to use telnet to post a http get on 443. This code calls the ping command and directs the output to a file Set objShell = CreateObject(""WScript.Shell"") Call objShell.run(""%comspec% /c ping 127.0.0.1 > c:\ping.log"" 0 False) 'code to read results from ping.log here You may also want to look at the winsock control. It may already be installed on the client computers. thanks but i prefer not to rely at all on local resources. also what ping has to do with specific arbitrary ports? telnet maybe. still it's external and i can't take any risks with this prog."
478,A,"Download file through FTP by Quartz scheduler I tried working Ftp download stand alone application and it works fine. But when I included that into Quartz scheduler in web application it stucks. Here is what I did. public class FtpTransfer implements StatefulJob { public void execute(JobExecutionContext arg0) throws JobExecutionException { FTPClient ftp = new FTPClient(); FileOutputStream br = null; try { ftp.connect(""localhost""); ftp.login(""admin"" ""admin""); String path = ""alfresco/MYPUB/Admin/TMM/Pickup""; ftp.setFileType(FTPClient.BINARY_FILE_TYPE); ftp.changeWorkingDirectory(path); System.out.println(""After Changing Directory path""); FTPFile[] ftpFile = ftp.listFiles(path); System.out.println(""After getting list of files""); System.out.println(""Length : ""+ftpFile.length); System.out.println(""----------------- Downloaded -------------""); for(FTPFile tempFtpFiles : ftpFile) { br = new FileOutputStream(""e:\\Downloaded\\""+tempFtpFiles.getName()); ftp.retrieveFile(tempFtpFiles.getName() br); System.out.println(tempFtpFiles.getName()); } System.out.println(""------------------------------------------""); } catch(Exception exception) { System.out.println(""Error : ""+exception); } finally { try { if(br!=null){ br.close(); } ftp.disconnect(); } catch(IOException e) { e.printStackTrace(); System.out.println(""Error : ""+e); } } } } When I start the server It prints After Changing Directory path After Changing Directory path After Changing Directory path Every 10 secs. But It is not downloading the files from the path given. Mailnly the program didn't crossed the line FTPFile[] ftpFile = ftp.listFiles(path). What did I do wrong? Check whether you have to use active or passive ftp if a firewall/net gateway is blocking active FTP you'll simply block and perhaps eventually time out - switching to passive FTP usually works around that. Agree with nos check whether the following FTPClient methods help you try that out: http://commons.apache.org/net/api/org/apache/commons/net/ftp/FTPClient.html#enterLocalPassiveMode%28%29 Thanks for your comments. I have found the problem. After included jakarta-oro.jar in lib its working fine. Thanks kelly vista and nos."
479,A,"Copying information from a server We have a server with a folder of local users on the C: drive. How would i go about copying the folder from that server to my local machine. Any help at all will be appreciated. i haven""t done this kind of programming before so i have no idea where to start Thanks If the machine the program is running on has access to that share it's exactly the same as copying from your local folder except changing the path. So let's for argument's sake say you want to copy from \Server\Users (where Server is the servername users is the share name) to your local c:\userShare folder then it's as simple as: File.Copy(@""\\Server\Users""@""c:\userShare""); You will obviously still need to implement the logic in terms of getting all the files in the directory etc see here for an example. If you need to give access to the machine first have a look here for more info.  Use a UNC path (\\server\volume\directory\file) to server location and use File.Copy Method How to: Copy Delete and Move Files and Folders (C# Programming Guide) thanks it works well on my local machine but if i try to access this server: @""\\10.4.46.219\C:\Users\Public\TestFolder""; it gives me a not supported exception. try @""\\10.4.46.219\C$\Users\Public\TestFolder"" assuming you have rights. You might need to create an explicit share I've tried TBohnen.jnr option but it doesn't seem to be working do you perhaps know how to insert the credentials?"
480,A,"What have you used to test (functional/load/stress) your network service with its custom protocol? I recently created a turn-based game server that can accept 10s of thousands of simultaneous client connections (long story short - epoll on Linux). Communication is based on a simple custom line-based protocol. This server allows clients to connect seek for other players in game matches play said games (send moves chat messages etc.) and be notified when the game has ended. What I'm looking to do now is test the server by simulating client connections. I'm hoping to support 10s of thousands of simultaneous connections so this testing is very important to me. What do you guys use for your own testing? Some things I'm researching now are: pexpect (python expect lib for the functional testing) and tsung for load testing. I'd like to be able to just test from my laptop since I do not have a cluster of client machines to connect from. Perhaps I'd need to use ip aliasing or some-such in order to generate 100s of thousands of outbound sockets (limit is 65K per interface AFAIK). Anyway it seems to me like I need something fairly custom but I thought I'd ask before I went down that path. Thanks! We are using HP LoadRunner it's the state of the art load testing product. (But also an expensive one). It can simulate thousands of requests to the server and provides metrics on response time etc.. $$$ How is it? Their pen testing looks good too  I decided it was best to ""roll my own"" to start with.  I've used JMeter with custom sampler and assertion components before to do automated regression/load testing for a banking application with a custom protocol (Java RMI based API). It's not exactly lightweight though and you'll end up doing a lot of extra coding in the JMeter components to support your custom protocol. I'm guessing you'd have to code your own Java socket based client in this case. But it gives you a lot of flexibility in defining the logic for testing the components so you can do whatever you want inside there. It scales nicely as well and allows you to throw a lot of concurrent connections at the system under test."
481,A,Python: List of addressable IP addresses What's the most Pythonic way to create a list of the addressable IP addresses given a netaddr IPRange or netaddr IPNetwork. If I use these then it includes subnet and broadcast addresses: hosts = list(IPRange('212.55.64.0''212.55.127.255')) hosts = IPNetwork('192.168.0.1/24') So what I need for say IPNetwork(192.168.0.0/27) is a list from 192.168.0.1 to 192.168.0.31 note that 192.168.0.0 and 192.168.0.32 must not be included. EDIT Thanks for info on how to do it with IPy. Does anybody know if it can be done with netaddr? After doing a bit of research I found this way:  l = list(netaddr.IPNetwork('192.168.0.0/27').iter_hosts())  The following is a quick script to do what you want using netaddr (Python 2.7 linux) from netaddr import * def addr(address prefix): ip = IPNetwork(address) ip.prefixlen = int(prefix) return ip myip = addr('192.168.0.0' '27') for usable in myip.iter_hosts(): print '%s' % usable  IPy Could you give an example please? From the linked page: `IP('127.0.0.0/8').net() IP('127.0.0.0-127.255.255.255').broadcast()` Thanks for that.
482,A,"Java: Proper Network IO handling? The problem I am having is that when I use an InputStream to read bytes it blocks until the connection is finished. EG:  InputStream is = socket.getInputStream(); byte[] buffer = new byte[20000]; while (is.read(buffer) != -1) { System.out.println(""reading""); } System.out.println(""socket read""); ""socket read"" doesn't print out until the FYN packet is actually recieved thus closing the connection. What is the proper way to receive all the bytes in without blocking and waiting for the connection to drop? the code is probably not doing what you think it does. read(buffer) returns the number of bytes it read in other words: it is not guaranties to fill up your buffer anyway. See DataInputStream.readFully() for code that fill up the entire array: or you can use this functions (which are based on DataInputStream.readFully()) : public final void readFully(InputStream in byte b[]) throws IOException { readFully(in b 0 b.length); } public final void readFully(InputStream in byte b[] int off int len) throws IOException { if (len < 0) throw new IndexOutOfBoundsException(); int n = 0; while (n < len) { int count = in.read(b off + n len - n); if (count < 0) throw new EOFException(); n += count; } } Your code would look like: InputStream is = socket.getInputStream(); byte[] buffer = new byte[20000]; readFully(is buffer); System.out.println(""socket read"");  Reading till you get -1 means that you want to read until EOS. If you don't want to read until EOS don't loop till the -1: stop sooner. The question is 'when?' If you want to read a complete 'message' and no more you must send the message in such a way that the reader can find its end: for example a type-length-value protocol or more simply a size word before each message or a self-describing protocol such as XML.  Take a look at java.nio which has non-blocking IO support. this sounds like a major overkill at this point. I think he should first master blocking io before even considering non blocking io.  Try this:  InputStream is = socket.getInputStream(); byte[] buffer = new byte[20000]; int bytesRead; do { System.out.println(""reading""); bytesRead = is.read(buffer); } while (is.available() > 0 && bytesRead != -1); System.out.println(""socket read""); More info: http://java.sun.com/j2se/1.4.2/docs/api/java/io/InputStream.html#available() Good answer welcome to SO. when you figured out it does not always work revisit my answer. Shouldn't the call to `is.available()` happen before the `is.read()` call considering that `is.read()` might block if no bytes are available on the first read? I do want it to block until bytes are available the first go round. I find this to be most optimized since it doesn't consume CPU resources and it doesn't poll. Also this code works pretty fine for me tested it this morning. If I run into problems I will update.  Example taken from exampledepot on java.nio // Create a direct buffer to get bytes from socket. // Direct buffers should be long-lived and be reused as much as possible. ByteBuffer buf = ByteBuffer.allocateDirect(1024); try { // Clear the buffer and read bytes from socket buf.clear(); int numBytesRead = socketChannel.read(buf); if (numBytesRead == -1) { // No more bytes can be read from the channel socketChannel.close(); } else { // To read the bytes flip the buffer buf.flip(); // Read the bytes from the buffer ...; // see Getting Bytes from a ByteBuffer } } catch (IOException e) { // Connection may have been closed } Be sure to understand buffer flipping because it causes a lot of headache. Basically you have to reverse your buffer to read from it. If you are to reuse that buffer to have the socket to write in it you have to flip it again. However clear() resets the buffer direction. Select on OP_CONNECT and check finishConnect() when you get it or else just connect in blocking mode. Follow-up: is there a way to get blocking in this? I want to block until it connects but using while(!sChannel.finishedConnect()) costs a lot of cpu cycles.  With traditional sockets the point is that usually you do want them to block: what you do when logically you don't want your program to block is you put your reading/writing code in another thread so that the separate read/write thread blocks but not your whole program. Failing that you can use the available() method to see if there is actually any input available before reading. But then you need to be careful not to sit in a loop burning CPU by constantly calling available(). Edit: if the problem is that you're happy to block until the bytes have arrived but not until the connection has dropped (and that is what is happeningh) then you need to make the client at the other end call flush() on its output stream after it has sent the bytes. Sorry I do in fact want it to block the first go-round read the bytes and my problem was my unfamiliarity with java IO since I was calling read again then I had to wait for EOS. Yes correct about that last point burning CPU. Is there an opitmized way that doesn't hurt me with cpu cycles or context switching (Does Thread.sleep(long) + polling cause a bad performance lost? Calling Thread.sleep() will cause some performence loss for the actual I/O (e.g. you could sleep for 500 millis but the data is ready after 100 millis and you don't receive it for another 400). In terms of context switching a few extra C/S per second don't really matter. But I'm still not clear: why can't you just let the socket block (in its own thread if necessary)?"
483,A,OMNet++: What are the differens between ethg and pppg gates? I have searched OMNet++ manual but i couldn't found any information about gates. What are the differences between ethg and pppg? ethg is Ethernet and pppg is PPP The difference of these two layer2 protocols you can look up on wikipedia. E.g. they will have different headers different MTUs ... Ethernet is used in LANs. PPP is used for the connection to the provider.
484,A,"How to simulate a dial-up connection for testing purposes? I have to code a server app where clients open a TCP/IP socket send some data and close the connection. The data packets are small < 100 bytes however there is talk of having them batch their transactions and send multiple packets. How can I best simulate a dial-up ut connection (using Delphi & Indy components just FYI)? Is it as simple as open connection wait a while (what is the definition of ""a while""?) close connection Are you after a way to test how your application would work over a dial-up line or are you trying to automate the dial-up connection? If you want to automate the dialing-up and hanging-up part you'll need to look into RAS it's not an Indy thing. What exactly do you want? Do you want to simulate a slow networkconnection like dail-up or do you want to simulate multiple clients connecting to your server? You can use a network emulator. If you have a Linux machine on hand with NISTnet (an old project but still useful used some years ago) you can create multiple scnearios limiting bandwidth or other characteristics to the network communication. There is an application called WANEM which seems to do the same but I have not used it so cannot tell you if is good or not. If you want a modem emulation the you can use com0com which provides exactly that. Hi Rob it's not homework (it's been a few decades since I had any of that :-) I have to develop a tester for someone sending specific data and checking the replies. Some of their users still use dial-up. Wanem does look great though. Thanks for the tip (+1). well I cna't use a n/w emulator as I have to code an app bu tmaybe I can reuse some of their code. I will look into it & get back to you thanks. Why can't you use an network emulator? It has nothing to do with how you programm your app it's just something that goes between your ""Client"" application and your ""Server"" application. You use it to make your fast office network slow (for an particular port number). I used an other Linux slow-buggy-connection-emulator (trickle) to test some of my networked applications. Mawg for testing your program in the face of slower or flakier network connections Wanem is perfect; I have some co-workers who've used it. Why do *you* have to code the app? Shouldn't it be enough that you *have* such an app at your disposal? Shouldn't it be *better* in fact that you got such an app *without* spending the time and money of coding it yourself? Or is this a homework assignment? (If it's homework you should have gotten a better description of the requirements such as the definition of ""a while."")"
485,A,"Able to ping but cannot browse after several hours running of my python program It's a GUI program I wrote in python checking website/server status running on my XP SP3 multi threads are used to check different site/server. After several hours running the program starts to get urlopen error timed out all the time and this always happens right after a POST request from a server(not a certain one might be A or B or C) and it's also not the first POST request causing the problem normally after several hours running and it happens to make a POST request at an unknown moment all you get from then on is urlopen error timed out. I'm still able to ping but cannot browse any site once the program closed everything's fine. It's definitely the program causing this problem well I just don't know how to debug/check what the problem is also don't know if it's from OS side or my program wasting too many resources/connections(are you still able to ping when too many connections used?) would anybody please help me out? As others have suggested regarding your other related questions: try your script on another computer running either the same or different OS (or a version of it). Are you sure you are closing TCP sessions after each request? Try to check netstat information from time to time and if you'll see that the number of active/established sessions is rising it means that you have some problems in your script. Yes usually you can ping even if you are out of free TCP sockets. Thanks a lot man! I though `del` would do the connection close job but it really does not. Using netstat I found tons of ""Close_Wait"" connections wasting all the resources. After changing the code to `.close()` everything is OK now. @Shane: I'm glad you've found your problem. I would suggest to use Wireshark network analyzer to figure out network problems also. Good luck ;)"
486,A,"TCP Echo server that only echoes first packet (C Language) I wrote some code in C for a TCP Server that echoes whatever it gets. The problem is when I send data first time it echoes it and next times the server sends back the first packet I sent. The log looks like: Client Send : Packet1 Server reply : Packet1 Client Send : Packet2 server reply : Packet1 The server code is as follows: int main(int argc char** argv) { int listenfdconnfd; pid_t childpid; socklen_t clilen; struct sockaddr_in servaddrcliaddr; listenfd = socket(AF_INETSOCK_STREAM0); printf(""Socket listenfd : %d with %d And %d\n""listenfdAF_INETSOCK_STREAM); bzero(&servaddr sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); servaddr.sin_port = htons(SERV_PORT); printf(""Server address: %d\n""servaddr.sin_addr.s_addr); bind(listenfd (SA*) &servaddr sizeof(servaddr)); printf(""Listened: %d\n""listenfd); listen(listenfdLISTENQ); printf(""After Listening: %d\n""listenfd); int num=0; for( ; ; ){ clilen=sizeof(cliaddr); connfd = accept(listenfd (SA*) &cliaddr&clilen); printf(""Client no. %d connected\n""++num); if( (childpid=fork())==0){ close(listenfd); echo(connfd); exit(0); printf(""Client no. %d Terminated\n""++num); } close(connfd); } return (EXIT_SUCCESS); } And my echo function: void echo(int sockfd) { ssize_t n; char buf[MAXLINE]; again: while ( (n = read(sockfd buf MAXLINE)) > 0) writen(sockfd buf n); if (n < 0 && errno == EINTR) goto again; else if (n < 0) printf(""read error""); } the client code main : int main(int argc char** argv) { int sockfd; struct sockaddr_in servaddr; sockfd= socket(AF_INETSOCK_STREAM0); bzero(&servaddrsizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port= htons(SERV_PORT); inet_pton(AF_INET""0.0.0.0""&servaddr.sin_addr); printf(""%d  %d \n""sockfdservaddr.sin_addr.s_addr); connect(sockfd (SA*) &servaddrsizeof(servaddr)); printf(""%d\n""sockfd); replyBack(stdinsockfd); printf(""RETURN\n""); return (EXIT_SUCCESS); } the replyBack function: void replyBack(FILE *fp int sockfd) { char sendline[MAXLINE] recvline[MAXLINE]; printf(""ENTER YOUR ECHOED: \n""); while (fgets(sendline MAXLINE stdin) != NULL) { write(sockfd sendline sizeof(sendline)); if (read(sockfd recvline MAXLINE) == 0) { printf(""str_cli: server terminated prematurely""); exit(-1); } fputs(recvline stdout); } } Your client and server work perfectly fine for me. Check to make sure that none of the system calls you're making (`read` `write` `connect` etc.) are returning errors or smaller-than-expected values. Try using Wireshark to see what's actually getting sent across the network which will tell you if the problem is on the client or the server. Try using `netcat` as a replacement client. THanks :) i will try to use them Well let's look at this section of your code: printf(""After Listening: %d\n""listenfd); int num=0; for( ; ; ){ clilen=sizeof(cliaddr); connfd = accept(listenfd (SA*) &cliaddr&clilen); printf(""Client no. %d connected\n""++num); if( (childpid=fork())==0){ close(listenfd); echo(connfd); exit(0); printf(""Client no. %d Terminated\n""++num); } close(connfd); } Calling exit exits your application so the printf following it will never get executed. Minor but worth pointing out. Also in the ""child"" process you should not close your listening socket. The only socket it should work with is the client connection so you should have something more along the lines of: if ( (childpid = fork ()) == 0 ) { echo ( connfd ); close ( connfd ); printf ( ""Client no %d terminated.\n"" num ); /* Don't use the ++ here or your count will be off */ exit ( 0 ); } Now let's look at your echo code: void echo(int sockfd) { ssize_t n; char buf[MAXLINE]; again: while ( (n = read(sockfd buf MAXLINE)) > 0) writen(sockfd buf n); if (n < 0 && errno == EINTR) goto again; else if (n < 0) printf(""read error""); } Have to remember that the calls to read and write may block (since I don't see you setting the socket to non-blocking IO) and write may not send the entire buffer when called so you need to check a few more things here. void echo ( int sockfd ) { ssize_t bytes_in bytes_out bytes_remaining; int write_err; char buf[MAXLINE]; char * send_start_pos; while ( 1 ) { bytes_in = read ( sockfd buf MAXLINE ); if ( bytes_in < 1 ) { if ( errno == EINTR ) continue; break; /* other error occurred or EOF (0 bytes read) */ } bytes_remaining = bytes_in; send_start_pos = buf; write_err = 0; while ( ( bytes_remaining > 0 ) && !( write_err ) ) { bytes_out = write ( sockfd send_start_pos bytes_remaining ); if ( bytes_out < 0 ) { if ( errno == EINTR ) continue; write_err = 1; break; } bytes_remaining -= bytes_out; send_start_pos += bytes_out; } if ( write_err ) break; } } Once your echo function exits the socket will be closed back in the calling function. Generally I would suggest closing the socket in the echo function unless you need it afterwards. I almost certainly would suggest closing it when an error occurs but again that is up to you. Just as an aside stay away from goto ... it has its purposes but for the most part well-written code rarely ever uses it. it worked :) thanks very much for the solution and ur precious advices :D Another minor remark: you should also check to see if `fork()` returns -1 which indicates an error. @Adam: most definitely. good practice is to check for -1 0 and anything else; was not focusing on that so I missed that tweak.  The biggest problem is that you're treating TCP as if it were a datagram protocol which it isn't. It is a stream protocol. I am yet to put my finger on what exactly is going on but right now my money is on the client printing the message it already has in its buffer (as opposed to getting it the second time). Show us the client code. Edit There are multiple things that are wrong. First of all a single call to write may require multiple calls to read on the other end. Secondly since TCP is a stream protocol you need to make sure that for every logical message the receiving side knows how many bytes to expect. You either need to stick to fixed-length messages or prefix each message with its length. You're kind of doing the former by always sending MAXLINE bytes but not quite consistently enough (for example writen(sockfd buf n) writes n bytes back and n can be different from MAXLINE). Another edit to address a point made in the comments. Telnet is a bad analogy: in telnet there is a stream of characters flowing one way and a stream of characters flowing the other way. In your protocol you're sending discrete multibyte messages (or datagrams). This is precisely the reason why you're finding a datagram protocol (UDP) easier to apply to your problem. You can use telnet as a client..but ok i will edit the post and add the client There i added the client.. Doesn't matter if you're using telnet as a client it's still a stream."
487,A,Processing all Packets generated by a browser I want to write a program which Controls all the web browsing activities on PC. i.e. Checking all the websites users go to filtering some of them ... . But I have no idea how to capture all the packets processing them and even act to some (think of filtering unwanted sites). Any help sample code open source program...? do you care about language we will choose for you? Or OS? Or anything? not about language but I prefer C or C#. Windows is the OS. There are different levels you can put yourself in the middle of the communication: By implementing a proxy and having the browser connect to the proxy By implementing a firewall/snooper and handling the raw packets By implementing a network driver and handling the raw packets IMHO number 1 is easiest. Look at SQUID for an example. Number 2 is doable too take a look at fiddler. You could take a look at the Click Modular Router for option number 3. Depending on the browser maybe a simple browser plugin could do? Thanks. I want it to be Browser independent. is Fiddler open source by any chance? @MBZ: Nope but wireshark is you can download its sourcecode. However if you don't want to edit the browser (and thus no proxy) my personal experience would advise you to go with the Click Modular Router.
488,A,"How to access File over the Network I am having a hard time on this one I have a folder over the network with public access (no credential restriction). I am trying to do a File.Exist or Directory.Exist and I keep on having a exception. Can someone tell me the good way to do IO over the network. EDIT 1 FOR DETAILS: if i do execture => \agoodip\Public\test.txt I get the file etc etc In my code it look like a basic Directory.Exist(@""\\agoodip\Public"") or File.exist(@""\\agoodip\Public\test.txt"") The exception I get is Path not found. EDIT 2 : I am using Silverlight 3 Is there any security pattern to be aware of to lookup file on the network? Thanks! Try looking at the actual filesystem calls being made with Process Monitor - it will tell you what the actual file and error is. Maybe your application is not accessing the file you think it is accessing. could you please paste the code you are executing + the exception details? I don't believe that is going to work for you. Silverlight doesn't allow arbitrary access to the file system or shares. Silverlight runs within a sandbox environment so you have restricted access to the file system. I dont try to lookup file system... I juste want to see the picture on my server and be able to upload by the LAN Network not by ftp or other...  You need to begin your UNC path with two backslashes (""\\"") if it refers to a network path rather than a local path. it is already with ""\\"" i edit my post"
489,A,"IPAddress.Any Fail ). I got into a bit of issues today with the TcpListener. Things are strange. Initially I used the new TcpListener(port) constructor but that has been marked as obsolete. So I dropped it and used this instead:  IPAddress ipAddress = Dns.GetHostEntry(Dns.GetHostName()).AddressList[0]; IPEndPoint ipLocalEndPoint = new IPEndPoint(ipAddress ServerPort); TcpListener tcpServer = new TcpListener(ipLocalEndPoint); _TCPClient = tcpServer.AcceptTcpClient(); GotClient(); I do that in a thread of course so that it doesn't lock the application. Now what happens there is that even though the ipAddress is correct the server NEVER accepts ANY incoming connection. HOWEVER changing to new IPEndPoint(IPAddress.Any ServerPort) seems to do the trick! Which is silly in 2 ways: 2 hours ago IPAddress.Any returned 192.168.1.102 which is my correct local IP. This is the same IP which was in ipAddress! But with ipAddress it didn't work while with IPAddress.Any it worked (that is it successfully accepted connections from my client). Right now: IPAddress.Any returns 0.0.0.0 (!?) while the ipAddress variable continues to be assigned my correct IP (192.168.1.102). The result? My client still cannot connect if using ipAddres but connects when using IPAddress.Any even though it is 0.0.0.0. I'm totally puzzled by this... Any thoughts? I currently have this in Form_HandleCreated but it was acting weird when I had it in the Form's constructor as well. LATER EDIT: I think I'm wrong about IPAddress.Any returning 192.168.1.102. I probably printed out something else as many of you have indicated 0.0.0.0 is what .Any should return. Sorry ::- D. I'm guessing it always returned 0.0.0.0... @Axonn: I don't think that's something you have to appologise for here. I believe we've ALL done it at least once :-) I later-edited-it ::- ). But this website is too fast *laugh*. Never ceases to amaze me. Really great community we got here. I do not answer questions but I do what I can to give back the community: free software & articles on websites ::- ). I might have made a mistake on that. Perhaps printing to the console some other variable (like ipAddress). Sorry if I did so.  0.0.0.0 is blank IP address meaning ""I don't care which IP you would use"" - i.e. it means you want to listen on all interfaces. It is documented it should return it. To later edit: First of all dont call [0]. Your hostname may not have any record associated. Even if it do you a) don't know the order b) you don't know where they are (i.e. it may be only 127.0.0.1 i.e. loopback). On mono such code work (checked with netstat etc.): using System; using System.Net; using System.Net.Sockets; public class test { public static void Main(String[] args) { IPAddress ipAddress = Dns.GetHostEntry(Dns.GetHostName()).AddressList[0]; IPEndPoint ipLocalEndPoint = new IPEndPoint(ipAddress 8888); TcpListener tcpServer = new TcpListener(ipLocalEndPoint); tcpServer.Start(); // Without this line there is exception var _TCPClient = tcpServer.AcceptTcpClient(); } } I responded to ""later edit"". Possibly as works-for-me but that at least some answer. Thank you Maciej ::- ). It may be related to the emulator - I'm connecting through it not via a physical device. However in the emulator I try to connect to 192.168.1.102 so I don't understand why it doesn't actually do that. Might be some NAT stuff as Mark already suggested. I have tried looking at what ports are being used but Wireshark doesn't appear to be working for some reason. I just moved my application to my HTC Tytn2 and now it works. That one connects to 192.168.1.102 so probably this was the answer: the emulator goes through some other address. Hi and thanks for answering ::- ). So that's why that works... I get it. Hm the issue remains... how to make it work on .1.102. I don't want to listen to all interfaces.  IPAddress.Any should return 0.0.0.0 that is normal. Are you actually sure it returned something else initially? As for why you couldn't connect before if you were listening on 192.168.1.102 and tried to connect to 127.0.0.1 (localhost) then it wouldn't work. You need to listen on 127.0.0.1 if you want to connect to that IP address. Essentially you must listen on the IP address you are attempting to connect to. Listening on 0.0.0.0 means ""listen on all available IP addresses"". Remember 127.0.0.1 (localhost) is not a synonym for ""my local network IP"". I just moved my application to my HTC Tytn2 and now it works. That one connects to 192.168.1.102 so probably this was the answer: the emulator goes through some other address. Hi & thanks for answering ::- D. Yeah I think I made a mistake with .Any and printed in the console ipAddress instead. Anyway I am trying to connect to 192.168.1.102 from the Device Emulator (from a .Net CF application). So I am not trying 127.0.0.1. Although... the thought comes to mind... what if the Emulator itself goes through that? Although in the socket on .Net Compact Framework I specifically asked for 192.168.1.102. If you're trying to connect from the emulator to the host machine I don't know what the IP address would look like... there might be some NAT in there I couldn't say. Also it strikes me to ask if you've set up networking between the emulator and the host machine. I seem to remember there are various options in the emulator for how networking works. Also verify your firewall settings it may be blocked depending on how you've got things set up. As I said the emulator successfully connects if I use 0.0.0.0. So no firewall issues. I'm gonna use a port sniffer to see exactly where / how things go."
490,A,Need help in understanding the mapping of user-space send sendto sendmsg to kernel-space sendmsg I am trying to implement my own transport layer protocol in Linux for an experiment. I am going to use socket interface and add my protocol using sock_register. For the proto_ops i can see that the parameters for the sendmsg and recvmsg are (struct kiocb *iocb struct socket *sock struct msghdr *msg size_t len int flags). But there are three types of user api's send sendto sendmsg. Of these three only sendmsg contains a parameter for msghdr. I find that the other two api's are incompatible with the parameters supplied by the kernel to my kernel-space sendmsg function. So what happens when we use send and sendto user-space api's? Hope i am clear.. Thanks Bala Assuming you are comfortable with the system call mechanism - start from net/socket.c and follow the call chains - it's more or less clear.  send() is implemented in terms of sendto(): send(s buf len flags); is equivalent to sendto(s buf len flags NULL 0); sendto() is in turn implemented in terms of sendmsg(). send(s buf len flags addr addr_len); is implemented with (in terms of the userspace interface): struct iovec iov = { .iov_base = buf .iov_len = len }; struct msghdr msg = { .msg_name = addr .msg_namelen = addr_len .msg_iov = &iov .msg_iovlen = 1 .msg_control = NULL .msg_controllen = 0 }; return sendmsg(s &msg flags); The kernelspace interface is slightly different - eg. you get the kiocb parameter - but the basic idea is the same. A send() or sendto() is converted into a sendmsg() with a msghdr that points at a single iovec that references the buffer.
491,A,"How to put NIC into promiscuous mode? Hi I want to put my NIC into promiscuous mode. Why? Because I wrote app which is able potentially to detect sniffing in my local network (send modyfied appropriately ethernet packets). I just want now check it out so in my second comp I want to set up promisc mode. There's Windows 7 and simply Dell Wireless 1397 WLAN Mini-Card...if it makes a difference. How can I do this? You might not be able to put that adapter into promiscuous mode. This is Windows and the adapter is a Wi-Fi adapter and according to this Microsoft documentation on 802.11 drivers on Windows ""It is only valid for the miniport driver to enable the NDIS_PACKET_TYPE_PROMISCUOUS NDIS_PACKET_TYPE_802_11_PROMISCUOUS_MGMT or NDIS_PACKET_TYPE_802_11_PROMISCUOUS_CTRL packet filters if the driver is operating in Network Monitor (NetMon) or Extensible Access Point (AP) modes."" If your application uses WinPcap (as does for example Wireshark) it can't put the driver into ""network monitor"" mode as WinPcap currently doesn't support that (because its kernel driver doesn't support version 6 of the NDIS interface for network drivers) so drivers that follow Microsoft's recommendations won't allow you to put the interface into promiscuous mode. And if it could put it into monitor mode that might disable transmitting packets; according to this Microsoft page on monitor mode ""While in NetMon mode the miniport driver can only receive packets based on the current packet filter settings. The driver cannot send packets either on its own or through a call to its MiniportSendNetBufferLists function.""  I`ve used wireshark for this sort of thing in the past IP is a layer above MAC (see layer 3 and 2 of OSI model respectively) - in the end if you have the wrong MAC the data link layer will not pass it up to the network layer. ICMP is at the network layer so it never received your ICMP request. I misinterpreted the question wireshark is the way to go Yes I tried this but sth is wrong. I've checked options ""Capture packets in promiscuous mode"" on laptop and then I send from PC modified ICMP Request (to correct IP but incorrect MAC address). This is one of the methods of detection sniffing in local network. As far as I know if NIC is in promisc mode it should send ICMP Reply. But there's no reply. Any ideas? No I're wrong. You're talking about ""normally"" situation but if NIC is in promisc mode it should send answer. Read about Ping test e.g http://www.napolifirewall.com/Sniffers.htm or http://www.linux-sec.net/Sniffer.Detectors/snifferdetection.pdf But if so then I don't know why I don't get this answer on my PC I don't know why wireshark works on my PC but not on notebook. If notebook is listning and sending request - PC is answering but vice versa don't work. Any ideas?"
492,A,"Odd performance with C# Asynchronous server socket I'm working on a web server in C# and I have it running on Asynchronous socket calls. The weird thing is that for some reason when you start loading pages the 3rd request is where the browser won't connect. It just keeps saying ""Connecting..."" and doesn't ever stop. If I hit stop. and then refresh it will load again but if I try another time after that it does the thing where it doesn't load again. And it continues in that cycle. I'm not really sure what is making it do that. The code is kind of hacked together from a couple of examples and some old code I had. Any miscellaneous tips would be helpful as well. Heres my little Listener class that handles everything (pastied here. thought it might be easier to read this way) using System; using System.Collections.Generic; using System.Net; using System.Net.Sockets; using System.Text; using System.Threading; using irek.Request; using irek.Configuration; namespace irek.Server { public class Listener { private int port; private Socket server; private Byte[] data = new Byte[2048]; static ManualResetEvent allDone = new ManualResetEvent(false); public Config config; public Listener(Config cfg) { port = int.Parse(cfg.Get(""port"")); config = cfg; ServicePointManager.DefaultConnectionLimit = 20; } public void Run() { server = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); IPEndPoint iep = new IPEndPoint(IPAddress.Any port); server.Bind(iep); Console.WriteLine(""Server Initialized.""); server.Listen(5); Console.WriteLine(""Listening...""); while (true) { allDone.Reset(); server.BeginAccept(new AsyncCallback(AcceptCon) server); allDone.WaitOne(); } } private void AcceptCon(IAsyncResult iar) { allDone.Set(); Socket s = (Socket)iar.AsyncState; Socket s2 = s.EndAccept(iar); SocketStateObject state = new SocketStateObject(); state.workSocket = s2; s2.BeginReceive(state.buffer 0 SocketStateObject.BUFFER_SIZE 0 new AsyncCallback(Read) state); } private void Read(IAsyncResult iar) { try { SocketStateObject state = (SocketStateObject)iar.AsyncState; Socket s = state.workSocket; int read = s.EndReceive(iar); if (read > 0) { state.sb.Append(Encoding.ASCII.GetString(state.buffer 0 read)); SocketStateObject nextState = new SocketStateObject(); nextState.workSocket = s; s.BeginReceive(state.buffer 0 SocketStateObject.BUFFER_SIZE 0 new AsyncCallback(Read) nextState); } if (state.sb.Length > 1) { string requestString = state.sb.ToString(); // HANDLE REQUEST HERE byte[] answer = RequestHandler.Handle(requestString ref config); // Temporary response /* string resp = ""<h1>It Works!</h1>""; string head = ""HTTP/1.1 200 OK\r\nContent-Type: text/html;\r\nServer: irek\r\nContent-Length:""+resp.Length+""\r\n\r\n""; byte[] answer = Encoding.ASCII.GetBytes(head+resp); // end temp. */ state.workSocket.BeginSend(answer 0 answer.Length SocketFlags.None new AsyncCallback(Send) s); } } catch (Exception e) { Console.WriteLine(e.Message); Console.WriteLine(e.StackTrace); return; } } private void Send(IAsyncResult iar) { try { SocketStateObject state = (SocketStateObject)iar.AsyncState; int sent = state.workSocket.EndSend(iar); state.workSocket.Shutdown(SocketShutdown.Both); state.workSocket.Close(); } catch (Exception) { } return; } } } And my SocketStateObject: public class SocketStateObject { public Socket workSocket = null; public const int BUFFER_SIZE = 1024; public byte[] buffer = new byte[BUFFER_SIZE]; public StringBuilder sb = new StringBuilder(); } ** EDIT ** I have updated the code with some suggestions from Chris Taylor. Check my response for additional updates. I notice you also have an object type mismatch in the BeginSend/Send pair. You should also note that there is a race condition in your code. In Run() you wait for allDone before calling BeginAccept again: while (true) { allDone.Reset(); server.BeginAccept(new AsyncCallback(AcceptCon) server); allDone.WaitOne(); // <------ } This is fine however in your AcceptConn callback the event is set at the top of the method: private void AcceptCon(IAsyncResult iar) { allDone.Set(); // <------ Socket s = (Socket)iar.AsyncState; Socket s2 = s.EndAccept(iar); SocketStateObject state = new SocketStateObject(); state.workSocket = s2; s2.BeginReceive(state.buffer 0 SocketStateObject.BUFFER_SIZE 0 new AsyncCallback(Read) state); } The callback will executed by a random thread from the pool but allDone will be set before anything is actually done. It's entirely possible for your Run() loop to run again in the first thread before the work in AcceptCon actually completes. This will cause you big problems. You should set allDone after you've performed your initialization (and especially after you've accessed any non-threadsafe class members) like so: private void AcceptCon(IAsyncResult iar) { Socket s = (Socket)iar.AsyncState; Socket s2 = s.EndAccept(iar); SocketStateObject state = new SocketStateObject(); state.workSocket = s2; allDone.Set(); // <------ s2.BeginReceive(state.buffer 0 SocketStateObject.BUFFER_SIZE 0 new AsyncCallback(Read) state); }  Just looking at the code quickly I suspect that you might stop enquing your AsyncReads because s.Available is returning 0 I am refering to the following code if (read > 0) { state.sb.Append(Encoding.ASCII.GetString(state.buffer 0 read)); if (s.Available > 0) { s.BeginReceive(state.buffer 0 SocketStateObject.BUFFER_SIZE 0 new AsyncCallback(Read) state); return; } } To confirm change the above to the following if (read > 0) { state.sb.Append(Encoding.ASCII.GetString(state.buffer 0 read)); SocketStateObject nextState = new SocketStateObject(); nextState.workSocket = s; s.BeginReceive(state.buffer 0 SocketStateObject.BUFFER_SIZE 0 new AsyncCallback(Read) nextState); } This is not the complete correction of the code but it will confirm if this is the problem. You need to make sure that you are closing your sockets correctly etc. Update I also noticed that you are sending the socket in as the state in the call to BeginSend.  state.workSocket.BeginSend(answer 0 answer.Length SocketFlags.None new AsyncCallback(Send) state.workSocket); However your callback Send is casting the AsyncState to SocketStateObject SocketStateObject state = (SocketStateObject)iar.AsyncState; This will be raising InvalidCastExceptions which you are just hiding by adding the empty catch. I am sure others will agree this is exceptionally bad practice having empty catches it hides so much info that you could be using to debug your problem. No unfortunately it didn't change. So I don't think this was my problem either. But this may have prevented other bugs :P @The.Anti.9 that is strange. I just copied your original code exactly and could replicate your problem. I then added the change I suggested and from that point the problem when away. Maybe you can post the updated code? ok I updated it. I just copied and pasted the code you put over top of mine so I don't think i made any mistakes. though I could have. I have been working on other parts of the app so now i think we need the `return` cause if not it goes on and tries to parse an empty request. and that throws an exception. ah HA! that was it it was that exception. I need to be more careful with my error handling.  Completely random guess: http://msdn.microsoft.com/en-us/library/system.net.servicepointmanager.defaultconnectionlimit.aspx The maximum number of concurrent connections allowed by a ServicePoint object. The default value is 2. This was a good thought but not it. The connections I'm trying aren't concurrent anyways. They are separate one after another. Though I did think that for some reason the connections weren't getting closed so it made sense. But alas not the case :/"
493,A,"how to send data with twisted protocol via factory I writing client implementing custom protocol and have factory for it. My problem is following: my client has bi-dir communication and sometime I want to tell it ""send this data"". But all I have object is factory: class MyFactory(ClientFactory): protocol = MyProtocol def __init__(self recv_callback): self.recv_callback = recv_callback def send_message(self msg): self.protocol.send_message(msg) So I create factory and have factory object I have no protocol object. When send_message above called I get error because self.protocol just a class not object. How can I do this? Should also expose the protocol for connection also in addition to factory? Thanks protocol = MyProtocol() @Dhaivat Pandya: you don't know twisted i see. please don't answer then can't you just use the constructor to make an object? @Dhaivat Pandya: i don't understand You have access to all of the objects you want. The factory is responsible for creating protocol instances so if you want to keep the protocol instance around where the factory can use it override buildProtocol and save the instance: class MyFactory(ClientFactory): protocol = MyProtocol ... def buildProtocol(self address): proto = ClientFactory.buildProtocol(self address) self.connectedProtocol = proto return proto However this approach is lacking in one important feature. It does not make it easy to tell when buildProtocol has been called and connectedProtocol has been set. If you try to use this attribute naively: factory = MyFactory() reactor.connectTCP(host port factory) factory.connectedProtocol.send_message(...) The code will fail with an AttributeError because the connection has not yet actually been set up. Since Twisted is event driven you need to make sure to use this code by responding to an event that says the connection has been set up. You might do this by firing a callback when the protocol is constructed instead of just setting an attribute. Twisted actually has a helper factory which does something like this already: from twisted.internet.protocol import ClientCreator cc = ClientCreator(reactor MyProtocol) whenConnected = cc.connectTCP(host port) # Or the equivalent with endpoints # from twisted.internet.endpoints import TCP4ClientEndpoint # from twisted.internet.protocol import ClientFactory # endpoint = TCP4ClientEndpoint(reactor host port) # factory = ClientFactory() # factory.protocol = MyProtocol # whenConnected = endpoint.connect(factory) def cbConnected(connectedProtocol): connectedProtocol.send_message(...) def ebConnectError(reason): # Connection attempt failed perhaps retry ... whenConnected.addCallbacks(cbConnected ebConnectError) You could also save the reference to connectedProtocol in the cbConnected callback so that you can continue to use it later on. You might also start whatever other operations want to use the connected protocol in cbConnected so that they don't try to use the connection before it is actually available. thanks a lot! thank you but docs say not use ClientCreator and do use endpoints instead right? endpoints are newer and nicer and somewhat more flexible. I thought ClientCreator would make for an easier to understand answer. :) I'll add an endpoints-based version."
494,A,Connecting to tcp server from client running in android device I am trying to connect to a standalone desktop tcp server(java) from tcp client application in android device. But I am not able to connect to that. I tried to write a desktop tcp client and tried to connect to server(remote server). It is working fine. But When I am trying to connecting from android I am getting a IOException while creating Socket instance. Can anyone help me in this ? Thanks in advance.... OK... for the time being we are doing this by checking the connection every n seconds. Timer.scheduleAtFixedRate( new TimerTask() { public void run() { ..... } } But still I want to know if there any way like getting callback upon socket connection lost... Hi thanks for the comment The problem is In Idle mode if I want to check socket status I cant do socket.getInputStream.read() to read from Server because it may not send anything to in Idle mode. SoI am using socket.getOutputStream().write() to write and at the same time I can check the socket status. But still with this method I am making unneccessary traffic on N/W and my app consume more CPU. I have seen one API socket.isConnected() but unfortunatly it is giving proper result when I closed the socket connection at server side. Please let me know if U come across any solution. Thanks You should make the callback yourself. socket.getInputStream().read() will return -1 when the socket is closed so when your 'read thread' encounters this you can call your own callback. But yeah I wish such a callback was build in too sometimes :).  I am answering my own question :) Now I am able to connect to server running in my PC from client running in Android device. The problem is My server running in PC is connected to different IP N/W than the Wi-Fi N/W used by my Mobile. So even if I give the same IP address in client to connect to server it can not connect because IP address is relative(local) to the IP N/W. So if we are using relative IP for Server we need to make sure that the both Server and client running devices should be connected over the same IP N/W. It is a silly question and silly mistake and silly answer. :) Hi All I need to know one thing. I need to keep on check the Socket health(is it alive or not). For this I am keep on writing 1 byte of data to the server's outstream. With this I am able to monitor the connection. But I kept this in infinite loop as I need to keep on check that forever. It is consuming more CPU cycles. Instead of that is there any way to check the connection status? Thanks in advance..
495,A,Where can I found the stack of protocols that ICQ using to develop my own client? I am going to develop ICQ client and I just wonder where can I find a reference to understand the protocols they used? Any links books etc. And by the way if I am not mistaken - all ICQ clients - QIP Miranda ICQ Lite and others - uses the same version of protocol to communicate correctly with ICQ servers? libpurple is a library that implements lots of IM protocols among them the oscar protocol used by icq. It is used by many IM clients and is pretty stable so I recommend you just use it. If you want to understand oscar though studying libpurple might be a good idea. it`s a library implemented in C++ right? so I can use any library in C++ I want to? QT boost etc.?  Pidgin is open source perhaps you can refer to its source code.  Wikipedia has an article on the OSCAR protocol although it's proprietary.
496,A,"Using QUdpSocket to send datagrams I am trying to send a datagram using QUdpSocket. The following is the code I am using: udpSocket = new QUdpSocket(this); QByteArray datagram = ""Message""; udpSocket->writeDatagram(datagram.data() datagram.size() QHostAddress::Broadcast 45454); Now if I run this on a computer that has only one network adapter it seems to work with no problem. However if there are multiple adapters I need to be able to control which is used to send the datagram. I have found that if I bind the socket as follows: udpSocket->bind(QHostAddress(""192.168.1.104"") 45454); then I can force the datagram to be sent out on the local network associated with that IP (otherwise it appears to choose one at random). However the 'bind' function sets up the socket to listen for packets which I am really not interested in at this point. Is this the correct way to control which adapter is used or is there some more straightforward way to do this? Thanks The broadcast address of a subnet is always the highest address in the subnet. In your case: adapter1: address 192.168.1.104 subnet mask 255.255.255.0 broadcast: 192.168.1.255 adapter2: address 192.168.56.1 subnet mask 255.255.255.0 broadcast: 192.168.56.255 So you need both the address of the adapter you want to broadcast on and the subnet mask to find the correct broadcast address. If you use adapter address and subnet mask to calculate the broadcast address this should work for IPv4 networks. You mean `QHostAddress::Broadcast` doesn't work? QHostAddress::Broadcast is a constant that is always 255.255.255.255 the generic broadcast address. Unfortunately an address that can work on any adapter doesn't tell your OS which adapter to use.  You need something like this QHostAddress myBroadcastAddress = QHostAddress(""192.168.255.255""); udpSocket->writeDatagram(datagram.data()datagram.size() myBroadcastAddress  45454 ) This will send udp broadcast packets. Thanks for the suggestion however using the address ""192.168.255.255"" does not help. The IP address of my computer on adapter 1 is 192.168.1.104 and on adapter 2 is 192.168.56.1 so I am assuming that with the above address it is still picking the wrong adapter. However if I use ""192.168.1.255"" it does seem to work. I do not have a lot of experience with networks however. How robust is this going to be. That is will it work on any (most) systems? Thanks."
497,A,"Proper way to write and read an xml string I've been beating my head against this wall for quite some time now so I thought I'd ask some experts. I need to send an xml string from one computer to the next. I would like to format the xml something like this: <xml> <author>Joe the Magnificent</author> <title>Joe Goes Home</title> </xml> Can anyone provide some assistance? Edit: More detail I control both the send and receive and have successfully transfered a hard coded string one-way. Here is the receive side:  Dim author As String Dim title As String Dim xDoc As New XmlDocument Dim xAuthor As XmlElement Dim xTitle As XmlElement xDoc.LoadXml(xml) xAuthor = xDoc.FirstChild.Item(""author"") xTitle = xDoc.FirstChild.Item(""title"") author = xAuthor.FirstChild.Value title = xTitle.FirstChild.Value ShowMessage(author title) Mostly this is an exercise in learning how to do XML for me so there's no real purpose to it other than my own knowledge. I was kind of looking for some opinions on the best way to do such things. Do you control both ends or just one? What have you done so far? What is the purpose? Some more information would help. Is the xml really this small? How fast does the transfer have to be? Are there security considerations? I can tell from the tag it's a vb.net system. The answers could be as diverse as 'write it to a file' or 'send it through tcp/ip'. The title of this question hides the fact that the OP wants to read and write across two different PC's. Well I don't know if this is what you are looking for but if you are using the latest version of VB and .NET then you should be able to use xml literals and LINQ to parse your xml: Like so-> Sub Send() Dim myxml = <xml> <author>Joe the Magnificent</author> <title>Joe Goes Home</title> </xml> Readxml(myxml) End Sub Sub Readxml(myxml as XDocument) Dim Data = From xml in myxml...<xml> _ Select New With {.Author = xml.<author>.value _ .title = xml.<title>.value} For each item in Data ShowMessage(item.AuthorItem.Title) Next End Sub Note the above is just air code so it may not run not at my computer so I can't test it.  Using the XmlDocument.Load method you have 4 options: From a Stream TextReader URL or XmlReader. You could use the NetworkStream class to go over a network. You could post your XML up on a website and suck it down via the URL option. You might want to be more specific about the protocol in which you want the transfer to occur. For example to write to a stream use the XmlWriter.Create overload for a stream. Use an XmlWriterSettings object to provide indentation.  Dim settings As XmlWriterSettings = New XmlWriterSettings() settings.Indent = true settings.IndentChars = (ControlChars.Tab) settings.OmitXmlDeclaration = true Dim myNetworkStream As New NetworkStream(mySocket) 'mySocket is a whole other code sample ' Create the XmlWriter object and write some content. writer = XmlWriter.Create(myNetworkStream settings) XmlDocument.WriteTo(writer) To construct xml documents [the old way] was quite cumbersome and I'd suggest looking at VB9 XML literals. However here is an example of .NET 2 style XmlDocument manipulation:  Dim doc As New XmlDocument() Dim root As XmlElement = doc.CreateElement(""xml"") Dim author As XmlElement = doc.CreateElement(""author"") author.Value = ""Joe the magnificent"" Dim title As XmlElement = doc.CreateElement(""title"") title.Value = ""Joe goes home"" root.AppendChild(author) root.AppendChild(title) doc.AppendChild(root)  Here's what I ended up doing: Public Function FormatMessage(ByVal author As String ByVal title As String ByVal genre As String) As String Dim xDoc As New XmlDocument ' Create outer XML Dim xNode As XmlNode = xDoc.AppendChild(xDoc.CreateElement(""xml"")) ' Create Author Node Dim xAuthor As XmlNode = xNode.AppendChild(xDoc.CreateElement(""author"")) xAuthor.InnerText = author ' Create Message Node Dim xTitle As XmlNode = xNode.AppendChild(xDoc.CreateElement(""message"")) xtitle.InnerText = title ' Create Genre Node Dim xGenre As XmlNode = xNode.AppendChild(xDoc.CreateElement(""genre"")) xGenre.InnerText = genre ' Create StringWriter to convert XMLDoc to string Dim xWriter As New IO.StringWriter() Dim xml_writer As New XmlTextWriter(xWriter) xDoc.WriteContentTo(xml_writer) Return xWriter.ToString End Function This function builds the xml string based on the input values then to break the xml string back down into the original values I used this: Dim author As String Dim title As String Dim genre As String Dim xDoc As New XmlDocument Dim xAuthor As XmlElement Dim xTitle As XmlElement Dim xGenre as XmlElement xDoc.LoadXml(xml) If xDoc.DocumentElement.Name = ""xml"" Then xAuthor = xDoc.FirstChild.Item(""author"") xTitle = xDoc.FirstChild.Item(""title"") author = xAuthor.FirstChild.Value title = xTitle.FirstChild.Value genre = xGenre.FirstChild.Value End If ShowMessage(author title genre) Thanks for the help! KJ"
498,A,Modeling software for network serialization protocol design I am currently designing a low level network serialization protocol (in fact a refinement of an existing protocol). As the work progress pen and paper documents start to show their limits: i have tons of papers new and outdated merged together etc... And i can't show anything to anyone since i describe the protocol using my own notation (a mix of flow chart & C structures). I need a software that would help me to design a network protocol. I should be able to create structures fields their sizes their layout etc... and the software would generate some nice UMLish diagrams. Sorry to say everything I've seen so far (various serial protocols for embedded devices/networks) has used Word documents with plain old tables showing allocations of fields to the bytes in the message. Alternatively I've seen it done in Excel documents! It works and people can read it. Unfortunately that's not helpful for automatic code generation unless you have a very strict format in e.g. an Excel doc that you can then parse with a tool to generate some code. It would be good to have a notation that can be easily machine parsed as well as human readable. For showing message handshaking and sequences a UML sequence diagram is good of course. There are lots of tools readily available to help you with that part of it.
499,A,"java.net.URLEncoder.encode(String) is deprecated what should I use instead? I get the following warning when using java.net.URLEncoder.encode: warning: [deprecation] encode(java.lang.String) in java.net.URLEncoder has been deprecated What should I be using instead? This is answered in the deprecation tag in the docs: ""Instead use the encode(StringString) method to specify the encoding."" See http://java.sun.com/javase/6/docs/api/java/net/URLEncoder.html. The first parameter is the String to encode; the second is the name of the character encoding to use (e.g. UTF-8).  Use the class URLEncoder: URLEncoder.encode(String s String enc) Where : s - String to be translated. enc - The name of a supported character encoding. Standard charsets: US-ASCII Seven-bit ASCII a.k.a. ISO646-US a.k.a. the Basic Latin block of the Unicode character set ISO-8859-1 ISO Latin Alphabet No. 1 a.k.a. ISO-LATIN-1 UTF-8 Eight-bit UCS Transformation Format UTF-16BE Sixteen-bit UCS Transformation Format big-endian byte order UTF-16LE Sixteen-bit UCS Transformation Format little-endian byte order UTF-16 Sixteen-bit UCS Transformation Format byte order identified by an optional byte-order mark Example: import java.net.URLEncoder; String stringEncoded = URLEncoder.encode(""This text must be encoded! aeiou áéíóú ñ peace!"" ""UTF-8"");  You should be using the other method in the URLEncoder class: URLEncoder.encode(String String) The first parameter is the String to encode; the second is the name of the character encoding to use (e.g. UTF-8). @jsh: I'm confused why shouldn't there be a URLDecoder? Why does this make Java bloated? These are static methods. It would take the same amount of effort to type either. If you like Python why are you programming in Java? Is it because more people use Java than Python and you got a Java job instead of Python job? @stepanian yep you gotta work where the work is. the python package i typically use puts all these encoding and decoding utilities under one roof. I mainly added this comment because I had to go and find the URLDecoder and I thought it might save somebody a couple of minutes to know this was what was required. @jsh: A lot of people like Python. I have never used it but I'm sure I would like it too if I had the time to try it. But I am chasing the dollar too. Java ain't that bad. He's calling it bloated because its overpopulating the global class namespace. Why have URLEncoder.encode and URLDecoder.decode when you could have URL.encode and URL.decode or even just URLEncoder.decode? Why make it all redundant and bloaty? Because its java. And then you have to handle the UnsupportedEncodingException even though UTF-8 should be supported pretty much everywhere. @DaveCameron UTF-8 support is required [since at least 1.4](http://docs.oracle.com/javase/1.4.2/docs/api/java/nio/charset/Charset.html) but apparently Java encourages you to write extra code to handle an exception that is guaranteed to never happen unless you happen to mistype a string which ought to be a symbolic constant in the first place. @tc.: Java 7 introduced these constants: `StandardCharsets.US_ASCII` `StandardCharsets.UTF_8` etc. Unfortunately `URLEncoder.encode` does not accept a `Charset`... (but many other moethods do). @sleske the constant from Java 1 is **HTTP.UTF_8** I agree with B T. These things are waaaay easier to deal with in Objective-C. @htafoya **wrong**. This class is found in Apache HttpClient 4.x library `org.apache.http.protocol.HTTP` class.  URLEncoder.encode(""NAME"" ""UTF-8"")  As an additional reference for the other responses instead of using ""UTF-8"" you can use: HTTP.UTF_8 which is included since Java 4 as part of the org.apache.http.protocol library which is included also since Android API 1. @BuhakeSindi true I read API 1 but it was of Android not Java either way it exists before Java 7 it is even deprecated already haha. **Wrong** this class is found in Apache HttpClient 4.x library `org.apache.http.protocol.HTTP` class. no this class **never** existed in any version of the Java JDK. Android follows the Apache HttpClient library (I won't be surprised if they took the source code from there as well)."
500,A,"bind before connect at client code I have multiple ethernet I/Fs. eth0eth1eth2... and I want to connect to an external server eg 1.2.3.4:80. My connections are OK but under some special circumstances I want to connect as eth1 and not eth0. the server's code checks the IP address of my interface. I think that I need to bind before connect. Without bind(2) the server always gets packets from eth0 I am looking for code that demonstrates this behavior. Does anybody has a link to an example? You don't need bind(2) for this. What you're looking to do here is to use a different network interface with your socket. To use a network interface other than the system default you need to use the SO_BINDTODEVICE socket option along with setsockopt. The interface you want to use such as ""eth1"" for example should be specified as a string in the ifr_name field of a ifreq struct which is to be passed to setsockopt. To do this you need to include the <net/if.h> header. Basically something like the following (untested) code: int set_interface(int socket_fd const char* interface_name) { ifreq interface; memset(&interface 0 sizeof(interface)); strncpy(interface.ifr_name interface_name IFNAMSIZ); int res = setsockopt(socket_fd SOL_SOCKET SO_BINDTODEVICE &ifreq sizeof(ifreq)); return res; } Also make sure you check the return code in case setsockopt fails. what is the difference between bind and SO_BINDTODEVICE? why bind does not solve the problem in my case? @cateof `bind(2)` binds a socket to a particular endpoint (IP address and port.) It has no notion of ""network interfaces"". To associate a socket with a certain network interface you need to use special facilities from the `` header."
501,A,What subject or area can show the relationship between programming and networking? Hi I am studying Software Engineering and I have to do a case study of relationships between network and programming but I don't know anything about networking I don't know where to start. If anyone could help me out to point me to the right direction would be great!. Thanks. I don't think you need to know about networking in any detail. I'd believe that you are being asked to think about how programming for a network environment generates specific conditions that need to be coded for such as disconnected operation bandwidth issues and intermittant connection (how code needs to cope with an unreliable connection). I hope that helps.
502,A,"Best way to ping client Right I have a PHP script at work where the server ping's a client. The problem I am facing is that sometimes the server cannot contact the client although when I manually ping the client it ping's successfully. The ping command I am using is this ping -q -w 3 -c 1 < ipaddresshere > What would be the best way of pinging the clients maybe 2/3 times leaving like a 2/3 second gap if a ping fails before a retry? for php you can try PEAR's Net_PING package. here is a link guiding you through it http://www.codediesel.com/php/ping-a-server-using-php/  As you are in the unix environment you can always make and then call a shell script to handle the looping and waiting. But I'm surprised that you can't do that inside of php. Also i'm not sure about your sample ping command the 2 different environments I checked seem to have different meanings for the options you mention than what you seem to intend. Try man ping OR ping --help The script below should give you a framework for implementing a ping-retry but I can't spend a lot of time on it. cat pingCheck.sh #! /bin/bash -vx IPaddr=$1 : ${maxPingTries:=3} echo ""maxPingTries=${maxPingTries}"" pingTries=0 while ${keepTryingToPing:-true} ; do if ping -n 3 -r 1 ${IPaddr} ;then keepTryingToPing=false else sleep ${sleepSecs:-3} if (( ++pingTries >= maxPingTries )) ; then printf ""Execeeded count on ping attempts = ${maxPingTries}\n"" 1>&2 keepTryingToPing=false fi fi done I hope this helps. P.S. as you appear to be a new user if you get an answer that helps you please remember to mark it as accepted and/or give it a + (or -) as a useful answer. Says upvote requires 15 reps great answer thanks very much. I will upvote when I get 15 reps."
503,A,"What causes udp reception delay? I have noticed that when I am sending packets at even intervals from a udp socket the first packet sent seems to be delayed. For example if I am sending the packets every 100 ms I find the delay between receiving the packets to be normally distributed with mean 100ms and average standard deviation of 4 on my network. However the gap between the receive time for the first and second packets is usually around 10 to 40 ms - as you can see that's clearly a statistically significant difference and so my question is what's causing it? I'm using the sendto function from C on linux. Someone suggested the delay might be caused by arp resolution preventing the packet from being sent until the destination ip has been converted to a mac address - is this likely? If I restart the sending program the first packet again takes too long though and the delay is inconsistent - 10 to 40 ms is a pretty big range. I need to find out why this first packet is taking too long and how to work around it. Edit: Further analysis with pcap indicates that the sending program is sending the packets at the right interval. Problem must be with the receiver which is using select() to wait for a readable socket then calling recvfrom and printing the packet. Is there some sort of buffering going on there I might not know about? Hey could whoever downvoted please post why? You can use a sniffer to check if its something related to your computer of something related with the transmission of the packet. Speculation will get us nowhere fire up Wireshark and it will tell you all you need to know. Yes - I managed to get this working finally and it seems the packets are being sent with the correct gap between them; the delay must be on the receiving end. I'm running a really stripped down system nothing on it except for ash and my program and I've been having difficulty getting pcap running. Wireshark is right out. If wireshark is out what about tcpdump? If this does not work if you can get you hands on a ethernet hub (as opposed to a switch) you should be able to sniff the network remotely.  Is this on a single LAN? If so it is (probably) down to ARP and/or hostname resolution. If it's across a larger network it may be anything (although probbaly related to route lookup and cache population). Yes single lan. two machines and a dhcp server connected via switch. @benubird ARP cache retention is OS-dependent. Looking at your last comment on the wireshark recommendation it seems to be something in the switch fabric causing the delay. Unlikely to be arp because I determined that the packets are being sent in proper sequence and because I'm sure the arp cache must last longer than a couple of seconds.  I don't have enough ""reputation"" to vote up what I consider to be the best solution so I'll just say that the guy who mentions ARP messages is most-likely right. I think the ARP messages should only apply on the lan. They'll show up on wireshark as a ""who has 192.168.0.24?"" for example... and then there will be a reply from any host that claims to have that IP address. The initial packet send via UDP to this address has to get an ARP response to resolve the IP address into an Ethernet MAC address... so UDP on the surface may seem like it should never BLOCK... however this is only true once the ARP address is resolved. I'm not sure but I think if you're sending UDP to an address that is not on the LAN it shouldn't require an ARP... unless there's a problem finding the primary gateway.  It is most likely the time required for the ARP address resolution. This is the protocol which resolves MAC addresses to IP addresses. For working around this try using static entries in the arp cache with arp -s ip-address hw_address. I have BusyBox v1.7.2 (2010-11-17 12:19:56 GMT) built-in shell (ash) Unfortunately I can't - arp command is not available. `-sh: arp: not found`. Good suggestion though. For me its part of the package ""net-tools"" (like ifconfig etc) and in /sbin/arp. Its a very basic tool which is on almost every distribution what do you have? can't open '/proc/dev/arp': No such file or directory. There is a /dev but not /proc/dev or dev/arp. Haven't tried the ioctl yet as I'd rather not have to use a static arp anyway.  You have asked about workarounds. One possible workaround if the reception time of the packet is important to know is to set the SO_TIMESTAMP socket option. This will allow you to obtain the time that each packet was received by the destination kernel - you can then use this time in subsequent processing rather than the ""current time"". Alternatively you could have the sender include a high resolution timestamp in the packet that it sends and use those."
504,A,"Find and start a VPN connection in VB .NET I am using NetworkInterface.GetAllNetworkInterfaces() to get all the interfaces on a PC. However this appears to only return ""active"" interfaces. How can I find ""inactive"" network interfaces such as unconnected VPNs disabled NICs etc. in .NET. I would like to find them by their name in ""Control Panel"" -> ""Network Connections"". So for example if I have a VPN called ""My Work"" I would like to be able to find it using the name ""My Work"". Using Win32_NetworkAdapterConfiguration does not seem to be an option as it does not return the name shown in ""Network Connections"" (as far as I can see). Many thanks RB. I have added info about the Win32_NetworkAdapter class to my answer below - hopefully this is more in line with what you are looking for Thanks Paul - looking into it now. I have got a VPN specific solution which involves shelling out to ""rasdial.exe"" but it's not....pretty.... I'll post it for info in a min anyway! OK - Last ditch effort :) How about the Win32_NetworkConnection class - I'm pretty sure this will handle VPN connections On my machine this shows all my mapped drives and any network shares I have open. It doesn't show VPNs even if they're connected :-( Thanks for your help though :-) Bugger - the info you need must be in WMI somewhere - just a case of wading through the documentation to find the right class :o( Glad you found a workaround  RasDialer dialer = new RasDialer(); ReadOnlyCollection<RasConnection> connections = dialer.GetActiveConnections(); foreach (RasConnection connection in connections) { // Do what you want to with the connections. } That will retrieve all connected dial up entries (including VPN connections) that are in use by Windows along with their related handles and other information. That component is part of the DotRas project on CodePlex. http://www.codeplex.com/DotRas Hi Jeff that is *exactly* what I was looking for. I'm going to download the source later to find out how you've programmed such magical wizardry! Many many thanks :-)  Ok this is a hacky solution I got to detect named VPNs. It will throw an error if it cannot connect to the VPN for whatever reason (including the network connection is down the VPN does not exist etc.). Specific error codes to test for include : Remote Access error 800 - Unable to establish the VPN connection. The VPN server may be unreachable or security parameters may not be configured properly for this connection. Remote Access error 623 - The system could not find the phone book entry for this connection. It doesn't really answer my question as posed (although it works well enough for the real-life problem). Dim p As New Process p.StartInfo.UseShellExecute = False p.StartInfo.RedirectStandardOutput = True p.StartInfo.RedirectStandardError = True p.StartInfo.FileName = ""rasdial.exe"" p.StartInfo.Arguments = """"""Company HQ"""""" p.Start() If Not p.WaitForExit(My.Settings.VpnTimeout) Then Throw New Exception( _ String.Format(""Connecting to """"{0}"""" VPN failed after {1}ms"" sVpn My.Settings.VpnTimeout)) End If If p.ExitCode <> 0 Then Throw New Exception( _ String.Format(""Failed connecting to """"{0}"""" with exit code {1}. Errors {2}"" sVpn p.ExitCode p.StandardOutput.ReadToEnd.Replace(vbCrLf """"))) End If  This example using WMI may get you most of the way (Sorry about the C# by the way!): using System.Management; string query = ""SELECT * FROM Win32_NetworkAdapterConfiguration""; ManagementObjectSearcher moSearch = new ManagementObjectSearcher(query); ManagementObjectCollection moCollection = moSearch.Get(); // Every record in this collection is a network interface foreach (ManagementObject mo in moCollection) { // Do what you need to here.... } For testing whether the adapter is enabled or not this link should help: Win32_NetworkAdapterConfiguration class Edit: You may have more luck with the Win32_NetworkAdapter class - it has Caption and Description properties which may give you what you need. There is a way to tie this info in with the previous class I mentioned to get IP addresses etc - not sure what it is without further research but let me know if you need to know. Hi Paul. The ""NetConnectionID"" will show the name (e.g. ""Local Area Connection"") for my NIC but won't show this for my VPN even if it is connected. Thanks for your help though."
505,A,"thread terminate called without an active exception I've been doing threaded networking for a game but the server dies randomly while i've been testing the networking so that I have several clients connecting and sending bunch of packets and disconnecting then connecting back again. I am using c++ with SFML/Network and SFML/System threads. I have thread which listens for connections in the server once connection is established it creates two new threads for sending and receiving packets. The event handler and the send/receive threads share data with two std::queues. I've been trying to debug the crash with gdb but i'm not that experienced with this so i'm looking for help. Here is gdb console input when the crash happens. OUT: 10 1 HELLO IN: 10 0 LOLOLOL OUT: 10 1 HELLO IN: 10 0 LOLOLOL OUT: 10 1 HELLO Out thread killed by in thread! In thread died! New client connected! [Thread 0x34992b70 (LWP 16167) exited] [New Thread 0x3118bb70 (LWP 16186)] terminate called without an active exception Program received signal SIGABRT Aborted. [Switching to Thread 0x35193b70 (LWP 16166)] 0x00110416 in __kernel_vsyscall () Here is the backtrace: (gdb) backtrace #0 0x00110416 in __kernel_vsyscall () #1 0x46a0967f in raise (sig=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:64 #2 0x46a0afb5 in abort () at abort.c:92 #3 0x47b8af0d in __gnu_cxx::__verbose_terminate_handler () at ../../../../libstdc++-v3/libsupc++/vterminate.cc:95 #4 0x47b88c84 in __cxxabiv1::__terminate (handler=0x47b8adc0 <__gnu_cxx::__verbose_terminate_handler()>) at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:40 #5 0x47b88cc0 in std::terminate () at ../../../../libstdc++-v3/libsupc++/eh_terminate.cc:50 #6 0x47b8878f in __cxxabiv1::__gxx_personality_v0 (version=1 actions=10 exception_class=890844228 ue_header=0x35193dc0 context=0x35192ea0) at ../../../../libstdc++-v3/libsupc++/eh_personality.cc:669 #7 0x46bdbfbe in _Unwind_ForcedUnwind_Phase2 (exc=0x35193dc0 context=0x35192ea0) at ../../../gcc/unwind.inc:175 #8 0x46bdc3a9 in _Unwind_ForcedUnwind (exc=0x35193dc0 stop=0x46b76fc0 <unwind_stop> stop_argument=0x35193444) at ../../../gcc/unwind.inc:207 #9 0x46b794e2 in _Unwind_ForcedUnwind (exc=0x35193dc0 stop=0x46b76fc0 <unwind_stop> stop_argument=0x35193444) at ../nptl/sysdeps/pthread/unwind-forcedunwind.c:132 #10 0x46b77141 in __pthread_unwind (buf=<optimized out>) at unwind.c:130 #11 0x46b6f5bb in __do_cancel () at ../nptl/pthreadP.h:265 #12 sigcancel_handler (sig=<optimized out> si=<optimized out> ctx=<optimized out>) at nptl-init.c:202 #13 sigcancel_handler (sig=32 si=0x35192f7c ctx=0x35192ffc) at nptl-init.c:155 #14 <signal handler called> #15 0x08049930 in out (data=0xb761c798) at src/layer7.cpp:40 #16 0x0804b8d7 in sf::priv::ThreadFunctorWithArg<void (*)(networkdata*) networkdata*>::Run (this=0xb761c7c8) at /usr/local/include/SFML/System/Thread.inl:48 #17 0x00116442 in sf::Thread::Run() () from /home/toni/ProjectRepos/sfml/build/lib/libsfml-system.so.2 #18 0x001166df in sf::priv::ThreadImpl::EntryPoint(void*) () from /home/toni/ProjectRepos/sfml/build/lib/libsfml-system.so.2 #19 0x46b70c5e in start_thread (arg=0x35193b70) at pthread_create.c:305 #20 0x46ab4b4e in clone () at ../sysdeps/unix/sysv/linux/i386/clone.S:133 Here is the thread code from src/layer7.cpp void out(networkdata * data) { bool running = true; while(running) { if(data->pipe_out->pipe_empty() == false) { sf::Packet packet = data->pipe_out->pop_message(); if(data->socket->Send(packet) == sf::Socket::Disconnected) { data->thread_in->Terminate(); std::cout << ""In thread killed by out thread!"" << std::endl; running = false; } } } std::cout << ""Out thread died!"" << std::endl; } Line 40 is the first if keyword after the while(running). The data->pipe_out->pipe_empty() is call to the queue->empty() The data->pipe_out->pop_message() is call which pops the front from the queue. Then it sends the packet and checks if the connection is not disconnected if socket is disconnected it terminates the ""in"" thread and stops the own thread. You are getting a signal (look at the line that says ""signal handler called""). I'm not sure which one I don't trust the ""sig=32"" bit. This usually terminates your thread. Perhaps you should try use your own signal handlers. @n.m.: From the runtime. A `SIGABRT` due to messing up his memory. Yes the problem was fixed. Where are the locks around `data`? Switch threads in `gdb` to get a different backtrace maybe. Actually i realized just now before checking your comments that i should probably lock data. :P Will test in a sec. @Temek: ""probably""?! Yeah stupid me. It seems the bug was there. Seems to be working now. 5 Minutes been running already and before it crashed under a minute. One possible reason for is an exception: exception should be caught withing thread. Also looks like data->thread_in->Terminate() sends cancelation request make sure that all established cancellation handlers are working correctly in that case.  You need locks around data to protect against concurrent access to the same data structure from multiple threads."
506,A,How can i get access to unshared folder I have an unshared folder on server and i want to access to it from clients by c# code.Is there any way to do this? If yes how? please help thanks in advance If it's not shared then you can't. - the server owner / admin chose (implicitly or explicitly) to not allow clients from other machines to access it. You'll need to contact the server owner to get the folder you need shared (possibly with some ACLs to limit who can access it). Right thanks for clarifying. Not always true see my answer.  If you are a admin (or domain admin) then you can use the implicitly shared C$. If the file is at 'c:\foo\bar.html' on server named myserver then try this:  \\myserver\c$\foo\bar.html Note that this only works when you are an administrator of the server. thanks too much so clients can not do this anyway? do you suggest remote desktop?if yes how can i use it?
507,A,How to debug unclean socket closure in C? I have a network daemon (poll()/accept()/fork() style) which is leaking socket file descriptors one per client in the TIME_WAIT state. As far as I can see I can shutdown()ing and then close()ing definitely-no-longer-needed sockets. Other sockets (for example the server socket in the client side of the fork) are just close()ed. All sockets have SO_REUSEADDR set and SO_LINGER is off. I am using _exit() to exit the program and I am using non-blocking polling socket operations so as to set a ''dying'' flag in my signal handler -- this allows me to later pick up the dying flag and free() shutdown() close() which would otherwise be dangerous in a signal handler. But still a fd leak -- What is the best way to debug this kind of problem? It would help to know which socket is loitering at exit as there are many fds involved in the process. Cheers! What OS are you using? Different OSes have quite different tools for debugging this kind of thing. Some code would be useful to see. I don't have a minimal example to hand - if no-one has any ideas i can post a tarball of the entire codebase. Sockets in TIME_WAIT mode are NOT leaking -- TIME_WAIT means that the application has finished with the socket and has closed it and cleaned it up but the kernel is still remembering the socket so as to respond properly to late/orphan/duplicate packets that might be floating around in the network. After a little while the kernel will automatically delete the TIME_WAIT sockets but until then they remain as a reminder to the kernel to not reuse the port unless an app specifically asks for it with SO_REUSEADDR.  I figured this out. Infact I had fixed the bug already by closing the cli_fd in the server side of the fork; however I did not notice the bug was fixed because i was using natstat wrongly to could open fds. For the record the output of netstat -n | grep TIME_WAIT | wc -l should not be used to count file descriptors for sockets which are hanging around -- this is what i was doing wrong. Use lsof or fstat instead. Anyway - the server is no longer running out of fds under considerable load. Cheers
508,A,"Should I use #defines or enums for commands over network? I have to transfer some values to be used as commands over the network and I wish to make it as efficient and robust as possible i.e. I need opinions on what to use for these commands #defines or enums? The range of commands shall not be more than 20 commands(lets say 40 with every defined response to the command) so according to most things I've heard it would fit well in the char limit. So far I'm assuming that the best approach would be to use an enum. I would also recommend using a line-delimited text-based protocol. Aside from not having to worry about enum-vs.#define values there are some other benefits: hacking/testing It's very easy to manually generate a set of commands for text-based protocols. Consider testing error conditions on an HTTP server: $ telnet www.yahoo.com 80 Trying 69.147.76.15... Connected to www-real.wa1.b.yahoo.com. Escape character is '^]'. GOAT / HTTP/1.1 I don't need anything fancy to understand how the server responded: HTTP/1.1 400 Bad Request Date: Sat 11 Jul 2009 16:20:59 GMT Cache-Control: private Connection: close Transfer-Encoding: chunked Content-Type: text/html; charset=iso-8859-1 This is great for debugging. You can create a set of test scripts to feed into a telnet/nc and they'll be much easier to maintain than any binary-protocol test scripts. portability If you define your protocol in terms of ASCII text (or maybe UTF-8) you never have to worry about lower-level issues on new platforms. Endianness structure alignment and word size are all issues when dealing with a binary protocol. ASCII may require an extra serialize/deserialize step but in a networking protocol that's generally necessary anyways. sharing If you want to work with other people on a project it's great to be able to describe network operations as plain-ole-text. If someone else wants to develop their own client there's no requirement for them to use your header files or anything they just have to adhere to an RFC-like standard. This is not useful for a pet project but it's a good thing to consider for anything that may get larger.  You want opinions? My opinion is that you should use a text-based protocol where each of the commands is a keyword. Whether and how you want to convert them to/from enums in your program is up to you but on the network level using text-based commands means your protocol is more extensible. A lot of real-life protocols are text-based and I dare say they're pretty efficient for what they do. e.g. HTTP FTP SMTP etc. are all text-based. Much easier to debug too. HTTP and SMTP are not very efficient for anything besides text and FTP's actual data transfer is binary. Those real-life text protocols are good because they're simple not because they're ""pretty efficient"".  From an efficiency standpoint it makes no difference whatsoever. #define or enum is just a way of assigning a numerical value... the real trick is how you pack that value onto the network. That said I agree with the other answers I see here and I think you are worrying about the wrong thing anyway. Designing a network protocol takes thought and making it portable and usable across many systems should generally override efficiency concerns (there are exceptions to that rule of course).  When transferring numeric values across the internet you have 2 things to bear in mind. The first is the endianess the order that bytes (or sometimes bits) appear in. Suppose you have the 32-bit value 0xAABBCCDD. Intel CPUs are little-endian machines which mean those bytes would be stored as { 0xDD 0xCC 0xBB 0xAA }. In other words the least significant byte is stored at the lowest address. In a big-endian machine the bytes would be stored as { 0xAA 0xBB 0xCC 0xDD } with the least significant byte at the highest address. When transferring multi-byte integers between 2 machines you have to ensure that they both interpret each others data correctly even if they have different byte orderings. Thankfully there is a standard called Network byte order which is big-endian and there are 4 useful functions for converting between host-order and network-order: ntohl (Network to Host long) ntohs (Network to host short) htonl (Host to network long) htons (Host to network short) The long versions work with 32-bit integers and the short versions with 16-bit integers. As long as you always call *hton** before transmitting data across the network and call *ntoh** when reading from the network data will be in the correct byte order. Of course the easiest way to get around this problem especially since you have only 20 commands is to just use single bytes or chars. The second problem you have to deal with is encoding. How are signed integers represented? Using a sign-bit? Twos complement? Again you run in to problems when different platforms on the network use different representations. If you stick to unsigned types you shouldn't really have a problem. What you use to represent your commands in your program is entirely up to you. Just make sure that you define your protocol well and adhere to that when transmitting and reading data. For example: enum command_t { one two three } void send_command(command_t c) { send((unsigned char)c); } command_t read_command() { return (command_t)recv(); } void send(unsigned char c) { ... } unsigned char recv() { ... } I think in this rather storing each command in unsigned char variable and equating it to a value i shall store it in #define as it has always been done. I think now i understand why #define have always been used for error-codes state-names commands... etc."
509,A,"First packet to be sent when starting to browse Imagine a user sitting at an Ethernet-connected PC. He has a browser open. He types ""www.google.com"" in the address bar and hits enter. Now tell me what the first packet to appear on the Ethernet is. I found this question here: http://stackoverflow.com/questions/177197/interview-questions-on-socket-programming-and-multi-threading As I'm not a networking expert I'd like to hear the answer (I'd assume it is ""It depends"" ;) ). With a tool like Wireshark I can obviously check my own computers behaviour. I'd like to know whether the packets I see (e.g. ARP DNS VRRP) are the same in each ethernet configuration (is it dependent on the OS? the driver? the browser even :)?) and which are the conditions in which they appear. Being on the data-link layer is it maybe even dependent on the physical network (connected to a hub/switch/router)? Interview question? ;) Yes as you can see if you click the link I provided. The answers that talk about using ARP to find the DNS server are generally wrong. In particular IP address resolution for off-net IP addresses is never done using ARP and it's not the router's responsibility to answer such an ARP query. Off-net routing is done by the client machine knowing which IP addresses are on the local subnets to which it is connected. If the requested IP address is not local then the client machine refers to its routing table to find out which gateway to send the packet to. Hence in most circumstances the first packet sent out will be an ARP request to find the MAC address of the default gateway if it's not already in the ARP cache. Only then can it send the DNS query via the gateway. In this case the packet is sent with the DNS server's IP address in the IP destination field but with the gateway's MAC address on the ethernet packet.  Actually it depends on a variety of initial conditions you left unspecified. Assuming the PC is running an operating system containing a local DNS caching resolver (mine does) the first thing that happens before any packets are sent is the cache is searched for an IP address. This is complicated because ""www.google.com"" isn't a fully-qualified domain name i.e. it's missing the trailing dot so the DNS resolver will accept any records already in its cache that match its search domain list first. For example if your search domain list is ""example.com."" followed by ""yoyodyne.com."" then cached resources matching the names ""www.google.com.example.com."" ""www.google.com.yoyodyne.com."" and finally ""www.google.com."" will be used if available. Also note: if the web browser is one of the more popular ones and the PC is running a reasonably current operating system and the host has at least one network interface with a global scope IPv6 address assigned (and the host is on a network where www.google.com has AAAA records in its DNS horizon) then the remote address of the server might be IPv6 not IPv4. This will be important later. If the remote address of the Google web server was locally cached in DNS and the ARP/ND6 cache contains an entry for the IPv4/IPv6 address (respectively) of a default router then the first transmitted packet will be a TCP SYN packet sourced from the interface address attached to the router and destined for the cached remote IPv4/IPv6 address. Alternatively the default router could be reachable over some kind of layer-2 or layer-3 tunnel in which case the SYN packet will be appropriately encapsulated. If the remote address of the Google web server was not locally cached then the host will first need to query for the A and/or AAAA records in the DNS domain search list in sequence until it gets a positive response. If the first DNS resolving server address in the resolver configuration is in one of the local IPv4 subnet ranges or in a locally attached IPv6 prefix with the L=1 bit set in the router advertisement and the ARP/ND6 cache already contains an entry for the address in question then the first packet the host will send is a direct DNS query for either an A record or a AAAA record matching the first fully-qualified domain name in the domain search list. Alternatively if the first DNS server is not addressable on-link and a default router has an ARP/ND6 cache entry already then the DNS query packet will be sent to the default router to forward to the DNS server. In the event the local on-link DNS server or a default router (respectively as the case above may be) has no entry in the ARP/ND6 cache then the first packet the host will send is either an ARP request or an ICMP6 neighbor solicitation for the corresponding address. Oh but wait... it's even more horrible. There are tweaky weird edge cases where the first packet the host sends might be a LLMNR query an IKE initiation or... or... or... how much do you really care about all this buckaroo?  It depends Got that right. E.g. does the local DNS cache contain the address? If not then a DNS lookup is likely to be the first thing.  If the host name is not in DNS cache nor in hosts file first packet will go to DNS. Otherwise the first packet will be HTTP GET. Actually unless the MAC address of the DNS server is cached the first frame will probably be a broadcast ARP request trying to find out who 201.16.47.53 (the DNS server) is.  You can always download wireshark and take a look. Though to spoil the fun. Assuming the IP address of the host is not cached and the MAC address of the DNS server is not cached the first thing that will be sent will be a broadcast ARP message trying to find out the MAC address of the DNS server (which the router will respond to with its own address). Next the host name will be resolved using DNS. Then the returned IP address will be resolved using ARP (again the router will respond with its own address) and finally the HTTP message will actually be sent. this description of ARP for remote servers is completely incorrect. You are wrong about the ARP packet to the DNS server. The IP address of the DNS server is usually outside the subnet of the client. If it is than the client knows that it's no use looking for it on the local network. See Alnitak's answer's to find out what happens instead. So the main influence whether packets appear is caching? Yes even the page could be cached so that no messages would be sent at all.  Well whatever you try to do the first thing happening is some Ethernet protocol related data. Notably Ethernet adapters have to decide whether the Ethernet bus is available (so there's some collision detection taking place here) It's hard to answer your question because it depends a lot on the type of ethernet network you're using. More information on Ethernet transmission can be found here and here"
510,A,Can UDP (unicast client) recvfrom() other servers other than the one sendto()? I am creating a UDP socket client in C (unicast) and is wondering why recvfrom() has a struct sockaddr * argument in which in the man page says A null pointer or points to a sockaddr structure in which the sending address is to be stored. Is it possible that I could receive a message from a different server other than the one I sendto? If yes how to create this scenario? If no is it correct to say that this argument is only useful when broadcast mode is used? The UDP socket will recvfrom() any host sending to this one with correct port unless you explicitly connect() in which case you can just write() and read() and get errors upon received ICMP messages.  Considering you always have two parties in UDP it seems rather obvious that someone has to recvfrom() first.  Yes this is perfectly possible. The reason for this is that UDP is not stream-based but packet-based. Every packet is treated without any history (other packets sent or received). For this reason you may also open a UDP port and then send packets to different hosts from it. However I do not remember how well this is supported by the API.
511,A,Is transmitted bytes event exist in Linux kernel? I need to write a rate limiter that will perform some stuff each time X bytes were transmitted. The straightforward is to check the length of each transmitted packet but I think it will be to slow for me. Is there a way to use some kind of network event that will be triggered by transmitted packets/bytes? It's protocol dependent actually. But for TCP you can setsockopt the SO_RCVLOWAT option to define the minimum number of bytes (watermark) to permit the read operation. If you need to enforce the maximum size too adjust the receive buffer size using SO_RCVBUF.  I think you may look at netfilter. Using its (kernel level) api you can have your custom code triggered by network events modify received messages before passing it to application and so on. http://www.netfilter.org/
512,A,"Write to c:\ on another computer I'm not even sure if this is possible but thought I'd ask anyway. Sorry in advance I'm sure the question could be worded better. Is it possible to write a file to the C: drive of another computer on your network? The C: drive of PC1 has a user account with full access rights called User1. User2 (who is logged into PC2) knows the username and password of user account User1. Can User2 write a file to the C: drive of PC1. Hope that makes sense. If you have any question I'll try my best to answer them. Thank you for everyone's answers. EDIT After doing a little more digging I think I've found the answer but I can't test until I get to work tomorrow. The answer - command line ""NET USE Z: \PC1\C$\ /user username1 /pass password1"" It's not possible unless the other computer is sharing the C:\ out on the network. If it is shared you can access it using \\computername\share (where ""share"" is the name of the c:\ share and ""computername"" is the network name of your computer.) As ChrisF points out in the comments there is a default share named C$ that you might be able to use. What about the ""\\machinename\c$"" ""share"" that admins have access to? @ChrisF: True but those can be disabled. It really depends on the particular computer. @chris - Say all the settings are correct to do this. What kind of thing would i put to a batch file. Say the user account's username is user1 and it's password is password1 and I want to copy the file C:/folder1/file.txt to C:/folder2 on the other PC. networking questions like this belong on serverfault not stackoverflow but the command you're looking for is [`NET USE s: \\computername\C$ password /USER:username`](http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/net_use.mspx?mfr=true). After running that command the contents of the remote C: drive would be visible as the local S: drive. Run `NET USE s: /DELETE` when you're done using it to terminate the connection. @Ash - you need to look into how you impersonate another user in the language you are using. Then just have ""\\machinename\c$\folder2\"" as the target path. @ben - Will try that tomorrow at work. Thank you in advance if it works :)  If you shared the C: drive on PC1 you could access the network drive just like a local drive..."
513,A,"Socket Descriptor Changed After Call to recvfrom EDIT: Removed code/explanation because this project has been given again and students can easily find the solution via this post. To clarify what happened I simply passed the wrong length/size in my recvfrom() call. In this line: if(recvfrom( temp->sockfd sendHostIP BUFFER_LEN 0 (struct sockaddr *)&recvAddr &recvLen) < 0) errorMsg(""recvfrom""); You pass BUFFER_LEN as the length (256) but sendHostIP is only of length MAXHOSTNAMELEN (64). This causes recvfrom() to overflow that buffer. The same problem occurs when you read in to localHostIP. Oh man that's right. I must have copy/pasted the array size incorrectly. Thanks!"
514,A,How can one decipher the IANA port registration form? I have created a simple Twisted based TCP server and Android client. I have used my own protocol for this purpose. While I have tried to lay out my protocol in a way which seems logical this is the first protocol that I have written so I doubt it conforms to any standards for a TCP protocol if such norms even exist. However I did design it so that I could easily add features without breaking backwards compatibility with older versions of the protocol. To avoid a potential port conflict I would like to register a port with IANA. I found the registration form here. However the questions seem to be highly technical and I only have a basic knowledge of protocols. So my question is this: Can anyone describe what these questions are asking? And how one would answer them in general? If possible please also include how the general answers would relate to my protocol. That way anyone else who has a similar question can see both types of answers. Thanks Sean W. I don't wish to be unkind but unless you can answer those simple questions you really aren't ready to register a port number request with IANA. Are you sure you need to? Couldn't you get away with configuration of the server and client? or a naming service? I suppose you are right. However I don't want to use dynamic ports because of the (rather small I admit) chance of a conflict. However surly there are resources where I can acquire the knowledge I would need to answer those questions? Can you update your answer with some resources to point me in the right direction? It would be a useful thing to learn if I want to write more complex applications anyway. Thanks. Actually now that I think more carefully about it. I only need the definitions of message formats message types and op codes. I know how to fill out the rest of the form.
515,A,"Large file transfer with sockets When i m transfering a large file using socket programming the received file is incomplete i.e. it is an mp3 file which when i play sounds weird. The code is: Server side: File myFile = new File(""abc.mp3""); { Socket sock = servsock.accept(); int packetsize=1024; double nosofpackets=Math.ceil(((int) myFile.length())/packetsize); BufferedInputStream bis = new BufferedInputStream(new FileInputStream(myFile)); for(double i=0;i<nosofpackets+1;i++) { byte[] mybytearray = new byte[packetsize]; bis.read(mybytearray 0mybytearray.length); System.out.println(""Packet:""+(i+1)); OutputStream os = sock.getOutputStream(); os.write(mybytearray 0mybytearray.length); os.flush(); } Client side: int packetsize=1024; FileOutputStream fos = new FileOutputStream(""zz.mp3""); BufferedOutputStream bos = new BufferedOutputStream(fos); double nosofpackets=Math.ceil(((int) (new File(""abc.mp3"")).length())/packetsize); for(double i=0;i<nosofpackets+1;i++) { InputStream is = sock.getInputStream(); byte[] mybytearray = new byte[packetsize]; int bytesRead = is.read(mybytearray 0mybytearray.length ); System.out.println(""Packet:""+(i+1)); bos.write(mybytearray 0mybytearray.length); } sock.close(); bos.close(); On the Client side i hav used new File(""abc.mp3"")).length just for simplicity.(I could send the length of the file from the server side). This code works perfectly if client and server are the same machine but the file gets distorted if they are on different machines. If you have a buffered output stream why are you manually buffereing it? Why don't you just read the entire thing in and write it all out at once? isnt there a limit to the size of the buffer.So i m dividing the file into chunks n sending them. @anonymous Of course there's a limit but you don't have to write all that to get chunked transfers. It will happen automatically. The canonical way to copy a stream in Java: int count; byte[] buffer = new byte[8192]; while ((count = in.read(buffer)) > 0) { out.write(buffer 0 count); } Works with any buffer size greater than zero. The temptation to relate the buffer size to the input size should be strenuously avoided.  Don't reinvent the wheel use IOUtils.copy(). +1 for telling to not reinvent the wheel  I think that the problem is that you are ignoring the values returned by various read calls and assuming they completely fill the buffer. This is problematic: When reading from a file the last read probably won't fill the buffer. When reading from a socket any read may return before filling the buffer. The net result that your writes will put junk into the stream (at the server end) and into the destination file (at the client end). Furthermore it is pointless dividing the file up into chunks based on the size of the file. Just read until you get to the end of file.  Don't use packets. Try using the ByteArrayOutputStream instead of using the static byte array. keep reading from the inputstream until u reach EOF. n write each of this to the ByteArrayOutputStream.  InputStream is = sock.getInputStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int byteToBeRead = -1; while((byteToBeRead = is.read())!=-1){ baos.write(byteToBeRead); } byte[] mybytearray = baos.toByteArray(); bos.write(mybytearray 0mybytearray.length); Hope this helps. @Sujay: it doesn't matter all that much. 8192 is a convenient number not too big not too small. The loop I posted will work for any buffer size > 0. Why? Arbitrary files don't necesarily fit into memory at all. It is better practice to assume they won't and proceed accordingly. Your solution has nothing to recommend it: redundant initializations; added latency; unbounded memory costs; and it is more code than the alternative I posted that has none of these problems. @EJP hw did u fix the size of the buffer?? @Sujay:I tried your code on the client side it gives a connection reset exception and also the file which is created on the client side is of size 1kb. I had come up with a soln but there was a dataloss.What i did was replaced the for loop of defaultpacketsize on the server with a while loop (bis.available!=0) n on the client size the same loop excpet while(true). if the file was 5 mb then 4.95mb was only getting copied. @EJP The read inside the while loop does not necessarily copy all the bytes being transfered by the client.I printed the count variable n each time a diff value was being printed. @anonymous123: Of course it does. It copies exactly and every byte read until EOS. Your observation doesn't prove anything. The amount read per read can vary arbitrarily. Specifically you cannot assume that the buffer was filled. What was the total bytecount transferred? And using available() in these loops is not correct. And if you are commenting on my answer you are doing so in the wrong place."
516,A,"Async Sockets example which shows passing an object? I need to create a server process which can push high frequency data (1000 updates per second) to around 50 client. I'm thinking the best way you do this is using async sockets with the SocketAsyncEventArgs type. The client -> server connections will be long running at least several days to indefinite. I plan to have a server process listening and the clients connect and the server starts pushing the data to the clients. Can someone point me to or show me an example of how to do this? I can't find any example showing a server process pushing an object to a client. EDIT: This is over a gigibit LAN. Using windows server with 16 cores and 24gb ram thanks You cannot make this work reliably on commodity networking hardware and operating systems. You're off by about two orders of magnitude at least without specifying what kind of latency and service interruptions you are willing to put up with. @Hans Passant: You are wrong ;-) Async sockets can handle ten of thousands of connections. OS is windows server. 16 cores 24gb ram. This is not a connection problem it is a frequency problem. Put all 1000 updates together in *one* notification that you transmit once a second and you won't have a problem. Google ""Nagle's algorithm"" if you're convinced that 1000 updates/sec is important for example. Yes - I was going to push every 250ms You can certainly reliably push TCP data to a client within a LAN at 4 updates per second so long as the data per push doesn't exceed several megabytes. What about rendezvous or 29 west? It would save reinventing the wheel. Dunno about amqp or zeromq they might work fine too....  First some more requirements from your side is required. You have server with lots of muscle but it will fail miserably if you don't do what has to be done. can the client live without some of the data? I mean does the stream of the data need to reach other side in proper order without any drops? how big is 'the data'? few bytes or? fact: scheduling interval on windows is 10 msec. fact: no matter WHEN you send clients will receive it depending on lots of stuff - network config number of routers in-between client processor load and so on. so you need some kind of timestamping here Depending on all this you could design a priority queue with one thread servicing it and sending out UDP datagrams for each client. Also since (4) is in effect you can 'clump' some of your data together and have 10 updates per second of 100 data. If you want to achieve something else then LAN will be required here with lots of quality network equipment. Transport needs to be order and guarantee so udp won't do. This is over a gigibit LAN. Using windows server with 16 cores and 24gb ram Well that leaves you with TCP and TCP won't allow you that what you are trying to do - sending 1000 somethings in a second - it has its own buffers and will repack the data as it seems fit. You will have to lose some requirements unfortunately. After reading a comment about the GIGABIT LAN I am sure that UDP is the only way to go. Try to do anything from one comp to another using TCP and you'll see that timing of the data arrival is broken. What is the size of one 'data'?  If you want to use .NET Sockets to create this server-client project then this is a good outline of what's needed: Since the server will be transferring data to several clients simultaneously you'll need to use the asynchronous Socket.Beginxxx methods or the SocketAsyncEventArgs class. You'll have clients connect to your server. The server will accept those connections and then add the newly connected client to an internal clients list. You'll have a thread running within the server that periodically sends notifications to all sockets in the clients list. If any exceptions/errors occurs while sending data to a socket then that client is removed from the list. You'll have to make sure that access to the clients list is synchronized since the server is a multithreaded application. You don't need to worry about buffering your send data since the TCP stack takes care of that. If you do not want to buffer your data at all (i.e. have the socket send data immediately) then set Socket.NoDelay to true. It doesn't seem like you need any data from your clients but if you do you'd have to make sure your server has a Socket.BeginReceive loop if using Socket.BeginXXX pattern or Socket.ReceiveAsync method if using SocketAsyncEventArgs. Once you have the connection and transmission of data between server and client going you then need to worry about serialization and deserialization of objects between client and server. Serialization which occurs on the server is easy since you can use the BinaryFormatter or other encoders to encode your object and dump the data onto the socket. Deserialization on the other hand which occurs on the client can be pretty complex because an object can span multiple packets and you can have multiple objects in one packet. You essentially need a way to identify the beginning and end of an object within the stream so that you can pluck out the object data and deserialize it. One way to do this is to embed your data in a well known protocol like HTTP and send it using that format. Unfortunately this also means you'd have to write a HTTP parser at the client. Not an easy task. Another way is to leverage an existing encoding scheme like Google's protocol buffers. This approach would require learning how to use the protocol buffers stack. You can also embed the data in an XML structure and then have a stream-to-XML decoder on your client side. This is probably the easiest approach but the least efficient. As you can see this is not an easy project but you can get started with the Socket.BeginSend examples here and here and the SocketAsyncEventArgs example here Other Tips: You can improve the reliability of your communication by having the client maintain two connections to the server for redundancy purposes. The reason being that TCP connections take a while to establish so if one fails you can still receive data from the other one while the client attempts to reconnect the failed connection. You can look into using TCPClient class for the client implementation since it's mostly reading a stream from a network connection."
517,A,How to detect Bluetooth activated laptops in range in Windows using C# I'm interested in creating a wireless network of laptops using Bluetooth developed in C#. I want to get the list of Bluetooth activated devices in range ( preferable computers not dongles headsets or phones). I don't think I need to use a Bluetooth adapter specific stack coz I have seen Windows searching for Bluetooth devices in range. How can I run such a search in my C# app and list the computers in range? Please help me to do this also let me know if my effort is wrong. Thank you. There is a nice C# Bluetooth library available in 32feet.NET. If you have no need for any of the additional features it offers and you wish to do the P/Invoke on your own documentation can be found here; specifically the BluetoothFindFirstDevice and BluetoothFindNextDevice for device discovery. Additionally you can use windows sockets to do device/service discovery. Information on that can be found here. If you wish to specifically filter out types of Bluetooth devices you will want to look at the ulClassOfDevice field in the BluetoothDeviceInfo. Thank you very much for your answer and very helpful links which im going through now. Is there anyway that I could ask windows to do the search for me and then filter the results through my program? is it possible? Like we can do to get the Printer info etc. Thanks again.
518,A,Mesaure upload and download speed in iPhone I would like to measure the upload and download speed of data in iPhone is any API available to achieve the same? Is it correct to measure it on the basis of dividing total bytes received with time taken in response? There is no direct API to know the speed. Total data received/sent and time only will give you average speed. There use to be lot of variation in the speed over the time so if you want more accurate value then do the speed calculation based on sampling. (Data transferred in 1 miniut) /(60 seconds) ---> this solution only if you need greater accuracy in the speed calculation. The sampling duration can changed based on the level of accuracy required.  Yes it is correct to measure the total bytes / time taken that is exactly what the speed is. You might want to take an average if you want to constantly show the download speed.. like using 500 bytes and the time it took to download those particular ones. For doing this you could like have an NSMutableArray as a buffer which you empty idk every 2 seconds. Then you do [bufferMutableArray length]/2 and you know how many bytes a second you had those 2 seconds. When you empty the buffer ofc append to the data you are downloading.
519,A,"gsoap client multiple ethernets I have a linux system with two eth cards. eth0 and eth1. I am creating a client that sends to endpoint 1.2.3.4. I send my webservice with soap_call_ functions. How can I select eth1 instead of eth0? the code is like that soap_call_ns__add(&soap server """" a b &result); How can I set inside the &soap variable the eth0 or the eth1? (gsoap does not have a bind for clients... like soap_bind) You want outgoing packages from your host to take a specific route (in this case a specific NIC)? If that's the case then you have to adjust kernels routing tables. Shorewall has excellent documentation on that kind of setup. You'll find there info about how to direct certain traffic through a particular network interface. how is that done? @cateof I've edited my answer and added a link to shorewall's howto. Please read the howto very carefully. Routing has always been a little bit tricky. :)  for gsoap we need to manually bind(2) before connect(3) in tcp_connect"
520,A,"Live monitoring of network connectivity status A new client has recently asked me to develop a Windows service that will monitor whether or not his server has internet connectivity. The service should log when the connection goes down to Local Only and back up to Internet Access as shown in Network and Sharing Center. My original idea was to have the service ping a website like google every 5 minutes or so but I don't know how to retrieve the results of the ping so I then thought I could code a WebBrowser control into it and write the log entries based on the results of connection attempts from that. This also seems like a rather impractical idea though so can anyone suggest the best way to go about this? Take a look at this answer this and... Probably you may use WinAPI funcs InternetGetConnectedState and InternetCheckConnection Correct declarations of them you can find at pinvoke.net  Since you are working in windows this is what you can try. http://www.go4expert.com/forums/showthread.php?t=2557 For linux a simple bach script will do the job. #!/bin/bash WGET=""/usr/bin/wget"" $WGET -q --tries=10 --timeout=5 http://www.google.com -O /tmp/index.google &> /dev/null if [ ! -s /tmp/index.google ];then echo ""no"" else echo ""yes"" fi"
521,A,"Flash client XMLSocket not connecting to server I have a Flash client that I want to connect to a server. Both are using localhost and port 50000 so there shouldn't be any cross-domain problems. I also set Access Network Only in the publishing settings. When I call the XMLSocket connect the server seems to get a new connection. But the XMLSocket.onConnect callback is not called with success=true. Any ideas on what may be wrong? Here's the ActionScript for creating the socket.  function myOnConnect(success) { if (success) { trace (""Connection succeeded!"") inputText.text = ""open""; // socket.send(""1\n""); gotoAndPlay(2); } else { trace (""Connection failed!"") inputText.text = ""failed""; } } btnConnect.onRelease = function() { inputText.text = ""started""; result = socket.connect(""localhost"" 50000); } socket = new XMLSocket(); socket.onConnect = myOnConnect; Looks fine but I'm suspicious about ""localhost"". Try using null (per the ActionScript Docs for a ""same machine"" connection) and see if that works. I run it in the Flash debugger and it connects immediately. I wonder if there is a security setting I need for it to work in a browser. It doesn't work in IE or Firefox. Just for funzies I also tried switching the Publishing setting to Access Local Files only. Didn't work. Using null for same machine didn't work. Thanks for the idea though. Firewall maybe? Never mind. If you can connect via the Flash debugger connectivity shouldn't be an issue though security still might be. After digging around the net I found this on socket policy files. I think this is an issue. http://www.adobe.com/devnet/flashplayer/articles/socket_policy_files.html I found this article about policy files too. http://gruchalski.com/2009/06/11/xmlsocket-send-socket-writeutfbytes-doesnt-work/ That's annoying (and weird that it works in the debugger but not ""live""). Let me know if setting things up this way works (by answering the question of course). This ended up being a security problem. The Flash Player has added security when a XMLSocket is used. The Flash Player now looks for a policy file on port 843. An alternative is to have the swf look for the policy file using the call Security.loadPolicyFile(). If the file exists and all the security settings permit the XMLSocket then the connection is created. Check out the Adobe article on Policy files and more info here. This is another good article about policy files. Here is the policy file that finally worked for me. It is not restrictive at all. But I figured I get things working and then make them right. <?xml version=""1.0""?> <!DOCTYPE cross-domain-policy SYSTEM ""/xml/dtds/cross-domain-policy.dtd""> <!-- Policy file for xmlsocket://socks.example.com --> <cross-domain-policy> <!-- This is a master socket policy file --> <!-- No other socket policies on the host will be permitted --> <!-- <site-control permitted-cross-domain-policies=""all""/> --> <!-- Instead of setting to-ports=""*"" administrator's can use ranges and commas --> <!-- This will allow access to ports 123 456 457 and 458 --> <allow-access-from domain=""*"" to-ports=""*"" secure=""false""/> </cross-domain-policy>"
522,A,"C# Remote Server Unique Identifier for License I am writing a basic licensing class for our program. I would like to tie this license to the server so they cannot move the license and expect it to work. The program is run locally on each workstation from a shared drive on the server. Majority of our customers run on a peer 2 peer network since they are smaller companies and only have a few computers. I'd like to get a unique identifier for the server so when the workstation starts the application it will verify the license is running against the correct server. I have almost completed the class except I cannot figure out how to get a consistent unique identifier for the server. Also I am writing this in C#.net. Any suggestions? Thanks! C# can be easily decompiled and modified it will be really easy to make it work anywhere no matter how unique your identifier will be Sounds like you want the computer's Security Identifier (SID). string fqdn = System.Net.Dns.GetHostEntry(System.Net.Dns.GetHostName()).HostName; int firstDot = fqdn.IndexOf('.'); string host = fqdn.Substring(0 firstDot); string domain = fqdn.Substring(firstDot + 1); NTAccount account = new NTAccount(domain host + ""$""); SecurityIdentifier sid = (SecurityIdentifier)account.Translate(typeof(SecurityIdentifier)); // we're done show the results: Console.WriteLine(account.Value); Console.WriteLine(sid.Value); (Code to get identifier copied from here. Tested though. It works.) In reference to the discussion below the way that would be most ""gentle"" to your users is to license by host name. This is the way dotNetCharting does it (or did it a few years ago when I bought their software). Basing on hardware or the computer's SID is prone to change and necessitates relicensing/activation when the hardware changes or the OS is re-installed. The first line in the code above returns the fully qualified domain which you could use for this purpose. Thats one way yes but would be ""reset"" with each OS install. Yup. But if you change hardware your licensing is gone too. Not an clean answer other than tying to host name maybe. But that would be relatively easy to game. True and you can do like microsoft and support X changes. But if you changed mothboard and CPU its a new machine in my book I keep getting ""Some or all identity references could not be translated."" I've tried all of the domains and computers on our network. Any suggestions? How would I get information motherboard and cpu like that remotely on a p2p network? Write out what you're getting for the domain and computer name? I don't have a pc available that's not part of a domain to test on at the moment unfortunately. This is a great start. I have most of what I want. I just need to grab a few more items. Thanks for a push in the right direction.  Get the serial number of the CPU and system harddrive and mainboard then hash it. You can off course mix up what hardware serials/models information you take. Article on codeproject that explains how to get the information: http://www.codeproject.com/KB/system/GetHardwareInformation.aspx"
523,A,What is the maximum packet size a python socket can handle? Hey hi guys i am new to network programming in python. I wanted to know that what is the maximum size packet we can transmit or receive on python socket? and how to find out it? I think it depends on the protocol that you plan to use. The actual amount of data that can be sent in a single packet depends on what the Maximum Transmission Unit (MTU) is for the protocol you're using. Read the Wikipedia article for more information. This is generally something you don't have to worry about though - if you send a TCP packet that's too big the operating system will fragment it (turn it into multiple packets) for you and it will be reassembled at the host. By the way Python's socket library uses the operating system's sockets so it's nothing particular to Python.  i think they are not at the same level if you want to manipulate the raw package take a look at dpkt & scapy.  I don't think there's any Python-specific limits. UDP packets have a theoretical limit of circa 65kb and TCP no upper limit but you'll have flow control problems if you use packets much more than a few kilobytes.
524,A,"Multiplayer game: how to sync FPS of different PC? i'm wondering how can i ""sync"" the speed (obviously to the less fast) of different PC ? My is a 2d racing game written in C# + SDL.NET. How to ""move"" object on screen at the ""same"" speed on all PCs sending ""state"" variable only 3 times per second (for instance) ? Thanks http://gamedev.stackexchange.com/ This is the friendliest article I've read on the matter. You should look at all the game physics articles from this guy (GafferOnGames) Fix Your Timestep! Even an RK4 integrator is sensitive to the amount of time you step when integrating. Decouple your physics timestep from the display framerate so that your simulation behaves exactly the same way each time it is run. ...then continue with this one: What every programmer needs to know about game networking A brief overview of the history of PC multiplayer games. Discover how RTS games were able to synchronize thousands of units over a 28k modem and how first person shooters hide lag with client side prediction and latency compensation. Hope you'll find it as eye-opening as I did.  Here's a great article on game loops. It shows a bunch of different ways to do things and the final one will allow the processing on all pc's to happen in sync regardless of fps. http://www.koonsolo.com/news/dewitters-gameloop/  maybe this can help"
525,A,C cross-platform network library Is there any cross-platform library written in C for networking with TCP/IP? I need a simple way to work with sockets to send and receive data. does windows not have `socket.h`? i thought BSD sockets were universal... but then i've never written a line of code for windows w/o cygwin so what do i know @tobyodavies: Windows has BSD sockets but there are few differences. For instance `close` vs `closesocket`. If one wants to go further WSA is a completely new world. I'm still looking for some universal library that would make programming transparent for linux/windows and C. I know QT Sock do this for C++ but not C. The sockets API is the standard cross-platform networking API. Windows has its own version Winsock which is almost identical. There's an excellent description of how to use this API at Beej's Guide to Networking which is perhaps the best resource on this topic. Hope this helps! Downvoter- Can you please explain what's wrong with this answer? I'd be happy to improve it but I'm not sure what you disagree with.  You can use: Netscape portable runtime Apache Portable Runtime
526,A,"Java UDP - Packet Send Problem Everytime I try to send a UDP packet to an address I get the following exception: java.lang.ArrayIndexOutOfBoundsException at java.net.PlainDatagramSocketImpl.send(Native Method) at java.net.DatagramSocket.send(DatagramSocket.java:625) at services.servicetypes.network.host.Server_UDP_Thread_Monitor.send(Server_UDP_Thread_Monitor.java:84) Which points directly to the socket send function - this.socket.send(packet); After placing a break point on my datagram packet I get the following info: packet DatagramPacket java.net.DatagramPacket@52c4c57 byte[] #1248(length=16) offset int 0 length int 16 bufLength int 16 address Inet4Address /192.168.0.101 Static Inherited port int 3889 I'm not clear on why this is occuring if anyone could shed any light on the problem that would be great. My inital thoughts were perhaps one of the datagram packet elements was empty. (Basic source) ByteArrayOutputStream byteOut = new ByteArrayOutputStream(); DataOutputStream out = new DataOutputStream(byteOut); // Game type out.writeInt(1); // Client out.writeInt(gamerToSend.getClientID()); // Position out.writeFloat(gamerToSend.getX()); out.writeFloat(gamerToSend.getY()); DatagramPacket packet = new DatagramPacket ( byteOut.toByteArray() byteOut.size() gamerToSend.getInetAddress() 3887 ); this.socket.send(packet); byteOut.close(); out.close(); INetAddress info: (Ip is correctly displaying destination machine) address Inet4Address /192.168.0.101 /192.168.0.101 Static INADDRSZ int 4 4 serialVersionUID long 3286316764910316507 3286316764910316507 loopback int 2130706433 2130706433 IPv4 int 1 1 IPv6 int 2 2 preferIPv6Address boolean false false nameService InetAddress$1 java.net.InetAddress$1@5bf0cf51 java.net.InetAddress$1@5bf0cf51 serialVersionUID long 3286316764910316507 3286316764910316507 addressCache InetAddress$Cache java.net.InetAddress$Cache@1ebafdff java.net.InetAddress$Cache@1ebafdff negativeCache InetAddress$Cache java.net.InetAddress$Cache@679801c java.net.InetAddress$Cache@679801c addressCacheInit boolean true true unknown_array InetAddress[] #1266(length=1) #1266(length=1) impl Inet6AddressImpl java.net.Inet6AddressImpl@12c9b196 java.net.Inet6AddressImpl@12c9b196 lookupTable HashMap ""size = 0"" ""size = 0"" $assertionsDisabled boolean true true Inherited hostName address int -1062731675 -1062731675 family int 2 2 canonicalHostName caport int 3889 3889 Server source http://pastebin.com/JCdjhFQM You need to post your code so we can see what you're doing. Apologies forgot source code added. The error is not in the part you posted. I wrapped your source in an example program and it works without any problems. package de.fencing_game.paul.examples; import java.io.*; import java.net.*; /** */ public class UDPTest { private static class Gamer { int clientID; float x; float y; InetAddress address; public int getClientID() { return clientID; } public float getX() { return x; } public float getY() { return y; } public InetAddress getInetAddress() { return address; } } private DatagramSocket socket; public UDPTest() throws IOException { this.socket = new DatagramSocket(); } public void sendPackage(Gamer gamerToSend) throws IOException { ByteArrayOutputStream byteOut = new ByteArrayOutputStream(); DataOutputStream out = new DataOutputStream(byteOut); // Game type out.writeInt(1); // Client out.writeInt(gamerToSend.getClientID()); // Position out.writeFloat(gamerToSend.getX()); out.writeFloat(gamerToSend.getY()); DatagramPacket packet = new DatagramPacket ( byteOut.toByteArray() byteOut.size() gamerToSend.getInetAddress() 3887 ); this.socket.send(packet); byteOut.close(); out.close(); } public static void main(String[] ignored) throws IOException { Gamer g = new Gamer(); g.address = InetAddress.getByName(""localhost""); UDPTest sender = new UDPTest(); sender.sendPackage(g); } } What are you doing different?  thanks for your help. It appears the InetAddress is corrupted somewhere as manually setting it to the remote machine like so: InetAddress addr = InetAddress.getByName(""192.168.0.101""); Works perfectly fine so I have more debugging to do but thats the problem. Again thanks for the assist! Make sure to accept your own answer so the question is closed.  I suspect it has something to do with using the DataOutputStream which is buffered then passing the underlying byte[]. Try out.flush() before creating your packet. I've tried but same error as before. out.flush(); DatagramPacket packet = new DatagramPacket.... The problem is that obviously the length isn't matching the byte[] you're passing in. Before your packet do a `byte[] foo = byteOut.toByteArray();` and pass that in with `foo.length` as the size. Or just compare `byteOut.size()` to that length and see if there's a difference. byte[] foo = byteOut.toByteArray(); System.out.println(""1~"" + byteOut.size()); System.out.println(""2~"" + foo.length); 1~16 2~16 No difference... You write 16 byte but your packet has only 13 byte (from the debugging data you posted). My bad I added some additional data by mistake. Both now show 16 bytes but the issue remains. Got me then. 16 bytes is correct for what you put into the buffer (4 + 4 + 4 + 4). According to the docs that should work. Unless you're doing something odd creating the InetAddress which I don't know how you would. Or something odd with the socket? http://pastebin.com/JCdjhFQM < that's the basic code for my server I've also added the debug output of the InetAddress in my first post. Note the other person's answer. Runs fine on my machine as well. What version of java and OS are you on? It's the native library that's puking."
527,A,"What kind of SCTP support is there on various Windows versions? What kind of SCTP support is there on various Windows versions? Does [Windows.next](http://en.wikipedia.org/wiki/Windows_8) support SCTP? Well yes google was more useful: Does any Support for SCTP? SctpDrv: an SCTP driver for Microsoft Windows  Recently Windows support has been added to this userspace SCTP stack: http://sctp.fh-muenster.de/sctp-user-land-stack.html It will have a paper in 2012 ICCCN describing its implementation.  Out of the box there are none on any versions of Windows.(Microsoft has claimed there is no customer demand so I always encourage anyone looking for SCTP on windows contact Microsoft and express their need for one..) There are 3. party implementations e.g. sctplib Netbricks Sigtran stack SctpDrv +1 for ""express their need for none"" well the microsoft guys wont be using the internet that often or maybe stackoverflow specifically. ""P You might want to watch out for SctpDrv - I kept getting BSoDs on Win7 SP1. Is there any SCTP 3rd party implementations for windows8?. So far as I know SctpDrv does not.  From the Wikipedia article: The following operating systems implement SCTP: AIX Version 5 Generic BSD with external patch at KAME project Cisco IOS 12 DragonFly BSD since version 1.4 FreeBSD version 7 and above HP-UX 11i v2 and above Operating systems using Linux kernel 2.4 and newer QNX Neutrino Realtime OS 6.3.0 and above Sun Solaris 10 and above Various third-party implementations of SCTP exist for other operating systems. FreeBSD contains the SCTP reference implementation. Userspace library: The SCTP library (sctplib) with a Windows XP port. So apparently there is no native support in Windows at all. Only through 3rd-party software or libraries. What other libraries are available and is there anything available for Vista and Win7? Since sctplib is a userspace library it should work without problems on Vista or Windows 7 too. Apart from that did you try Google already? I hear they return all kinds of fancy results when searching for ""windows sctp library""."
528,A,"Using TUN/TAP to read incoming data encapsulate as UDP and transmit I have a tun/tap device which is used to read incoming packets from one interface and send them as UDP packets via another interface. I could implement this and could read ICMP pakcets send to the tun/tap interface and also get them remotely using UDP. But the issue happens when I try to change the default gateway of the input interface to the tun/tap device so that I can read all the incoming data from the tun/tap. When this is done I cant send the UDP packets as the routing isnt proper. I also tried to you the ""SO_BINDTODEVICE"" option in socket comm but still didnt work. Please note that I havent used the write() method in the tun/tap. I just used the read() function collected the data and send them via UDP socket communication. Please let me know if my approach is wrong or any other work around to overcome this. Thanks. /********More Details********/ Thanks Rob. What I am trying to achieve is a simulation of IP based header commuication(ROHC) in a high latency channel. For this I have 4 virtual machines. VM1 is a normal desktop machine. VM2 is a gateway which takes the packets using tun/tap(from VM1) and does the UDP based communication with VM4. VM3 is the channel where parameters like latency error rate etc can be set. VM4 is connected to the WAN. The user in VM1 should be able to browse the WAN just like normal. Please find the diagram below.  IP Packets | | +------------------+ +--------------+ +----------------+ '---|eth1..... | | | | | | | | | | | | | tun/tap | | eth0|___|UDP Sock eth0|___ | | | | | | | | | | | ..UDP Sock|_____|eth1 | | | | | | | | | | | +tun/tap+ | ' +------------------+ +--------------+ +----------------+ WAN VM2 VM3(Channel) VM4 Update: Thanks Tommi. Your solution worked. I could get the UDP packets one way to the final NAT gateway. But I could not get the reverse way to work till now. I tried enabling the masquerade using iptables and also setting up the host route to the tuntap at VM1 but it wasnot working. I have a few queries regarding this. 1) In VM4 I receive the UDP data and write to the tun/tap. This will get routed to the WAN by the kernel. But for the incoming packet do I again need to read using the tun/tap? In this case do I need to make the read and write in different threads? I am asking this because I need to transport them back also as UDP data. Let me know if I am missing something here. Once again thanks a lot for your help. Welcome to Stack Overflow. You've done a good job describing what you have tried. Can you tell us what you are trying to accomplish? What is the function of the program you are trying to write? I suspect that the design you have chosen doesn't allow your program to do what you originally intended. Please edit your question and add to the bottom of the question your general goals for your program. Your udp packets will get routed to your tuntap interface too. (well depending on some settings they may just get discarded). You need to add a route rule for the udp peer you are sending them to a host rule or a smaller network rule that wont interfere with your other communication. Thanks Tommi. I will try this way and will let you know. Hi Tommi. Please see the update above as I could not write the whole thing in the comment section here. Thanks. Now I see what you are trying to accomplish. Few additional words of advice: remember to put a MASQUERADE rule to VM4 eth0 add a similar host rule to vm4 using the VM1 IP and tun/tap interface for vm2 it should be sufficient that both its eth1 and eth2 networks are in routing table (with netmasks other than default) and then you can indeed set the default route to the tun/tap device that way your udp packets to vm3 and the tunneled traffic with vm1 will find their routes. I've done stuff like a lot feel free to ask if it doesn't seem to work. Also I might have overlooked something. Hi Tommi Finally I could get the thing to work. As you said the route at the VM4 pointing back to the fist tun tap is right. It wasnt working for me as there was a routing issue between all these interfaces. Now thats fixed and its working fine. For my second query of needing a thread I have used the select() fuction call to multiplex the read and write request. I have taken this idea from [link]http://www.secdev.org/projects/tuntap_udp/ and from simple tun program of David Brini. Thanks again for ur help :-) Hi Tommi Just a reminder in case you missed the previous comment. Can you please let me know if it makes sense to implement the read and write functions of tun/tap in different threads? Thanks. Sorry I was away for a few days. Glad things worked out for you. I suggest epoll. A lot like select but a lot nicer. Threads tend to complicate things needlessly. I mean when you need threads you need threads but when you don't really need them you _really_ don't want them around. :) Ok the way I see the packet flow for return packets. 1) the vm4 must masquerade the tuntap packets sent out through eth0 to WAN. 2) they then come back to vm4 kernel ""unmasquerades"" them and route them to the tuntap device 3) your epoll fires epollin and you read the packet then encapsulate it to udp and send it to vm3 which toys around with it and chucks in onward to vm2.4) vm2 decapsulates the packet and you write out to the tuntap interface 5) vm2 sends it out through eth1 (assuming the original address was from eth1 lan)"
529,A,"Python Interpreter blocks Multithreaded DNS requests? I just played around a little bit with python and threads and realized even in a multithreaded script DNS requests are blocking. Consider the following script: from threading import Thread import socket class Connection(Thread): def __init__(self name url): Thread.__init__(self) self._url = url self._name = name def run(self): print ""Connecting..."" self._name try: s = socket.socket(socket.AF_INET socket.SOCK_STREAM) s.setblocking(0) s.connect((self._url 80)) except socket.gaierror: pass #not interested in it print ""finished"" self._name if __name__ == '__main__': conns = [] # all invalid addresses to see how they fail / check times conns.append(Connection(""conn1"" ""www.2eg11erdhrtj.com"")) conns.append(Connection(""conn2"" ""www.e2ger2dh2rtj.com"")) conns.append(Connection(""conn3"" ""www.eg2de3rh1rtj.com"")) conns.append(Connection(""conn4"" ""www.ege2rh4rd1tj.com"")) conns.append(Connection(""conn5"" ""www.ege52drhrtj1.com"")) for conn in conns: conn.start() I dont know exactly how long the timeout is but when running this the following happens: All Threads start and I get my printouts Every xx seconds one thread displays finished instead of all at once The Threads finish sequentially not all at once (timeout = same for all!) So my only guess is that this has to do with the GIL? Obviously the threads do not perform their task concurrently only one connection is attempted at a time. Does anyone know a way around this? (asyncore doesnt help and I'd prefer not to use twisted for now) Isn't it possible to get this simple little thing done with python? Greetings Tom edit: I am on MacOSX I just let my friend run this on linux and he actually does get the results I wished to get. His socket.connects()'s return immediately even in a non Threaded environment. And even when he sets the sockets to blocking and timeout to 10 seconds all his Threads finish at the same time. Can anyone explain this? Have you tried just using socket.getaddrinfo(host port) to see if that has the same limitation? I unfortunately can't replicate this because it's a dns problem. In most cases you should get a ""gaierror: (-2 'Name or service not known')"" rather quickly. yea I have tried this and it has the same limitation. I'm pretty sure that OSX uses getaddrinfo from the BSD libraries which probably has the restriction listed by below. ""isn't it possible to get this simple little thing done with python?"" Well twisted is python. It is a pure-python library. Just check its source code to see how it does. i wonder if twisted will suffer under the same limitations when run on mac os X It won't. Twisted.names is done in all python and doesn't call the hosts getaddrinfo function at all. From the api docs ""The functions exposed in this module can be used for asynchronous name resolution and dns queries"". Shit! It's true. I got the same result. I don't know why Python sucks in this area! if it's suitable you could use the multiprocessing module to enable process-based parallelism import multiprocessing socket NUM_PROCESSES = 5 def get_url(url): try: s = socket.socket(socket.AF_INET socket.SOCK_STREAM) s.setblocking(0) s.connect((url 80)) except socket.gaierror: pass #not interested in it return 'finished ' + url def main(url_list): pool = multiprocessing.Pool( NUM_PROCESSES ) for output in pool.imap_unordered(get_url url_list): print output if __name__==""__main__"": main("""""" www.2eg11erdhrtj.com www.e2ger2dh2rtj.com www.eg2de3rh1rtj.com www.ege2rh4rd1tj.com www.ege52drhrtj1.com """""".split()) Well it contains code... so an answer! is that an answer or a question? :P It isn't available in 2.5. It works perfectly.  Send DNS requests asynchronously using Twisted Names: import sys from twisted.internet import reactor from twisted.internet import defer from twisted.names import client from twisted.python import log def process_names(names): log.startLogging(sys.stderr setStdout=False) def print_results(results): for name (success result) in zip(names results): if success: print ""%s -> %s"" % (name result) else: print >>sys.stderr ""error: %s failed. Reason: %s"" % ( name result) d = defer.DeferredList(map(client.getHostByName names) consumeErrors=True) d.addCallback(print_results) d.addErrback(defer.logError) d.addBoth(lambda _: reactor.stop()) reactor.callWhenRunning(process_names """""" google.com www.2eg11erdhrtj.com www.e2ger2dh2rtj.com www.eg2de3rh1rtj.com www.ege2rh4rd1tj.com www.ege52drhrtj1.com """""".split()) reactor.run()  On some systems getaddrinfo is not thread-safe. Python believes that some such systems are FreeBSD OpenBSD NetBSD OSX and VMS. On those systems Python maintains a lock specifically for the netdb (i.e. getaddrinfo and friends). So if you can't switch operating systems you'll have to use a different (thread-safe) resolver library such as twisted's. I dont know what more to say except oh my fuckin god. And thanks a lot I am working on this for the last 6 hours and starting to grow grey hair. Good to know all my code I have trashed (last attempt was asyncore with non blocking sockets) was perfectly fine and it was MacOSX's fault. Thx a bunch. +1 use twisted! I need this done pretty quickly but I just ordered a book on twisted people seem to really like it cant be so bad after all. Will try to get into it a bit but I am really busy lately. There are other Python resolver libraries such as dnspython and pydns which might be simpler to use than twisted. They probably aren't fully thread-safe (creating a new UDP socket for each DNS request and thus potentially exhausting port numbers) but that may not be a problem if you don't perform many queries. I believe curl/libcurl use the c-ares resolver for this same reason."
530,A,"Alternative to Thread.Sleep() before socket read I'm using this FtpClient library to connect to mainframe from WinForms application. I'm using thread.Sleep for the thread to wait for the response before it starts reading otherwise it freezes. Is there an alternative to do this? public void Login() { if (this.loggedin) this.Close(); Debug.WriteLine(""Opening connection to "" + this.server ""FtpClient""); IPAddress addr = null; IPEndPoint ep = null; try { this.clientSocket = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); addr = Dns.Resolve(this.server).AddressList[0]; ep = new IPEndPoint(addr this.port); this.clientSocket.Connect(ep); } catch (Exception ex) { // doubtfull if (this.clientSocket != null && this.clientSocket.Connected) this.clientSocket.Close(); throw new FtpException(""Couldn't connect to remote server"" ex); } **Thread.Sleep(4000);** this.readResponse(); ... } private void readResponse() { this.message = """"; this.result = this.readLine(); if (this.result.Length > 3) this.resultCode = int.Parse(this.result.Substring(0 3)); else this.result = null; } private string readLine() { while (true) { this.bytes = clientSocket.Receive(this.buffer this.buffer.Length 0); this.message += ASCII.GetString(this.buffer 0 this.bytes); if (this.bytes < this.buffer.Length) break; } string[] msg = this.message.Split('\n'); if (this.message.Length > 2) { this.message = msg[msg.Length - 2]; try { response = msg[msg.Length - 3]; } catch { } } else { this.message = msg[0]; } if (this.message.Length > 4 && !this.message.Substring(3 1).Equals("" "")) return this.readLine(); if (this.verboseDebugging) { for (int i = 0; i < msg.Length - 1; i++) { Debug.Write(msg[i] ""FtpClient""); } } return message; } public void sendCommand(String command) { if (this.verboseDebugging) Debug.WriteLine(command ""FtpClient""); Byte[] cmdBytes = Encoding.ASCII.GetBytes((command + ""\r\n"").ToCharArray()); clientSocket.Send(cmdBytes cmdBytes.Length 0); this.readResponse(); } Writing Thread.Sleep() is indeed a bad practice. If you are actually doing a FTP client use FtpWebRequest BeginGetResponse(asynccallback object) And pass the readResonse as a callback. It will be called exactly when the response is ready. Bad practice? I think not. But yes there are other approaches that can work. Ok let's say it's a proof of bad design then http://stackoverflow.com/questions/1457282/alternatives-to-thread-sleep-for-simulating-pauses i dont agree with that article. its speculative about why you use a thread.sleep(). sleeping a thread is in my book a good way of saying ""i dont have anything to do please do process something before i try again"". Still. My idea of a decent sleep is on <100 milliseconds not on seconds or minutes. Looping when you actually need to wait for a result is a waste of CPU time. I think an asynchronous call is a better design.  Yes  if you can work with .NET 4.0 you've the task API. You may learn more by reading this article: http://www.codethinked.com/post/2010/01/25/NET-40-and-SystemThreadingTasks.aspx It provides you a way of creating a flow in which a task waits until previous one has ended. Take a look!  Use async programming model: socket.BeginConnect(ep new AsyncCallback(Connected) socket); void Connected (IAsyncResult result) { var socket = (Socket)result.AsyncState; // do the stuff socket.EndConnect(result); } I'm still getting the same error. clientSocket.Send(cmdBytes cmdBytes.Length 0); //on this line @user558138: Where do you put your `Send`?. Check out also [this example](http://msdn.microsoft.com/en-us/library/bew39x2a.aspx) i've added that method sending the commands Its working for logging in but while sending other commands(upload/download) i'm getting this error ""Cannot block a call on this socket while an earlier asynchronous call is in progress."". @user558138: Sorry forgot to say that you need to call `EndConnect()`"
531,A,"Using WMI to Determine which adapter(s) is connected to the internet I am writing a VB Script that uses WMI to determine which adapter is used for internet connectivity? For example - if I have a LAN and a 3G board it needs to tell the user which is connected. I understand that a machine might have >1 internet connection but for now let's assume 1. edit: Ok how can I do this using any command tool? Given the roaring silence I guess this is not do-able using WMI. :-) Would trace print work? I'm not too familiar with trace. Thanks in advance for any help! Rich Using Win32_NetworkAdapterConfiguration find the network device that has the lowest IPConnectionMetric this will be the first device used for internet access. strComputer = ""."" Set objWMIService = GetObject(_ ""winmgmts:\\"" & strComputer & ""\root\cimv2"") Set colItems = objWMIService.ExecQuery(""Select * From Win32_NetworkAdapterConfiguration"") metric = 500 description = """" For Each objItem in colItems If (objItem.IPConnectionMetric < metric AND objItem.IPConnectionMetric >= 0) then metric = objItem.IPConnectionMetric description = objItem.Description End If Next Set WshShell = CreateObject(""WScript.Shell"") WshShell.Popup(description) VBScript examples for accessing the WMI can be found on MSDN"
532,A,"Control a desktop application using an android phone I want to build an application where an android phone would control a desktop application. I only need to send coordinates from the phone to the desktop when user's finger is on the phone screen. But I am kinda confused on the networking side if i should use bluetooth usb or wifi (intranet). I did some research on bluetooth doing bluetooth socket programming on an android phone shouldn't be a problem but on the desktop side there are only a few free SDK/library. any suggestion on what to use? If I were to use USB/cable What API on the android side I need to use? I am actually more familiar with general socket programming (wifi) but I think it's going to be slow (correct me if Im wrong) so this would be my last option. PS: I am using Java for the desktop application too any suggestions on what method to use? or even maybe I should use .net on the desktop side? Thanks personally i dont see why you would be limited by sending only coordinates on network. this is pretty light. if a youtube video streams ok  coordinates should go well. Also on desktop side. you any language you want that supports network sockets Wifi is probably the most supportable. Bluetooth requires hardware and drivers on the PC side USB would ordinarily seem like the most sensible if the wire isn't a problem but the catch is that it requires that the user enable ""USB debugging"" on the phone and have either the android SDK or equivalent functionality to the adb forward command installed plus a compatible USB driver for the phone. If all that were the case you'd just forward a port from the PC to the phone and have a pc program connect to that port on the loopback interface which will be forwarded to a service running on the phone. It's possible you could do something piggybacked on the USB tethering capability of more recent releases to get you a network-over-usb that you could use to connect programs on the PC and phone but you'd need modified PC drivers so that you don't actually push the PC's internet traffic through the phone (unless you mean to tether as well).  You might find the open source RemoteDroid app to be useful in creating your app. It may even do everything that you want. The source code is here. You may need an svn client like TortoiseSVN in order to download it. wow this is all I need. Yea it seems like using network won't affect performance at all. I will use it as a reference. Thanks"
533,A,"Memory Management and Exception Handling I have a simple class that handles the connection being made between a client and server. To let more than one user communicate with the server at one time each new Client connection is made on a separate thread. In this class I create two streams that act as the inbound and outbound streams for the client. I create the fields first and then initialise the object in a separate method simply because the object is used in several other places. I've come to the point where I want to refactor the code to make it more robust my first port of call was memory management. I've come to love the using() statement but noticed that I can't really see a way to do implement it due to the way the code is structured. This means I have a fairly annoying method that is just used for closing the underlying connections and nothing more. Furthermore I came to implement exception handling and was curious whether the notion of wrapping the entire code in a method with a try{} statement and then having sequential catch() blocks with the applicable exception types was the best idea. I hope I explained myself correctly I'll post a snippet for you to look at. Thanks! //Fields TcpClient tcpClient; //The thread that will send information to the client private Thread thrSender; private StreamReader srReceiver; private StreamWriter swSender; private string currentUser; private string strResponse; //The constructor of the class takes in a TCP connection public Connection(TcpClient tcpCon) { tcpClient = tcpCon; //The thread that accepts the client and waits messages thrSender = new Thread(AcceptClient); //The thread calls the AcceptClient method thrSender.Start(); } private void CloseConnection() { //Close the currently open objects tcpClient.Close(); srReceiver.Close(); swSender.Close(); } //Occurs when a new client is accepted private void AcceptClient() { srReceiver = new StreamReader(tcpClient.GetStream()); swSender = new StreamWriter(tcpClient.GetStream()); //Read account information from the client currentUser = srReceiver.ReadLine(); //Examine response from client if (currentUser != """") { //Store the user name in the hash table if (ChatServer.htUsers.Contains(currentUser) == true) { //0 means not connected - Writes error to Client and Server log swSender.WriteLine(""0|This username already exists.""); swSender.Flush(); CloseConnection(); return; } //More if/else if/else statements //... } } You can dispose of the two streams fairly easily within the AcceptClient method by making them local variables since they aren't referenced elsewhere something like this: private void AcceptClient() { using (StreamReader srReceiver = new StreamReader(tcpClient.GetStream())) { using (StreamWriter swSender = new StreamWriter(tcpClient.GetStream())) { // ... } } } The tcpClient is more tricky because it is being created on one thread and cleaned up on another. Unless you can change that then perhaps the best option is going to be to implement the cleanup within a try/finally. private void AcceptClient() { try { using (StreamReader srReceiver = new StreamReader(tcpClient.GetStream())) { using (StreamWriter swSender = new StreamWriter(tcpClient.GetStream())) { // ... } } } finally { tcpClient.Dispose(); } } The finally clause will get called whether or not the try clause throws an exception. Excellent thanks! You're welcome!"
534,A,"Simple open socket hanging very infrequently I have a rather simple piece of code that is hanging in java. The hang is VERY infrequently. Maybe on the order of once every 1000 executions. Running it in a loop to a device doesn't seem to reproduce the problem. long timeout = 10000; long endTime = System.currentTimeMillis() + timeout + 5000; Socket pingSocket = null; String host = ""host""; String port = ""22""; do { try { pingSocket = new Socket(); pingSocket.bind(null); pingSocket.connect(new InetSocketAddress(host port) 5000); if (pingSocket.isConnected()) { pingSocket.close(); return true; } pingSocket.close(); } catch (UnknownHostException e) { throw e; } catch (IOException e) { // All other errors are subclassed from IOException and i want // to ignore till after my spin period. } try { Thread.sleep(SPIN_SLEEP_DELAY); } catch (InterruptedException e) { return false; } } while (System.currentTimeMillis() <= endTime); Since it happens so infrequently in production it's been hard to narrow down what caused the problem. I'm currently in the process of instrumenting the code so that next release of our product will have more information when this happens but I thought I would ask if anyone has seen just a simple bind/connect/isConnected/close hang before? Thanks! @nos: no. That is the extent of the code. It's just a simple socket connection to make sure that something is on the other end before we go further. @Duck: I could certainly approach it from that end. My biggest concern is why is the stupid thing hanging in java. We are using this just as a simple ""isAlive"" before trying to run ssh in case anyone hasn't figured that out. There's no read/writing to the socket ? There's hundreds of cases where a simple read/write loop can block Indefinitely. I think the first and easiest thing to do is look at the server's ssh logs and see if there is something obvious going on there during the time you hang. Have you generated a Java thread dump during the hang? This will tell you where in your code the hang is occurring. So a typical customer production environment then. :-( I'm going to accept this one. This seems like the only real solution is to catch it when it's happening. Great idea Darron -- The problem is this only happens once every couple of weeks at most. So it's more of a ""did it happen did a customer let us know can we get to it before they kill it etc"".  I've read that if a socket fails to connect then the calling code still has to close it before continuing (can't find the reference now). Otherwise resources are still consumed and future attempts to open sockets may hang. So move the socket close to a finally block to ensure that your socket is closed even if it fails to connect. Thanks @Joe I think this is a very likely solution though I can't confirm it very easily. If I could accept 2 answers I would. I will be adding this to the code after researching it some more."
535,A,Boost ASIO Buffering Not Working I'm writing a networking application that uses ASIO/UDP to send and receive between a single remote/locale endpoint pair. I had used udp::socket::receive to receive data and everything in my code worked logically but I was losing an enormous number of packets. What I discovered was that any packet received while not blocked on the receive function was lost - it wasn't buffering. This was particularly odd because I had set the receive buffer to 2MB using the following command: sock_udp.connect( remote_endpoint ); sock_udp.set_option( boost::asio::socket_base::receive_buffer_size(2*1024*1024) ); This and the fact that if I sent only two packets of about 100 bytes each I would still lose the second one if I spent any time processing the first. I figured that this was perhaps a flaw with udp::socket::receive so I re-wrote my networking code to use udp::socket::async_receive but I still have the same problem. That is once my handler is called I drop any packets until I call async_receive again. Am I fundamentally misunderstanding something? Is there a different approach I should be using for boost to buffer incoming packets? If it helps I've verified that this happens both in OS X in XCode using their custom gcc4.2 build as well as Ubuntu 10.10 using gcc4.5. I have no yet been able to try it in Windows. as Nim said you should post more code to give us a better idea how your program is structured. maybe you can post a more complete example it's difficult to say from your description what could be wrong I've used `boost::asio::async_read` with udp sockets and have had no issues with dropping packets... The general idea here is that your program should spend the vast majority of it's time waiting on the socket to deliver something either blocked in the UDP receive or waiting in the io_service for notification that the socket has asynchronously received something. The socket implicitly has a small buffer in the OS for receiving packets there's no way to avoid it. So the problem is more likely in how your program is behaving. Is your thread anywhere but within the ASIO io_service? If so you can easily overflow any underlying socket buffer. Can you prove that on average the time spent between blocking calls is less than the time between packets being sent? You do have to call async_receive again after you receive data from the socket. For example you can issue another async_receive from within your receive handler. The reason I marked this as the answer is that the error turned out to be a PEBKAC error (Problem Exists Between Keyboard and Chair). Some portions of the code were being done in different threads and when data was arriving too quickly I was over writing. I discovered this when trying to come up with a short example - and the example didn't exhibit the error.
536,A,"Low speed network with high risk of disconnection I have some computers on a network the network is somehow serial I mean computers are connected to each other via radio transmitters and in a line if a radio transmitter goes off or face some problems other computers are not accessible. the architecture would be something like this: RT = radio transmitters Computer#1<-RT#1->Computer#2<-RT#2->Computer#3<-RT#3->....<-RT#(N-1)->Computer#N Distances between computers are about 20 to 30 kilometers and network speed varies between 0 and 10Kbps. computers are running windows XP and a very high loaded program (2GB of ram 90% CPU Usage) are always running on them all computers IP addresses are known and static. I'm going to write a program which is going to run on every computer and get commands or packages to run or install and update on that computer. because of the low speed network and high risk of disconnection sending data between computers should have resume capability. the question is how should I implement that? using .NET remoting or use simple network sockets and TCP? which one is better? feel free to explain your suggestion or approaches you think suites this solution. Are the machines on a domain? Servers workstations? Have a read up on UUCP historically the solution to this problem and configuration. If I want to use such thing as UUCP i will use .NET remoting! @HPT I'm not saying is an amazing technology but it is certainly a proven technology.  You could use a framework like NServiceBus that is based on messaging and then abstract your programs from the connections themselves. The system can allow ""repeat until success"" messaging with a buffer somewhere along the line and you won't need to worry about too much low level stuff. http://www.nservicebus.com/ I'm not sure if it allows large data transfer but check it out for the commands  How about adding a new system which will behave as a server. This system will host a service(and perhaps a database) that will control all the operations on the network. As all the IP addresses are static they can easily be tracked and re-tracked in case of disconnections. For large transmissions you can compress and split packages and send them separately. In case of a failure the transmission will restart from the broken packet. With a separate server all these statistics can easily be maintained and traffic can be controlled accordingly. The clients can even ping the server for pending commands/upgrades in case of low CPU load. With WCF it is possible to do bidirection/duplex transmissions. What You Need To Know About One-Way Calls Callbacks And Events WCF Duplex Messaging great idea! tnx  I used to develop on a terrible connection. I had packets returning literally hours (3-6 hours IIRC) later. Thanks goodness that contract is over :) Have fun! I feel your pain."
537,A,"Program structure for bi-directional TCP communication using Boost::Asio First off I hope my question makes sense and is even possible! From what I've read about TCP sockets and Boost::ASIO I think it should be. What I'm trying to do is to set up two machines and have a working bi-directional read/write link over TCP between them. Either party should be able to send some data to be used by the other party. The first confusing part about TCP(/IP?) is that it requires this client/server model. However reading shows that either side is capable of writing or reading so I'm not yet completely discouraged. I don't mind establishing an arbitrary party as the client and the other as the server. In my application that can be negotiated ahead of time and is not of concern to me. Unfortunately all of the examples I come across seem to focus on a client connecting to a server and the server immediately sending some bit of data back. But I want the client to be able to write to the server also. I envision some kind of loop wherein I call io_service.poll(). If the polling shows that the other party is waiting to send some data it will call read() and accept that data. If there's nothing waiting in the queue and it has data to send then it will call write(). With both sides doing this they should be able to both read and write to each other. My concern is how to avoid situations in which both enter into some synchronous write() operation at the same time. They both have data to send and then sit there waiting to send it on both sides. Does that problem just imply that I should only do asynchronous write() and read()? In that case will things blow up if both sides of a connection try to write asynchronously at the same time? I'm hoping somebody can ideally: 1) Provide a very high-level structure or best practice approach which could accomplish this task from both client and server perspectives or somewhat less ideally 2) Say that what I'm trying to do is impossible and perhaps suggest a workaround of some kind. TCP is full-duplex meaning you can send and receive data in the order you want. To prevent a deadlock in your own protocol (the high-level behaviour of your program) when you have the opportunity to both send and receive you should receive as a priority. With epoll in level-triggered mode that looks like: epoll for send and receive if you can receive do so otherwise if you can send and have something to send do so. I don't know how boost::asio or threads fit here; you do need some measure of control on how sends and receives are interleaved. Priority doesn't really matter... @tc. damn you're right. letting the send buffers fill up in a non-blocking way would force the program to switch to receiving. I'll just catch some sleep. So the fact that TCP is full-duplex would imply that it's no big deal for both sides to try writing at the same time -- I just have to make sure I don't wait indefinitely for writes to complete because then I'd encounter a deadlock situation. Is that correct? @aardvarkk That's correct.  The word you're looking for is ""non-blocking"" which is entirely different from POSIX asynchronous I/O (which involves signals). The idea is that you use something like fcntl(fdF_SETFLO_NONBLOCK). write() will return the number of bytes successfully written (if positive) and both read() and write() return -1 and set errno = EAGAIN if ""no progress can be made"" (no data to read or write window full). You then use something like select/epoll/kqueue which blocks until a socket is readable/writable (depending on the flags set). fcntl is not appropriate when using the boost asio library  What you want to do is absolutely possible. Web traffic is a good example of a situation where the ""client"" sends something long before the server does. I think you're getting tripped up by the words ""client"" and ""server"". What those words really describe is the method of connection establishment. In the case of ""client"" it's ""active"" establishment; in the case of ""server"" it's ""passive"". Thus you may find it less confusing to use the terms ""active"" and ""passive"" or at least think about them that way. With respect to finding example code that you can use as a basis for your work I'd strongly encourage you to take a look at W. Richard Stevens' ""Unix Network Programming"" book. Any edition will suffice though the 2nd Edition will be more up to date. It will be only C but that's okay because the socket API is C only. boost::asio is nice but it sounds like you might benefit from seeing some of the nuts and bolts under the hood.  My concern is how to avoid situations in which both enter into some synchronous write() operation at the same time. They both have data to send and then sit there waiting to send it on both sides. Does that problem just imply that I should only do asynchronous write() and read()? In that case will things blow up if both sides of a connection try to write asynchronously at the same time? It sounds like you are somewhat confused about how protocols are used. TCP only provides a reliable stream of bytes nothing more. On top of that applications speak a protocol so they know when and how much data to read and write. Both the client and the server writing data concurrently can lead to a deadlock if neither side is reading the data. One way to solve that behavior is to use a deadline_timer to cancel the asynchronous write operation if it has not completed in a certain amount of time. You should be using asynchronous methods when writing a server. Synchronous methods are appropriate for some trivial client applications. This is getting very close to what I'm looking for. Where can I read about or see example of these higher-level application protocols for simple bi-directional communication? @aardvarkk protocols are largely application dependent I suggest asking another question with more specific information about your application and how to design its protocol."
538,A,"select on UDP socket doesn't end when socket is closed - what am I doing wrong? I'm working on Linux system (Ubuntu 7.04 server with a 2.6.20 kernel). I've got a program that has a thread (thread1) waiting on a select for a UDP socket to become readable. I'm using the select (with my socket as the single readfd and the single exceptfd) instead of just calling recvfrom because I want a timeout. From another thread I shutdown and close the socket. If I do this while thread1 is blocked in a recvfrom then the recvfrom will terminate immediately. If I do this while thread1 is blocked in a select with a timeout then the select will NOT terminate immediately but will eventually timeout properly. Can anyone tell me why it is that the select doesn't exit as soon as the socket is closed? Isn't that an exception? I can see where it isn't readable (obviously) but it's closed which seems to be to be exeptional. Here's the opening of the socket (all error handling removed to keep things simple): m_sockfd = socket(PF_INET SOCK_DGRAM 0); struct sockaddr_in si_me; memset((char *) &si_me 0 sizeof(si_me)); si_me.sin_family = AF_INET; si_me.sin_port = htons(port); si_me.sin_addr.s_addr = htonl(INADDR_ANY); if (bind(m_sockfd (struct sockaddr *)(&si_me) sizeof(si_me)) < 0) { // deal with error } Here's the select statement that thread1 executes: struct timeval to; to.tv_sec = timeout_ms/1000;// just the seconds portion to.tv_usec = (timeout_ms%1000)*1000;// just the milliseconds // converted to microseconds // watch our one fd for readability or // exceptions. fd_set readfds exceptfds; FD_ZERO(&readfds); FD_SET(m_sockfd &readfds); FD_ZERO(&exceptfds); FD_SET(m_sockfd &exceptfds); int nsel = select(m_sockfd+1 &readfds NULL &exceptfds &to); UPDATE: Obviously (as stated below) closing the socket isn't an exceptional condition (from select's point of view). I think what I need to know is: Why? And Is that intentional?. I REALLY want to understand the thinking behind this select behavior because it seems counter to my expectations. Thus I obviously need to adjust my thinking on how the TCP stack works. Please explain it to me. I would say the difference is that recvfrom is actively trying to read a message from a single socket where select is waiting for a message to arrive possibly on multiple handles and not necessarily socket handles.  I think the most obvious solution is that being closed isn't considered an exceptional condition. I think the root of the problem is that you're not really embracing the philosophy of select. Why on earth are you fiddling around with the socket in another thread that sounds like a recipe for disaster. The only thing I do with the socket from another thread is close it for purposes of stopping a wait in progress. This way I can get my program to exit BEFORE the select timeout expires.  UDP is a connectionless protocol. Since there is no connection none can be broken so the consumer doesn't know that the producer will never send again. You could make the producer send an ""end of stream"" message and have the consumer terminate upon receiving it. All correct and true and a viable workaround but: The recvfrom DOES terminate when I close the socket. Why doesn't the select? I'm really confused as to why closing the socket doesn't produce an exception that terminates the select. I believe you misunderstood the question: the two threads are not manipulating two UDP sockets. There is **only one UDP socket**.  Could you not send a signal (e.g. USR2) to the thread which would cause select() to return with EINTR? Then in the signal handler set a flag telling it not to restart the select()? That would remove the need for waiting on multiple file descriptors and seems a lot cleaner than using a pipe to kill it. Yes that would work though in this particular case I don't think I can use signals internally (some of the badly written base classes in our library use and/or block signals in weird ways). If I can clean the underlying libraries I might try this at some point.  Maybe you should use something else to wake up the select. Maybe a pipe or something like that. That would be a good solution. Have the select wait on both the socket and pipe and the other thread would write to the pipe to cause the select to return. That would work and probably will be what I end up with. I had just wanted to avoid any extra moving parts. Thanks!"
539,A,"select says socket is ready to read when it is definitely not (actually is closed already) In my server I check if any socket is ready to read using select() to determine it. As a result in main loop select() is executed every time it iterates. To test the server I wrote a simple client that sends only one message and then quits. BTW. I use protocol buffers to send information - message means an object of type class Message in this library. The test session looks like: select() server's socket ready to read accept() client's socket read message from client's socket select() server's socket not ready to read client's one ready read message from client's socket The last step is wrong because client has already closed connection. As a result protobuf library gets Segmentation fault. I wonder why FD_ISSET says the socket is ready in step 6 when it is closed. How can I check if a socket is closed? EDIT: I've found how to check if the socket is open int error = 0; socklen_t len = sizeof (error); int retval = getsockopt (socket_fd SOL_SOCKET SO_ERROR &error &len ); The socket used for communication between a client and your server will be flagged as readable (i.e. select() will return) when there is data to read or when there's an EOF to read (i.e. the peer closed the connection). Just read() when select() returns and your fd is flagged. If read() returns a positive number you got data. If it returns 0 you got EOF. If it returns -1 you have a problem (unless errno is EAGAIN).  the socket is ""readable"" if the remote peer closes it you need to call recv and handle both the case where it returns an error and the case where it returns 0 which indicates that the peer shut down the connection in an orderly fashion. Reading the SO_ERROR sockopt is not the correct way as it returns the current pending error (from eg. a non-blocking connect) Right - `select()` returns *if a `read()` will not block* not only when there's data available. there is one problem: when I want to read data from client I get information the connection is closed. Yes it is. However there is data to be read in the socket. I need to know if there is data in the socket not if the connection is open. @lord.didger: If there was data and the connection was closed gracefully you will get the data prior to the 0 sized read."
540,A,Descriptor passing with unix domain sockets When we want to pass a descriptor from child to process UNP V1(Unix network programming V1) specifies a complex procedure of doing this first to create msghdr struct and then something etc. Why cant we pass a descriptor as normal data means as we do send  recv for other data through unix domain sockets? I am able to understand the process. Please explain the method given in that book. Also in that book in read_fd() function he declares the union to properly align the msghdr struct. How union makes it aligned? and why alignment is required? Without having a copy of that book it's going to be difficult for us to explain to you the method given in the book. There are several general techniques to pass a file descriptor from a parent process to a child process. If you know the file descriptor number before launching the child: put it in a command line parameter put it in an environment variable put it in a file on disk (not a great option) If you only know the file descriptor after launching the child then some sort of IPC will be needed: put it in shared memory send it to the child on a socket It sounds like the book you're reading describes the last option. When sending a message to a child you are free to choose the representation of whatever data you are sending. If you're sending a file descriptor (an integer) and no other information then you're free to choose pretty much any representation. An ASCII representation of the integer might be suitable or you could send four bytes representing a 32-bit integer. Again it's hard to guess why the book author is doing things the way they are without knowing what the method actually is.  Why cant we pass a descriptor as normal data means as we do send  recv for other data through unix domain sockets? Because an open file descriptor is not usefully serializable as a stream of bytes. While file descriptors are actually just integers they are mapped by the kernel (in a per-process manner) to kernel-internal data structures that describe the details of the opened ‘file’ (is it ‘normal’ file? is is a block/character special device? is it a network socket of some sort? is it anonymous pipe? etc.). The goal of file descriptor passing is to create a new file descriptor (probably with some other integer value) in some other (possibly unrelated) process that is mapped to the same kernel-internal data structure as the original descriptor in the sending process. The machinations you have to go through to do this are just the “API” to access this functionality (note that System V based Unix systems have an alternate method of file descriptor passing based on STREAMS which uses a different “API”). I do not know the history but I would guess that the “complexity” of the BSD file descriptor passing “API” is due to shoehorning the functionality into the preexisting sendmsg(2)/recvmsg(2) API. How union makes it aligned? and why alignment is required? I do not have the UNP implementation in front of me but using a union is not the only way to go. Kragen Sitaker's portlisten example uses the CMSG_* macros instead of a union. The idea is to make sure that <struct msghdr>.msg_control points to the <struct cmsghdr>.
541,A,"what is it called when a program talks with other programs or other computers As a true beginner one often finds that the barrier to further knowledge is not knowing what to google. I've made programs like pong and a sudoku solver simple things. These days I would like to work on something more like tabslock or make a multiplayer pong or at least start thinking about what that would involve. So what exactly is it called when programs cause other programs to do this or that or when a program talks with a copy of itself on another computer somewhere else in this vast internet. z. How about Peer-to-Peer networking just using Sockets?  In a more general sense you may want to look up ""Service-Oriented Architecture"" (SOA) a service is a program and a given service on 1 machine can communicate with other services on other machines. A bit weighty a concept for a muliplayer Pong game  The current posts are good also Remote Method Invocation (RMI) or Web Services SOAP could also be useful. Also Remote Procedure Call (RPC) as in XML-RPC.  I expect that searching for server/client examples and socket programming in general will get you headed in the right direction. For example a google search for socket client server c will get you to An Introduction to Socket Programming. A similar search for socket client server java gets you to a Lesson on Socket Communications  I think in a very general sense you're talking about Inter-Process Communication.  sockets are logical slots in the OS for running programs (processes) to use for communication. protocols are languages encodings etc. that are agreed upon by both sides of such a communication and are used to make sense of the data. remote procedure calling or RPC is when a process in one place causes another process in another place to run some code as if one had just called a function on the other. This can be done over the network. XML-RPC and SOAP are two protocols for doing RPC over the web for instance.  SkyNet or if you prefer Colossus/Guardian.  Network communication TCP/IP communication etc. Try googling for ""socket server examples in """
542,A,"How to send a WOL package(or anything at all) through a nic which has no IP address? I'm trying to send a WOL package on all interfaces in order to wake up the gateway(which is the DHCP server so the machine won't have an IP yet). And it seems that I can only bind sockets to IP and port pairs... So the question is: How can a create a socket(or something else) that is bound to a NIC that has no IP? (Any languge is ok. c# is prefered) @ctacke: I know that WOL is done by MAC address... My problem is that windows only sends UDP broadcasts on the NIC what Windows considers to be the primary NIC (which is not even the NIC with the default route on my Vista machine). And I can not seems to find a way to bind a socket to an interface which has no IP address. (like DHCP clients do this) @Arnout: Why not? The clients know the MAC address of the gateway. I just want a send a WOL packet like a DHCP client does initially...(DHCP discover packets claim to come from 0.0.0.0) I don't mind if I have to construct the whole packet byte by byte... You're right -- I thought this was a kind of chicken-and-egg situation but it is basically the same thing as a DHCP client looking for a server (as you stated). Good thing SO has no voting on comments :-) So you want to wake up a DHCP server from a client that depends on that exact server to get an IP? I don't think that will be possible. WOL is a very flexible protocol that can be implemented in multiple different ways. The most common are: Sending a WOL as the payload of an ethernet packet. Sending a WOL as the payload of a UDP packet (for routing across the net). Once it lands on the local network it's passes to all the hosts on the network using the broadcast MAC address. For an Ethernet packet the structure is: Destination MAC: FF:FF:FF:FF:FF:FF (Broadcast) A Magic Packet Payload For a UDP packet the structure is: Destination MAC: FF:FF:FF:FF:FF:FF (Broadcast) UDP Port: 9 A Magic Packet Payload The Magic Payload consists of: The Synchronization Stream: FFFFFFFFFFFF (that's 6 pairs or 6 bytes of FF) 16 copies of the MAC of the computer you're signaling to WOL An optional passphrase of 0 4 or 6 bytes. To receive WOL packets from the internet (through a firewall/router): Configure router port 9 to forward to IP 255.255.255.255 (Broadcast IP) Set the destination IP: The external IP of the router Note: This can only be achieved using the UDP example because Ethernet packets lack the IP layer necessary for the packet to be routed through the internet. IE Ethernet packets are the local-network-only option. The issue with sending WOL packets over UDP is security because you have to set the router to enable IP broadcasting (255.255.255.255). Enabling broadcasting over IP is usually considered a bad idea because of the added risk of internal attack within the network (Ping flooding cache spoofing etc...). For more info on the protocol including a sample capture see this site. If you want a quick-and-dirty command line tool that generates WOL packets (and you're running on a debian linux mint or Ubuntu) you can install a tool that already does this. Just install using the command line with: sudo apt-get install wakeonlan Update: Here's a working example that generates a WakeOnLan packet using the current version of SharpPcap. using System; using System.Collections.Generic; using System.Net.NetworkInformation; using PacketDotNet; using SharpPcap; namespace SharpPcap.Test.Example9 { public class DumpTCP { public static void Main(string[] args) { // Print SharpPcap version string ver = SharpPcap.Version.VersionString; Console.WriteLine(""SharpPcap {0} Example9.SendPacket.cs\n"" ver); // Retrieve the device list var devices = CaptureDeviceList.Instance; // If no devices were found print an error if(devices.Count < 1) { Console.WriteLine(""No devices were found on this machine""); return; } Console.WriteLine(""The following devices are available on this machine:""); Console.WriteLine(""----------------------------------------------------""); Console.WriteLine(); int i = 0; // Print out the available devices foreach(var dev in devices) { Console.WriteLine(""{0}) {1}""idev.Description); i++; } Console.WriteLine(); Console.Write(""-- Please choose a device to send a packet on: ""); i = int.Parse( Console.ReadLine() ); var device = devices[i]; Console.Write(""What MAC address are you sending the WOL packet to: ""); string response = Console.ReadLine().ToLower().Replace("":"" ""-""); //Open the device device.Open(); EthernetPacket ethernet = new EthernetPacket(PhysicalAddress.Parse( ""ff-ff-ff-ff-ff-ff"") PhysicalAddress.Parse(""ff-ff-ff-ff-ff-ff"") EthernetPacketType.WakeOnLan); ethernet.PayloadPacket = new WakeOnLanPacket( PhysicalAddress.Parse(response)); byte[] bytes = ethernet.BytesHighPerformance.Bytes; try { //Send the packet out the network device device.SendPacket(bytes); Console.WriteLine(""-- Packet sent successfuly.""); } catch(Exception e) { Console.WriteLine(""-- ""+ e.Message ); } //Close the pcap device device.Close(); Console.WriteLine(""-- Device closed.""); Console.Write(""Hit 'Enter' to exit...""); Console.ReadLine(); } } } Note: This is a fully functional Wake-On-Lan packet sending console application built on the Example09 that can be found in the SharpPcap source. The libraries used in this example that can't be found in the .NET framework are: using PacketDotNet; This library (.dll) comes packaged with SharpPcap. It is responsible for all of the packet construction and parsing within SharpPcap. This is where the WakeOnLan class resides. Note: The packet construction/parsing code was originally bundled within SharpPcap.dll. It was migrated to its own lib because SharpPcap is meant to be a wrapper for winpcap. Many of its users deal with designing protocols and/or handling raw networking packets. using SharpPcap; SharpPcap contains all of the winpcap(windows)/libpcap(*nix) wrapper code. It's needed to select the interface and send the actual packets across the wire.  .NET operates as a virtual machine (the CLR) so it abstracts away much of the underlying real machine. For example it only provides interfaces for TCP and UDP networking which is much higher in the network protocol stack that what you are discussing. You might be able to find a third-party component that provides access to a lower-level interface but I would not count on it (I have looked in the past for .NET and Java). For access to that low in the network protocol stack you probably will need to code in C to the relevant OS system calls. You may find this easiest in Python and you may find this functionality already implemented in Python's or third-party libraries. For example I suggest taking a look at the Twisted networking libraries. That was one of the reasons that I switched to Python for much of my work. Best wishes. I did find one :) (We posted at the same time)  It seems that I have found a solution. One can use winpcap to inject packets to any interface. And there is good wrapper for .net: http://www.tamirgal.com/home/dev.aspx?Item=SharpPcap (I would have prefered a solution which requires no extra libraries to be installed...) UPDATE: Here is what I came up for sending a WOL packet on all interfaces: //You need SharpPcap for this to work private void WakeFunction(string MAC_ADDRESS) { /* Retrieve the device list */ Tamir.IPLib.PcapDeviceList devices = Tamir.IPLib.SharpPcap.GetAllDevices(); /*If no device exists print error */ if (devices.Count < 1) { Console.WriteLine(""No device found on this machine""); return; } foreach (NetworkDevice device in devices) { //Open the device device.PcapOpen(); //A magic packet is a broadcast frame containing anywhere within its payload: 6 bytes of ones //(resulting in hexadecimal FF FF FF FF FF FF) followed by sixteen repetitions byte[] bytes = new byte[120]; int counter = 0; for (int y = 0; y < 6; y++) bytes[counter++] = 0xFF; //now repeat MAC 16 times for (int y = 0; y < 16; y++) { int i = 0; for (int z = 0; z < 6; z++) { bytes[counter++] = byte.Parse(MAC_ADDRESS.Substring(i 2) NumberStyles.HexNumber); i += 2; } } byte[] etherheader = new byte[54];//If you say so... var myPacket = new Tamir.IPLib.Packets.UDPPacket(EthernetFields_Fields.ETH_HEADER_LEN etherheader); //Ethernet myPacket.DestinationHwAddress = ""FFFFFFFFFFFFF"";//it's buggy if you don't have lots of ""F""s... (I don't really understand it...) try { myPacket.SourceHwAddress = device.MacAddress; } catch { myPacket.SourceHwAddress = ""0ABCDEF""; }//whatever myPacket.EthernetProtocol = EthernetProtocols_Fields.IP; //IP myPacket.DestinationAddress = ""255.255.255.255""; try { myPacket.SourceAddress = device.IpAddress; } catch { myPacket.SourceAddress = ""0.0.0.0""; } myPacket.IPProtocol = IPProtocols_Fields.UDP; myPacket.TimeToLive = 50; myPacket.Id = 100; myPacket.Version = 4; myPacket.IPTotalLength = bytes.Length - EthernetFields_Fields.ETH_HEADER_LEN; //Set the correct IP length myPacket.IPHeaderLength = IPFields_Fields.IP_HEADER_LEN; //UDP myPacket.SourcePort = 9; myPacket.DestinationPort = 9; myPacket.UDPLength = UDPFields_Fields.UDP_HEADER_LEN; myPacket.UDPData = bytes; myPacket.ComputeIPChecksum(); myPacket.ComputeUDPChecksum(); try { //Send the packet out the network device device.PcapSendPacket(myPacket); } catch (Exception e) { Console.WriteLine(e.Message); } device.PcapClose(); } } another c# example: http://www.codeproject.com/KB/IP/wolclass.aspx There are two issues with this comment. First to pass the packet to all the computers on the network you need to enable IP broadcasting (255.255.255.255) on the router which is usually considered a bad idea security wise (because of added potential for internal network attacks like ping flooding. Second you're using a version of SharpPcap that is years out of date. If you download the latest it already contains a class for creating/parsing WOL packets. I know because I wrote it back in November/December as per a User's request. Here's the link to the SharpPcap project. http://sourceforge.net/projects/sharppcap/  WOL is done by MAC not IP. Here's an example. I know that WOL is done by MAC address... My problem is that windows only sends UDP broadcasts on the NIC what Windows considers to be the primary NIC. And I can not seems to find a way to bind a socket to an interface which has no IP address. (like DHCP clients do this)"
543,A,iPhone to iPhone communication I have been looking into communication methods between two iPhone devices and as I understand it there are two main methods to do this: Bluetooth and WiFi. However I wanted to know if anyone knows of any other way of sharing information between two iPhones? If I were in an area with no WiFi or mobile signal and couldn't use Bluetooth for whatever reason is there anything that could be done to broadcast data from one iPhone to another? I'm sure its a bit of a silly question but if you don't ask you don't get; and if someone knows of anything I might be able to look into with regards to this I would be very appreciative. Thanks Dan AFAIK Wifi and Bluetooth are your only options for direct phone-to-phone communication. Yeah thats pretty much what I thought but didn't know if the iPhone had the capability of broadcasting a 'presence' that could be detected e.g. creating its own wireless network. Pretty far fetched maybe but thought I'd ask :) Thanks anyway!  Offcourse there is still cellular data network because you are talking about the iPhone ;) Indeed but surely that wouldn't work in an area with no mobile coverage?
544,A,Socket Programming in C++ Can anybody provide me some sample example on Client and server connection using sockets in C++. I have gone through some tutorials now i want to implement it. How to start ? If you've actually gone through tutorials you should have *already* implemented a (simple) client-server application. @Anon : I am new to C++  I have done this in C#. I want to do this in C++ now. I wrote something that might help here: http://stackoverflow.com/questions/2843277/c-winsock-p2p/2920787#2920787 You can find a working client-server program here: Beej's Guide to Network Programming  A good C++ networking library is ACE. The only problem is that there aren't any good tutorials online that I have found. This book is pretty good though. ACE logo = illuminati  There is no socket API in the C++ Standard. The POSIX C API is fairly portable (the GNU libC documentation provides examples of UDP and TCP clients and servers that I usually turn to when I'm scratching together another server) or you could use the Boost.ASIO library for a more C++ experience....
545,A,"easy to write a script to test whether the network is ever down for the next 24 hours? is it easy to write a script to test whether the network is ever down for the next 24 or 48 hours? I can use ssh to connect to a shell and come back 48 hours later to see if it is still connected to see if the network has ever been down but can i do it programmatically easily? You might get more specific help if you can identify what platform and any constraints on what kind of script. Windows and *nix are different enough to make a general answer difficult. Ummm even if the network went down the ssh would remain connected. Just to clarify ssh would remain connected only if sshd was configured so that it doesn't send echo packets. Otherwise it'll detect network down and disconnect you :) Went to check (that's sshd other servers might act differently): if ClientAliveInterval is 0 you'll remain connected otherwise not. However the client may think it's still connected until there's some activity unless ServerAliveInterval (in the client's config) is also nonzero Most platforms support a ping command that can be used to find out if a network path exists to an IP address somewhere ""else"". Where exactly to check depends on what you are really trying to answer. If the goal is to discover the reliability of your first hop to your ISP's network then pinging a router or their DNS regularly might be sufficient. If your concern is really the connection to a single node (your mention of leaving an ssh session open implies this) then just pinging that node is probably the best idea. The ping command will usually fail in a way that a script can test if the connection times out. Regardless it is probably a good idea to check at a rate no faster than once a minute and slower than that is probably sufficient.  If your need is more adminstrative in nature and using existing software is an option you could try something like monit:  http://mmonit.com/monit/ Wikipedia has a list of similar software:  http://en.wikipedia.org/wiki/Comparison_of_network_monitoring_systems You should consider also whether very short outages need to be detected. Obviously a periodic reachability test cannot guarantee detecting outages shorter than the testing interval. If you only care about whether there was an outage not how many there were or how long they lasted you should be able to automate your existing ssh technique using expect pretty easily.  http://expect.nist.gov/  The Internet (and your ethernet) is a packet-switched network which makes the definition of 'down' difficult. Some things are obvious; for example if your ethernet card doesn't report a link then it's down (unless you have redundant connections). But having a link doesn't mean its up. The basic 100 mile view of how the Internet works is that your computer splits the data it wants to send into ~1500-byte segments called packets. It then pretty much blindly sends them on their way however your routing table says to. Then that machine repeats the process. Eventually through many repetitions it reaches the remote host. So you may be tempted to define up as the packet reached its destination. But what happens if the packet gets corrupted e.g. due to faulty hardware or interference? The next router will just discard it. Ok that's fine you may well want to consider that down. What if a router on the path is too busy or the link it needs to be sent on is full? The packet will be dropped. You probably don't want to count that as down. Packets routinely get lost; higher-level protocols (e.g. TCP) deal with it and retransmit the packet. In fact TCP uses the packet drop on link full behavior to discover the link bandwidth. You can monitor packet loss with a tool like ping as the other answer states."
546,A,"Default Gateway in C on Linux How do you find the default gateway of a routing table using C on Linux? I don't want to issue a call to the shell or read a file. There are ioctls for adding and deleteing routes (SIOCADDRT SIOCDELRT) and I've found on reference to getting routes (SIOCGRTCONF) but it seems that the version of the kernel I'm using doesn't support SIOCGRTCONF. NICs don't have default gateways routing tables have default gateways. You will probably need to use a NETLINK_ROUTE socket part of the PF_NETLINK family of sockets. Check out the source code of the 'ip' program part of 'iproute'. Specifically its 'route' subcommand. Here is a link to sample code. http://www.linuxquestions.org/questions/linux-networking-3/howto-find-gateway-address-through-code-397078/ I've implemented this with some modifications and it works well.  You could use/proc/net/route like this: int GetDefaultGw ( std::string & gw ) { FILE *f; char line[100]  *p  *c *g *saveptr; int nRet=1; f = fopen(""/proc/net/route""  ""r""); while(fgets(line  100  f)) { p = strtok_r(line  "" \t"" &saveptr); c = strtok_r(NULL  "" \t"" &saveptr); g = strtok_r(NULL  "" \t"" &saveptr); if(p!=NULL && c!=NULL) { if(strcmp(c  ""00000000"") == 0) { //printf(""Default interface is : %s \n""  p); if (g) { char *pEnd; int ng=strtol(g&pEnd16); //ng=ntohl(ng); struct in_addr addr; addr.s_addr=ng; gw=std::string( inet_ntoa(addr) ); nRet=0; } break; } } } return nRet; }  I think reading /proc/net/route will be your best bet. Would you consider this a ""file""? The format of /proc/net/route is well-known and in-memory so there's no I/O penalty or fear of this changing (i.e. versus reading something from /etc/network/*)"
547,A,IP address in TCP sockets I have a root node(server) connected to many other nodes(clients) through TCP sockets. I want to send some data from server to client but the data is different for each node and depends on the ip address of that node. Thus I should have ip address of each node connected to server. How can I have that information? When you call accept(2) you can choose to retrieve the address of the client. int accept(int socket struct sockaddr *restrict address socklen_t *restrict address_len); You need to store those addresses and then send(2) to each what you need to send. So the workflow should be something like this: Keep a list of connected clients. Initially the list is empty of course When you accept a connection push its details into that list (the address and the socket returned by accept(2)). When you need to send something to every client simply walk the list and send it (using the stored socket) The one tricky part is that socklen_t *restrict address_len is a value-result argument so you need to be careful with that.  As long as you have access to the list of file descriptors for the connected TCP sockets it is easy to retrieve the addresses of the remote hosts. The key is the getpeername() system call which allows you to find out the address of the remote end of a socket. Sample C code: // This is ugly but simpler than the alternative union { struct sockaddr sa; struct sockaddr_in sa4; struct sockaddr_storage sas; } address; socklen_t size = sizeof(address); // Assume the file descriptor is in the var 'fd': if (getpeername(fd &address.sa &size) < 0) { // Deal with error here... } if (address.sa.family == AF_INET) { // IP address now in address.sa4.sin_addr port in address.sa4.sin_port } else { // Some other kind of socket... } If you don't have the FDs you won't be communicating with those sockets at all… This is also basically how you get the values from an `accept()` call (as in cnicutar's answer). It just has the advantage of not needing you to remember it. One final remark: getting the host _name_ from an IP address is very expensive (DNS is not optimized for that case) and not necessarily informative either (many hosts don't have reverse records or the address may be shared because of NAT).  This is a more nuanced question than it first appears. If the clients are sitting behind a NAT you may get the same IP from more than one client. This is perfectly natural and expected behavior. If you need to distinguish between multiple clients behind the same NAT you'll need some other form of unique client id (say IP address and port). They are not sitting behind any NAT
548,A,"What are the effects of incorrectly setting the netmask? What are the effects of incorrectly setting the netmask? I have a C++ application that sets the network mask of a device. If the netmask is set incorrectly tftp doesn't seem to work properly. Why would this happen? What other problems occur when the netmask is not properly set for a device/PC? I promise I won't flame/retaliate. If you leave the downvote comment as an answer below I'll even upvote it! I think downvote comments are just as important as other comments and answers. Things can be learned from them as well! I went on meta.stackoverflow.com and saw a question about downvote comments. Jeff Atwood has a comment on it. http://meta.stackexchange.com/questions/135/encouraging-people-to-explain-down-votes/2373#2373 Whoever downvoted please leave comment for downvote. I would like to fix/explain whatever caused the downvote. Thanks. @zooropa: unfortunately it is a common practice to have ""drive-by down-votes"" around here... Perhaps the downvote is due to the fact that this isn't really a programming question as such so could be considered off topic for this site. (I consider it close enough though and it wasn't me!) The netmask determines which IP adresses are local (non-routed); IP adresses outside that range go through the router. If the netmask is wrong the program tries to directly access sites where it has to go through the router or vice versa.  While this question is probably more about IP networks than programming it is a challenging subject for many developers. The netmask delimits the host address (your PC or server) and the network address (the part of the logical network infrastructure in which your system lives). The two parts are used to deliver the data packet to the correct device. The network address is obtained by ANDing the netmask with the IP Address. Consider the following scenario: IP Address: 10.0.1.1 Netmask: 255.255.0.0 The host address portion of the IP address for our PC is 1.1 so the PC knows that any host addresses starting 10.0. are local to it. Any addresses that then start 10.1 etc are not 'local' and will need to be forwarded to a router. If you have another device intended to be on the same network that is: IP Address: 10.0.2.1 Netmask: 255.255.255.0 Here the netmask is wrong for our example setup this device is now going to see the network address as 10.0.2 and the host address as 1 if it tries to communicate with 10.0.1.1 it will see a network address of 10.0.1! Not local and so will refer it to the default router for forwarding. If the netmask was correctly set (i.e. the same as the first example assuming that's the correct setting for your network) then the second device would see the first as local i.e. on the 10.0 network and wouldn't attempt to forward the packet to a router. Many protocols will happily cope with this but tftp is intended to operate within a single network and so will fail as there's a perception that the target is on a different network. This may not describe your exact situation but I hope that the example demonstrates the important principle that configuration matters you can't have an inaccurately configured environment and expect it to work.  The netmask defines which part of the IP-address is used as address for the network and which part is used for the workstations. First Example: IP1: 192.168.20.4 IP2: 192.168.192.4 NM: 255.255.0.0 Both IPs are in the same net. They can communicate with each other without needing a router. That's because the IP-addresses will result in the same bitmask when you or it with the netmask. Second Example: IP1: 192.168.20.4 IP2: 192.168.192.4 NM: 255.255.128.0 Now both IPs are in different networks because when you or the IP-addresses with the Netmask the resulting bitmask will be different and they wont be able to communicate with each other without a router that routes between the two networks. You can test this by yourself with ipcalc."
549,A,"How do I set the socket timeout in Ruby? How do you set the timeout for blocking operations on a Ruby socket? Found a much better solution than your accepted one here: http://stackoverflow.com/a/12111120/216314 This article shows a way of timing out sockets. the first article does not exist (anymore). But thanks for the other pointer Using socket options SO_RCVTIMEO and SO_SNDTIMEO with Ruby as in the article has the expected behaviour only on Ruby 1.8. On 1.9 and 2.1 it didn't. Using C it worked as expected on OS X 10.9.4 but not on Ubuntu 14.04 - http://moret.pro.br/2014/09/03/socket-read-timeout/  The timeout object is a good solution. This is an example of asynchronous I/O (non-blocking in nature and occurs asynchronously to the flow of the application.) IO.select(read_array [ write_array [ error_array [ timeout]]] ) => array or nil Can be used to get the same effect. require 'socket' strmSock1 = TCPSocket::new( ""www.dn.se"" 80 ) strmSock2 = TCPSocket::new( ""www.svd.se"" 80 ) # Block until one or more events are received #result = select( [strmSock1 strmSock2 STDIN] nil nil ) timeout=5 timeout=100 result = select( [strmSock1 strmSock2] nil niltimeout ) puts result.inspect if result for inp in result[0] if inp == strmSock1 then # data avail on strmSock1 puts ""data avail on strmSock1"" elsif inp == strmSock2 then # data avail on strmSock2 puts ""data avail on strmSock2"" elsif inp == STDIN # data avail on STDIN puts ""data avail on STDIN"" end end end  I think the non blocking approach is the way to go. I tried the mentioned above article and could still get it to hang. this article non blocking networking and the jonke's approach above got me on the right path. My server was blocking on the initial connect so I needed it to be a little lower level. the socket rdoc can give more details into the connect_nonblock def self.open(host port timeout=10) addr = Socket.getaddrinfo(host nil) sock = Socket.new(Socket.const_get(addr[0][0]) Socket::SOCK_STREAM 0) begin sock.connect_nonblock(Socket.pack_sockaddr_in(port addr[0][3])) rescue Errno::EINPROGRESS resp = IO.select([sock]nil nil timeout.to_i) if resp.nil? raise Errno::ECONNREFUSED end begin sock.connect_nonblock(Socket.pack_sockaddr_in(port addr[0][3])) rescue Errno::EISCONN end end sock end to get a good test. startup a simple socket server and then do a ctrl-z to background it the IO.select is expecting data to come in on the input stream within 10 seconds. this may not work if that is not the case. It should be a good replacement for the TCPSocket's open method. This works for me very well indeed. Timeout should not be used under any circumstances because of MRI's green threads. A non-blocking connect comes up writable when ready. Use `IO.select(nil[sock]nil timeout.to_i)` Only worked for me after using tmm1's comment.  The solution I found which appears to work is to use Timeout::timeout: require 'timeout' ... begin timeout(5) do message client_address = some_socket.recvfrom(1024) end rescue Timeout::Error puts ""Timed out!"" end Using timeout is a mistake. Due to green theards anything that blocks on IO will block the timeout thread as well thus stopping it from working. @Sardathrion thanks for the feedback but a solution would be better than a negative comment :) Ruby threads are broken I am afraid. Find a better language -- harsh but true. The SystemTimer gem can be used to implement safe timeouts on Ruby 1.8. Ruby 1.9 does not have the timeout limitation described by Sardathrion since it uses native threads. This uses threads for timeouts which is very bad for performance in multithreaded applications in ruby. See my answer here: http://stackoverflow.com/a/12111120/216314 @TylerBrock I am using Rails 3 with Apache/Passenger which from my understanding is not multithreaded. I think Passenger spawns a new process for each request. Is your solution still recommended over this? Can anyone confirm that the `Timeout::timeout` implementation above is reasonable for ruby 1.9.x and newer?"
550,A,Why does java NetworkInterface.getHardwareAddress return empty byte array on Windows? I have the following code on a Windows machine: for(Enumeration enm = NetworkInterface.getNetworkInterfaces(); enm.hasMoreElements();){ NetworkInterface network = (NetworkInterface) enm.nextElement(); if(null != network.getHardwareAddress()){ return EthernetAddress.valueOf(network.getHardwareAddress()); } } This fails because the network.getHardwareAddress() returns an empty byte array instead of null as stated in the javadocs for NetworkInterface. Does anyone know why this may happen? I did this and saw that the loopback (which apeared first) has no mac address. My guess is that null is intended for when the OS returns that the MAC was unavailable. However it may have returned empty data instead. On Linux it doesn't show me the loopback.
551,A,"IPv6 Multicast Check Java Is there any better to way to check if I can receive a given IP Multicast transmission. Following code works fine but there is a problem in this code - it blocks the current thread until it gets the multicast packets. Thank you. import java.net.DatagramPacket; import java.net.InetAddress; import java.net.MulticastSocket; public class MulticastTest { static String MCAST_ADDR = ""FF7E:230::1234"";// ""235.1.1.1""; static int DEST_PORT = 1234; static int BUFFER_LENGTH = 16; public static void main(String args[]) { try { byte[] b = new byte[BUFFER_LENGTH]; DatagramPacket dgram = new DatagramPacket(b b.length); MulticastSocket socket = new MulticastSocket(DEST_PORT); socket.joinGroup(InetAddress.getByName(MCAST_ADDR)); socket.receive(dgram); // blocks until a datagram is received System.err.println(""Received "" + dgram.getLength() + "" bytes from "" + dgram.getAddress()); dgram.setLength(b.length); // must reset length field! } catch (Exception e) { } } } You could set a timeout-value with the method socket.setSoTimeout(int). http://download.oracle.com/javase/1.4.2/docs/api/java/net/Socket.html#setSoTimeout(int) If you don't receive any data within the timeout a SocketTimeoutException is raised Thank you very much it works."
552,A,"How come NetworkStream.DataAvailable is always true? I open a TcpClient and then call tcpClient.GetStream().Read(message 0 8) in order to read messages from the connection. For some reason I keep getting garbage data even though the other side of the connection does not send any data. Read() never blocks DataAvailable is always true and I get a lot of garbage as the data. What could be the reason? Thank you in advance for your help! I'm sorry turns out I made a stupid mistake in my code. Sorry for wasting everyone's time :) Did you check the return value of `Read`? In non blocking mode it indicates how many bytes were actually read. To echo codeinchaos's point - indeed check that. but: without some of the receiving code (and ideally the matching network spec I.e. What we should expect to see on the wire) this is largely impossible to answer. Keep the opening ceremony of the 1936 Olympics in mind when you debug this. Not posting a code snippet is a big mistake. @Hans - I feel ignorant for not getting the reference... @Marc - the movie ""Contact"" with Jodie Foster. @Hans - ok it has been *years* since I last watched that... That is hard to answer without seeing both ends of the pipe (but in particular the sending end). DataAvailable only really indicates the state of the local buffer (not the stream itself); in terms of determining the end of a stream it is largely useless (t reports something unrelated). I expect this is a bug in the transmitting code. A classic error here is the following: var buffer = memoryStream.GetBuffer(); networkStream.Write(buffer 0 buffer.Length); When it should be: var buffer = memoryStream.GetBuffer(); networkStream.Write(buffer 0 (int)memoryStream.Length); The first (and incorrect) version sends the garbage portion of the memory-stream's backing buffer. Or just `memoryStream.ToArray()` in your first sample. Thanks Marc but I should have mentioned that I'm only replacing the legacy **receiving** end so the sending end should be ok as it works fine with the legacy listener. `ToArray` allocates a new array. And for large memory streams that might not be desirable because it causes an allocation on the large object heap."
553,A,"Why is Socket.BeginReceive losing packets from UDP? The following code waits for data over UDP. I have a test function that sends 1000 packets (datagrams?) of 500 bytes each. Each time I run the test function the receiver gets only the first few dozen packets but drops the rest. I looked at the incoming network data using Wireshark and I see all 1000 packets are actually received but just don't make it to may app's code. Here is some of the relevant VB.NET 3.5 code: Private _UdbBuffer As Byte() Private _ReceiveSocket As Socket Private _NumReceived As Integer = 0 Private _StopWaitHandle As AutoResetEvent Private Sub UdpListen() _StopWaitHandle = New AutoResetEvent(False) _UdpEndPoint = New Net.IPEndPoint(Net.IPAddress.Any UDP_PORT_NUM) _ReceiveSocket = New Socket(AddressFamily.InterNetwork SocketType.Dgram ProtocolType.Udp) _ReceiveSocket.Bind(_UdpEndPoint) ReDim _UdbBuffer(10000) While Not _StopRequested Dim ir As IAsyncResult = _ReceiveSocket.BeginReceive(_UdbBuffer 0 10000 SocketFlags.None AddressOf UdpReceive Nothing) If Not _StopRequested Then Dim waitHandles() As WaitHandle = {_StopWaitHandle ir.AsyncWaitHandle} If (WaitHandle.WaitAny(waitHandles) = 0) Then Exit While End If End If End While _ReceiveSocket.Close() End Sub Private Sub UdpReceive(ByVal ar As IAsyncResult) Dim len As Integer If ar.IsCompleted Then len = _ReceiveSocket.EndReceive(ar) Threading.Interlocked.Increment(_NumReceived) RaiseStatus(""Got "" & _NumReceived & "" packets"") End If End Sub I am sending the data as follows (not worried about the packet content for now): For i as UShort = 0 to 999 Dim b(500) as Byte _UdpClient.Send(b b.Length) Next If I add a small delay after each call to Send more packets make it through; however since Wireshark says that they were all received anyways it seems that the problem is in my receive code. I should mention that UdpListen is running on a separate thread. Any idea why I am dropping packets? I also tried UdpClient.BeginReceive/EndReceive but had the same problem. A second issue that bothers me is the global nature of the receive buffer when using Sockets and I am not sure if I don't process incoming packets quickly enough that the buffer will be overwritten. Not sure what to do about that just yet but I am open to suggestions. Sep 26: Update Based on the various somewhat conflicting suggestions from replies to this and other posts I made some changes to my code. Thanks to all who chimed in various bits; I now get all my packets from dial-up to Fast Ethernet. As you can see it was my code at fault and not the fact that UDP drops packets (in fact I have not seen more than a tiny percentage of packets being dropped or out of order since my fixes). Differences: 1) Replaced BeginReceive()/EndReceive() with BeginReceiveFrom()/EndReceiveFrom(). By itself this had no notible effect though. 2) Chaining BeginReceiveFrom() calls instead of waiting for the async handle to set. Not sure if any benefit here. 3) Explicitly set the Socket.ReceiveBufferSize to 500000 which is enough for 1 second worth of my data at Fast Ethernet speed. Turns out this is a different buffer than the one passed to BeginReceiveFrom(). This had the biggest benefit. 4) I also modified my send routine to wait a couple of ms after having sent a certain number of bytes to throttle based on expected bandwidth. This had a big benefit for my receiving code even though Wireshark said all my data still made it across even without this delay. I did NOT end up using a separate processing thread because as I understand it each call to BeginReceiveFrom will invoke my callback on a new worker thread. This means that I can have more than one callback running at the same time. It also means that once I call BeginReceiveFrom I have time to do my stuff (as long as I don't take too long and exaust the available worker threads). Private Sub StartUdpListen() _UdpEndPoint = New Net.IPEndPoint(Net.IPAddress.Any UDP_PORT_NUM) _ReceiveSocket = New Socket(AddressFamily.InterNetwork SocketType.Dgram ProtocolType.Udp) _ReceiveSocket.ReceiveBufferSize = 500000 _ReceiveSocket.Bind(_UdpEndPoint) ReDim _Buffer(50000) _ReceiveSocket.BeginReceiveFrom(_Buffer 0 _Buffer.Length SocketFlags.None _UdpEndPoint AddressOf UdpReceive Nothing) End Sub Private Sub UdpReceive(ByVal ar As IAsyncResult) Dim len As Integer = _ReceiveSocket.EndReceiveFrom(ar _UdpEndPoint) Threading.Interlocked.Increment(udpreceived) Dim receiveBytes As Byte() ReDim receiveBytes(len - 1) System.Buffer.BlockCopy(_Buffer 0 receiveBytes 0 len) _ReceiveSocket.BeginReceiveFrom(_Buffer 0 _UdbBuffer.Length SocketFlags.None _UdpEndPoint AddressOf UdpReceive Nothing) //' At this point do what we need to do with the data in the receiveBytes buffer Trace.WriteLine(""count="" & udpreceived) End Sub What is not shown above is the error handling and dealing with UDP data being out of order or missing. I think this handles my issue but if anybody still sees anything wrong with the above (or something I could do better) I would love to hear about it. What do you mean by 'A second issue that bothers me is the global nature of the receive buffer when using Sockets'? There is one per socket. Nothing global about it. ""I am not sure if I don't process incoming packets quickly enough that the buffer will be overwritten"". It won't be overwritten. Incoming packets will be *dropped* if the buffer is full. @EJP that comment applied to my original bit of code. Given how I was using the buffer it could be overwritten by the next bunch of data while I was reading it. The new code does not have this problem since it does a block copy of the buffer (about 200 bytes) before I go back to listen for more data. The 'RaiseStatus' method is *very* heavy. You don't want to call it for every packet received. As others have already said UDP is not a guaranteed delivery mechanism. SO even though wireshark is showing you that the packets were sent it does not mean that the packets were received at the destination. The TCP/IP stack on the receiving host could still discard the datagrams. You can confirm this is happening by monitoring the following performance counters in perfmon.exe. Local Computer\IPv4\Datagrams Received Discarded or Local Computer\IPv6\Datagrams Received Discarded if you are using IPv6 protocol. Also you can try reducing the rate at which you send datagrams and see if that reduces the discard rate. I had Wireshark running on both sender and receiver. It showed all packets were making it across the wire even with the old version. According to System.Net.NetworkInformation.IPv4InterfaceStatistics there were no packets dropped from either end. Not sure how or at what layer Wireshark gets its data from but my conclusion is that somehow my code was not pulling from the network fast enough (even though I was in a pretty tight loop). Increasing the Socket.ReceiveBufferSize seemed to give me more time to ensure I had all the packets processed. Reducing the send rate also improved things.  As said above UDP is not a reliable protocol. It's a connectionless protocol which puts much less overhead on IP packets than TCP does. UDP is quite good for many functions (including broadcast and multicast messages) but it can't be used for reliable message delivery. If the buffer is overloaded network driver will just drop datagrams. If you need message-based communication with reliable delivery or you expect many messages to be sent you can check our MsgConnect product (free open-source version available) which provides message-based data transfer over sockets as well as over UDP. Thanks please see my above reply as it does not appear to me that I am dropping packets according to Wireshark. I know that UDP is not reliable but getting 80% of packets dropped on a local area network when other UDP test apps work fine tells me I have a problem with my code. @DanC: Packets will be dropped by the receiving driver if there is no buffer waiting to receive them. I.e. they reached the NIC but not your application. Yes as jgauffin pointed above you should not process messages immediately. Pass them to other thread if you need to process messages on-the-fly. Finally there can be some issue with BeginReceive in particular but I am not aware of specifics of UDP operations in this scenario - we use synchronous operations in UDP transport of MsgConnect  Sorry. I do not understand your code. Why are you wrapping asynchronous methods in a loop? You should begin with reading up on asynchronous handling. UDP only guarantee that complete messages are received nothing else. A message can be dropped or come in incorrect order. You need to apply your own algorithm to handle that. There are one called Selective Repeat for instance. Next You should not process each message before receiving a new one if you are expecting to receive messages a lot of messages in a short period of time. Instead enqueue each incoming message and have a separate thread that takes care of the processing. Third: Udp messages should be processed with BeginReceiveFrom/EndReceiveFrom for asynchronous processing or ReceiveFrom for synchronous. Thanks for your suggestions. It is in a loop because I want to read more than one packet. When the async event occurs it signals the wait handle to continue and I start checking for the next packet while I process the current one. This is one of two techniques I have seen for efficient use of the processor (the other option is to chain the call BeginReceive in the callback itself). Also I tried just replacing my code with BeginReceiveFrom/EndReceiveFrom but still had the same problem. Do you have any sample code suggestions? I should also add that the problem occurs with the callback test code cut down to nothing more than incrementing a counter so processing in very minimum.  Try a simpler approach. Have the receiver run in a separate thread that would look something like this in pseudo code: do while socket_is_open??? buffer as byte() socket receive buffer 'use the blocking version add buffer to queue of byte() loop In another thread loop on the queue size to determine if you have received packets. If you have received packets process them else sleep. do if queue count > 0 do rcvdata = queue dequeue process the rcvdata loop while queue count > 0 else sleep end if loop while socket_is_open??? Thanks I will keep this in mind for when I actually start doing something with the buffer (though i will more likely use signal instead of sleep since the data can come in any time). However my example above is literally just counting the received packets so the processing time should not be a factor should it? Notice that I am using the blocking version of receive which blocks that thread unless there is data. When there is data it simply places it in a queue that the other thread processes. The thread that processes the data has to have a way to relinquish control when there is no data to process. Years ago I wrote a UDP app that tested bandwidth using the method described. I did not have a problem testing 100Mbps FDX switches at wire speed (200 Mbps).  UDP can drop packets whenever it likes. In this case you could try setting a much larger socket receive buffer at the receiver to mitigate. Thanks that's the buffer that I ended up finding and increasing to 500000 in my laste udpate. I am talking about the socket receive buffer. This is a data structure in the Kernel whose size you can control via the API. By default it is 8k in Windows absurdly small. The problem is that according to Wireshark all the packets made it to the remote computer so I don't think they are actually dropped by UDP. Also each call to EndReceive gets 500 bytes and when I look at the contents of the buffer only 500 of the 10000 bytes are filled so I don't think it's overflowing. Are you referring to a different buffer? UDP doesn't stop at the remote computer. It includes the UDP stack *in* the remote computer and the socket receive buffer is part of that. If there is no room there the packet is dropped. So make it big enough or make your receiving code fast enough or make your sending code slow enough. And even then you can still get dropped packets. So your application just has to cope. If you want reliability use TCP. My sample code above sends 500000 bytes in about 1 second. Are you saying that the buffer that I pass to BeginReceive/BeginReceiveFrom has to be that big? Seem rather excessive especially since each time I call EndReceive/EndReceiveFrom the rest of that buffer seems unused. Is there a different buffer I should be initializing?"
554,A,"my server program can be seen as process what is meaning of S in ps output Here is my server program #include<stdio.h> #include<stdlib.h> #include<sys/socket.h> #include<sys/un.h> #include<sys/types.h> #include<unistd.h> int main () { int server_sockfdclient_sockfd; int server_lenclient_len; struct sockaddr_un server_address; struct sockaddr_un client_address; unlink(""server_socket""); server_sockfd=socket(AF_UNIXSOCK_STREAM0);//created socket server_address.sun_family=AF_UNIX; strcpy(server_address.sun_path""server_socket""); server_len=sizeof(server_address); bind(server_sockfd(struct sockaddr *)&server_addressserver_len);//binded it listen(server_sockfd5); while (1) { char ch; printf(""server waiting\n""); client_len=sizeof(client_address); client_sockfd=accept(server_sockfd(struct sockaddr *)&client_address&client_len); read(client_sockfd&ch1); ch++; write(client_sockfd&ch1); close(client_sockfd); } } I compiled the above program as follows  cc server.c -o server.o when I run a ps -el | grep server.o I get following output 0 S 1000 4450 2228 0 80 0 - 965 skb_re pts/0 00:00:00 server.o I want to know what is the meaning of S in the above output? S Marks a process that is sleeping for less than about 20 seconds. D Marks a process in disk (or other short term uninterruptible) wait. I Marks a process that is idle (sleeping for longer than about 20 seconds). L Marks a process that is waiting to acquire a lock. R Marks a runnable process. T Marks a stopped process. W Marks an idle interrupt thread. Z Marks a dead process (a ""zombie""). Source thanks for pointing that out  It means ""Interruptible sleep"". It likely means it is waiting in a blocking system call. In your case that system call is most likely accept or read. ps(1) Here are the different values that the s stat and state output specifiers (header ""STAT"" or ""S"") will display to describe the state of a process.  D Uninterruptible sleep (usually IO) R Running or runnable (on run queue) S Interruptible sleep (waiting for an event to complete) T Stopped either by a job control signal or because it is being traced. W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z Defunct (""zombie"") process terminated but not reaped by its parent. what does interruptible sleep means? @Registered User: In an interruptible sleep the process handles signals. If it's in an uninterruptible sleep signal processing is postponed. [More on the subject](http://www.linuxjournal.com/article/8144)"
555,A,"programing practices : how to choose a packet size for UDP datagrams? Disclaimer : this is not a ""how to"" question. I would more like to know as a background information what are the different actual practices actually used. We know that UDP does not have a PMTU discovery like TCP. So I see several approaches to avoid IP fragmentation with UDP : Sending 512 bytes packets max (the UDP approach) re-implementing some PMTU  (using ICMP ""needs fragmentation"" message). Relying on local MTU (but how far is it reliable as UDP isn't a connected protocol how can it knows which interface its packets are going to go through ?) others... ? So what I would like is to have a ""background"" idea of which approaches are being used by current UDP programs/protocols especially regarding streaming/VoIP common applications ? Thanks by advance Jocelyn Limiting to 576 bytes is very common. Most of the Internet protocols such as DNS do this. Most real-time streaming protocols also use smaller packets since it has the added benefit of providing lower serialization delay and less impact if a single packet is lost. Some protocols have ways of negotiating a larger packet size though often not in a way that's as robust as PMTU discovery (DHCP for example allows a maximum message size negotiation). There's also stuff that defaults to 1500 or so and lets the user lower it if necessary. Most implementations of SNMP seem to do something like this. In any event the DF bit isn't generally set so the consequence of being overly optimistic is fragmentation not brokenness. Interesting answer thanks ! If anyone have specific information about SIP & skype I'm interested also. According to http://arxiv.org/pdf/cs/0412017 skype uses a 67-byte UDP payload"
556,A,iPhone app architecture considering good design is it better for each view controller to manager their own connection / networking / loading or to centralize it in the app delegate or a separate object? Context: I have a multi-tab app each with a navigation controller and a number of view controller below. Each view controller is doing networking loading XML and images. Currently i have it setup that it calls to the app delegate to get the xml asynchronously processing it and then calling back the top view controller to display the info and then launching a separate process of loading the images into an array and sending callbacks for when each is loaded. From an architectural view-point is it better to have more networking code in each of the view controllers or calling back to the app delegate? Ideas / opinions? TIA. When I have a situation like this I tend to create an object that handles all the networking stuff for me. That way I end up being able to write code like: [netObj getXML:somePlace]; The main reason I like this approach is because it keeps my code base tidy and minimizes duplicated code.  It's going to make much more sense to have this in each View Controller I think. The way you've got it set up now sounds a bit weird - you must either be using delegation so that the App Delegate can speak to each view controller or you have a ton of references to your View Controllers in your app delegate which you probably don't need. I'd imagine your app delegate is cluttered and I'm curious as to how you're handling things like if the user decides to stop looking at a particular view before the XML related to that view has been sent back to your app and parsed. If you're worried about having code duplication in your View Controllers you can probably mitigate that by using Categories. In the end though I think it's probably best for domain objects to handle this and not the View Controllers. For example in viewWillAppear you get or create an instance of a domain object and kick off a getData method which has the view controller as a delegate. All of the requesting/parsing is done in your domain object and when it's completed it sends your view controller a getDataDidFinish message or something like that. What I have been doing is placing most of the networking calls in the app delegate and when the xml parser returns it checks which view is the top view controller and then calls reload table on that. But the problem with this approach is that there is far too much code in the app delegate. I have opted to refactor out all the networking into a new singleton. It simplifies the app delegate a bit but not as much as I'd like.
557,A,"NamedPipeServerStream C# proper usage - I keep getting IOExceptions I've been searching for a while but it seems to be difficult to find any definite answers on how to recover from exceptions thrown by calling NamedPipeServerStream.BeginWaitForConnection. I have an application that sets up a named pipe server on a well known name and listens for messages to perform certain actions. It works fine and dandy until the application is restarted. This usually results in an IOException saying ""The pipe is being closed."" How can I correctly recover from this exception and use the same name for my named pipe? Any good resources on a good production worthy implementation of a named pipe server would be appreciated. If you're sending the message immediately after starting the process. You should call wait for pipe drain before closing the handle. Make sure to WaitForPipeDrain on the stream before close. - Good luck! Wow! this worked for me because I tried sending a message to the pipe as the process was starting. But I closed the client connection before the server had a chance to start up. Calling waitforpipedrain before close ensured that the server connected to the pipe before I closed the handle."
558,A,"Safe use of HttpURLConnection When using HttpURLConnection does the InputStream need to be closed if we do not 'get' and use it? i.e. is this safe? HttpURLConnection conn = (HttpURLConnection) uri.getURI().toURL().openConnection(); conn.connect(); // check for content type I don't care about if (conn.getContentType.equals(""image/gif"") return; // get stream and read from it InputStream is = conn.getInputStream(); try { // read from is } finally { is.close(); } Secondly is it safe to close an InputStream before all of it's content has been fully read? Is there a risk of leaving the underlying socket in ESTABLISHED or even CLOSE_WAIT state? According to http://docs.oracle.com/javase/6/docs/technotes/guides/net/http-keepalive.html and OpenJDK source code. (When keepAlive == true) If client called HttpURLConnection.getInputSteam().close() the later call to HttpURLConnection.disconnect() will NOT close the Socket. i.e. The Socket is reused (cached) If client does not call close() call disconnect() will close the InputSteam and close the Socket. So in order to reuse the Socket just call InputStream close(). Do not call HttpURLConnection disconnect().  is it safe to close an InputStream before all of it's content has been read You need to read all of the data in the input stream before you close it so that the underlying TCP connection get's cached.I have read that it should not be required in latest java but it was always mandated to read the whole response for connection re-use Check this post: keep-alive in java6 Very interesting. This question is actually background for a problem I have where I'm seeing LOTS of CLOSE_WAIT sockets to the same IP but because of caching (I don't call URLConnection.disconnect() explicitly) I expect there to be only one which should be reused. @Joel:By calling the `HttpUrlConnection.disconnect()` the underlying tcp socket is closed. By closing the input stream the underlying tcp socket is pooled for later reuse.The only caveat is that the whole response (OR the whole error response) should be read from the input stream in order for the tcp connection to be cached. This has always been adviced regardless you actually need the whole data from the stream. Check the post in my answer What if you don't read ANY data as per my first example - I'm guessing you still need to close the IS but will it still not be cached if no data is read but it's still closed. @Joel:Please read the post I mention in my answer Good article thanks. A couple of things I'm still not clear on. 1) How long is a cached connection kept for before it is discarded? I could not see a ""discard after 60s inactivity"" type setting. 2) It wasn't clear to me what state the connection would be left in after calling close but not before reading all the content - it says it will not be available for re-use/caching which is fine - but will the underlying socket actually be closed? @Joel:Your question is related to the HTTP protocol.The connection MUST remain alive for the timeframe specified by the server in the HTTP response (server sends in HTTP header the max number of requests this connection can be used or the max time-period to keep the connection open).The http client must honor this and this is also the behavior of HttpURLConnection.If server sends no such info in the response the connection is closed really soon (I think arround after few seconds of inactivity) not to waste resources.  If you really want to make sure that the connection is close you should call conn.disconnect(). The open connections you observed are because of the HTTP 1.1 connection keep alive feature (also known as HTTP Persistent Connections). If the server supports HTTP 1.1 and does not send a Connection: close in the response header Java does not immediately close the underlaying TCP connection when you close the input stream. Instead it keeps it open and tries to reuse it for the next HTTP request to the same server. If you don't want this behaviour at all you can set the system property http.keepAlive to false: System.setProperty(""http.keepAlive""""false""); Thanks. Assuming the connection is not in use do you know for how long it is cached before being closed and is there any way to control this timeout period?  Here is some information regarding the keep-alive cache. All of this information pertains Java 6 but is probably also accurate for many prior and later versions. From what I can tell the code boils down to: If the remote server sends a ""Keep-Alive"" header with a ""timeout"" value that can be parsed as a positive integer that number of seconds is used for the timeout. If the remote server sends a ""Keep-Alive"" header but it doesn't have a ""timeout"" value that can be parsed as a positive integer and ""usingProxy"" is true then the timeout is 60 seconds. In all other cases the timeout is 5 seconds. This logic is split between two places: around line 725 of sun.net.www.http.HttpClient (in the ""parseHTTPHeader"" method) and around line 120 of sun.net.www.http.KeepAliveCache (in the ""put"" method). So there are two ways to control the timeout period: Control the remote server and configure it to send a Keep-Alive header with the proper timeout field Modify the JDK source code and build your own. One would think that it would be possible to change the apparently arbitrary five-second default without recompiling internal JDK classes but it isn't. A bug was filed in 2005 requesting this ability but Sun refused to provide it. interesting thanks. Nice research on a poorly documented subject. Thank you for sharing.  When using HttpURLConnection does the InputStream need to be closed if we do not 'get' and use it? Yes it always needs to be closed. i.e. is this safe? Not 100% you run the risk of getting a NPE. Safer is: InputStream is = null; try { is = conn.getInputStream() // read from is } finally { if (is != null) { is.close(); } } The second question was with reference to the underlying socket state i've deliberately posted an incomplete snippet with regards to full runtime code safety. I really want to know if there's a danger of a socket left in CLOSE_WAIT or ESTABLISED by closing the socket before all content is read Or `IOUtils.closeQuietly(is)`"
559,A,"how can I get the other users ip address on network with c#? I want to get the ip address of devices which's are on my wireless network and I want to detect when someone connect to network? How can handle network connection in c#? Thanks. Since ""you"" are not the router (or whatever kind wireless dispatcher it is) there is no safe way for you to learn when someone connects to the network. Sure most OS would broadcast their connection on the network and such but it's not ""mandatory"" to do so. What I would suggest is to query all the possible IP. WAIT! Don't scream yet. My guess is that you're aware of the wireless IP range of the network which I guess shouldn't be THAT big. If you do some kind of Ping on a timer on each IPs you won't know exactly when the client connected but you'll know he's there. This is true only if the router don't block the Pings on the LAN and if the clients accept to respond to the ping. hımm I will try that the real time when he connect is not important  especially If I use I can detect the time when he connect approximately. thanks."
560,A,in_addr_t to string I have an IP address stored in in_addr_t and I want to create the string representation of this data type ie in_addr_t to 10.0.0.1. How can I do that? Try inet_ntop as documented here:- manpagez @cateof: You can simply pass the address of your `in_addr_t` variable as the `src` argument with `AF_INET` as the `af` argument. inet_ntop takes const void *restrict src as argument. My type is in_addr_t. How can I convert between the two of them?  char *inet_ntoa(struct in_addr in); is the desired function. inet_ntoa() seems to be considered deprecated. inet_ntoa takes an in_addr that is not the same as inet_addr_t. Please consider changing your answer.
561,A,"Ascii on TCP socket Anyone could pass me an example of sending Ascii msg over TCP?(couldnt find example on the net) thanks ray. It is nearly unbelievable that you could not find *any* example in the WWW: http://download.oracle.com/javase/tutorial/networking/index.html Here's an example of writing to and reading from an echoing server. A simplified excerpt:  Socket echoSocket = null; PrintWriter out = null; BufferedReader in = null; try { echoSocket = new Socket(""taranis"" 7); out = new PrintWriter(echoSocket.getOutputStream() true); in = new BufferedReader(new InputStreamReader( echoSocket.getInputStream())); } catch (UnknownHostException e) { System.err.println(""Don't know about host: taranis.""); System.exit(1); } catch (IOException e) { System.err.println(""Couldn't get I/O for "" + ""the connection to: taranis.""); System.exit(1); } BufferedReader stdIn = new BufferedReader( new InputStreamReader(System.in)); String userInput; while ((userInput = stdIn.readLine()) != null) { out.println(userInput); System.out.println(""echo: "" + in.readLine()); } he needs ASCII any reader in such case is useless just socket's input/output stream will do (no need to wrap 'em w/ BufferedXXX) So this example wont work for me? I disagree with bestsss on both arguments. Firstly using a reader is safer (if one days he switches from ASCII to Unicode there will be no trouble). Secondly using a buffered reader is necessary for speed (without buffering you lose a lot of time due to constantly polling the socket). Lastly the combination of both is fast unicode compatible and provides handy methods (like readLine() rather than handling an array of bytes)"
562,A,How much different is windows and linux in network programming? I'm reading <<Understanding Linux Network internals>>but not sure how much of it will also apply to windows? I think you will find that they are very similar as both are based on the standard Berkley sockets API. Assuming you are programming in C here you'll see the same sorts of functions with similar usage in both windows and unix (listen accept bind etc). There are of course many windows-specific socket functions though mostly for things like async IO. Here's a list of the Windows socket functions so you can have a look for yourself. In your link I saw `winsock` and `winsock2`what's the main difference and which one should we use now? winsock2 is just the current version of winsock. It's backwards-compatible with the original winsock but just adds additional functions and behavior (mostly for dealing with protocols other than TCP/IP)
563,A,"Why can't I connect with a non-browser application to this url? Anyone knows why this url rejects connection requests being sent by a non-browser application (wgetcurl elinks!): http://sube.garanti.com.tr https://sube.garanti.com.tr/isube/login/en It's my bank account and I'm trying to make my transfers with a script but as you see this super secured servers do not allow me. Any suggestions? Azer Some websites check the User-Agent: header. You might have to configure your downloader to identify itself as ""Mozilla"" rather than itself.  Well I've tried doing this: wget http://sube.garanti.com.tr which timed-out. but doing this: wget https://sube.garanti.com.tr/isube/login/en gave me website's source. It is frames base and I'm getting the frames definitions. The reason for that is probably that the site is inaccesible through normal (http) connection you have to use secured one (https). However as a rule of thumb I'd try to set User-Agent: header for any such application as noted by pjc50.  AFAIK wgte curl opens the site in the server side. So check for your firwall (if any) and see if it is blocked. Also the site can also be blocking incoming requests. Its a banking site and we can expect some security restrictions.  Maybe it's because you need to login before doing anything at this url? You can login into sites by using perl's LWP for example by submitting login forms properly. P.S. I can't connect to sube.garanti.com.tr with my browser either. Yep the two url you give let to nowhere for me too.  Try this: wget --referer=""http://www.google.com"" --user-agent=""Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.6) Gecko/20070725 Firefox/2.0.0.6"" --header=""Accept: text/xmlapplication/xmlapplication/xhtml+xmltext/html;q=0.9text/plain;q=0.8image/png*/*;q=0.5"" --header=""Accept-Language: en-usen;q=0.5"" --header=""Accept-Encoding: gzipdeflate"" --header=""Accept-Charset: ISO-8859-1utf-8;q=0.7*;q=0.7"" --header=""Keep-Alive: 300"" This may trick the site into thinking you have a ""legitimate"" browser thank you very much!"
564,A,"Erlang-B formula sumation in php I've tried to port the following sum in a php for loop this way:  $prod = 1; for($i=0;$i<$_POST[""capacity""];$i++){ $prod = $prod * (($_POST[""capacity""] - (i+1)) / $toffered); } ?> p(c) is: <?php echo floatval(1.00/floatval((1+ floatval($prod)))); ?><br /> <br /> but for some reason it seems to give me the wrong result. Any hints on what is wrong? EDIT: i've modified the initial value of prod as well as adding brackets for i+1 which is subtracted from the capacity. The results aren't better still. $prod = 0.0; for($i=1;$i<$capacity;$i++){ $prod = (1.0 + $prod) * (floatval($i) / $toffered); } ?> p(c) is: <?php echo (1.0 / ( 1.0 + $prod)); ?> % works!  I think you are actually not doing the sum only the product part (the dots ...). If I'm not mistaken you'll need 2 nested loops here one for i = 1 to c (computing the sum) and one for 1 to i (computing the product)."
565,A,"How to write a http1.0 proxy server in c in linux? I must develop proxy server that work with only HTTP 1.0 in Linux and by c . I need some hint to start developing . I assume you are confident in using linux and the language c (no hints for that else don't start with developing a proxy) Read and understand the RFC 1945 HTTP/1.0 (pay attention to the specific mentioning of proxy) Determine what kind of proxy you want (web/caching/content-filter/anonymizer/transparent/non-transparent/reverse/gateway/tunnel/...) Start developing the server Basic steps Open port Listen on port Get all request sent from the client to that port (maybe make the whole thing multithreaded to be able to handle more than 1 request at a time) Determine if it is a valid HTTP 1.0 request Extract the request components Rebuild the request according to what type of proxy you are Send the new request Get the response Send response to client +1 for being tolerant and at least trying to provide a helpful answer. ""Send response to client"" how to ? with table ? which information must be saved ? the response is whatever the website returned to the proxy after accepting the http request (should be the html of the web page or a message form the server).  How to create a proxy server: Open a port to listen on Catch all incoming requests on that report Determine the web address requested Open a connection to the host and forward the request Receive response Send the response back to the requesting client Additionally: Use threads to allow for multiple requests to the server. for better performance (or if you like some challenge) try using a threadpool instead of just a bunch of threads http://en.wikipedia.org/wiki/Thread_pool_pattern"
566,A,"Search for host with MAC-address using Python I'd like to search for a given MAC address on my network all from within a Python script. I already have a map of all the active IP addresses in the network but I cannot figure out how to glean the MAC address. Any ideas? This article ""Send hand-crafted Ethernet Frames in Python (ARP for example)"" seems to be exactly what you are looking for.  Mark Pilgrim describes how to do this on Windows for the current machine with the Netbios module here. You can get the Netbios module as part of the Win32 package available at python.org. Unfortunately at the moment I cannot find the docs on the module.  I don't think there is a built in way to get it from Python itself. My question is how are you getting the IP information from your network? To get it from your local machine you could parse ifconfig (unix) or ipconfig (windows) with little difficulty.  You would want to parse the output of 'arp' but the kernel ARP cache will only contain those IP address(es) if those hosts have communicated with the host where the Python script is running. ifconfig can be used to display the MAC addresses of local interfaces but not those on the LAN.  Depends on your platform. If you're using *nix you can use the 'arp' command to look up the mac address for a given IP (assuming IPv4) address. If that doesn't work you could ping the address and then look or if you have access to the raw network (using BPF or some other mechanism) you could send your own ARP packets (but that is probably overkill).  as python was not meant to deal with OS-specific issues (it's supposed to be interpreted and cross platform) I would execute an external command to do so: in unix the command is ifconfig if you execute it as a pipe you get the desired result: import os; myPipe=os.popen2(""/sbin/ifconfig""""a""); print(myPipe[1].read()); Python has OS-specific modules so I think you answer is misleading. The pythonic way to handle this would be a module to have an OS independent interface that could be sub classed by OS specific plugins.  If you want a pure Python solution you can take a look at Scapy to craft packets (you need to send ARP request and inspect replies). Or if you don't mind invoking external program you can use arping (on Un*x systems I don't know of a Windows equivalent).  You need ARP. Python's standard library doesn't include any code for that so you either need to call an external program (your OS may have an 'arp' utility) or you need to build the packets yourself (possibly with a tool like Scapy.  It seems that there is not a native way of doing this with Python. Your best bet would be to parse the output of ""ipconfig /all"" on Windows or ""ifconfig"" on Linux. Consider using os.popen() with some regexps."
567,A,"IPv6 address to domain name I'm searching for a function that take an IPv6 address as argument and returns the domain name. To make it clear 2a00:1450:8006::68 returns ipv6.google.com. (The aim is to give this domain name to the getaddrinfo function.) Thanks :-) edit1 : I've tried getaddrinfo(""2a00:1450:8006::68"" ""http"" NULL &result);  it returns ""address family for hostname not supported"" and getaddrinfo(""ipv6.google.com"" ""http"" NULL &result); return an error ""no address is associated with hotname"". EDIT2 : I agree with you i've trouble with IPV6 system I've tried http://test-ipv6.com/ and it appears that I've got no IPV6 adress but with ifconfig it returns : adr inet6: fe80::15b:fcff:fe65:d516/64 Scope:Lien No idea about your code but I should mention that 2a00:1450:8006::68 actually has no reverse DNS. It would help if you actually tried to use an IPv6 address that has reverse DNS. Seems like a roundabout way to get an address structure to me. Why not just pass ""2a00:1450:8006::68"" to `getaddrinfo`? It sounds like your machine and/or network doesn't support IPv6 - is it supposed to work? Absolutely every single interface gets an IPv6 address in the fe80::/64 range if your stack is even capable of IPv6. Make that fe80::/10. Differing sources say different things. But I believe the standards say fe80::/10. I think you do not have a valid IPv6 configuration. getaddrinfo() will only return IPv6 answers that are reachable so if your system does not have an IPv6 address with global scope and a route to the resolved address the result will be removed from the result set. The basic idea is that you call getaddrinfo once and get a list of addresses to connect to -- if that list were to include unreachable addresses programs would have to run into a timeout first before trying another address. ""Address family for hostname not supported"" means that it has recognized that the address is an IPv6 address that need not be resolved via DNS so it tries to match it against the list of allowed address families fails and returns the error. Resolving the host name attempts to get an ""A"" record for the host name as that is appropriate for the only address family supported locally. No such record exists hence it returns the information that no record exists. Since it never asked for the IPv6 address (that would have been pointless) it doesn't complain about the address family mismatch here. I think you're right about what's happening but I also think this is a bug. The behavior you describe is only supposed to happen if the `AI_ADDRCONFIG` is set in the hints. @R.: For glibc supplying `hints` as `NULL` is equivalent to `ai_family = AF_SPEC` and `ai_flags = AI_V4MAPPED | AI_ADDRCONFIG`. This would seem to be against the POSIX spec. What is `AF_SPEC`? @R.: I meant `AF_UNSPEC` there. The deviant behaviour is noted in the ""NOTES"" section of the glibc `getaddrinfo(3)` man page as something that is _""...considered an improvement on the specification.""_.  You're right to use getaddrinfo as the first step but it cannot do reverse-dns lookups for you. You'll need to use getaddrinfo to convert the string form of the address to a sockaddr which you can then pass to getnameinfo to do the reverse lookup. With that said I think Carl's comment is also relevant. It seems like your system is configured not to support IPv6..."
568,A,"File Descriptor Sharing between Parent and Pre-forked Children In Unix Network Programming there is an example of a Pre-forked server which uses message passing on a Unix Domain Pipe to instruct child processes to handle an incoming connection: for ( ; ; ) { rset = masterset; if (navail <= 0) FD_CLR(listenfd &rset); /* turn off if no available children */ nsel = Select(maxfd + 1 &rset NULL NULL NULL); /* 4check for new connections */ if (FD_ISSET(listenfd &rset)) { clilen = addrlen; connfd = Accept(listenfd cliaddr &clilen); for (i = 0; i < nchildren; i++) if (cptr[i].child_status == 0) break; /* available */ if (i == nchildren) err_quit(""no available children""); cptr[i].child_status = 1; /* mark child as busy */ cptr[i].child_count++; navail--; n = Write_fd(cptr[i].child_pipefd """" 1 connfd); Close(connfd); if (--nsel == 0) continue; /* all done with select() results */ } As you can see the parent writes the file descriptor number for the socket to the pipe and then calls close on the file descriptor. When the preforked children finish with the socket they also call close on the descriptor. The thing which is throwing me for a loop is that because these children are preforked I would assume that only file descriptors which existed at the time the children were forked would be shared. However if that was true then this example would fail spectacularly yet it works. Can someone shed some light on how it is that file descriptors created by the parent after the fork end up being shared with the children process? You can send (most) arbitrary file descriptors to a potentially unrelated process using the FD passing mechanism in Unix sockets. This is typically a little-used mechanism and rather tricky to get right - both processes need to cooperate. Most prefork servers do NOT do this rather they have the child process call accept() on a shared listen socket and create its own connected socket this way. Other processes cannot see this connected socket and there is only one copy of it so when the child closes it it's gone. One disadvantage is that the process cannot tell what the client is going to request BEFORE calling accept so you cannot handle different types of requests in different children etc. Once one child has accept()ed it another child cannot. Yeah I can't actually see many reasons to use a pre-forked server at all these days; I would pretty much always choose a pre-threaded server. The only case I can think of where I would run a pre-forked server would be maybe for security reasons where you want each clients memory space isolated from any other clients so that hacking or generating an error in one client doesn't take down the whole system or expose sensitive user data. Preforked servers are good because they prevent memory leaks from breaking stuff. The child processes can quit and be recycled without interrupting service reclaiming memory. forked servers also shield you from 3. party stuff that's not thread safe and assumes there's only one execution thread. There's lots of such libraries around.  Take a look at the Write_fd implementation. It uses something like union { struct cmsghdr cm; char control[CMSG_SPACE(sizeof(int))]; } control_un; struct cmsghdr *cmptr; msg.msg_control = control_un.control; msg.msg_controllen = sizeof(control_un.control); cmptr = CMSG_FIRSTHDR(&msg); cmptr->cmsg_len = CMSG_LEN(sizeof(int)); cmptr->cmsg_level = SOL_SOCKET; cmptr->cmsg_type = SCM_RIGHTS; *((int *) CMSG_DATA(cmptr)) = sendfd; That is sending a control message with type SCM_RIGHTS is a way unixes can share a file descriptor with an unreleated process."
569,A,"Twitter API for iPhone Possible Duplicate: Is there an iPhone SDK API for twitter? I am trying to use the Twitter API but am not looking to use OAuth/some other person's library. I would like basic code for a user name and password and than a tweet. Is this possible? I have been browsing the web for anything on this for at least 7 hours and have come up empty handed. I found one but it didn't work. Please help me with this. Thanks. If you want to look at the code I found you can download it here. The problem is that it says it works but my Twitter wall doesn't reflect that. To test it just put your username and password in the viewDidLoad. Accept I said I didn't want to use oauth I just wanted to do It a basic way. I don't want to use someone else's library. Basic auth is no longer supported. You must use OAuth there is no other way. What is your objection to using OAuth? That is by the way the ""basic way"". MGTwitterEngine works quite well http://mattgemmell.com/2008/02/22/mgtwitterengine-twitter-from-cocoa Also if you saw the Apple iOS5 Keynote earlier this week you might have noticed that Twitter is built into iOS5. If you are targeting your app to be ready for iOS5 I would recommend using the native SDK. Problem is that it uses OAuthConsumer which is one big memory leak. I was looking into it but I would like my apps to still be 4.2 compatible. Plus I said I didn't want to use anyone else's libraries or use aouth. Any other ideas. Is there an easy way to get around it?  There are quite a few libraries that makes this very easy. Just look around. MGTwitterEngine is one such library that works great. Also check out ShareKit. Plus I said I didn't want to use anyone else's libraries or use aouth. Any other ideas. Um aren't you required to use OAuth now with the Twitter API? Not sure because I don't use Twitter in my app. That was one of my questions I wasn't positive but I saw tons of examples that didn't use it. AFAIK OAuth used to be optional but I'm pretty sure that as of June 2010 it became mandatory. How do I use oauth without someone elses lib. I don't want attribution. You could write your own code? But really that's a huge task. There's nothing unprofessional about attribution lots of high-profile apps attribute libraries and other things. I looked at some of the libraries and it had a (webpage) to log in. If you look at the pulse app on the app store it just has 2 fields and its not a website. How do you do this??? I really think they're using an OAuth library and just making it look like it's a website. Sorry I haven't used Pulse so I don't know what it looks like. If you like you can also use a different OAuth library: [Twitter-OAuth-iPhone](https://github.com/bengottlieb/Twitter-OAuth-iPhone). You can view a nice tutorial [here](http://mobile.tutsplus.com/tutorials/iphone/twitter-api-iphone/). Not sure about whether attribution is required or not. That is one that is a website.  Twitter API requires OAuth. If you only want to use username and password you need to emulate browsing the Twitter web and authenticating. That's fragile and less clean than using OAuth code. I'm not going to bother posting details. However if you are planning a future release in iOS 5 you need less than 100 lines: http://www.pastie.org/2057025 I want it to also work on ios4.2"
570,A,"C++ network multi application protocols I am programming a Client/Server application for my project in C++. The whole application protocol was given or discussed by the working group. The main idea is that we have 3 protocols. Text-protocol: send and receive some information in string format between client and server. Binary-protocol: client sends continiously some status data to server. Binary-protocol: client sends continiously some data like sound/video/images/text All Protocols should run on different ports. I implemented a Socket-Class which is responsible to create and listen socket accept the connection from the client. Also there is a function to receive/send string-based data and receive/send binary-based data. In the next step I wanted to define 3 Classes. Each of them should be responsible for creating one socket in new Thread and responsible for the protocol which was defined for that port (s. 1-3). So at the end I will get 3 Sockets (1 Socket for one Port). My question is if I think in right direction? Maybe you can recommend my some design patterns for using different application protocols. It would be great if you can recommend me some projects or code which can be similar with my project. Thank you. yeah it looks like you're on the right track. Are you using `boost` library? actually not. because the specification of each protocol is really specific. :-) and I am not so good in the C++. So it can be really hard to learn language and use some library. At least for me. I found really good book for Network programming. So I can implement all what I want by myself :-) you can use `boost` for generic sockets it will make things easier for you and let you concentrate on the project-specific functionality but its up to you. I do agree with the responder below though that a generic socket implementation should be common to all the protocols and separate of them. So nobody knows some example of similary projects? :-) You should decouple your socket class from the 3 protocol handlers - don't have methods for both text and binary data handling on the Socket or you unintentionally encourage people to mix and match data types over the same socket which is clearly not what you want. Your socket should provide simple connect/disconnect data transmission and receipt functionality and the decoding and encoding of sent/received data is then done in a different object likely picked from 3 new classes (one per protocol). On a general note I question the use of text data. It's inefficient compared to virtually any serialization library you could name. You could be trading a little extra debuggability for a lot of hard-written data parsing and error checking code and concomitant CPU cycle wastage. If the text data is fairly simple (not actually structured like XML say) then this is less of a concern. Protocol 2. might be implementable using UDP rather than TCP if the status info is not mission-critical. That's one less connection you'd have to manage. I can't agree with the serialization comments. There's nothing in the post to indicate that the data is binary. He states it's text and there's nothing wrong with transferring text directly. Thank you for your answer. Right I can make send/receive function more abstract. And all decoding/enconding in each Class which are responsible for Protocol implementation. I am not sure that I can use some serialization libraries because our specification of protocol is really specific. :-) yeah the text is just one sentence. somethink like client writes ""I am alive"". All protocols should be TCP. @Jay - agreed in the context as now clarified by OP. If the text was XML or similar heavily structured data that just happens to be sent as text I would prefer serialization.  You might consider using enet. It does reliable and unreliable UDP communications and will do most of the heavy lifting of the communications for you."
571,A,"Read from network problem .NET I try to develop an simple smtp server the problem is when I try to read message from network all messages are a string with a lot of ""/0"" instead of typed command. I use telnet to connect to my application and to type messages. this is my code: public void StartListen() { SMTPParser parser = new SMTPParser(); SMTPResponder responder = new SMTPResponder(); Listening = true; IPEndPoint endPoint = new IPEndPoint(IPAddress.Parse(""127.0.0.1"") 25); TcpListener listener = new TcpListener(endPoint); listener.Start(); while(Listening) { byte[] data = new byte[2048]; Socket socket = listener.AcceptSocket(); socket.Receive(data); string cmd = Encoding.ASCII.GetString(data); if(cmd !="""") parser.Parse(cmd responder); } } Socket.Receive returns the number of read bytes. Currently you're always getting a string from the 2048 bytes of the buffer even if only a few bytes are read. Change to: int length = socket.Receive(data); string cmd = Encoding.ASCII.GetString(data 0 length); it helped thanks a lot"
572,A,"Multiple threads for multiple ports? To start off I don't know very much about network programming... So given that I have a program (process) which needs to listen on 3 ports... Two are TCP and the other UDP. The two TCP ports are going to be receiving large amounts of data every so often (could be as little as every 5 minutes or as often as every 20 seconds). The third (UDP) port is receiving constant data. Now does it make sense to have these listening on different threads? For instance when I receive a large amount of data from one of the TCP ports I don't want my UDP stream interrupted... are these common concerns for network programming? Thanks guys... feel free to ask clarification questions if I'm not being clear. I'll be using the Boost library on Windows if that has any bearing. EDIT: After reading this again I realized I'm not asking a specific question lol... I guess I'm just looking for some thoughts/ideas/guidance on this issue and how to manage multiple connections. Thanks I suggest you follow the advice of one of the ""boost:asio/select/Windows Completion Port API"" and ""thread pools"" answers (and mark it as correct). Note that for your application doing 1 thread per port is the simplest implementation. While you might be able to say you will never need more than 4-6 sockets today some day you might need this app to scale to many more sockets. So you might save yourself (or someone who follows you) a whole lot of trouble doing it with thread pools today. The responses to this question are kind of irritating. Everyone seemed to think you wanted to build a web server. I don't know the answer (which is why this is just a comment) but I suspect that if the UDP stream is high throughput and you want to avoid dropped UDP packets you'd really want it to be able to run on a different processor. First ...are these common concerns for network programming? Yes threading issues are very common concerns in network programming. Second your design idea of using three threads for listening on three different ports would work allowing you to listen on all three ports simultaneously. As pointed out in the comments this isn't the only way to do it. Last one design that's common in network programming is to have one thread listen for a connection then spawn a new helper thread to handle processing the connection. The original listener thread just passes the socket connection off to the helper then goes right back to listening for new connections. You don't need multiple threads to listen on different ports. Use select() or equivalent to wait for input events on all the listening sockets. is select() something from Boost? If not could you point me towards the documentation for this function so I could find a boost equiv? Thanks karunski select is from your operating system. In boost see the asio library which uses select (if appropriate) in its implementation. @karunski: Thanks. I edited my answer to get rid of that. Select is part of the original BSD socket API and is implemented in Windows and Unix-like operating systems. Check the MSDN or man pages. Boost::asio is a cross-platform abstraction around select() and other equivalent asynchronous IO APIs.  In general avoid threads unless necessary. On a modern machine you will get better performance by using a single thread and using I/O readiness/completion features. On Windows this is IO Completion Ports on Mac OS X and FreeBSD: kqueue(2) on Solaris: Event ports on Linux epoll on VMS QIO. Etc. In boost this is abstracted by boost::asio. The place where threads would be useful is where you must do significant processing or perform a blocking operating system call that would add unacceptable latency to the rest of the networking processing. This is wrong. You will have terrible performance if you try to get a single thread to handle multiple sockets. Rubbish! As you scale up the number of sockets performance will drop significantly if you have a thread per socket. There are many examples of this architecture including most modern web servers. For older well known examination see: http://www.kegel.com/c10k.html Have you actually tried to scale these things up?  Using threads one per receiving connection will help keep the throughput high and prevent one port's data from blocking the processing of another ports. This is a good idea especially since you're talking about only having 3 connections. Spawning three threads to handle the communication will make your application much easier to maintain and keep performant.  Multiple threads do not scale. Operating system threads are far too heavy weight at the scale of thousands or tens of thousands of connections. Granted you only have 3 so it's no big deal. In fact I might even recommend it in your case in the name of simplicity if you are certain your application will not need to scale. But at scale you'd want to use select()/poll()/epoll()/libevent/etc. On modern Linux systems epoll() is by far the most robust and is blazingly fast. One thread polls for socket readiness on all sockets simultaneously and then sockets that signal as ready are either handled immediately by the single thread or more often than not handed off to a thread pool. The thread pool has a limited number of threads (usually some ratio of the number of compute cores available on the local machine) wherein a free thread is grabbed whenever a socket is ready. Again you can use a thread per connection but if you're interested in learning the proper way to build scalable network systems don't. Building a select()/poll() style server is a fantastic learning experience. For reference see the C10K problem: http://www.kegel.com/c10k.html It might scale to 4 or 5 connections... maybe 6 at most. This application is fairly specific in use. Thanks for the tips!"
573,A,Measure connection speed and show it in a web application we have a Java software to receive and send customer and contracts data from and to our servers. The software runs in a web browser. Sometimes our agents complain about the lag but it is related to their network connection. It's our thought to build in some kind of network monitor to show the current network speed. My first thought was it to implement a java version of the Ping program to measure the time before the ping and after the ping. With this information I would have been able to calculate the connection speed. The problem is that Java doesn't support SOCK_RAW type sockets so I can't send ICMP packets. Due to security reasons I can not create some kind of UDP client-server-connection to use UDP sockets. The servers with our customer and contract data is not modifiable due to security reasons so you can't use any sockets at all. In my opinion there doesn't seem to be a convenient way to solve this with Java. My colleage has another approach. He thought about downloading a picture to measure the download time but this is no good solution to permanently measure the connection speed. So are there any usable possibilities to measure the connection speed with Java (JSFJSP) or Javascript (jquery) and embed it on a web site? Have you considered using Flash or Flex? @Zoidberg we can not use some kind of proprietary technology. so you mean no one wants to pay for any technology. Just a hint FLEX is free is the dev environment that costs money. @Zoidberg: I mean that we are not allowed to modify our existing applications for using other technologies than Java or Javascript. This is something I can not decide. That really sucks even on the client machine? if its a web app the client is really outside of the security zone... unless this is only hosted on a local network. To do a good measurement of the network connection you should probably try and send as much information as possible to see how long it took. A picture would be great but another approach might be to measure the RTT (Round trip time) to your servers to detect possible latency-related problems. If you don't want to program it yourself and do not have the need to bundle it with your software you might use http://www.speedtest.net for example :-) The problem is that there has to be something like a special JavaScript or the like on client site. The application runs on the server side. So more and more I don't think that there is a practical solution for this. I came to the conclusion that downloading a picture to measure kbit/s is the only one solution for this. In this case the webpage could only show the speed when I click on the (download) button to check the speed. It cannot show it continuously.
574,A,How can I exchange data via IP on my T-Mobile cell phone? I have multiple T-Mobile Android cell phones and I am writing an app to go on all the phones that exchanges data via IP. Is there a way to get the private IP addresses of the cellular connections and send data to each phone using those addresses? Thanks how much data? are the phones in proximity? You might wish to create some kind of intermediary web service that all of the devices talk to to share their IPs. Some kind of simple RESTful API that I device could call to POST their IP/MEID and fetch the other devices' IPs. You can use something like this to get your device IP.
575,A,"Speeding up socket send behavior (in Python) I have a script which sends 5-10 requests a second to the server. The most crucial requirement i have is a limit of requests per second. It must always be the specific figure not more and no less. To do it i send requests after a given interval of time (minus time required to send previous request). Problem: some requests are sent fast enough however others take too much time at sock.sendall() step. I believe this is because send buffer is full and execution is blocked until buffer is cleared. What can i do to flush that buffer quicker? One of the options i tried is to disable Nagle: sock.setsockopt(socket.IPPROTO_TCP socket.TCP_NODELAY 1) but it didn't seem to improve things. Another option which even sounds too wrong to try it is to set send buffer to the length of the request before each sendall() call. Is there anything i can do to get more predictable requests per second? One more option i just thought about: have several processes which will do a small amount of requests per second each hopefully it will make results more predictable. OS in question is Centos. Update: It seems that my error in setting socket options after connect. Looks like size buffer can only be set prior connect() call. Same with TCP_NODELAY. Haven't yet had time to test if it makes any difference. Some of that may be out of your control being due to network problems or throttling at the server side. The most crucial requirement i have is a limit of requests per second. It must always be the specific figure not more and no less. That requirement is completely unimplementable via TCP. You would also need real-time guarantees of service times at the peer. Some ""flushing"" would atleast make it so that data is sent more regularly and predictably from my side. At the moment i too often go above the limit. As you say this can also due to the server on the other end... @alexeit Your requirement as stated cannot be implemented. ""More regularly"" doesn't meet that requirement either.  (From How can I force a socket to send the data in its buffer?) You can't force it. Period. TCP makes up its own mind as to when it can send data. Now normally when you call write() on a TCP socket TCP will indeed send a segment but there's no guarantee and no way to force this. There are lots of reasons why TCP will not send a segment: a closed window and the Nagle algorithm are two things to come immediately to mind. Read the full post it is quite in-depth and clarified some of the things for me eg when disabling Nagle algorithm makes sense and so on."
576,A,"Connecting to MySQL Database over server I'm quite a beginner when it comes to working with networks and the like so apologizes up front. A while back I set up a mySQL database locally on my machine and have built a number of simple programs that work with it. (aka root:localhost sort of thing). This has been great but I'm now hoping to allow other colleagues at my work to access the database from their machines but I have no idea how. Likely there will be some network protection issues (firewalls etc) so that may need to be taken into account... (although I have IT's help on this neither IT or myself really know what is required to 'connect' to the database). For example is it just an IP I need? Do I have to change the setup of my database? I understand that localhost would not work from my colleagues computer's for obvious reasons I have no idea what would go in its place for others to access it. I also do not mind having my machine run as the dedicated database machine... I would not be able to run it off a dedicated server or anything like that beyond my machine. Any help would be much appreciated! Thanks EDIT: Thanks for the suggestions - will go through them tomorrow when I'm a bit more awake! They look solid though. You will use following commandline to connect - mysql -u<user-id> -p<password> -h<your-hostname-or-ipaddress> For applications running on different machines trying to connect to your database you only need to replace 'localhost' with your machine's hostname or ipaddress. In general if you are able to ping your machine from a different system your database can be connected to from that machine just use whatever name you used for 'pinging' in place of localhost. Great answer! There was one piece that was missing for my puzzle though. mysql -u -p -h --port if you are running on a nonstandard port. Your answer helped me get most of the way there. Thanks.  Use your workstation IP address or workstation name. You will need to enable remote access. Go to this link for how: http://www.cyberciti.biz/tips/how-do-i-enable-remote-access-to-mysql-database-server.html  For example is it just an IP I need? Yes. You'll be much happier if you set up proper domain names but a domain name is just an alias for the IP address. Do I have to change the setup of my database? No but... You have to add some user credentials to support remote logins. That's a change but not a change to a schema. It's changes to the permissions. I understand that localhost would not work from my colleagues computer's for obvious reasons I have no idea what would go in its place for others to access it. What MySQL Admin tools are you using? Often there is good help there. You must read this: http://dev.mysql.com/doc/refman/4.1/en/connecting.html or this: http://dev.mysql.com/doc/refman/5.0/en/connecting.html For whatever version is appropriate. It's very clear. A user is identified by a username@hostname. You can specify IP addresses (or even ""%"" for the hostname. Thanks - will give those a look tomorrow  First of all what your colleagues need are: The IP Address where MySQL server is running. User and Password to connect remotely Have the por 3306 open on the network A MySQL Client (mysql workbench mysql query browser toadheidi or just the Command Line tool). When you create user in MySQL the have to be something like this: 'root'@'localhost' That means the user will work if you connect from localhost with the user root. So you can create user allowed to connect from anywhere: 'juanperez'@'%' And finally you have be careful about what privileges are you granting to them... Do not forget to comment a line in the options file of the MySQL that says ""bind-address"" (this options prevents remote connection). Cool - thanks a lot. Will give it a shot tomorrow. Port 3306 eh..... Will look into that."
577,A,"how to count bytes sent and received per TCP connection (system-wide)? e.g. recent versions of TCPView has such functionality: showing bytes sent/received per TCP connection (counting starts when TCPView is launched). is it possible w/o packet sniffering? does windows provides any API for this? I haven't found such Performance Counter how to enumerate all connections are described here EDIT: does TDI help to receive per-socket transfer statistics? NetBIOS? any links where to dig? @Anders: as I got it's system wide info not per TCP connection right? I have this question as a favorite I won't provide this answer but simply a comment to guide but have you looked at http://msdn.microsoft.com/en-us/library/aa394291%28v=vs.85%29.aspx ? Seems that way. Have you found anything else? I googled and found this question because TCPView is not showing me sent/received bytes in 64 bit Win7. Several other people have reported this but no solution has ever been posted. Since you guys are elbows deep in the internals any ideas why this could be happening? The sysinternals version of netstat (netstatp) does this. IIRC it uses SNMP to gather its info. Search the net and find a version you're comfortable with. The file names are netstatp.c and netstatp.h Sysinternals no longer publishes netstatp that I am aware of. You can also go here and get tcpview and/or tcpconv one of which is available in source form. +1. tnx for the info. I didn't find any code related with per-connection bytes sent/received in [netstatp.c](http://www.google.com/url?sa=t&source=web&cd=2&ved=0CBcQFjAB&url=http%3A%2F%2Fsource.sphene.net%2Fsvn%2Fgoim%2Ftrunk%2Fnet.sphene.goim.netstat%2Fnative%2Fsrc%2Fnetstatp.c&ei=y9Q0TeiiCMaeOsK1vLcC&usg=AFQjCNEY07vDNoSm8XcEgwxmvjOHlxZoaQ&sig2=ywN__2eZr7fkMxh_Df0vKw). tcpview is not available in source and tcpconv doesn't provide such functionality. any additional info what to dig in SNMP? @robin hood:Sorry I cannot find the old code that sysinternals used to publish. The next time I go to my physical office I'll look to see if it's there. I know you can get the info from SNMP but I do not know the OIDs.  Check the WinSock LSP Sample project at http://connect.microsoft.com/WNDP/Downloads You will find a sample in nonifslsp\sockinfo.cpp which ""illustrates how to develop a layered service provider that is capable of counting all bytes transmitted through a TCP/IP socket."" +1: tnx interesting info still looking for something easier  My best bet would be hooking the ""send"" API calls and recording the amount sent each time. Although this really doesn't seem worth it I'm pretty sure it would work. Good luck! `SetWindowsHookEx()` can't hook `send()` No you shouldn't need to get your hands dirty in the kernel: you can hook `send()` using a layered protocol as a service provider within Windows Sockets as Maxim's answer referred to. Ah yea I spoke too soon sorry. Anyway maybe you would have to look at kernel-mode code as that's where all send()'s end up.. just a thought.  all I have basically fully reverse tcpview 3.0.2 and implement the same feature as its according to what I have learnt. tcpview use ETW for monitoring network activity. The key APIs are StartTrace OpenTrace ProcessTrace. Use the KERNEL_LOGGER_NAME and enable EVENT_TRACE_FLAG_NETWORK_TCPIP flags. Then you can retrieve network activity data from EventCallback then parse it as TcpIp_TypeGroup1 and other structures. According to the document these structures are only supported from vista. However you can call and use it in xp(guess from reverse) and 2003(My environment is 2003 no test on xp). Certainly you have to define all these structures by yourself. From vista win provides some APIs for retrieving every connections statistic information. Such as GetPerTcpConnectionEStats GetPerUdpConnectionEStats you can get more details from MSDN. Also from vista you can use RAW Socket to finish the same work(more precise I think). Before vista RAW Socket can't retrieve SEND packets it's a pity.  I want to implement this function also so I reverse tcpview 3.0.2. I found tcpview use a WMI performance counter MSNT_TcpIpInformation. But MSNT_TcpIpInformation is not supported in xp and 2003 officially. here is the description you can reference to. http://www.scriptinternals.com/new/us/support/Internal/WMI_MSNT_TcpIpInformation.htm by the way MSNT_TcpIpInformation have no information about packets so tcpview just increment sent and revd packets everytime. here is the disassemble: CPU Disasm Address Hex dump Command Comments 0040B41B |. 83E8 02 SUB EAX2 ; Switch (cases 2..3 3 exits) 0040B41E |. 74 29 JE SHORT 0040B449 0040B420 |. 83E8 01 SUB EAX1 0040B423 |. 75 40 JNE SHORT 0040B465 0040B425 |. 8B57 1C MOV EDXDWORD PTR DS:[EDI+1C] ; Case 3 of switch Tcpview.40B41B 0040B428 |. 0196 90060000 ADD DWORD PTR DS:[ESI+690]EDX 0040B42E |. 119E 94060000 ADC DWORD PTR DS:[ESI+694]EBX 0040B434 |. 8386 C0060000 ADD DWORD PTR DS:[ESI+6C0]1 0040B43B |. 119E C4060000 ADC DWORD PTR DS:[ESI+6C4]EBX 0040B441 |. 5E POP ESI 0040B442 |. 5F POP EDI 0040B443 |. 5D POP EBP 0040B444 |. 5B POP EBX 0040B445 |. 83C4 3C ADD ESP3C 0040B448 |. C3 RETN 0040B449 |> 8B47 1C MOV EAXDWORD PTR DS:[EDI+1C] ; Case 2 of switch Tcpview.40B41B 0040B44C |. 0186 78060000 ADD DWORD PTR DS:[ESI+678]EAX 0040B452 |. 119E 7C060000 ADC DWORD PTR DS:[ESI+67C]EBX 0040B458 |. 8386 A8060000 ADD DWORD PTR DS:[ESI+6A8]1 0040B45F |. 119E AC060000 ADC DWORD PTR DS:[ESI+6AC]EBX 0040B465 |> 5E POP ESI ; Default case of switch Tcpview.40B41B 0040B466 |. 5F POP EDI interesting. ""is not supported in xp and 2003 officially"". but it's supported right? what about vista/w7? from vista windows provides some APIs for get every connections statistics. such as GetPerTcpConnectionEStats GetUdpTcpConnectionEStats you can get more information from MSDN.  Have a look at the source code for BitMeterOS it works on xp+. you many also want to look at TCPDump/Libpcap as well. both of these monitor network network traffic libpcap will probably be what your after though there is also Winpcap a more windows orientated 'version' a simple tutorial on network traffic stats can be found here you'll also be interested in this for filtering based on connection and this for the size of the raw packets. checked BitMetereOS looks like its system-wide statistics not per socket/connection. Winpcap: as I remember it doesn't capture loopback traffic. Pls correct me @Andy T: according to the FAQ it doesn't but thats a windows problem: http://www.winpcap.org/misc/faq.htm however wireshark which uses winpcap has a method on how to do it: http://wiki.wireshark.org/CaptureSetup/Loopback though I'd just use a physical loop plug from work :P"
578,A,writing a http sniffer I would like to write a program to extract the URLs of websites visited by a system (an IP address) through packet capture.. I think this URL will come in the data section ( ie not in any of the headers - ethernet / ip / tcp-udp ).. ( Such programs are sometimes referred to as http sniffers  i'm not supposed to use any available tool ). As a beginner  I've just now gone through this basic sniffer program : sniffex.c.. Can anyone please tell me in which direction i should proceed.. i've just edited my question  now that i understood better... Sounds exactly like my old CSE 409 assignment.. I was researching on something similar and came across this. Hope this could be a good start if you are using linux - justniffer. http://justniffer.sourceforge.net/ There is also a nice http traffic grab python script that would help if you are looking to get information from HTTP requests.  What you might want is a reverse DNS lookup. Call gethostbyaddr for that.  Have a look at PasTmon. http://pastmon.sourceforge.net  If you are using Linux you can add a filter in iptables to add a new rule which looks for packets containing HTTP get requests and get the url. So rule will look like this. For each packet going on port 80 from localhost -> check if the packet contains GET request -> retrieve the url and save it This approach should work in all cases even for HTTPS headers. He's actually referring to iptables rules iptables being the built-in firewall. Ya  I am using Linux.. By rules  are you referring to conditional blocks within the program .. Oh  okay.. thanks !  Note: In the info below assume that GET also includes POST and the other HTTP methods too. It's definitely going to be a lot more work than looking at one packet but if you capture the entire stream you should be able to get it from the HTTP headers sent out. Try looking at the Host header if that's provided and also what is actually requested by the GET. The GET can be either a full URL or just a file name on the server. Also note that this has nothing to do with getting a domain name from an IP address. If you want the domain name you have to dig into the data. Quick example on my machine from Wireshark: GET http://www.google.ca HTTP/1.1 Host: www.google.ca {other headers follow} Another example not from a browser and with only a path in the GET: GET /ccnet/XmlStatusReport.aspx HTTP/1.1 Host: example.com In the second example the actual URL is http://example.com/ccnet/XmlStatusReport.aspx Hmmm  so i first need to parse the payload section  look for GET and Host portions and extract the URLs then.. Is that correct.. Not just GET. There are other http commands as well. You need to start by reading some documentation. Yep. That and POST and... Wireshark being an opensource tool  will I be able to get any code handy for this.. ( I dont have much time to write it as this is just a part of my project - which involves the URLs thus captured ) probably. I haven't perused the wireshark source but it'll be in there somewhere.  No there is not enough information. A single IP can correspond to any number of domain names and each of those domains could have literally an infinite number of URLs. However look at gethostbyaddr(3) to see how to do a reverse dns lookup on the ip to at least get the canonical name for that ip. Update: as you've edited the question @aehiilrs has a much better answer. how can i get the url of a site from network packet? You can't. You might as well ask how I can tell a person's hair colour from their email address. What if it's something like redhead1234@freecrapemail.com?
579,A,Login to windows xp programatically Anyone heard that its possible to login to windows xp programatically I want to modify the way the Domain Controller authenticates the user and if the user authenticated in my special way it returns the user name and password to the service working on the client machine so it should login the user using its passed credentials from the DC without waiting for user to insert his original credentials. This kind of stuff worries me. Pass credentials to local service? Really I didn't implement it yet but I hope it will work something about design modification may be done because of using Gina.dll anyway Thanks too much Have a look at this: Gina.dll MSDN Gina is the system that fingerprint readers etc. use to customise the login screen. You may be able to use this to achieve your purpose? To future proof your app (Vista and Windows 7) you may wish to look into these: MSDN Mag Vista Credential Providers Cheers I had always wondered how the hell windows allowed a fingerprint to substitute for a 3 finger salute so I looked it up one day... ;) Thanks Man I exactly want to login using fingerprint.
580,A,"How to migrate existing udp application to raw sockets Is there a tutorial for migration from plain udp sockets (linux C99/C++ recv syscall is used) to the raw sockets? According to http://aschauf.landshut.org/fh/linux/udp_vs_raw/ch03s04.html raw socket is much faster than udp. Application is client-server. client is proprietary and must use exactly same procotol as it was with udp server. But server can be a bit faster with raw sockets. What parts of udp I must to implement in server? Is there a ""quick migration"" libraries? Also which part of udp work will benefit from rawsockets - sending or receiving? this is for 2.6.12. raw is 25-40 % faster http://docs.google.com/viewer?a=v&q=cache:l1HoSmjLpGMJ:ftp://affonso:web@users.dca.ufrn.br/artigos/2007/iwt2007.pdf+udp+linux+performance&hl=en&pid=bl&srcid=ADGEESj-KwQe_NDKc5R_TfsNoiy2HTgfY3malOUewRHZt3sYOxeOt2dn7Xe8xiveefAybTLhotNYdQaA_wNe8udxMypZpNEY0AVqf538bxN9fvCaenMyHcyDJ_LkyOltrRKoeLScXw0u&sig=AHIEtbRdkM3H3Hpanm340WUvir2o3pqZEQ Steve-o is it a software (RDMAoE) or hardware? Does it require both machines to be in the same subnetwork (only switches are between them)? The speed performance comes from dropping the IP layer i.e. routing filtering port management etc. A good direction to look at would be RDMAoE that is RDMA over Ethernet talk to Mellanox. A raw socket allows you to communicate with lower level protocols like Ethernet IP etc. Yes going lower can give you some advantages however you have to balance that with what you are losing. In this case you mention that the server is written to use Udp protocol so on the wire the communications has to be Udp. Now if you go to use a raw socket you will have to make sure to send your application data encapsulated in a Udp packet. You will also need to write code to make sure you obey the Udp protocol and state machine so that to the server your client appears as just another Udp client. Doing all this requires writing a lot of code and has some downsides of increased maintenance increased cost to get it working correctly etc. I did not fully read the paper you linked above but the question to ask yourself is can you get the gains given in that research paper and replicate them for your scenario? In my opinion you should first try to figure out why your client is so slow. What are your requirements? Do you have any metrics as to what constitutes a good fast client? If I were you I would first measure the current implementation keeping in mind some metrics that are useful for the scenario for eg Bytes/sec transferred etc. Then I would profile the client to see where it is spending too much time and try to see if I can reduce the overhead and make it much faster. To summarize look for savings in the top of the stack (i.e in your application) before going down the stack. If your app is not written well then no matter how low you go you will not see the perf gains you expect. my appl is benchmark code for now it is doing recvfrom/sendto in the while. so there is nothing to ontimize in client/server. The OS udp/ip stack is limiting rate. yes this is ping-pong test and real application works in the same style. Client and server can be rewritten both in raw sockets."
581,A,"Keeping objects in sync across a network? In my application I have two DLLs. One is written in C# and the other in C++. They communicate with each other across a network. Each has a list of object instances that are expected to be in sync with each other at all times. What network libraries are available for doing this? Why can't you do both DLLs in the same language? There are various reasons which I won't get into. The short version is that it wasn't my decision :) Can Windows be assumed in both applications? @chrish. Yes the application will be windows only There is a project called Velocity which ""provides highly scalable in-memory application cache for all kinds of data."" It is only for managed code. Is the C++ managed code (or could it call managed code)? Here is a MSDN Magazine article on the project. unfortunately the C++ is unmanaged. And I don't think it'll ever be.  I don't understand your question particularly what you mean by ""in sync"" but maybe you are just casting a wide net. I toss out ICE. It's a lighter weight easier to use version of CORBA that supports C# and C++. Look at it. It may or may not be the least bit applicable to what you are doing.  This is a pretty vague question. First a library that works with both c++ and c#? Not likely. Second ""in sync"" how? Who has priority when there are changes made to both? What sort of locks are you going to have in place? This are all very important rules that will be highly specific to the type of operations and rules you're trying to go for. It's not unlike peer-to-peer networking for games I suppose The question was intentionally vague since I'm not sure what's out there. While I'm familiar with a few game-centric network libraries I don't know what's available for more serious applications.  I don't know if it is exactly what you had in mind but I wrote a system (C++ library with optional server process) to handle this sort of thing. It includes client APIs for C# C++ C Java and Python and they can all communicate with each other using the same data serialization protocol so it works well for cross-language and/or cross-platform communication. The two processes can communicate with each other directly or if you want to support N processes communicating you can run the server which can hold the shared objects in its 'central location' and let the various clients see them and notify them when they've been changed broadcast/multicast messages to each other etc. The code is all open source (BSD) and can be found here: https://public.msli.com/lcs/muscle/  I'm not sure I fully understand your situation but I'm wondering if you're talking about ensuring that both the client and server are on the same interface version during the initial connection. If they're not then the out of date side could either fail or request an update. That's basically what MMORPG's do.  You could use DCOM if everything's running on Windows.  This is actually a fairly difficult problem to get done correctly and as such there are many ways to approach it. I think the best way to do it would be to use something like Protocol buffers which has both a c++ and c# library. Depending on the size of your data you could simply serialize the entire data object and send it across the wire and then de-serialize it on the other side and then repeat this whenever the object changes. Of course then you might have problems with syncing if both sides change the object at the same time. In this case you might have to do something like Google Wave does and send diffs of the data and merge the changes together."
582,A,"How to measure network performance (how to benchmark network protocol) First a bit of a background. There are many various comparisons of distributed version control systems (DVCS) which compare size of repository or benchmark speed of operations. I haven't found any that would benchmark network performance of various DVCS and various protocols used... beside measuring speed of operations (commands) involving network like 'clone' 'pull'/'fetch' or 'push'. I'd like to know then how would you make such comparison; how to measure network performance of an application or how to benchmark network protocol. I envision here among others also measuring dependence of performance on both bandwidth of network and latency (ping time) of network; some protocols sacrifice latency in the form of more round-trip exchanges (negotiation) to send minimal required final ""pack"". I would prefer solutions involving only one computer if possible. I'd like to see open source solutions working on Linux. But I would also welcome more generic answers. Preferred OS: Linux Preferred languages: C Perl shell script Possible measurements: total number of bytes transferred from server to client and from client to server in one session; this can also be used to measure overhead of protocol (bandwidth) number of round-trips (connections) in one transaction (latency) dependency of speed of network operation (time it takes to clone/pull/push) from network bandwidth and from network latency (ping time) How to make such measurements (such benchmarks)? Added 02-06-2009: A simplest benchmark (measurement) would be a network version of time command i.e. command which run would give me number of bytes transferred and number of round trips / network connections during execution of a given command. Added 09-06-2009: Example imaginary output for mentioned above solution of network version of time command could look like the following: $ ntime git clone -q git://git.example.com/repo.git ... bytes sent: nnn (nn kiB) bytes received: nnn (nn kiB) avg: nn.nn KB/s nn reads nn writes Note that it is only an example output detailing kind of information one might want to get. Added 09-06-2009: It looks like some of what I want can be acheived using dummynet tool (originally) for testing networking protocols... Are you asking how to obtain the required data in order to come up with such benchmarks for an arbitrary networked program? Yes I'd like to have at minimum get number of total bytes transferred and perhaps number of changes from read to write for a given command something like time / times for CPU and memory and SystemTap iotimes for I/O. Added. Preferred operating system is Linux preferred languages include (in no order) C Perl shell script but can be Python or Java or other programming language. I think to really answer your question you should ideally tell us more about your preferred platform/OS as well as your preferred programming language. Also a detailed list of measurements that you are interested in would tell us what approach is most feasible. Possible answer would be to use SystemTap. Among example scripts there is nettop which displays (some of) required network information in the ""top""-like fashion and there is iotime script which shows I/O information in required form.  If I am understanding you correctly you are basically interested in something like Linux 'strace' (Introduction) for network-specific system calls? Possibly a combination of a profiler and a debugger for network applications (i.e. 'ntrace') providing a detailed analysis of various optional measurements? Under Linux the strace utility is largely based on functionality that is provided by the Linux kernel namely the ptrace (process tracing) API: Process Tracing Using Ptrace Playing with ptrace - part I Playing with ptrace - part II System Call Tracing using ptrace Bytecode Injection into a Running Process using Ptrace() Better process control in Linux with an improved ptrace() Using ptrace it should be possible to obtain most of the data that you're interested in. On Windows you'll probably want to look into detours in order to intercept/redirect Winsock API calls for inspection/benchmarking purposes. If you don't really need all that much low level information you can probably also directly use strace (on linux) and only use it to trace certain system calls for example consider the following line which would only trace calls to the open syscall (Using the additional -o FILE parameter you can redirect all output to an output file): strace -e trace=open -o results.log By passing an additional -v flag to strace you can increase its verbosity to get additional information (when working with SCMs like git that are composed of many smaller shell utilities and standalone tools you'll probably also want to look into using the -f flag in order to also follow forked processes). So what you would be interested in is all syscalls that are related to sockets namely: accept bind connect getpeername getsockname getsockopt listen recv recvfrom send sendto setsockopt shutdown socket socketpair (in the beginning you'll probably only want to look into dealing with the send.../recv... calls though) To simplify this you can also use ""network"" as parameter to trace which will trace all network-related calls: -e trace=network: Trace all the network related system calls. So a corresponding strace invocation could look like this: strace -v -e trace=acceptbindconnectgetpeernamegetsocknamegetsockoptlistenrecvrecvfromsendsendto setsockoptshutdownsocketsocketpair -o results.log -f git pull When the program is finished running you'll then mainly want to examine the log file to evaluate the data this can then be easily achieved by using regular expressions. For example when running the following in a linux shell: strace -v -o wget.log -e trace=connectrecvrecvfromsendsendto wget http://www.google.com The resulting log file contains messages like these: recv(3 ""HTTP/1.0 302 Found\r\nLocation: htt""... 511 MSG_PEEK) = 511 sendto(4 ""\24\0\0\0\26\0\1\3^\206*J\0\0\0\0\0\0\0\0""... 20 0 {sa_family=AF_NETLINK pid=0 groups=00000000} 12) = 20 Looking at the man pages for these two system calls it's obvious that 511 and respectively 20 are the number of bytes that are transferred. If you also need detailed timing information you can pass the -T flag to strace: -T -- print time spent in each syscall In addition you can get some statistics by passing the -c flag: -c: Count time calls and errors for each system call and report a summary on program exit. On Linux this attempts to show system time (CPU time spent running in the kernel) independent of wall clock time. If -c is used with -f or -F (below) only aggregate totals for all traced processes are kept. If you also need to examine the actual data processed you may want to look into the read/write specifiers: -e read=set: Perform a full hexadecimal and ASCII dump of all the data read from file descriptors listed in the specified set. For example to see all input activity on file descriptors 3 and 5 use -e read=35. Note that this is independent from the normal tracing of the read(2) system call which is controlled by the option -e trace=read. -e write=set: Perform a full hexadecimal and ASCII dump of all the data written to file descriptors listed in the specified set. For example to see all output activity on file descriptors 3 and 5 use -e write=35. Note that this is independent from the normal tracing of the write(2) system call which is controlled by the option -e trace=write. You can also customize the max length of strings: -s strsize: Specify the maximum string size to print (the default is 32). Note that filenames are not considered strings and are always printed in full Or have strings be dumped as hex: -xx: Print all strings in hexadecimal string format. So using strace for much of this seems like a good hybrid approach because it is very easy to do but still there's a good amount of low level information available if you find that you need additional low level information you may want to consider extending strace instead or filing corresponding feature requests with the strace project on sourceforge. However thinking some more about it a less involved and more platform-agnostic way of implementing a fairly simple network traffic benchmark would be to use some form of intermediate layer in between the client and the actual server: a server that's basically metering analyzing and redirecting the traffic to the real server. Pretty much like a proxy server (e.g SOCKS) so that all traffic is tunneled through your analyzer which can in turn accumulate statistics and other metrics. A basic version of something like this could probably be easily put together just by using netcat and some shell scripts more complex versions may however benefit from using perl or python instead. For a python implementation of a SOCKS server you may want to look into pysocks. Also there's of course twisted for python: Twisted is an event-driven networking engine written in Python and licensed under the MIT license. If you do need to have more low level information you'll probably really want to look into intercepting system calls though. If you also need protocol-specific efficiency data you might want to look into tcpdump."
583,A,"How does WSAStartup function initiates use of the Winsock DLL? How does WSAStartup function initiates use of the Winsock DLL? According to the documentation The WSAStartup function must be the first Windows Sockets function called by an application or DLL. It allows an application or DLL to specify the version of Windows Sockets required and retrieve details of the specific Windows Sockets implementation. The application or DLL can only issue further Windows Sockets functions after successfully calling WSAStartup. This function initializes WSADATA data structure but in socket programming we don't pass WSDATA to any function so how does the program comes to know about the Windows Sockets version and other details? For example in this code #include <stdio.h> #include <winsock2.h> #pragma comment(lib ""ws2_32"") void Run(int argc char* argv[]) { char* host = argc < 2 ? """" : argv[1]; struct hostent* entry = gethostbyname(host); if(entry) { struct in_addr* addr = (struct in_addr*) entry->h_addr; printf(""IP Address: %s\n"" inet_ntoa(*addr)); } else printf(""ERROR: Resolution failure.\n""); } int main(int argc char* argv[]) { WSADATA wsaData; if(WSAStartup(0x202 &wsaData) == 0) { Run(argc argv); WSACleanup(); } else printf(""ERROR: Initialization failure.\n""); } In this example I am initializing WSADATA data structure using WSAStartup() function and after wards I'm not passing wsaData anywhere. So how does my program comes to know about wsaData details? Thanks. WSAStartup has two main purposes. Firstly it allows you to specify what version of WinSock you want to use (you are requesting 2.2 in your example). In the WSADATA that it populates it will tell you what version it is offering you based on your request. It also fills in some other information which you are not required to look at if you aren't interested. You never have to submit this WSADATA struct to WinSock again because it is used purely to give you feedback on your WSAStartup request. The second thing it does is to set-up all the ""behind the scenes stuff"" that your app needs to use sockets. The WinSock DLL file is loaded into your process and it has a whole lot of internal structures that need to be set-up for each process. These structures are hidden from you but they are visible to each of the WinSock calls that you make. Because these structures need to be set-up for each process that uses WinSock each process must call WSAStartup to initialise the structures within its own memory space and WSACleanup to tear them down again when it is finished using sockets. In addition see this link: http://stackoverflow.com/questions/1869689/is-it-possible-to-tell-if-wsastartup-has-been-called-in-a-process on a previous question I asked and discovered the answer through self discovery. Thanks for the explanation. @Matt H Thanks for the additional information."
584,A,What is Address Family? When I read about socket programming I get to know that AF_ stands for Address Family. But literally a family should have many members. So what are the members of say AF_INET address family? In my opinion I think it would be more appropriate to say Address Type than Addresss Family. Also this applies to PF (Protocol Family). Thanks. **Type** could mean different things; a broadcast address is one type a multicast address is another type etc. Members of AF_INET address family are IPv4 addresses. Members of AF_INET6 address family are IPv6 addresses. Members of AF_UNIX address family are names of Unix domain sockets (/var/run/mysqld/mysqld.sock is an example). Members of AF_IPX address family are IPX addresses and so on. I don't think you really need to distinguish between family and type here they are merely synonyms except that family looks like more specialized well-suited for this purpose whilst type is too much general word.
585,A,Network programming and Packets interactions Greeting This month I will start working on my master thesis. My thesis's subject is about network security. I need to deal with network interfaces and packets. I've used shappcap before to interact with packets but I'm not sure if C# is the most powerful language to deal with network programing and packets. I worked a bit with wireshark and I saw how powerful it is and as you know winsharp is open source developed using C++. I'm not sure if I should use C# or C++ for network security programming and I want your through about the best language might be for network programming and packets interaction. should I use C# C++ or java or some thing else? please give me your advice. Thank you UPDATE .......................... I'm going to do different packet mining by taking each packet and read each field on it then use these values and in same stages I would modify some of the packets value then resend them back. I want to control the packet since it received by the network interface until it passes to the application layer. also i'm also working on network security. what you will you work on? can you more be clear? Go with the language you know the best. C/C++ probably would be better but if you don't know em well it can be tough. Use C++ Boost and Poco and you can do what you want. Boost asio is: Portable networking including sockets timers hostname resolution and socket iostreams. Poco library also provides solutions for network cryprography NetSSL ... and more. For more information you can visit www.boost.org and www.pocoproject.org I don't think Boost asio provides raw sockets. libpcap is the only portable option I know of for manipulating non-conformant packets.  You can use java if you like - jpcap works well.  You'd be able to do network programming using almost any language you want to. If you are equally comfortable in all of the languages you've mentioned you should determine what system libraries or APIs will you be interfacing with. For example if you will be doing packet-level network programming on a Unix system C would probably be your best best. If you want to integrate with Wireshark go with C++. If you want to use an Apache Commons component use Java. I suggest you come up with a more specific set of requirements for your actual program before trying to decide which language to use. i agree c has the most tutorials and best ever for socket programming but depends. he should be more clear to get the best answer. I'm going to do different packet mining by taking each packet and read each field on it then use these values and in same stages I would modify some of the packets value then resend them back. I want to control the packet since it received by the network interface until it passes to the application layer. Sounds like you want to use something in the *pcap family (WinPcap libpcap and so on). Either C or C++ would probably be the most common choices and have the most already-existing code out there that you could leverage.  I would suggest using C# since there is a very strong library called Pcap.Net that wraps WinPcap with .NET code. This should make it easy for you to receive send and interpret packets different packets of different protocols.  WireShark uses WinPCap so you could go that route as well. For security application is that a intrution detection system or do you actually want to drop offending packets? WinPCap SharpPCap etc. do not allow you you drop packets for this you will need to look at some kind of intermediate driver or look at Windows Filtering Platform (WFP) http://www.microsoft.com/whdc/device/network/WFP.mspx IMHO if you can find a callback driver that calls back to user mode and allows you to filter the packets from C# or C++ this would probably be fine for experimental purposes etc. but for a production solution I think you would need to stick to the kernel level to ensure that you can keep-up with the peek volume.
586,A,"Get client IP from UDP packages received with UdpClient I am developing an action multiplayer game with the help of the System.Net.Sockets.UdpClient class. It's for two players so one should open a server and wait for incoming connections. The other player inputs the host IP and tries to send a ""ping"" to make sure a connection is possible and there is an open server. The host then responds with a ""pong"". Once the game is running both have to send udp messages to each other so they both need the opponents ip address. Of course the server could also input the clients IP but that seems unneccessary to me. How can I get the clients IP from the udp package when the ""ping"" message is received? Here is my receiving code (server waiting for ping):  private void ListenForPing() { while (!closeEverything) { var deserializer = new ASCIIEncoding(); IPEndPoint anyIP = new IPEndPoint(IPAddress.Any 0); byte[] recData = udp.Receive(ref anyIP); string ping = deserializer.GetString(recData); if (ping == ""ping"") { Console.WriteLine(""Ping received.""); InvokePingReceiveEvent(); } } } In your example when a client connects the anyIP IPEndPoint object will contain the address and port of the client connection. heh so simple. thanks  private void ListenForPing() { while (!closeEverything) { IPEndPoint anyIP = new IPEndPoint(IPAddress.Any 0); byte[] recData = udp.Receive(ref anyIP); string ping = Encoding.ASCII.GetString(recData); if (ping == ""ping"") { Console.WriteLine(""Ping received.""); Console.WriteLine(""Ping was sent from "" + anyIP.Address.ToString() + "" on their port number "" + anyIP.Port.ToString()); InvokePingReceiveEvent(); } } } http://msdn.microsoft.com/en-us/library/system.net.sockets.udpclient.receive.aspx"
587,A,"During transmissions over localhost which layers are used in OSI Model? While transmiting data via localhost address or 127.0.0.1 which layers are used in the OSI model? I believe communication starts through application layer and goes down till some layer but no data goes through physical layer or does any? Traffic to 127.0.0.1 will be looped back by the internet layer of the TCP/IP model which is matched in the OSI model by the Network layer. This is the layer where routing and address resolution take place.  I believe (although I'd be happy to be corrected) that data may actually go down as far as Layer 2 (""Datalink Layer""). There's no need to add any protocol headers or encoding to handle loopback interfaces but never-the-less the operating system will normally treat the loopback interfaces as if they were real interfaces each with their own packet counters etc. There's no reason at all why a loopback interface can't be used for non-IPv4 protocols - indeed many current systems automatically put an IPv6 address on the loopback interfaces. In that sense a loopback interface is just a ""null"" Layer 2 device."
588,A,"Looking for regex/code for hostname/machine name validation Looking for hostname validation regex. Regular expression to match hostname or IP Address? In that link gentlemen propose a decent regex. I have a few problems/questions with that: On windows computers/networks names like 1abcd are allowed (verified on our local network) In the proposed regex the dot might appear only once. I'd assume that abc.def.gh is a valid hostname as well isn't it. Strange but also couldn't find any .NET class that can validate hostname string (is it the situation?). Any advice will be highly appreciated. Update: for any class/method proposal - please advice something that will work both in .NET/C# and SilverLight. In the proposed regex the dot might appear only once. I'd assume that abc.def.gh is a valid hostname as well isn't it. The dot MAY appear more than once. Test the regular expression here and you will see that it matches. Relevant fragment of the regular expression (first part is): ([a-zA-Z0-9]|[a-zA-Z0-9][a-zA-Z0-9\-]*[a-zA-Z0-9])\.)* On windows computers/networks names like 1abcd are allowed (verified on our local network) From wikipedia: The original specification of hostnames in RFC 952 mandated that labels could not start with a digit or with a hyphen and must not end with a hyphen. However a subsequent specification (RFC 1123) permitted hostname labels to start with digits. I referred to RFC 952. I will try to update the regular expression for hostnames to be compliant with RFC 1123.  Is Uri.CheckHostName a method that can help you ? If the hostname is not recognize the result will be ""Unknown"". The method doesn't work in Silverlight and the code is supposed to serve SL platform as well. I'd see [Dns.GetHostEntry](http://msdn.microsoft.com/en-us/library/system.net.dns.aspx) in Silverlight (but never use). It resolve the hostname and launch ArgumentException if it's not a valid hostname. On the right there is an other question [validate hostname string in Python](http://stackoverflow.com/questions/2532053/validate-hostname-string-in-python) with this regex ""(?!-)[A-Z\d-]{163}(?"
589,A,TraceRt command in the .NET CF Application I am trying to connect my Mobile device (WM 6.0 PE) through a GPRS Connectivity. I want to check the tracert of a server to which this device is connected as to check whether the connection is stable or not using Tracert. Is there anyway i can use the Tracert or Ping command to check the Latency of my connection in a windows mobile application. Thanks in advance Biju Theres a oldshool application called vxUtil you could used it can be found at http://www.cam.com/vxutil_pers.html
590,A,"QTcpServer retrieve more than one address I use QTcpServer (http://doc.qt.nokia.com/4.7/qtcpserver.html) from the Qt-Framework with C++. I started it with ret = tcpServer->listen(QHostAddress::Any 9871) With QHostAddress serverAddress () const I retrieve the Address from it. Which is 0.0.0.0. In my opinion it should listen on more than one address (like 127.0.0.1 and the LAN Address). Is there a way to retrieve more than one address from this class? Thanks! What do you mean by ""retrieve"" ? I'm not sure what your question is exactly but... Usually a 0.0.0.0 means that the socket listen to all interfaces that are both up and configured with a valid IPv4 address. If you want to get all the available IP addresses on the system you should enumerate the network interface then query their respective IPv4 address (Probably with an ioctl specifying SIOCGIFSWADDR). Ah ok. Nice to know that this is the meaning of 0.0.0.0. Thanks! that answered my question! @Herrbert: You're welcome :) Btw if you ever need the IPv6 equivalent it's `::`.  I agree with ereOn's answer above. If you want to list your network interface adresses take a look at QNetworkInterface. http://doc.qt.nokia.com/4.7/qnetworkinterface.html"
591,A,"WCF discovery finds the endpoint but the address is localhost I have a wcf discoverable service called ""GetNameService"" which is hosted on a PC @ 10.0.0.5:8732. It is hosted with wsHttpBinding and is discoverable thru' UdpEndpoint. I also have a client @ 10.0.0.9 which discovers for such services in the same network. When I ran the client I am able to discover the service but the end point of the service discovered is having localhost in it. How can this happen please help me out in this. Some more information App.config used <?xml version=""1.0"" encoding=""utf-8"" ?> <configuration> <system.web> <compilation debug=""true"" /> </system.web> <!-- When deploying the service library project the content of the config file must be added to the host's app.config file. System.Configuration does not support config files for libraries. --> <system.serviceModel> <services> <service name=""NameService.GetNameService""> <host> <baseAddresses> <add baseAddress = ""http://10.0.0.5:8732/Design_Time_Addresses/NameService/GetNameService/"" /> </baseAddresses> </host> <!-- Service Endpoints --> <!-- Unless fully qualified address is relative to base address supplied above --> <endpoint address ="""" binding=""wsHttpBinding"" contract=""NameService.IGetNameService""> <!-- Upon deployment the following identity element should be removed or replaced to reflect the identity under which the deployed service runs. If removed WCF will infer an appropriate identity automatically. --> <identity> <dns value=""localhost""/> </identity> </endpoint> <!-- Metadata Endpoints --> <!-- The Metadata Exchange endpoint is used by the service to describe itself to clients. --> <!-- This endpoint does not use a secure binding and should be secured or removed before deployment --> <endpoint address=""mex"" binding=""mexHttpBinding"" contract=""IMetadataExchange""/> </service> </services> <behaviors> <serviceBehaviors> <behavior> <!-- To avoid disclosing metadata information set the value below to false and remove the metadata endpoint above before deployment --> <serviceMetadata httpGetEnabled=""True""/> <!-- To receive exception details in faults for debugging purposes set the value below to true. Set to false before deployment to avoid disclosing exception information --> <serviceDebug includeExceptionDetailInFaults=""False"" /> </behavior> </serviceBehaviors> </behaviors> </system.serviceModel> </configuration> I opened up the browser in the client machine and typed the service address I get to see the service details page but still the wsdl link points to localhost (shown below) svcutil.exe http://localhost:8732/Design_Time_Addresses/NameService/GetNameService/?wsdl instead of svcutil.exe http://10.0.0.5:8732/Design_Time_Addresses/NameService/GetNameService/?wsdl Also the same application seems to work in my work place where there is a dns available and the systems are connect thru' a switch where as it doesn't work at my home where the pcs are connect thru' a router. Could this influence something ?!? Update It is not running in IIS it is hosted thru' a normal console application If you are running on IIS you have to have a default hostname rule for your site: Open IIS Right click on your website and click ""properties"" Click the ""Web Site"" tab and then click the ""Advanced"" button under the ""Web site identification"" heading You should have a default Identity for the website (TCP port 8732). Edit that and make sure the host header value is the domain name for your website (so it should say www.yourdomain.com) See http://forums.asp.net/p/1096811/1659596.aspx from more details. It is not hosted on IIS hosted thru' a console application  Solved it in code (for a tcp.net connection) by using. // Add discovery feature. m_Host.AddServiceEndpoint(new Discovery.UdpDiscoveryEndpoint()); m_Host.Description.Behaviors.Add(new Discovery.ServiceDiscoveryBehavior()); // Add ordinary service feature var binding = new NetTcpBinding(); binding.Security.Message.ClientCredentialType = MessageCredentialType.None; binding.Security.Transport.ClientCredentialType = TcpClientCredentialType.None; m_Host.AddServiceEndpoint( typeof(IMyServiceContract) new NetTcpBinding(SecurityMode.None) new UriBuilder { Scheme = Uri.UriSchemeNetTcp Port = 8732 Host = ""10.0.0.5""  Path = ""MyServiceContractPath"" }.Uri ); The only (perhaps smaller) problem that remains is which of the computers IP to set as host... The Uri entered is the one that is returned in the DiscoveryClient.Find(...) response.  In case you still need this it turns out you can just use a wildcard in baseAddress: <add baseAddress = ""http://*:8732/Design_Time_Addresses/NameService/GetNameService/"" />"
592,A,how to differentiate if client is using TCP or UDP from server side I am writing simple client-server program. Client send some messages to server using UDP or TCP. Server must be able to support both UDP and TCP. If client sends message using UDP sequence of method calls in client is socket()bind()sendto()recvfrom()close() and that in server is socket()bind()sendto()recvfrom()close(). If it uses TCP sequence of call in server is socket()bind()listen()accept()send()recv()close(). and that in client is socket()bind()connect()send()recv()close() In my program user/client is given choice in the start to select what he want to use UDP or TCP. So my main problem is how can I know or differentiate in the server side if the client is sending message using TCP or UDP. If it uses TCP I must call listen()accept()send()recv() and if it uses UDP I don't call listen()accept() but call sendto() and recvfrom(). So how can I differentiate/know this in the beginning so that I can make appropriate function calls. Thanks. So should I create two sockets in the server side one for UDP and another for TCP ? seg.server.fault yes. @seg.server.fault: I would recommend _not_ calling `bind()` in your client code. While it's technically possible (and required in very specific situations) the default behavior without a client-side `bind()` is usually what you want at the application layer. Also keep in mind that you _can_ call `connect()` on a UDP socket. It doesn't do any handshaking but it does mean you can use `send/recv` instead of `sendto/recvfrom` which may end up making the client logic simpler. just let the TCP socket listen on port X and do the UDP connections through port Y you can safely listen on UDP and TCP on the same port.  Well you could set up listening sockets for all UDP and all TCP. Then when you receive a packet you can open the header check if it's UDP/TCP and continue from there. This project example here: http://www.codeproject.com/KB/IP/CSNetworkSniffer.aspx?fid=373251&df=90&mpp=25&noise=3&sort=Position&view=Quick&fr=76#xx0xx demonstrates (with a handy parser for TCP and UDP) how to set up the sockets/listeners and distiguish between the two. Technically correct but practically wrong. When people ask questions about building socket-using applications they are really asking about how to do it with the socket API not with a lower-level slower more-complex network-sniffing API. That code example uses the classes and methods provided through the System.Net.Sockets API ...but it was more for the wording and description by the author who explains the process and what the various pats do. Then you can certainly write a higher-level app armed with that knowledge. Although I appreciate the code amount may occasionally frighten more than educate :) Given the level of (in)expertise of the OP I don't think it's very productive to recommend they subvert the TCP and UDP layers of the network stack. Better to work with the system than against it.  When you create the socket you pass a type - SOCK_STREAM (TCP) or SOCK_DGRAM (UDP) So the two kinds of traffic will be on two different sockets. That doesn't help the server side. The OP already knows that the client side will decide TCP vs. UDP. The issue is supporting them both on the server side and that's significantly more effort than simply changing the socket type.  Before the packet reaches you you don't know whether it's UDP or TCP. So you want to bind to both UDP and TCP sockets if you expect requests both ways. Once you did you just know which way it came by the socket you received the packet through.
593,A,Broadcast server presence over network I'm writing a client-server application where client should be able to automatically detect server presence in the network so that user would not have to mess with manual configuration. I think that server should broadcast its presence somehow. What is the best way to do it? Server is to be run on either Windows / Linux or OS X so the solution should be cross-platform. Client and server are written in C++. If you want to do it yourself you can use multicasting. Some (braindead) ways to do that are: 1) have the server periodically advertise it's presence to the subscribed clients followed by unicast transmissions between the client & the server; 2) have the client signal (a number of times) it's presence to the subscribed server(s) again followed by unicast transmissions between server & client. One more thing that needs to be considered it that the multicast will be all UDP and the unicast can be either TCP or UDP.  You should look into ZeroConf as a generic way of broadcasting this information. It's also known as Bonjour (formerly Rendezvous) on Apple systems but it's not an Apple-specific thing - plenty of other devices (both embedded and OSs) broadcast/use it. There are plenty of libraries that can deal with ZeroConf in plenty of languages - finding one is a matter of Googling it. Bear in mind that ZeroConf itself is the advertising/discovery part of the process and identifies a machine and a socket - but you can then do whatever you want over the socket(s) that you identify. For example Apache mod_zeroconf allows you to broadcast Apache sites. Your solution will be dependent on what licensing and usage rights you're dependent on.
594,A,Are there any good APIs for maintaining an open connection between Flash <--> Java I have a flash app which will send/receive a constant stream of data. the flash app should open a connection to the Java server keep it open and attempt to reconnect on socket failure. Are there any good APIs that offer this functionality? Smartfox Server is one option but if I had my way I'd embed an API in my app rather than contort my app to run under Smartfox. Ideally it would offer an API on the flash end another on the Java end and efficiently (java nio preferably) handle the network component in between. I would just define handlers on both the client & server. I'm not sure this is relevant to your situation so I make it a comment rather than an answer but there is an ActionScript API for socket communication http://help.adobe.com/en_US/AS3LCR/Flash_10.0/flash/net/Socket.html You could use BlazeDS. It's an open source RTMP server from Adobe written in Java. You don't need any special library on the client side as RTMP is native in Flash. There are many examples on the web: here  here.
595,A,"UDP Delay Potential I have an application that consists of numerous systems using UDP clients in remote locations. All clients send UDP packets to a central location for processing. In my application it is critical that the central location knows what time the packet was sent by the remote location. From a design perspective would it be ""safe"" to assume that the central location could timestamp the packets as they arrive and use that as the ""sent time""? Since the app uses UDP the packets should either arrive immediately or not arrive at all? The other option would be to set up some kind of time syncing on each remote location. The disadvantage to this is that then I would need to continually ensure that the time syncing is working on each of potentially hundreds of remote locations. My question is whether timestamping the UDP packets at the central location to determine ""sent time"" is a potential flaw. Is it possible to experience any delay with UDP? How accurate do you need it to be? Down to the minute? Second? Millisecond? Down to the second. I don't know anything at all about network programming so I'm just throwing this out there as an option but I wonder if you could assume there is a delay and easily compensate for it? Something like a handshake procedure... so when a remote client connects to the central location it gets pinged maybe 1000 times and you keep a Dictionary? I'm sure it's not a great solution but it sure seems better than assuming no delay. One option would be to require that the client clocks are synchronized with an external atomic clock. To ensure this and to make your UDP more robust the server can reject any packets that arrive ""late"" (as determained by the difference in the server's clock--also externally sync'd--and the packet timestamp). If your server is acking packets it can report to the client that it is (possibly) out of sync so that it can re-sync itself. If your server is not acking packets your whole scheme is probalby going to fail anyhow due to dropped or out of ordered packets.  There is always a delay in transmission and UDP packets do not have guaranteed delivery nor are they guaranteed to arrive in sequence. Would need more information about the context to recommend a better soluion.  For seconds resolution you can use time stamping of when you receive the packet but you still need to use a sequence number to block re-ordered or duplicate packets. This can make your remote stations less complex as they won't need a battery backed clock or synchronisation techniques. For millisecond resolution you would want to calculate the round trip time (RTT) and use that offset to the clock on the receiver. Unless you are using the precision time protocol (PTP) in a controlled environment you can never trust the clock of remote hosts. ""Unless you are using the precision time protocol (PTP) in a controlled environment you can never trust the clock of remote hosts."" This is precisely why I began using the host time to stamp each packet. Even with all of my remote hosts using NTP there are time drifts that equate to a few seconds."
596,A,Effecient network server design examples written in C I am interested in learning how to write extremely efficient network server software and I don't mind getting my hands dirty with pointers sockets and threading. I'm talking a server being able to handle thousands of concurrent connections. There is not much processing for each client but a little. Do you know of any code examples for really efficient network servers? Optionally points for small well documented code that is cross-platform as well. Have a look at nginx lighttpd and varnish for some popular high performance http servers. BTW I am currently working on combining edge-triggered epoll with multithreading (plus user-level swapcontext-style threads/fibers) - see http://svn.cmeerw.net/src/nginetd/trunk/ for some work-in-progress code (although this one is written in C++).  This may not be exactly what you are looking for but I briefly recall looking at Space Tyrant a few years back and thinking it sounded cool. http://librenix.com/?inode=6240 Hope it helps!  an ldap-server handles lots of transactions per second http://de.wikipedia.org/wiki/Lightweight_Directory_Access_Protocol  Read this http://www.evanmiller.org/lxr/http/source/  ACE is a wise choice.  You'll find a lot of good references and discussion about building highly scalable network servers on Dan Kegel's The C10K problem page. A list of open-source server implementations using the various described approaches can be found near the bottom of the page. It's a good page been there before. But I still feel that the example part of the page is lacking mostly in edge-triggered solutions. Hence the question :)
597,A,"glib network connection example Can you advice some network connection example made with glib/gio libraries. There is quite a good reference manual but no full example even for basic things. It will be used for simple sending and receiving files as a part of program. I've found unanswered question on SO http://stackoverflow.com/questions/2417681/need-help-implementing-simple-socket-server-using-gioservice-glib-glib-gio and try to use it as an example but still want to find finished example. How about like this? There is similar question at Fetch a file from web: in GTK using C #include <gio/gio.h> int main() { const gchar *uri = ""http://stackoverflow.com/questions/5758770/""; GFile *in; GFile *out; GError *error = NULL; gboolean ret; g_type_init(); in = g_file_new_for_uri(uri); out = g_file_new_for_path(""/tmp/a""); ret = g_file_copy(in out G_FILE_COPY_OVERWRITE NULL NULL NULL &error); if (!ret) g_message(""%s"" error->message); return 0; }"
598,A,Any difference between socket connection and tcp connection? Are these 2 concepts refer to the same thing? Do they have difference? In my opinion they are different and socket connection is based on tcp connection. A socket contains an IP address and port and it could only connect to another socket but an IP address and port in the same machine could be connected with many other IP addresses and ports with TCP connection. Is this right? TCP/IP is a protocol stack for communication a socket is an endpoint in a (bidirectional) communication. A socket need not be TCP based but it is quite often the case. The term socket is also often used to refer to the API provided by the operating system that allows you to make a connection over the TCP/IP stack for example the Winsock API provides an API for connections over the TCP/IP stack on Windows. A socket is mapped uniquely to an application as the ports are managed for you by the operating system. Further reading: http://en.wikipedia.org/wiki/Internet_socket and http://en.wikipedia.org/wiki/Winsock  Socket connection implies two peer connected with each otherProtocol can be TCP or UDP.So connection does not specify type of connection.it is generic term for connection. When you say TCP connection it implies two nodes are connected using TCP protocol. Thanks for the clarification. So how about the socket connection based on TCP? What's the difference between TCP based socket connection and TCP connection?
599,A,"Socket programming - API doubt There was this question posted in class today about API design in socket programming. Why are listen() and accept() provided as different functions and not merged into one function? Now as far as I know listen marks a connected socket as ready to accept connections and sets a max bound on the queue of incoming connections. If accept and listen are merged can such a queue not be maintained? Or is there some other explanation? Thanks in advance. Under the hood bind assigns an address and a port to a socket descriptor. It means the port is now reserved for that socket and therefore the system won't be able to assign the same port to another application (an exception exists but I won't go into details here). It's also a one-time-per-socket operation. Then listen is responsible for establishing the number of connections that can be queued for a given socket descriptor and indicate that you're now willing to receive connections. On the other hand accept is used to dequeue the first connection from the queue of pending connections and create a new socket to handle further communication through it. It may be called multiple times and generally is. By default this operation is blocking if there are no connections in the queue. Now suppose you want to use an async IO mechanism (like epoll poll kqueue select etc). If listen and accept were a single API how would you indicate that a given socket is willing to receive connections? The async mechanism needs to know you wish to handle this type of event as well. With quite different semantics it makes sense to have them apart.  listen() means ""start listening for clients"" accept() means ""accept a client blocking until one connects if necessary"" It makes sense to separate these two because if they were merged then the single merged function would block. This would cause problems for non-blocking I/O programs. For example lets take a typical server that wants to listen for new client connections but also monitor existing client connections for new messages. A server like this typically uses a non-blocking I/O model so that it is not blocked on any one particular socket. So it needs a way to ""start listening"" on the server socket without blocking on it. Once listening on the server socket has been initiated the server socket is added to the bucket of sockets being monitored via select() (called poll() on some systems). The select() call would indicate when there is a client pending on the server socket. Then the program can then call accept() without fear of blocking on that socket. Oh I guessed as much. I thought one couldnt maintain many connections simultaneously. Thanks. @ryan happy to be able to help  listen(2) makes given TCP socket a server socket i.e. creates a queue for accepting connection requests from the clients. Only the listening side port and possibly IP address are bound (thus you need to call bind(2) before listen(2)). accept(2) then actually takes such connection request from that queue and turns it into a connected socket (four parts required for two-way communication - source IP address source port number destination IP address and destination port number - are assigned). listen(2) is called only once while accept(2) is usually called multiple times."
600,A,"How can i get more than one jpg. or txt file from any folder? Dear Sirs; i have two Application to listen network Stream : Server.cs on the other hand; send file Client.cs. But i want to send more files on a stream from any folder. For example. i have C:/folder whish has got 3 jpg files. My client must run. Also My server.cs get files on stream: Client.cs:  private void btn_send2_Click(object sender EventArgs e) { string[] paths= null; paths= System.IO.Directory.GetFiles(@""C:\folder"" + @""\"" ""*.jpg"" System.IO.SearchOption.AllDirectories); byte[] Dizi; TcpClient Gonder = new TcpClient(""127.0.0.1"" 51124); FileStream Dosya; FileInfo Dos; NetworkStream Akis; foreach (string path in paths) { Dosya = new FileStream(path  FileMode.OpenOrCreate); Dos = new FileInfo(path ); Dizi = new byte[(int)Dos.Length]; Dosya.Read(Dizi 0 (int)Dos.Length); Akis = Gonder.GetStream(); Akis.Write(Dizi 0 (int)Dosya.Length); Gonder.Close(); Akis.Flush(); Dosya.Close(); } } Also i have Server.cs  void Dinle() { TcpListener server = null; try { Int32 port = 51124; IPAddress localAddr = IPAddress.Parse(""127.0.0.1""); server = new TcpListener(localAddr port); server.Start(); Byte[] bytes = new Byte[1024 * 250000]; // string ReceivedPath = ""C:/recieved""; while (true) { MessageBox.Show(""Waiting for a connection... ""); TcpClient client = server.AcceptTcpClient(); MessageBox.Show(""Connected!""); NetworkStream stream = client.GetStream(); if (stream.CanRead) { saveFileDialog1.ShowDialog(); // burası degişecek string pathfolder = saveFileDialog1.FileName; StreamWriter yaz = new StreamWriter(pathfolder); string satir; StreamReader oku = new StreamReader(stream); while ((satir = oku.ReadLine()) != null) { satir = satir + (char)13 + (char)10; yaz.WriteLine(satir); } oku.Close(); yaz.Close(); client.Close(); } } } catch (SocketException e) { Console.WriteLine(""SocketException: {0}"" e); } finally { // Stop listening for new clients. server.Stop(); } Console.WriteLine(""\nHit enter to continue...""); Console.Read(); } Please look Client.cs: icollected all files from ""c:\folder""  paths= System.IO.Directory.GetFiles(@""C:\folder"" + @""\"" ""*.jpg"" System.IO.SearchOption.AllDirectories); My Server.cs how to get all files from stream? is it too hard question? or is it not clear? It's not a typical way of doing things. Usually you would write a single file in a single stream and repeat the process for multiple files. You risk data corruption this way as you'll have to place some kind of marker in the stream to know where to split it.  The easiest way is to use multiple sockets one for commands and one (or more) for sending files. The below code have NOT been tested just wrote it up to show what I mean. It's multithreaded and can receive multiple files from the same client just call sendfiles multiple times in the client. You might want to add error/exception handling if you decide to finish the code. Server.cs using System; using System.IO; using System.Net; using System.Net.Sockets; using System.Text; namespace Test2 { public class Server { private readonly TcpListener _listener = new TcpListener(1234); public void Start() { _listener.BeginAcceptTcpClient(OnClient null); } private void OnClient(IAsyncResult ar) { // End async accept and start wait for a new connection again TcpClient client = _listener.EndAcceptTcpClient(ar); _listener.BeginAcceptTcpClient(OnClient null); // Let's start receiving files from the accepted client. var context = new Context {Client = client Buffer = new byte[8196]}; client.GetStream().BeginRead(context.Buffer 0 context.Buffer.Length OnReceive context); } /// <summary> /// Got some stuff from a client /// </summary> /// <param name=""ar""></param> private void OnReceive(IAsyncResult ar) { // got a file command var context = (Context) ar.AsyncState; int bytesRead = context.Client.GetStream().EndRead(ar); string cmd = Encoding.UTF8.GetString(context.Buffer 0 bytesRead); string[] parts = cmd.Split(';'); string command = parts[0]; // want to send another file if (command == ""sendfile"") { // context info for receiving files var client = new FileClient(); client.FileName = parts[1]; client.Size = long.Parse(parts[2]); client.FileStream = new FileStream(""C:\\"" + client.FileName FileMode.CreateNew FileAccess.Write); // startup listener where we are going to receive the file. var listener = new TcpListener(IPAddress.Any 0); // get a kernelassigned number client.Listener = listener; listener.Start(); listener.BeginAcceptTcpClient(OnFileSocket client); // send reply var ep = (IPEndPoint) listener.LocalEndpoint; byte[] reply = Encoding.UTF8.GetBytes(ep.Port.ToString()); context.Client.GetStream().Write(reply 0 reply.Length); } } // Receiving the actual files. private void OnFileSocket(IAsyncResult ar) { var client = (FileClient) ar.AsyncState; client.Socket = client.Listener.EndAcceptTcpClient(ar); var buffer = new byte[8192]; client.Buffer = buffer; client.Socket.GetStream().BeginRead(buffer 0 buffer.Length OnReceiveFile client); } private void OnReceiveFile(IAsyncResult ar) { var client = (FileClient) ar.AsyncState; int bytesRead = client.Socket.GetStream().EndRead(ar); client.Received += bytesRead; client.FileStream.Write(client.Buffer 0 bytesRead); // recieved complete file disconnect and exit. if (client.Received == client.Size) { client.FileStream.Close(); client.Socket.Close(); return; } client.Socket.GetStream().BeginRead(client.Buffer 0 client.Buffer.Length OnReceiveFile client); } #region Nested type: Context private class Context { public byte[] Buffer; public TcpClient Client; } #endregion #region Nested type: FileClient private class FileClient { public byte[] Buffer; public string FileName; public FileStream FileStream; public TcpListener Listener; public long Received; public long Size; public TcpClient Socket; } #endregion } } Client.cs using System; using System.IO; using System.Net; using System.Net.Sockets; using System.Text; namespace Test2 { internal class Client { private readonly IPAddress _server; private readonly TcpClient _tcpClient = new TcpClient(); public Client(IPAddress server) { _server = server; _tcpClient = new TcpClient(); } public void Connect() { _tcpClient.Connect(new IPEndPoint(_server 1234)); } // asks server on which port the file should be sent. private int RequestPort(string fileName long length) { // lock tpcClient for each request so we dont mix up the responses. lock (_tcpClient) { // send request byte[] bytes = Encoding.UTF8.GetBytes(""sendfile;"" + fileName + "";"" + length); _tcpClient.GetStream().Write(bytes 0 bytes.Length); // get reply var buffer = new byte[1024]; int bytesRead = _tcpClient.GetStream().Read(buffer 0 buffer.Length); string reply = Encoding.UTF8.GetString(buffer 0 bytesRead); // error message or port? int port; if (!int.TryParse(reply out port)) throw new InvalidOperationException(""Server sent an error:"" + reply); return port; } } public void SendFiles(string[] fileNames) { // Use a buffer to not preserve memory (instead of reading whole file into memory) var buffer = new byte[8192]; foreach (string fileName in fileNames) { using (var fileStream = new FileStream(fileName FileMode.Open FileAccess.Read FileShare.ReadWrite)) { // Send on commandchannel that we want to send a file. int filePort = RequestPort(Path.GetFileName(fileName) fileStream.Length); var client = new TcpClient(new IPEndPoint(_server filePort)); NetworkStream stream = client.GetStream(); // repeat until there are no more bytes to read. int bytesRead = fileStream.Read(buffer 0 buffer.Length); while (bytesRead > 0) { stream.Write(buffer 0 bytesRead); bytesRead = fileStream.Read(buffer 0 buffer.Length); } stream.Close(); client.Close(); } } } } }"
601,A,"does tcp/ip conveys complete messages? I'm building a library that maintainces connection over network between apps in c++. During some debuging I have found that some information I get from socket seems incomplete. It is vital this information is complete because if I decode it with protocol buffers' library I need complete message. The phenomenon is not constant. Sometimes I get all data from the socket sometimes not. The test procedure looks like: start server start client multiple times. What I get is output from function receive() that depict how values of some variables change. One of them is the size of the buffer I use to store data. That size tells me how much data is in the buffer. The clients send two messages each of size 3 (bytes) - in this particular test. I expect the buffer size to be multiple of 3. However sometimes the size is 4!. That means program read one complete message and 1/3 of the second message. I don't understand why I always get first message complete. Otherwise protobuf would terminate the program. I thought tcp/ip should take care I get complete messages. My problem is I don't know the size in advance. I expect to receive full message so that I can interpret it corecty. TCP is allowed to fragment your data into multiple datagrams which are assembled in-order at the recipient. You are guaranteed that your data will eventually arrive in the order it was sent. But some portion of the data you send may arrive earlier than the rest. What you are experience may be that when you call receive a portion of the data you sent has arrived at your computer but some portion of it is still being sent over the network. You will have to buffer the portions of data you receive and wait until you have received a complete ""message."" One way you can handle this is by sending a fixed-length record at the beginning of each message which encodes the length of the remaining bytes in the message. For example you might send a 4-byte unsigned long in network byte order followed by that many bytes of data.  TCP is a stream based protocol not a message based protocol. TCP has no idea when your messages begin or end and so there is no guarentee that you will get a full message in a single call. There is also no guarentee that you'll get only one full message. You must buffer the data you receive and split it into messages yourself. If you don't get a full message store what data you have received and wait until the rest arrives."
602,A,"Network programming abstraction decomposition I have a problem as follows: Server process 1 Constantly sends updates that occur to a datastore Server process 2 Clients contact the server which queries the datastore and returns a result The thing is the results that process 1 and process 2 are sending back the client are totally different and unrelated. How does one decompose this? Do you just have one process constantly sending data and define the protocol to have a bit which corresponds to whether the return type is 1 or 2? Do you have two processes? How do they share the datastore then (it is just a structure not a database)? Thanks! Why not use a database instead of ""just a structure""? Both relational and non-relational DBs offer many practical advantages (separate processes using them take care of replication [[and/or snapshots backups ...]] rich functionality if you need it for the ""queries"" and so on and so forth). Worst case the ""just a structure"" can be handled by a third process that's entirely dedicated to it (basically mimicking what any DB engine would offer -- though the engine would probably do it better and faster;-) allowing you to at least keep a good decomposition (with the two server processes both interacting with the ""datastore process""). I was hoping nobody would grab onto that because I said ""datastore"". A database is very much unsuited to this application. Think of it like an array of ints they are changing a server has to send out their changes the user can request some complicated calculation on the array. @Dev`*`3 whatever -- I still suggest you devote a process to the care and feeding of that store (thereby also intrinsically serializing access to it a safe choice -- if you need some overlapping accesses you can then have fun implementing range locking R vs W and multithreading the datastore process;-).  If you can restrict yourself to Twisted I recommend to use Perspective Broker. It's essentially an RPC system and doesn't care much about the notion of ""client"" and ""server"" - either the initiator of a TCP connection or the responder can start RPC calls in PB. So server 1 would accept registration calls with a callback object and call the callback whenever it has new data available. Server 2 provides various RPC operations as clients require them. If they operate on the very same data I would put both servers into a single process. @DevDevDev: To get a real world example of PB take a look at the buildbot (buildbot.net). The build slaves log into the master; then RPCs go in both directions over a single TCP connection (allowing to call into slaves that live behind a firewall or NAT). If the clients are not written in Python you have to roll your own RPC protocol over TCP. I recommend to use a TLV encoding. And just store the data as a processwide global variable? That is more or less the road I am going down right now using Twisted. Can you point me to an example of using callbacks over a network? Wouldn't the client then have to act as an RPC server as well? Right now I am doing just what you said except the client has two threads one just blocks on receiving data from Server 1 and the second is available to send RPC calls to server 2. I can only use Twisted on the server side does this limit me to using straight TCP?  It sounds like you want to stream your series of ints ""somewhere"" and also collect them in a datastore. In my system I am streaming sensor readings into a database and also allowing them to go directly to web clients giving them live power readings. I've written a blog entry on why a database is not suitable for live data - though it is perfect for saving the data for later analysis. I'd have the first server process be a twisted server that uses txamp to stream the ints to RabbitMQ. Any clients that want live data can subscribe to the stream in RabbitMQ also using Txamp. Web browser clients can use Orbited here is a worked example. In your design server 1 saves to the database. You could instead have server3 collect data from RabbitMQ and stream it to the database. I plan to have a server that collects chunks of data and render graphs to store to a central fileshare. Don't create your own messaging system RabbitMQ is well tested scalable and can persist your ""messages"" (raw data) if something goes wrong. You don't need a STOMP library if you are working natively on iphone what you want then is a C++ AMQP client library like txAMP is a library for python. Do some googling to find one you like. That was what I initially thought but there is no robust STOMP client that runs natively on the iPhone."
603,A,"C# byte streams through TCP I'm a Jr. Engineer hoping to seek some advice from all of the experienced people in here in regards to how to approach this. I've been assigned a project to create a server/client application that does byte streaming through TCP. Our company deals with 2-way radios with GPS with a dispatch software and we would like to make a server/client application out of that. Currently the dispatch software can be hooked up to a central base station where a user has to be but we want to make this software accessible from a remote location (if the base station is by a repeater miles away from where a dispatcher can be at). User/Client -> poll location of a mic -> server -> base station -> OTA signal -> radio and back I've been looking at Windows Communication Foundation but what are other ways I can approach this? I'll be primarily using C# / .NET / Visual Studio 2008 If you just want a raw byte stream then a Socket or the slightly more encapsulated TcpClient might be a more light weight solution. But if you want to send complete data structures and ""call functions"" then WCF seems like a good choice. With WCF you have a programming experience close to calling regular methods with real objects as parameters but with a Socket you just send byte arrays and need to interpret the result all by your self.  We have used UDP to send GPS updates from cars to a server that processes the updates. In applications like that (where you often have limited bandwidth) you can really tell the difference (in terms of how long it takes to get the data from the remote host to the server) between UDP and momentary TCP connections (like HTTP). A UDP packet will get to its destination in what seems like an instant and the setup for the TCP connection is very conspicuous often taking several seconds to complete. I like the WCF framework but if your app is the sort of system I've been working with I doubt you'll be happy with it (...unless it's OK to have a long interval between updates). Lately I've been working with persistent TCP connections (using raw Sockets) which is a good way to go if you want to make sure your packets arrive at their destination. Though the way to do that I believe is to leave the connection open as long as you can and incorporate code to reconnect it if it breaks. If you are interested here is a link to another post on the subject of using sockets: http://stackoverflow.com/questions/3609280/tcpclient-send-data-and-recieve-data-over-network/3609784#3609784 Thanks for your reply. Is this also a good approach if I'm looking into handling multiple clients/connections at the same time? I tried working with threads in the server while forever listening for new clients but I think it locks up in the infinite while loop. Any good reads on threads? I tend to think that this is a good way to handle multiple clients. Just as you mentioned I spawn a thread for each new connection though I haven't seen the lock up problem. In the other thread I posted some sample code with comments that I think is a reasonably good start on a server. That code doesn't create each new connection in its own thread though one of my comments describes that. If you find any of that useful I would be happy to answer any subsequent questions. Fortunately there are lots of articles on socket programming in C#... ...like this one http://www.codeguru.com/csharp/csharp/cs_network/sockets/article.php/c7695 I have one more question: Is it possible to send a byte stream without pre-defining a byte buffer? Like I'm also using the COM ports/USB port for serial communication and there is a ""BytesToRead."" Is there something similar (that can be easily fit to your example) to do that? I have a 15 byte length so far but there are certain frames in there are variable. I apologize for the delayed response. Assuming you haven't already come up with an answer I'll take a stab here: I'm assuming (please let me know if I'm wrong) that the problem is if you use a 15 byte buffer but get 17 bytes of data you have a trouble. If that's so one easy solution is just to use a byte buffer large enough to hold your largest message. Having said that however another option to consider would be to have a string buffer in addition to the byte buffer. It would go like this: each time you get bytes you convert them to a string and add them to a StringBuilder... If your messages have control characters (like end-of-text) or if you can in any other way detect that you have a complete message you can then just keep accepting bytes putting the data into the StringBuilder until you know you have a complete one. Then you can get the StringBuilder's contents and process the full message. In this way your messages can be arbitrarily large. Great examples thank you sir."
604,A,"Resource temporarily unavailable in Boost ASIO I get the error message ""Resource temporarily unavailable"" when I use the method receive_from() it's a member of ip::udp::socket located here. I pass to it: boost::asio::buffer pointer to an endpoint object flags (set to zero) and an error_code object. I create the endpoint with just  new udp::endpoint() There doesn't seem to be too much information available on this error message too. I've tried it on several machines and always get this error. ""Resource temporarily unavailable"" is normally the text description for EAGAIN indicating that the operation should be retried. In the case of UDP it indicates that there isn't any data available at present and you should try later. It's generally worth looking at the man page for the underlying libc function; which is recvfrom in this case. I see... I'm sending a packet with another program I wrote which works fine as I can see the packet with tcpdump. So how is the data not available if it is received by the machine?"
605,A,"How do you determine if an IP address is private in Python? In Python what is the best way to determine if an IP address (e.g. '127.0.0.1' or '10.98.76.6') is on a private network? The code does not sound difficult to write. But there may be more edge cases than are immediately apparent and there's IPv6 support to consider etc. Is there an existing library that does it? Check out the IPy module. If has a function iptype() that seems to do what you want: >>> from IPy import IP >>> ip = IP('127.0.0.0/30') >>> ip.iptype() 'PRIVATE' As expected for a public IP the string is 'PUBLIC'. There are a few unexpected results: `print(IP('127.0.0.1').iptype())` yields `PRIVATE`. `print(IP('::1').iptype())` yields `LOOPBACK`. `print(IP('2001:0658:022a:cafe:0200::1').iptype())` yields `ALLOCATED RIPE NCC`  A few days after asking this question I found out about this Google project ipaddr-py which appears to have some of the same functionality with respect to determining if an address is private (is_rfc1918). Apparently this will be standard in Python 3.1. It is provisional in 3.3 now see the [ipaddress][1] module. [1]: http://docs.python.org/3.3/library/ipaddress.html See the ""is_private"" property.  You can check that yourself using http://tools.ietf.org/html/rfc1918 and http://tools.ietf.org/html/rfc3330. If you have 127.0.0.1 you just need to & it with the mask (lets say 255.0.0.0) and see if the value matches any of the private network's network address. So using inet_pton you can do: 127.0.0.1 & 255.0.0.0 = 127.0.0.0 Here is the code that illustrates that: from struct import * from socket import * def lookup(ip): f = unpack('!I'inet_pton(AF_INETip))[0] private = ( [ 2130706432 4278190080 ] # 127.0.0.0 255.0.0.0 http://tools.ietf.org/html/rfc3330 [ 3232235520 4294901760 ] # 192.168.0.0 255.255.0.0 http://tools.ietf.org/html/rfc1918 [ 2886729728 4293918720 ] # 172.16.0.0 255.240.0.0 http://tools.ietf.org/html/rfc1918 [ 167772160 4278190080 ] # 10.0.0.0 255.0.0.0 http://tools.ietf.org/html/rfc1918 ) for net in private: if (f & net[1] == net[0]): return True return False # example print(lookup(""127.0.0.1"")) print(lookup(""192.168.10.1"")) print(lookup(""10.10.10.10"")) print(lookup(""172.17.255.255"")) # outputs True True True True different implementation is to compute the int values of all the private blocks: from struct import * from socket import * lookup = ""127.0.0.1"" f = unpack('!I'inet_pton(AF_INETlookup))[0] private = ([""127.0.0.0""""255.0.0.0""][""192.168.0.0""""255.255.0.0""][""172.16.0.0""""255.240.0.0""][""10.0.0.0""""255.0.0.0""]) for net in private: mask = unpack('!I'inet_aton(net[1]))[0] p = unpack('!I'inet_aton(net[0]))[0] if (f & mask) == p: print lookup + "" is private""  If you want to avoid importing a module you can just apply a simple regex: ^127.\d{123}.\d{123}.\d{123}$ ^10.\d{123}.\d{123}.\d{123}$ ^192.168.\d{123}$ ^172.(1[6-9]|2[0-9]|3[0-1]).[0-9]{123}.[0-9]{123}$ Shouldn't those . characters be escaped via \. ? I guess the pattern will still work either way though."
606,A,What are the available PHP extensions for TCP Socket Networking? I'm looking for a PHP extension that allows me to connect bind/listen send and receive data on a TCP socket. There is a PEAR package Net_Socket: http://pear.php.net/package/Net_Socket But this would require PEAR to be installed which we wouldn't want to do as this increases memory consumption. Is there an available purely C extension of this? Thanks Kenneth a few options: PHP Sockets (available in most builds) PHP Streams (5+)
607,A,"Find DNS HostName from IP Address in LAN Hey all. I have written a program that sequentially scans certain parts of a LAN for computers (code will be provided). However when I run this code it only returns the DNS HostName of the computer it is running on. I looked into using WMI but I cannot as I will not always have priveleges to the computers being found. Is there any other way to find a local computers HostName? using System; using System.Net; using System.Net.NetworkInformation; using System.Text; namespace CheckLocalNetwork { class PingCheck { public static string fullip; public void CheckSequentialIP() { IPHostEntry IpEntry = Dns.GetHostEntry(fullip); Ping pingSender = new Ping(); PingOptions options = new PingOptions(); options.DontFragment = true; string data = ""a""; byte[] buffer = Encoding.ASCII.GetBytes(data); int timeout = 120; PingReply reply = pingSender.Send(fullip timeout buffer options); if (reply.Status == IPStatus.Success) { Console.WriteLine(""Address: {0}"" reply.Address.ToString()); Console.WriteLine(""Host Name: {0}"" IpEntry.HostName); Console.WriteLine(""RoundTrip time: {0}"" reply.RoundtripTime); Console.WriteLine(""Time to live: {0}"" reply.Options.Ttl); Console.WriteLine(""Don't fragment: {0}"" reply.Options.DontFragment); Console.WriteLine(""Buffer size: {0}"" reply.Buffer.Length); Console.WriteLine(""""); } } static void Main(string[] args) { Console.WriteLine(""Press enter to search for ip adresses that begin with 192.168.1""); Console.ReadLine(); for (int endofip = 1; endofip < 101; endofip++) { fullip = ""192.168.1."" + Convert.ToString(endofip); PingCheck checkfullip = new PingCheck(); checkfullip.CheckSequentialIP(); } Console.ReadLine(); } All help is much appreciated. Hmm - your code sample behaves as expected on my machine - i.e. it returns the hostname of the machine being scanned. To investigate your problem deeper have you tried using nslookup to check the ip addresses resolve? Microsoft Windows [Version 6.1.7600] Copyright (c) 2009 Microsoft Corporation. All rights reserved. C:\Users\Rob>nslookup <-- type this at a command prompt Default Server: mydns.mydomain.co.uk <--- these two lines indicate the dns server being used to resolve your queries Address: 192.168.0.1 <----| > 192.168.0.5 <---- type in the ip address of one of the machines in question Server: mydns.mydomain.co.uk Address: 192.168.0.1 Name: myworkstation.mydomain.co.uk <---- this is the hostname as reported by the DNS using a reverse lookup Address: 192.168.0.5 If this doesn't return the machine name then you may have a name resolution issue that is not related to your code. If this all looks ok then it might also be worth enumerating the IpEntry.Aliases collection. Are there any entries here and do they make sense? Finally - is the code you have above exactly the code that is going wrong for you or is it a ""distilled"" example? The reason I ask is that the documentation for Dns.GetHostEntry states ""When an empty string is passed as the host name this method returns the IPv4 addresses of the local host."" I also notice you're holding ""fullip"" in a static. If this is not the exact code that is causing the problem especially if this runs multithreaded is there a chance you are not initialising ""fullip"" before the Dns.GetHostEntry is called? I may be way off but I thought is was worth giving a brain dump of what occured to me as I looked at your problem :) [EDIT:] - your comment to kdt has clarified something I misunderstood. I thought you were saying you always got back the hostname for your local machine no matter which machine you ""scanned"" - which is very odd behaviour. In fact I think you are saying you just get back IP addresses for other machines (their IP address) and only get a hostname for your local. Disregard my last bit about the threading and the empty argument. This is far more easily explained - your machine is almost certainly just not able to resolve the machine names - I expect my nslookup test I suggested will not return the machine names either. In order to resolve these IP's to host names your machine needs a DNS that has entries for these machines or to have them in its local hosts file; your machine isn't actually asking the remote machine for its name when you do this call so it won;t be able to find it out without help from one of its usual name resolution paths. It works for me because my local DNS really does have entries for all the machines on my network resolving their host names to ip addresses and vice-versa. I was writing another ""not so"" comment when your edit appeared. And you were right the nslookup did not return anything useful for the other addresses that had appeared when running my program. That is the program in its entirety. Suggest a correct question to fix this problem then? A sure way to get the DNS HostName without using WMI is what is needed here it still sounds like. The problem here is not really code related - it is to do with your network setup. Basically if one machine on the network wants the hostname of another given only its ip address it only has a few choices. 1) See if there is a mapping in the local hosts file - not helpful in your case. 2) Ask a DNS. Apart from some legacy NetBIOS approaches that's all you've got. WMI won't help either. This is really a sysadmin issue - your DNS servers should have entries for these machines. I really can't think of a solution without this off the top of my head. Thanks very much for all your help. actually - minor change to my comment above - WMI would help if you could do it as you can use WMI to actually connect to the remote machine and ask it for its host name (I'm pretty sure you can anyway). But between WMI privelege issues you mention and DNS issues I fear both solutions are out of your hands."
608,A,Books for transport layer protocol implementation in Computer Networks I am trying to implement a transport layer protocol for my project. I am going to use Linux as my operating system. Could you please suggest me some books or links that explain the implementation of transport layer (like TCP)? Thank you.. Thanks Bala As far as I can tell the latest (December 2008) book dedicated to this one and only topic is TCP/IP Architecture Design and Implementation in Linux. 772 pages enough to keep anyone busy for a little while. Note that it doesn't cover UDP deferred to a future book. Caveat: Even though the book was released end dec 2008 the base kernel tree it describes is a 2.4.20 one. OTOH I don't think the TCP/IP stack has changed that much since then. The author announces in his preface that 2.6 specifics will be covered in a future edition.  The book on the topic - TCP/IP Illustrated Volume 2: The Implementation by W. Richard Stevens the great. Thank you for your answer. Do you know any book that deals with the implementation in Linux? Here's first Google hit: http://www.amazon.com/Understanding-Network-Internals-Christian-Benvenuti/dp/0596002556 Disclamer - I haven't read it.
609,A,Http request via one network interface I would like to call HTTP request for rest webservice only on selected network interface. Any advice on this would be much appreciated. Same question here http://stackoverflow.com/questions/1008781/is-it-possible-to-specify-which-network-interface-for-a-jvm-or-ide-to-use This is more or less out of the control of your Java program. It's up to the OS to decide where to send the packets. It'll send the packets on the interface that's configured to have a route to the destination IP - or send it on the default route. So you have to just make sure you've configured your OS properly to route the IP packets where you want them and it'll only send them on that interface. Hmm so you think that solution is in routing table in OS. It was my first idea too but I would like to create more abstract app. I found http://download.oracle.com/javase/1.4.2/docs/api/java/net/NetworkInterface.html but it seems that there is now way how to specify network interface in http request. I dont know how sockets work at Java. Iam Java beginner. @Petr Velký yes. the solution is the OS routing table. The NetworkInterface class has nothing to do with HTTP requests nor how to specify a network interface for your IP packets. Receiving requests is another matter though there's APIs for listening to incoming connections on a partiular IP address(which is normally bound to only one network interface).  I found out that you can define network interface in url like this : 192.168.1.122%eth0 It's working in Ubuntu and I hope that it works in different systems too.
610,A,"What is the difference between NetworkStream and Socket classes? I have a project where I might want to abstract the communication between client and server. I was initially using Sockets and TCP. I then thought that it might be nice to be able to switch to an inter process communication channel. I then looked at System.IO.PipedStream class and saw that there was a lot of overlap between the PipeStream and the Socket class. But when I had a look at the Socket class it inherits from Object. So there is no common interface or abstract class between Socket and PipeStreams. I then remembered something about NetworkStream. It apparently wraps a Socket class. But at least both NetworkStream and PipeStream inherit from Stream. Which means I can swap my implementation out. I think I haven't tried this yet. I have been using sockets all this time. So my question is: Is there any disadvantage to using the NetworkStream class over the Socket class. Any gotchas or anything to watch out for? @Stephen. I imagined that it would be faster than getting the TCP/IP stack involved. What exactly do you mean ""it might be nice...""? Keep in mind that TCP/IP sockets are a perfectly valid form of inter-process communication. For high-level communication it's better to user WFC than raw TCP or Pipes WCF is not an option for us. If you do not know about any disadvantages switching from Socket to NetworkStream. Socket gives you a lot more control but since you are not aware of disadvantages you will most likely not need that control. Just remember that things to not become reliable or fast just because you are switching to a NetworkStream. (Might be easy to forget when you start to switch between different stream implementations).  If you are forced to use low level options such as sockets and pipes and you want an abstraction for pulling streaming data from those data sources then the Stream class is perfect as it provides that abstraction for that model. If you code against Stream instances then you can have anything implement the Stream and not worry about the underlying transport. In reference to using a NetworkStream vs a Socket a NetworkStream just wraps a Socket instance and applies the calls to the pull model (the Stream) to the Socket. @casperOne: The model is as follows. We need one thread that keeps posting to the stream and getting acks from the stream. Then another thread that keeps receiving and sending acks back. Sockets were perfect. But now we have a situation where the client and server can be on the same machine. I thought it might be even faster if they just used named pipes instead of sockets. @ casperOne: Thanks for the answer. Can you expand on what you mean by pull model? @uriDium: Updated answer to distinguish between pull and push models. ehh. what are you talking about? What have Pull and Push to do with things? @uriDium: That's basically a duplex channel where both parties can send messages to each other at any time. You should *really* consider WPF at this point as it supports this messaging pattern as well as using both TCP (socket-based) and named pipe transport channels. @jgauffin: I just realized I was confusing the named pipe stream with the SerialPort class (I don't know why) which uses a push vs a pull model. I've updated the answer to remove the distinction and keep it more on target."
611,A,not able to connect to server on internet I have a very simple client server code written java(server listens on some port and client connects to server port and after connection is established client ip is displayed on server console). This program is working very well in intranet but if client and server are on internet my server cannot detect it.I have no firewall installed on my client and server and port forwarding is done on server(I can see it from canyouseeme.org). Server is directly connected to modem along with other three computers(they are also connected to modem directly) Please help me figure out why I am not able to detect client on internet.Thanks in advance. Client code: ------------ String remoteIP = //remote ip int port =1888; try{ new Socket(remoteIpport); }catch(Exception e){ System.out.println(e.message()); } Server code ----------- ServerSocket serversocket = new ServerSocket(port); Socket socket = serverSocket.accept(); displayIp(socket); Please post the code of this 'very simple client server code'. added the code. What do you mean by 'if client and server are on internet my server cannot detect it'? Can the client connect to the server in the first place? Check your proxy configurations on the client side see http://download.oracle.com/javase/6/docs/technotes/guides/net/proxies.html Which protocol do you use? I use TCP protocol(using java sockets). But the thing is only server on my system is not detecting client. If I run server on my office system(which is directly connected to modem it is able to detect client).So it must be some problem on my server. But I am able to see my server on canyouseeme.org Try to change your code to port 80 also on WAN router with NAT where server is connected. Check server again from canyouseeme.org on port 80. Then open browser in client and go to e.g. http: //www.google.com then goto http:// . This *must* work.
612,A,"Can a client determine whether the server has accept()'d a unix socket? I'm dealing with a buggy server that will sometimes fail to accept() connections (but leaves its listening socket open). This is on Linux with unix domain sockets. Currently the only way to detect this is that after sending a bunch of data the buffer fills up and blocks and the server isn't sending any replies. This long-after-the-fact failure mode is hard to distinguish from other bugs - the server could be unresponsive for other reasons. Especially for unix domain sockets it seems the kernel should know whether accept() has occurred; is there any way to find this out? Can the client block until accept() happens somehow or at least check whether it has? This is just for debugging purposes so it can be a little ugly. By default connect will block until the connection is accepted. Not sure if I understand your question @nc3b: No `connect()` blocks until the connection has been set up and is pending in the listen backlog. This what the `backlog` parameter in the `listen()` call is all about - it sets the number of connections that can be pending. Looks like the answer to this is ""no""; the kernel offers no way to get at this info. (change the protocol isn't an answer since I was debugging a pre-existing unchangeable protocol)  I'd say that the answer was to change the protocol so that the server speaks first (like SMTP not like HTTP) The kernel accepts sockets already before the accept() system call is made; if it never arrives then the sockets sit around in a waiting state. There is no way to distinguish this from the server accepting the connection but not speaking. This is the way BSD sockets has always worked. yeah unfortunately I can't change the protocol or the server or there would be no need to do this ;-) it's just something that would be useful to work around the screwy server. I don't anticipate there's a way to do this but the kernel does in theory know if the accept() has happened - on a local unix socket - so just asking if anyone knows a way. pretty clearly it would be impossible in the TCP case"
613,A,"What's the simplest way to call Http GET url using Delphi? There's a web services I want to call in my application I can use it with importing the WSDL or by just use ""HTTP GET"" with the URL and parameters so I prefer the later because it's simple thing. I know I can use indy idhttp.get to do the job but this is very simple thing and I don't want to add complex indy code to my application. UPDATE: sorry if I was not clear I meant by ""not to add complex indy code"" that I don't want add indy components for just this simple task and prefer more lighter way for that. i didn't know REST can have wsdl too AhmetCiftci as I said before the service has ""HTTP GET"" url to call it so can use the WSDL or by just request ""HTTP GET"" Do you think just using HTTP GET is enough to invocating a webservice function ? It is if it's a RESTful web service. If it's okay to download to a file you can use TDownloadURL from the ExtActns unit. Much simpler than using WinInet directly. procedure TMainForm.DownloadFile(URL: string; Dest: string); var dl: TDownloadURL; begin dl := TDownloadURL.Create(self); try dl.URL := URL; dl.FileName := Dest; dl.ExecuteTarget(nil); //this downloads the file finally dl.Free; end; end; It's also possible to get progress notifications when using this. Simply assign an event handler to TDownloadURL's OnDownloadProgress event.  Use the Synapse TCP/IP function in the HTTPSEND unit (HTTPGetText HTTPGetBinary). It will do the HTTP pull for you and doesn't require any external DLL's other than Winsock. The latest SVN release works perfectly well in Delphi 2009. This uses blocking function calls so no events to program. Update: The units are very light and are not component based. The latest version from SVN runs perfectly well in Delphi XE4 also. HTTP calls in Indy are also blocking.  If your application is Windows-only I would suggest using WinSock. It's simple enough allows to execute any HTTP request can work both synchronously and asynchronously (using non-blocking WSASend/WSARecv with callbacks or good old send/recv in a dedicated thread).  Actually code in accepted answer did't work for me. So I modified it a little bit so it actually returns String and gracefully closes everything after execution. Example returns retrieved data as UTF8String so it will work well for ASCII as well as for UTF8 pages. uses WinInet; function GetUrlContent(const Url: string): UTF8String; var NetHandle: HINTERNET; UrlHandle: HINTERNET; Buffer: array[0..1023] of byte; BytesRead: dWord; StrBuffer: UTF8String; begin Result := ''; NetHandle := InternetOpen('Delphi 2009' INTERNET_OPEN_TYPE_PRECONFIG nil nil 0); if Assigned(NetHandle) then try UrlHandle := InternetOpenUrl(NetHandle PChar(Url) nil 0 INTERNET_FLAG_RELOAD 0); if Assigned(UrlHandle) then try repeat InternetReadFile(UrlHandle @Buffer SizeOf(Buffer) BytesRead); SetString(StrBuffer PAnsiChar(@Buffer[0]) BytesRead); Result := Result + StrBuffer; until BytesRead = 0; finally InternetCloseHandle(UrlHandle); end else raise Exception.CreateFmt('Cannot open URL %s' [Url]); finally InternetCloseHandle(NetHandle); end else raise Exception.Create('Unable to initialize Wininet'); end; Hope it helps for somebody like me who was looking for easy code how to retrieve page content in Delphi. Cheers Aldis :)  Calling a RESTful web service using Indy is pretty straight forward. Add IdHTTP to your uses clause. Remember that IdHTTP needs the ""HTTP://"" prefix on your URLs. function GetURLAsString(const aURL: string): string; var lHTTP: TIdHTTP; begin lHTTP := TIdHTTP.Create(nil); try Result := lHTTP.Get(aURL); finally lHTTP.Free; end; end; Bruce Lars is right I said in my questions I don't want to use indyt ""idhttp.get"" it's the simplest way but I wanted the lighted way :) Your ""lightest"" option is to use external code whether it's Indy Synapse or WinINet. If you can't or won't use Delphi components or classes use Delphi's wrapper functions around WinINet. Also external but at least the DLL is installed with Windows. The question was if it could be done without Indy The question was to not add complex Indy code. The code isn't complex. In its defence Indy ships with Delphi but WinInet version and behaviour changes with installed IE version which is like DLL hell. In my situation adding idHTTP made my exe jump from 300K to 2MB which doesnt always matter but in my case did matter. @Toby: I'd be very curious to know why that is. What version of Delphi and what kind of application (VCL/Console)? @BruceMcGee I think it was Delphi XE . In Delphi 7 it was a bit of a jump maybe to about 800K but in XE much bigger. Console app. No idea why it made it so much bigger a lot of code to compile in I suppose. @Toby: An empty console app is 40K in D7 and 83K in DXE. Adding IdHttp to the uses clause takes them to 139K and 739K respectively. I'm not sure where that 2MB comes from. It sounds like it could be a later version of Delphi compiled in Debug mode. `TIdHTTP.Create(nil);` can be shortened to `TIdHTTP.Create;` with Indy 10.6  You could use the WinINet API like this: uses WinInet; function GetUrlContent(const Url: string): string; var NetHandle: HINTERNET; UrlHandle: HINTERNET; Buffer: array[0..1024] of Char; BytesRead: dWord; begin Result := ''; NetHandle := InternetOpen('Delphi 5.x' INTERNET_OPEN_TYPE_PRECONFIG nil nil 0); if Assigned(NetHandle) then begin UrlHandle := InternetOpenUrl(NetHandle PChar(Url) nil 0 INTERNET_FLAG_RELOAD 0); if Assigned(UrlHandle) then { UrlHandle valid? Proceed with download } begin FillChar(Buffer SizeOf(Buffer) 0); repeat Result := Result + Buffer; FillChar(Buffer SizeOf(Buffer) 0); InternetReadFile(UrlHandle @Buffer SizeOf(Buffer) BytesRead); until BytesRead = 0; InternetCloseHandle(UrlHandle); end else { UrlHandle is not valid. Raise an exception. } raise Exception.CreateFmt('Cannot open URL %s' [Url]); InternetCloseHandle(NetHandle); end else { NetHandle is not valid. Raise an exception } raise Exception.Create('Unable to initialize Wininet'); end; source: http://www.scalabium.com/faq/dct0080.htm The WinINet API uses the same stuff InternetExplorer is using so you also get any connection and proxy settings set by InternetExplorer for free. Getting the IE settings ""for free"" is only a feature if you use IE. :) Thanks Lars that's the lighted way :) As bruce says if you had an IE misconfigured to use a proxy setting that is down for example you would be waylaid too. Obscure and frustrating if it happened. I have learned from hard experience now that you also get all the bugs in IE's wininet.dll that is installed on your client's system for free including the dread TIMEOUT bug. BE AWARE. @Warren: those get fixed in security updates and service packs while your own code or anyone elses does not get the same critical review by so many eyeballs. And they get broken. Why are broken WinInet versions never fixed until you upgrade to a new version of IE? Terrible. Not fixed in a service pack unless IE itself is force-fed into a service pack."
614,A,"How to assure a UDP server does not lose incoming data? There is a data feed server receives feed from various clients by means of UDPbecause the clients are pumping data so fastthe receiving buffer is very easily to get full if the server spends time on processing the received dataso Will it help that if the feed server just multicasts all data it received to the other servers on LAN which the data feed server has a second NIC connects to? Each of the other servers only picks up data it concerns to process and leave the other data to the other servers. If the incoming data still arrives too fast is there any strategy to assure not losing any data? Thanks. If you need reliability with UDP then you have to build your own on top of it. As others have said you need some method of sending an ACK from one peer to another to tell the other peer that the data has arrived safely. The other peer can then selectively resent datagrams until they have been ACKed. I have a question open here: http://stackoverflow.com/questions/107668/what-do-you-use-when-you-need-reliable-udp which is collecting various options for adding reliability to UDP based transfers.  UDP can be thought of as standing for 'Unreliable Datagram Protocol'. So that kind of explains your problem immediately: you want reliability which is a service the underlying protocol does not provide. Probably you want congestion control as well since lack of network buffers is a source of congestion just as much as lack of bandwidth. The solution is to use something other than UDP or to add reliability and congestion control on top of UDP; acknowledgments and rate limiting essentially. Possible replacements for UDP include TCP and SCTP. SCTP would be good because it has a datagram mode so you don't have to convert the protocol to working with streams. See here: http://en.wikipedia.org/wiki/Stream_Control_Transmission_Protocol  well you need to write your own ""ack"" method. Client resends chunk untill gets ack from server that it has been received. And very soon you will have a broken version of TCP. Either use TCP or accept UDP limits.  If I understand what you are asking then no because you would be receiving data then sending it out again effectively doubling the bandwidth you are using so it probably wouldn't help. The best strategy to ensure no lost data would be to use TCP over UDP. However if you have to use UDP you could write some code that numbers each packet that gets sent (so you can ensure they arrive in order) and add more functionality that allows the server to request a missing packet e.g.: Sender sends 1 2 3 4 but Receiver recieves only 1 2 4 then request 3 again. Thanks for you comments.But if the the clients(incoming data) and the others servers are in two separated networks(the data feed server has two NIC one used to receive incoming feedsthe other one used to forward data)the bandwidth won't be an issue?no?  Some strategies to try include Ensure the receiving process does very little in the receive thread simply read the data and post to an internal queue for processing on another thread - this should reduce the likelyhood of the receive buffer filling up Include a sequence number in your message. If a receiver notices a missed message it can re request it from the publisher. This re request will be very expensive but make the general case very quick. This assumes the publisher either keeps a reasonable amount of published messages in memory (to allow re requests) or persists them somewhere to cope with replay."
615,A,"Java HTTP Response Code URL IOException I'm getting a 503 response from a server that I'm trying to communicate with. Now it shows me this in the log but I want to catch the IOException that it fires and deal with it if and only if the response code is 503 and nothing else. How would I do this? EDIT: Here's part of my code: inURL = new BufferedReader(new InputStreamReader(myURL.openStream())); String str; while ((str = inURL.readLine()) != null) { writeTo.write(str + ""\n""); } inURL.close(); What HTTP client API are you using? Can you post some part your code? See my edit. Balus: A URL wrapped around a InputStreamReader which is wrapped around a bufferedreader. If using java.net.HttpURLConnection use the method getResponseCode() If using org.apache.commons.httpclient.HttpClient executeMethod(...) returns the response code  Handle the case you want in you IOEXception catch clause if it falls into that code section. Check the http status code if it is 503 handle it the way you want if not throw the exception again.  here's an HTTPClient I have done : public class HTTPClient { /** * Request the HTTP page. * @param uri the URI to request. * @return the HTTP page in a String. */ public String request(String uri){ // Create a new HTTPClient. HttpClient client = new HttpClient(); // Set the parameters client.getParams().setParameter(""http.useragent"" ""Test Client""); //5000ms client.getParams().setParameter(""http.connection.timeout""new Integer(5000)); GetMethod method = new GetMethod(); try{ method.setURI(new URI(uri true)); int returnCode = client.executeMethod(method); // The Get method failed. if(returnCode != HttpStatus.SC_OK){ System.err.println(""Unable to fetch default page"" + "" status code : "" + returnCode); } InputStream in = method.getResponseBodyAsStream(); // The input isn't null. if (in != null) { StringBuilder sb = new StringBuilder(); String line; try { BufferedReader reader = new BufferedReader(new InputStreamReader(in ""UTF-8"")); while ((line = reader.readLine()) != null) { sb.append(line).append(""\n""); } } finally { in.close(); } //return statement n0 return sb.toString(); } else { //return statement n1 return null; } } catch (HttpException he) { System.err.println("" HTTP failure""); } catch (IOException ie) { System.err.println("" I/O problem""); } finally { method.releaseConnection(); } //return statement n2 return null; } } Maybe it can help you."
616,A,"Passing a hostname of over 255 characters to getaddrinfo causes a getaddrinfo failed: memory allocation failure why? I am currently upgrading our software to support ipv6 and in the meantime I'm expanding hostname/ip fields to the max hostname size. In sun documentation it seems like this can be up to 1025 (netdb.h:#define NI_MAXHOST 1025 - this is the recommended hostname allocation define) but when I pass a hostname of over 255 to getaddrinfo I get ""getaddrinfo failed: memory allocation failure"". I am testing on a Sol10 box. Ideas? Looks like a general Sol10 limit hostnames are restricted short too http://chihungchan.blogspot.com/2008/01/hostname-maximum-length-in-solaris.html can you show the code where you are getting the error? does it work with smaller hostnames? NI_MAXHOST is for getnameinfo() getaddrinfo() uses the MAXHOSTNAMELEN. (see here)  and that seems pretty much to be hardcoded - and there's little you can do about it. Exactly why these differ I don't know but see e.g. also here/here for some info on max host name lengths.  You have no choice: this limit of 255 characters is the standard. Let me quote RFC 1123 section 2.1: Host software MUST handle host names of up to 63 characters and SHOULD handle host names of up to 255 characters."
617,A,how to send message digest alongwith the actual message over the network Let E=encrypt(msg) D=digest(E) If I send E+D to the peer. How is the peer supposed to know that it needs to split the incoming message at E and calculate its digest and verify against D. Should the final msg contain some kind of length parameter or a delimiter There's no magic mechanism to add signatures independent of protocol. Each network protocol that allows for digital signatures or digests or encryption does so differently. For example: http://en.wikipedia.org/wiki/WS-Security The answer is 'it depends'. One protocol might send it glued to the front another glued to the back and a third may stick it in a header off somewhere. Completely ignoring the standard protocols for encryption and hashing I was wondering how (in terms of raw messages) the hash is sent. Upon asking someone else I learnt that the hash is sent separately and the message separately. Is this correct? And then use a delimiter/length to denote the message? If the signature is fixed length you don't need that if it's first. If the message is self-describing in length .... well that's your case. Thank you.... back to getting my program to work
618,A,"Handling Linux Network Connectivity in Java/C++? I am working on a program to handle connections in Ubuntu. I am currently doing it in Java but aim to do it in C++ too. Essentially atm I am just running system commands through Java: proc.exec(""ifconfig wlan0 down""); etc. I get some printout saying it is 'deconfiguring interfaces' and 'fail' is printed out. First off does anybody know why? Secondly can anyone think of a better way of doing this? In the end I want the program to have the computer run an Ad Hoc network drop the connection and connect to another network and pick up the Ad Hoc again when it is done with the other network. Thanks in advance! I think the fail issue was due to me not writing sudo -i before the commands. Thanks for pointing that out! You're not really giving us a lot of info to work with. Are you running the program with root priviledges? Probably you can try to interface NetworkManager which uses D-Bus as its interface and there exists Java implementations of the protocol. Thanks just downloaded and looks like it should do what Im after. Felt a bit inefficient to have a program run through terminal commands a lot harder to detect or handle errors!"
619,A,"Why do C or C++ books not contain anything on networking? I've just been looking at the following books: -K&R C -The Complete Reference C++ -The Complete Reference C -Deitel How to Program C -Deitel How to Program C++ and not one of them contains any networking how to create sockets etc. Is there any 'definitive' reference for C network programming? Google wasn't particularly helpful. I'm probably considering windows and unix platforms because C are C++ are programming languages. networking is one such area where they are used. ""Why do car factory service manuals not contain anything about driving?"" The C/C++ is a language and the networking is not part of the language and that is the why If you do  man 2 accept man 2 select man 2 connect man 2 recv man 2 send you will see template code and references needed for most of your TCP networking need If you're on Linux of course. even if your not on linux then a google search on ""man select"" will provide you the man page.... If you find that overly complicated then check out ZeroMQ instead. It's a supersocket library based on messages which is much easier to work with than plain low-level sockets.  I agree with cnicutar's answer... If you need a good starting point - see http://beej.us/guide/bgnet/output/html/multipage/index.html  ""C books"" usually describe the language and the standard library. Sockets aren't in the standard library. If you want a good book on sockets UNP by Stevens is arguably the best. The Stevens books are widely considered the ""Bible"" on sockets. Also look at the Beej guide on network programming (http://beej.us/guide/bgnet/) for a quick overview or tutorial. so if i wanted to do socket programming I would have to install a 3rd party library of some sort? no - usually your OS comes with necessary library pre-installed..."
620,A,"Suggestion for book on wireless network programming I want to learn wireless network programming. I tried to explore native wifi API from msdn but unable to understand. I hardly know anything about wireless networking. In msdn they have mentioned ""The Native Wifi API is designed for C/C++ developers. Programmers should be familiar with wireless networking concepts and terminology."" So I have decided to start from basic. Can anybody suggest me a good book or tutorial for this purpose?My aim is to programmatically opening wifi connection in windows 7. I have knowledge of basic socket programming in windows and linux.(This is just to inform. I don't know socket programming is related to wireless or not.) I also know basic C programming. What do you mean by ""opening wifi connection""? Do you want to connect to a WiFi network or do you want to communicate with a server and the use of WiFi is incidental? I want to connect to wifi network but I want to open that connection in my pc programmatically. It sounds to me like you are talking about doing standard user-mode network programming. In which case there is no difference between wireless and wired programming in this regard. The only reason you would use the Wireless API is if you were writing drivers for wireless hardware or you were trying to interact with the wireless system for a specific purpose (such as getting signal strength or configuring the Wireless card). If you just want to send data wirelessly then there is no difference between that and wired programming. As I have mentioned ""My aim is to programmatically opening wifi connection in windows 7.""  I would suggest 802.11 Wireless Networks: The Definitive Guide by Matthew Gast and published by O'Reilly. It should get you started with in-depth wireless knowledge of concepts/terminology and inner-workings of the protocol. Great book. Amazon link.  As Shinnok commented its necessary to read/comprehend Matthew Gast's book for understanding basic elements in WiFi technology. For Windows specific WiFi code development you can start from here: http://msdn.microsoft.com/en-us/library/windows/desktop/aa816369(v=vs.85).aspx  found this one on my bookshelf: ""Designing a wireless network"" ISBN: 1-928994-45-8"
621,A,Having connection to Latest Yahoo Messenger Protocol in C# I want to program a simple GUI to connect to Yahoo! Protocol and act as a messengers which use them now. So could any one describe Yahoo! Protocol structure ( Please help me in using of latest version of this protocol ). Thanks about your attention. As your question is it isn't specific enough. Try something and if necessary come back when you have a specific problem that we can answer. I think you will find these links helpful as they describe several aspects the Yahoo! Protocol... http://sourceforge.net/projects/ycs/ http://www.ycoderscookbook.com/ http://www.codeproject.com/KB/vb/YCC_Trainer.aspx Yahoo Messenger Library C# http://www.carbonize.co.uk/ymsg16.html http://libyahoo2.sourceforge.net/ http://www.venkydude.com/articles/yahoo.htm some general information http://en.wikipedia.org/wiki/Yahoo!_Messenger_Protocol Thank you these links are very useful for me. :)
622,A,"How to listen to a TCP port which is already being listened by another app I've a plugin which always listening to the port 6002 and i have an ASP.net application which sending messages to the same port and receiving the reply from the plugin on the same port Sending is working fine and the plugin sends a reply on the same port but i don't know how to catch this reply when i try to listen to the same port using Tcplistener the start method throws this exception : Only one usage of each socket address (protocol/network address/port) is normally permitted is there any way to catch the received message Thanks From what I can tell it's impossible to open a port once it's being used. PLEASE AVOID ALL CAPS WHEN WRITING. It's very rude of you. It's being listened by a java plugin is that means that i can't access it at all ??? In short no. Once a port is opened an exception will be thrown if further attempts are made to utilise that same port from a different source - as you are experiencing right now. There isn't a way to get around this. Why could I guess that Mr. Disappointment would say 'No' ;)  If you want to inspect traffic you can use winpcap. edit: I don't think you are asking the right question. In this case the plugin is the server (listening on port 6002) and your ASP.net app is the client listening on some arbitrary port. You only need to bind to a different port in your ASP.net app if it also needs to run as a server with the plugin acting s the client. In this case you should pick a different port even though there are in fact ways to make it work when they are both bound to the same port. In your case though you should just read back responses from the connection you established from the client.  It sounds like you are wrongly assuming that the Socket which you get from TcpListener.AcceptSocket can only be used in one direction. Sockets can actually be bidirectional. You can use Send to send something and Receive to listen for get the replies. Open one socket and then use it for both sending and receiving. So i can listen !! every body says no and voting down my question ! no you can't (easily) listen. ""listen"" in the TCP sense means listen for new incoming connections.  I've solved this problem using this way  I know it's old method but it's working !! : '/*Variables Initialization*/ dim objSocket strServicePort strIpAddr strResult strMsgTo strMsgResponse strServicePort = ""6002"" strIpAddr = ""127.0.0.1"" '/* Create a TCP/IP socket. */ objSocket = Server.CreateObject(""Intrafoundation.TCPClient.3"") objSocket.ClearLog() '/* Establish socket connection. */ objSocket.Open (strIpAddrstrServicePort) objSocket.Timeout=60.0 strMsgTo =""---- Message here ----"" '/* Send request message to plugin */ objSocket.SendRN(strMsgTo) '/* receive XML Request Message from plugin */ strMsgResponse = objSocket.Recv() strMsgResponse = Replace(strMsgResponse vbLf """") objSocket.Close()"
623,A,Change IP settings using C++ How do I change the IP settings of a Windows CE 6 box Programatically via C++? Functions for Windows might also work. I found that I can change the hostname via sethostname but couldn't find how to change IP address settings such as: IP Address DHCP Subnet Gateway DNS1 / DNS2 WINS1 / WINS2 Any advice / pointers would be great. Thanks. P.s. How would you get the box to update to those settings - is a refresh or the programming equivalent of ipconfig /renew required? Have you checked out the IP Helper Routines on MSDN? I think these provide some if not all of what you need. *EDIT: * Updated link. Thanks ctacke The link for the CE versions is http://msdn.microsoft.com/en-us/library/aa923597.aspx  Most of these fall under the IpHlp API. You don't really change an IP address -- you use DeleteIpAddress delete the old one then AddIpAddress to add the new one. You specify the subnet mask when you add an address. It's not clearly what you want to know about DHCP. You can use DHCP via IpReleaseAddress and IpRenewAddress. You can get the address of the current DHCP server with GetAdaptersInfo (among others). At least if memory serves getting its address is mostly for information though -- since the basic idea of DHCP is to avoid manual configuration you normally find/use it via a broadcast message. You can set the DNS and WINS servers via the the WMI Win32_NetworkAdapterConfiguration class (SetDNSServerSearchOrder and SetWinsServer) You can adjust quite a few (most?) of the other parameters via WMI as well. WMI isn't available for WinCE. @ctacke: Oops -- in the process of writing the reply I forgot the minor detail about its being for CE. Thanks for the reminder.
624,A,"Linux programmatically up/down an interface kernel What is the programmatic way of enabling or disabling an interface in kernel space? What should be done? What type of interface and using what language? In fact I did not mentioned but it will be in C and eth types. Code to bring eth0 up: int sockfd; struct ifreq ifr; sockfd = socket(AF_INET SOCK_DGRAM 0); if (sockfd < 0) return; memset(&ifr 0 sizeof ifr); strncpy(ifr.ifr_name ""eth0"" IFNAMSIZ); ifr.ifr_flags |= IFF_UP; ioctl(sockfd SIOCSIFFLAGS &ifr);  ...by using IOCTL's... ioctl(skfd SIOCSIFFLAGS &ifr); ... with the IFF_UP bit set or unset depending on whether you want bring the interface up or down accordingly i.e.: static int set_if_up(char *ifname short flags) { return set_if_flags(ifname flags | IFF_UP); } static int set_if_down(char *ifname short flags) { return set_if_flags(ifname flags & ~IFF_UP); } Code copy-pasted from Linux networking documentation."
625,A,boost asio async_read_until: how to raise error from a custom MatchCondition? I am trying to optimize an application layer protocol that has a mixed mode protocol ( line protocol for meta info handshake acknowledgement etc. and binary for data ). In a state where the client is waiting for an ACK I do async_read_until(socket buffer untill_crlf_maxbytes_1024_match_condition handler); Where untill_crlf_maxbytes_1024_match_condition is implemented acxcording to async_read_until documented here. Here the idea is to read untill CR+LF but wait for only 1024 bytes to be read so that if anything spooky goes on and if the socket gets some junk it would not stay reading. My question is is it a good idea to raise an error in such a scenario from untill_crlf_maxbytes_1024_match_condition? How do I raise an error in that scenario? If not whats the best alternative? by raise error do you mean throw exception? if so it's a bad idea because most functions of boost asio have two versions: throwing exceptions and returning error codes. error codes is very useful for asynchronous nature of boost asio. you can return true from your match condition for all cases including errors. just check errors in you handler Thanks AndyT but that as I understand from the linked documentation the bool return value only suggests whether the async_read can stop or not. It doesnt really tell the handler that there was an error; By error I mean the fact that a CR+LF was not found in 1024 bytes read so far. @CodeMedic I think your handler could infer an error based on `bytes_transferred` placeholder being larger than 1024 bytes. @CodeMedic @Sam Miller: or any other convenient way up to setting a global flag etc. boost::asio just doesn't provide you such facility @CodeMedic: somebody downvoted this answer I hope it's not you (no other critical comments here) because I tried to help and I believe I provided correct answer @Andy FWIW I upvoted you since this seems like the best answer. @AndyT I think there is no easy way to do it other than checking `bytes_transferred`. So a combination of restricting read length from `untill_crlf_maxbytes_1024_match_condition` and `bytes_transferred` check from handler is the solution. Thanks Sam & Andy!
626,A,Pinging from a C/C++ program I want to write a C or C++ program that given an IP address Pings it and then performs further action based on whether the Ping was successful or not. How to do this? Depending on what you want to accomplish the nmap sources may be interesting to look through. ubuntu Operating System What operating system? Have a blast at The Ping Page which has a link to full source on the original Unix ping(8).  EDIT I saw after I posted you are on Ubuntu. However someone searching this question may still find these links helpful for Windows. Ping: Raw Sockets Method: http://tangentsoft.net/wskfaq/examples/rawping.html Implementing Internet Pings Using Icmp.dll: http://support.microsoft.com/default.aspx?scid=kb;en-us;170591 IcmpSendEcho Function: http://msdn.microsoft.com/en-us/library/aa366050%28VS.85%29.aspx Ping for Windows: http://www.codeproject.com/KB/IP/winping.aspx
627,A,"Powershell: How do I connect to a network folder on a different domain with stored non-plaintext username/password I'm trying to connect to a network share via powershell. The network share is on a different domain so I need to supply credentials. Since New-PSDrive doesn't support credentials I was going to use net use but I'm worried about putting my username/password right there in plaintext into the script. I forced ConvertTo-SecureString to make a secure string out of my password and then used ConvertFrom-SecureString and stored the result in a file. Then I pulled it out of the file like this and tried net use: $password = Get-Content <locationOfStoredPasswordFile> | ConvertTo-SecureString net use q: ""\\server\share"" $password /user:domain\username But net use doesn't recognize the secure string. Anyone have any ideas on how to store the credentials so net use can use them? Thanks! Here's two functions I use for encrypting/decrypting strings. They were adapted from a powershell.com post but I don't have the link handy. The idea is that your password file would store the output of Protect-String and then to convert back to a regular string read in file and call UnProtect-String that value returned from UnProtect string would be your $password variable. ####################### function Protect-String { param([string]$InputString) $secure = ConvertTo-SecureString $InputString -asPlainText -force $export = $secure | ConvertFrom-SecureString write-output $export } #Protect-String ####################### function UnProtect-String { param([string]$InputString) $secure = ConvertTo-SecureString $InputString $helper = New-Object system.Management.Automation.PSCredential(""SQLPSX"" $secure) $plain = $helper.GetNetworkCredential().Password write-output $plain } #UnProtect-String Brilliant it works. Thank you very much!"
628,A,"Listening on two devices at once with libpcap I am trying to listen on two devices with libpcap but I still cant find out how to do the trick. I tried to set device to ""any"" but it isnt working. I am trying to write dhcp relay agent so i need to listen on eth0 and eth1. I tried to create two pcap_loops each with different device and handler but only first pcap_loop works second one is ignored. Is there any way how to do this or should I leave libpcap and try to do it with raw sockets? You'll need to run your pcap_loop() in separate threads one for each interface we do that and it works. Some parts of libpcap isn't thread safe though atleast pcap_setfilter() so provide your own locking around that. If you do no want to use threads you'll have to provide an event loop yourself where you monitor the file descriptors of each device with select/poll or similar. You can get the file descriptor for a device handle with pcap_get_selectable_fd(). Thanks :) I ended up using sockets :)"
629,A,"Network Communication Design Patterns I've come to realize that several questions I asked in the past such as this really boil down to a more fundamental question. Are there any well known design patterns for network communications and by virtue of it's nature protocol construction/parsing? A google search has not revealed much. Note that i'm not looking for solutions for any given problem i'm looking for documented design patterns dealing with network communications and their protocols. EDIT: Please don't suggest various implementation details or discuss specific protocols unless it's tied to a design pattern. Protocol design is not the issue it's the design patterns for creating or parsing protocols that i'm looking for not to mention the communication patterns themselves. EDIT2: I find it hard to believe that nobody has come up with any common patterns for network communication. Yes I know ""it depends"" but you can say that about any project yet there are lots of patterns that cover general ideas. Hi I was trying to ask similar question and I found your question. I have developed numerous applications where there is listener thread and client on every node. As you said different applications can have different needs. Some might require ACK some may rely on duplicate messages etc. While developing most of the times I look back at my previous code or go with what feels like the most logical way of doing something. It would be really helpful if there were any patterns that talk about application layer communication Or should one just look at existing protocols to find solutions? Check out also Proactor / Reactor patterns. Here is an article for example: [http://www.artima.com/articles/io_design_patterns.html](http://www.artima.com/articles/io_design_patterns.html) @MystereMan The age of the post doesn't matter if it's not *currently* suitable for the site then it gets closed. If you disagree with the closing feel free to bring it up on [Meta]. @casperOne - What is the purpose of closing a 3 year old question that is already answered? I recommend: abstract away the network protocol/s. First decide what are the functionality the modules and the APIs between them. Then decide what protocol is the data going to ride across the network. Then carefully encapsulate all the network issues in their own layer so you can later apply encryption compression add http transport (to pass firewalls) or whatever you want to add later in a manner orthogonal to functionality.  I don't know about patterns as such but there's a few ""obvious"" selection points. First do you want to use ASN.1 or not (this influences a WHOLE lot)? Second do you want a human-readable protocol or a binary one? Third do you want any security aspects in your protocol? Not that answering ""want to use ASN.1"" will force teh answer to quite a few of the protocol design questions.  Haven't gone through this thoroughly but I guess this is good doc: Also this looks like a good book:  This is a pretty broad question and its treatment likely requires a fairly dense book. I don't know of any such resource myself but lets think this through and consider what would be the dimensions of a network communication pattern space: connection modality: { connection-based connection-less} interaction modality: { synchronous asynchronous } conversation complexity: { command-response dialog} message form: { freeform-stream semi-structured block fully-structured block } ..? A good place to start is to take the TCP/IP family of protocols map them to the above space and take a look at the implementation(s) of one or more specimens that occupy a unique position in the above protocol-characteristics pattern space. Source code of your favorite *nix os would be a good place to look. Parser implementations would probably fall into two broad categories: {command-switched processing finite-state-machine}. The former is (obviously) the simpler of the two and likely the initial implementation (unless you've done this sort of thing before). The latter is (likely) more robust efficient (in terms of loc) and would allow for adopting changes to a protocol (if it is still subject to design change). (The underlying (virtual) OS networking facilities (of course) also greatly influence the implementation. Take JVM for example: NIO selection based channel processing would work quite nicely with a FSM.) Hope that helps.  Acceptor/Connector pattern : http://www.cs.wustl.edu/~schmidt/PDF/Acceptor.pdf Chain of filters base on Gof Chain Of responsabiliy it's used in a lot of network stack/framework. State machines for encoding/decoding PDUs.  I don't know about design patterns but researching existing protocols is probably a good starting point especially ""modern"" ones that have been standardized. BitTorrent is a highly popular decentralized protocol that has a number of extensions. OpenSSH is another good candidate; it supports feature negotiation multiple encryption types and de/muxing channels. VoIP protocols are good for streaming applications: RTP and H.323 Network routing protocols are good as well: BGP (and extensions) LDP VRRP/CARP.  I would say that the chain of responsability pattern could be usefull to send/receive data from/to the network. You build a series of commands to send to the server from the client. Each command is processed through the chain of responsability with data added to handle the command correctly. On data send the chain could look like that  Command --> Wrap some --> Encrypt --> Send data to send data around the command (source extra information if needed) On data receive the chain could be similar but the other way around  Receive Data --> Decrypt --> Unwrap extra data --> Execute command You can check this article for more informations about the chain of responsability. http://www.vincehuston.org/dp/chain.html"
630,A,"How can i get the available bandwidth rate? We have a DSL  how can we calculate the available bandwidth so that we can shape the packet We can assume the bandwidth to be 100mbps on the ethernet interface However in DSL devices the train rate(Bandwidth rate ) is varies according to different situation. How can i get the available bandwidth rate(from varies)?. Any method is there for getting the available bandwidth value? i can't find it right now but Google's Measurement Lab (http://www.measurementlab.net) has an API to test our internet connection speed. µTorrent is an application that uses this service. Every network interface can vary...you are not guaranteed 100mbps; it is simply the speed at which it CAN transmit. Hi sir Aaron! so any idea how to determine the available bandwidth sir? Can you explain more about how you plan to ""shape the packet""? What is your goal? Available bandwidth changes every time a packet is injected into the network from anywhere. It is not available via any API. TCP/IP already does 'packet shaping'. Your objective is unclear.  ""Bandwidth"" is not something you can determine for a single computer. As a minimum you need to specify both endpoints. Even your assumption of 100 mbps on Ethernet may be wrong as the cable or the other endpoint may not be capable of the full transfer rate. Usually this is done by Quality of Service functionality incorporated into the network transfer and it is not an easy thing to do. In our multiplayer game (using UDP) we have used Packet Pair Probing successfully and even if it is not universal it was the most reliable from all methods we have tried. I am afraid it is out of scope of this answer to describe it in more detail but this answer gives quite good description of something very similar in easy to understand terms: Basically if you start from zero bandwidth and increase bandwidth use latency very slowly increases - until you hit about 90% of your bandwidth. Then latency goes through the roof up to another plateau after which it again increases slowly. +1 very good and informative answer."
631,A,"i am unable to connetct udp server and udp client in this code? UDP_SERVER int main() { int sockfdclilen; char arr[20]; struct sockaddr_in serv_addcliaddr; if((sockfd=socket(AF_INETSOCK_DGRAM0))<0) printf(""error""); bzero((char *)&serv_addsizeof(serv_add)); serv_add.sin_family=AF_INET; serv_add.sin_port=htons(6060); serv_add.sin_addr.s_addr=inet_addr(""127.0.0.1""); if(bind(sockfd(struct sockaddr *)&serv_addsizeof(serv_add))<0) printf(""bind error""); //while(1) { //recv(sockfd&arr100); clilen=sizeof(cliaddr); recvfrom(sockfdarr100(struct sockaddr *)&cliaddr&clilen); printf(""%s""arr); } close(sockfd); return 0; } UDP_CLIENT int main() { int sockfd; struct sockaddr_in servaddrcliaddr; sockfd=socket(AF_INETSOCK_DGRAM0); bzero((char *)&servaddrsizeof(servaddr)); servaddr.sin_family=AF_INET; servaddr.sin_port=6060; servaddr.sin_addr.s_addr=inet_addr(""127.0.0.1""); bzero((char *)&cliaddrsizeof(cliaddr)); cliaddr.sin_family=AF_INET; cliaddr.sin_port=htons(0); cliaddr.sin_addr.s_addr=htonl(INADDR_ANY); //printf(""%s"") if(bind(sockfd(struct sockaddr *)&cliaddrsizeof(cliaddr))<0) printf(""error""); sendto(sockfd""subh.singh""100(struct sockaddr *)&servaddrsizeof(servaddr)); close(sockfd); return 0; } In the client you forgot htons: servaddr.sin_port=6060; /* Wrong. */ servaddr.sin_port = htons(6060); /* Right. */"
632,A,"Programming a P2P distributed system using Microsoft WCF I am about to develop a distributed system using WCF. I need to do the following: Send and receive packets ensuring delivery. Send and receive echo messages. Determine distances (if possible). Encrypt data and send them decript received data. I need to do this without discovery services or so on. I just need something that allows me to put an IP address and a port and to enstablish a communication. Is it possible to use TCP? What about UDP? Regarding your third point: what do you mean by ""distance""? Perhaps a time-based metric like latency? UDP is not a good choice for persistant connections; TCP is a much better choice. What you're talking about should be possible with WCF; the main issue is going to be getting the first IP address to connect to without having a centralized location. Basically you'll write a WCF service that has a specific endpoint your service could be hosted by the ""client"" application so it that you can connect to others running the same program; you'd simply need their IP and port to connect to their end-point. All that said depending on the nature of your P2P system writing your own TCP client and server may serve you better since WCF is mostly about passing messages back and forth."
633,A,"Writing a socket-based server in Python recommended strategies? I was recently reading this document which lists a number of strategies that could be employed to implement a socket server. Namely they are: Serve many clients with each thread and use nonblocking I/O and level-triggered readiness notification Serve many clients with each thread and use nonblocking I/O and readiness change notification Serve many clients with each server thread and use asynchronous I/O serve one client with each server thread and use blocking I/O Build the server code into the kernel Now I would appreciate a hint on which should be used in CPython which we know has some good points and some bad points. I am mostly interested in performance under high concurrency and yes a number of the current implementations are too slow. So if I may start with the easy one ""5"" is out as I am not going to be hacking anything into the kernel. ""4"" Also looks like it must be out because of the GIL. Of course you could use multiprocessing in place of threads here and that does give a significant boost. Blocking IO also has the advantage of being easier to understand. And here my knowledge wanes a bit: ""1"" is traditional select or poll which could be trivially combined with multiprocessing. ""2"" is the readiness-change notification used by the newer epoll and kqueue ""3"" I am not sure there are any kernel implementations for this that have Python wrappers. So in Python we have a bag of great tools like Twisted. Perhaps they are a better approach though I have benchmarked Twisted and found it too slow on a multiple processor machine. Perhaps having 4 twisteds with a load balancer might do it I don't know. Any advice would be appreciated. I like Douglas' answer but as an aside... You could use a centralized dispatch thread/process that listens for readiness notifications using select and delegates to a pool of worker threads/processes to help accomplish your parallelism goals. As Douglas mentioned however the GIL won't be held during most lengthy I/O operations (since no Python-API things are happening) so if it's response latency you're concerned about you can try moving the critical portions of your code to CPython API.  How about ""fork""? (I assume that is what the ForkingMixIn does) If the requests are handled in a ""shared nothing"" (other than DB or file system) architecture fork() starts pretty quickly on most *nixes and you don't have to worry about all the silly bugs and complications from threading. Threads are a design illness forced on us by OSes with too-heavy-weight processes IMHO. Cloning a page table with copy-on-write attributes seems a small price especially if you are running an interpreter anyway. Sorry I can't be more specific but I'm more of a Perl-transitioning-to-Ruby programmer (when I'm not slaving over masses of Java at work) Update: I finally did some timings on thread vs fork in my ""spare time"". Check it out: http://roboprogs.com/devel/2009.04.html Expanded: http://roboprogs.com/devel/2009.12.html in addition you could pull in the code for other modules that you know you will use before you start the child processes. This will prevent them from being tokenized (JIT-ed whatever) over and over. Second keep data in the parent small use ""exit"" as a super garbage collector.  asyncore is basically ""1"" - It uses select internally and you just have one thread handling all requests. According to the docs it can also use poll. (EDIT: Removed Twisted reference I thought it used asyncore but I was wrong). ""2"" might be implemented with python-epoll (Just googled it - never seen it before). EDIT: (from the comments) In python 2.6 the select module has epoll kqueue and kevent build-in (on supported platforms). So you don't need any external libraries to do edge-triggered serving. Don't rule out ""4"" as the GIL will be dropped when a thread is actually doing or waiting for IO-operations (most of the time probably). It doesn't make sense if you've got huge numbers of connections of course. If you've got lots of processing to do then python may not make sense with any of these schemes. For flexibility maybe look at Twisted? In practice your problem boils down to how much processing you are going to do for requests. If you've got a lot of processing and need to take advantage of multi-core parallel operation then you'll probably need multiple processes. On the other hand if you just need to listen on lots of connections then select or epoll with a small number of threads should work. I think epoll is in the stdlib in 2.6+ and easy_installable for 2.5. The package is called select-something. Sorry for vagueness. Twisted can also use epoll. In fact Twisted turns all supported event-notification APIs into a uniform API which it presents to you. So if the best the platform can do is select your app uses select. If it has epoll your app uses epoll. All transparently to you. It's spelled `asyncore` @new123456 you're right no one has pointed that out before.  http://docs.python.org/library/socketserver.html#asynchronous-mixins As for multi-processor (multi-core) machines. With CPython due to GIL you'll need at least one process per core to scale. As you say that you need CPython you might try to benchmark that with ForkingMixIn. With Linux 2.6 might give some interesting results. Other way is to use Stackless Python. That's how EVE solved it. But I understand that it's not always possible. Thanks but have you benchmarked those things? They are slow. I wouldn't invent the wheel if I didn't have to. +1 stackless/EVE but I did say CPython  Can I suggest additional links? cogen is a crossplatform library for network oriented coroutine based programming using the enhanced generators from python 2.5. On the main page of cogen project there're links to several projects with similar purpose.  One sollution is gevent. Gevent maries a libevent based event polling with lightweight cooperative task switching implemented by greenlet. What you get is all the performance and scalability of an event system with the elegance and straightforward model of blocking IO programing. (I don't know what the SO convention about answering to realy old questions is but decided I'd still add my 2 cents)"
634,A,"Is the select() wrapper in IO::Select thread-safe? How to work around? Let's say I have a thread: sub new { my $class = shift; my $self = ref $class || $class; bless { 'read_set' => IO::Select->new 'write_set' => IO::Select->new 'error_set' => IO::Select->new } $self; } sub start { my $self = shift; $self->{'thread'} = threads->create(\&worker_thr $self); $self->{'thread'}->detach; } sub worker_thr { my $self = shift; while(1) { my($read_active $write_active $error_active) = IO::Select->select( $self->{'read_set'} $self->{'write_set'} $self->{'error_set'} undef ); } } Since I have provided no timeout to select() and it blocks indefinitely until activity is detected on one of the descriptor (well ""handle"") sets what happens if another thread modifies the contents of the sets (e.g. adds a new socket for polling)? Is it implemented in a thread-safe manner relative to the Perl POSIX threads implementation? If not I suppose I can stick the blocking select() call in a block with its own scope and lock the sets or more cleanly the entire package data. What is the best way to wake select() up to return from another thread so that the contents of the sets can be manipulated? Thread-specific signal? Semaphore? Suggestions welcome. Thanks muchly! Edit: Well thread-specific ""signal"" is out. As explained here ( http://perldoc.perl.org/threads.html ): ""Correspondingly sending a signal to a thread does not disrupt the operation the thread is currently working on: The signal will be acted upon after the current operation has completed. For instance if the thread is stuck on an I/O call sending it a signal will not cause the I/O call to be interrupted such that the signal is acted up immediately."" That still leaves me wondering how I should handle the issue of waking select() up in a thread if I want it to block indefinitely instead of returning after a fixed timeout. By default perl threads don't share any data so if one thread changes its sets it will not affect the other. So yes it is thread-safe but it probably doesn't do what you want it to do. If you really want to wake-up another thread or process that's blocking on a select an easy solution is to add a pipe to it's read queue. You can then wake it up by writing a byte to that pipe. Hey that's not a bad idea!"
635,A,"Convert Ping application to multithreaded version to increase speed - C# I have an application that pings every possible IP on your local subnet in order to compile a list of responsive IP addresses. Currently it pings all 255 one at a time. Is it possible to convert this app to use multiple threads to increase the speed by pinging more than one at a time? I am new to the concept of multiple threads and figure this would be a good way to learn(as long as it's possible of course). also any stylistic improvements you can educate me on would also be helpful. thanks ahead of time Here is the current pinging method in a backgroundWorker1_DoWork event.  private void backgroundWorker1_DoWork(object sender DoWorkEventArgs e) { count = 0; for (int i = 1; i < 255; i++) { Ping ping = new Ping(); PingReply pingreply = ping.Send(IPAddress.Parse(locip[0] + ""."" + locip[1] + ""."" + locip[2] + ""."" + i)); count += 1; if (pingreply.Status == IPStatus.Success) { status = ""o""; repAddress = pingreply.Address.ToString(); ; repRoundtrip = pingreply.RoundtripTime.ToString(); repTTL = pingreply.Options.Ttl.ToString(); repBuffer = pingreply.Buffer.Length.ToString(); string[] lineBuffer = { status repAddress repRoundtrip repTTL repBuffer }; ipList.Rows.Add(lineBuffer); } progressBar.Invoke(new MethodInvoker(UpdateProgressBarByOne)); progressStatus.Text = (""Pinging IP "" + count + "" of 254""); } button1.Enabled = true; progressBar.Invoke(new MethodInvoker(ResetProgressBar)); } Well my advice is to look into your concurrency points. Firstly you will hit a bug with any access to windows forms objects off the thread. i.e your access to button1 WILL throw an MDA in debug and might crash at runtime randomly. You have to use a delegate and invoke the method back on the main thread using a pattern like this. this.Invoke(delgatetomyupdatermethod) Secondly your time is spent in the ping itself. So I would recommend writing a threadsafe list (just write a method with a lock on it private object locker = new object(); private void InsertIntoList(string linebuffer) { lock(locker) { ipList.Rows.Add(linebuffer); } } I would recommend using the .Net threadpool to run your method instead to ping a given IP. To do this write a function that will take in the IP to ping and to update the list with your result then call it by queueing up items on the threadpool. Indeed if you pass in an object with a ManualResetEvent you can even write your code to say System.Threading.WaitHandle[] waits = new System.Threading.WaitHandle[255]; //initialise the list of these and create the objects to ping. foreach (var obj in mylistofobjectvalues) { System.Threading.Threadpool.QueueUserWorkItem(method obj); } System.Threading.WaitHandle.WaitAll(waits); where method is the ping method obj contains an object with the manualresetevent and information your method needs to ping it's target. Each time your waithandle completes you can update your state. With some more effort on your GUI you could even have the system working asynchronously so that your gui is updated on each response not just at the end. I'm sure this makes complete sense I just have no idea what it says lol. Sorry if I'm remedial this solution is over my head which probably means my question is over my head also. how do I pass an argument while invoking a method like this ipList.Invoke(new MethodInvoker(UpdateIpList(lineBuffer))); It keeps telling me ""UpdateIpList(lineBuffer)"" isn't a method. Windows forms and threads aren't easy to do. same thing with concurrency it takes a bit of learning to get it right. I highly recommend ""Programming Visual C# 2008 the base class library"" which has a great intro to threading (and all the good parts of the framework too.) it will take ""UpdateIpList"" just fine but when I try to write the method to accept an argument it won't let me and I'm needing to pass the info out.  It looks like Ping has a SendAsync function. This post (I know its vb but just to get an idea) has an example. In summary just change your Send to SendAsync and listen for the PingCompleted event"
636,A,"Multiple Clients Using Same IP I'm having trouble using UDP with 2 clients using the same IP - after they both logged in one of them getting both's packets. How can I solve that? That should not happen[tm]. The clients should be on different ports and you should thus be able to differentate between them. However without more info on your problem it'll be hard to answer. Hmmm it's a game client I'm building.. The server is also my code.. I'm using SFML for server and asyncsocket for the client (iOS).. What more data? BTW I'm sending and getting on the same port. Maybe that's the problem? ""the same port"" - server-side or client-side? Server-side you listen on a single port but client-side every client that comes from the same IP will have a different port. Actually I'm quite surprised that you somehow manage to create two clients on the same IP *and* the same port :) I mean that I bind on the server in the same port I bind on the client side. Clients usually don't bind on a specific port - they open any ol' port (usually by passing a ""0"" port to the system call). That ensures that connections (uniquely identified by {clientIP clientPort hostIP hostPort} tuples) don't get mixed up clients get a random free port. Wait but if I will bind to 0 in the client how can I know what port I should send in the server side to the client? 0 also? No you send the port you get bound to. getsockname() can be used to grab the socket address you got assigned by the OS when binding without a specified port. Oh ok I will send it via TCP but that weird because you can trust UDP so the server will get the port... Thanks anyway! No need to do TCP. recv() has a version that allows you to pass a sockaddr pointer that gets filled in with the client info. Or you just pass it in your protocol in the first UDP packet. (and no you cannot trust UDP by the way :-)) @cdegroot: please compile this conversation into an answer so OP can accept it @andy - will do. The short answer is: don't bind to a specific port on the conversation-initiating side of either an UDP or TCP connection. By letting the OS assign a port to you (INADDR_ANY and port 0 in the socket address structure on bind()) you can have multiple clients on the same IP. For non-connection oriented conversations (e.g. UDP) the server still needs to know where to talk back to. To this end check the man page for recv() which has a version (recvfrom()) that lets you pass a pointer to a socket address structure - the address will be filled in with the client info and thus can be used immediately to send packets back. So on the server use: ssize_t recvfrom(int socket void *restrict buffer size_t length int flags struct sockaddr *restrict address socklen_t *restrict address_len); and you'll have the sender address in the sockaddr pointer passed. Hmmm I don't get it.. Do I need to first send the port from the client to the server? How? This recv() - that's for the client or server? I got the main idea I just don't know how to implomment it. send the first packet recv() it on the server side and you'll have the ip/port combo where you need to send back to. BTW - for anyone doing TCP/IP programming I recommend having ""TCP/IP Illustrated"" by Stevens at hand (all volumes except the one that deals with the BSD kernel implementation - that's for nerds ;-))"
637,A,What's the difference between async and nonblocking in unix socket? I'm seeing such code in nginx: if(fcntl(ngx_processes[s].channel[0] F_SETFL fcntl(s F_GETFL) | O_NONBLOCK) == -1) { ... if (ioctl(ngx_processes[s].channel[0] FIOASYNC &on) == -1) { ... Anyone can tell me what's the difference between fcntl(s F_SETFL fcntl(s F_GETFL) | O_NONBLOCK) and ioctl(s FIOASYNC &on) aren't async and nonblocking the same thing?? +1 your series of questions has taught me to steer clear of nginx. :-) FIOASYNC toggles the O_ASYNC flag (which is usually set in open(2) or fcntl(2)) for a file descriptor which will ask the kernel to send SIGIO or SIGPOLL to the process when the file descriptor is ready for IO. O_ASYNC is not used often: it is extremely difficult to properly handle IO in signal handlers; they are best left as tiny as possible because signals interrupt the control flow of the program they 'cost more' to run than standard system calls such as select(2) or poll(2) signals provide less information than other calls: they only report one fd ready vs many fds that might be ready. The O_NONBLOCK doesn't provide any notification to the user process that a fd is ready for read(2) or write(2) -- instead it changes the behavior of read(2) and write(2) and similar calls to return immediately if the file descriptor isn't ready for reading or writing. O_NONBLOCK is typically used in conjunction with select(2) or poll(2) or similar calls to guarantee that the main loop of a client or server won't block on one specific peer and thus starve all its peers. @sarnoldso you mean both are the same thing? Also `FIOASYNC` is a completely nonstandard/legacy way to do the equivalent of `O_ASYNC` with `open` or `fcntl`. nginx must have its reason to set this flag I think... @cpuer: No async and nonblocking are completely different concepts. @cpuer: It may have a good reason to set the flag but it should do so in the standards-sanctioned way using `fcntl` with `O_ASYNC` not some ancient 1980s-style `ioctl`. @sarnoldI saw your change. When using `ASYNC` and notification callback is invoked1.will the process stop its current routine ? 2. or both the current routine and the callback routine are running the same time? @R..can you describe the situation when `ASYNC` callbacks are invoked? Will it block the original process routine ? It's not a callback but a signal. If a signal handler for the signal has been installed it will asynchronously stop execution of the thread the signal was delivered to and execute the signal handler until it returns. After the signal handler returns the interrupted thread will continue. Being asynchronous signal handlers are rather dangerous and there are mildly complex rules for what you can and cannot do from a signal handler. Breaking those rules results in undefined behavior and may *appear to work* but suddenly crash deadlock or corrupt memory in timing-specific ways. @R: Can you cite the standard that specifies `fcntl` with `O_ASYNC` (or indeed mentions `O_ASYNC` at all)?
638,A,"How do I send an ARP packet from a C program? I'm working on an embedded linux system in C I'm looking for the source code to the equivalet of SendARP in Windows. Any pointers? This may be of interest: http://cvs.linux-ha.org/viewcvs/viewcvs.cgi/linux-ha/resources/heartbeat/SendArp.in?rev=1.4 It is an implmenetation in a Bourne Shell script. I really need C - but that's the right idea  I've never seen anything specifically for ARP but I think you can send any kind of packet you want using libpcap and the appropriate RFCs.  Take a look at arping. The quick and dirty way of sending an arp would be to do:  foo = system(""/somepath/arping somehost""); But a look through the arping source should be able to give you a better solution. For the all-out solution though you can construct your own by hand and use either a raw socket or libpcap to send it. btw. If all you're trying to do is force an arp to be sent (but necessarily from you) you could achieve that by deleting any arp entry that you already have for your host. The next access to that address will require an arp to be sent. eg. /usr/sbin/arp -d destination_host_ip"
639,A,"C socket programming: Connecting multiple clients to server using select() I'm trying to make a server that can be connected to by multiple clients. Here's my code so far: Client: int main(int argc char **argv) { struct sockaddr_in servaddr; int sock = socket(AF_INET SOCK_STREAM IPPROTO_TCP); if (sock == -1) perror(""Socket""); bzero((void *) &servaddr sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(6782); servaddr.sin_addr.s_addr = inet_addr(<server_ip_address>); if (-1 == connect(sock (struct sockaddr *)&servaddr sizeof(servaddr))) perror(""Connect""); while(1) { char message[6]; fgets(message 6 stdin); message[5] = '\0'; send(sock message 6 0); } close(sock); } Server: int main(int argc char **argv) { fd_set fds readfds; int i clientaddrlen; int clientsock[2] rc numsocks = 0 maxsocks = 2; int serversock = socket(AF_INET SOCK_STREAM IPPROTO_TCP); if (serversock == -1) perror(""Socket""); struct sockaddr_in serveraddr clientaddr; bzero(&serveraddr sizeof(struct sockaddr_in)); serveraddr.sin_family = AF_INET; serveraddr.sin_addr.s_addr = htonl(INADDR_ANY); serveraddr.sin_port = htons(6782); if (-1 == bind(serversock (struct sockaddr *)&serveraddr sizeof(struct sockaddr_in))) perror(""Bind""); if (-1 == listen(serversock SOMAXCONN)) perror(""Listen""); FD_ZERO(&fds); FD_SET(serversock &fds); while(1) { readfds = fds; rc = select(FD_SETSIZE &readfds NULL NULL NULL); if (rc == -1) { perror(""Select""); break; } for (i = 0; i < FD_SETSIZE; i++) { if (FD_ISSET(i &readfds)) { if (i == serversock) { if (numsocks < maxsocks) { clientsock[numsocks] = accept(serversock (struct sockaddr *) &clientaddr (socklen_t *)&clientaddrlen); if (clientsock[numsocks] == -1) perror(""Accept""); FD_SET(clientsock[numsocks] &fds); numsocks++; } else { printf(""Ran out of socket space.\n""); } } else { int messageLength = 5; char message[messageLength+1]; int in index = 0 limit = messageLength+1; while ((in = recv(clientsock[i] &message[index] limit 0)) > 0) { index += in; limit -= in; } printf(""%d\n"" index); printf(""%s\n"" message); } } } } close(serversock); return 0; } As soon as a client connects and sends its first message the server just runs in an infinite loop and spits out garbage from the message array. recv doesn't seem to receive anything. Can anyone see where i go wrong? In the while loop for the server change the code to do recv(i) instead of recv(clientsocks[i]). I have implemented this code and it works with this change.  You need to check for limit <= 0 in your read loop before you call read.  I replaced the else with the below and it works  } else { /* int messageLength = 5; char message[messageLength+1]; int in index = 0 limit = messageLength+1; memset ( &message[index]  0 sizeof ( message [index] ) ); while ((in = recv(i &message[index] limit 0)) > 0) { index += in; limit -= in; } printf(""%d\n"" index); printf(""%s\n"" message); */ bzero(buf sizeof(buf)); if ((rval = read(i buf 1024)) < 0) perror(""reading stream message""); else if (rval == 0) printf(""Ending connection\n""); else printf(""-->%s\n"" buf); }  Two issues in your code: You should do recv(clientsock[i] ...) instead of recv(i ...) After that you do not check if recv() failed and therefore printf() prints out the uninitialised buffer message hence the garbage in the output"
640,A,"Looking for a framework for logging arbitrary objects in heavily distributed and parallelized system in .NET (Note: I have seen several questions regarding .NET logging frameworks but haven't seen any which resembles our requirements. So please don't close this one as a duplicate.) We will need a logging framework for a highly distributed and parallel .NET application with the following features: It must be thread-safe as log messages will be produced by many threads. Each log messages will need to have a timestamp. If the framework would add this automatically that would make it easier. Besides a timestamps there must at least be a severity/verbosity or something similar to attach to log messages so that they can be filtered by it. More (orthogonal) predefined parameters (message source intended audience message kind whatever...) to play with would be great. However we need to add our own parameters anyway. The more these are treated like first-class framework citizens (can do the same with them as with the predefined stuff) the better. The framework should be able to filter messages based on at least the predefined parameters. We need to change filtering at whim during run-time. We need to put objects through this not strings. Strings might be made from those objects in the end but log message parameters must not be put into a flat string right from the start. Since some of the logging will be seen by customers we might have to internationalize them. (Given that we want to log objects not strings this whouldn't be too hard but the more support the better.) We need to be able to write the log messages left after filtering into files and read those files back in. If the framework has the ability to stream the out and in we don't have to implement this ourselves. Each log message might be sent to several targets (file window remote machine) at the same time. If log messages have to be sent over the network it might be necessary to do so in a dedicated thread. The thing will be used from C#. OK so we figure that it's nigh impossible to find something out there that does all of this out of the box. These are just too many requirements and probably too many uncommon ones. But is there anything worth to use as a foundation and build our stuff upon? AFAIK log4net does almost all of these. It's mainly verbosity-level based for filtering and though you can use Filters to filter using alternative criteria they're not necessarily ""first-class citizens"". Also re. ""reading the files back in"" - I presume you don't expect the logging framework to support this directly. Files are files after all. +1. See http://log4net.sourceforge.net/release/1.2.0.30316/doc/manual/faq.html#thread-safety . It's also relatively easy to extend log4net by creating custom appenders. Well I think if this is the only answer then there probably isn't much else out there. Thanks!"
641,A,"Parse large XML files over a network I did some quick searching on the site and couldn't seem to find the answer I was looking for so that being said what are some best practices for passing large xml files across a network. My thoughts on the matter are to stream chunks across the network in manageable segments however I am looking for other approaches and best practices for this. I realize that large is a relative term so I will let you choose an arbitrary value to be considered large. In case there is any confusion the question is ""What are some best practices for sending large xml files across networks?"" Edit: I am seeing a lot of compression being talked about any particular compression algorithm that could be utilized and in terms of decompressing said files? I do not have much desire to roll my own when I am aware there are proofed algorithms out there. Also I appreciate the responses so far. The usual type of compression used for this is gzip. Since you tagged this with ""servlet"" I'm assuming you're working in Java so you can use java.util.zip.GZIPOutputStream. if performance or memory usage are important there are SAX StaX and VTD-XML to consider Are you reading the XML with a proper XML parser or are you reading it with expectations of a specific layout? For XML data feeds waiting for the entire file to download can be a real waste of memory and processing time. You could write a custom parser perhaps using a regular expression search that looks at the XML line-by-line if you can guarantee that the XML will not have any linefeeds within tags. If you have code that can digest the XML a node-at-a-time then spit it out a node-at-a-time using something like Transfer-Encoding: chunked. You write the length of the chunk (in hex) followed by the chunk then another chunk or ""0\n"" at the end. To save bandwidth gzip each chunk. Sorry had to downvote because of the suggestion of a regex on xml which as I have seen is a bad idea. Thanks for the second bit though.  If you can keep a local copy and two copies at the server you could use diffxml to reduce what you have to transmit down to only the changes and then bzip2 the diffs. That would reduce the bandwidth requirement a lot at the expense of some storage.  Compression is an obvious approach. This XML bugger will shrink like there is no tomorrow.  Depending on how large it is you might want to considering compressing it first. This of course depends on how often the same data is sent and how often it's changed. To be honest the vast majority of the time the simplest solution works fine. I'd recommend transmitting it the easiest way first (which is probably all at once) and if that turns out to be problematic keep on segmenting it until you find a size that's rarely disrupted. Lets say for the sake of my learning proper approaches I have a large file that is changed constantly and a large file that is changed rarely. Testing is really the best way to see what works best. Gzip then send repeat 1000 times see what the total time is. Compare to sending without zipping. Also be sure to account for transmission errors.  Compressing and reducing XML size has been an issue for more than a decade now especially in mobile communications where both bandwidth and client computation power are scarce resources. The final solution used in wireless communications which is what I prefer to use if I have enough control on both the client and server sides is WBXML (WAP Binary XML Spec). This spec defines how to convert the XML into a binary format which is not only compact but also easy-to-parse. This is in contrast to general-purpose compression methods such as gzip that require high computational power and memory on the receiver side to decompress and then parse the XML content. The only downside to this spec is that an application token table should exist on both sides which is a statically-defined code table to hold binary values for all possible tags and attributes in an application-specific XML content. Today this format is widely used in mobile communications for transmitting configuration and data in most of the applications such as OTA configuration and Contact/Note/Calendar/Email synchronization. For transmitting large XML content using this format you can use a chunking mechanism similar to the one proposed in SyncML protocol. You can find a design document here describing this mechanism in section ""2.6. Large Objects Handling"". As a brief intro: This feature provides a means to synchronize an object whose size exceeds that which can be transmitted within one message (e.g. the maximum message size – declared in MaxMsgSize element – that the target device can receive). This is achieved by splitting the object into chunks that will each fit within one message and by sending them contiguously. The first chunk of data is sent with the overall size of the object and a MoreData tag signaling that more chunks will be sent. Every subsequent chunk is sent with a MoreData tag except from the last one. As this was the most descriptive and informative post I selected this as the answer."
642,A,"What happens when we say ""listen to a port""? When we start a server application we always need to speicify the port number it listens to. But how is this ""listening mechanism"" implemented under the hood? My current imagination is like this: The operating system associate the port number with some buffer. The server application's responsibiliy is to monitor this buffer. If there's no data in this buffer the server application's listen operation will just block the application. When some data arrives from the wire the operating system will know that and then check the data and see if it is targeted at this port number. And then it will fill the corresponding buffer. And then OS will notify the blocked server application and the server application will get the data and continue to run. Question is: If the above scenario is correct how could the opearting system know there's data arriving from wire? It cannot be a busy pooling. Is it some kind of interrupt-based mechanism? If there's too much data arriving and the buffer is not big enough will there be data loss? Is the ""listen to a port"" operation really a blocking operation? Many thanks. Your description is basically correct except for the blocking part. OSes normally use interrupts to handle I/O events like arriving network packets so there is no need to block. Yes if too many connection attempts happen at the same time some will get bounced. The number of connections to queue is specified when you call listen or its equivalent. No it is not. The OS raises an event on your control socket when a connection arrives. You may choose to block while waiting for this event or you may use some nonblocking (select poll/epoll) or asynchronous (overlapped I/O completion ports) mechanism. Thanks Anton. Interrupt make sense. Then is it software or hardware interrupt? If it is software interrupt then the interrupt must be raised by some piece of code and this code NEEDs to know when data arrives. So it is basically back to my original question - busy pooling or what. If it is hardware interrupt then the NIC (network interface card) must be designed in a manner that when ever some signal is detected from wire it will interrut the CPU. The interrupt signal could go through a 8259 chip or similar things. Please correct me if I am wrong. And any comment is appreciated. @smwikipedia It is a hardware interrupt. The NIC has several packet-filters included which monitor incoming packets(so called datagrams) about the embedded MAC and IPV4 addresses. These filters are implemented mostly in a hardware state machine or dedicated processors (matching these fields is not a complex task but it performs bad on general purpose processors). The NIC fires interrupts when it receives a datagram with a matching IPV4 address (ip.dest==nic.ip or ip.dest==multicast/broadcast) or a matching MAC address. Hardware interrupt of course. NICs are somewhat more complicated than you describe — they handle Ethernet-level and parts of IP/TCP-level processing (this is called offloading) — but basically the OS allocates buffers for NIC to write incoming packets to by DMA/busmaster and the NIC raises interrupts when packets arrive. Sending data is arranged similarly: the OS maintains a queue of buffers describing packets to be sent the NIC processes this queue and raises an interrupt when the queue becomes empty. All modern I/O controllers (USB IDE/SATA in busmaster mode) work basically the same way. **damn 600 char limit** Both IPV4 and MAC filter can also be disabled on (almost) every NIC (needed to monitor communication on a virtualization host hot-standby scenarios ...). NB: this disabling @Rudi mentioned is called 'promiscuous mode'.  If the above scenario is correct how could the operating system know there's data arriving from wire? It cannot be a busy pooling. Is it some kind of interrupt-based mechanism? Hardware tells it by sending an event a hardware interrupt causes an event handler to run. If there's too much data arriving and the buffer is not big enough will there be data loss? Yep but TCP uses a windowing mechanism. The OS tells the other end how much buffers it has it can do this dynamically. So it may start with I have 4k of buffers. After 2k has arrived the other end can send 2k more but we can acknowledge the first 2k. If the other end sends to much to quickly our OS will discard it. It will also tell it to slowdown and re-acknowledge what is has already got. When buffers are free we can tell the other end to continue it will resend what we have not acknowledged. The OS does all this for us when using TCP but not for UDP. Is the ""listen to a port"" operation really a blocking operation? Yes but very very quick. Listen does next to nothing just a note to the OS. If someone tries to connect to that port it is me that will handle it. It is accept that waits for that connection. The os need not allocate any buffer this early. Listen wrote some meta data into an os table. A connection comes in uses the next connection handling buffer. Later data comes in and uses a data buffer data buffer need not be allocated per connection. Lots of pending data on one connection could cause the available buffers on other connections to be reduced. Your OS may have policies and mechanisms to make thing faire.  While the other answers seem to explain things correctly let me give a more direct answer: your imagination is wrong. There is no buffer that the application monitors. Instead the application calls listen() at some point and the OS remembers from then on that this application is interested in new connections to that port number. Only one application can indicate interest in a certain port at any time. The listen operation does not block. Instead it returns right away. What may block is accept(). The system has a backlog of incoming connections (buffering the data that have been received) and returns one of the connections every time accept is called. accept doesn't transmit any data; the application must then do recv() calls on the accepted socket. As to your questions: as others have said: hardware interrupts. The NIC takes the datagram completely off the wire interrupts and is assigned an address in memory to copy it to. for TCP there will be no data loss as there will always be sufficient memory during the communication. TCP has flow control and the sender will stop sending before the receiver has no more memory. For UDP and new TCP connections there can be data loss; the sender will typically get an error indication (as the system reserves memory to accept just one more datagram). see above: listen itself is not blocking; accept is.  What happens when we say ""listen to a port"" ? The typical TCP server sequence of calls is socket() -> bind()-> listen() -> accept() -> read()/write() -> close() A socket created by socket function is assumed to be an active socket (that will issue a connect()). listen() function converts unconnected socket to passive socket. This means that kernel should start accepting incoming connection requests. The second argument to listen() function specifies the total queue length for a given listening socket of 2 queues - (1) complete connection queue - 3 way handshake completed for connection (2) incomplete connection queue - SYN received from client waiting for completion of 3 way TCP handshake Finally accept() is called by TCP server to return the next completed connection from the front of completed connection queue. If accept() is successful it returns a new socket descriptor that refers to the TCP connection between client and server. Now to answer your question * The networking stack in the operating system kernel reads each incoming IP packet classifies the packet according to it's TCP/IP header fields. The arrival of IP packet on wire is serviced as an interrupt by an Ethernet driver and from there onwards kernel mode TCP/IP stack takes over With respect to data if you mean the SYN packet Posix.1g has an option to either ignore the new incoming SYN or send a RST to the client when the connection queue is full. Data that arrives after 3 way handshake completes but before server calls accept should be queued by server TCP up to the size of connected socket's receive buffer. listen() operation is a blocking call and returns after the connection state is said to passive to allow incoming TCP client connections. Refer to Wikipedia for more details on TCP protocol -handshake sequencing and acknowledgments for reliable transmission. This book gives a very good details on TCP/IP Unix network programming and can provide more insight on this topic."
643,A,"Why would a blocking socket repeatedly return 0-length data? I'm having a significant problem using a standard BSD-style socket in a C++ program. In the code below I connect to a local web server send a request and simply create a loop waiting for data to return. I actually do receive the data but then I get an endless stream of 0-length data as if it was a non-blocking socket. The web server presumably didn't kill the connection because if so I would have received a length of -1. Please ignore simple typos I make below as I'm writing the code from memory not a direct copy/paste. The code produces the same result on OSX and Windows. int sock = socket(AF_INET SOCK_STREAM 0); //assume serv_addr has been created correctly connect(sock (sockaddr*)&serv_addr sizeof(serv_addr)) < 0); std::string header = ""GET / HTTP/1.1\r\n"" ""Host: 127.0.0.1:80\r\n"" ""Keep-Alive: 300\r\n"" ""Connection: keep-alive\r\n\r\n""; send(sock header.c_str() header.length()+1 0); for (;;) { char buffer[1024]; int len = recv(sock buffer 1024 0); cout << len << endl; //this outputs two numbers around 200 and 500 //which are the header and html and then it //outputs and endless stream of 0's } No recv will only return -1 if you have an error on the client side. If the server disconnects from you you will get a 0 which is basically like an EOF. From the man page of recv For TCP sockets the return value 0 means the peer has closed its half side of the connection. Wow thanks I guess that was a misunderstand on my part but why would it close the connection if I specified ""keep-alive""? When recv() returns 0 the connection was closed; but if you want to poll (or wait on) the socket for incoming data check out the man page for select()."
644,A,"Java Socket Programming I am building a simple client/server application using java sockets and experimenting with the ObjectOutputStream etc. I have been following the tutorial at this url http://java.sun.com/developer/technicalArticles/ALT/sockets starting half way down when it talks about transporting objects over sockets. See my code for the client http://pastebin.com/m37e4c577 However this doesn't seem to work and i cannot figure out what's not working. The code commented out at the bottom is directly copied out of the tutorial - and this works when i just use that instead of creating the client object. Can anyone see anything i am doing wrong? You need to uncomment it. Hah seriously though what is the compilation error? There's no error - it just doesn't work with my uncommented code and i can't see why? The commented code works - i've just seperated that and put it into an actual object... it's driving me mad. The problem is the order you are creating the streams: In the server from the article (which I assume is what you are using) when a new connection is opened the server opens first an input stream and then an output stream: public Connect(Socket clientSocket) { client = clientSocket; try { ois = new ObjectInputStream(client.getInputStream()); oos = new ObjectOutputStream(client.getOutputStream()); } catch(Exception e1) { // ... } this.start(); } The commented example code uses the reverse order first establishing the output stream then the input stream: // open a socket connection socket = new Socket(""localhost"" 2000); // open I/O streams for objects oos = new ObjectOutputStream(socket.getOutputStream()); ois = new ObjectInputStream(socket.getInputStream()); But your code does it the other way around: server = new Socket(host port); in = new ObjectInputStream(server.getInputStream()); out = new ObjectOutputStream(server.getOutputStream()); Establishing an output stream/input stream pair will stall until they have exchanged their handshaking information so you must match the order of creation. You can do this just by swapping lines 34 and 35 in your example code.  If you tried a debugger it would have told you where the problem was.(Perhaps not why) The issue you have is that ObjectOutputStream writes a header and ObjectInputStream reads that header. You have created the ObjectInputStream first which means it is trying to read a header which will never be written. Solution: Always create the ObjectOutputStream first and flush() it before creating the ObjectInputStream.  Probably you'll like to learn the most basic first. Here's a sample I have just coded. It start a server that attends only ONE client and it sends an object and die. When the user ( you ) press enter a new client is created it connects to the previously created server and read the object that server will send. No exception is handled here. Just to make things simpler but this is NOT the way exception should be handled. When you understand all the concepts here it will be easier to understand those in the tutorial. import java.io.*; import java.net.*; import java.util.*; public class SimpleServer implements Runnable { // Creates the server send a ""date"" and die. public void run() { try { ServerSocket server = new ServerSocket( 8090 ); Date creationDate = new Date(); System.out.println(""(Server) Server is ready "" + ""and running at "" + creationDate ); Socket uniqueClient = server.accept(); // We will write using this ""chained"" object. ObjectOutputStream out = new ObjectOutputStream( uniqueClient.getOutputStream()); out.writeObject( creationDate ); // close all at this point forget about the exceptions. // this is lesson #1 out.close(); uniqueClient.close(); server.close(); System.out.println(""(Server) The server is down""); }catch( IOException ioe ) {} } public static void main ( String [] args ) throws IOException  ClassNotFoundException { Thread serverThread = new Thread( new SimpleServer() ); serverThread.start(); // start the server thread ... doh.. System.out.println(""(Client) Press enter when you want ""+ "" to connect to the server...""); Scanner scanner = new Scanner( System.in ); scanner.nextLine(); Socket client = new Socket(""localhost"" 8090 ); // Read from this object. ObjectInputStream in = new ObjectInputStream( client.getInputStream() ); Date date = ( Date ) in.readObject(); System.out.println(""(Client) Current time is: "" + new Date() ); System.out.println(""(Client) Object read from server : "" + date ); in.close(); client.close(); } } I hope this helps.  You are not writing the object anywhere. See that link again somewhere you have to write:  oos.writeObject( new Date() ); In your code you only have ois.readObject(); That's why The code posted only includes the client so it doesn't need to write an object.  Just a reminder. When you use ObjectOutputStream keep in mind that it keeps a reference cache. If you write an object change the object contents and then send the same object again you will get duplicate data. For example: List list = new ArrayList(); list.add(""value1""); out.writeObject(list); list.clear(); list.add(""value2""); out.writeObject(list); Will produce in the client side two lists with the string ""value1"". To avoid this the reset method must be invoked to reset the stream cache when writing the same object reference multiple times: List list = new ArrayList(); list.add(""value1""); out.writeObject(list); out.reset(); list.clear(); list.add(""value2""); out.writeObject(list); Cheers - this will be a useful reminder!"
645,A,"Asymmetric packet round trip time? Recently our project has been through a situation which machine A and machine B has different round trip time against each other. ex. machine A sends a packet to B and it arrives in several milliseconds while machine B sends a packet to A and it arrives in several MINUTES! Sometimes it is simply lost. Our setup is fairly simple: machine A and machine B connect to the same router. It works just fine mostly. However sometimes the situation described in the example above will happen after we run a stress test for one or two days. Finally our underlying API is Raknet. Packets are sent in IMMEDIATE priority. Any comments is appreciated. Thanks! As with any problem you're going to need to start narrowing down the conditions under which it happens. Right now you're basically saying ""we have a problem that happens sometimes"". That makes it hard to give anything other than a general answer. The first thing to do is start simplifying your setup. Can you connect the two systems directly with a crossover cable? If not try a simple 10/100 hub if you can find one. Eliminate the router if at all possible. If the problem still occurs you'll need to see what else might be happening. Maybe set up Wireshark on both machines with a circular buffer to hold the last several MB of data. Then try and stop the data capture when it happens and look for weirdness."
646,A,"HttpWebRequest.GetResponse() returns error 500 Internal Server Error I'm using HttpWebRequest to make a request to a url: HttpWebRequest req = (HttpWebRequest)HttpWebRequest.Create(urlAddress); HttpWebResponse resp = (HttpWebResponse)req.GetResponse(); but it throws error 500 (Internal Server Error) but when i visit the URLAddress with browser it works fine urlAddress= www.khademnews.com it is a simple GET operation but it throws an exception for me how can I solve this? The server apparently expects some HTTP headers in the request that a web browser typically sends but the HttpWebRequest does not. You need to figure out which headers these are (for example using [Fiddler](http://www.fiddler2.com/fiddler2/)) and add them to the HttpWebRequest. You might need to set up the user agent as some sites might require it. Also you could use a WebClient to simplify your code: using (var client = new WebClient()) { client.Headers[HttpRequestHeader.UserAgent] = ""Mozilla/5.0 (Windows NT 6.1; rv:5.0) Gecko/20100101 Firefox/5.0""; string result = client.DownloadString(""http://www.khademnews.com""); } The server might expect other headers as well. You could check with FireBug which headers are sent went you perform the request in your browser and add those headers."
647,A,"Loopback adapter name in Linux Is it safe to assume that the loopback network adapter on a Linux system will always be called 'lo' - is this just a naming convention that may not be adhered to or must it always be the case? RFC3330 defines 127.0.0.0/8 to always be the loopback subnet. The use of localhost however defined on Windows in c:\windows\system32\drivers\etc\hosts and Linux in /etc/hosts is purely convention. Furthermore the name lo is the typical name given to the localhost interface in Linux. If you must be absolutely certain use 127.0.0.1. RFC3330 defines 127.0.0.0/8 as the subnet for loopback addresses. All it says about 127.0.0.1 is that it is ordinarily used as the loopback address. You could be using 127.0.1.1 as a loopback address if you want. -1 for not reading the RFC you linked to and then giving wrong advice.  Interfaces can be renamed to anything you want - but anyone who renames the loopback interface is being extremely silly and deserves to have a nonworking system :) Yes you can enumerate the interfaces and get their names. But perhaps it's just as easy to just assume it's going to be ""lo"".  In my experience it is a common name although you shouldn't always trust in it being so. Maybe enumerating the interfaces and looking for the one with an address of 127.0.0.1 would be the way to go?  I don't know of any Linux system that has a loopback interface anything other than lo. I would rely on this naming convention if I write a system-specific script but not when writing a portable program. For example loopback in OSX is lo0. A reliable way in C is calling a SIOCGIFCONF ioctl on a socket iterating over the interfaces calling SIOCGIFFLAGS ioctl on each one and checking which interfaces have a IFF_LOOPBACK flag set (see /usr/include/linux/if.h). SIOCGIFCONF will also give you interface names. This appears to have the advantage of not requiring any IPv4 (""127.0.0.1"") address to be configured on the system.  It's a pretty old convention in fact I have not seen a Linux box/distro yet that didn't call it 'lo'. However device names in *nix systems are so diverse it can be assumed they will change. Use the standards if you want portability (in this case 127.0.0.1).  Using 127.0.0.1 is probably the failsafe way to go about it."
648,A,How to track which network ports any program is trying to use? I want to see what kind of connections any program is doing which port and see the program exe path etc. I'm trying to achieve some sort of firewall notification system it would pop up a window for me to tell that this and that port needs to be opened in order the program could work properly. How do i get started on this? You'll need to hook the socket API for each process you're willing to provide this functionality or write a filter using WFP and a client application that receive the informations from your filter and shows a notification window.  netstat /o gives you the process id for each network connection and netstat /b gives you the executable name. Right but this won't tell you when net access is prohibited since there will be nothing for netstat to see in the case of a failure. You need a trackable event at the time of every connection attempt.  Check out Windows Filtering Platform (WFP) API. Not sure how you correlate network activity to applications but this is the correct place for you to intercept it. +1. The correlation may be occur at the ALE (see `processId` within `inMetaValues`). With PID you can easily retrieve executable path and parameters. @Jweyrich - thanks for adding the extra info
649,A,"Getting list of network devices inside the Linux kernel I've been looking through net/core/dev.c and other files to try to find out how to get the list of network devices that are currently configured and it's proving to be a little difficult to find. The end goal is to be able to get network device statistics using dev_get_stats in dev.c but I need to know the current interfaces so I can grab the net_device struct to pass in. I'm having to do this inside the kernel as I'm writing a module which adds in a new /proc/ entry which relates to some statistics from the current network devices so from what I can gather this must be done inside the kernel. If someone could point me to how to get the interfaces it would be much appreciated. Given a struct net *net identifying the net namespace that you are interested in you should grab the dev_base_lock and use for_each_netdev(): read_lock(&dev_base_lock); for_each_netdev(net dev) { /* Inspect dev */ } read_unlock(&dev_base_lock); (In newer kernels you can use RCU instead but that is probably an overcomplication in this case). To obtain the net namespace to use you should be registering your proc file with register_pernet_subsys(): static const struct file_operations foostats_seq_fops = { .owner = THIS_MODULE .open = foostats_seq_open .read = seq_read .llseek = seq_lseek .release = foostats_seq_release }; static int foo_proc_init_net(struct net *net) { if (!proc_net_fops_create(net ""foostats"" S_IRUGO &foostats_seq_fops)) return -ENOMEM; return 0; } static void foo_proc_exit_net(struct net *net) { proc_net_remove(net ""foostats""); } static struct pernet_operations foo_proc_ops = { .init = foo_proc_init_net .exit = foo_proc_exit_net }; register_pernet_subsys(&foo_proc_ops) In your foostats_seq_open() function you take a reference on the net namespace and drop it in the release function: static int foostats_seq_open(struct inode *inode struct file *file) { int err; struct net *net; err = -ENXIO; net = get_proc_net(inode); if (net == NULL) goto err_net; err = single_open(file foostats_seq_show net); if (err < 0) goto err_open; return 0; err_open: put_net(net); err_net: return err; } static int foostats_seq_release(struct inode *inode struct file *file) { struct net *net = ((struct seq_file *)file->private_data)->private; put_net(net); return single_release(inode file); } The foostats_seq_show() function can then obtain the net walk the devices gather the statistics and produce the output: static int sockstat6_seq_show(struct seq_file *seq void *v) { struct net *net = seq->private; struct net_device *dev; int foostat barstat; read_lock(&dev_base_lock); for_each_netdev(net dev) { /* Inspect dev */ } read_unlock(&dev_base_lock); seq_printf(seq ""Foo: %d\n"" foostat); seq_printf(seq ""Bar: %d\n"" barstat); return 0; }  This ought to do the trick: #include <linux/netdevice.h> struct net_device *dev; dev = first_net_device(&init_net); while (dev) { printk(KERN_INFO ""found [%s]\n"" dev->name); dev = next_net_device(dev); } Locking is required (`dev_base_lock`) against concurrent updates to the list. Excellent! Works like a charm just looks like I was looking in the wrong file cheers!"
650,A,What network port to use for mobile app I currently have a mobile app that uses a socket connection to communicate with a server. However it seems that some users are complaining that the port is blocked by their ISP or wireless network. Is there a range of ports that are are well-known enough to not be blocked but also not used by mobile devices that I could use? Or is there a better way to address this issue? Asking the ISP/wireless network admin to unblock a port is currently not the most feasible thing for me to do. EDIT: I'm looking at Canadian US and European mobile operators in particular. BTW including country/network might also help other users to give you better advice based on experience with those networks. The best and only port to use would be 80 standard http traffic. You could also try... 443 SSL http traffic 8080 backup HTTP traffic port normally used for proxy servers or the links. But honestly if you are even remotely worried about networks blocking your traffic standard HTTP and port 80 are the only ways to go. Most networks will allow port 80 if they allow any thing. BTW some times the only way is to ask the ISPs/Networks to open up that port to your server this isn't ideal but if they block port 80 you are going to have to ask them.
651,A,"Bridge multiple BlackBerry connections to increase bandwidth I am interested in doing something like ""link aggregation"" over the data connection on multiple BlackBerry devices. What tools are available to do this? If I write this myself how do I balance out the data across the different devices? Can this be done entirely on the remote side - with just the BlackBerries and a computer or will I need a server to connect all the data back together? Though I think you could rephrase in a way that is more stackoverflow approrpriate such as asking about algorithms and libraries that might help you do this instead of what looks like a request for COTS software. I've edited your question to be more appropriate for stackoverflow Not even remotely a programming question. @Andrew So you don't understand that Tether is a program or that I am looking for a program solution to allow this program to run multiple instances? Perhaps you are not a programmer? @Michael Donohue Thank you for your rewording of my question. For data tethering the easiest way is to play with routing table. For example you have 3 BlackBerries connected to one computer and they will be shown as three different network adapters. Now you can design your routing metrics by dividing IP address range. Say BB1 routes traffic to and from 1.1.1.1 to 70.255.255.255 BB2 routes 71.0.0.1 to 142.255.255.255 and all other traffic goes through BB3. Ideally the three devices will have balanced load. If you want to have something like trunking i.e. triple bandwidth connecting to same IP/port that will be challenging because you're handling a three IP to one problem and you have no control over the IP addresses assigned to the BlackBerries (your ISP assign them to you). In that case having your own server as a relay makes sense but I seriously doubt the usefulness. Switch to LTE will be a much easier choice in this case."
652,A,"C#: get information about computer in domain What classes should I use in C# in order to get information about a certain computer in my network? (Like who is logged on that computer what Operating System is running on that computer what ports are opened etc) Are you trying to run this code on the target machine? or are you trying to run this from your computer and get information about another machine over the network? I want to run the code from my computer to get info about another machine over the network knowing its IP Provided example of remote WMI query in my answer. Look into the WMI library.  Checkout System.Management and System.Management.ManagementClass. Both are used for accessing WMI which is how to get the information in question. Edit: Updated with sample to access WMI from remote computer: ConnectionOptions options; options = new ConnectionOptions(); options.Username = userID; options.Password = password; options.EnablePrivileges = true; options.Impersonation = ImpersonationLevel.Impersonate; ManagementScope scope; scope = new ManagementScope(""\\\\"" + ipAddress + ""\\root\\cimv2"" options); scope.Connect(); String queryString = ""SELECT PercentProcessorTime PercentInterruptTime InterruptsPersec FROM Win32_PerfFormattedData_PerfOS_Processor""; ObjectQuery query; query = new ObjectQuery(queryString); ManagementObjectSearcher objOS = new ManagementObjectSearcher(scope query); DataTable dt = new DataTable(); dt.Columns.Add(""PercentProcessorTime""); dt.Columns.Add(""PercentInterruptTime""); dt.Columns.Add(""InterruptsPersec""); foreach (ManagementObject MO in objOS.Get()) { DataRow dr = dt.NewRow(); dr[""PercentProcessorTime""] = MO[""PercentProcessorTime""]; dr[""PercentInterruptTime""] = MO[""PercentInterruptTime""]; dr[""InterruptsPersec""] = MO[""InterruptsPersec""]; dt.Rows.Add(dr); } Note: userID password and ipAddress must all be defined to match your environment. thank you for the links  Here is a list of all the relevant WMI clases that you can investigate to obtain the required information  Here is an example of using it in like an about box. MSDN has the rest of the items you can all. using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Text; using System.Windows.Forms; using System.Management; namespace About_box { public partial class About : Form { public About() { InitializeComponent(); FormLoad(); } public void FormLoad() { SystemInfo si; SystemInfo.GetSystemInfo(out si); txtboxApplication.Text = si.AppName; txtboxVersion.Text = si.AppVersion; txtBoxComputerName.Text = si.MachineName; txtBoxMemory.Text = Convert.ToString((si.TotalRam / 1073741824) + "" GigaBytes""); txtBoxProcessor.Text = si.ProcessorName; txtBoxOperatingSystem.Text = si.OperatingSystem; txtBoxOSVersion.Text = si.OperatingSystemVersion; txtBoxManufacturer.Text = si.Manufacturer; txtBoxModel.Text = si.Model; } } } This will only work if the code is run ON the machine in question; and will not work to get information about another machine on the network. Where is this class (SystemInfo) defined?  WMI Library and here is a VB.net example. It shouldn't be difficult to convert it to C#"
653,A,"Find Packet Loss and Trace Route in .NET I am trying to code to capture the packet loss on computers but the only way I've been able to do so was to run the NetStat.exe and the TraceRt.exe processes and capture them to a listbox (using the code below): Private Sub myProcess() Dim p As System.Diagnostics.Process Dim theFile sTemp sLineOut mySent myRetrans As String Dim intSentStart As Double Dim isEnabled As Boolean p = New System.Diagnostics.Process theFile = t.Name If File.Exists(""c:\windows\system32\"" & theFile) Then theFile = ""c:\windows\system32\"" & theFile Else MessageBox.Show(""Unable to find the "" & theFile & "" file on your computer."" ""File Find Error!"" MessageBoxButtons.OK MessageBoxIcon.Error) End If Select Case t.Name Case ""TraceRt.exe"" p.StartInfo.Arguments = ""-h 30 www.stackoverflow.com"" Case ""NetStat.exe"" p.StartInfo.Arguments = ""-s -p tcp"" End Select Try p.StartInfo.FileName = theFile p.StartInfo.CreateNoWindow = True p.StartInfo.UseShellExecute = False p.StartInfo.RedirectStandardOutput = True p.Start() p.PriorityClass = System.Diagnostics.ProcessPriorityClass.Normal sLineOut = """" Do While Not p.HasExited sTemp = p.StandardOutput.ReadLine() If sTemp <> """" Then sLineOut = sLineOut & sTemp & vbNewLine 'Send to listbox setTrace(sTemp) End If Loop If sLineOut = """" Then 'Clear Listbox clearTrace() myProcess() Exit Sub End If If t.Name = ""NetStat.exe"" Then Try If lboxTrace.Items.Count < 9 Then 'Clear Listbox clearTrace() myProcess() Exit Sub End If mySent = """" mySent = lboxTrace.Items.Item(8) '(""Segments Sent"") mySent = Trim(mySent.Replace(""Segments Sent"" """")) mySent = Trim(mySent.Replace(""="" """")) myRetrans = """" myRetrans = lboxTrace.Items.Item(9) myRetrans = Trim(myRetrans.Replace(""Segments Retransmitted"" """")) myRetrans = Trim(myRetrans.Replace(""="" """")) intSentStart = Math.Round((myRetrans / mySent) * 100 2) 'setTrace sends data to the listbox If intSentStart < 2 Then setTrace(""Your Current Packet Loss is: "" & intSentStart & ""%."") setTrace(""Your packet loss is within acceptable ranges."") Else setTrace(""Your Current Packet Loss is: "" & intSentStart & ""%."") setTrace(""Your packet loss is below acceptable ranges!"") setTrace(""Please contact your Internet Provider about your Internet Connection."") End If Catch ex As Exception 'Clear Listbox clearTrace() myProcess() Exit Sub End Try End If Catch ex As Exception MessageBox.Show(ex.Message) End Try End Sub Does anyone have a better idea to get this information? I'd rather use managed code instead of hacking through it this way. Thanks in Advance! -JFV Not sure if this is what you're looking for but have you taken a look at the Ping class? Edit: Actually for TraceRoute take a look at the answers here: http://stackoverflow.com/questions/142614/traceroute-and-ping-in-c. For Netstat take a look at http://towardsnext.wordpress.com/2009/02/09/netstat-in-c/."
654,A,"How can I run two instances of the iPhone simulator (Aspen) to try out the WiTap example? Has anyone had any luck running two instances of the iPhone simulator to test network code between the two? possible duplicate of [Is there a way to simulate multiple iphones using xcode/iphone sim?](http://stackoverflow.com/questions/896487/is-there-a-way-to-simulate-multiple-iphones-using-xcode-iphone-sim) Follow these steps to test your app. You have to do one thing for whole process first : Go to Project - > Edit Project Settings -> tick on option - Build independent targets in parallel. Debug code that will create your app in simulator For Example your App name is - Instance Close Simulator Go to Project->New Target -> Instance2 Add ""Bundle display name : Instance2"" in Instance2.info.plist file Set Executable Name : Instance2 Select Instance2 Target -> Go to Project -> Set Active Target -> Instance2 Build with Instance2 Note : Do all carefully otherwise you have to reset your simulator For Reset Simulator -> Run Simulator -> iPhone Simulator -> Reset Content & Settings... Now You have two apps in simulator and you can test app on all instance. Hope it will help you :) Ravikant Nagar  No but you can run the Simulator and the WiTap code on your phone. I've done that and it's pretty cool. Just provision the WiTap code for your development provisioning profile and load it on the phone. Then making sure the phone is on the same network wifi you can run your development machine with the Simulator running WiTap and the phone running WiTap. Works like a charm. Charms don't work but we get your meaning.  Just an idea but you could try adding another user to your mac and run the simulator within both accounts. (I am pretty sure that the simulator config is stored relative to the user home directory) First step would be to get that going under user switching. If that works it should hopefully set up everything the simulator needs for a second instance. Then log out the second account go back to your usual account. Now try running a second simulator instance from the command line in a terminal but as the second user account (use 'su -' to switch the other user account before running the simulator). I'm not at my mac and offhand I don't know the name of the simulator binary or I would try this out and paste some code for you. This doesnt work :/  FYI frankodwyer's idea of running iPhoneSimulator as a different user doesn't work. Quoth Finder: ""You can't open the application 'iPhone Simulator' because another user has it open. Ask the other user to quit the application then try again."""
655,A,"Get the Network Interface Name in C# I'm writing a program which uses the wPCAP library. I managed to get out the list of interface names: \Device\NPF_{A9734063-CA83-4D91-A35B-CC727749256A ... Now my question is: how do I know which interface this is? I'm attempting to get the IP of the interface with that name. I tried the NetworkInterface.GetAllNetworkInterfaces() function but in those classes I could only find the ""readable"" name and not the \Device... name. Thank you! Yvan Edit: I need to get the IP address of the device with the name ""\Device\NPF_..."". How do I accomplish this? Actually I found a solution to circumvent the problem: I just listen on each interface :-S. That does accomplish also a desireable result (namely the logging of each visited web page). Thank you all!  My first guess would be that it's more a device problem than a network problem see : http://www.codeproject.com/KB/system/DeviceStatus.aspx and http://msdn.microsoft.com/en-us/library/aa394353%28v=vs.85%29.aspx Hope this helps But how do I get the IP address of such a device with only the Device ID? http://msdn.microsoft.com/en-us/library/aa394216 if you use the device ID you should be able to get everything you need  A long time ago I did something similar. Look up the following and I am sure you will find what you are looking for: IPInterfaceProperties properties = adapter.GetIPProperties(); Yeah I know. I have the IP. But how do I convert the adapter class to a \Device\... id? That's the method I'm looking for... Ah ok you could try the managementobjectsearcher: http://msdn.microsoft.com/en-us/library/system.management.managementobjectsearcher.aspx"
656,A,Easy way to create fragmented IPv4 packets I'm working on a network stack at the moment and I'm trying to implement IPv4 fragmentation. Whilst I have an implementation which works in theory I would like to actually test that it works by throwing actual fragmented packets at it. Is there any software or perhaps an easy way to write code to do this? I'd rather not go and modify MTUs on the hosts I'm testing with just to get this working. My development environment is Windows. Not sure if this is useful http://stackoverflow.com/questions/2121458/how-can-i-simulate-tcp-ip-errors Would trickling out the data from your test app would do the job? I think the simplest way is to use ping:  ping -l 2000 192.168.0.1 for linux it should be:  ping -s 2000 192.168.0.1  I ended up writing my own little application to send an n-sized packet. I reduced the MTU of the network segment I was testing on and the stack received and reassembled the fragmented packets. Most of the software I found to do this required a listener on the target computer which is implausible for the kind of testing I'm doing.
657,A,Writing XML Data to OutputStream with Mutable objects in java What is the best way to write XML to an OutputStream (TCP Socket) without generating/creating too many Immutable objects? So I have my data in a ConcurrentHashMap. I want to loop this data create a custom XML and then write the XML to an OutputStream. This process will be repetitive and so I dont want to generate too many objects during the conversion/writing process so that GC doesnt have too much load. I've been looking at JAXB and XStream to make the Map to XML conversion easier but it seems like with the XMLAdapter in JAXB and Convertor approach in XStream I'll end up with objects created during the conversion process. I'm willing to roll my own too. I want a solution where I end up reusing mutable objects. I could use a StringBuffer and concatenate everything (XML tags and my data) using append method and then do mystringBuffer.toString().getBytes() and write the bytearray to the outputstream. In this approach I could reuse StringBuffer and ByteBuffer. Only the bytearray will be a new object each time. Any other approach? It seems like I'm getting ahead of my self and assuming that memory/GC could have issues - I could be totally wrong and merely using JAXB or XStream could be the solution. I could then just do some performance testing to find out bottleneck. Thanks Why are they immutable? Are you inside a thread or something? I don't think you're giving the garbage collector enough credit - it's extremely good at handling a large number of short-lived objects. Don't worry about object creation overhead until you can see an actual problem with the performance. This question sounds like a classic case of premature optimisation. @skaffman - you should convert your comment as an answer. Agreed with skaffman's comment: I don't think you're giving the garbage collector enough credit - it's extremely good at handling a large number of short-lived objects. Don't worry about object creation overhead until you can see an actual problem with the performance. This question sounds like a classic case of premature optimisation. So yes I'd also do some performance testing to see if there is even a bottleneck that you need to be concerned with. (Primarily posting this answer in an attempt to either get this question some additional attention / competitive answers or at the least to simply remove this from the growing list of unanswered questions.)
658,A,"Application deadlocks when creating cipher streams from socket I'm having problems encrypting and decrypting my streams between two sockets. ObjectInputStream oIn = new ObjectInputStream(new FileInputStream(new File(""key""))); SecretKeySpec spec = (SecretKeySpec) oIn.readObject(); //'key' file was saved previously Cipher cEncrypt = Cipher.getInstance(""AES""); cEncrypt.init(Cipher.ENCRYPT_MODE spec); Cipher cDecrypt = Cipher.getInstance(""AES""); cDecrypt.init(Cipher.DECRYPT_MODE spec); //should have no problems here I tried the ciphers out by encoding and decoding a String works fine ObjectOutputStream objectOutputStream= new ObjectOutputStream(new CipherOutputStream(socket.getOutputStreamcEncrypt)); objectOutputStream.flush(); ObjectInputStream objectInputStream = new ObjectInputStream(new CipherInputStream(socket.getInputStreamcDecrypt)); Then the program stops. The code is the same on both sides of the socket. Without using ciphered streams the program transfers data fine. Any help is much appreciated thanks! See CipherOutputStream.flush(): Any bytes buffered by the encapsulated cipher and waiting to be processed by it will not be written out. For example if the encapsulated cipher is a block cipher and the total number of bytes written using one of the write methods is less than the cipher's block size no bytes will be written out. Basically your data can only be written out in chunks of 16 bytes because you're using a block cipher like ECB or CBC. You can avoid this problem by using a stream cipher. See block ciphers modes of operation for details. You'll need to select CFB OFB or CTR. E.g. when you get your Cipher instance: Cipher cipher = Cipher.getInstance(""AES/CTR/PKCS5Padding""); If I use CTR mode do I have to use a common IV for both terminals? Or is that handled by the initialization of the Cipher obj? Thanks for the tip it's working now! Although I used ""AES/CFB8/NoPadding"""
659,A,"Delphi Network programming I have a classic client/server (fat client and database) program written in Delphi 2006. When certain conditions are met in the client I need to notify all the other clients very quickly. Up until now this has been done using UDP broadcasts but this is no longer viable as clients now connect from outside the LAN and the UDP broadcast is limited to the local network. I'm aware of the Indy libraries but am not really sure of which components to use and how to structure it. I'm guessing I'll need to have a server that the clients connect to which will receive and distribute the messages...? Any samples out there to get me started? Are there any other component sets or technologies I should look at instead/as well? FirebirdSQL project use the concept of notifications as being server-client connections that send a string to the client. For this the db server uses an other port. And require the client to register it's interesting of receiving a certain type of notification through an API call. You could use the same idea.  There are a few really easy ways to do this with Delphi although I am sure the RemObjects SDK works really well too. Have a central server that has a * TIdTCPServer listening* on it. Then each client has a TIdTCPClient on it. They connect to the server and block on a read waiting for the server to write. Once the server receives a notification via a listening socket it broadcasts to each of the waiting clients. This is pretty much immediate notification of all the clients. Have a central server that has a TIdTCPServer listening on it. Then each client has a TIdTCPClient on it. Those clients can ""ping"" the server to ask for updates at a regular interval (use a session token to maintain state). The frequency of the interval determines how quick the notification will be. When once one of the clients needs to notify the others it just notifies the server. The server then uses a message queue to make a list of all active client sessions and adds a notification for each. Then the next time each of the clients connects it gives it the notification and remove it from the queue. Maintain a session table in the database where each client updates regularly that they have an active session and removes itself when it disconnects. You will need a maintenance process that removes dead sessions. Then you have a message queue table that a client can write an update to with one row for each current active session. Then the other clients can regularly ping that table to see if there are any pending notifications for its session if there are it can read them act on them and then remove them. Some sort of peer to peer approach were the clients are aware of each other through information in the database and then they connect directly to each other and notify or ask for notifications (depending on firewall and NAT configurations). A little more complex but possible. Obviously the choice of implementation will depend on your setup and needs. Tunning will be necessary to achieve the best results. The components you need for this are the TIdTCPServer (listener) and TIdTCPClient (sender). Both of which are in the Indy libraries in Delphi.  RabbitMQ should fit your bill. The server is free and ready to use. You just need a client side to connect push/send out message and get/pull notified message Server: http://www.rabbitmq.com/download.html Do a google for client or implement yourself Cheers  You should be able to use Multicast UDP for the same purpose. The only difference will be to join the multicast group from every client. http://en.wikipedia.org/wiki/IP_Multicast http://en.wikipedia.org/wiki/Internet_Group_Management_Protocol Edit: Just to clarify multicast let you join a given ""group"" associated to a multicast ip address. Any packet sent to that address will reach every client who has join the group  ICS components from http://www.overbyte.be are great. a.) Better compatibility than Indy b.) PostCard ware Good examples and support. Use TClientSocket and TServerSocket  The simple answer is that the standard protocols available in Delphi (and other tools) don't allow for notification in reverse. I looked into this for a project where I wanted to use SOAP. They all assume client asks server server responds and that's it. For me the solution was the RemObjects SDK. This allows you to send notifications to clients and the notification can have any data you like (just like the client to server). Myself I use the SuperTCP connection but it works with others too. It can still offer a SOAP interface for clients that must use it but for where you have control of both client and server it works extremely well. They do allow for notifications in reverse it just isn't immediately obvious since the connections need to already be made or be made at regular intervals to check for the notifications.  You can watch weonlydo wodVPN component which permit you to create a robust UDP hole punching and gain a port-forwading or a normal VPN (with a fornished network adapter) so you can connect two PC behind a NAT. I'm using this control for our communication program and works very fine."
660,A,Network discovery programmatically Is there a specific protocol used for network discovery? I'm looking to code this into an existing java project. Currently I'm using a port scanner to handle the case but I'm looking to swap that out for something that can give me a little more info. If possible I'd like to scan to discover machine and pull the Ip addr host name MAC addr NIC make/model OS and anything else I can get. Network discovery of what? What machines the host computer is connected to? Other machines running a certain server program? Can you be more specific? With a topological map? this will prove a very interesting task... Check out the SNMP protocol. It has a way to autodiscover devices on the network. Keep in mind that there are some security concerns with this (especially older versions of the protocol). that's certainly one piece of the puzzle.  There is no one protocol that will do all this for you. I've had to do exactly this and basically the best approach involves using a combination of heuristics to locate analyze and cross-reference network nodes and topology. Here are the data sources I used: Traceroute allows you to identify edge devices and routers in the network Port-scanner allows you to identify what services are running on each node SNMP allows you to detect the type of device as well as all its network interfaces other IP addresses the IP of devices connected to each port on switches the routing table the process table the network configuration etc... This is the best source of data but requires the node to be running an snmp server (installed by default on windows and most linux distros) and to have credentials. WMI for windows hosts will provide roughly the same info as SNMP Here's an accademic resource I dug up while working on my topology mapper. Hopefully it will help. Good luck!  WBEM (similar to WMI on Windows) can be used to scan Unix computers and other devices. Of course only if the WBEM instrumentation is installed... WBEM even works for ESX(i) servers if it is enabled there. Otherwise you might also use SSH to programatically logon to Unix computers and issue system commands to extract the information you need. There is a nice Java library for WBEM called SBLIM There are many Java library for telnet and SSH out there  Check nmap for what it can. It is network scanner can scan with ARP TCP-SYN and many other sniffing techniques. It also contains large database of different machines fingerprints so it can guess what OS/version given system runs. @Nick I think the easiest way to determine MAC is to do ARP (Address Resolution Protocol) scan (check `arping` utility) or simply try to `ping` address and check ARP table of your system. Keep in mind that you can only get MAC addres of hosts in your local network (or only 1 segment). yes im familiar with nmap. One of the features I'm looking to get more info to implement is determining MAC addr from the scan. Any ideas on where to start?
661,A,network applications in iphone I want to build an application to connect to a network server and share the datas with it. Could any one please inform me any sample applications or tutorials to help me in this path. Thank you. What kind of network server is that? I want to connect to the web and access the datas stored in that web server You might want to read the documentation for NSURLConnection and related questions here on Stack Overflow. There’s also a lot of interesting source code on GitHub. Thank you Zoul. Thank you for your help.  IU think you need to study about externalAccessories framework . to add external devices through iphone He doesn’t want to connect the server directly to the machine he just wants to access some data over HTTP. Or at least that’s the way I read the question.  http://developer.apple.com/library/mac/#documentation/cocoa/reference/Foundation/Classes/NSSocketPort_Class/Reference/Reference.html Read it it will help you. Why start with sockets? I think that some higher-level interface like `NSURLConnection` would be better.
662,A,"Scheduling Packet Retransmission I'm programming a network protocol over UDP using C/C++ in Linux. The protocol must provide reliability so I'm going to simulate something like TCP retransmission over UDP. This can be done using pthreads or fork but I believe that's an overkill and consumes a lot of system resources. A better approach is to exploit a scheduler. I probably can't use Linux internal scheduler since I'm programming in user space. Are there standard C/C++ libraries to accomplish this? How about 3rd party libraries? Edit: Some people asked why I'm doing this. Why not use the TCP instead? The answer is since I'm implementing a tunneling protocol. If someone tunnels TCP over TCP the efficiency will drop considerably. Here's more info Why TCP Over TCP Is A Bad Idea. TCP over TCP has one major advantage in some networks: You can offload the handling of nagle's algorithm to the NIC. Of course if you want to implement a different windowing algorithm for your specific network topology or one that can scale from modem speed to ten-G you have to disable that and might as well use UDP. @SilverbackNet: Thanks. That's some good news! Isn't TCP always implemented over IP and you tunnel IP over something else (for example IP or ATM); if a packet gets dropped then TCP will detect it and retransmit. A tunnelled packet being dropped (or a fragment) normally means the whole frame being resent by TCP. Is this not good enough? Wifi uses its own ack/retry for TCP performance because it has a lot of packet loss itself. What makes you think that either threads or forks consume a lot of system resources or that a scheduler is necessarily better if you haven't used any of them before? What do you mean by ""reliability"" ? Do you mean to say that the packets must arrive? or do you also require that they not only arrive but also arrive in the correct order? each requirement will require a set of underlying logic which is not easy to implement for all kinds of n/w activity (not even for the general case if there is such a thing) hence the existence of TCP. @SilverbackNet: I used pthreads and forks before! And I said they're overkill because they use a lot of extra data structures. For example consider a webserver which wants to send 1000 packets (to many clients). It's not wise to create 1000 threads/process just to monitor timeouts. @Beh Tou Cheh: I don't want to complicate matters. Scheduling does not have to do with either interpretations. However you can read [Why TCP Over TCP Is A Bad Idea](http://sites.inka.de/sites/bigred/devel/tcp-tcp.html) to get why I'm trying this. Probably insufficient information for meaningful answer; how you handle retransmission is probably application-specific. If you are trying to duplicate TCP don't; use it instead. @MarkR: see the updated answer for more info. If you are implementing a tunneling protocol why use retransmission at all? I suppose there are some reasons (for example like wifi which uses its own acknowledgements and retransmission because it would otherwise lose an unacceptable amount of packets for many uses) @MarkR: I get your point but for some reason not totally known to me the reliability mechanisms in the tunneled protocol does not seem to suffice (I deduced this from the link I provided as well as implementations such as OpenVPN---see http://openvpn.net/index.php/open-source/documentation/security-overview.html). @MarkR: When tunneling a reliable protocol (such as TCP) over an unreliable protocol (such as UDP) it is mandatory to implement reliability issues such as ACKs and retransmission. This is mainly because there's not a 1-1 correspondence between the tunneled protocol (TCP) and the tunneling protocol (UDP): The TCP segments might be fragmented. Here's a sample of how to accomplish asynchronous coroutines with Boost. Boost manages the overhead of creating a thread to run the coroutine in this case so that you don't need to. If you would like the kernel to manage your interrupts you can use alarm & setitimer but they're very limited in what they can do. Any solution will include threads forks or some variant of them at some level unless you synchronously manage the transmission in the main thread using something like select(). No need for 3rd party libs asio already has inbuilt coroutines check-out http_server version4: http://think-async.com/Asio/boost_asio_1_4_7/doc/html/boost_asio/examples.html#boost_asio.examples.http_server_4 Though you should realize that coroutines in this sense are used to make the code easier to understand (in an fsm manner) and to also reduce the cost incured when using bind (binding and invoking)  Not clear what exactly you are trying to schedule. You can use libevent for efficient and somewhat portable interface. This is basically similar to Matthew's suggestion of using select but using the most efficient interface (which select is not) on FreeBSD Linux and MacOS X (actually their page now claims Windows support as well but I'm not too familiar with that). This will give ability to do non-blocking event-driven network calls. It will not solve the scheduling part. DOing it in a separate thread is not going to hurt your performance. I think running a pthread per connection is not the best approach but having a single scheduling thread and some worker threads dealing with the network events and maybe some non-trivial processing usually works well. There's also libev.  The ""scheduler"" you're after is called ""select"" and it's a user-space call available in linux. Type ""man 2 select"" to read the help page for how to use it. If you need a timeout just call select() with a timeout value. The select call will return either when new data has arrived or a timeout has expired. You can then do retransmissions if there was a timeout. Side note: `select` is generally good for a small amount of sockets otherwise I'd suggest using `poll` or `epoll`. yep definitely agree."
663,A,"Implementing DHCP client On unix using C my client is listening on port 68 with superuser mode. After sending DHCP discover message when I try to receive it blocks in recvfrom means there is no message received or is it like system has a process (DHCP client) listening on same port 68 which receives the message and thats my process are not able to receive the message. What is the problem? I have set the socket option SO_REUSEADDR and SO_BROADCAST. I am sending to port 67. struct dhcpmessage { uint8_t op; uint8_t htype; uint8_t hlen; uint8_t hops; uint32_t xid; uint16_t secs; uint16_t flags; uint32_t ciaddr; uint32_t yiaddr; uint32_t siaddr; uint32_t giaddr; char chaddr[16]; char sname[64]; char file[128]; char magic[4]; char opt[3]; } __attribute__((__packed__)); #include<stdio.h> #include<ctype.h> #include<stdlib.h> #include<string.h> #include<unistd.h> #include<signal.h> #include<sys/wait.h> #include<sys/types.h> #include<sys/socket.h> #include<netinet/in.h> #include<arpa/inet.h> #include<errno.h> #include<sys/file.h> #include<sys/msg.h> #include<sys/ipc.h> #include<time.h> #include""defs.h"" int main() { int sockfdlistenfdconnfd; const int on=1; struct sockaddr_in servaddrcliaddrrservaddr; if((sockfd=socket(AF_INETSOCK_DGRAM0)) < 0) die(""socket""); if(setsockopt(sockfdSOL_SOCKETSO_REUSEADDR&onsizeof(on)) < 0) die(""setsockopt""); if(setsockopt(sockfdSOL_SOCKETSO_BROADCAST&onsizeof(on)) < 0) die(""setsockopt""); bzero(&servaddrsizeof(servaddr)); bzero(&cliaddrsizeof(cliaddr)); cliaddr.sin_port = htons(68); cliaddr.sin_family = AF_INET; cliaddr.sin_addr.s_addr = htonl(INADDR_ANY); if(bind(sockfd(struct sockaddr*)&cliaddrsizeof(cliaddr)) < 0) die(""bind""); servaddr.sin_port = htons(67); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = inet_addr(""255.255.255.255""); struct dhcpmessage dhcpmsg; bzero(&dhcpmsgsizeof(dhcpmsg)); dhcpmsg.op = 1; dhcpmsg.htype = 1; dhcpmsg.hlen = 6; dhcpmsg.hops = 0; dhcpmsg.xid = htonl(1000); dhcpmsg.secs = htons(0); dhcpmsg.flags = htons(0x8000); dhcpmsg.chaddr[0] = 0x00; dhcpmsg.chaddr[1] = 0x1A; dhcpmsg.chaddr[2] = 0x80; dhcpmsg.chaddr[3] = 0x80; dhcpmsg.chaddr[4] = 0x2C; dhcpmsg.chaddr[5] = 0xC3; dhcpmsg.magic[0]=99; dhcpmsg.magic[1]=130; dhcpmsg.magic[2]=83; dhcpmsg.magic[3]=99; dhcpmsg.opt[0]=53; dhcpmsg.opt[1]=1; dhcpmsg.opt[2]=1; if(sendto(sockfd&dhcpmsgsizeof(dhcpmsg)0(struct sockaddr*)&servaddrsizeof(servaddr)) < 0) die(""sendto""); struct dhcpmessage recvdhcpmsg; socklen_t rservlen = sizeof(rservaddr); if(recvfrom(sockfd&recvdhcpmsgsizeof(recvdhcpmsg)0(struct sockaddr*)&rservaddr&rservlen) < 0) die(""recvfrom""); char *str = (char*)&recvdhcpmsg; int i; for(i=0;i<sizeof(recvdhcpmsg);i++) printf(""%d_""str[i]); printf(""\n""); return 0; } Why someone has voted to close this question? You could look at the source code for an existing DHCP client and learn how it works: ftp://ftp.isc.org/isc/dhcp/ You may also want to post a relevant snippet of your source code so that people can see exactly how you are trying to do this. Added the source code now  The caveat is you don't have an IP address you want to obtain one. - most OSs won't allow you to bind/send UDP messages on a NIC w/o an IP address. dhcp clients typically use raw sockets for that purpose/and should set the src address to 0.0.0.0) Such a raw socket will get all the packets and your application will not if there's a system dhcp client running u mean to say that I make a raw socket bind on 68 port and then system will put received data on both my raw socket and system's dhcp client raw socket. Can u give me souce code/snippet giving an idea of how to go about this thing?  Is it using UDP or TCP? It should be UDP. U mean to say that I make a raw socket bind on 68 port and then system will put received data on both my raw socket and system's dhcp client raw socket. Can u give me souce code/snippet giving an idea of how to go about this thing? @nos: Thanks for ur suggestion. I was making a blunder. @nos please answer my comment. Ya its UDP obviously. I am broadcasting to 255.255.255.255  I just forked dhclient as dhcprobe. It is a dhcp client for linux. It's pretty straightforward to hack and might be a good starting point for your project."
664,A,"Increment IP address In that program I want to increment IP address. And I see output like that: 125.23.45.67 126.23.45.67 127.23.45.67 128.23.45.67 129.23.45.67 130.23.45.67 131.23.45.67 132.23.45.67 133.23.45.67 134.23.45.67 But I want to see output like this: 124.23.45.67 124.23.45.68 124.23.45.68 124.23.45.70 124.23.45.71 124.23.45.72 124.23.45.73 124.23.45.74 124.23.45.75 124.23.45.76 Here is program code: #include <stdlib.h> #include <stdio.h> #include <iostream> using namespace std; #include ""winsock2.h"" #pragma comment(lib""wsock32.lib"") void main() { in_addr adr1; in_addr adr2; int i; adr1.s_addr=inet_addr(""124.23.45.67""); adr2.s_addr=inet_addr(""as.34.34.56""); if (adr1.s_addr!=INADDR_NONE) cout << "" adr1 correct"" << endl; else cout << "" adr1 incorect "" << endl; if (adr2.s_addr!=INADDR_NONE) cout << "" adr2 correct"" << endl; else cout << "" adr2 incorect"" << endl; cout << inet_ntoa(adr1) << endl; cout << inet_ntoa(adr2) << endl; for (i=0;i<10;i++) { adr1.s_addr ++; cout << inet_ntoa(adr1) << endl; } } Instead of using adr1.s_addr: adr1.s_addr=inet_addr(""124.23.45.67""); adr2.s_addr=inet_addr(""as.34.34.56""); Use this: u_long addr1=inet_addr(""124.23.45.67""); And increment addr1 i.e. addr1++ the last octet gets incremented. Or follow this formula: if IP is A.B.C.D then u_long addr = A + 256*B + 256*256*C + 256*256*256*D  To increment an IP address you will need to break up the in_addr object into 4 int objects (a short int will also do) and increment the 4th one until it hits 256 and then reset it to 1 and increment the 3rd one etc. You shouldn't be using ++ on the in_addr object directly. EDIT: Okay so you can properly increment it if you reverse the byte order. I personally wouldn't do it that way. Especially if all you're doing is outputting IP strings and not using them as an in_addr elsewhere in code. Your version is too complicated IMHO.  Big endian and little endian gets another one! Use htonl and ntohl to convert back and forth. for (i=0;i<10;i++) { adr1.s_addr = htonl(ntohl(adr1.s_addr) + 1); cout << inet_ntoa(adr1) << endl; } @selbie: doing math on a pointer seems odd. What happens when the math generates a pointer off the array? And yet... ;-) Hm what do you mean pointer? @Steve Jessop I mean a pointer (http://en.wikipedia.org/wiki/Pointer_%28computing%29#C_and_C.2B.2B). People do math on them despite the fact that this math could exceed valid bounds. It's the responsibility of the person doing the math to avoid this. In this IP address example if you know that the subnet mask is at least 7 bits then the range x.y.z.67 to x.y.z.76 is not off the subnet. thx a lot. i've jast looked for this function ""htonl"". +1 but flipping the byte ordering seems hackish. [Other people](http://stackoverflow.com/questions/1505676/how-do-i-increment-an-ip-address-represented-as-a-string/1505709#1505709) agree with you though. Doing ""math"" on an IP address seems odd anyway. What happens when the math generates an IP address off the subnet? All the corner cases have to be handled. A bit of history. Back in the 90's when Sun ruled the Unix planet with their Sparc architecture. No one would properly use the endian conversion functions because they didn't need to. Then Linux on x86 came around the same time as Winsock. It was really annoying to port network code to these new platforms- because it would compile just fine but not actually work. This works fine. But how to avoid generating network and broadcast addresses ? The OP never really said what he was trying to do so I gave him a very specific answer for the problem he was trying to solve. My golden rule of socket and network programming: The developer should have good knowledge on the fundamentals of TCP/IP before attempting to write such code."
665,A,"Help designing a secure network protocol In an app I Have a network server and clients. After a handshake let's say the client sends ""userId sessionId SOME_COMMAND param param param"". I have already identified the client and the sessionId is checked on the server accordingly so identity is no more an issue. But I'd like to prevent a hacker to modify the message or create a false one for example sending ""userId sessionId SOME_COMMAND paramModified paramModified paramModified"". I thought about using a pair of private/public encryption keys and send the hash of the message in the message itself. But since it's automated in the client program I may have to send the public key during the handshake. So the hacker could simply retrieve it and generate the proper hash. I could also use complex encryption seeds or algorithms but my experience with hackers has shown me that they will decompile anything. So the bottom line is: I can hide everything that runs on the server but I can't hide anything on the client program. And I'd like to to forbid to modify the message that the client program is supposed to send. I don't even know if it's possible. And I'm opened to any suggestion. And by the way I'm using Java although it should not be very relevant. Thanks. Forget it. Use SSL like everybody else. There are complexities which you haven't even begun to address."
666,A,net.unix.max_dgram_qlen questions According to the kernel documentation net.unix.max_dgram_qlen sysctl controls the maximum length of a datagram socket receive queue (for AF_UNIX/AF_LOCAL sockets that is). I can always send 1 more than this value before send calls to that receiver start blocking. Anyone know why? Also does anyone know if this was ever implemented as a socket option. (Kind of like SO_SNDBUF corresponds to wmem_default and wmem_max). This thread mentions that possibility but I can't find where anyone ever did it. 1. That's how it is checked: static inline int unix_recvq_full(struct sock const *sk) { return skb_queue_len(&sk->sk_receive_queue) > sk->sk_max_ack_backlog; } That's why you can initiate one more connection than the value you had set. 2. The backlog parameter you pass to listen() is also used as max_ack_backlog. No other way exists to influence it though. listen() is only for stream sockets so for datagram your only option is the sysctl knob. You're right about the check being counter-intuitive but that's I guess for historical reasons. Are you talking about datagrams or stream sockets? My question concerns datagrams. Are you saying that both max_dgram_qlen (for datagrams) or the backlog parameter (for stream sockets) map to max_ack_backlog? My question isn't so much how it is checked (that seems clear) but rather why is it checked that why? Why > instead of >=? It's hard to believe there isn't a reason since it's counter-intuitive.
667,A,"how can i learn my client ip with .NET? i need my client ip from whatismyip.com. But Regex pattern is not correct i think? Can you help me this patttern? Why are you using whatismyip.com to retrieve your ip? :) Voting to close. Don't ask the same question twice. If your question does not get you the answer try expanding it with more detail. http://stackoverflow.com/questions/2272483/how-can-i-get-ip-address-of-my-3g-modem For what it's worth I believe my answer to your original question should do the trick: http://stackoverflow.com/questions/2272483/how-can-i-get-ip-address-of-my-3g-modem/2272673#2272673 Yes Jørns answer to the original question is entirely sufficient. Have you read the comment in the obtained HTML: Please set your code to scrape your IP from www.whatismyip.com/automation/n09230945.asp For more info please see our ""Recommended Automation Practices"" thread in the Forum. So this should get you going: using (var client = new WebClient()) { Console.WriteLine(client.DownloadString( ""http://www.whatismyip.com/automation/n09230945.asp"")); } using is a good technique to automatically delete the resource used by variable..  This is how you get the ip in ASP.NET C# string pstrClientAddress = HttpContext.Current.Request.UserHostAddress;  Try to use http://www.whatismyip.org/ It's much simpler. Or you want to parse information exactly whatismyip.com? I think they are already arguing on it. you are late.......  This can be achieved way easier using the automation interface from www.whatismyip.com so there's no need for any regex: static void Main(string[] args) { const string url = ""http://www.whatismyip.com/automation/n09230945.asp""; var client = new WebClient(); try { var myIp = client.DownloadString(url); Console.WriteLine(""Your IP: "" + myIp); } catch (Exception ex) { Console.WriteLine(""Error contacting website: "" + ex.Message); } } good one nice and short one  Do it like this instead: class Program { static void Main(string[] args) { string whatIsMyIp = ""http://www.whatismyip.com/automation/n09230945.asp""; WebClient wc = new WebClient(); UTF8Encoding utf8 = new UTF8Encoding(); string requestHtml = """"; try { requestHtml = utf8.GetString(wc.DownloadData(whatIsMyIp)); } catch (WebException we) { // do something with exception Console.Write(we.ToString()); } IPAddress externalIp = null; externalIp = IPAddress.Parse(requestHtml); Console.Write(""IP Numaram:"" + externalIp.ToString()); Console.ReadKey(); } }"
668,A,how to determinate destination MAC address My application is running on CentOS 5.5 I need to send raw packets using libpcap API: pcap_inject() or pcap_sendpacket() To the specific IP address How can I determinate MAC address belongs to a specific target? In general MAC addresses don't matter for remote targets. They are not routable; a router here at my office doesn't know the MAC addresses of network cards across the Internet. That's what IP addresses are for. Do you mean local only? I need to send a raw Ethernet packets in order to do it I need to fill Ethernet header that contains source and destination MACs. I'm looking for a way how can I programmatically ask to get MAC from ARP cache and if my MAC is not there to execute ARP request and return me the result.  It looks like what you want is ioctl and SIOCGARP. That should let you query your arp cache. I'm assuming that the host in question is on your local network or all you're going to get is your router. You can also read from /proc/net/arp which seems easier. You'll need to get an arp request returned first but you'll be doing that whether your tool does it or some third-party makes the request. See `man arping` Sending a datagram might be the easiest way certainly easier than some ioctl type interactions. How can I make an ARP request for host that is not in the ARP cache table? As I understanding I can just ping the host but maybe there is a formal API for ARP request?
669,A,"pcap_findalldevs_ex function is undefined I am trying out an example of obtaining advanced information about installed n/w devices from WinPcap. I have even followed the instructions for including WinPcap library still the compiler complains that pcap_findalldevs_ex is undefined at line if (pcap_findalldevs_ex(source NULL &alldevs errbuf) == -1). My Code : #include ""stdafx.h"" #include <stdio.h> #include ""pcap.h"" #include <winsock2.h> #pragma comment(lib ""ws2_32"") // Function prototypes void ifprint(pcap_if_t *d); char *iptos(u_long in); char* ip6tos(struct sockaddr *sockaddr char *address int addrlen); int _tmain(int argc _TCHAR* argv[]) { pcap_if_t *alldevs; pcap_if_t *d; char errbuf[PCAP_ERRBUF_SIZE+1]; char source[PCAP_ERRBUF_SIZE+1]; printf(""Enter the device you want to list:\n"" ""rpcap:// ==> lists interfaces in the local machine\n"" ""rpcap://hostname:port ==> lists interfaces in a remote machine\n"" "" (rpcapd daemon must be up and running\n"" "" and it must accept 'null' authentication)\n"" ""file://foldername ==> lists all pcap files in the give folder\n\n"" ""Enter your choice: ""); fgets(source PCAP_ERRBUF_SIZE stdin); source[PCAP_ERRBUF_SIZE] = '\0'; /* Retrieve the interfaces list */ if (pcap_findalldevs_ex(source NULL &alldevs errbuf) == -1) { fprintf(stderr""Error in pcap_findalldevs: %s\n""errbuf); exit(1); } /* Scan the list printing every entry */ for(d=alldevs;d;d=d->next) { ifprint(d); } pcap_freealldevs(alldevs); return 1; return 0; } /* Print all the available information on the given interface */ void ifprint(pcap_if_t *d) { //Code removed to reduce length and it contains no errors. } /* From tcptraceroute convert a numeric IP address to a string */ #define IPTOSBUFFERS 12 char *iptos(u_long in) { //Code removed to reduce length } char* ip6tos(struct sockaddr *sockaddr char *address int addrlen) { //Code removed to reduce length } Can some one point me in the right direction? Edit : If I use pcap_findalldevs(&alldevs errbuf) in the above code it builds successfully. So I guess it has no problem linking to the dll. Edit 1 : Error error C3861: 'pcap_findalldevs_ex': identifier not found IntelliSense:identifier ""pcap_findalldevs_ex"" is undefined Thanks. pcap_findalldevs_ex is only present if you define HAVE_REMOTE Add HAVE_REMOTE as a preprocessor definition in project properties or do the following for every include of pcap.h: #define HAVE_REMOTE #include ""pcap.h"" @Searock Ruzario: Which winpcap version do you have? Also it's generally better to copy-paste text (if possible) rather than images @Erik It is version 4.1.2. And sorry for the image I have added the error in text. @Searock Ruzario: See updated answer @Erik You Rock. I have been banging my head for a week to make this program work.Thanks a lot. @Searock Ruzario: Well the bounty helps - it made me spend the time needed to actually go and look it up so I guess the system works as intended :) @Erik I wish I could award the bounty right now but it has a time limit. @Searock Ruzario: No worries hehe. BTW your Q also got me interested in playing with winpcap again been years since I looked at it :) @Erik It still give me the same error `pcap_findalldevs_ex': identifier not found`. Should I post the screenshots of how I have added the WinPcap library in my project ? And the main problem is if I build a sample example which contains `pcap_findalldevs` function it builds successfully but it does not build with `pcap_findalldevs_ex` programs. @Searock Ruzario: See updated answer @Erik I think there's a problem with a function because if I remove `pcap_findalldevs_ex(source NULL &alldevs errbuf)` and replace it with `pcap_findalldevs(&alldevs errbuf)` it builds successfully. But then it won't work as required. @Searock Ruzario: Please post *all errors* from a build - exact @Erik I have update my question. Should I also upload the screen shots of how I added the library to my project?"
670,A,"how to restrict number of connections in client server program I want a server program which should accept only maximum of one connection and it should discard other connections. How can I achieve this? Only accept() a single connection. Here is a typical server routine: s = socket(...); bind(s ...); listen(s backlog); while (-1 != (t = accept(s ...))) { // t is a new peer maybe you push it into an array // or pass it off to some other part of the program } Every completed accept() call returns the file descriptor for a new connection. If you only wish to receive a single connection only accept() once. Presumably you're done listening after this so close your server too: s = socket(...); bind(s ...); listen(s backlog); t = accept(s ...); close(s); // do stuff with t If you wish to only handle a single connection at a time and after that connection closes resume listening then perform the accept() loop above and do accept further connections until t has closed. You don't need any IPC to use the connection in the child. If you wish to communicate that the connection is finished you are better off terminating the child and having the parent process `wait()` on it. You are not using both the server socket and accepted socket simultaneously so there is no need to `fork()`. Also if you do `fork()` the child should close its handle to the server socket and the parent its handle to the accepted socket. Thanks for your valuable comments Actually how can I know whether the first connection is already closed or not? because if my first connection is ideal for certain period of time? if I receive a new connection I have to check whether first connection is active or not before accepting this new connection. Do we have any way to achieve this scenario? When you `read()` from a socket that the peer has closed it will return 0. Also you may close it yourself by calling `close()` or `shutdown()`. Just don't return from your function handling `t` until you are satisfied that you are done with the connection. Keep in mind it is possible to handle more than one connection at a time but I presume that handling a single connection is what you are after. Really awesome answer Matt Jonier here we have one more problem assume the read/write is handled by its child process and connection is handled by its parent process. In this case we need IPC to send data from parent to child process if we actually read some data using read() to check the connection status is this not overhead? Do we have solution for this too? I have one solution. We can use getpeername() to check the status of current active connection would there be any side effect? I am not sure whether this would correct solution? please share your thought.  Do you want to reject all connection or to make a queue? I think that what you are looking for is the so-called ""singleton"". Look at the wikipadia for the Singleton design pattern. I don't think that he will find the Singleton Pattern for C ;)  Corrections see underneath: You can define the amount of accepted requests in the listen method. listen(socketDescription numberOfConnectionsPending); Second parameter is for setting the number of pending connections and not the amount of connections itself.. If you set the numberOfConnections to 1 all the other clients which sends a request to the server will receive a timeout error.. Here you can find more informations: http://shoe.bocks.com/net/#listen I read the listen documentation wrong. You should work with the accept method which is described in Matt's answer. This is incorrect that value is the _backlog_ it's the number of unaccepted connections that may queue. How can I alert the user that already some other connection is active and this time out is not due to network outage? Hmm I actually don't know out of my mind how to solve this problem. You can't send the user this information because the only connection available is used by another one. So you can't send the user this information over the socket. But what you could do is implement this behaviour on the client side. Whenever you receive a ""timeout error"" you display the warning ""Another Connection is active..."" Hope this helps you. And what the accept rate is should be now clear by the comment added at your question.. Is there any way to check whether last opened socket descriptor is active or not? I think you could check this at the accept method. But I am not sure.. accept() returns a positive integer which is the socket descriptor for the accepted socket. If an error occurs -1 is returned and errno is set to indicate the cause. Check this website: http://shoe.bocks.com/net/#accept Thanks for the correction! I editted my answer. Unfortunately I read the documentation of listen wrong..."
671,A,C# Socket server Ports Hey so I'm trying to build a simple lan game using sockets (not tcpclient or tcplistener and yes I know they are the same thing). I managed to get the chat working and now I'm trying to get the game to work. I read somewhere that the best way to manage the game data would be through another port on the server (if anyone can suggest a better way feel free to do so). I'm going to be as straightforward as possible so I will post some code snippets so you can get the general idea. (btw I'm not a c# guru it's my first socket project so please understand my newbiness) the server is initialized like this: sv = new Socket(AddressFamily.InterNetwork SocketType.Dgram ProtocolType.Udp); ip = new IPEndPoint(IPAddress.Parse(b.ToString()) 1000); sv.Bind(ip); (b is my local ip) messages are sent using sv.SendTo(); now on the client I have: svIp = new IPEndPoint(IPAddress.Parse(txtIp.Text.Trim()) 1000); and a thread that listens for incoming data (smt like this): rcv = sv.ReceiveFrom(data ref svIp); From my understanding the client is listening for everything that the server sends to it from the port 1000. I hope I'm right so far because this is how the chat client seems to work. Ok after that I created on the server another socket server which is binded to port 1001: gameIp = new IPEndPoint(IPAddress.Parse(b.ToString()) 1001); now I want to send a message from the new socket server let's call it gameSv so I send a message towards a client using gameSv.SendTo(); that should from what I understand send a message from port 1001 which the client shouldn't be able to receive because he's only listening for data coming from port 1000. Ok good so far(i hope) after this I create another thread which listens for data coming from the server on port 1001. So now when I send a message from port 1001 the thread listening for port 1000 gets it than if I send another message the other thread listening for port 1001 gets it and the other doesn't and so on it goes from one to the other. Any idea why this happens and how to fix it? Thx in advance. Any reason why you're not using WCF? Tbh I didn't knew what WCF is I had searched google seems interesting but I don't have the knowledge to use it yet. Go'ta do more reading I guess. Thx for the advice anyway I will look into it. A Socket can only receive messages targeted to the port the socket is bound to. To do this with the Bind() method. The IPEndPoint which you pass into ReceiveFrom() is only there to save the remote address from which the message was received. It it not to filter from which ports/addresses to receive data. Thanks I guess I'm too tired to think straight I kinda knew that ReceiveFrom uses that EndPoint to store the remote address but I forgot. Anyway thank you for reminding me that :)  It sounds like you are confusing your ports. The port you are using to send a message doesn't matter. The 2nd parameter to SendTo determines which port the message gets sent to. This has to match the port that the client is listening on. Thank you for your answer I understand now where I was going wrong.
672,A,To inetd or not to inetd... when should I use inetd for my network server program? Can anyone give a concise set of real-world considerations that would drive the choice of whether or not to use inetd to manage a program that acts as a network server? (If inetd is used I think it alters the requirements around networking code in the program so I think it's definitely programming-related and not general IT) The question is based around an implementation I've seen that uses a control program managed by inetd to start a network listener that then runs forever and takes constant and heavy load. It didn't seem like a good fit with the textbook inetd usage profile (on-demand infrequently used lightweight) and got me interested in the more general question. Thanks What alternative strategy are you considering? inetd is a good way of ensuring your server starts when the OS boots in the appropriate run level. Even if you do design your server to have some other management mechanism inetd could still wrap all the commands quite simply. It's only shell scripts after all. Well - the alternative strategy would be to run the network program directly I guess started from an rc.d script Actually so maybe Windows/UNIX compatibility is a reason why you would choose inetd seeing mcrute's response just shows me how little I know about inetd. However many apps I've used/developed runs a daemon then scripts are added to inetd to start/stop them. I guess the equivalent in windows is to install as a service.  I think another factor worth considering when deciding to use inetd is how much memory the process handling the request consumes on an average? If this is fairly high then under high load you risk running out of memory (since inetd forks). The same server might be implementable in a multithreaded or select-poll manner possibly allowing for higher load / less memory per connection.  Hooking into inetd will make your service slightly easier to manage from an operational point of view as inetd allows a sysadmin to control practically all of how network communications with your program happens. However it will require you to make a few code changes to your program. Also it may not be as efficient as just making your program run as a daemon to begin with. EDIT: I personally never use inetd and always opt to write server processes as standalone daemons.  It depends on the usage pattern for your service. If the startup time for your daemon is low and you expect it to be used infrequently then inted might be a good fit. It reduces or even eliminates the need to write any additional networking code. If your daemon is more heavyweight or more frequently used you're probably better off writing it standalone. You can just as easily write an init.d script and some conf.d configuration to go with it and it will be no harder for an admin to manage. Most programming languages these days have easy to use socket libraries so in many cases the networking code may not even be that difficult. I've found in my experience that few admins these days are familiar with inetd. Most daemons just provide their own init script. In fact of the few hundred systems which I manage I can't think of a single one that launches anything through inetd at all. That's something worth considering.
673,A,"localhost vs LAN : speed difference? I'm currently in the process of performance profiling. We have a basic client/server application. Would the TCP transfer speed be different if I ran client/server on the same machine (localhost) vs across two computers on a LAN? The transfer times would almost certainly be faster if the client and server were on the same machine. That may not actually matter to the performance of your program as a whole depending on the other resources consumed by the client and server.  I don't know if it measurable (that also depends on your LAN's speed) but from a logical point of view of course there is a difference. Localhost will always be the fastest as the data has not be sent through another medium (like air or copper wire). But depending on what your application does this might or might not matter.  TCP transfer speed will be! because if you ran it on same computer it will forward packets locally without even touching LAN and network adapter. But overall speed of client+server may be better on different machines especially if you do not communicate with server too often. @Andrey: This is not necessarily true. Local connections are still passed through the adapter using the loopback if the application dictates that it use the network layer for transfers. please explain how can application dictate to use network?  It really depends on what your application does.... As an example: If it transfers 10GB files from client to server then yes it will make a difference.  I just hit this issue on a project at work. Using UDP with localhost is at least an order of magnitude faster than over a network connection (maybe two orders of magnitude) and I believe with localhost there is no MTU ceiling of 1500 as normally exists for network ports. One unconfirmed suspicion is the built in network ports on PCs are not all the same quality so even if they claim to be gigabit you may not be able to really go that fast. But it may also be making lots of Windows system calls (one OS call per packet) may be a significant overhead. With TCP I can hand the OS a large chunk of data to write in a single call. With UDP I have to hand it a packet at a time limited by the MTU size resulting in a much larger number of OS calls. But unconfirmed as yet. Have not tried Linux yet.  Yes definitely the latency of sending it across the network would slow the program down. The throughput wouldn't but if you're waiting for replies before sending data then this builds up because of extra latency. For localhost I get ping times of 0.024ms for pinging my router I get times of 0.339ms LnDCobra this is actually a great answer. ""If you're waiting for replies""... this happens when you talk to somebody on skype. The whole conversation slows down due to a few ms of latency. Yes if you think about the difference between 0.024 and 0.339 is massive according to those timings localhost is about 15x faster than LAN.  When using localhost local resources are more likely to be the performance bottleneck because of memory disk cpu etc. When using two computers its more likely the network will be the bottleneck because of latency bandwidth throughput packet loss etc. It depends on what your application does and how it uses the network client and server."
674,A,Unit Testing Network Connection I am doing a unit testing and I need to find a way to simulate a disconnection during DB transactions without actually or physically removing the network cable..The process must be automated since this is just part of a bigger scale of unit testing. I need to check if roll backs are handled accordingly I am using C# for this project What language you are using? You could stop the DB server process. If you let us know which DB platform I am sure one of us will have a shell command for you that you could execute within your unit test. Kindness Dan Forgot to mention that this is just part of the whole process of unit testing and the process must be automated  If you are using C# or .Net you can use Typemock to fake the connection and return the disconnection effect.  I just need this network to be disabled. What I did was call through C# a DOS command NETSH assigning a non-existing gateway to my test pc thus disabling or cutting the network connection execute the test case and then setting it back to the original to enable again the connection. Thanks for the responses.. :D  You could mock the Connection and throw an exception when you need connection to disconnect. You can't test a real DB connection in a unit test - because it ceases to be a unit test. +1 for mocks!  write a proxy which will connect you to the DB for example in python: http://code.activestate.com/recipes/483730/ then just kill the proxy to end to connection
675,A,Network error handling in silverlight 3 We are writting silverlight3 application which use Tcp connction. Our project use client server architecture. Client part is silverlight application and server part is winform application. We use TcpClient and TcpListner for connection. We can handle following errors: 1. user shut down 2. users network cable unpluging 3. server shut down But we can't handle servers network cable unpluging on user level(for server we handle it with NetworkChange.NetworkAvailabilityChanged event). How we can do it? And is there other network which we must handle for application correct work? Thanks. We added ping to our application and with it can handle all network problems.
676,A,"What might cause an infinite loop error I am working on a network programming and I have this code void WorkHandler::workLoop(){ . . . while(1){ if(remainLength >= MAX_LENGTH) currentSentLength = send(client->getFd() sBuffer MAX_LENGTH MSG_NOSIGNAL); else currentSentLength = send(client->getFd() sBuffer remainLengthMSG_NOSIGNAL); if(currentSentLength == -1){ log(""WorkHandler::workLoop connection has been lost \n""); break; } sBuffer += currentSentLength; remainLength -= currentSentLength; if(remainLength == 0) break; } } Also I am creating a child thread like this bool WorkHandler::initThreads(){ for(int i=0; i < m_maxThreads; i++){ pthread_t *thread(new pthread_t); m_workThreadList.push_back(thread); if(pthread_create(thread NULL runWorkThread reinterpret_cast<void *>(this))!=0){ log(""WorkHandler::initThreads pthread_create error \n""); return false; } pthread_detach(*thread); } return true; } void* WorkHandler::runWorkThread(void *delegate){ printf(""WorkHandler::runWorkThread called\n""); WorkHandler *ptr = reinterpret_cast<WorkHandler*>(delegate); ptr->workLoop(); return NULL; } I am running this code on gdb and it doesn't blow up but it gets stuck at the second send function in the if then else loop. I put log statements every single line and it prints a log right above the second send function and stopped. currentSentLength = send(client->getFd() sBuffer remainLength MSG_NOSIGNAL); What might cause this problem and how do I fix this issue? Thanks in advance.. I doubt it's your main problem but that while should really say `while (remainLength > 0)`. I shudder whenever I see `while(true)` or equivalent. Although it might be related to remainlength going negative for some reason; you're only breaking if it's ==0. check that the client(s) read the data I guess this is too trivial but the while loop will get stuck if currentSentLength is 0 or negative or if remainLength gets negative. Are `remainLength` and `MAX_LENGTH` of same type ? (signed or unsigned). Often the comparison happens between signed and unsigned and it results either always true or always false. Is there anyone who can modify the send part of my code? Thanks in advance.. With blocking IO send will block if the kernel buffer is full and will block untill the clients have read the data. Do you send large chunks? If so check your client. If you don't trust clients (they can abuse this to do denial of service attacks) there are a couple of ways to do this properly: poll (with timeout) on the sockets for writeability send with timeout use nonblocking I/O ...  I guess you're calling send() with a negative size... Your test to exit the while should be remainLength <= 0 and not remainLength == 0 why would it be negative? @yi_H: Because `remainLength -= currentSentLength;` could become negative depending on what `send()` returns. nope `send` cannot send more than what you've requested. It does not fix this problem. I changed the condition of while loop to ""remainLength > 0"" but it still gets stuck at the second send function...How do I fix this? humm... try replacing the send() with your own function that just traces its parameters... this way you'll be able to pinpoint where the problem is either inside send() or doing the loop... Maybe this can also help you (http://linux.die.net/man/2/send): When the message does not fit into the send buffer of the socket send() normally blocks unless the socket has been placed in non-blocking I/O mode. In non-blocking mode it would return EAGAIN in this case. The select(2) call may be used to determine when it is possible to send more data. So according to my previous comment check that the send buffer is not full and that the socked is ok to send data..."
677,A,"arp -a and route print I need to write a program that displays this information: netstat TCP / UDP connections Information about the IP ipconfig /all arp-a route print I already have most of them but I have a problem with the route print and arp -a. I do not want to execute this command using Process.Start() because it does not look too spectacular: Process p = new Process (); p.StartInfo.WindowStyle = ProcessWindowStyle.Hidden; p.StartInfo.UseShellExecute = false; p.StartInfo.FileName = ""route""; p.StartInfo.Arguments = ""PRINT""; p.StartInfo.RedirectStandardOutput = true; p.StartInfo.CreateNoWindow = true; p.StartInfo.StandardOutputEncoding = Encoding.Default; p.StartInfo.WindowStyle = ProcessWindowStyle.Hidden; p.Start(); TextBox1.Text = p.StandardOutput.ReadToEnd(); I would like to use a foreach loop to get the data into ListView or DataGrid columns. Has anyone ~ t be able to help me? How can I get this data into each column: Destination Netmask gateway interface metrics and permanent route? And in the case of ARP Internet address type of physical address? Sorry just saw that I forgot about the question about ""arp -a"". Take a look at the [GetIpNetTable2 Function](http://msdn.microsoft.com/en-us/library/aa814420(v=vs.85).aspx) fairly sure that that might be what you're looking for. Though not sure how easy it is to call from C# might be some easier WMI way but I've never looked at it. Imparted Thank you very much. I have already written with WMI CODER CREATOR who have directed and IPv4routetable the following code: private void button1_Click(object sender EventArgs e) { try { ManagementObjectSearcher searcher = new ManagementObjectSearcher(""root\\CIMV2"" ""SELECT * FROM Win32_IP4RouteTable""); ListViewItem buf; foreach (ManagementObject queryObj in searcher.Get()) { string destination = queryObj[""Destination""].ToString(); string mask = queryObj[""Mask""].ToString(); string metric = queryObj[""Metric1""].ToString(); string interfaceIndex = queryObj[""InterfaceIndex""].ToString(); string nexthop = queryObj[""NextHop""].ToString(); string protocol =queryObj[""Protocol""].ToString(); string type = queryObj[""Type""].ToString(); string status; if (queryObj[""Status""]!=null) { status = queryObj[""Status""].ToString(); } else { status = string.Empty; } buf = new ListViewItem(new string[] {destinationmaskmetricinterfaceIndexnexthopprotocolstatustyp}); list_route.Items.Add(buf); } } catch (ManagementException ex) { MessageBox.Show(""An error occurred while querying for WMI data: "" + ex.Message); } } Just do not know in which class I find information on arp-a can not find on google. If someone knew he was asking for the answer. If anyone has other useful aids such as WMI Coder Creator was grateful. I found some information. About GetIpNetTable but I can not use this function in GUI applications to pass the result to the listview. : ( using System; using System.Runtime.InteropServices; using System.ComponentModel; using System.Net; namespace GetIpNetTable { class Program { / / The max number of physical addresses. const int MAXLEN_PHYSADDR = 8; / / Define the MIB_IPNETROW structure. [StructLayout (LayoutKind.Sequential)] struct MIB_IPNETROW { [MarshalAs (UnmanagedType.U4)] public int dwIndex; [MarshalAs (UnmanagedType.U4)] public int dwPhysAddrLen; [MarshalAs (UnmanagedType.U1)] public byte mac0; [MarshalAs (UnmanagedType.U1)] public byte mac1; [MarshalAs (UnmanagedType.U1)] public byte mac2; [MarshalAs (UnmanagedType.U1)] public byte mac3; [MarshalAs (UnmanagedType.U1)] public byte mac4; [MarshalAs (UnmanagedType.U1)] public byte mac5; [MarshalAs (UnmanagedType.U1)] public byte mac6; [MarshalAs (UnmanagedType.U1)] public byte mac7; [MarshalAs (UnmanagedType.U4)] public int dwAddr; [MarshalAs (UnmanagedType.U4)] public int dwType; } / / Declare the GetIpNetTable function. [DllImport (""Iphlpapi.dll"")] [Return: MarshalAs (UnmanagedType.U4)] static extern int GetIpNetTable ( IntPtr pIpNetTable [MarshalAs (UnmanagedType.U4)] pdwSize ref int bool border); / / The Insufficient buffer error. const int ERROR_INSUFFICIENT_BUFFER = 122; static void Main (string [] args) { / / The number of bytes needed. bytesNeeded int = 0; / / The result from the API call. int result = GetIpNetTable (IntPtr.Zero ref bytesNeeded false); / / Call the function expecting an Insufficient buffer. if (result! = ERROR_INSUFFICIENT_BUFFER) { / / Throw an exception. throw new Win32Exception (result); } / / Allocate the memory do it in a try / finally block to ensure code / / That it is released. IntPtr buffer = IntPtr.Zero; / / Try / finally. try { / / Allocate the memory. buffer = Marshal.AllocCoTaskMem (bytesNeeded); / / Make the call again. If it did not Succeed then / / Raise an error. result = GetIpNetTable (buffer ref bytesNeeded false); / / If the result is not 0 (no error) then throw an exception. if (result! = 0) { / / Throw an exception. throw new Win32Exception (result); } / / Now we have the buffer the have to marshal it. We can read / / The first 4 bytes to get the length of the buffer. int entries = Marshal.ReadInt32 (buffer); / / Increment the memory pointer by the size of the int. IntPtr = new IntPtr currentBuffer (buffer.ToInt64 () + Marshal.SizeOf (typeof (int))); / / Allocate an array of entries. MIB_IPNETROW [] table = new MIB_IPNETROW [entries] / / Cycle through the entries. for (int index = 0; index <entries; index + +) { / / Call PtrToStructure getting the information structure. table [index] = (MIB_IPNETROW) Marshal.PtrToStructure (new IntPtr (currentBuffer.ToInt64 () + (index * Marshal.SizeOf (typeof (MIB_IPNETROW )))) typeof (MIB_IPNETROW)); } for (int index = 0; index <entries; index + +) { IPAddress ip = new IPAddress (table [index]. DwAddr); Console.Write (""IP:"" + ip.ToString () + ""\ t \ TMAC""); byte b; b = table [index]. mac0; if (b <0x10) { Console.Write (""0""); } else { Console.Write (""""); } Console.Write (b.ToString (""X"")); b = table [index]. mac1; if (b <0x10) { Console.Write (""-0""); } else { Console.Write (""-""); } Console.Write (b.ToString (""X"")); b = table [index]. mac2; if (b <0x10) { Console.Write (""-0""); } else { Console.Write (""-""); } Console.Write (b.ToString (""X"")); b = table [index]. mac3; if (b <0x10) { Console.Write (""-0""); } else { Console.Write (""-""); } Console.Write (b.ToString (""X"")); b = table [index]. mac4; if (b <0x10) { Console.Write (""-0""); } else { Console.Write (""-""); } Console.Write (b.ToString (""X"")); b = table [index]. mac5; if (b <0x10) { Console.Write (""-0""); } else { Console.Write (""-""); } Console.Write (b.ToString (""X"")); Console.WriteLine (); } } finally { / / Release the elephant. Marshal.FreeCoTaskMem (buffer); } } } }  I suppose you could probably just parse the text you retrieve to get out the values you want and put them in the ListView etc. However I think that there are better ways to retrieve this and I think you'd be able to get out this information using the WMI class Win32_IP4RouteTable. If you haven't used WMI before you might be able to use the WMI Code Creator v1.0 to help you get started (I haven't used it myself but I saw someone else suggesting it sometime). The WMI .NET Overview would probably be useful too. thanks for your help I can not use GetIPNetTable :("
678,A,"Tell whether a text string is an IPv6 address or IPv4 address using standard C sockets API I have a program where an external component passes me a string which contains an IP address. I then need to turn it into a URI. For IPv4 this is easy; I prepend http:// and append /. However for IPv6 I need to also surround it in brackets []. Is there a standard sockets API call to determine the address family of the address? Considered this; decided it would be too hacky and might not extend if we want to support something other than IPv4 and IPv6 (not very likely but in my experience I would greatly increase its likelihood by writing code that doesn't support it). I know that it's not part of the sockets API but if IP4 and IP6 are the only possibilities can't you do this with `strlen`? ;-) @Steve Jessop consider e.g. ff02::2 and 8.8.8.8 @nos: fair enough didn't realise the former was a valid address. More generally is there some way to examine the strings that's simpler code than using the sockets API? In this case the structure containing the parsed address isn't needed. If you know that this string is either an ipv4 or ipv6 address looking for . or : might be enough. Parsing an ipv4 address is rather trivial depending on the level of validation you need an ipv6 address is quite a bit worse. Use getaddrinfo() and set the hint flag AI_NUMERICHOST family to AF_UNSPEC upon successfull return from getaddrinfo the resulting struct addrinfo .ai_family member will be either AF_INET or AF_INET6. EDIT small example #include <sys/types.h> #include <stdio.h> #include <string.h> #include <sys/socket.h> #include <netdb.h> int main(int argc char *argv[]) { struct addrinfo hint *res = NULL; int ret; memset(&hint '\0' sizeof hint); hint.ai_family = PF_UNSPEC; hint.ai_flags = AI_NUMERICHOST; ret = getaddrinfo(argv[1] NULL &hint &res); if (ret) { puts(""Invalid address""); puts(gai_strerror(ret)); return 1; } if(res->ai_family == AF_INET) { printf(""%s is an ipv4 address\n""argv[1]); } else if (res->ai_family == AF_INET6) { printf(""%s is an ipv6 address\n""argv[1]); } else { printf(""%s is an is unknown address format %d\n""argv[1]res->ai_family); } freeaddrinfo(res); return 0; } $ ./a.out 127.0.0.1 127.0.0.1 is an ipv4 address $ ./a.out ff01::01 ff01::01 is an ipv6 address  Here's a simple function to do it: http://tdistler.com/2011/02/25/how-to-test-if-an-address-is-ipv4-or-ipv6 It will also handle hostname strings and return the address type of the first DNS entry returned. Set AI_NUMERICHOST to disable the hostname stuff.  Kind of. You could use inet_pton() to try parsing the string first as an IPv4 (AF_INET) then IPv6 (AF_INET6). The return code will let you know if the function succeeded and the string thus contains an address of the attempted type. getaddrinfo() has bunch of overhead but basically also calls inet_pton(). inet_pton() is really all you need and IMHO the best answer to this question. It also doesn't require a cleanup function."
679,A,"Obfuscation or hiding of server to client state updates I'm not actually writing this software myself but it occurred to me that I have no idea how to solve the problem. As the best way to explain the problem I'll describe a specific scenario from a hypothetical multi-player first-person shooter game... Player A is hiding in some bushes facing west Player B is sneaking up on player A from the east or sneaking up behind player A A popular ""hack"" written for this game would be for player A to have a radar showing him the location of player B even though he can't see player B on his screen and the game does not support a radar. This hack is possible because the server is sending info to player A's client on all players within a certain range (perhaps within player A's clipping plane). It would not be realistic (as far as I know) for the server to attempt to only send info to player A's client on players within player A's view frame. Because the server must send info on all nearby players to player A's client player A could write a hack that overlays a radar on his screen which is populated by watching for data sent to the client and pulling out enemy player state updates. I think these are commonly known as ""radar"" or ""wall"" hacks. Is there any way to obfuscate or hide state updates of enemy players in the information sent to the client? From what I understand encryption wouldn't be viable for real time solutions? Even if the server was able to only send state updates on players within player A's view frame this would still allow player A's hack to show players hiding behind camouflage or cover objects (which were presumably transparent in a minor way). The only thing I could really think of would be to implement some type of ""punkbuster"" solution. That is have player A's client regularly scan for illegal processes. The idea being that any popular hacks would be monitored. Unpopular hacks would affect a small enough player base that they're too small to go after. That's the only way. In addition to punkbuster you should do some research on Warden (beyond wikipedia there lots of interesting stuff that I won't link here). It's a very interesting battle given the laws of online world design ... The client is in the hands of the enemy. Another element to it is that people will replace their video drivers with modded drivers that give them advantages (things are opaque or highlighted players can't be ""blinded"" etc). I'm not sure if any games check for modded drivers or not. warden owned me once :> hehehe imho if you're not cheating you're not trying! :D Any idea if Blizzard licenses the use of Warden for other games? Talk about a license to print money (in addition to WoW).  Simple encryption might be viable for real-time with today's hardware but the problem is that in order for the client to access the encrypted data it must have the key. If the client has the key it will probably not be all too difficult for a cheater program to find and retrieve it. The best way is for the server only to send B's position to A if it's at all possible to see B. It will still be possible to cheat but to a somewhat lesser extent. A possible solution for the cases when B is hiding but a small part may be visible is for the server to simply say that there's something at B's position. It may be a rock or whatnot. The problem is that in order for the client to be able to render the something the server has to give a fairly accurate description. Given that today's multiplayer games fake low latency by simulating what the server does this is most likely not feasible until we have ultra-low latency networks.  There was a program called ""eqmon"" that did something like this for the game Everquest. At first the game packets were completely unencrypted and contained all characters (both PC and NPC) for the entire level. Eventually they ended up using encryption to make it harder to snoop the packets but eqmon got around that by brute forcing the encryption key (a new key was created by the server every time you entered a level so it sometimes took a few minutes to decrypt the key). They then changed it so that your client only received updates for PCs or NPCs within a certain range of your location. This made eqmon less helpful for camping and monitoring ""rare spawns"" far away in the same level."
680,A,"What host do I have to bind a listening socket to? I used python's socket module and tried to open a listening socket using import socket import sys def getServerSocket(host port): for r in socket.getaddrinfo(host port socket.AF_UNSPEC socket.SOCK_STREAM 0 socket.AI_PASSIVE): af socktype proto canonname sa = r try: s = socket.socket(af socktype proto) except socket.error msg: s = None continue try: s.bind(sa) s.listen(1) except socket.error msg: s.close() s = None continue break if s is None: print 'could not open socket' sys.exit(1) return s Where host was None and port was 15000. The program would then accept connections but only from connections on the same machine. What do I have to do to accept connections from the internet? The first problem is that your except blocks are swallowing errors with no reports being made. The second problem is that you are trying to bind to a specific interface rather than INADDR_ANY. You are probably binding to ""localhost"" which will only accept connections from the local machine. INADDR_ANY is also known as the constant 0x00000000 but the macro is more meaningful. Assuming you are using IPv4 (that is ""regular internet"" these days) copy the socket/bind code from the first example on the socket module page. http://docs.python.org/library/socket.html#socket.getaddrinfo doesn't say anything about an interface. Would socket.INADDR_ANY replace socket.AF_UNSPEC or socket.SOCK_STREAM? The interface is implied by the address you bind to in your case that would be the variable `sa`. Quoth that manual page: ""For IPv4 addresses two special forms are accepted instead of a host address: the empty string represents INADDR_ANY..."". The call to `getaddrinfo` will return a list of addresses none of which are INADDR_ANY.  Try 0.0.0.0. That's what's mostly used.  You'll need to bind to your public IP address or ""all"" addresses (usually denoted by ""0.0.0.0""). If you're trying to access it across a network you also might need to take into account firewalls etc. How would I do that? I know that both computers are behind different wireless routers. Difficult to answer. Try it first (it might not be a problem) but you'll need to find how to configure each of the devices involved. Usually only the ""server"" side will need any special attention as outgoing packets aren't usually filtered by default. But serverfault might be a better place to get answers on that."
681,A,Observing particular Port for measuring network game performance How can i check/inspect the performance of some network application via observing the port it usedi want to test the performance of my network game. is it some way or is it some practice like this? If my game is connect to port 1009 how can i check the different parameters data outflow  inflow and other metrics on this port. any ideas thanks Jibbylala You could ask this over on http://gamedev.stackexchange.com/ but you'd need to be much more specific about the metrics you want to capture and why. NO I don't think the question need any more information as you mentioned and didn't get the idea to take that as non software development question. Sounds like you just need a Packet Sniffer
682,A,"libCURL or Boost for network programming? I'm starting to learn network programming in C++ and since the standard library doesn't support networking I have to resort to one of these. Which do you think is simpler and thus easier to learn? You do not need to use boost or libcurl; the ""standard library"" most certainly does support networking. After all if it didn't how would libcurl or boost do their jobs? The C++ standard library does not support networking. They do different things. If you're looking for HTTP FTP and support for a slew of other protocols then use libcurl. If you want to directly use sockets and implement all your own protocols then use Boost Asio. A third choice would be to use the Pion Network Library which is built on top of Asio. It only supports HTTP though and for clients is more of a building block than a ready-made solution."
683,A,Simple auto updater for OpenVPN config files We have deployed several OpenVPN servers globally and our 8k+ Windows users have OpenVPN GUI installed. Each server is defined in a config file in the program files directory. As we add new servers to the network (we require almost a dozen globally) we would like a slick solution to auto update the config directory with the new config file/s. Either something which we can incorporate into the open source Win32 C code that OpenVPN GUI is written in or a simple executable that the user can run separately to check if there are any new config files and if so download them to the relevant directory. Any ideas greatly appreciated. Not at all clear to me what your programming-related question is. http download with wget? better to integrate in the gui maybe using libcurl
684,A,"What's a portable way of converting Byte-Order of strings in C I am trying to write server that will communicate with any standard client that can make socket connections (e.g. telnet client) It started out as an echo server which of course did not need to worry about network byte ordering. I am familiar with ntohs ntohl htons htonl functions. These would be great by themselves if I were transfering either 16 or 32-bit ints or if the characters in the string being sent were multiples of 2 or 4 bytes. I'd like create a function that operates on strings such as: str_ntoh(char* net_str char* host_str int len) { uint32_t* netp hostp; netp = (uint32_t*)&net_str; for(i=0; i < len/4; i++){ hostp[i] = ntoh(netp[i]); } } Or something similar. The above thing assumes that the wordsize is 32-bits. We can't be sure that the wordsize on the sending machine is not 16-bits or 64-bits right? For client programs such as telnet they must be using hton* before they send and ntoh* after they receive data correct? EDIT: For the people that thing because 1-char is a byte that endian-ness doesn't matter: int main(void) { uint32_t a = 0x01020304; char* c = (char*)&a; printf(""%x %x %x %x\n"" c[0] c[1] c[2] c[3]); } Run this snippet of code. The output for me is as follows: $ ./a.out 4 3 2 1 Those on powerPC chipsets should get '1 2 3 4' but those of us on intel chipset should see what I got above for the most part. I assume it depends on the size of each character. You don't need to concern yourself about it if only one byte is used per character. Related: http://stackoverflow.com/questions/526030/byte-order-with-a-large-array-of-characters-in-c http://stackoverflow.com/questions/1568057/ascii-strings-and-endianness I upvoted this. It really is a good question and well explained even if it's based on a bit of a misconception and even if the author seems overly skeptical. But on the other hand answers have been wrong before so it is probably good to be a bit skeptical. Thanks James gonna fix that right now. If you cast a uint32_t* into a char* then yes the order matters. What ""we who think endianness doesn't matter for char*"" are saying is if you declare an array of chars and work only with chars and never larger types you don't have to swap it's only multi-byte integers for which it matters (uint32_t is a multi-byte integer so that's why your example behaves the way it does). That's the point. There is more than one char more than one byte. So byte ordering matters. If your argument is that byte order doesn't matter for THIS application I beg to differ. Have you done a at least an echo client or server in C? Your example code does not use a string it uses the memory representation of a `uint32_t`. There's a difference. If you instead do `char *c = ""foo"";` then the output is 66 6f 6f 0. No matter what the endian-ness of the machine. I haven't written an echo client/server but I have written an HTTP client and server in C. You do not need to byte-swap strings. @Derrick: Stop being insulting. Instead why don't you try it? char foo[] = ""Hello World""; write(fd foo sizeof(foo)); -- Do the write() on an Intel machine and the read() on a PowerPC machine and witness how it works. Otherwise I suggest you re-read some of the comments here. @asveikau My apologies if it came off that way I don't want to seem insulting. I do realize that everyone is obviously here to help. I'd love to do that but I have no access to a PowerPC machine. @Steve please note that using a (char*) to point to a uint32_t does not change what's inside of memory. I've read them and they all say don't worry about it. At this point for my assignment I will ignore this byte-order. I'm sure I won't run into any problems by following this advice but because all of it is on intel machines. @Steve the ""hae you written an echo server"" question was to see if people had actually had experience with it. So writing an http server means you definitely do. I still am curious if anyone here has been able to test sending and recieving char arrays w/o conversion between different endian based systems. @Derrick: a sample server: http://stackoverflow.com. It returns strings and different clients don't need to do anything special to detect what endianness the server had. @Derrick: If you examine what your reasoning implies then you will (incorrectly) come to the conclusion that LE machines deal with strings that are in the reverse order of strings that BE machines deal with. This is obviously wrong. You may want to read the ""Misconceptions"" section at: http://www.cs.umd.edu/class/sum2003/cmsc311/Notes/Data/endian.html. By the way your arrogance is hilarious. It seems to me that the function prototype doesn't match its behavior. You're passing in a char * but you're then casting it to uint32_t *. And looking more closely you're casting the address of the pointer rather than the contents so I'm concerned that you'll get unexpected results. Perhaps the following would work better: arr_ntoh(uint32_t* netp uint32_t* hostp int len) { for(i=0; i < len; i++) hostp[i] = ntoh(netp[i]); } I'm basing this on the assumption that what you've really got is an array of uint32_t and you want to run ntoh() on all of them. I hope this is helpful.  With your function signature as posted you don't have to worry about byte order. It accepts a char* that can only handle 8-bit characters. With one byte per character you cannot have a byte order problem. You'd only run into a byte order problem if you send Unicode either in UTF16 or UTF32 encoding. And the endian-ness of the sending machine doesn't match the one of the receiving machine. The simple solution for that is to use UTF8 encoding. Which is what most text is sent as across networks. Being byte oriented it doesn't have a byte order issue either. Or you could send a BOM.  If you'd like to send them as an 8-bit encoding (the fact that you're using char implies this is what you want) there's no need to byte swap. However for the unrelated issue of non-ASCII characters so that the same character > 127 appears the same on both ends of the connection I would suggest that you send the data in something like UTF-8 which can represent all unicode characters and can be safely treated as ASCII strings. The way to get UTF-8 text based on the default encoding varies by the platform and set of libraries you're using. If you're sending 16-bit or 32-bit encoding... You can include one character with the byte order mark which the other end can use to determine the endianness of the character. Or you can assume network byte order and use htons() or htonl() as you suggest. But if you'd like to use char please see the previous paragraph. :-) Should be noted that UTF-8 isn't affected by byte order. UTF-16 and UTF-32 are.  Maybe I'm missing something here but are you sending strings that is sequences of characters? Then you don't need to worry about byte order. That is only for the bit pattern in integers. The characters in a string are always in the ""right"" order. EDIT: Derrick to address your code example I've run the following (slightly expanded) version of your program on an Intel i7 (little-endian) and on an old Sun Sparc (big-endian) #include <stdio.h> #include <stdint.h> int main(void) { uint32_t a = 0x01020304; char* c = (char*)&a; char d[] = { 1 2 3 4 }; printf(""The integer: %x %x %x %x\n"" c[0] c[1] c[2] c[3]); printf(""The string: %x %x %x %x\n"" d[0] d[1] d[2] d[3]); return 0; } As you can see I've added a real char array to your print-out of an integer. The output from the little-endian Intel i7: The integer: 4 3 2 1 The string: 1 2 3 4 And the output from the big-endian Sun: The integer: 1 2 3 4 The string: 1 2 3 4 Your multi-byte integer is indeed stored in different byte order on the two machines but the characters in the char array have the same order. strings are sequences of characters yes. Sending this data over the network between two computers of the same endian-ness would not matter. however if you did not do any byte-order conversions and did something like char* str = ""abcd""; and sent it on a little endian machine then received on a big-endian when you addressed str[0] it would be d and not a. http://stackoverflow.com/questions/526030/byte-order-with-a-large-array-of-characters-in-c @Derrick: No that's wrong. With strings the first character will always be in the first position and so on. It's not like multi-byte integers. @Derrick: To illustrate Thomas's point... Let's say you had an array of integers { 0xaabb 0xccdd }. Take this to a different endianness and the order of bytes within the integer get warped into 0xbbaa 0xddcc. However the order of the integers within the array doesn't change. So it's { 0xbbaa 0xddcc } and not { 0xddcc 0xbbaa }. Now imagine these are 8 bit integers instead of 16. If you had an array {0xaa 0xbb 0xcc 0xdd} within an array element (0xaa) there are no bytes to swap it is a single byte. And you wouldn't swap the individual bytes because that changes the order of the array. really? Run this: int main(void) { uint32_t a = 0x01020304; char* c = (char*)&a; printf(""%x %x %x %x\n"" c[0] c[1] c[2] c[3]); } When I run it I get 4 3 2 1 @Derek - That's a 32-bit integer not an array of bytes. If you declare: char a[] = {1234}; it will always be in the same order."
685,A,"local IP after gethostbyname() How do I get the local IP address before using sendto() that is going to be used for the transmission to a remote host? If I understand correctly it should be known already after gethostbyname(remoteHostname) call. I need this for including the local IP in the transmitted packet (127.0.0.1 wouldn't make sense). UDP is used. Why do you need your local IP ? the IP stack of your operating system will insert the local and correct source address. You ususally need the remote IP address  so you know which address to send the packet to. It is a protocol requirement. It is like agent IP in an SNMP trap. Info grabbed from: http://www.suite101.com/content/socket-programming-gethostbyname-a19557 As you said localhost doesn't work. Instead look up your hostname first and pass that to gethostbyname. char hostName[255]; gethostname(hostName 255); struct hostent* host_entry; host_entry=gethostbyname(hostName); char* localIP; szLocalIP = inet_ntoa (*(struct in_addr*)*host_entry->h_addr_list); Should this not be suitable you can also do a DNS lookup do a WMI query or look it up in the registry. How do I know the IP found such way is the one that is going to be used for transmission to a given remote IP (that is resolved by gethostbyname)?  You can call connect() on the socket first. UDP ofcourse is not connection oriented but connect will just register the socket to only send to the given remote IP address so you can e.g. just use send() instead of sendto() However this allows you to call getsockname() on the socket and at least under Linux this will fetch the local address the socket uses. Here's an example you can extract the info in the print_local_addr() here and use that info in your SNMP trap. (the bind() call is only needed if you want a fixed source port too) #include <arpa/inet.h> #include <netinet/in.h> #include <stdio.h> #include <string.h> #include <sys/types.h> #include <sys/socket.h> #include <unistd.h> #include <stdlib.h> void diep(char *s) { perror(s); exit(1); } void print_local_addr(int s) { struct sockaddr_in my_addr; socklen_t len = sizeof my_addr; if(getsockname(s(struct sockaddr*)&my_addr&len) == -1) diep(""getsockname""); //print the local address of the socket that will be used when //sending datagrams to the ""connected"" peer. printf(""local addr %s:%u\n""inet_ntoa(my_addr.sin_addr)ntohs(my_addr.sin_port)); } int main(void) { struct sockaddr_in si_me si_other; int s; if ((s=socket(AF_INET SOCK_DGRAM IPPROTO_UDP))==-1) diep(""socket""); memset(&si_me 0 sizeof(si_me)); memset(&si_other 0 sizeof(si_me)); //bind to local port 4567 si_me.sin_family = AF_INET; si_me.sin_port = htons(4567); si_me.sin_addr.s_addr = htonl(INADDR_ANY); //""connect"" google's DNS server at 8.8.8.8  port 4567 si_other.sin_family = AF_INET; si_other.sin_port = htons(4567); si_other.sin_addr.s_addr = inet_addr(""8.8.8.8""); if(connect(s(struct sockaddr*)&si_othersizeof si_other) == -1) diep(""connect""); print_local_addr(s); close(s); return 0; } Excellent little program. Thanks. Thanks for the code!  Try calling getsockname(2) on a connected UDP socket; the stack would need to perform a fair amount of routing to connect the socket sufficient to know which local address would be used once packets are sent or received. @Noros That's ok you can just call connect() anyway before you send that first message. For UDP connect() is just internal to the local machine/your process. No messages are exchanged with the remote party. (but in the case you need to send to many different remote addresses you then have to create a new socket and connect it for each remote address - unless you absolutely know your machine will only ever have 1 IP address) I hoped that the routing done to get the local IP by gethostname was enough. The message itself is trap like and therefore the socket is not ""connected"" before transmission (as it might be the very first transmission)."
686,A,How to prevent network ports being left open on program crash I took Computer Networking last semester and did some C programming in linux (using gcc) for my projects. One extremely tedious thing I kept running into was if my program crashed or stalled (which I then would have to hit Ctrl+C to kill it) the network port would still be left open for a minute or so. So if I wanted to immediately run the program again I would have to first go into the header file change the port remake the program and then finally run it. Obviously this gets very tedious very fast. Is there any way to configure it where the port is immediately released as soon as the process is killed? Either via some setting in linux or in the makefile for my program or even programmatically in C? Edit: I'm referring to when writing a server and choosing a specific port to host the program. You can sidestep the problem by choosing a random port and printing it out on execution. @Tordek Good point but I'm just going to note here for posterity that if you do that you have to choose from a safe range to ensure you don't stumble upon any standard ports Well yeah... sidestepping carelessly can make you step on dog poo... I bet it's about two minutes :) As @Cogsy noted the SO_REUSEADDR socket option is your friend. Make yourself familiar with TCP states it's TIME_WAIT state that causes you problems:   Aww I would have been much happier with a copy of the amazing ASCII diagram from page 23 of RFC 793 instead. Oh yeh! I totally forgot about wonderful art of ASCII pictures :)  Set the the option SO_REUSEADDR on the socket. int yes = 1; setsockopt(sockfd SOL_SOCKET SO_REUSEADDR &yes sizeof(int)); From Beej's Guide to Network Programming. That's a good idea. Another alternative is code your program for accepting the port number as input to avoid a recompile. To actually release the port like you want you need to write a Ctrl-C handler and code the close there. (Unrelated) What was somebody thinking to make that fourth argument a pointer to an int instead of an int? That's true the above solution does not actually release the socket but it does allow it to be re-bound on the next execution. @Kai: (get|set)sockopt support all sorts of options on all sorts of sockets. Some values for the 2nd and 3rd argument result in the 4th argument being treated as a pointer to a structure or a string. It's ok if it doesn't technically release the socket as long as the port can be reused because that still solves my problem. Thanks!  I assume the program you're writing is a server so you need to use a known port. If that's the case you should use the SO_REUSE_ADDR option on the socket as pointed out by Cogsy. If on the other hand you're writing a client sw then you should avoid choosing a particular port allowing the system to hand you a random one. Yes I'm referring to when writing a server thanks.
687,A,Android WifiInfo.getMacAddress on Toshiba Folio I've been using WifiInfo.getMacAddress string as seen on my nexus one or any other mobile device I've been able to use i.e. in the format of 01:02:03:04:05:06 But one of the users of my app just reported a crash on a Toshiba Folio100 my MAC address parsing failed. Since I've not any log files and the report is anonymous I cannot contact that man so I ask here if you know how it's encoded in such kind of device. I suppose it could be like 010203040506 but I'm not so confident. Although I can't comment on The Toshiba Folio specifically I would urge you to reconsider how you parse the MAC address string. I wrote code many years ago to get the MAC addresses from NICs in PCs - as it used the cards' PC drivers the format could vary depending on manufacturer. The docs for WifiInfo.getMACAddress() don't actually give a guarantee of how this string is returned so consider preparing for all eventualities. I saw the following formats... 01:02:03:04:05:06 // Delimited with ':' and padded to 2 chars 1:2:3:4:5:6 // Delimited but if < 16 there was no leading '0' 010203040506 // No delimiters but octets ALWAYS pre-padded with '0' when < 16 NOTE: Also the alphabetic characters may have been upper or lower case (although consistent in each case i.e. all upper or all lower case). Example... 0A0B0C0D0E0F 0a0b0c0d0e0f A:B:C:D:E:F a:b:c:d:e:f To cope with the above I first forced the string toLower(). I then checked for presence of ':'. If it existed I'd split the string into an array and I'd then check the length of each 'octet string' in the array and if length = 1 then padding wasn't used so I'd prefix '0'. Finally I'd reassemble the string by concatenating each of the array elements using ':' as a delimiter. If the string didn't contain ':' then I'd confirm that its length = 12 at this point I'd rebuild the string using each pair of characters and delimit with ':'. If the string wasn't delimited and its length wan't 12 then it would be impossible to parse correctly and I had to assume that requesting the MAC address string had failed and I reported a bad MAC address. It worked thanks! @Carlo Medas: That's great news. Good luck. I agree with you MisterSquonk. I'm gonna try to parse your cases with my wrong algorithm and see if the exception is similar to the one got by the user. And I just read that this Toshiba Folio 100 is almost retired from market because of too many bugs so I guess also the networking part is not standard and that's why it's the only case seen by my app which has been tested on many different devices. I hope in the meanwhile to be able to reach at least one Toshiba Folio100 user in order to verify the fix.
688,A,"Network Programming: to maintain sockets or not? I'm currently translating an API from C# to Java which has a network component. The C# version seems to keep the input and output streams and the socket open for the duration of its classes being used. Is this correct? Bearing in mind that the application is sending commands and receiving events based on user input is it more sensible to open a new socket stream for each ""message""? I'm maintaining a ServerSocket for listening to the server throwing events but I'm not so sure that maintaining a Socket and output stream for outbound comms is such a good idea. I'm not really used to Socket programming. As with many developers I usually work at the application layer when I need to do networking and not at the socket layer and it's been 5 or 6 years since I did this stuff at university. Cheers for the help. I guess this is more asking for advice than for a definitive answer. Oh wait I see... I'm confused you see what? I know this question is 2.5 years old but I'd say this (as I think it's worth saying): connection pools. That's what you want to use (or implement if your app does not have access to one already built). You can also look at DatagramSocket and DatagramPacket. The advantage is lower over-head the disadvantage is the over-head that regular Socket provides. I don't have the option of changing protocol I'm connecting to a vendor application. And require reliability to be implemented by the dev? No sir!  It depends on how frequent you expect the user to type in commands. If it happens quite infrequently you could perhaps close the sockets. If frequent creating sockets repeatedly can be an expensive operation. Now having said that how expensive in terms of machine resources is it to have a socket connection open for infrequent data? Why exactly do you think that ""maintaining a Socket and output stream for outbound comms is not such a good idea"" (even though it seems the right thing to do)? On the other hand this is different for file streams if you expect that other processes might want to use the same file. Closing the file stream quickly in this case would be the way to go. How likely is it that you are going to run out of the many TCP connections you can create which other processes making outbound connections might want to use? Or do you expect to have a large number of clients connecting to your server at a time? You'll run out of connections long before you run out of port numbers. one server one client So not going to run out of connections. port is specified as the API is a wrapper around a TCP based protocol. My bad I guess I've forgotten my network programming as well  There is a trade off between the cost of keeping the connections open and the cost of creating those connections. Creating connections costs time and bandwidth. You have to do the 3-way TCP handshake launch a new server thread ... Keeping connections open costs mainly memory and connections. Network connections are a resource limited by the OS. If you have too many clients connected you might run out of available connections. It will cost memory as you will have one thread open for each connection with its associated state. The right balanced will be different based on the usage you expect. If you have a lot of clients connecting for short period of times it's probably gonna be more efficient to close the connections. If you have few clients connecting for long period of time you should probably keep the connections open ...  If you've only got a single socket on the client and the server you should keep it open for as long as possible.  I suggest you look at using an existing messaging solution like ActiveMQ or Netty. This will handle lot of the issues you may find with messaging.  If your application and the server it talks to are close network-wise it MAY be sensible to close the connection but if they're distant network-wise you are probably better off letting the socket live for the duration. Guillaume mentioned the 3-way handshake and that basically means that opening a socket will take a minimum of 3 times the shortest packet transit time. That can be approximated by ""half the ping round-trip"" and can easily reach 60-100 ms for long distances. If you end up with an additional 300 ms wait for each command will that impact the user experience? Personally I would leave the socket open it's easier and doesn't cost time for every instance of ""need to send something"" the relative cost is small (one file descriptor a bit of memory for the data structures in user-space and some extra storage in the kernel)."
689,A,"Where to store preferences under Windows in a networked application? Where should I store preferences and setting if my goal is that my app should be runnable from a network share and the user's settings should persist regardless of the exact machine he is logged in on at the time? Similarly where should I store application wide settings? The development environment in use is not .Net but can compile apps compatable with Windows 2000 or later if that is relevant. It seems to me that If ""the user's settings should persist regardless of the exact machine he is logged in"" the only place where you can store the settings is ""on the server"". Exactly how this implemented is another story. If the share is writable then storing the config in file(s) on the same path/folder as the application is the easiest way. If the share is not writable you will have to find another way to do it - like a simple http web application.  Maybe Isolated Storage is what you are looking for (in case of per-user settings / files) Isolated Storage is a bit out of the question here since he's not using .Net. Darn I mis-read the question..  The users profile. Assuming that her profile is roaming the settings will follow the user wherever she is logging in. For applications settings should be stored in the Application Data special folder. You can get the path to this folder using SHGetSpecialFolderPath. For more info on special folders take a look at this article on Wikipedia. When you say ""application wide settings"" I assume you mean settings available to all users. These could be stored alongside the application on the network share. Where is the user's profile in this context? I'm talking about the logged on user profile stored on each local machine."
690,A,"Socket data length questions I have a couple of questions related to the following code: char buffer[256]; memset(buffer0256); read(socket_fdbuffer255); The questions: Why I read 255 not 256 ? Let's say I want to send the word: ""Cool"" from the client to the server. How many bytes should I write ""in client"" and how many bytes should i read ""in the server""? I'm really confused. You are not half as confused as I am You look at the return value from read(); it tells you how many bytes were read. You use the number of bytes read when you want to write the same data. You don't have to use 255 in the read unless you definitely want to be able to put a NUL at the end - but since you know how many bytes were read you won't go beyond that anyway. So the 255 is an insurance policy against carelessness by the programmer. The memset() is likewise most an insurance policy against carelessness by the programmer - it is not really necessary unless you want to mask out previous sensitive data.  So that the buffer retains the NUL at the end as extra insurance against string overflows. Reading 256 would allow it to get overwritten. You would write five bytes. Either write ""Cool\0"" or write 4 (the length) followed by the 4 characters in ""Cool"". Read all of it and figure out the length after.  You already have good answers here but I think there's a concept we should explain. When you send data through streams (that is something that writes a number of bytes from one end and those bytes can be read in the same order in the other end) you almost always want to know when to stop reading. This is mandatory if you'll send more than one thing: when does the first message stop and the second begin? In the stream things get mixed up. So how do we delimit messages? There are three simple ways (and many other not so simple ones of course): 1 Fixed-length messages: If you know beforehand that every message is say 10-bytes long then you don't have a problem. You just read 10 bytes and the 11th one will be part of another message. This is extremely simple but also extremely rigid. 2 Delimiting characters or strings: If you are sending human-readable text you might delimit your messages the same way you delimit strings in your char*'s: putting a 0 character at the end. That way when you read a 0 you know the message ended and any remaining data in the stream belongs to another message. This is okay for ascii text but when it comes to arbitrary data it's also kinda rigid: there's a character or a sequence of characters that your messages can't contain (or your program will get confused as to where a message ends). 3 Message headers: This is the best approach for arbitrary length arbitrary content messages. Before sending any actual message data send a fixed-length header (or use technique nr 2 to mark the end of the header) specifying metadata about your message. For example it's length. Say you want to send the message 'Cool' as you said. Well first send a byte (or a 2-byte short or a 4-byte integer or whatever) containing '4' the length of the message and receive it on the other end. You know that before any message arrives you must read 1 byte (or 2o or 4 or whatever) so store that somewhere and then read the remaining specified bytes. Very simple example: struct mheader { int length; }; (...) struct mheader in_h; read(fd &in_h sizeof(struct mheader); if (in_h.length > 0) read(fd buffer in_h.length) Hope it helps. Good luck! Very good explanation. Thanks!"
691,A,"My.Computer.Network.UploadFile What should I use for the ""User"" parameter of this function? Is this the name of the account that the user logs in with? Computer name? I am trying to send a file across to another computer on my network. This is the username to pass for HTTP basic authentication. If you're uploading to a site that gives a standard browser login dialog (not a normal login form) you should pass the username and password to this function. Most web pages uses forms-based authentication with a normal login form; if so you'll need to login using cookies and a separate HTTP request. Ah sorry. I really should have mentioned that I want to upload a file to another computer on my network. I keep getting an login failure because of bad password/username. Although I will keep that in mind when I get into that stuff. @Skeela: You're calling the wrong function; this uploads to an HTTP server. You should either run a server on the target machine or add a network share and do a normal filecopy. I could do that. This is more of a theory since I was able to use this function to upload a file to myself so I was just venturing out. It's just for testing purposes I don't really need it for anything so it's fine if a solution can't be come to."
692,A,"Simple Java client and server not interacting properly I can't understand why the code below doesn't work. The client sends a message to the server and the server prints the message to standard output. Code for the server: import java.net.*; import java.io.*; import java.math.BigInteger; public class server { public static void main(String args[]) { try { ServerSocket server = new ServerSocket(8080); while (true) { // initializations Socket connection = server.accept(); BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream())); PrintWriter out = new PrintWriter(connection.getOutputStream()); // listen for client message String message = in.readLine(); // print raw message from client System.out.println(message); // close resources if (out != null) out.close(); if (in != null) in.close(); if (connection != null) connection.close(); } } catch (Exception e) { System.out.println(e.getMessage()); System.exit(1); } } } Code for the client: import java.net.*; import java.io.*; import java.math.BigInteger; public class client { public static void main(String args[]) { try { // initializations Socket connection = new Socket(""localhost"" 8080); BufferedReader in = new BufferedReader(new InputStreamReader(connection.getInputStream())); PrintWriter out = new PrintWriter(connection.getOutputStream()); // send message to server out.println(""Hello world!""); // close resources if (in != null) in.close(); if (out != null) out.close(); if (connection != null) connection.close(); } catch (Exception e) { System.out.println(e.getMessage()); System.exit(1); } } } Any insights? Thanks! what exactly it's the error say?? There is no ""error""; the code compiles fine. But after I start up the server and then the client the server hangs without printing anything to standard output. Hangs where? Run under a debugger or add some printlns to find out where? Does the client run and terminate without any error? Using ""printlns"" I found that the server hangs on in.readline() and the client hangs on out.println(). Add a flush after you write to the stream in your client :   // send message to server out.println(""Hello world!""); out.flush();  Also Make sure the server socket is not blocked by firewall  What you should be doing to figure out this type of problem is to isolate where the problem comes from. Is it the Server portion or the client portion? An easy test for the server is to start it up then telnet to that port (ex. ""telnet 127.0.0.1 8080"") type in something and see if it is outputing. (btw your server code works fine). Doing this would allow you to focus on your client code. As Affe stated you simply didn't flush out the input stream. Learning methodologies for troubleshooting code is at least as important as learning to write code. Also as an aside by convention Java classes start with a capital letter so it should be ""Server"" and ""Client"" Thanks for the advice. Unfortunately for the assignment the spec calls for lowercase-lettered names.  That default PrintWriter does not autoflush. (and doesnot flush on close as abstract Writer deceptively may lead you to believe. Either do:  PrintWriter out = new PrintWriter(connection.getOutputStream() true); or else  out.println(""Hello world!""); out.flush();"
693,A,my realtime network receiving time differs a lot anyone can help? I wrote a program using tcpip sockets to send commands to a device and receive the data from the device. The data size would be around 200kB to 600KB. The computer is directly connected to the device using a 100MB network. I found that the sending packets always arrive at the computer at 100MB/s speed (I have debugging information on the unit and I also verified this using some network monitoring software) but the receiving time differs a lot from 40ms to 250ms even if the size is the same (I have a receiving buffer about 700K and the receiving window of 8092 bytes and changing the window size does not change anything). The phenomena differs also on different computers but on the same computer the problem is very stable. For example receiving 300k bytes on computer a would be 40ms but it may cost 200ms on another computer. I have disabled firewall antivirus all other network protocol except the TCP/IP. Any experts on this can give me some hints? This is more a server issue than anything else To help answer this we would need more details such as network architecture machines software you are running. Thank you for the reply. The server is DSP based. The client is just a dos test program. not anyone have any clue on this? I already had no way out now. Please help I've found the answer to this question. The problem is due to the even/odd number of packets before the last fragment packet caused by the Nagle's algorithm. see the link which is very informative: http://www.stuartcheshire.org/papers/NagleDelayedAck/
694,A,"Am I using CNCopyCurrentNetworkInfo correctly? I'm having some issues with the data returned by CNCopyCurrentNetworkInfo and was wondering if I'm doing something wrong. I'm using the following code to display the BSSID of the currently connected Access Point: NSArray* interfaces = (NSArray*) CNCopySupportedInterfaces(); for (NSString* interface in interfaces) { CFDictionaryRef networkDetails = CNCopyCurrentNetworkInfo((CFStringRef) interface); if (networkDetails) { NSLog(@""all details: %@"" (NSDictionary *)networkDetails); NSLog(@""BSSID: %@"" (NSString *)CFDictionaryGetValue (networkDetails kCNNetworkInfoKeyBSSID)); CFRelease(networkDetails); } } By reviewing the NSLog statements it appears as though CNCopyCurrentNetworkInfo is hanging onto old data for kCNNetworkInfoKeyBSSID under certain circumstances. I have two access points set up and I'm trying to obtain the BSSID of the currently connected AP. If I start with only one AP turned on it returns the correct BSSID. If I switch that AP off I get no BSSID (correct) and when I switch on the second AP and connect to it I am given the correct BSSID of the second AP. However if I start with one AP turned on run this code and am correctly given the BSSID of that AP. I then turn on the second AP then turn off the first (forcing the device to roam to the second AP) the log statements still return the BSSID of the first AP (which I have turned off and am not possibly connected to). Does anyone have more experience with this than I've had? Am I meant to be manually flushing the values returned by CNCopyCurrentNetworkInfo between calls? I am have run this on both a 4th gen iPod touch running iOS 4.3 built using Xcode 4 and an iPhone 4 running iOS 4.1 built using Xcode 3.2.4 I'm having the same problem now on iPhone 4 and iOS 5.0.1. I don't suppose you've found a solution. I'm going to provide an answer to this question because it seems to get the occasional up vote and could do with a solution. Short version is that yes that is how you're supposed to use CNCopyCurrentNetworkInfo. This is a bug in iOS 4 and 5. If you're currently running an iOS 6 beta I would suggest investigating whether it still operates this way on there and reporting it to Apple if it does. I was fortunate in that I work on enterprise applications and was able to gain this information using calls to private APIs. For info on how to do this I would probably recommend looking here: iphone-wireless on google code"
695,A,"How can I make reading from a DatagramSocket only block for a limited time (time out)? I'm running a simulation for network packet transmission loss. My server app sometimes doesn't send data back to my client. In my client I'm running a ping to the server every seconds 10 times. However in the case where my server doesn't send anything back my client will wait just wait until the next packet is received. How can I continue to the next iteration if I don't get anything from the server? //attempt to read from server DatagramPacket receivePacket = new DatagramPacket(receiveData receiveData.length); receivePacket.getData(); //client blocks on this line until something is received from server clientSocket.receive(receivePacket); String receivedFromServer = new String(receivePacket.getData()); System.out.println(""FROM SERVER:"" + receivedFromServer); You'll want to set a timeout: clientSocket.setSoTimeout(TIMEOUT_IN_MILLISECONDS) thanks. that worked perfectly. I had to catch SocketTimeoutException @Tony: Yes and you're welcome! :)"
696,A,Open Source & Cross Platform Multiplayer/Networking Libraries? While raknet seems fairly interesting and really appealing from a feature-point of view its licensing terms seem to be possibly troublesome for GPL'ed projects that may be leveraged commercially something which is explicitly forbidden by the terms of the creative commons license. While there's also opentnl it doesn't seem to be as actively maintained anymore nowadays in fact downloading the latest stable tarball even fails during compilation because it doesn't seem to support gcc >= 3.0 (?) Of course there's still also enet but this one cannot be really compared to the abstract features that are supported by raknet/opentnl. So apart from any non-trivial dependencies such as ACE Boost or Poco are there any viable alternatives for embedding a fairly compact well-maintained UDP-networking library? Thanks Though this answer comes late to the party I'm using OpenTNL for my game Bitfighter and I really like it. I use it on OS X Windows and Linux without a hitch. True it's not maintained by its creator but when I get the time I'm going to create a new SourceForge project for it so people have a place to post their patches. It's stable and (fairly) well documented so I would recommend giving it another look.  The wiki of Ogre3D provides a list of networking libraries and a short description for them.  Unfortunately network programming tends to be non-trivial. Said that you would be advised to get aquainted with the network programming facilities from either Boost or ACE as both are mature libraries that have been successfully employed in many applications. I would also suggest to read C++ Network Programming: Mastering Complexity Using ACE and Patterns and C++ Network Programming: Systematic Reuse with ACE and Frameworks Thanks I was mostly referring to these dependencies being specifically non-trivial rather than the task itself.  I have been looking for something very similar but to no avail. So I decided to create my own C++ Networking Library at the time of this writing it isn't complete but will be very soon. I will keep you up to date if your interested in trying it out. It's features so far are TCP/UDP IPv4 IPv6 Async/Sync and multicasting. If there are any other features you have in mind that should be implemented just let me know :)
697,A,Unique identifier for user profiles in Windows For a client/server application I need to centrally store parts of the configuration information that usually goes into the users profile directory. The client application would on first use write a file or registry entry with a GUID into the current profile. This GUID would subsequently be used as a key in the configuration database on the server. Now I'm wondering if Windows user profiles already have unique identifiers I could use instead of generating my own GUIDs. The username won't work because users might have multiple profiles. Combining it with the computer name won't work because there might be roaming profiles. Update: I just looked at the SIDs in HKEY_LOCAL_MACHINE\Software\Microsoft\Windows NT\CurrentVersion\ProfileList on two computers in the same domain. Roaming is not enabled so my user account has a separate profile on each machine. Both profiles are listed with the same SID. This means I have to keep generating my own GUIDs. You could use the user profile's security identifier (SID). The LookupAccountName() Win32 API takes a user name and computer name as input and gives you back the associated SID.  Windows users and groups use security identifiers (SIDs). A security identifier (SID) is a unique value of variable length that is used to identify a security principal or security group in Windows operating systems. There is a list of predefined SIDs that Windows has built-in. Other SIDs are generated by combining the current computer's (randomly generated 96-bit) SID with an incremented number. SIDs of users that have accounts on a computer are stored in the registry under HKEY_LOCAL_MACHINE\Software\Microsoft\Windows NT\CurrentVersion\ProfileList Sample SIDs (taken from The Microsoft policy concerning disk duplication of Windows XP installations): The following example displays the SIDs for four local user accounts. Note that only the last four digits are incremented as new accounts are added. S-1-5-21-191058668-193157475-1542849698-500 Administrator S-1-5-21-191058668-193157475-1542849698-1000 User 1 S-1-5-21-191058668-193157475-1542849698-1001 User 2 S-1-5-21-191058668-193157475-1542849698-1002 User 3 Because of how SIDs are generated they should be unique. Since they are part of the windows profile system roaming profiles should have the same SID on every system.  I might use a more LDAP-centric solution to this problem but it might be a lot more work for your app. There are a few unique fields in AD for user. You could use the whole DN of a User record (i.e. DC=comDC=exampleCN=UsersDN=bob smith). That's what uniquely identifies a record in AD. However MS also has a field called UPN which looks like an email address (sometimes it is) and takes the form user@domain. Of course this information requires read access to AD and that may not be practical for your app.  R Bemrose and snowccrash are correct the account SID is precisely what you have requested. You are correct that in order for this solution to work you much enable roaming profiles; that's why they're called roaming profiles. If you don't want to use domain authentication to identify users then your other option is WAS (Windows Authentication Services). This is typically but not necessarily implemented atop Microsoft SQL Server in the ubiquitous ASPNETDB database. WAS is a dotnet solution with elaborate support for ASP.NET that is also available for desktop software. If you don't like that either you can roll your own but this seems to me a suboptimal application of resources. If you aren't building dotnet software you could still exploit WAS but it won't be quite so convenient.
698,A,how to get started in p2p networking programming? I want to learn how to write a software using a peer to peer networking architecture but i don't know where to start knowing that I use as a programming languages : c/c++  lisp a little of python. any pointer to documentation or tutorials is appreciated. Learn how to do ordinary socket programming first. here are some guidelines which i found with google  Why not just start with a Bittorrent API such as http://libtorrent.rakshasa.no/ rather than writing your own. After you have gained experience with a well-used P2P network then you may start to see what you could do better then start to write your own.
699,A,"How to change internal buffer size of DataInputStream I'm using this kind of code for my TCP/IP connection: sock = new Socket(host port); sock.setKeepAlive(true); din = new DataInputStream(sock.getInputStream()); dout = new DataOutputStream(sock.getOutputStream()); Then in separate thread I'm checking din.available() bytes to see if there are some incoming packets to read. The problem is that if a packet bigger than 2048 bytes arrives the din.available() returns 2048 anyway. Just like there was a 2048 internal buffer. I can't read those 2048 bytes when I know it's not the full packet my application is waiting for. If I don't read it however - it'll all stuck at 2048 bytes and never receive more. Can I enlarge the buffer size of DataInputStream somehow? Socket receive buffer is 16384 as returned by sock.getReceiveBufferSize() so it's not the socket limiting me to 2048 bytes. If there is no way to increase the DataInputStream buffer size - I guess the only way is to declare my own buffer and read everything from DataInputStream to that buffer? Regards Why can't you read it in chunks? @Max - I just did. Thanks. I'm going to make an assumption about what you're calling a ""packet"". I am going to assume that your ""packet"" is some unit of work being passed to your server. Ethernet TCP packets are limited to 1536 bytes. No matter what size writes the peer is performing. So you cannot expect to atomically read a full unit of work every time. It just won't happen. What you are writing will need to identify how large it is. This can be done by passing a value up front that tells the server how much data it should expect. Given that an approach would be to have a thread do a blocking read on din. Just process data as it becomes available until you have a complete packet. Then pass the packet to another thread to process the packet itself. (See ArrayBlockingQueue.) You socket reader thread will process data at whatever rate and granularity it arrives. The packet processor thread always works in terms of complete packets. That's exactly how I finally did it. Thanks  Wrap your data input stream around larger buffered input stream: DataInputStream din = new DataInputStream( new BufferedInputStream( sock.getInputStream( ) 4096 ) ); But I don't think it's going to help you. You have to consume the input from the socket otherwise the sender will get stuck. You should probably invest more time in working out a better communication protocol.  If you dig into the source of DataInputStream getAvailable() is actually delegated to the stream its reading from so the 2048 is coming from the socket input stream not the DataInputStream (which is implemented by default in native code). Also be aware that the API states for an InputStream Note that while some implementations of InputStream will return the total number of bytes in the stream many will not. It is never correct to use the return value of this method to allocate a buffer intended to hold all data in this stream. So just because you are receiving a value of 2048 does not mean there is not more data available it is simply the amount that is guaranteed to be read without blocking. Also note that while BufferedInputStream as suggested by Alexander is an option it makes no guarantee that the buffer will always be filled (in fact if you look at the source it only attempts to fill the buffer when one of the read calls is made). So if you want to make sure you are always receiving ""full packets"" you are likely better off creating your own input stream wrapper where you can add a specialty method ""byte[] readPacket()"" that will block until it can fill its own buffer as data is read from the underlying socket stream.  This is not how you use InputStreams. You never want to use the available method it's pretty much useless. If you need to read ""packets"" of data then you need to design that into your protocol. One easy way to do that is send the length of the packet first then send the packet. the receiver reads the length of the packet then reads exactly that many bytes from the stream."
700,A,"Constantly reading/writing data over a TCP/IP Port. Which one? Unfortunately I don't know much networks. I am writing a program that has two versions. A server version and a client version. Lets assume that the client versions are installed on say 20 PCs that are connected to the server over ethernet. The client versions needs to CONSTANTLY get some data from the server. The data is kind of serial. I wanted to know a way to broadcast the data that gets updated every second and make it available to all the other PCs in the network. Could I use the HTTP Port for this? like writing the data to an HTML page or something? or Is there a better port or method for doing this? Any ideas will be greatly appreciated. What kind of data are you transmitting? Text (specifically numbers) There are many ways. you can choose any of them but i think document below will help you a lot. Multicast over TCP/IP HOWTO: http://www.ibiblio.org/pub/Linux/docs/howto/other-formats/html_single/Multicast-HOWTO.html#sect-trans-prots  This sounds like a pretty straightforward application of TCP sockets. The server would be set up to ""listen"" on a particular port (you pick the port number say 12345) and each client would make a TCP connection to the server on that port. Whenever the server has data to send it would send it once to each connected client. This could mean that the server sends the data up to 20 times on different sockets but that's fine. The client would read the data from its connected socket to the server. There are other alternatives such as UDP or even UDP multicast but these usually end up being a lot more complicated because UDP doesn't guarantee that packets always arrive at the destination (and they may even be duplicated or out of order). TCP ensures that the data you send either arrives complete in the correct order or doesn't arrive at all (in that case the connection would be dropped). An example of this sort of multiple TCP connection is VNC: VNC is widely used in educational contexts for example to allow a distributed group of students simultaneously to view a computer screen being manipulated by an instructor or to allow the instructor to take control of the students' computers to provide assistance. It's important to note for someone new to sockets/TCP that the data you transmit may not arrive all in the same packet or multiple transmits may arrive in the same packet. ZeroMQ appears to be a simple way to overcome this but I haven't tried it. Alternatively you need some manual framing in your protocol. Thanks. Is it alright if we have a lot of data to send? I mean more than ten numbers every second for 20 clients? @Auxiliary: that's actually a very *small* amount of data as far as modern networks are concerned."
701,A,"How to manage multiple client sessions within server application? I'm writing web chat application similar to GTalk. It based on Orbited + Sinatra for client side and Ruby for server side. I've already implemented all the protocol everything working good. But. I got a problem - dont know how to deal if there are multiple connections from one user. Let`s say for example i logged to chat from 2 different browsers. Google handles that really nice two chats appear to be exactly the same. But my app just shows 2 exactly the same users in contact list which is incorrect. Here is a small example of server clients pool: Server --> Connections | - Client (User Information ConnectionID) - Client (User Information ConnectionID) .... - Client (...) I have 2 types of messages: Private (user-to-user) Public (user-to-conference). Im trying to figure out how to deal with such situation? Any suggestions? You are reinventing the wheel. XMPP is the standard for instant messanging. There are many free implementations. Google Talk is just another implementation of XMPP. XMPP is modular so you can put your own needs into the protocol without breaking it. When your clients connect you should give them a unique identifier. Classic ASP had that SessionID but you just need something unique maybe sending that key back to client through a cookie. After that any message sent by clients should be placed in a common area; again in classic ASP you had that Application objects nicely suitable for this task. From this you can go anywhere: implement chat rooms filtering messages whispering something etc. I did something like that about 7 8 years ago storing messages in XML files. But you could also use some database to do that. As pseudo code you'll have following for every web request:  if request DON'T have cookie ID create an unique ID and set cookie process incoming action case ""private"": write message for that unique id case ""public"" write message without target user case ... display user interface list all messages for your unique id or without target create a users list using unique IDs as value (except yours) refresh every n seconds No no i got the point. But in your case if you login to your chat app from different locations with the same account you will receive messages from yourself. Right? Or i just dont see the idea of keeping uniq id for each connection. you'll not receive messages from yourself as second connection will have another unique id.  Sorry for the vague answer but here goes: you need to ""push"" chat text out to every connection for a given user ID not just responde to a ""pull"" from a given connection / session. I don't know how your client works but if it polls for updates you probably need to save a per user account image of recent messages in a database then get all the relevant updates for that user from the DB and not just associate the chatting with ""point to point"" sessions. Grr. I don't have time to explain this better now... update: - - - - - Make some kind of ""set"" data structure for each conversation identifying the sessions (and therefore users) involved regardless of whether it is one on one or a large group. Make a list of posts for each conversation ordered chronologically which you can scan to update the display of each client supplying any as yet un-viewed posts. As an aside to a comment on the question itself: Somebody made the point that ""it's been done"" download the code. Perhaps that's a valid point perhaps not. If you can find an existing code base in a form you can embrace and extend great. If not (because it's homework or because corporate policy says ""do it from scratch here and now"" well then downloading a ""solution"" is not a valid criticism is it? Ok im already using server push approach. The thing is - when you log in with your account into the chat from different browser tabs - it creates another connection on server with the same user so there are 2 or more same users on chat list. I cant figure out the best way to allocate session pool for users who have more than one connection."
702,A,"Best IDE for Java Network Programming I'm going to develop a network application using Java for my University project. I have used Netbeans previously for other Java development but for this special development project I don't know what is the best IDE that I can use. I will be getting the IP addresses and connecting to the machines and will be doing some socket programming apps. What is the best IDE for that kind of application? I will be working using that IDE for about 6+ months so it's better if it is a good one with usability. Take your pick. NetBeans or Eclipse. Otherwise it's Emacs or VI. There's no real one true answer to this question which is simply a variant on which IDE is ""best"". The answer is whichever one you are most productive in. hi thanks for the openions. i am mostly familier in netbeans environment. familier the better is it? :) What does ""best"" mean for you? Use eclipse and then from this page http://www.eclipse-plugins.info/eclipse/plugins.jsp?category=Network find networking plugins that suites you most and install them. Some networking plugin that you can find on that page are: Netwiser: a unique platform for network software development parfumball: a Java based network analyzer and packet sniffer i have experience with eclipse only for php development will try this also and see... :) thanx! have tried it with the plugins  it's ideal... thanx for the great help...! :) You are very welcome.  IntelliJ IDEA is free for Java SE projects and my coworkers and I really love it. The commercial (aka ""ultimate"") version is mostly for Java Enterprise development. IntelliJ IDEA community edition is fully-featured and not limited in any way for plain Java and Android development. it's a commercial one isn't it? hmmm... i'm still a student can't afford that. :( There's a pared down free version that's not bad. is that so.. then i will try it and see.. :)  I'd recommend IntelliJ IDEA. It's generally considered the one to go for at my university. I've been using it predominantly for networking programming for 4 or 5 months now. It has a ton of productivity hints and tools. But it is nevertheless very easy to use tidy and efficient. And yes there is a ""community supported"" free version which is the one I am using.  IntelliJ free version. It's even better than the full version for your purpose."
703,A,"What is the IPv6 equivalent for SIOCADDRT? Can SIOCADDRT be used to add IPv6 routes? If not what's the equivalent for ioctl()? Yes you can use SIOCADDRT. Just create AF_INET6 socket for it. For example take a look at Busybox implementation of ""route"" command. Also consider using Netlink instead perhaps via libnl. Sorry for late answer I was looking for this myself and found the accepted answer a bit lacking.  I don't know but # strace ip route add <ipv6-route> should tell you.  Not writing low-level network configuration code in your program but instead delegating the task to the existing system utilities would be one very good solution. I generally agree with that sentiment but this is still one valid solution to the problem. Please don't assume you know better than the questioner what the correct approach to their problem is. It happens that from time to time people *are* wrong about that but it's rude and often just plain wrong to start from that perspective."
704,A,"How to crawl a wordpress blog? I write a c program to crawl blogs. It works well until it meets this blog: www.ipujia.com. I send the HTTP request: GET http://www.ipujia.com/ HTTP/1.0 to the website and get the response as below: HTTP/1.1 301 Moved Permanently Date: Sun 27 Feb 2011 13:15:26 GMT Server: Apache/2.2.16 (Unix) mod_ssl/2.2.16 OpenSSL/0.9.8e-fips-rhel5 mod_auth_passthrough/2.1 mod_bwlimited/1.4 FrontPage/5.0.2.2635 mod_perl/2.0.4 Perl/v5.8.8 X-Powered-By: PHP/5.2.14 Expires: Wed 11 Jan 1984 05:00:00 GMT Cache-Control: no-cache must-revalidate max-age=0 Pragma: no-cache Last-Modified: Sun 27 Feb 2011 13:15:27 GMT Location: http://http/www.ipujia.com/ Content-Length: 0 Connection: close Content-Type: text/html; charset=UTF-8 This is strange because I cannot get the index page following the Location. Does anyone have any ideas? Thank you for your answers. I solve the problem with your help. Are you using a Library for doing this? If so there should be built in functions to follow redirects contained in responses. shouldn't you be doing `GET / HTTP/1.0`? Well spotted Dan but given that it's a C program which is behaving this way we have no way of knowing whether that's the way the C program is invoked or whether it is the actual request header. Need more details OP. The Location field in the response contains a malformed URI. Location: http://http/www.ipujia.com/ (notice the protocol error) Should be Location: http://www.ipujia.com/ Unless you are in control of the server there is little you could do here. To solve it could you not parse the ""Location"" response and attempt to extract a valid URI from the it?"
705,A,How to forward packets via C# to another TCPClient running on another port How can I forward packets using C# to another TCPClient running on another port. I am new to Sockets programming in .Net. I am trying to make a VNC Repeater much like this perl script: http://snipt.org/wonG but I cannot keep my stream continuous. An example of what I want to achieve is this: Port 5500 listens inspects the packet RemoteEndPoint and then and sends the packet back out on Port 5901. Because the bulk of the traffic is considered 1 big open stream (it does not close or send chunks of data it sends traffic for upwards of 10 minutes) how can I do this? Does anyone have any suggestions or ideas on how to execute a TCPClient packet forwarding solution that supports large open streams(sessions)? A TCP stream is the same no matter how it is used. Just loop calling Read() and Write() back out whatever you get until the socket is closed. Could you possibly post an example link? I should probably stop trying to tinker and get my head around how .Net handles sockets first. http://tcpsoftrouter.codeplex.com/ Is a good example. EDIT: Since this time I have written NRepeat. Available here https://github.com/jeremychild/NRepeat
706,A,"When will a TCP network packet be fragmented at the application layer? When will a TCP packet be fragmented at the application layer? When a TCP packet is sent from an application will the recipient at the application layer ever receive the packet in two or more packets? If so what conditions cause the packet to be divided. It seems like a packet won't be fragmented until it reaches the Ethernet (at the network layer) limit of 1500 bytes. But that fragmentation will be transparent to the recipient at the application layer since the network layer will reassemble the fragments before sending the packet up to the next layer right? At the application layer there are any number of reasons why the whole 1500 bytes may not show up one read. Various factors in the internal operating system and TCP stack may cause the application to get some bytes in one read call and some in the next. Yes the TCP stack has to re-assemble the packet before sending it up but that doesn't mean your app is going to get it all in one shot (it is LIKELY will get it in one read but it's not GUARANTEED to get it in one read). TCP tries to guarantee in-order delivery of bytes with error checking automatic re-sends etc happening behind your back. Think of it as a pipe at the app layer and don't get too bogged down in how the stack actually sends it over the network.  If a packet exceeds the maximum MTU of a network device it will be broken up into multiple packets. (Note most equipment is set to 1500 bytes but this is not a necessity.) The reconstruction of the packet should be entirely transparent to the applications.  The question makes an assumption that is not true -- TCP does not deliver packets to its endpoints rather it sends a stream of bytes (octets). If an application writes two strings into TCP it may be delivered as one string on the other end; likewise one string may be delivered as two (or more) strings on the other end. RFC 793 Section 1.5: ""The TCP is able to transfer a continuous stream of octets in each direction between its users by packaging some number of octets into segments for transmission through the internet system."" The key words being continuous stream of octets (bytes). RFC 793 Section 2.8: ""There is no necessary relationship between push functions and segment boundaries. The data in any particular segment may be the result of a single SEND call in whole or part or of multiple SEND calls."" The entirety of section 2.8 is relevant.  Different network segments can have different MTU values. In that case fragmentation can occur. For more information see TCP Maximum segment size This (de)fragmentation happens in the TCP layer. In the application layer there are no more packets. TCP presents a contiguous data stream to the application.  Fragmentation should be transparent to a TCP application. Keep in mind that TCP is a stream protocol: you get a stream of data not packets! If you are building your application based on the idea of complete data packets then you will have problems unless you add an abstraction layer to assemble whole packets from the stream and then pass the packets up to the application.  Correct - the most informative way to see this is using Wireshark an invaluable tool. Take the time to figure it out - has saved me several times and gives a good reality check  This page is a good source of information about some of the issues that others have brought up namely the need for data encapsulation on an application protocol by application protocol basis Not quite authoritative in the sense you describe but it has examples and is sourced to some pretty big names in network programming.  If a 3000 byte packet enters an Ethernet network with a default MTU size of 1500 (for ethernet) it will be fragmented into two packets of each 1500 bytes in length. That is the only time I can think of. Wireshark is your best bet for checking this. I have been using it for a while and am totally impressed So will the receiving application receive two packets or one at the application layer? That's not what he asked... Application layer has no concept of MTU and packet fragmentation. It is a seperate layer with seperate responsibilities from the Network and Transport layer. So shot answer App layer cannot see two packets.  A the ""application layer"" a TCP packet (well segment really; TCP at its own layer doesn't know from packets) is never fragmented since it doesn't exist. The application layer is where you see the data as a stream of bytes delivered reliably and in order. If you're thinking about it otherwise you're probably approaching something in the wrong way. However this is not to say that there might not be a layer above this say a sequence of messages delivered over this reliable in-order bytestream.  It will be split when it hits a network device with a lower MTU then the packets size. Most ethernet devices are 1500 but it can often be smaller 1492 if that ethernet is going over PPPoE (DSL) because of the extra routing information even lower if a second layer is added like Windows Internet Connection Sharing. And dialup is normally 576! In general though you should remember that TCP is not a packet protocol. It uses packets at the lowest level to transmit over IP but as far as the interface for any TCP stack is concerned it is a stream protocol and has no requirement to provide you with a 1:1 relationship to the physical packets sent or received (for example most stacks will hold messages until a certain period of time has expired or there are enough messages to maximize the size of the IP packet for the given MTU) As an example if you sent two ""packets"" (call your send function twice) the receiving program might only receive 1 ""packet"" (the receiving TCP stack might combine them together). If you are implimenting a message type protocol over TCP you should include a header at the beginning of each message (or some other header/footer mechansim) so that the receiving side can split the TCP stream back into individual messages either when a message is received in two parts or when several messages are received as a chunk. it does depend on the MTU (actually the smallest MTU) along every hop along the way. But for small payloads (10 or 20 bytes as in the OP's question) it can be ""almost sure"" that it won't be fragmented if the whole packet (including all headers of each protocol layers even the ones added on top if going through say a VPN) + the size of the payload are within limits of ipv4 minimal size. On http://en.wikipedia.org/wiki/Maximum_transmission_unit you can see that for ipv4 you are guaranteed a maximum MTU of at least 68 bytes (NOT payloads!) on the hops."
707,A,Good lightweight C or C++ master-slave networking library I have a C/C++ server process that needs to broadcast data to some number of client programs on other computers. Client programs are also C/C++. I need a reliable and lightweight communication library that will allow my server to communicate with any number of clients. Clients must be able to detect when a socket connection is no longer open hopefully without having to constantly poll the server. I do not want to use something huge like Boost. I want something lightweight and simple. Any ideas? See the answer to this question. It looks like Qt may include what you need and it's known as relatively simple to use. SDL.net is another alternative that might meet those requirements.
708,A,"how do i make users to download jpg files i need to make users download jpg files. i know that if i send them the link to the jpg file the browser will just display the jpg file. Then the user must do rightclick -> save picture as However i want to force the download dialog box to appear so that they can download the jpg file. Anyone can help me with that? You need to set the correct headers in your PHP script that serves the image to the user header(""Cache-Control: public""); header(""Content-Description: File Transfer""); header(""Content-Disposition: attachment; filename=$file""); header(""Content-Type: image/jpg""); header(""Content-Transfer-Encoding: binary""); //read image write to user You probably already printed a header somewhere. Debug your file (inspect the code run it from console or attach a debugger) and see where the header was already sent. $filename = file_get_contents(""../mathieu-opticien-images/advertising/bigversion/Chrysanthemum.jpg""); header(""Content-Disposition: attachment; filename="" . $filename); i tried it like this it now says: Warning: Header may not contain more than a single header new line detected. in D:\wamp\www\mathieu\includes\downloadjpg.php on line 9 `readfile($file);` is a good way to read & write the image file Well spotted. The filename just a string so the user's browser knows how to name the file when he presents the download box to the user. No need for paths in the filename! @Konerak well spotted yourself. I know I've done this but not something I do everyday so I need to look it up (perhaps in stackoverflow) We're here to help and we learn while helping ;) @mahen23 I think what you needed to do is `$file = ""../mathieu-opticien-images/advertising/bigversion/Chrysanthemum.jpg"";` Instead you're setting file name equal to the contents of the jpg image."
709,A,Strange client's address returned throug accept(..) function I'm a socket programming newbie. Here's a snippet: struct sockaddr_storage client_addr; ... client_addr_size = sizeof(client_addr); client_socket = accept( server_socket (struct sockaddr *)&client_addr &client_addr_size ); ... result = inet_ntop( AF_INET &((struct sockaddr_in *)&client_addr)->sin_addr client_addr_str sizeof(client_addr_str) ); I'm working as a server. Whenever the client connects the address I get is 0.0.0.0 regardless from the host. Can anybody explain what I'm doing wrong? Thanks. Can you show a bit more code...what IP address/service are you trying to connect to? The clue is in the IP address itself 0.0.0.0 commonly a situation where a network interface has no IP address assigned possibly looking for a DHCP server to renew/accept a DHCP lease from somewhere.. I am shooting myself in the foot as you have not provided enough information and hence would be deemed unfair to do so and get downvoted as a result as this answer does not satisfy your question!! He's not connecting *to* anything; he's acting as a server and accepting connections.  Just a guess - what's the declaration of client_addr_str? If it's char* then sizeof(client_addr_str) would return size of pointer (4 or 8 depending on 32- or 64-bit platform.) Try the following: char client_addr_str[INET_ADDRSTRLEN]; if ( inet_ntop( AF_INET &((struct sockaddr_in *)&client_addr)->sin_addr client_addr_str INET_ADDRSTRLEN ) == NULL ) { /* complain */ }  Check client_addr.ss_family - it may be returning an AF_INET6 family address. Yes that was right. I've fixed the problem. Yet another one arouse. I use getaddrinfo function with AI_PASSIVE in hints. The address I later bind my socket to is :: . Is it okay? Yes that's fine (as long as the socket is still an `AF_INET6` one).
710,A,"Best way to resolve a DNS TXT record on Linux/Unix/Posix/BSD type systems? I want to write some portable (as possible) C code to look up DNS TXT records. I would also prefer not to have dependencies on libraries that don't ship with the machine. What is the best way to do this on Unix-like machines using standard libraries? I found some sample code that works using libresolv but it's not reentrant (not thread safe) and is very ugly. Is there a better way? Also what about Windows? If there were a way that worked there too that would be perfect. you can use res_query which uses the standard libresolv. There's an example here from clamav: if((len = res_query(domain C_IN T_TXT answer PACKETSZ)) < 0) { mprintf(""@Can't query %s\n"" domain); return NULL; } Yes that's the sort of code that I found. You then have to parse the answer quite a bit. But it's not reentrant. If you used it from multiple threads you would have to surround it with a static mutex. There are reentrant calls for getting A AAAA and CNAME addresses via getaddrinfo() and pals. I was curious if there's anything I don't know that does something similar for TXT records.  This is a FAQ. See Code to do a direct DNS lookup or How might I perform DNS lookups using C/C++ on Linux?."
711,A,"Remote procedure call: how to declare two versions of same program in a XDR file I am writing a program which makes a RPC to print a message that I send as a parameter to the remote function. The remote function is not supposed to return anything however just out of curiosity I have designed the remote function to return an integer. Anyways everything worked fine and I was able to print a message remotely. Now I am trying to create two versions of a program in XDR file (Just curious!!!) but its not working out for me. here is my new XDR file anirudh@anirudh-Aspire-5920:~/Documents/C/DS/RPC$ cat spec.x program MSGPROG{ version PRINTMSGVERSION{ int PRINTMSG(string) = 1; } = 1; version PRINTMSGVERSION{ int PRINTMSG(string) = 1; } = 2; } = 0x2000001; Here is the server code: anirudh@anirudh-Aspire-5920:~/Documents/C/DS/RPC$ cat server.c #include<stdio.h> #include ""spec.h"" int *printmsg_1_svc(char **msgstruct svc_req * sr){ static int ret; printf(""version = 1--%s\n""*msg); ret = 1; return &ret; } int *printmsg_2_svc(char **msgstruct svc_req * sr){ static int ret; printf(""version = 2--%s\n""*msg); ret = 1; return &ret; } Here are the errors that I get at compile time: anirudh@anirudh-Aspire-5920:~/Documents/C/DS/RPC$ rpcgen -C spec.x anirudh@anirudh-Aspire-5920:~/Documents/C/DS/RPC$ gcc server.c spec_svc.c -o ani_server -lnsl In file included from server.c:2: spec.h:32: warning: ""PRINTMSGVERSION"" redefined spec.h:18: note: this is the location of the previous definition In file included from spec_svc.c:6: spec.h:32: warning: ""PRINTMSGVERSION"" redefined spec.h:18: note: this is the location of the previous definition So the error is coming because I am not able to figure out how to declare two versions of a program in XDR file. Thanks a lot for reading my post. Please help me out. Thanks in advance. So I just checked the ""spec.h"" file. rpcgen has done `#define PRINTMSGVERSION 1`. it then does `#define PRINTMSGVERSION 2`. So if I declare another version of the program with same name but different version number (like I did above) things won't work out for me. One can not #define a variable twice without #undef ing it. This way there is no point in giving version number because there would be no way I would be able to declare a program with same name but different version number. Any suggestions ? this my new updated spec.x file `program MSGPROG{ version PRINTMSGVERSION{ int PRINTMSG(string) = 1; } = 1; version PRINTMSGVERSION_NEW{ int PRINTMSG(string) = 1; } = 2; } = 0x2000001;` but now because of this everytime my client wants to connect to server it has to create two different handles to connect to two different versions of the same program. BUT ya! ya! THAT IS HOW ""`clnt_create(..)`"" function is defined. I still can not understand the significance of VERSION NUMBER. If you find me wrong somewhere please enlighten me. :) ""A version name cannot occur more than once within the scope of a program definition. Nor can a version number occur more than once within the scope of a program definition."" - RFC 1057 You simply have to give the version string distinct names e.g.: program MSGPROG{ version PRINTMSGVERSION_1 { int PRINTMSG(string) = 1; } = 1; version PRINTMSGVERSION_2{ int PRINTMSG(string) = 1; } = 2; } = 0x2000001; Thanks for the reply. So what is the point in giving version numbers if I can distinguish by names. I mean if the names are same and version numbers are different then RPCGEN could have internally given them different names by appending the version numbers at the end instead of making it the job of the programmer to distinguish between the versions by giving different names and different versions. Your .x files are specification for the RPC protocol it gives you full control over the fundamentals (such as a program version number). Every vendor of the rpcgen program had to use the same algorithm to invent a version numer if it should be autogenerated and you'd have little chance of being able to reverse engineer an existing protocol that didn't have a .x file to begin with. Perhaps rpcgen could have appended the version in the .define but it doesn't it leaves it up to you which has some pros and some cons."
712,A,"EOF faults and keep alive I have a SOAP server process that fails with ""EOF"" errors. Does anybody know if EOF faults are related with keep alive timeouts? If yes how? My code is in C/C++ Can you give us the actual error you're seeing and the code that's reporting the error? Please ignore this topic. It is not generic behavior. This topic is bound to my software. If you feel it appropriate you should be able to delete your own question."
713,A,"TCP handshake process I have TCP client application and trying to connect with Server located at remote machine. I am able to connect it. when I Send Message Called Hello packet the Server should respond with data and time info. but to my surprise recv returns 0 at client. since I can't Debug code at Server. I am not sure but may be there is problem in encoding the message format hello packet at th client upon receiving the wrong packet server is clsoing the connection I wanted to confirm the meaning of following sequence is I got following info from wire shark  src IP------>dst ip SYN dst ip ----->src ip SYNACK src IP------>dst ip ACK src IP------>dst ip continuation or non http traffic ""Hello Packet"" dst ip------>ACK dst ip------>FIN ACK Does this means Server is closing the connection once it receives the hello packet? Yeah the FINACK sequence is sent by one of the entity connected when they want to close the connection http://www.freesoft.org/CIE/Course/Section4/11.htm"
714,A,Performance impact of using write() instead of send() when writing to a socket I am working on writing a network application in C++ on the Linux platform using the typical sockets API and I am looking at 2 alternative ways of writing a byte array to a TCP stream: either by calling write() or by calling send(). I know that since this is Linux the socket handle is simply a file descriptor and therefore it is valid to perform read() and write() calls on the socket however the sockets API also provides the send() and recv() functions to perform the same tasks. I am therefore wondering if there is any particular reason to choose one class of functions over the other - are the send/recv functions optimized for network writing/reading do they perform better etc? Or is it really arbitrary which functions I use? Do read() and write() behave properly in all cases? Thanks for any insights! recv and send allow you to specify flags such as for out-of-band packets. If you don't need to specify the flags read and write are perfectly adequate.  write v.s. send for a single byte or a single array probably won't be much different - they will soon end up the same code path (ultimately they perform the same operation). The overhead in network transmission is highly unlikely to be at that level; it will the actual TCP connection and moving the bits over wires. However if you are intending to send large multipart messages in a single go you should look at the sendmsg() syscall - this allows you to specify a list of non-contiguous arrays of data to send. At the end of the day the normal guidelines apply - write the application first then benchmark to see where your bottlenecks are. On non-contiguous arrays to receive/send: readv and writev also allow that. Yay for scatter/gather I/O!  There should be no difference. Quoting from man 2 send: The only difference between send() and write() is the presence of flags. With zero flags parameter send() is equivalent to write(). So long as you don't want to specify and flags for send() you can use write() freely. Is this true of FreeBSD or other platforms as well? - that line does not appear in my send(2) man page.
715,A,"How can I edit a js file sent by the server before it gets to my browser? During a normal browsing session I want to edit a specific javascript file before the browser receives since once it gets there it's impossible to edit. Is there are any tool for this? For what I need it I can't just save it and edit it on my disk. I'm ready to learn how to program it myself but if anyone can point out more or less what I have to do I'd be very grateful. I'd have to intercept the packets until I have the whole file while blocking the browser from receiving it any part of it then edit it manually and forward it to the same port. I don't think I can do this by just using pcap I've read a bit about scapy but I'm not sure if it can help me either. Thanks in advance. If you are happy to rather then editing a file replace it with a local one then I would* use Charles and its Map To Local function. Actually ""did"". This helped me debug a problem with a browser and a JS file I couldn't edit yesterday. I downloaded it I'll try it out thank you.  You can probably achieve whatever it is you are wanting to do by using the firefox firebug plugin chrome's development tools or the firefox greasemonkey plugin. Or you could enter the files domain into your hosts file and point that domain to your local machine (running a web server) edit & save that javascript file locally and serve it from your own web server.  You'd need to implement some sort of proxy or hook into an existing one and intercept the file as it's being downloaded and replace it. Not trivial for a beginner but a good learning project. +1. was about to suggest the proxy idea as well. check out http://www.parosproxy.org/index.shtml Thanks for your comment. I'd very much like to learn how to do this I've been googling it but there isn't much out there. Can you give any tips on where I should start: library tutorials etc?"
716,A,How do you programmatically get through a firewall to a SOAP server from a remote client? I have been looking at posts discussing SSH tunneling which sounds analogous to what I want to accomplish. However we are programming in Java and have a remote SOAP server behind a firewall. The server is running under Linux but the rest of the network might be either Linux or Windows or anything else for that matter. We tend to make a firewall exception and open that port but then you need to add more security to the webservice and make certain your webserver is secure. we (...) have a remote SOAP server behind a firewall If you are supposed to be able to access it then the logic would be that the firewall should be configured to allow incoming requests using the transport mechanism you're using. If it doesn't contact the network admins.  Simply put you don't. Your options are either to open a port in the firewall or to tunnel over a permitted port. There are packages like the one I like that will tunnel your data over a permitted protocol. http://www.jcraft.com/jhttptunnel/ This one allows you to tunnel over HTTP but you've got to be running it at both ends.
717,A,"Pinging Computer through specefic route I have a network of computers connected in form of a graph. I want to ping from one computer(A) to another computer(B). A and B are connected to each other through many different ways but I want to PING via only a particular edges only. I have the information of the edges to be followed during pinging available at both A and B. How should I do this? It seems that you need strict source routing for that purpose. See IP protocol options for more detail. @Vishal I think that you have a better chance to get an answer to this on serverfault I will try there too any ways I think strict source routing will serve the purpose @Shiraz not necessarily this looks like it might be a programming question...need more information from the poster... Well this is actually done by routing protocols that are configured on the media in between the computers (routers I expect). I think there isn't a way where you can say ""use that specific route"". The routers have different protocols (OSPF EIGRP RIPv2) and they do the load balancing. The only way you would be sure of one specific route is to use static routing but this isn't dynamically done where your computer decides the route. This is normal because : if you would be able to chose a route DoS would be quite easy to do to kill one route.  You could source route the ping but the return would choose its own path. Furthermore source-routed packets are often filtered due to security concerns. (Not always they are useful and sometimes even required at edge routers.) If the machines are under your local administrative control then you could ensure that source-routed packets are permitted. As long as you are able to start a daemon on machine B you could also easily enough design your own ping protocol that generates source-routed echo returns."
718,A,"Why is writing a closed TCP socket worse than reading one? When you read a closed TCP socket you get a regular error i.e. it either returns 0 indicating EOF or -1 and an error code in errno which can be printed with perror. However when you write a closed TCP socket the OS sends SIGPIPE to your app which will terminate the app if not caught. Why is writing the closed TCP socket worse than reading it? there's something else going on here that's fairly subtle: a TCP connection can be half closed meaning one side has closed the socket (sent a FIN packet) but the other side still has data to send. If you're going rooting around at this level please read: http://superuser.com/questions/298919/what-is-tcp-half-open-connection-and-tcp-half-closed-connection Think of the socket as a big pipeline of data between the sending and the receiving process. Now imagine that the pipeline has a valve that is shut (the socket connection is closed). If you're reading from the socket (trying to get something out of the pipe) there's no harm in trying to read something that isn't there; you just won't get any data out. In fact you may as you said get an EOF which is correct as there's no more data to be read. However writing to this closed connection is another matter. Data won't go through and you may wind up dropping some important communication on the floor. (You can't send water down a pipe with a closed valve; if you try something will probably burst somewhere or at the very least the back pressure will spray water all over the place.) That's why there's a more powerful tool to alert you to this condition namely the SIGPIPE signal. You can always ignore or block the signal but you do so at your own risk.  +1 To Greg Hewgill for leading my thought process in the correct direction to find the answer. The real reason for SIGPIPE in both sockets and pipes is the filter idiom / pattern which applies to typical I/O in Unix systems. Starting with pipes. Filter programs like grep typically write to STDOUT and read from STDIN which may be redirected by the shell to a pipe. For example: cat someVeryBigFile | grep foo | doSomeThingErrorProne The shell when it forks and then exec's these programs probably uses the dup2 system call to redirect STDIN STDOUT and STDERR to the appropriate pipes. Since the filter program grep doesn't know and has no way of knowing that it's output has been redirected then the only way to tell it to stop writing to a broken pipe if doSomeThingErrorProne crashes is with a signal since return values of writes to STDOUT are rarely if ever checked. The analog with sockets would be the inetd server taking the place of the shell. As an example I assume you could turn grep into a network service which operates over TCP sockets. For example with inetd if you want to have a grep server on TCP port 8000 then add this to /etc/services: grep 8000/tcp # grep server Then add this to /etc/inetd.conf: grep stream tcp nowait root /usr/bin/grep grep foo Send SIGHUP to inetd and connect to port 8000 with telnet. This should cause inetd to fork dup the socket onto STDIN STDOUT and STDERR and then exec grep with foo as an argument. If you start typing lines into telnet grep will echo those lines which contain foo. Now replace telnet with a program named ticker that for instance writes a stream of real time stock quotes to STDOUT and gets commands on STDIN. Someone telnets to port 8000 and types ""start java"" to get quotes for Sun Microsystems. Then they get up and go to lunch. telnet inexplicably crashes. If there was no SIGPIPE to send then ticker would keep sending quotes forever never knowing that the process on the other end had crashed and needlessly wasting system resources.  Usually if you're writing to a socket you would expect the other end to be listening. This is sort of like a telephone call - if you're speaking you wouldn't expect the other party to simply hang up the call. If you're reading from a socket then you're expecting the other end to either (a) send you something or (b) close the socket. Situation (b) would happen if you've just sent something like a QUIT command to the other end. @Robert: Yes it sounds like you've got it. But that doesn't really tell me why `write` or `send` can't just return an error directly the same way `read` or `recv` do. Why whack the app over the head with `SIGPIPE` like that? There's got to be some deeper reason for such an extreme response by the OS. Say I've got a socket that just received a `RST`. If I `read` it I get -1 with ECONNRESET why not just get the same thing when I write? In both cases I'm expecting to engage in consensual I/O and not getting what I expected. @Robert: The typical use case for pipe input and output on Unix was historically for ""filter"" programs that read from an input pipe and write to an output pipe (the `grep` program is such an example). In order to make such a filter terminate immediately when the output is no longer listening the `SIGPIPE` signal default behaviour was set to terminate the program. Without this the filter would continue writing to the output until its input was exhausted (which could be a while). Tell me if this sounds right: The real reason for `SIGPIPE` is that filter programs like grep typically write to `STDOUT` which may be redirected by the shell to a pipe. Since the filter program doesn't know and has no way of knowing that it's output has been redirected then the only way to tell it to stop writing to a broken pipe is with a signal since return values of writes to `STDOUT` are rarely checked. The analog with sockets would be `inetd` accepting a connection spawning the server and `dup2` ing the socket onto `STDIN``STDOUT``STDERR`!  I think a large part of the answer is 'so that a socket behaves rather similarly to a classic Unix (anonymous) pipe'. Those also exhibit the same behaviour - witness the name of the signal. So then it is reasonable to ask why do pipes behave that way. Greg Hewgill's answer gives a summary of the situation. Another way of looking at it is - what is the alternative? Should a 'read()' on a pipe with no writer give a SIGPIPE signal? The meaning of SIGPIPE would have to change from 'write on a pipe with noone to read it' of course but that's trivial. There's no particular reason to think that it would be better; the EOF indication (zero bytes to read; zero bytes read) is a perfect description of the state of the pipe and so the behaviour of read is good. What about 'write()'? Well an option would be to return the number of bytes written - zero. But that is not a good idea; it implies that the code should try again and maybe more bytes would be sent which is not going to be the case. Another option would be an error - write() returns -1 and sets an appropriate errno. It isn't clear that there is one. EINVAL or EBADF are both inaccurate: the file descriptor is correct and open at this end (and should be closed after the failing write); there just isn't anything to read it. EPIPE means 'broken PIPE'; so with a caveat about ""this is a socket not a pipe"" it would be the appropriate error. It is probably the errno returned if you ignore SIGPIPE. It would be feasible to do this - just return an appropriate error when the pipe is broken (and never send the signal). However it is an empirical fact that many programs do not pay as much attention to where their output is going and if you pipe a command that will read a multi-gigabyte file into a process that quits after the first 20 KB but it is not paying attention to the status of its writes then it will take a long time to finish and will be wasting machine effort while doing so whereas by sending it a signal that it is not ignoring it will stop quickly -- this is definitely advantageous. And you can get the error if you want it. So the signal sending has benefits to the o/s in the context of pipes; and sockets emulate pipes rather closely. Interesting aside: while checking the message for SIGPIPE I found the socket option: #define SO_NOSIGPIPE 0x1022 /* APPLE: No SIGPIPE on EPIPE */ So basically you're saying that `SIGPIPE` exists because lots of programmers ignore error codes in the case of write which could cause the process to hog system resources when it's not actually accomplishing anything? Or to put it another way people are more meticulous about checking their input than their output and that's the reason for the asymmetry in `read` and `write`? @Robert: yes basically. People tend to write their code on the assumption that the output device won't go away or run out of space. When the output is a pipe and the receiving program stops reading before the end of the output it is important to ensure that the writing program pays attention. And this is a simple mechanism that leaves programs simpler to write. So was there a time that pre-dated `SIGPIPE`? Since you're saying this is to certain extent a result of user / programmer bad behavior was there once a version of Unix that returned an error when writing to a closed pipe and then they changed it to return a signal or was `SIGPIPE` in there from the beginning in anticipation of bad behavior? @Robert S. Barnes: pipe was added fairly early to Unix. According to Dennis Ritchie in ""The Evolution of the UNIX Time-sharing System"" pipes were not available on the PDP-7 versions of UNIX and were added to the PDP-11 version in 1972 (2-3 years after the first versions of UNIX). (Ref: ""UNIX® SYSTEM: Readings and Applications Volume II"" 1987. This was a second edition of the AT&T (Bell) journal dedicated to Unix and contains some interesting stuff. It was published by Prentice-Hall as ISBN 0-13-939845-7 - http://www.amazon.com/Unix-System-Readings-Applications-UNIX-R/dp/0139398457)"
719,A,"Simple network-programming program - not working I just began learning network programming. My first program is very simple: once connected the server program displays what the client program sends (characters). Everything works fine until they get connected. At first I did the sending loop using putc: while((c = getc(stdin)) != '\n') putc(c (FILE *) connection); and the receiving one with getc: while((c = getc((FILE *) connection)) != '\n') putc(c stdout); Once connected I get a segmentation fault. Then I tried using send: while(1) { memset(str null strlen(memset)); while(((c = getc(stdin)) != '\n') && sizeof(str) <= 100) { strcat(str (char *) c); // cast to char * is to make gcc happy } send(connection (void *) str sizeof(str) (int) NULL); } and recv: while((stat = recv(connectionstr1000)) != 0) { printf(""\nReceived %d bytes of data from %s: %s"" stat client str); } Now recv() keeps returning -1 and errno is set to 107. What does this mean? Where am I doing wrong? By the way I'm using Linux with gcc. Thanks a lot for your help! :) EDIT: Even if I used getc() and putc() I get connection with socket() then I connect() (client). The server after obtaining connection (with socket()) bind()s then listen()s and accept()s. Sorry if I ometted. Here's the server code: #include <stdio.h> #include <string.h> #include <stdlib.h> #include <sys/types.h> #include <sys/socket.h> #include <errno.h> #include <arpa/inet.h> #include <netdb.h> void main(int argc char *argv[]) { struct addrinfo hint; struct addrinfo *servinfo *count; struct sockaddr_storage conn_addr; struct sockaddr host; int connessione; int stat; int conn_sock; int conn_size; int success; int c; char t[INET_ADDRSTRLEN]; char str[100]; char *client; if(argc != 1 && strcmp(argv[1] ""aiuto"") == 0) { printf(""%s(porta)\n""argv[0]); printf(""porta: specifica su quale porta installarsi\n""); exit(-1); } memset(&hint0sizeof hint); hint.ai_family = AF_INET; hint.ai_socktype = SOCK_STREAM; hint.ai_flags = AI_PASSIVE; if((stat = getaddrinfo(NULL argv[1] &hint &servinfo)) != 0) { printf(""\n[FATAL]: getaddrinfo: %s""gai_strerror(stat)); exit(-1); } success = 0; for(count = servinfo; count != NULL; count = count->ai_next) { if((connessione = socket(count->ai_family count->ai_socktype count->ai_protocol)) <= 0) continue; if((stat = bind(connessione count->ai_addr count->ai_addrlen)) == 0) { success = 1; break; } else { continue; } } if(success == 0) { printf(""\n[FATAL]: Unable to bind()!\n""); exit(-1); } else { printf(""\n[DEBUG]: %s: listening..."" argv[0]); } servinfo = count; freeaddrinfo(count); if(listen(connessione 20) == -1) { printf(""\n[FATAL]: listen: %s (%d)\n"" gai_strerror(errno) errno); close(connessione); exit(-1); } conn_size = sizeof(conn_addr); if(accept(connessione (struct sockaddr *) &conn_addr &conn_size) == -1) { printf(""\n[FATAL]: accept: %s"" gai_strerror(errno)); close(connessione); exit(-1); } client = (char *) malloc(INET_ADDRSTRLEN * sizeof(char)); client = inet_ntop(servinfo->ai_family &conn_addr t INET_ADDRSTRLEN); printf(""\n[DEBUG]: %s: connection accepted: %s"" argv[0] client); while((stat = recv(connessionestr1000)) != 0) { printf(""\nReceived %d bytes of data from %s: %s"" statclient str); } printf(""\n[DEBUG]: connection closed by %s\n""client); } And the client's code: #include <stdio.h> #include <string.h> #include <stdlib.h> #include <assert.h> #include <sys/types.h> #include <sys/socket.h> #include <errno.h> #include <arpa/inet.h> #include <netdb.h> void main(int argc char *argv[]) { struct addrinfo hint; struct addrinfo *servinfo *count; struct sockaddr_storage conn_addr; struct sockaddr host; int connessione; int stat; int conn_sock; int conn_size; int success; int c; char t[INET_ADDRSTRLEN]; char *indirizzo; char str[100]; memset(&hint0sizeof hint); host.sa_family = AF_INET; if(inet_pton(host.sa_family argv[1] (void *) host.sa_data) == -1) { printf(""\n[FATAL]: pton: %s (%d)\n"" gai_strerror(errno) errno); exit(-1); } hint.ai_family = AF_INET; hint.ai_socktype = SOCK_STREAM; hint.ai_flags = AI_PASSIVE; hint.ai_addr = &host; if((stat = getaddrinfo(argv[1] argv[2] &hint &servinfo)) != 0) { printf(""[FATAL]: getaddrinfo: %s\n""gai_strerror(stat)); exit(-1); } success = 0; for(count = servinfo; count != NULL; count = count->ai_next) { if((connessione = socket(count->ai_family count->ai_socktype count->ai_protocol)) == -1) continue; if(connect(connessione count->ai_addr count->ai_addrlen) != -1) { success = 1; break; } } if(success == 0) { printf(""\n[FATAL]: impossibile trovare l'host specificato\n""); exit(-1); } servinfo = count; freeaddrinfo(count); indirizzo = (char *) malloc(INET_ADDRSTRLEN * sizeof(char)); indirizzo = inet_ntop(servinfo->ai_family &conn_addr t INET_ADDRSTRLEN); printf(""\n[DEBUG]: %s: connesso a: %s\n""argv[0] indirizzo); while(1) { strcpy(str""buffer for data to send""); while(((c = getc(stdin)) != '\n') && sizeof(str) <= 100) { strcat(str (char *) c); } send(connessione (void *) str sizeof(str) (int) NULL); } } At the end... It works perfectly! Thanks everyone. :) How do you do network programming with functions that expect a file handle? what is the value of 'connection'? Essentially you should be using 'socket/send/recv'. 107 on Linux is ""ENOTCONN"". The code 'getc((FILE *) connection))' looks suspicious: what's the type of 'connection'? int. obtained with socket(). When you use a cast (like (FILE *) connection) you're telling the compiler: ""trust me this is the truth"". When you lie to the compiler it believes you. You have lied to the compiler. connection is not a pointer to a FILE structure. A segmentation fault is the nicest result you can get because it tells you something is badly wrong. This makes sense. Thanks. +1  Normally you don't use a FILE * handle to do network programming. Rather you need a socket descriptor returned by the socket(2) system call. Once you have it you can either connect(2) it (the client) or bind(2) it (the server). The ongoing communication can be performed using recv(2) and send(2). If you really want to use a FILE * handle to do network programming take a look at the fdopen(3) library function. It takes a descriptor that could be returned by socket(2) and returns a FILE * handle that is compatible with fread / fwrite / fprintf. After posting your full code the most significant error that I could see was: if (accept(connessione (struct sockaddr *) &conn_addr &conn_size)) accept(2) returns a socket which you have to use with recv(2) / send(2) and you discard it using the server socket instead. Look at my edit. I forgot that. I did but it doesn't work anyway. Post your code including the initialization calls and that might help us spot something. Thank you very much. =)  I suggest You reading http://beej.us/guide/ This is a very nice guide to socket programming I studied right on that! =) and where did you get that from?: putc(c (FILE *) connection); That's mine. I tried with it because I wanted to send a character a time it looked simpler to me. @BlackBear: Do you have a clue what happens when you cast `(FILE *) connection`?  You make some programming errors: 1) memset(str null strlen(memset)); should be : memset( str 0 sizeof(str) ); // if str is declared as char str[100]; 2) while(((c = getc(stdin)) != '\n') && sizeof(str) <= 100) shhould be : while(((c = getc(stdin)) != '\n') && (strlen(str) <= 100)) 3) send(connection (void *) str sizeof(str) (int) NULL); should be : send(connection (void *) str strlen(str) (int) NULL); Thanks. I'll fix. =)"
720,A,"What does concurrent requests really mean? When we talk about capacity of a web application we often mention the cocurrent requests it could handle. As my another question discussed Ethernet use TDM (Time Division Multiplexing) and no 2 signls could pass along the wire simutaneously. So if the web server is connected to the outside world through a Ethernet connection there'll be literally no cocurrent requests at all. All requests will come in one after another. But if the web server is connected to the outside world through something like a wireless network card I believe the multiple signals could arrive at the same time through the electro-magnetic wave. Only in this stituation there're real concurrent requests to talk about. Am I right on this? Thanks. By ""concurrent requests"" we don't mean that they *arrived* simultaneously just that they're processed simultaneously. Although having multiple network interfaces could allow for requests to actually arrive at the same time. It's true that no two packets can arrive at the exact same time (unless multiple network cards are in use per Gabe's comment). However web request usually requires a number of packets. The arrival of these packages is interspersed when multiple requests are coming in at near the same time (whether using wired or wireless access). Also the processing of these requests can overlap. Add multi-threading (or multiple processors / cores) to the picture and you can see how lengthy operations such as reading from a database (which requires a lot of waiting around for a response) can easily overlap even though the individual packets are arriving in a serial fashion. Edit: Added note above to incorporate Gabe's feedback.  I imagine ""concurrent requests"" for a web application doesn't get down to the link level. It's more a question of the processing of a request by the application and how many requests arrive during that processing. For example if a request takes on average 2 seconds to fulfill (from receiving it at the web server to processing it through the application to sending back the response) then it could need to handle a lot of concurrent requests if it gets many requests per second. The requests need to overlap and be handled concurrently otherwise the queue of requests would just fill up indefinitely. This may seem like common sense but for a lot of web applications it's a real concern because the flood of requests can bog down a resource for the application such as a database. Thus if the application has poor database interactions (overly complex procedures poor indexing/optimization a slow link to a database shared by many other applications etc.) then that creates a bottleneck which limits the number of concurrent requests the application can handle even though the application itself should be able to handle them."
721,A,"Java RMI Resources Hi everyone I am currently undertaking a project that involves extensive use of Java RMI and I was wondering if anyone is aware of any good resources about it. The problem I am having with the material I am finding currently is that its usually quite out of date (like Java 1.3) and / or half complete. I would even be happy to buy a book on it but looking on Amazon all the books are like 7 years old. So if anyone is aware of any good resources books or good example implementations I would be very interested to hear about them. Thanks in advance for your help. In addition to the ones already mentioned the article Understanding Java RMI Internals was quite decent IMHO going a little bit through the internals.  The O'Reilly RMI book is pretty good. Go for it. Translation? Every context is a subcontext unless it's at the top of the tree - no parents. An quotation from te book ""A context is a subcontext of another context if it is either a direct subcontext or a subcontext of a context that is a subcontext of the containing context"".  The Java.RMI has changed very little over the years so most of the old documentation can still be referenced. Note that one significant change is the need to compile the RMI stubs if you are using a version of Java 5.0. Most people have moved away from RMI and have embraced River (previously called Jini) for distributed systems. If you are still thinking of moving forward with RMI then I would suggest reading the Oracle documentation or posting your questions on their forums. As for books… Java RMI by William Grosso or Java Network Programming by Elliotte Harold. Jini and RMI are compltely different layers. Jini usually uses RMI as its communications protocol and AFAIK it's anything but widely adopted. JINI doesn't specify _anything_ about a communications protocol. that is it's best feature. Using JINI over RMI would work but is a lot less interesting than over run-time networks.  Thank you all for your answers I think what people said about RMI not having changed much is correct however the tutorials could still be a bit better they gloss over some important points. In the end the best book by far that I found that covers some of the really good bits of RMI such as Activation was Java Network Programming and Distributed Computing. I did look at the other O'reilly Java RMI book and in my opnion its not very good at all for anything bigger than the smallest of RMI projects.  Absolutely invaluable article on multi homed RMI (hosts with multiple IPs) Explains RMI in a very easy to understand way and goes into the issues of hosting registries on machines with more than a single IP address and how private IPs can be dealt with too. Saved my bacon.  RMI hasn't changed that much. I think 1.3 era books will be just fine. I tend to agree.  RMI Hello World looks nice for a start. Of course it's still a simple example so maybe tips on RMI performance/scalability will be useful since you're already familiar with RMI. Cheers for this one tips on RMI performance/scalability looks very promising.  If you are going to make heavy use of RMI I would suggest having a look at Spring Remoting. It helps a lot in abstracting the remoting protocol help you switch remoting implementation later if you need to (for example switch to Hessian or SOAP). One thing to keep in mind if you use RMI or any other remote object protocol is that you can generate a lot of traffic by calling methods on remote objects. It might not be obvious in your code that those objects are remote. It might generate performances problems. If you can I would advise you to have a architecture more or less service oriented where you call remote services to get data object but not to have too much behaviour on those objects ...  Have you tried Sun't RMI tutorial? The Sun tutorials are all very good and they're usually the first place I start for learning anything about Java. A book that we used in school that has good example code is J2EE Developer's Handbook. Bear in mind that this is a huge reference book of about 1500 pages with only one chapter (about 50 pages) on RMI."
722,A,"Why DHCP client listens on port 68? If suppose client does not listen on 68 portwhen DHCP server receives the request it can send it to the address from where it received request (with ephemeral port chosen by client at time of sending) then why does protocol specifies client to be listening on port 68? The main reason is that the DHCP server might broadcast the ""DHCP offer"" on the mac level instead of sending it unicast to the mac address it had received the request. If the port wasn't constant some hosts that are listening by chance to the this same random port will accept the packet to layer 5 - the application layer. In other words an application will get message from completely different application not an healthy situation. This answer doesn't seem different than the one from 3+ years ago. If you want to add something consider suggesting an edit to the existing accepted answer :) Thanks this is actually the correct answer.  Because it's in the RFC (Request for Comments) that specifies how DHCP behaves. RFC 2131 is the document that specifies how a DHCP client and server must behave. See here for more info on DHCP (section 4.1 in particular). See here for info on what the RFCs are. The question reads ""why does protocol specifies client to be listening on port 68?"" so what they are asking seems to be more ""why does the RFC say that?"" DHCP is based on the earlier BOOTP protocol which used ports 67 (server) and 68 (client). Why BOOTP used those ports was probably because they were unused by any of the other protocols at the time (SMTP uses 25 FTP uses 21 etc.). While a host usually has a single IP address it can have thousands of ports. By assigning particular port numbers to specific protocols it was possible for multiple parties to develop standard services and clients. As long as you listened on the right port you could write your own DHCP client or server. Ok means there is no technical reason? The technical reason is that if you want your client to be able to hear the response from any normal DHCP server you need to listen on the right port. I guess if you were writing your own DHCP server/client software (or you could configure your server to use a different port) then you could use whatever port you wanted as long as you were not trying to use a port already being used by some other protocol."
723,A,"problem sending image file over the socket in c I am writing a C based server supporting HTTP. &buffer[5] has the file name. I set the header in next step and then send the file in block of 8Kb. It is working fine when I am requesting a txt file but when I request a jpg file it is not giving any o/p. Is there any specific care I need to take for image file or there is some other problem. void web(int fd) { int j file_fd; int buflen; int len; long i ret; char * fstr; static char buffer[BUFSIZE+1]; /* static so zero filled */ ret =read(fdbufferBUFSIZE); /* read Web request in one go */ if(ret == 0 || ret == -1) { /* read failure stop now */ //log1(SORRY""failed to read browser request""""""fd); } if(ret > 0 && ret < BUFSIZE) /* return code is valid chars */ buffer[ret]=0; /* terminate the buffer */ else buffer[0]=0; for(i=0;i<ret;i++) /* remove CF and LF characters */ if(buffer[i] == '\r' || buffer[i] == '\n') buffer[i]='*'; log1(LOG""request""buffer1); if( strncmp(buffer""GET ""4) && strncmp(buffer""get ""4) ) log1(SORRY""Only simple GET operation supported""bufferfd); for(i=4;i<BUFSIZE;i++) { /* null terminate after the second space to ignore extra stuff */ if(buffer[i] == ' ') { /* string is ""GET URL "" +lots of other stuff */ buffer[i] = 0; break; } } for(j=0;j<i-1;j++) /* check for illegal parent directory use .. */ if(buffer[j] == '.' && buffer[j+1] == '.') log1(SORRY""HTTP/1.0 400 Bad Request: Invalid URI: \n Parent directory (..) path names not supported""bufferfd); if( !strncmp(&buffer[0]""GET /\0""6) || !strncmp(&buffer[0]""get /\0""6) ) /* convert no filename to index file */ (void)strcpy(buffer""GET /index.html""); /* work out the file type and check we support it */ buflen=strlen(buffer); fstr = (char *)0; for(i=0;extensions[i].ext != 0;i++) { len = strlen(extensions[i].ext); printf(""%s\t%s\t%d\n"" &buffer[buflen-len]extensions[i].ext strncmp(&buffer[buflen-len] extensions[i].ext len)); if( !strncmp(&buffer[buflen-len] extensions[i].ext len)) { fstr =extensions[i].filetype; break; } } if(fstr == 0) log1(SORRY""HTTP/1.0 400 Bad Request: Invalid URI:\n file extension type not supported""bufferfd); printf(""Buffer Value: %s\n""&buffer[5]); if(( file_fd = open(&buffer[5]O_RDONLY)) == -1) /* open the file for reading */ { log1(SORRY ""HTTP/1.0 404 Not Found\n failed to open file""&buffer[5]fd);} printf(""file Descrptr:%d\n""file_fd); log1(LOG""SEND""&buffer[5]1); (void)sprintf(buffer""HTTP/1.0 200 OK\r\nContent-Type: %s\r\n\r\n"" fstr); (void)write(fdbufferstrlen(buffer)); printf(""Buffer Header:%s\n""buffer); //int rec = read(file_fd buffer 10*BUFSIZE); /* send file in 8KB block - last block may be smaller */ while ( (ret = read(file_fd buffer BUFSIZE)) > 0 ) { (void)write(fdbufferret); printf(""BufferData:%d\n""strlen(buffer)); } #ifdef LINUX sleep(1); /* to allow socket to drain */ #endif exit(1); } o/p for jpg file: server: got connection from 127.0.0.1 Buffer Header:HTTP/1.0 200 OK Content-Type: image/jpeg BufferData:4 BufferData:95 BufferData:122 BufferData:24 BufferData:217 for txt file: Buffer Value: config.txt file Descrptr:4 Buffer Header:HTTP/1.0 200 OK Content-Type: text/plain BufferData:416 You probably need to specify the correct Content-Type in the HTTP header. It isn't just 'text' any more. HTTP response lines are supposed to end with CRLF - read the standard. Most internet protocols require this line ending in fact. You haven't specified a Content-Encoding; you haven't specified the length of the data. I think you're supposed to do both. Why don't you send a request to a web server that handles requests correctly for a small JPEG file and see what it generates. You'd check that the URL works in a browser showing you the image then you'd use a low-level program (possibly even telnet web.server.example.com 80) to send the request and collect the response. Compare that with what your code generates. Or you could read the standard. You should check that the number of bytes written was the number you intended to write(). If you get a short write you should try again to write the rest - so you need a loop around your write (or a function that contains the loop). You should also report errors if an error is reported to you (by write() for instance). Here's a shell script that sends a request to a particular server and collects a 368-byte gif: { echo ""GET /img/mailtruck.gif HTTP/1.0^M"" echo ""^M"" } | tee request | nc my.earthlink.net 80 The '^M' characters are carriage returns; I used CONTROL-V and CONTROL-M to enter those. The program nc (netcat) sends the request to the server. The response comes back starting with: HTTP/1.1 200 OK Cache-Control: Tue 25 Jan 2011 16:34:03 GMT ETag: W/""368-1219455793000"" Last-Modified: Sat 23 Aug 2008 01:43:13 GMT Content-Type: image/gif Content-Length: 368 Date: Tue 25 Jan 2011 08:34:03 GMT Server: Apache-Coyote/1.1 Connection: Keep-Alive The full response in a hex-dump format is: 0x0000: 48 54 54 50 2F 31 2E 31 20 32 30 30 20 4F 4B 0D HTTP/1.1 200 OK. 0x0010: 0A 43 61 63 68 65 2D 43 6F 6E 74 72 6F 6C 3A 20 .Cache-Control: 0x0020: 54 75 65 2C 20 32 35 20 4A 61 6E 20 32 30 31 31 Tue 25 Jan 2011 0x0030: 20 31 36 3A 33 32 3A 35 39 20 47 4D 54 0D 0A 45 16:32:59 GMT..E 0x0040: 54 61 67 3A 20 57 2F 22 33 36 38 2D 31 32 31 39 Tag: W/""368-1219 0x0050: 34 35 35 37 39 33 30 30 30 22 0D 0A 4C 61 73 74 455793000""..Last 0x0060: 2D 4D 6F 64 69 66 69 65 64 3A 20 53 61 74 2C 20 -Modified: Sat 0x0070: 32 33 20 41 75 67 20 32 30 30 38 20 30 31 3A 34 23 Aug 2008 01:4 0x0080: 33 3A 31 33 20 47 4D 54 0D 0A 43 6F 6E 74 65 6E 3:13 GMT..Conten 0x0090: 74 2D 54 79 70 65 3A 20 69 6D 61 67 65 2F 67 69 t-Type: image/gi 0x00A0: 66 0D 0A 43 6F 6E 74 65 6E 74 2D 4C 65 6E 67 74 f..Content-Lengt 0x00B0: 68 3A 20 33 36 38 0D 0A 44 61 74 65 3A 20 54 75 h: 368..Date: Tu 0x00C0: 65 2C 20 32 35 20 4A 61 6E 20 32 30 31 31 20 30 e 25 Jan 2011 0 0x00D0: 38 3A 33 32 3A 35 39 20 47 4D 54 0D 0A 53 65 72 8:32:59 GMT..Ser 0x00E0: 76 65 72 3A 20 41 70 61 63 68 65 2D 43 6F 79 6F ver: Apache-Coyo 0x00F0: 74 65 2F 31 2E 31 0D 0A 43 6F 6E 6E 65 63 74 69 te/1.1..Connecti 0x0100: 6F 6E 3A 20 4B 65 65 70 2D 41 6C 69 76 65 0D 0A on: Keep-Alive.. 0x0110: 0D 0A 47 49 46 38 39 61 25 00 21 00 B3 08 00 FD ..GIF89a%.!..... 0x0120: 9C 00 DE 36 03 58 44 29 19 15 04 D2 D3 D3 F1 85 ...6.XD)........ 0x0130: 66 A4 A4 A3 FE FE FE FE FE FE 00 00 00 00 00 00 f............... 0x0140: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 21 ...............! 0x0150: F9 04 01 00 00 08 00 2C 00 00 00 00 25 00 21 00 ...........%.!. 0x0160: 00 04 FF 10 C9 49 AB BD 38 EB 49 84 27 5B 78 19 .....I..8.I.'[x. 0x0170: 1E 60 7A 86 28 76 82 E9 9E 02 A8 8E E5 6B 03 E8 .`z.(v.......k.. 0x0180: CC D5 77 FF A9 A4 56 6F E8 12 84 58 C4 E4 69 43 ..w...Vo...X..iC 0x0190: 52 26 03 81 C1 A5 83 20 0C 9C 43 28 54 6A B1 22 R&..... ..C(Tj."" 0x01A0: 3C 03 21 56 5B 28 6C A7 03 EB 60 CD CB 06 08 E5 <.!V[(l...`..... 0x01B0: F2 B9 0B 5E B3 C3 6E F3 5B 1F CD D8 FF 6C 62 00 ...^..n.[....lb. 0x01C0: 64 7A 70 5A 5C 16 02 77 80 61 2D 01 02 5A 04 01 dzpZ\..w.a-..Z.. 0x01D0: 07 01 72 73 15 4D 51 8A 8C 26 6B 50 66 72 96 7D ..rs.MQ..&kPfr.} 0x01E0: 89 2F 60 9B 6B 38 9E 7B 7A 83 87 74 37 8B 2D 9B ./`.k8.{z..t7.-. 0x01F0: 50 86 50 AD 97 13 82 36 9B A9 61 B4 7C 66 A7 32 P.P....6..a.|f.2 0x0200: 12 4D 44 01 83 76 9F 96 7A 8D 6B C3 06 C7 4A 51 .MD..v..z.k...JQ 0x0210: 5A 95 05 70 7C 03 06 24 69 13 DA 04 4F D4 BF 95 Z..p|..$i...O... 0x0220: 87 69 DE 14 DA 41 6E E1 E1 1E ED 31 98 50 EE 36 .i...An....1.P.6 0x0230: EB EC EF 55 06 C3 12 56 05 00 05 EE 62 D4 20 1D ...U...V....b. . 0x0240: B2 A7 A1 83 36 13 FE 3E 48 22 37 40 53 BE 0D 2C ....6..>H""7@S.. 0x0250: B4 F1 F3 67 80 8F 22 16 0F 81 A0 30 60 E2 98 16 ...g..""....0`... 0x0260: 45 07 1D 8C E8 A0 23 40 E2 20 61 22 47 5A 08 82 E.....#@. a""GZ.. 0x0270: 2E 46 44 95 19 58 94 84 39 43 1B CD 9B 30 23 00 .FD..X..9C...0#. 0x0280: 00 3B .; 0x0282: The request is minimal; the response probably is not but you'll have to experiment with what you have to send. You could play with the URL http://sstatic.net/stackoverflow/img/apple-touch-icon.png if you prefer. Guys thanks a lot for your reviews and answers......it really helped me a lot. The two important things which were problem were: 1. the content length was not given...I used stat.h and put content length in the header. 2. there was formatting problem in the Content-Type. as I was reading type from a config file I was not taking care of removing '\n'  and that format didn't matched with HTTP std..........Thanks again for pointing such things to me......:) @Jonathan...as u can see on the o/p I printed the HTTP header looks alright. I am getting problem only with .jpg files......is there any scope if I increase my Buffsize from 8Kb to higher value.  If your code is working for txt files give fread/fwrite a try instead of read/write . There might be a problem with NULL characters in jpg image which are not present in txt files. And do include a content length in header otherwise in some cases your browser will keep on requesting even if your download has finished. In simple words browser don't know when to stop."
724,A,"Distributed network monitoring - how to tell if the monitored resource has fallen over or the monitor it's self is at fault I'm building a system for monitoring several large web sites (resources) using distributed web services controlled by a central controller. I'm coming to a specific part of the design - the actual reporting of resources that are thought to have fallen over. My problem is that there is always the chance that the actual monitor it self is at fault or has lost its network connection to a resource and the resource is actually fine. I don't want to report issues if they are not really there. My plan at the moment is to have the monitor request that all other monitors check the resource if it encounters a problem and then make a decision as to whether the resource has really fallen over based on collective results. I'm sure there's someone out there with more experience of this type of programming than myself. Is there a common solution to this type of problem? Is my solution a decent way of looking at this? Quis custodiet ipsos custodes? (Who will watch the watchers?) -- Juvenal ""Satires""  Your solution is about one of the only pragmatic ones. There is nothing new under the sun. The IETF Routing Information Protocol wasn't the first attempt at addressing this problem but it is well documented and works. Note well that there is no optimal (or perfect) solution to the class of problems which you are facing the best you can do with in-band monitoring is make good guesses about where the fault is. In systems that need a very high degree of accuracy of fault information (e.g. the public switched telephone network) a parallel out-of-band monitoring network is established which itself must necessarily be monitored by humans. Thanks. That's a reassuring answer even if the solution is tricky!"
725,A,Linux kernel network driver. How do I deal with transmit errors after hard_start_xmit() has returned success? In a linux network driver I must provide a function hard_start_xmit() that actually sends packets on the wire. I understand that if it can't send the packet hard_start_xmit() should return an error which will cause the packet to be retried later. However since hard_start_xmit() may be called at with IRQs disabled it cannot wait very long to determine whether the packet could be sent. How do I deal with a transmission error that happens after hard_start_xmit() has already returned success? Is it correct to simply drop the packet free the skb and count a transmit error? Yes. Many transmit errors are only detectable after the NIC has actually tried to transmit the frame. Note that there are several different error counters that you can increment if your device returns sufficient information on the error - see struct net_device_stats.
726,A,Are IP-addreses classful or classless? Given an IP datagram how can one elucidate whether the IP address' are classful or classless? Is this possible? If so how? how does this relate to programming? writing an program to acquire information about a network from captured packets. Frankly I kind of expected a close but was hoping an answer or two would sneak in before the door slammed shut. networking questions are more welcome on ServerFault They are all classless these days. Yeah i figured. So for classless addressing how could the subnet mask be determined given a datagram? You can't. Unless it's local and you can query the router (ICMP netmask). Or look at the configs.
727,A,"Most seen newbie mistakes in multiplayer/online game programming? What kind of newbie mistakes you've seen and what are the cures? One which occurs again and again is client is not checked any way against server. For example: User decompiles flash game source or listens to network traffic and sees where high score data is going and sends bogus high scores there not even playing the game. User uses trainer and gets item which may even not appear in current level. This sent to server like ""client X got item Y"" and server just accepts that. The simple cure is of course handling gaming client only as API to the server. Then user can use trainers and other memory manipulations as much they like but server just says you can't do it. Think server as a database where you can query things with game rules on top of it. For example Client: starts game Client: connects to server Client: queries amount of available money from server User: enables trainer which sets money to infinite Client: server.buyItem('very expensive') Server: Checks gamestate (user can buy things now). Checks player[0].money -> no bonus. Client: server.buyItem('can get this') Server: Checks gamestate (user can buy things now). Checks player[0].money ok. player[0].items.add('can get this') which will reduce it's cost from player[0].money. Then inform client send(player[0] 'items' 'can get this'); send(player[0] 'money' player[0].money). The other way is to record client's movements and send that to highscore server where server plays it. Of course this can lead to that that record is very big. So this is your blog now? +1 - maybe move the second part of your question into an answer? Is there some subtext I'm missing as to why this is an invalid question? Closers and downvoters please clue me in... Its a wide open question with not single true answer. IE. Its subjective There's no question. That's why I voted to close. The question is at the beginning of the post and in the topic. Looks like a completely valid question to me. This reads like a blog post that's framed around a question. Awesome question You guys have pretty much covered all that can go wrong with server trusting clients. There are no other real issues I can think of. So instead of telling you what can go wrong look at what can go right. Valve has put a lot of work into their netcode. Read it up Actually Valve are one of the main culprits for 'trusting the client'. A player who is in cover according to the server and observers can still be shot by another player if he's visible on that player's client. Valve prefer this for their games but it's not a universally good solution. The server has the final say but it relies on 'rewinding time' based on estimates of lag which can ultimately be manipulated by the client. This gives a better feel to the shooter at the expense of consistency. (http://developer.valvesoftware.com/wiki/Lag_Compensation) Uhh nope. The server has the final say in all cases. If what you say it true then it would be possible to create a hack to kill people without ever seeing them. rewinding of time is still done on the server so while you could receive damage from around the corner that's because the server as calculated that the client did hit you in the past. Its not manipulable by the client this is to reduce lag not trust the client And while its true clients can induce lag to make this bullets around corners effect more apparent it doesn't give them any advantage to do so. they could have just sent the same information earlier and killed the player faster.  The approach Valve took (at least at one point) was to have the clients and the server simulate the game independently. Then the server does the authoritative simulation and sends status updates to all of the clients to correct their mistakes/hacking attempts. E.g. if you press the left arrow your character will move left immediately; there is no wait for the server to say 'OK'. But if it turns out you've hacked yourself through a wall the next server update will make you appear right next to wall as the server sees it as solid. Similarly if a client sees a character move forwards it will assume it is going to continue moving forwards until the server comes back with an authoritative response. This approach defeats hacking attempts as the server makes the main decisions (and performs sanity checks in the process of simulation) and also deals with lag as the clients make predictions until they get word from the server. Another big mistake related to 'trusting the client' is thinking your network protocol cannot be faked. People tend to assume that if they send binary chunks to the server that these binary chunks will never be reverse-engineered and that nobody will attempt to mess with the data to see what happens. This has led to all sorts of problems.  Without a doubt blind trust of the client. In a game I'm working on we now keep all ""business logic"" server-side and have the client machines only send us what commands they are making; for instance ""Player B wants to move right"" - but the server calculates just how far to the right they moved. This has a performance overhead (and of course issues with lagging which could be handled better) so a possible middle-ground could be to do the heavy calculations client-side and still have checks in place on the server; for instance checking whether the client's player is moving more than is supposedly possible in the time between updates; i.e. if the max player speed is 200 units/second if you get an update after 0.5 seconds saying that they moved 150 units boot them. Of course this doesn't necessarily stop someone from coding a bot to send those key presses so there are other ways to guard against this. Still having no validation at all is very much a newbie mistake (which admittedly I was guilty of when I took shortcuts) +1 - better said than my answer. =) One way to handle your example of pure server-side player movement is through client-side prediction code though that is tough to nail properly. Client-side prediction code hey? Can't say I've heard of it. Have you a link to an explanation perchance? I am intrigued =) It's called interpolation and it's used by pretty much all modern FPS engines. (It's adjustable in the Source engine via cl_interp for example.)"
728,A,"Socket programming in C Ok so I'm trying to get some UDP code working and I'm barely green when it comes to network programming using C. I'm using the sample file from here Basically I'm just listening for incoming UDP packets on a given port and then I want to send some data back the same way. Below is the relevant part. At this point the socket is set up and bound to a port of choice and awaits incoming packets: printf(""GSProxy: waiting to recvfrom...\n""); addr_len = (socklen_t) sizeof their_addr; if ((numbytes = recvfrom(sockfd buf MAXBUFLEN-1  0 (struct sockaddr *)&their_addr &addr_len)) == -1) { // argument 6 gives a warning but is correct int perror(""recvfrom""); exit(1); } printf(""GSProxy: got packet from %s\n"" inet_ntop(their_addr.ss_family get_in_addr((struct sockaddr *)&their_addr) s sizeof s)); printf(""GSProxy: packet is %d bytes long\n"" numbytes); buf[numbytes] = '\0'; printf(""GSProxy: packet contains \""%s\""\n"" buf); char retmsg[] = ""Hello!""; if ((numbytes = sendto(sockfd retmsg 7 0 (struct sockaddr *) &their_addr &addr_len)) == -1) { perror(""GSPProxy: sendto""); exit(1); } printf(""GSProxy: Sent %i bytes.\n"" numbytes); I just want to send the ""Hello!"" string right back to the sender. This fails with Error ""GSPProxy: sendto: File name too long"". Which is the [ENAMETOOLONG] error code as far is I can tell. But... what the ** does it mean? What file? What is too long? Is it that I can't reuse the socket to send data back or have I just made another newb mistake? Best regards Erm at a minimum the '&addr_len' you passed in sendto probably should have not been passed by address: ie: should have just been addr_len.  You should not be passing the address of the socket structure length to sendto() - it requires the actual length (i.e. ""addr_len"" not ""&addr_len""). The reason you pass the address of the length to recvfrom() is that it is changed by that function if the real address happens to be shorter. In other words replace: if ((numbytes = sendto (sockfd retmsg 7 0 (struct sockaddr *) &their_addr &addr_len)) == -1) { with: if ((numbytes = sendto (sockfd retmsg 7 0 (struct sockaddr *) &their_addr addr_len)) == -1) {"
729,A,Why would recvfrom() under report the packet size? I am working on a Linux server that listens for UDP messages as part of a discovery protocol. My code for listening follows:  rcd = ::select( socket_handle + 1 &wait_list 0 // no write 0 // no error &timeout); if(rcd > 0) { if(FD_ISSET(socket_handle&wait_list)) { struct sockaddr address; socklen_t address_size = sizeof(address); len = ::recvfrom( socket_handle rx_buff max_datagram_size 0 // no flags &address &address_size); if(len > 0 && address.sa_family == AF_INET) { struct sockaddr_in *address_in = reinterpret_cast<struct sockaddr_in *>(&address); event_datagram_received::cpost( this rx_buff rcd ntohl(address_in->sin_addr.s_addr) ntohs(address_in->sin_port)); } } } In the meantime I have written a windows client that transmits the UDP messages. I have verified using wireshark that the messages are being transmitted with the right format and length (five bytes). However when I examine the return value for recvfrom() this value is always one. The size of my receive buffer (max_datagram_size) is set to 1024. The one byte of the packet that we get appears to have the correct value. My question is: why am I not getting all of the expected bytes? In case it matters my Linux server is running under Debian 5 within a VirtualBox virtual machine. Where in the code do you see the length of 1 ? Your `event_datagram_received::cpost` doesn't seem to deal with the `len` variable ? is `max_datagram_size` 1 or 5? Can you show how you are setting up the socket in both the sender and the receiver ? The comment made by nos is spot on and I feel really goofy. I was using the return value from select() rather than the return value of recvfrom(). If that note were to be formed as an answer I would be happy to accept it. nos answered my question in the first comment. I was using the wrong variable to report the buffer length.
730,A,"Java NIO: Sending large messages quickly leads to truncated packets and data loss I've got this nasty problem where sending multiple large messages in quick succession from a Java (NIO) server (running Linux) to a client will lead to truncated packets. The messages have to be large and sent very rapidly for the problem to occur. Here's basically what my code is doing (not actual code but more-or-less what's happening): //-- setup stuff: -- Charset charset = Charset.forName(""UTF-8""); CharsetEncoder encoder = charset.newEncoder(); String msg = ""A very long message (let's say 20KB)...""; //-- inside loop to handle incoming connections: -- ServerSocketChannel ssc = (ServerSocketChannel)key.channel(); SocketChannel sc = ssc.accept(); sc.configureBlocking(false); sc.socket().setTcpNoDelay(true); sc.socket().setSendBufferSize(1024*1024); //-- later actual sending of messages: -- for (int n=0; n<20; n++){ ByteBuffer bb = encoder.encode(CharBuffer.wrap(msg+'\0')); sc.write(bb); bb.rewind(); } So if the packets are long enough and sent as quickly as possible (i.e. in a loop like this with no delay) then on the other end it often comes out something like this: [COMPLETE PACKET 1] [COMPLETE PACKET 2] [COMPLETE PACKET 3] [START OF PACKET 4][SOME OR ALL OF PACKET 5] There is data loss and the packets start to run together such that the start of packet 5 (in this example) arrives in the same message as the start of packet 4. It's not just truncating its running the messages together. I imagine that this is related to the TCP buffer or ""window size"" or that the server here is just providing data faster than the OS or network adapter or something can handle it. But how do I check for and prevent it from happening? If I reduce the length of message per use of sc.write() but then increase the repetitions I'll still run into the same problem. It seems to simply be an issue with the amount of data in a short amount of time. I don't see that sc.write() is throwing any exceptions either (I know that in my example above I'm not checking but have in my tests). I'd be happy if I could programmatically check if it is not ready for more data yet and put in a delay and wait until it is ready. I'm also not sure if ""sc.socket().setSendBufferSize(1024*1024);"" has any effect or if I'd need to adjust this on the Linux side of things. Is there a way to really ""flush"" out a SocketChannel? As a lame workaround I could try to explicitly force a complete send of anything that is buffered any time I'm trying to send a message of over 10KB for example (which is not that often in my application). But I don't know of any way to force a send of the buffer (or wait until it has sent). Thanks for any help! You are not checking the return value of: sc.write(bb); This returns the number of bytes written which might be less than the data available in your buffer. Because of how nio works you can probably just call remaining() on your bytebuffer to see if there are any left. Yep! It's not reasonable to expect to be able to send data over the network as fast as you can spew it. If you're not blocking you need to test the results of your writes.  There are many reasons why sc.write() would not send some or all of the data. You have to check the return value and/or the number of bytes remaining in the buffer. for (int n=0; n<20; n++){ ByteBuffer bb = encoder.encode(CharBuffer.wrap(msg+'\0')); if(sc.write(bb) > 0 && bb.remaining() == 0) { // all data sent } else { // could not send all data. } bb.rewind(); }  You are using the non-blocking mode: sc.configureBlocking(false); Set blocking to true and your code should work as it is. Suggestions made by others here to check send count and loop will also work.  Don't assume you have any control over what data ends up in which packet. More here: http://stackoverflow.com/questions/453609/whats-the-best-way-to-monitor-a-socket-for-new-data-and-then-process-that-data/453951#453951 It seems that ""sc.socket().setTcpNoDelay(true);"" will help to ensure that multiple messages are not combined into single packets. I'm not entirely familiar with that method call but help != guarantee which means the technique I've explained in the link above is more prudent anyway.  I haven't done any NIO programming but according to the Javadocs sc.write() will not write the entire ByteBuffer if the SocketChannel is in non-blocking mode (as yours is) and the socket's output buffer is full. Because you are writing so quickly it is very likely that you are flooding your connection and your network or receiver cannot keep up. I'd be happy if I could programmatically check if it is not ready for more data yet You need to check the return value of sc.write() to find out whether your output buffer is full."
731,A,C++ & Boost: I'm trying to find an example TCP program with a server that accepts connections from multiple clients A chat program would be a good enough example. Just need a server that can accept multiple connections from the clients and the server needs to be able to send messages to individual clients. I plan to turn this into a distributed computing program to work with multiple Neural Networks. I cannot give you an example progam. But to write a server things that you have to do: 1. server will listen at a port for connection 2. thread pool which will accept the connection and serve request 3. write the server code in thread safe manner You have to use socket programming A good link for that http://beej.us/guide/bgnet/ you can use win32 api in windows and posix for linux I know the basics I know I need to use sockets. Just... not with boost. =\ I have edited the post u dont have to use boost if you dont want to.  Asio is the Boost library that handles networking. There's a chat server example listed here.
732,A,"Get ip adresses on a lan with backgroundworker control I want to get alive or dead ip addresses from a big lan. but it takes so many times. I decided to use backgroundworker here is my code: try { this.Status.Text = ""Collecting Information...""; if(this.TxtWorkGroup.Text.Trim() == """") { MessageBox.Show(""The Work Group name Should Not be Empty""); return; } // Use Your work Group WinNT://&&&&(Work Group Name) DirectoryEntry DomainEntry = new DirectoryEntry(""WinNT://"" + this.TxtWorkGroup.Text.Trim()); DomainEntry.Children.SchemaFilter.Add(""computer""); // To Get all the System names And Display with the Ip Address foreach(DirectoryEntry machine in DomainEntry.Children) { string[] Ipaddr = new string[3]; Ipaddr[0] = machine.Name; System.Net.IPHostEntry Tempaddr = null; try { Tempaddr = (System.Net.IPHostEntry)Dns.GetHostByName(machine.Name); } catch(Exception) { //MessageBox.Show(""Unable to connect woth the system :"" + machine.Name ); deadHostList.Items.Add(machine.Name); continue; } System.Net.IPAddress[] TempAd = Tempaddr.AddressList; foreach(IPAddress TempA in TempAd) { Ipaddr[1] = TempA.ToString(); byte[] ab = new byte[6]; int len = ab.Length; // This Function Used to Get The Physical Address int r = SendARP( (int) TempA.Address 0 ab ref len ); string mac = BitConverter.ToString( ab 0 6 ); Ipaddr[2] = mac; } System.Windows.Forms.ListViewItem TempItem = new ListViewItem(Ipaddr); this.ListHostIP.Items.Add(TempItem); } this.Status.Text = ""Displayed""; } catch(Exception ex) { MessageBox.Show(ex.Message""Error""System.Windows.Forms.MessageBoxButtons.OK ); Application.Exit(); } but when I try to use these codes in backgroundWorker1_DoWork event it gives me error messages Cross-thread operation not valid: Control 'deadHostList' accessed from a thread other than the thread it was created on How can I modify my codes? As Chris and Reed stated....you can only modify a Ui control from the thread that the control was created on... You can also use the BackgroundWorker's ProgressChanged event for Ui updates....  var worker = new BackgroundWorker() { WorkerReportsProgress = true WorkerSupportsCancellation = true }; /// runs on background thread worker.DoWork += (s e) => { while (!done) { DoSomeWork(); // send a message to the Ui // via the ProgressChanged event worker.ReportProgress(percent statusMessage); } }; /// the ProgressChanged event runs on the UI thread worker.ProgressChanged += (s e) => { var msg = (MyStatusMessage)e.UserState; someUiControl.Items.Add(msg.Text); }; /// also runs on Ui thread worker.RunWorkerCompleted += (s e) => { }; worker.RunWorkerAsync(); Thanks for your answer. I understand to get my records in worker.ProgressChanged event. I will try this but I want to use multi thread because there are mode than 1000 computers in my network. can I do something with backgroundworker?  You cannot and should not access UI controls from any thread other than the thread that created the control. I guess your deadHostList is a ListView control or something similar. You can marshal a request from the background thread to the UI thread by using Control.Invoke or Control.BeginInvoke  You need to always marshal calls to UI elements such as your list control onto the UI thread. You can do this via Control.Invoke. Change this: deadHostList.Items.Add(machine.Name); To: string name = machine.Name; deadHostList.Invoke(new Action( () => deadHostList.Items.Add(name))); You'll also need to do the same thing later for ListHostIP - make sure to use Control.Invoke to wrap that call as well."
733,A,"Windows Network Information Manipulation I am working as a trainee engineer in a networking firm and am getting annoyed by having to change the IP information from time to time. I am in need of building a software to help me change these details easily. I have managed to set the IP information. But I still have problems. I need to run the program as Administrator [right click] is there a way to program to prompt for it at startup? How can I change adapter to DHCP? The code is quite long and I hope not to fill bore you with it. But I have been using Management Management Class Management Base Object Management Object Collection in my development. I'd prefer to make my own program to develop my programming skills. But if there is an application to do it I don't mind knowing. I once had to write a very similar program. I used some of the source code from these two projects to help me get started: Chameleon Project and Configuring TCP/IP Settings using WMI and C# The Chameleon Project is a C# project to help change network settings of a particular adapter. The other project is a tutorial on how to use C# to change network settings using WMI and C#. You can look at the source code and learn from it to help you make your own software that does what you need.  I hope this answer gives you some insisght and direction to go. Okay the network adapter one isn't that straight forward but I believe you can achieve it with WMI specially this WMI object here. The MSDN documentation tells you all the properties methods (which there are for setting DHCP etc) and the datatypes and values it takes. This may be one approach as using WMI through C# is pretty easy. I wish I could provide you an example but I've never used that specific WMI class before. You can also access the above WMI class through the Visual Studio Server Explorer which you can see here. ..and it has your ""EnableDHCP"" method you are probably looking for. As far as asking for your program to run with administrative priviledges here is the code from my setup project in my framework. What this does is before it runs any sort of form or logic requests the ""runas"" verb which invokes UAC (if Windows has its Vista/7 and requests admin priviledges from the user) namespace Setup { using System; using System.Collections.Generic; using System.Linq; using System.Windows.Forms; using Setup.Forms; using System.Security.Principal; using System.Diagnostics; static class Program { /// <summary> /// The main entry point for the application. /// </summary> [STAThread] static void Main() { Application.EnableVisualStyles(); Application.SetCompatibleTextRenderingDefault(false); WindowsPrincipal principal = new WindowsPrincipal(WindowsIdentity.GetCurrent()); bool administrativeMode = principal.IsInRole(WindowsBuiltInRole.Administrator); if (!administrativeMode) { ProcessStartInfo startInfo = new ProcessStartInfo(); startInfo.Verb = ""runas""; startInfo.FileName = Application.ExecutablePath; try { Process.Start(startInfo); } catch { return; } return; } Application.Run(new ShellForm()); } } } As far as a program to do it Windows Network Connection Manager? I know its cumbersome because of all the dialogs but.. its already there. Rather than code it yourself the best thing to do to get an App to run as Administrator is to set it properly in the application's manifest: http://stackoverflow.com/questions/5276674/how-to-force-a-wpf-application-to-run-in-administrator-mode/5276770#5276770"
734,A,"Refusing connection from a host I'm writing a simple tcp server application using sockets. As far as I know I can obtain the client's ip address and port after calling accept(). Now lets assume I have a banlist and I want to ban some ip addresses from my server. Is there a better way than accepting the connection and then dropping it? Is there a way to get the client's ip and port before accepting the connection? If we have accept() why don't we have something like refuse()? Is there a way to refuse the connection or simply ignore connection attempt from a host? Language tags added: C/C++ OS tag added: cross platform. could you add tags for what language you use? I am not a Sockets expert but I have some experience from Actionscript and C#/.NET And the OS as well... Windows vs Unix could make a big difference here. I don't think there is a portable solution but I could be wrong. On windows: look at WSAAccept may be it is what you need: SOCKET WSAAccept( __in SOCKET s __out struct sockaddr *addr __inout LPINT addrlen __in LPCONDITIONPROC lpfnCondition __in DWORD dwCallbackData ); lpfnCondition -- the address of an optional application-specified condition function that will make an accept/reject decision based on the caller information passed in as parameters and optionally create or join a socket group by assigning an appropriate value to the result parameter g of this function. If this parameter is NULL then no condition function is called. For linux solution look at: GNU Common C++ -- TCPSocket class it has onAccept() and reject() methods. virtual bool TCPSocket::onAccept ( const InetHostAddress & ia tpport_t port ) [inline protected virtual] A method to call in a derived TCPSocket class that is acting as a server when a connection request is being accepted. The server can implement protocol specific rules to exclude the remote socket from being accepted by returning false. The Peek method can also be used for this purpose. However you can just close socket after accept if pre-condition is false :) If there is no other solution I will accept and close the socket immediately like opening a door to ask who are you and close it if you are banned... But I don't want to open that door at all. I want to make banned users think that the server machine is simply down. What you want is more elegant however in development we need balance beetween features gratefulness and time/money spent on it. Like in XP said -- do it in the simpliest way later you can come and refactor it.  the concept of not accepting connection based on IP address will not work if you are behing NAT.  TCP Wrapper has become very widespread on UNIX-like platforms. Yes the connection is only checked after accept; however if you're doing this anyways you might as well use the same configuration that everybody else uses. In essence your program is either run with tcpd or modified to use libwrap directly like so: #include <tcpd.h> connfd = accept(listenfd (struct sockaddr *)&addr &addrlen); if (!hosts_ctl(""my_program"" STRING_UNKNOWN inet_ntoa(addr.sin_addr) STRING_UNKNOWN)) { fprintf(stderr ""Client %s connection disallowed\n"" inet_ntoa(addr.sin_addr)); close(connfd); } Then you configure /etc/hosts.allow and /etc/hosts.deny according to your desires e.g.  # /etc/hosts.allow my_program: 127.0.0.1 10. 192.168. # /etc/hosts.deny my_program: ALL This will cause my_program to reject all connection attempts from non-local addresses but can be changed without touching my_program at all.  The TCP implementation normally completes the TCP 3-way handshake before the user process even has access to the connection and the accept() function merely gets the next connection off the queue. So it is too late to pretend that the server is down. This works the same way for regular TCP data; the TCP implementation does not wait for the application to actually recv() the data before a TCP ACK is sent. This keeps the other side from needlessly retransmitting packets that were received correctly and allows the throughput to remain high even when the application is bogged down with other things. In the case of new connections (SYN packets) this also allows the kernel to protect itself (and the application) from SYN flood attacks. Although not portable many platforms provide some sort of firewall capability that will allow filtering incoming connections based on IP address/port. However that is usually configured system-wide and not by an individual application. So if I listen on a port the connection will be established in background regardless I want or not? Calling listen() tells the OS that you want any connection that can be successfully established on your port. So unless you filter the packet at a lower level (firewall rule) it will respond to any connection request to this port. If you decide that you don't want a particular connection you can always close() it which is what some of the other answers here will do."
735,A,Skip receiving messages broadcasted by me over wireless network I have a code which transmits messages (broadcasts) continuously on wireless network and also receives messages continuously. Hence I have the transmission and receiving happening simultaneously using pthreads. The problem that I am facing is that I want to skip receiving messages sent by me and only receive the packets/messages sent by others. Is it possible to do this? Note that this code runs on a wireless modem and not on the laptop. The modem runs using OpenWrt OS. In general there is no way to prevent receipt of your own node's messages. Your best bet is to continue letting it receive its own packets and then ignore any that have your MAC or IP address.
736,A,"Finding an IP from an interface name On a Linux box the common interface names look like eth0 eth1 etc. I know how to find at least one IP using gethostbyname or similar functions but I don't know any way to specify which named interface I want the IP of. I could use ifconfig and parse the output but shelling out for this information seems... inelegant. Is there a way to say enumerate all the interfaces and their IPs (and maybe MAC addresses) into a collection? Or at least something along the lines of gethostbyinterface(""eth0"")? See this [question: Get the IP Address of local computer](http://stackoverflow.com/questions/212528) From TLUG: #include <sys/types.h> #include <sys/socket.h> #include <sys/ioctl.h> #include <net/if.h> #include <netinet/in.h> #include <arpa/inet.h> #include <string.h> #include <stdio.h> #include <unistd.h> /** * getIPv4() * * This function takes a network identifier such as ""eth0"" or ""eth0:0"" and * a pointer to a buffer of at least 16 bytes and then stores the IP of that * device gets stored in that buffer. * * it return 0 on success or -1 on failure. * * Author: Jaco Kroon <jaco@kroon.co.za> */ int getIPv4(const char * dev char * ipv4) { struct ifreq ifc; int res; int sockfd = socket(AF_INET SOCK_DGRAM 0); if(sockfd < 0) return -1; strcpy(ifc.ifr_name dev); res = ioctl(sockfd SIOCGIFADDR &ifc); close(sockfd); if(res < 0) return -1; strcpy(ipv4 inet_ntoa(((struct sockaddr_in*)&ifc.ifr_addr)->sin_addr)); return 0; } int main() { char ip[16]; if(getIPv4(""eth0"" ip) == 0) printf(""IPv4: %s\n"" ip); else printf(""No IP\n""); return 0; } Of course there is one drawback: you open a dummy socket. Anyway nice job.  edit: I saw you don't like shelling. Then you can look at how ifconfig does its job (it extracts at least some information from /proc). When you have interface name you can do this (in your shell): ifconfig eth0 | grep 'inet addr' | sed -e 's/:/ /' | awk '{print $3}' To enumerate interfaces you can use this: ifconfig | egrep '^[^ ]' | awk '{print $1}' Combined: for x in `ifconfig | egrep '^[^ ]' | awk '{print $1}'`; do echo -n ""${x}"" echo -n "" "" ifconfig ""${x}"" | grep 'inet addr' | sed -e 's/:/ /' | awk '{print $3}' done"
737,A,"Why won't NetworkChange events fire I am fairly new to all this but I have looked through questions and can't find anything that answers my question. I am writing a simple service that checks whether it can reach one of our servers whenever there is a change to it's network settings. I am using the two events under the NetworkChange class NetworkAddressChanged and NetworkAvailabiltyChanged. When either fire the service tries to ping the server and depending on the result changes the ProxyEnable setting. The Code: using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Diagnostics; using System.Linq; using System.ServiceProcess; using System.Text; using System.Net.NetworkInformation; using Microsoft.Win32; namespace ProxyManager { public partial class ProxyManager : ServiceBase { static RegistryKey rk = Registry.CurrentUser.OpenSubKey(@""Software\Microsoft\Windows\CurrentVersion\Internet Settings""true); static string currentProxy = rk.GetValue(""ProxyEnable"").ToString(); static string newProxy; public ProxyManager() { InitializeComponent(); NetworkChange.NetworkAddressChanged += new NetworkAddressChangedEventHandler(NetworkChange_NetworkAddressChanged); NetworkChange.NetworkAvailabilityChanged += new NetworkAvailabilityChangedEventHandler(NetworkChange_NetworkAvailabilityChanged); newProxy = ""0""; } protected override void OnStart(string[] args) { } void NetworkChange_NetworkAvailabilityChanged(object sender NetworkAvailabilityEventArgs e) { ProxySwitcher(); EventLog evt = new EventLog(""ProxyManager""); string message = ""Proxy Manager Newtwork availabilty changed. Proxy switched to "" + newProxy.ToString(); evt.Source = ""Proxy Manager""; evt.WriteEntry(messageEventLogEntryType.Information); } void NetworkChange_NetworkAddressChanged(object sender EventArgs e) { ProxySwitcher(); EventLog evt = new EventLog(""ProxyManager""); string message = ""Proxy Manager Newtwork address changed. Proxy switched to "" + newProxy.ToString(); evt.Source = ""Proxy Manager""; evt.WriteEntry(messageEventLogEntryType.Information); } void ProxySwitcher() { if (currentProxy == ""0"") { newProxy = ""1""; } else { newProxy = ""0""; } try { Ping myPing = new Ping(); string host = ""FILE SERVER""; byte[] buffer = new byte[32]; int timeout = 1000; PingOptions pingOptions = new PingOptions(); PingReply reply = myPing.Send(host timeout buffer pingOptions); if (reply.Status == IPStatus.Success) { if (currentProxy == ""0"") { rk.SetValue(""ProxyEnable"" newProxy); } } else { if (currentProxy == ""1"") { rk.SetValue(""ProxyEnable"" newProxy); } } } catch (PingException pingEx) { rk.SetValue(""ProxyEnable"" 0); } } protected override void OnStop() { } } } My problem is that when the service is installed and running the events are either not being triggered or not being picked up. There related events are not being logged and the proxy is not switching. I tried the code minus the event handlers in a console app that I could run at will and that worked fine depending on the network availability. I am running Windows 7 x86 but am coding in .NET 3.5. Any help will be gratefully received. Consider using WMI. Some sample VBScripts on http://msdn.microsoft.com/en-us/library/aa394595(v=vs.85).aspx I know from experience that WMI works but never tried it with ""NetworkChange"" object. I checked out the WMI and it looks like it can return the information but I am unsure what would cause an event to fire to check the information. I saw the EventWatcher and may play with it. I think the problem is my service. I took everything out of the code bar the event handler and wrote a line to a file when it fired. It created the file but didn't wirte the text and the service stopped itself??? +1 for your help and introducing me to a really useful tool but I can't accept the answer as correct - mostly cause the question was asking the wrong thing. Thanks I don't know enough about WMI to mark this as the correct answer at the moment but I will read up try it out and come back. WMI is the place where all OS control is. Microsoft tried to put it there instead of leaving it spread around in random API-calls kind of like Linux /proc filesystem. .Net has great WMI-support. WMI contains static info (CPU RAM OS etc) dynamic info (IP resources etc) and events you can subscribe to (something changed). A good staring point is to fool around in WMIC. Just start CMD and type WMIC and then type the command /? and maybe the command NICCONFIG LIST (scroll right afterwards to see all fields) Thanks for the further info - and this can all be called from within a c# project? Using System.Management;?  You may try logging to a file instead of the event log. There are permission issues with the event log and when you are running the service the security model may be preventing you from writing to the log. If so a simple breakpoint would reveal the problem. I have tried adding a breakpoint. but nothing happens. Also just tried logging to a file but again nothing. I have totally swapped the eventlog out for writing to a text file. Funny thing is the file gets created... So the trigger is firing. See my comment to Tedd's answer. +1 for helping me find the problem. hmmm... just to be sure... are you writing to the file before calling the event log? In case of security permissions error it throws an exception... this may prevent your code from writing to the file...  I wrote a rather large WMI application and this is the one feature that I could never get to work well with WMI. Sorry to disagree with Ted but .NET WMI support is terrible particularly with dealing with exceptions properly and when dealing with pre-Vista operating systems. Two caveats when dealing with these events. First they only work if subscribed on the main thread. However since your log file gets created then you appear to have verified that the event is being triggered. The second problem is in the assumption your code makes about the current state of the network. From the look of your code you are starting with Proxy = 0 so no proxy. Then when you receive a network change event you toggle the proxy. Consider what happens when your service starts and the network is ""down"" already which might very well happen during bootup depending on when your service starts in relation to the network stack initialization. You will be setting ""no proxy"" when the NIC is down then setting ""Proxy"" when the NIC is up. This seems to be inverted to what you were trying to accomplish. The fix is simple in the event you need to check the state of the network to know why the change event was fired. So in your NetworkAvailabilityChanged event you are going to want to check NetworkInterface.GetIsNetworkAvailable() which returns true as long as there is one network interface that isn't loopback or ms-tunnel. For the NetworkAddressChanged event presumably you are going to want to check the address to see if it's valid or not to determine if your proxy needs to be invoked. You can do this by grabbing the UnicastIPAddressInformationCollection like this... NetworkInterface[] niList = NetworkInterface.GetAllNetworkInterfaces();  foreach (NetworkInterface ni in niList) { switch (ni.NetworkInterfaceType) { case NetworkInterfaceType.Ethernet: // 10baseT case NetworkInterfaceType.FastEthernetT: // 100baseT case NetworkInterfaceType.GigabitEthernet: GatewayIPAddressInformationCollection gwIPColl = ni.GetIPProperties().GatewayAddresses; UnicastIPAddressInformation uniIPInfo = null; UnicastIPAddressInformationCollection IPcoll = ni.GetIPProperties().UnicastAddresses; if (IPcoll.Count <= 0) { LogFile.LogMe(""No valid unicast IP address""); broken = true; break; // Cannot continue if we don't have an IP in the colletion } . . . or something similar depending on how complicated your want to make it. When I check for network address validity I check the Unicast address Netmask (.IPv4Mask) and the defined gateway from GatewayIPAddressInformationCollection (.GetIPProperties().GatewayAddresses) Hopefully that helps you out a little bit. I should also mention that when an interface goes up or down both of these events get fired presumably because the IP goes to 0.0.0.0 when the interface goes down and then restores to a proper IP when the interface comes up. So by hooking both events and doing ProxySwitch() in both you are going to get nowhere fast."
738,A,"What is the proper backlog for an asynchronous server socket? If you're working with an Asynchronous server socket in C# what is the proper backlog to put on the socket? For instance: server = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); IPEndPoint iep = new IPEndPoint(IPAddress.Any port); server.Bind(iep); server.Listen(10); I always tend to use `5` but I have no sharply reasoned explanation for `why` -- it's just the default I've always been using for all listening sockets async or not;-) It's likely in most applications that the listener itself is not the bottleneck. More often you need to respond to those TCP requests by doing some kind of work and the amount of workers your application can effectively use at one time will be smaller than the number of connections it could theoretically sustain. This probably explains Alex Martelli's ""rule of thumb"" in the comments: I always tend to use 5 but I have no sharply reasoned explanation for why. On a quad-core server this makes sense if clients kick off CPU-intensive tasks; 4 connections for 4 workers/threads/cores and one more to keep the service responsive when it's heavily-loaded. If your work is not CPU-intensive however then the above limit/explanation is irrelevant for you you might be able to manage 10 or 50 connections or you might need to use some machine-exclusive resource and only allow one. I basically agree with Hans in that there's no hard and fast ""right answer"" for this you'll need to look at what your application does make an estimate as to how many connections you think it can actually handle and tweak it for maximum efficiency by testing."
739,A,"WebClient.Download() exception: Access to the path I have a very simply code of: WebClient webClient = new WebClient(); webClient.DownloadFile(privateHTML @""\\192.168.0.12\imagedepot\AT"" + carID + "".jpeg""); However i get the error: Access to the path '\\192.168.0.12\imagedepot\AT296866482.jpeg' is denied. I can browse this folder from my machine (same one this app runs on) right click and create new text file. The target is a windows 2008 server. Any ideas? That folder share is set to everyone with full. When I change \192.168.0.12\imagedepot to be a mapdrive of Z:\ i get. Could not find a part of the path 'Z:\AT294577469.jpeg' Any ideas? EDIT after comment: Then this is perhaps a permissions/rights issue... what sort of application is it ? WinForms ? Windows Service ? ASP.NET ? EDIT 2: Now that it is clarified by the OP (see comments) that it is a Windows Service the problem is clear: Accsessing Network drives/UNC paths from a service is definitely a permissions/rights issue as Service is usually NOT allwed to access a network share and/or a mapped drive! see How to map a network drive to be used by a service see thats the thing i was using this just fine for like a month i just switched it over to new install of winsows 2008 see my edit above its a console app an permissions for that folder is everyonefull. which makes no sense because like i said I can create a file manually using either of those paths. does the console app run with the same user as when you try it manually ? umm i'm not sure i dont specify a user but again that folder permisisons is Everyone and full. everyone doesnt need a user. I am NOT talking about the persmissions of the folder/share... BUT I am talking about the persmission/rights/user your console app is running under... oh yes the console app does use the same user. then there is something weird going on with your OS installation... can you write to the share from within the console app using some other method (not WebClient) ? Yeah i can use File.appendalltext to create a text file and it goes straight to Z:\ then I am out of ideas... sorry hmm interesting i put that code in directly above the line and it cant find that path. I was trying that outside in a test code. So yeah it's a windows service console app and it cant find path see my edit above... thanks for the help but i really have been used that same exact code for like a month my guess is this new install of winsows 2008 something must have been configured differently."
740,A,"how to communicate between client and server in java I have a chat program. Now the code works for communicate between client and server via command line. But it gives an exception (java.net.SocketException: Socket is closed) while running. Please help me to fix that problem. In a java chat programhow will the communication be implemented between client and server? ie. client<-->server (between server and client)  or client A<-->server<-->client B (server act as a bridge between two clients) Is the 2 way communication can be implemented through a single socket? Are there any other methods ? How to communicate more than one client simultaneously? server code class Server { ServerSocket server; Socket client; public Server() { try { server = new ServerSocket(2000); System.out.println(""\tServer Started..........""); while (true) { client = server.accept(); Send objsend = new Send(client); Recive objrecive = new Recive(client); //client.close(); } } catch (Exception e) { System.out.println(""Exception4 "" + e); } } public static void main(String arg[]) { new Server(); } } class Recive implements Runnable { Socket client; public Recive(Socket client1) { client=client1; Thread trsend=new Thread(this); trsend.start(); } public void run() { ObjectInputStream ois; Message M=new Message(); try { ois = new ObjectInputStream(client.getInputStream()); M = (Message)ois.readObject(); M.display(); ois.close(); } catch (Exception e) { System.out.println(""Exception1 "" + e); } } } class Send implements Runnable { Socket client; public Send(Socket client1) { client=client1; Thread trrecive=new Thread(this); trrecive.start(); } public void run() { Message M=new Message(); InputStreamReader isr=new InputStreamReader(System.in); BufferedReader br=new BufferedReader(isr); try { System.out.println(""Me(server)""); M.strmessage=br.readLine(); ObjectOutputStream oos=new ObjectOutputStream(cli ent.getOutputStream()); oos.writeObject((Message)M); oos.flush(); oos.close(); } catch (Exception e) { System.out.println(""Exception "" + e); } } } client code class Client { public static void main(String arg[]) { try { Send objsend=new Send(); Recive objrecive=new Recive(); } catch(Exception e) { System.out.println(""Exception ""+e); } } } class Send implements Runnable { public Send() { Thread trsend=new Thread(this); trsend.start(); } public void run() { try { Message M=new Message(); InputStreamReader isr=new InputStreamReader(System.in); BufferedReader br=new BufferedReader(isr); while(true) { System.out.println(""Me(client)""); M.strmessage=br.readLine(); Socket client=new Socket(""localhost""2000); ObjectOutputStream oos=new ObjectOutputStream(client.getOutputStream()); oos.writeObject((Message)M); oos.flush(); oos.close(); } } catch(Exception e) { System.out.println(""Exception ""+e); } } } class Recive implements Runnable { public Recive() { Thread trrecive=new Thread(this); trrecive.start(); } public void run() { try { while(true) { Socket client=new Socket(""localhost""2000); ObjectInputStream ois=new ObjectInputStream(client.getInputStream()); Message CNE=(Message)ois.readObject(); CNE.display(); ois.close(); } } catch(Exception e) { System.out.println(""Exception ""+e); } } } possible duplicate of (http://stackoverflow.com/questions/5433378/run-application-both-as-server-and-client) There could be multiple places where the exception could be thrown. Without a stack trace it is difficult to state so accurately as to the cause of failure. But the root cause is essentially due to the fact that you are closing the InputStream of the socket in your Receiver threads after reading a message and closing the OutputStream of the socket in your Sender threads after sending a message. Closing either of these streams will automatically close the socket so you if attempt to perform any further operation on it a SocketException will be thrown. If you need to ensure that your server and client do not shutdown in such an abrupt manner you'll have to keep reading the InputStream (until you get a special message to shutdown for instance). At the same time you'll also have to keep writing to the OutputStream. Two-way communication is definitely possible and the posted code is capable of the same (if the socket remains open). If you have to handle multiple clients you'll need multiple reader and writer threads on the server each listening on an instance of a Socket returned from ServerSocket.accept(); in simpler words you need a reader-writer pair listening on a distinct socket on the server for each client. At the moment multiple clients can connect to the Server as each incoming connection is provided its own client Socket object on the Server that is provided to individual reader and writer threads. The main Server thread can continue to receive incoming connections and delegate the actual work to the reader-writer pairs. Thanks......Did you meanthere is no need of ""client = server.accept();"" in while loop?.If so how to know a message is recived? My bad. The while loop enables multiple clients to connect. I'll edit my answer. Are there need while loops in all run method?If sohow to get an event while getting a new message??? Did you mean the sender and receiver? If so then yes and you should break out of the loop once you have encountered the end of the stream (for the InputStream) or once you no longer wish to write to the stream (for the OutputStream). In the current scenario only 1 message would be either sent in total depending on who sends first. now i made a chat program with 2 clients and a server control the clients. i.e the client can chat through server.but i need to chat with more than two.then how to do that? @KIRAN ask this as a new question.  first of all Don;t close the streams in every run(). Secondly check whether port for server which u r using is free. I am sorryI dont know how to check the port is free?How to find a suitable port? now i made a chat program with 2 clients and a server control the clients. i.e the client can chat through server.but i need to chat with more than two.then how to do that?  This program makes your pc both host and server. import java.io.IOException; import java.net.*; public class ClientServer { static byte[] buffer = new byte[100]; private static void runClient() throws IOException { byte buffer[] = new byte[100]; InetAddress address = InetAddress.getLocalHost(); DatagramSocket ds=new DatagramSocket(); int pos = 0; while (pos<buffer.length) { int c = System.in.read(); buffer[pos++]=(byte)c; if ((char)c =='\n') break; } System.out.println(""Sending "" + pos + "" bytes""); ds.send(new DatagramPacket(buffer pos address 3000)); }  private static void runServer() throws IOException { InetAddress address = InetAddress.getLocalHost(); DatagramSocket ds = new DatagramSocket(3000 address); DatagramPacket dp = new DatagramPacket(buffer buffer.length); ds.receive(dp); String s=new String(dp.getData()0dp.getLength()); System.out.print(s); } public static void main(String args[]) throws IOException { if (args.length == 1) { runClient(); } else { runServer(); } } } also follow this link now i made a chat program with 2 clients and a server control the clients. i.e the client can chat through server.but i need to chat with more than two.then how to do that? @ KIRAN you can visit (http://www.ntu.edu.sg/home/ehchua/programming/java/J6a_Networking.html)  chat programms normaly have a server through which all communication goes. The reason is that other wise every client needs to know how to reach every other client. And that doesn't work in the general case. So you'll have a server every client registers and talks with the server which will forward messages to other clients. Mostly communication is done via HTTP cause this is likely to go through firewalls and proxies. You probably want to read up on long polling if you are planning for anything serious. now i made a chat program with 2 clients and a server control the clients. i.e the client can chat through server.but i need to chat with more than two.then how to do that?"
741,A,"C Network programming question So I was looking over Beej's guide to network programming. http://beej.us/guide/bgnet/ Anyways in one of his intro examples he talks about getting the IP address for a hostname (like google.com or yahoo.com for instance) Anyways here is the code. /* ** showip.c -- show IP addresses for a host given on the command line */ #include <stdio.h> #include <string.h> #include <sys/types.h> #include <sys/socket.h> #include <netdb.h> #include <arpa/inet.h> int main(int argc char *argv[]) { struct addrinfo hints *res *p; int status; char ipstr[INET6_ADDRSTRLEN]; if (argc != 2) { fprintf(stderr""usage: showip hostname\n""); return 1; } memset(&hints 0 sizeof hints); hints.ai_family = AF_UNSPEC; // AF_INET or AF_INET6 to force version hints.ai_socktype = SOCK_STREAM; if ((status = getaddrinfo(argv[1] NULL &hints &res)) != 0) { fprintf(stderr ""getaddrinfo: %s\n"" gai_strerror(status)); return 2; } printf(""IP addresses for %s:\n\n"" argv[1]); for(p = res;p != NULL; p = p->ai_next) { void *addr; char *ipver; // get the pointer to the address itself // different fields in IPv4 and IPv6: if (p->ai_family == AF_INET) { // IPv4 struct sockaddr_in *ipv4 = (struct sockaddr_in *)p->ai_addr; addr = &(ipv4->sin_addr); ipver = ""IPv4""; } else { // IPv6 struct sockaddr_in6 *ipv6 = (struct sockaddr_in6 *)p->ai_addr; addr = &(ipv6->sin6_addr); ipver = ""IPv6""; } // convert the IP to a string and print it: inet_ntop(p->ai_family addr ipstr sizeof ipstr); printf("" %s: %s\n"" ipver ipstr); } freeaddrinfo(res); // free the linked list return 0; } The part that confuses me is the for loop. for(p = res;p != NULL; p = p->ai_next) { void *addr; char *ipver; // get the pointer to the address itself // different fields in IPv4 and IPv6: if (p->ai_family == AF_INET) { // IPv4 struct sockaddr_in *ipv4 = (struct sockaddr_in *)p->ai_addr; addr = &(ipv4->sin_addr); ipver = ""IPv4""; } else { // IPv6 struct sockaddr_in6 *ipv6 = (struct sockaddr_in6 *)p->ai_addr; addr = &(ipv6->sin6_addr); ipver = ""IPv6""; } // convert the IP to a string and print it: inet_ntop(p->ai_family addr ipstr sizeof ipstr); printf("" %s: %s\n"" ipver ipstr); } Would anyone mind going through psuedo-step-by-step at whats going on or what these things are? Is it iterating through a linked list?......I have a general idea of what the struct addrinfo are but what the heck is struct *res and struct *p or void *addr and *char ipversion. The guide is good but it jumped to this quickly and I got very confused. I just want to understand whats going on :) About `""} else { // IPv6`: that should be ok in most practical contexts but I don't like it for a teaching sample; it gives the wrong idea. Sockets support protocols other than IP(4) and IPv6 and values of `ai_family` other than AF_INET and AF_INET6 (IPv6). In fact I'm sure that AF_INET6 is by far the most recent addition to the list. At a minimum I might have made a more extensive comment. First thing's first do you know what a linked list is? If you understand that you'll recognise what that for loop is going. p is a pointer to a structure that also references (links) the next structure in the list. So you're looping through a list of those structures which are addrinfo structs. 4 Now the thing you need to know about network packets is that they're made up of a header. Specifically the Ethernet frame. This is the hardware to hardware protocol. It let's you get things around on a physical bounded network but knows nothing about routing across physical network boundaries. Next up comes tcp or possibly another transport layer protocol which sits somewhere inbetween the two levels. TCP versus UDP versus X is about how you manage the packets - for example TCP requires packets be reassembled in order whereas UDP is a ""broadcast""-type protocol. Finally you have the internet protocol suite (IPv4 IPv6). These are higher level protocols that control the broader sense of routing so they know about the internet at large but less about the steps needed to get there. A great explanation of this is the handy diagram on this page. To complete the picture BGP is how routers know how to move stuff around. tcp/udp fit into this picture by being a part of (enscapulated in) the protocol in question (IPv4 for example) So ethernet frames contain other protocols most notably IPv4 which contain the information routers need to get it out across the internet (across multiple physical networks). The internet protocol specifies where you want to go from where you are. So a typical's IPv4 body remains unchanged across its whole transit but every time it traverses physical networks it gets wrapped up in a different ethernet packet. Now in the ethernet header there is a field for finding out what the ""ethernet body"" contains. This line:  if (p->ai_family == AF_INET) { Does. AF_INET is a constant that matches the value tcp uses to identify the packet body as IPv4. So if you're looking at an IPv4 header this loop then goes on to read that information. The else clause is technically wrong because not being IPv4 doesn't automatically make it IPv6. You could change it to test for IPv6 like this:  else if (p->ai_family == AF_INET6) { Which you might want to do just in case you pick up something else. Now it's worth explaining this bit of magic: struct sockaddr_in6 *ipv6 = (struct sockaddr_in6 *)p->ai_addr; This basically takes the network or raw form of the data which appears as a sequence of bytes and casts it (coverts it) into fields in a struct. Because you know how big the fields are going to be this is a very quick and easy way to extract out what you need. The last thing that needs explanation is this: inet_ntop(p->ai_family addr ipstr sizeof ipstr); There are other ways of achieving this specifically ntohs(). Basically network data is transmitted in big endian encoding and in order to read it you need (potentially) to convert the data to the encoding of your system. It could be big endian or it could be little it depends on your system for the most part. Have a read of the wikipedia article on endianness. Summary: what you're looking at here is a combination of computer science structures how networks work and C code. I learnt all this as part of an old job - I had to basically re-engineer a small subset of tcpdump... if you have the time to work through their codebase it'll teach you everything you ever wanted to know about every network protocol there is... Have edited it to give a more correct complete explanation. Dang nice explanation! thanks. This is exactly what I was looking for! If only every book was as descriptive to this  Yes res points to a linked list of addrinfo structures that represent the different IP addresses of a host. The MSDN documentation on the getaddrinfo function is pretty good. I don't know what platform you're running on but it shouldn't be much different on other platforms. Unix Platform. Im guessing *p is just the iterator or something through the linked list? Yes *p gets set at the top of the loop to the first element in the list (res) and then for each iteration it gets reset to p->ai_next which is a pointer to the next element in the list. When p->ai_next is NULL then you are at the end of the list. Ok that makes more sense then.  Well it's not that complicated. getaddrinfo returns a linked list of addrinfo structs (struct addrinfo **res in the manpage) where each of these structs contains information about one address available to the given interface (const char *node in the manpage). Now every struct is being inspected and information about the struct is being printed out. To print out either IPv4 or IPv6 the variable ipver is set accordingly. Before printing out the information the address has to be converted from a binary form to a string. This is done by inet_ntop (*n*umber to *p*ointer). The resulting string of inet_ntop (ipstr) and ipver are now printed out to console. Printing ipver however is not neccessary since you would recognize the address type from the ipstr: an IPv4 address (as we all know) gets written 192.168.1.10 whereas IPv6 addresses use colons to separate the address elements: 2001:0db8:85a3:0000:0000:8a2e:0370:7334. I know it'd be something simple but this is literally my first face to face battle with network programming ;) so it can be a bit confusing for a nub like me lol. +1 for number to pointer; very nice explanation."
742,A,"Help communicating with sockets through nat i want to make a simple p2p messenger in java i have managed to communicate with 2 computers in the same network but when i try to send a message to another computer from outside the network the message is disappearing... I know that i could make forwards in the router but i don't want to do that i want to make it like yahoogtalk or other im servers do. I think that they use the nat system to do that but i don't know very well how :D. Is somebody who can explain me how NAT works and if this is the way of solving my problem? I read that I need an static and public ip for the server and when the client(with a private ip) sends a request to the server his NAT will know what to do with the message that will come from the server next time... but if the client don't send a paket to the server and the server sends a packet to client then the NAT doesn't know where to send the packet inside the network(to the client). Is it right what i think? Thank you very much! All the other system you are mentioning use a server with a well known public IP to help clients communicate with each other. Each client communicates with the server and the server sends every message to the appropriate client. This is perhaps the only way to do it because otherwise: Every client should have a public IP in order for other clients to reach him. Each client should also act both as client and a server. It will be very difficult to discover clients as you would need to know their IPs Your system may work inside a local network but it won't work in the internet. Each computer in a network has a local IP assigned to it by a local gateway. This local IP is invisible to outsiders unless there is NAT setting that maps a public IP to it. Also there must be a setting for outgoing messages. Not every ISP gives local IP addresses. I have two Internet connections and one of them gives me a public IP via PPP and it's almost always the same. Even if it changes sometimes I have DynDNS set up. Another connection used fixed private IP but then I asked them to switch IP to public. Now I have a fixed public IP but I have to pay for it (very little though). Of course but if you create a home network the second computer would get a local IP. In fact if you are using a router most probably even your first computer has a local IP and knows nothing about your public IP.  The client by definition initiates the request the server services that request and sends back a response. The assumption is that clients can be trusted to make network connections servers cannot. BTW: On some networks clients are not even trusted to do that and must use proxies. I repeat; the client is the one creating the connection whether it is on your network or on the internet.  You're almost right in your NAT description. The only mistake you've made is that pointed out by Peter - the client is always the one that initiates a connection. Now if both sides have public IPs everything is fine and either side can be a server. If one side is behind a NAT and the other one has a public IP then it's okay if the one that's behind a NAT initiates the connection. In such case the NATing router will remember where to send responses from the server. And finally if both sides are behind NATs it looks like it is impossible to initiate a connection but in fact it's not always so. There's one trick that is called ""hole punching"". This is a kind of white magic and it doesn't always work but it works often enough to be successfully used by Skype for example. Further some router allow to define dynamic port forwarding through UPNP but it has to be enabled. That would be my recommended way if both sides are behind a NAT. Thank you for your answers now i understand the process! That is if you have access to those routers. If a router belongs to your ISP would that work?"
743,A,"Linking Error in Sun Studio 10 under Solaris I wrote a test program like this: #include <sys/socket.h> int main( void ) { int sock = socket(AF_INET SOCK_DGRAM 0); return 0; } And tried to compile it: $ /tool/sunstudio/bin/cc test.c Undefined first referenced symbol in file socket test.o ld: fatal: Symbol referencing errors. No output written to a.out The output is ""symbol socket is not referenced"". Kindly give me the direction so that I can resolve this. What error do you get ? What's that linking error? fatal: Symbol referencing errors. you have to link in the socket library in the command line: -lsocket kindly give me some explanation. The Ide you're using (SunStudio) when it compiles uses options you provide in the project settings or makefile. Those options must include all the libraries your program needs; libraries have to be linked (statically or dynamically) to your final executable program. To link socket library the option is the above -lsocket.  Here's the question. I wrote a test program like this: #include <sys/socket.h> int main( void ) { int sock = socket(AF_INET SOCK_DGRAM 0); return 0; } And tried to compile it so (this is the output that really helps you have to remember that modern compilers really try their best to help you fix any problems): $ /tool/sunstudio/bin/cc test.c Undefined first referenced symbol in file socket test.o ld: fatal: Symbol referencing errors. No output written to a.out Now from the output we can see that the symbol socket is not referenced. So if you type man socket you will get the following from the man page: SYNOPSIS cc [ flag ... ] file ... -lsocket -lnsl [ library ... ] The -l flag indicates that to use this function you need to also link the named library. In this case you are being told to add -lsocket -lnsl to the cc command line as follows: $ /tool/sunstudio/bin/cc test.c -lsocket -lnsl @PP first of all I apologize about my question.. I didn't get your point.. Please explain. He means you should have supplied more information: in particular the error message that you're getting from the linker. He's also suggesting you provide a complete compilable example for us to reproduce with but I'm not sure if that's always necessary. @Arman apologies I do come across harsh sometimes. It was specifically the linker error I was looking for. You were lucky in that your problem is a common one and most of us can determine the solution straight away. But 9 times out of 10 this is not the case - and it is the specific error returned by the program in question that will help others solve your problem. Thanks PP.. I have got it.. And I will concentrate on my question next time. @PP I have update my question. @Arman if you update your question with the linker error you were getting I'll remove the downvote and upvote your question.  You need to add at least -lsocket to your link-step i.e. link against libsocket.so. I don't know how to do that in the SunStudio UI though - are its projects makefile based? The man page is usually a good place to look for required libraries; in this case the man page for socket also recommends -lnsl (see the synopsis) so that might be required too but I don't remember it being necessary."
744,A,"Should network packet payload data be aligned on proper boundries? If you have the following class as a network packet payload: class Payload { char field0; int field1; char field2; int field3; }; Does using a class like Payload leave the recipient of the data susceptible to alignment issues when receiving the data over a socket? I would think that the class would either need to be reordered or add padding to ensure alignment. Either reorder: class Payload { int field1; int field3; char field0; char field2; }; or add padding: class Payload { char field0; char pad[3]; int field1; char field2; char pad[3]; int field3; }; If reordering doesn't make sense for some reason I would think adding the padding would be preferred since it would avoid alignment issues even though it would increase the size of the class. What is your experience with such alignment issues in network data? Here's an interesting post about the things a network programmer should know - http://stackoverflow.com/questions/366257/everything-a-c-developer-should-know-about-network-programming Correct blindly ignoring alignment can cause problems. Even on the same operating system if 2 components were compiled with different compilers or different compiler versions. It is better to... 1) Pass your data through some sort of serialization process. 2) Or pass each of your primitives individually while still paying attention to byte ordering == Endianness A good place to start would be Boost Serialization. +1: Proper serialization beats messing around with this. +1 Serialization is the right answer.  You practically can't use a class or structure for this if you want any sort of portability. In your example the ints may be 32-bit or 64-bit depending on your system. You're most likely using a little endian machine but the older Apple macs are big endian. The compiler is free to pad as it likes too. In general you'll need a method that writes each field to the buffer a byte at a time after ensuring you get the byte order right with n2hll n2hl or n2hs.  We use packed structures that are overlaid directly over the binary packet in memory today and I am rueing the day that I decided to do that. The only way that we have gotten this to work is by: carefully defining bit-width specific types based on the compilation environment (typedef unsigned int uint32_t) inserting the appropriate compiler-specific pragmas in to specify tight packing of structure members requiring that everything is in one byte order (use network or big-endian ordering) carefully writing both the server and client code If you are just starting out I would advise you to skip the whole mess of trying to represent what's on the wire with structures. Just serialize each primitive element separately. If you choose not to use an existing library like Boost Serialize or a middleware like TibCo then save yourself a lot of headache by writing an abstraction around a binary buffer that hides the details of your serialization method. Aim for an interface like: class ByteBuffer { public: ByteBuffer(uint8_t *bytes size_t numBytes) { buffer_.assign(&bytes[0] &bytes[numBytes]); } void encode8Bits(uint8_t n); void encode16Bits(uint16_t n); //... void overwrite8BitsAt(unsigned offset uint8_t n); void overwrite16BitsAt(unsigned offset uint16_t n); //... void encodeString(std::string const& s); void encodeString(std::wstring const& s); uint8_t decode8BitsFrom(unsigned offset) const; uint16_t decode16BitsFrom(unsigned offset) const; //... private: std::vector<uint8_t> buffer_; }; The each of your packet classes would have a method to serialize to a ByteBuffer or be deserialized from a ByteBuffer and offset. This is one of those things that I absolutely wish that I could go back in time and correct. I cannot count the number of times that I have spent time debugging an issue that was caused by forgetting to swap bytes or not packing a struct. The other trap to avoid is using a union to represent bytes or memcpying to an unsigned char buffer to extract bytes. If you always use Big-Endian on the wire then you can use simple code to write the bytes to the buffer and not worry about the htonl stuff: void ByteBuffer::encode8Bits(uint8_t n) { buffer_.push_back(n); } void ByteBuffer::encode16Bits(uint16_t n) { encode8Bits(uint8_t((n & 0xff00) >> 8)); encode8Bits(uint8_t((n & 0x00ff) )); } void ByteBuffer::encode32Bits(uint32_t n) { encode16Bits(uint16_t((n & 0xffff0000) >> 16)); encode16Bits(uint16_t((n & 0x0000ffff) )); } void ByteBuffer::encode64Bits(uint64_t n) { encode32Bits(uint32_t((n & 0xffffffff00000000) >> 32)); encode32Bits(uint32_t((n & 0x00000000ffffffff) )); } This remains nicely platform agnostic since the numerical representation is always logically Big-Endian. This code also lends itself very nicely to using templates based on the size of the primitive type (think encode<sizeof(val)>((unsigned char const*)&val))... not so pretty but very very easy to write and maintain.  If you don't have natural alignment in the structures compilers will usually insert padding so that alignment is proper. If however you use pragmas to ""pack"" the structures (remove the padding) there can be very harmful side affects. On PowerPCs non-aligned floats generate an exception. If you're working on an embedded system that doesn't handle that exception you'll get a reset. If there is a routine to handle that interrupt it can DRASTICALLY slow down your code because it'll use a software routine to work around the misalignment which will silently cripple your performance.  My experience is that the following approaches are to be preferred (in order of preference): Use a high level framework like Tibco CORBA DCOM or whatever that will manage all these issues for you. Write your own libraries on both sides of the connection that are are aware of packing byte order and other issues. Communicate only using string data. Trying to send raw binary data without any mediation will almost certainly cause lots of problems. In very old versions of TIBCO (e.g. back when it was still called Teknekron; TIB = Teknekron Information Bus) running on SPARC hardware one had to memcpy doubles out of received messages and into properly-aligned storage avoid SIGBUS.  You look into Google protocol buffers or Boost::serialize like another poster said. If you want to roll your own please do it right. If you use types from stdint.h (ie: uint32_t int8_t etc.) and make sure every variable has ""native alignment"" (meaning it's address is divisible evenly by it's size (int8_ts are anywhere uint16_ts are on even addresses uint32_ts are on addresses divisble by 4) you won't have to worry about alignment or packing. At a previous job we had all structures sent over our databus (ethernet or CANbus or byteflight or serial ports) defined in XML. There was a parser that would validate alignment on the variables within the structures (alerting you if someone wrote bad XML) and then generate header files for various platforms and languages to send and receive the structures. This worked really well for us we never had to worry about hand-writing code to do message parsing or packing and it was guaranteed that all platforms wouldn't have stupid little coding errors. Some of our datalink layers were pretty bandwidth constrained so we implemented things like bitfields with the parser generating the proper code for each platform. We also had enumerations which was very nice (you'd be surprised how easy it is for a human to screw up coding bitfields on enumerations by hand). Unless you need to worry about it running on 8051s and HC11s with C or over data link layers that are very bandwidth constrained you are not going to come up with something better than protocol buffers you'll just spend a lot of time trying to be on par with them."
745,A,"how does my web browser resolve Domain Names? I'm developing a network application which should be capable of contact DNS servers. I was wondering what would be the best way to do it. And browsers came to my mind. For example how Firefox or Chrome resolve the Domain names i put in the URL bar? I mean i type http://www.google.com how does it know that has to make a TCP request to the IP 209.85.195.104? Thanks! I've read around that DNS resolution is made by your ISP but I don't know how much this is true and don't remember where I read it sorry. Yes in fact in my resolv.conf the first 4 nameservers (from 5) belongs to my ISP. Try this: http://www.codeproject.com/script/Articles/ViewDownloads.aspx?aid=7992 Relevant RFC is rfc1035.txt or google for DNS resolver source. Thanks! Very useful!  Here is a good comic that describes how your browser resolves host names: http://www.labnol.org/internet/comic-how-browser-works/18086/ Your environment should be able to handle host name resolution for you. Is there a reason you would need to implement this yourself? There's also the pretty nifty video on DNS: http://www.youtube.com/watch?v=hcaJ1Vp_Ntg Thanks but i'm not an idiot. I know how DNS servers work. Just want to know how technically web browsers do it. @santiagobasulto: I imagine their code hits the host OS' API to get this information. I wouldn't think they'd do it manually. Something like this: http://msdn.microsoft.com/en-us/library/ms682016%28v=vs.85%29.aspx Thanks David! Do you have any article book or something like that to read a little bit more about it? Actually I'm using Java then i've a magic class with a magic method: InetAddress.getAllByName(String name) The thing is that i like to know how everything works. Thanks again.  In the simplest scenario browsers would use a function such as gethostbyname() to resolve names to addresses. However this function is not always implemented in such a way that's convenient for a browser (it usually blocks until it gets an answer). Browsers today are starting to use ""DNS prefetch"" where the browser will send DNS requests directly to a DNS server as the page is loading to resolve addresses before the user clicks on the next link. That way the user doesn't have to wait for name resolution when they click and the browsing experience appears faster."
746,A,"Forgin/Buiilding TCP packets in ANSI C How do I without using third-party tools craft TCP (and even UDP for that matter) packets in ANSI C? I want to be able to set all option flags source ip address etc. So full control. Haven't found any good text about it online. Or I'm just using the wrong search criteria. It's not a complete code example to describe it all but I found this now for all interested: http://www.tenouk.com/Module42a.html. Look under ""Raw vs Coocked socket"". But if anyone has more info and code examples please write an answer. When I started cooking raw sockets on my own I found Beej's guide to network programming as valuable as venerable Stevens ""TCP/IP Illustrated"" books serie."
747,A,"A socket operation was attempted to an unreachable host One customer reported this error at my WCF client connecting to our server service. ""Message: A socket operation was attempted to an unreachable host Type : System.Net.Sockets.SocketException"" From this link http://msdn.microsoft.com/en-us/library/ms740668%28v=vs.85%29.aspx i see: WSAEHOSTUNREACH 10065 No route to host. A socket operation was attempted to an unreachable host. See WSAENETUNREACH. WSAENETUNREACH 10051 Network is unreachable. A socket operation was attempted to an unreachable network. This usually means the local software knows no route to reach the remote host. Is this a network error on customer network? Is it something we can advise customer to do? WCF Client was able to connect to the server service before and seems that problem disappeared after reboot of machine (or maybe network problem was fixed in the meantime). Thank you Adriana Yes network error or glitch on the one machine. You can help your customers or FAEs to develop an SOP for identifying connectivity issues. Say pinging the server IP from client machine do an ipconfig and see if the machine's network adapter is up or not.  This error indicates that the network was not connected or not configured correctly. It is definitely an error on the client machine not your server. There isn't much you can do to ""solve"" the problem. Pretty much all you can do is upgrade the client's network drivers and check for connection problems (maybe they're barely within wireless range or the Ethernet cable is missing its locking tab)."
748,A,Is there a program that can send data via sockets to a server to test it? I have a simple socket server that waits for a message and responds. How can I test this? Is there a client app or something that can help me send some data on a specific port to a server and see the output? Telnet? Windows XP? That's what I'm working on right now. What OS do you want to run it on? Windump - tcpdump works well and has binaries available to Windows  hit the command line and just use telnet. if you have a simple echo server go to town.  Simple telnet client works well for such tests. You can also try PuTTY in either Telnet or Raw connection modes. Both allow you to choose the port you want to connect to. Also a tool like Microsoft Network Monitor is pretty useful to analyse the protocol's data flow if you don't have direct control (by logs) on what's being sent over the wire.  netcat should be able to handle anything you need to do with socket testing. http://pctechtips.org/netcat-the-swiss-army-knife-useful-commands/#more-103
749,A,"Python TCP stack implementation Is there a python library which implements a standalone TCP stack? I can't use the usual python socket library because I'm receiving a stream of packets over a socket (they are being tunneled to me over this socket). When I receive a TCP SYN packet addressed to a particular port I'd like to accept the connection (send a syn-ack) and then get the data sent by the other end (ack'ing appropriately). I was hoping there was some sort of TCP stack already written which I could utilize. Any ideas? I've used lwip in the past for a C project -- something along those lines in python would be perfect. I ended up implementing a pretty basic TCP stack in Python after all. It is just what I needed for the project I was working on but could use some work. If you'd like to look at it anyway the source is here: http://github.com/dound/vns/blob/master/TCPStack.py I know this isn't directly Python related but if you are looking to do heavy network processing you should consider Erlang instead of Python. Just a suggestion really... you can always take a shot a doing this with Twisted... if you feel adventurous (and have lots of time on your side) ;-) Why Erlang specifically? Why not C? C++? Java? etc... Because Erlang lands itself very well for doing asynchronous processes. I voted this back up - whoever voted it down should be ashamed of yourself - there is nothing wrong with asking someone to question their tooling requirements. With that said - why erlang specifically? Why Erlang? because doing packet manipulation is one of the reason Erlang exists in the first place: it was born out of Ericsson for their network products.  You don't say which platform you are working on but if you are working on linux I'd open a tun/tap interface and get the IP packets back into the kernel as a real network interface so the kernel can do all that tricky TCP stuff. This is how (for example) OpenVPN works - it receives the raw IP packets over UDP or TCP and tunnels them back into the kernel over a tun/tap interface. I think that there is a tun/tap interface for windows too now which was developed for the OpenVPN port to windows. I like this idea. I'm using linux so getting the tun/tap device and a tap0 intf setup wasn't hard. Unfortunately I'm at a loss on how to make use of it. To start I'm writing my raw unencapsulated packet onto tap0 (with a raw socket). I see the correctly form SYN packet show up on tap0 (addressed to tap0's MAC and IP addresses). However if I run netcat listening on tap0's IP address I don't see it respond with a syn-ack. I am able to connect to it with telnet (though the kernel seems to just shortcut this traffic through lo). Any pointers on how I could get this to work? Thanks Nick!  If you are already committed to the software at the other end of the socket that is forwarding TCP packets to you then perhaps TCPWatch will show you how to get at the SYN packets. SCAPY is certainly great for sending exactly the packets that you want but I'm not sure that it will work as a proxy. http://hathawaymix.org/Software/TCPWatch However if you are not committed to what is on the sending end then consider using Twisted Conch or Paramiko to do SSH forwarding. Even if you don't need encryption you can still use these with blowfish which has a low impact on your CPU. This doesn't mean that you need Conch on the other end since SSH is standardised so any SSH software should work. In the SSH world this is normally referred to as ""port forwarding"" and people use an SSH terminal client to log into an SSH server and set up the port forwarding tunnel. Conch and Paramiko allow you to build this into a Python application so that there is no need for the SSH terminal client.  You might be able to use the ctypes module to import lwip and use it again. Not a bad idea but I think this would involve a lot more work than I'm looking for on this one.  Glancing over Scapy it looks like it might be able to handle these low-level situations. I haven't used it myself so I can't confirm that it does what you've explained; I've only glanced over the documentation. Scapy is neat but I don't think it will help me with handling the protocol itself -- just decoding the packet into its constituent fields?"
750,A,How to get Client IP in Lua Sockets I'm having trouble locating how I go about getting the ip address of each client as they connect to my server using LuaSockets; Also: I apologise if this has been answered in another post but I could find it; Link me if this is the case. Cheers! getpeername should work.  Use conn:getpeername() it returns host and port of the client. Thanks! oh and you had a small typo it should be: conn:getpeername() Fixed thank you.
751,A,"Using SO_REUSEADDR - What happens to previously open socket? In network programming in unix I have always set the SO_REUSEADDR option on the socket being used by server to listen to connections on. This basically says that another socket can be opened on the same port on the machine. This is useful when recovering from a crash and the socket was not properly closed - the app can be restarted and it will simply open another socket on the same port and continue listening. My question is what happens to the old socket? Without a doubt all data/connections will still be received on the old socket. Does it get closed automatically by the OS? To clarify: After a program crashes the OS e.g. Linux Kernel will close the socket automatically. There are protocol-reasons (TCP) why you don't want to reopen a connection immediately. A must read on this topic: http://www.serverframework.com/asynchronousevents/2011/01/time-wait-and-its-design-implications-for-protocols-and-scalable-servers.html A socket is considered closed when the program that was using it dies. That much is handled by the OS and the OS will refuse to accept any further communication from the dead conversation. However if the socket was closed unexpectedly the computer on the other end might not know that the conversation is over and may still be attempting to communicate. That is why there is designed into the TCP spec a waiting period before that same port number can be reused. Because in theory however unlikely it may be possible for a packet from the old conversation to arrive with the appropriate IP address port numbers and sequence numbers such that the receiving server mistakenly inserts it into the wrong TCP stream by accident. The SO_REUSEADDR option overrides that behavior allowing you to reuse the port immediately. Effectively you're saying: ""I understand the risks and would like to use the port anyway.""  Yes the OS automatically closes the previous socket when the old process ends. The reason you can't normally listen on the same port right away is because the socket though closed remains in the 2MSL state for some amount of time (generally a few minutes). The OS automatically transitions the old socket out of this state when the timeout expires. Ahh.. of course. The socket is just a file handle after all and will get cleaned up by the OS. Thank you very much :) To be precise the TCP endpoint remains in the 2MSL state. The socket is gone. The Waiting-Period was added to the spec for a reason. Using SO_REUSEADDR causes the TCP stack to violate the waiting-period. (Your 2MSL State)"
752,A,"How to use gethostbyname_r in linux I am currently using thread unsafe gethostbyname version which is very easy to use. You pass the hostname and it returns me the address structure. Looks like in MT environment this version is crashing my application so trying to replace it with gethostbyname_r. Finding it very difficult to google a sample usage or any good documentation. Has anybody used this gethostbyname_r method ? any ideas ? How to use it and how to handle its error conditions if any. Found one blog about this but the code is not working. http://dimitry-i.blogspot.com/2010/09/how-to-use-gethostbynamer-correct.html The function is using a temporary buffer supplied by the caller. The trick is to handle the ERANGE error. int rc err; char *str_host; struct hostent hbuf; struct hostent *result; while ((rc = gethostbyname_r(str_host &hbuf buf len &result &err)) == ERANGE) { /* expand buf */ len *= 2; buf = realloc(buf buflen); if (NULL == buf) { perror(""realloc""); } } if (0 != rc || NULL == result) { perror(""gethostbyname""); } EDIT In light of recent comments I guess what you really want is getaddrinfo. Thanks! Do you know how to derive server_addr.sin_addr which I need to pass to the socket connect call ? @harry See my edited answer. If you don't get `getaddrinfo` (or plain don't like it) ask another question. Yeah that solved my problem. Though rare but assigning the return value of realloc to the original pointer variable may cause memory leakage."
753,A,"How can I loop through an IP address range? I want to perform a set of network tasks on an IP address range. As soon as the range is getting bigger than a class c network I fail to enumerate all hosts in the range. I want to be able to iterate through all hosts of a network with the netmask 255.255.240.0.  From: 192.168.0.100 To: 192.168.10.100 How would one approach this? It must be a pretty common task. I come from the green fields of Cocoa iPhone programming so a C-stylish solution would be appreciated. :-) Am I reading the question wrong? There actually aren't any addresses lying between 192.158.0.255 and 192.168.1.0. Perhaps you need to edit to make it more clear. alxp: you're assuming that the normal IP address map is being used. At least on my home router I'm not aware of anything stopping me from running the LAN on 123.234.123.# despite that being a completely incorrect use of the addressing range available thanks. I editet my question. The range was indeed nonsense This is a piece of code that will quickly introduce you to the nuances involved in interpreting the IP Address and iterating through it. Things get quite simple once you start looking at an IP Address as a 32-bit unsigned integer. #include <stdio.h> int main (int argc char *argv[]) { unsigned int iterator; int ipStart[]={1921680100}; int ipEnd[] = {19216810100}; unsigned int startIP= ( ipStart[0] << 24 | ipStart[1] << 16 | ipStart[2] << 8 | ipStart[3]); unsigned int endIP= ( ipEnd[0] << 24 | ipEnd[1] << 16 | ipEnd[2] << 8 | ipEnd[3]); for (iterator=startIP; iterator < endIP; iterator++) { printf ("" %d.%d.%d.%d\n"" (iterator & 0xFF000000)>>24 (iterator & 0x00FF0000)>>16 (iterator & 0x0000FF00)>>8 (iterator & 0x000000FF) ); } return 0; } Just check that none of the elements for ipStart and ipEnd are greater than 255. That will not be an IP Address and it will mess up the code too.  Here's a PHP solution: <?php $sIP1 = '192.168.0.0'; $sIP2 = '192.168.1.255'; $aIPList = array(); if ((ip2long($sIP1) !== -1) && (ip2long($sIP2) !== -1)) // As of PHP5 -1 => False { for ($lIP = ip2long($sIP1) ; $lIP <= ip2long($sIP2) ; $lIP++) { $aIPList[] = long2ip($lIP); } } ?> There's a good summary of the (basic) maths involved here  Use the modulus operator %. Here is a primitive example: #include <stdio.h> int main(void) { int counter; unsigned int ip[] = { 192 168 0 0 }; for ( counter = 0; counter < 1000; ++counter ) { ip[3] = ( ++ ip[3] % 256 ); if ( !ip[3] ) { ip[2] = ( ++ ip[2] % 256 ); } printf(""%u:%u:%u:%u\n"" ip[0] ip[1] ip[2] ip[3]); } return 0; }"
754,A,Switching Endian of encrypted char array I am working on a project at school where we set up a simple key distribution center on our network encrypted with blowfish. I have successfully coded it and have it working on same-endian machines. The issue arises when I have to send it to a machine of a different endian. The key is encrypted as a character array and sent and received across the network as such. When the encrypted key is printed on either end it displays the same encrypted string but decrypting fails. I have tried reversing the order of the array and decrypting but the results are the same. My questions: Is my reversing of the char array the proper way to deal with endian issues in this situation? Could the issue be that it has been encrypted on a machine of one endian style and there for can not be decrypted with the same algorithm on a little endian machine? (here is the version of blowfish I used: http://www.codeproject.com/KB/security/blowfish.aspx) yes. you should change somehow the algo why don't you encode in the opposite endianess so that it decrypts properly? It depends on the algorithm implementation. Looking at the implementation your are using (look at the BytesToBlock and BlockToBytes functions) it is casting the blocks of bytes to unsigned ints. This transformation is endian dependant and so the the algorithm will have to be adjusted based on the endianness of the machine it is being run on.  Endianness applies to smaller elements in your data. If a different endian machine can't read your char array then it possibly means it is expecting bytes in the same order but with their bits reversed. So you need to reverse the order of bits in your char elements. Your array order should remain intact.  It is possible that the bytes are reversed in some capacity but that usually only happens with numbers (see http://beej.us/guide/bgnet/output/html/multipage/htonsman.html). More likely it is the bits with in each char. You just need a simple algorithm to switch them around before decrypting. Of course if the bits AND bytes are reversed that's even more fun. Now the fact that it displays the same string leads me to believe that the issue is not actually in the character array - it's something in the decryption algorithm itself that is depending on byte order. In that case you definitely need to look at the above link and think about operations you perform on multi-byte types. (64-bit endian switchers generally have to be manually written but byte-swapping isn't a hard algorithm to write.)
755,A,"Qt- QNetworkReply delete operator- Runtime Crash In my Qt application I am using QNetworkAccessManager in a Thread so as to keep my main thread free to do its task. For every get operation that I do I am storing the QNetworkReply* in a list and upon a response I retrieve it from my list delete the entry in the list and call deleteLater() on the QNetworkReply* object. However after a couple of request/responses here is the crash i get in runtime: The code that I used is: void NetworkManager::responseFromServer(QNetworkReply* pReply) { // Retrieve the TileRequestMessage. QImage *pImage = imageMapper.value(pReply); // Get the bytes from the response. QByteArray byteArray = pReply->readAll(); // Load the QImage with the data. bool loaded = pImage->loadFromData(byteArray); // Remove the request from book-keeping. imageMapper.remove(mapIterator.key()); pReply->deleteLater(); return; } where pImage is a pointer to a object of type QImage. The Object is created in advance and its pointer mapped to a QNetworkReply* is stored in a QMap. The error I get is: Stopped at 0x637837aa (operator delete) in thread 1 (missing debug information). sException at 0x637837aa code: 0xc0000005: read access violation at: 0xffffffffcdcdcdc1 flags=0x0 The call stack is: 0 operator delete MSVCR90D 0 0x637837aa 1 QList::node_destruct qlist.h 418 0x64071704 2 QList::free qlist.h 744 0x6407153b 3 QList::~QList qlist.h 718 0x64070b1f 4 QQueue::~QQueue qqueue.h 58 0x6407076f 5 QNetworkReplyImplPrivate::handleNotifications qnetworkreplyimpl.cpp 358 0x6406c99d 6 QNetworkReplyImpl::event qnetworkreplyimpl.cpp 868 0x6406e646 7 QApplicationPrivate::notify_helper qapplication.cpp 4445 0x6507153e 8 QApplication::notify qapplication.cpp 3845 0x6506f1ba 9 QCoreApplication::notifyInternal qcoreapplication.cpp 732 0x671c2fb1 10 QCoreApplication::sendEvent qcoreapplication.h 215 0x671c8159 11 QCoreApplicationPrivate::sendPostedEvents qcoreapplication.cpp 1373 0x671c3f0b 12 qt_internal_proc qeventdispatcher_win.cpp 506 0x67206bf9 13 IsThreadDesktopComposited USER32 0 0x77bb86ef 14 IsThreadDesktopComposited USER32 0 0x77bb8876 15 IsThreadDesktopComposited USER32 0 0x77bb89b5 16 DispatchMessageW USER32 0 0x77bb8e9c 17 QEventDispatcherWin32::processEvents qeventdispatcher_win.cpp 807 0x67207b96 18 QEventLoop::processEvents qeventloop.cpp 150 0x671c0abe 19 QEventLoop::exec qeventloop.cpp 201 0x671c0bf0 20 QThread::exec qthread.cpp 490 0x670643d6 21 DispatcherThread::run DispatcherThread.cpp 226 0x1001031a 22 QThreadPrivate::start qthread_win.cpp 317 0x6706852f 23 beginthreadex MSVCR90D 0 0x636edff3 24 beginthreadex MSVCR90D 0 0x636edf89 25 BaseThreadInitThunk kernel32 0 0x77191194 26 RtlInitializeExceptionChain ntdll 0 0x77ccb429 27 RtlInitializeExceptionChain ntdll 0 0x77ccb3fc I am using msvc to compile my Qt code. Any heads-up on what the problem might be ?? Thanks Vishnu. Some code on how you manage (and delete) the QNetworkReply objects might help. modified the question to add the code.. Just an idea: Since you use deleteLater() you do not know when the delete will take place and thus when the pointer QNetworkReply* may be invalid in your list. Thus maybe try wrapping your pointer in a guared pointer (QPointer) and then just remove it from the list if it deleted/null. If it's still a valid pointer you call deleteLater();  Without looking at your actual code and based on your error description it could be possible that you are deleting the QNetworkReply before it has emitted the finished signal. So after the deletion when new data becomes available - QNetworkReply emits the readyRead signal which is when it would be trying to access the already deleted entry and hence the ""read access violation"" errors. That's the exact reason I am calling the deleteLater() method in my slot. Also I am wondering how can QNetworkReply emit readyRead signal after the QNetworkReply object has been deleted. Also on further investigation i found that sometimes the finished() signal of QNetworkReply* fires after the finished() signal of QNetworkAccessManager (where the QNetworkReply*) is deleted and thus sender in my slot connected to finished() signal in QNetworkReply* is marked 0x00. If that is the case where should i delete my QNetworkReply* object ??? For your first comment QNetworkReply object is not created by you and it is obtained in the `QNetworkAccessManager::get`. So this means you do not have control on the QNetworkReply object thus created and hence it can emit the `readyRead` signal whenever there is new data available to read. Please refer to the documentation of the `QNetworkAccessManager::finished` slot[link](http://doc.qt.nokia.com/latest/qnetworkaccessmanager.html#finished). It is clearly mentioned not to perform `delete` of the QNetworkReply object in this slot. So may be you can try deleting the QNetworkReply object using deleteLater() at the end of all network accesses. Before performing the `deleteLater()` on the QNetworkReply object you can check if it has finished the processing using `isFinished()`. Thanks for the suggestion I did see a crash after the such a change. :) You can try calling `QNetworkReply::abort()` on the QNetworkReply objects and then issue the `deleteLater()` operation. Please refer to the [abort](http://doc.qt.nokia.com/latest/qnetworkreply.html#abort) for more information. @Purnima: I apologize if I sounded like i am using delete operator itself. I am using deleteLater() to delete the instance of QNetworkReply objects. Is there a way for me to stop all the activity of QNetworkReply* once I call my deleteLater () ??"
756,A,"SendToAsync memory leak? I have a simple .net 3.5sp1 windows application (in C#) that acts as a UDP server. It listens on a port receives data from an endpoint and then retransmits what it receives to another endpoint (i.e. a relay for a live broadcast data stream). What I'm experiencing is after the connection is up for about 20 minutes it starts to deteriorate. Also I notice that it is gobbling up about 50-100K of memory per second which doesn't ever get released after GC. I have to close the app and restart it. Not good. I have narrowed down the problem to the following code which does the retransmission to the other side: var sendBuffer = new byte[readCount]; Array.Copy(readData sendBuffer readCount); SocketAsyncEventArgs args = new SocketAsyncEventArgs(); args.RemoteEndPoint = p.EP; args.SetBuffer(sendBuffer 0 sendBuffer.Length); SwitchWindow.LineSocket.SendToAsync(args); Does anyone have any experience with memory leaks with SendToAsync? UPDATE: I instantiate a state object (only done once) when the socket is initialized. The state object has a property called ""buffer"" which is a byte array. I receive data async from the socket like such: private void beginReceiveData(ref MessageState state) { var ipeSender = new IPEndPoint(IPAddress.Any 0); var epSender = (EndPoint)ipeSender; state.workSocket = LineSocket; state.EP = epSender; state.workSocket.BeginReceiveFrom(state.buffer 0 MessageState.BufferSize SocketFlags.None ref epSender new AsyncCallback(ReceiveDataCB) state); } And then on my callback (ReceiveDataCB) I am retrieving the async object and then passing the byte buffer to another function for processing which in turn calls the code posted above for retransmission to the other side (state.buffer becomes readData). UPDATE #2: Following my gut I changed the sending code to the following getting rid of SocketAsyncEventArgs and SendToAsync: var sendBuffer = new byte[readCount]; Array.Copy(readData sendBuffer readCount); SwitchWindow.LineSocket.BeginSendTo( sendBuffer 0 sendBuffer.Length SocketFlags.None p.EP new AsyncCallback(echoCB) null); And of course I added an ""echoCB"" callback that does nothing other than calling EndSendTo. The memory leak is now gone! I suspect it had something to do with creating so many SocketAsyncEventArgs objects and the async function hanging onto these one for each packet (at 33 packets per second that can add up fast). I looked once again at the MSDN documentation for SocketAsyncEventArgs and I noticed that on the server ""example"" code provided they used a pool of SocketAsyncEventArgs objects. I don't think its really designed to work the way I was using it. I think the whole point is to not have to instantiate these buffers etc for each call therefore reusing them and allowing the server better performance. You can always use !gcroot <address> to track down who's keeping a reference on your objects. See CLR Memory Leak.  Where is the readData buffer allocated? Is the whole receive/send loop inside the same scope? see my edits I posted  You are probably not calling SocketAsyncEventArgs.Dispose()  See my answer posted in UPDATE #2 (above)  I think your relay node is doing much more work receiving data and is ultimately receiving data faster than it is sending. As data comes in off the wire you are allocating memory for it and queuing that up to be sent async style but your outgoing data may be sent much slower than you are receiving (and allocating for) new data resulting in an overload of incoming data. This is often the case on network nodes that have higher download speeds than upload speeds. You can throttle the amount of incoming data you accept and allocate for. You can use a callback to do this (args.Completed) by tracking the amount of data queued up to be sent versus actually sent (by tracking sent in the callback). If the amount of data waiting to be sent is larger than some arbitrary limit don't accept new data. If you are doing streaming video you may have to skip frames or discard entire packets. Good thought. I am doing streaming audio however it does not get behind or skip. In fact I am monitoring the bytes/in bytes/out and they are the same. I am able to maintain about 1/3 second delay due to transmission."
757,A,"problem with char arrays in network programming I have the following: nodeID = abcde.abc.edu_12345 and I need to append the an underscore and a 10digit value returned by time() to create the following: node_inst_ID = abcde.abc.edu_12345_1016320007 where 1016320007 is the 10 digit value returned by time(). I am trying the following code but it doesnt seem to work: #define NODE_ID_LENGTH 20 #define NODE_INST_ID 31 char nodeID[NODE_ID_LENGTH] char node_inst_ID[NODE_INST_ID]; int main() { /* Building of nodeID is a bit complex. So its hard to put all the code here. But printing the following at this point gives this for(int i = 0; i < NODE_ID_LENGTH; i++) { printf(""%c"" nodeID[i]); } gives abcde.abc.edu_12345 If you need to know any more info about nodeID I can post the print out of things that you suggest. */ long int seconds = time(NULL); char buf[10]; sprintf(buf ""%ld"" seconds); snprintf(&node_inst_ID[0] 20 ""%s"" &nodeID[0]); node_inst_ID[20] = '_'; node_inst_ID[21] = '\0'; cout<<""Node instance ID = ""<<node_inst_ID<<endl; /* I havent yet added the code for appending the time stamp. But I am expecting a print out for the above code like this: Node instance ID = abcde.abc.edu_12345_ But the code is printing only Node instance ID = abcde.abc.edu_12345 It is not adding the underscore at the end. */ } Can anyone point out what the mistake is? Thanks in advance. snprintf(&node_inst_ID[0] 20 ""%s"" &nodeID[0]); node_inst_ID[20] = '_'; node_inst_ID[21] = '\0'; This assumes that the string is exactly 20 characters long - it isn't. Try this: snprintf(&node_inst_ID[0] 20 ""%s"" &nodeID[0]); strcat(node_inst_ID ""_""); // strcat is safe in this context usually you should use strncat. EDIT: If you want to still use snprintf the easy way is: int len = strlen(node_inst_ID); snprintf(node_inst_ID + len sizeof(node_inst_ID) - len ""%whatever"" the args); thanks. using strcat worked. Also how do I print out the actual data in a char array? For e.g. I even want to print out that I have a '\0' at the end of the array. But if I use printf(""%c"" char_array[LIMIT]); it doesnt print out '\0'. i could find something out by using %x in printf. printf(%x char_buf[i]); and incrementing i printed out 0 for '\0'  nodeID is only 19 characters long. So it has a \0 terminator in position 19 and so does the copy you make in node_inst_ID. Then you store some more stuff starting at position 20 after the terminating zero-byte. So your underscore is after the end of the string.  You've hardwired the underscore to be at index 20 of node_inst_ID however it only contains 18 characters and the null terminator at that point: 0123456789012345678 abcde.abc.edu_12345 Since you are not overwriting the null terminator that's where the printout stops when you print the result. Don't assume positions and lengths for strings; instead use string library functions to manipulate them."
758,A,"Receiving serialized custom message types over a network how to cast? I'm writing a networked application. Assume a class called Packet. This class has some public properties. public class Packet { public Module DestinationModule { get; set; } public EncryptionCompressionFlag EncryptedOrCompressed { get; set; } public PacketPriority Priority { get; set; } public Origin Origin { get; set; } // You'll see why I need this later // This StackOverflow question is asking how I can do without this property public Type UltimateType { get; set; } } Now assume a class called LoginPacket. This class extends Packet and includes even more properties. public class LoginPacket : Packet { public string Username { get; set; } public string Password { get; set; } } But LoginPacket is not the only type of custom packet. There are many more custom packets. My client application is constantly serializing custom packets and sending them over the wire. When they are deserialized on the other side however I can only deserialize them as a Packet not as a specific LoginPacket. Question: How can I know the 'ultimate' or 'original' specific type of the Packet? Per the example the specific type would have been LoginPacket. I don't know the specific type because I receive it over the wire simply as a Packet and without a ""type"" property (like UltimateType which sounds lame) I don't know how to discover the specific original type. Response: ""How do you serialize the packet?"" I use YAXSerializer. I tested a specific serialization and deserialization of LoginPacket and it was successful. I was able to deserialize the string (YAX serializes to a string not byte array difference doesn't really matter..) into a Packet and a LoginPacket.  public static string SerializePacket(Packet packet) { YAXSerializer serializer = new YAXSerializer(packet.UltimateType); return serializer.Serialize(packet); } How are you serializing the packets? you don't need the tags (C#/.net etc) in the title of the question. That's what the tags are for :) Otherwise good question. According to the YAXSerializer docs the opening XML element for each class is the class name... so each LoginPacket in your string should look like... <LoginPacket ...> .... </LoginPacket> Just parse the string yourself and look for the class name if you don't want to put it in a property of the 'Packet' class. Hmm that seems kind of obvious don't know why I didn't think of that... If it answers your question could you mark it as an answer? Thanks !"
759,A,how to send a complete (.txt.doc) file in java i am making a client server program in java a portion of which requires to send a complete file (.txt or .docx) file from client to server. I am not sure how to do it i have tried using this webpage http://www.rgagnon.com/javadetails/java-0542.html  but it seems that it don't work correctly in form + multi-threaded application is there any other way to send a complete file from client to server? Any help/suggestions are appreciated... regards usama the first question you need to answer for yourself is if you need to send that file through http and the server is an application server. Depending on that answer you would do it in different ways. It would be better to use a Messaging Application for your purpose. I would recommend using JMS. I personally use ActiveMQ . If this does not suit your need then try out Apache Mina. It would abstract you from the Network Programming. hi Abhishek!  thanks for ur reply i will explore ur suggested links but i was wandering that is it possible to do it without using any external libraries ? just by using the framework of java i just found that java serialization can be helpful? Yes Java Serialization would be better if you only wish to use Core Java components and no Web Components.  In principle you have to read the file on your client application open an OutputStream to the server and write the contents of the file to this stream. On the server you read from the other end of the stream. Depending on your architecture you either will use a new socket-pair for the file transfer or somehow embed it into your existing communication protocol. hi Paulo! thanks for the answer ur advice is really very helpful i will try to implement it.
760,A,"How can non-exclusive ports exist when tcp identify applications using 16-bit port number? I can't come up with the exact socket apibut I remember that there is a socket option that demonstrates the port exclusive/non-exclusive. If it's not exclusivehow can TCP know which application it should forward to for a specific destination port? A socket is identified by the IP address and the port - both. Together in the BSD socket API they are called a ""name"". You can bind your socket to a name and if that name comprises a specific address you can bind another socket to another name with the same port. Also: a connection is a pair of two sockets so a single address/port pair can be used in several connections - meaning more than one connected socket can have the same name (but not the same fd). Can you illustrate by some examples?  I think you might be referring to the SO_REUSEPORT option available on some systems. From the BSD man page: SO_REUSEPORT allows completely duplicate bindings by multiple processes if they all set SO_REUSEPORT before binding the port. This option permits multiple instances of a program to each receive UDP/IP multicast or broadcast datagrams destined for the bound port. Implementations vary a lot for this (from non-existant to restricted to UDP to allowing TCP also). In the cases where TCP is allowed the connexions are distinguished by both source and target (ipport) pairs. This is sufficient to allow the implementation to decide which app needs which packet. (see Trek - Socket options for instance.) With multiple apps bind to the same TCP port you could only have one socket accepting on the port. The others would use the port to initiate outbound connections. The TCP stack always knows where to send the packets to. incoming packets that initiate (SYN) a connection go to the only accepting socket incoming packets for a connected stream are routed to the socket they belong to Note: the sockets themselves (including I believe the accepting socket) can be shared across multiple processes. See Is there a way for multiple processes to share a listening socket? for example. Here's how it could work. Voluntarily simplifying TCP (no three-way handshake). Let's note the socket information held by the TCP stack like this (socketname)[owner app (local IP local port) (state remote IP remote port)] With that let's set up three apps A B and C: App A -> bind (localhost12345SO_REUSEPORT) TCP stack: create socket (s1)[belongs to A (localhost12345) (not connected)] App A <- s1 App B -> bind (localhost12345SO_REUSEPORT) TCP stack: create socket (s2)[belongs to B (localhost12345) (not connected)] App B <- s2 App C -> bind (localhost12345SO_REUSEPORT) TCP stack: create socket (s3)[belongs to C (localhost12345) (not connected)] App C <- s3 App C -> s3.listen() TCP stack: update socket (s3)[belongs to C (localhost12345) (open for business)] App C -> s3.accept() At this point no data has been sent but the kernel knows exactly what socket belongs to what application. Let's have A actually try to do something with its socket: App A -> s1.connect(otherhost54321) TCP stack: update socket (s1)[belongs to A (localhost12345) (connecting otherhost54321)] TCP stack: send SYN Here three things can happen: incoming ACK packet from (otherhost54321) with correct sequence: this is fine it's for s1 no other possibility  TCP stack: update socket (s1)[belongs to A (localhost12345) (connectedotherhost54321)] TCP stack: send SYN/ACK TCP stack: notify App A that socket is connected App A <- connect succeeded you can start doin' stuff incoming SYN packet from (client4343): can only be for s3 only socket ready for SYN  TCP stack: create new socket (s4)[belongs to C (localhost12345) (connectedclient4343)] TCP stack: send ACK to client TCP stack: notify App C that (s4) has been accepted App C <- s4 returned from accept() incoming packet from somewhere else:  TCP stack: drop or reject there are no matching sessions Let's imagine the two things above happened. The kernel information is now: (s1)[belongs to A (localhost12345) (connectedotherhost54321)] (s2)[belongs to B (localhost12345) (not connected)] (s3)[belongs to C (localhost12345) (open for business)] (s4)[belongs to C (localhost12345) (connectedclient4343)] Now four kinds of packets can come in: incoming normal packet from (otherhost54321): this matches s1 hand it over to App A incoming normal packet from (client4343): this matches s4 hand it over to App C incoming SYN packet from (otherclient2398): this matches s3 same as before anything else: drop or reject invalid state The TCP stack always nows which socket a packet belongs to. So it knows where to deliver the data. SO_REUSEADDR is different: it only allows to bind to a port in TIME_WAIT state - i.e. the port is already closing down the socket that had opened it has been issued a close already (more or less there are other conditions). With SO_REUSEADDR only one socket at a time holds the socket open. sorrybut it's still too abstract for me to make sense of it... @mysql_go: updated answer can't do more than that. how can TCP know which application it should forward to for a specific destination port? it's not really a matter of selecting the application it's a matter of selecting the right socket which is doable with both source and destination (ip/port) pairs. What application uses which socket is not really a problem that's dealt with like other I/O aspects (i.e. open files on Unix-like plateforms)"
761,A,"Designing ""Scalable Multi Threaded Socket Push Server"" in .Net We are at the beginning of designing Scalable Multi Threaded Socket Push Server in .Net which will initially handle 10K clients per node. Since designing this can be very complex it would really help me if you all experts can provide any guidence resources best practices etc. Since I dont know much about socket programming it would really good if someone can suggest me an opensource project to get start with... I googled it but couldn't find any good solution. This will also be helpful for other people who would like to design such server. [For your information we are planning to use .Net 4.0 for this server] I have been reading lots of information regarding Async Socket and its Pinning related problem. Is that problem still exists in .Net 4.0? Thanks in advanced... don't take this the wrong way but given you have little socket programming experience is designing ""Scalable Multi Threaded Socket Push Server in .Net which will initially handle 10K clients per node"" a good fit? I have little socket experience but I believe I am a very good developer. And also I have got some time to study the subject before starting this project so that is fair enough.... I also belive there will be lots of people like me who have got this query. Answers to this question will help them all... With scalability being one of your high priorities and I am also going to assume stability I have found that Erlang is a good fit. It is fault tolerant and highly scalable. If you have the time to investigate different options I would suggest at least looking into that. Event though I don't have any bit knowledge about Erlang I would definitely look in to this option as well. Thanks.  10k concurrent connections using .Net on reasonable hardware shouldn't be that much of a problem but as always it will depend on what you're actually doing in your server and how isolated each connection is from each other. After all saying 'handle 10k clients per node' is pretty meaningless when you don't define your hardware requirements for each node the amount of data that will be flowing in and out on each connection and how much CPU each connection is likely to need to do its work. I talk about these vague hand wavey scalability questions here: http://www.serverframework.com/asynchronousevents/2010/12/one-million-tcp-connections.html However: First you should be looking at the asynchronous sockets API and pretty much ignoring the threading issue. .Net's async sockets will take care of the threading for you and use I/O completion ports (a very scalable kernel object) to manage your threading for you. Also bear in mind that if you are using async sends then these can take a long while to complete if TCP flow control kicks in see here. and... Secondly you should test your scalability from day 0 and keep testing it all the way through your development - I write about why here: http://www.serverframework.com/asynchronousevents/2010/10/how-to-support-10000-or-more-concurrent-tcp-connections---part-2---perf-tests-from-day-0.html Thank you very much I will definitely consider your points...  Some people might say that 'scalable' and 'multi-threaded' are mutually contradictory. You need to explore the design space before making decisions like that. If you need scalability it may be possible to use just one thread but it depends on what you are serving up. As suggested in other answer I will use Async Socket connection so .Net will take care of threading stuffs..."
762,A,"Detect If IPv6 is Enabled on Windows Machines I am writing a powershell script that will act as a build compliance test for our servers. One of the things I need to do is detect if IPv6 networking has been disabled. WMI indicates that this information can be found in the IPAddress Property of Win32_NetworkAdapterConfiguration but can be both IPv6 or IPv4. This does not give me a ""yes/no"" answer I am hoping to find. Other caveats are that I would prefer not to scrape the details by accessing the registry directly nor scrape from the output of a command such as ipconfig. Given our environment has a mix of 2003/2008 machines can anyone think of a way to test for IPv6? Cheers If http://msdn.microsoft.com/en-us/library/aa822883(VS.85).aspx is anything to go by the only way to detect IPv6 using WMI is to look at the IPAddresses you find and even then you can't use it on Win2003. As far as I'm aware that leaves checking the netsh output (or some other utility) or digging through the registry. If you want to specifically check that IPv6 is enabled and working I'd say the best way would be to try and connect to a test host. That way you won't just test some specific software setting but you'll also find out so that all the routing configuration and everything around it works as it should.  Testing for IPv6 would usually be more complicated than a yes/no answer with XP you have Is the ""Microsoft TCP/IP version 6"" network component installed. Is there an adapter has an IPv6 address - one would usually assume (1) implies (2). Whether an adapter has a global-scope IPv6 i.e. not just a link local fe80:: prefixed address or loopback interface ::1. Presumaly (1) can be found with a Powershell script as demonstrated by Microsoft: http://gallery.technet.microsoft.com/ScriptCenter/en-us/c4eb1596-7ac9-4be7-a711-b43446a3e3df  This is what you didn't want to do but it seems the best method going over the output that netsh interface provides. You may want to look here for what you're after. Your implementation would be simpler but iterating over the netsh interface ipv6 show interfaces results is all you need if you find interfaces it's enabled somewhere if you don't find any it's not.  You can use the .NET way : Write-Host 'OS Supports IPv6: ' $( [System.Net.Sockets.Socket]::OSSupportsIPv6 ) The property will be true if it is possible to create an IPv6 datagram socket. This property is also affected by the global machine.config file so the IPv6 status may not always be accurate. Edit: To test this on a remote machine you can still use powershell but it has to have powershell 2.0 installed and WinRM enabled. If those two conditions are true then you can probably use Invoke-Command to do that. I think that should should true in most cases even if certian adapters have it disabled?  I've not tested this but the .NET Socket class has a OSSupportsIPv6 property. So something along the lines [System.Net.Sockets]::OSSupportsIPv6 The documentation says ""true if the operating system and network adaptors support the IPv6 protocol; otherwise false."" It isn't clear to me if that means the value is based on what the OS is capable of or what it is configured for. I looked this up in my local help so I don't have a MSDN link handy. Shouldn't be too hard to find.  Actually a colleague came up with a nice way of doing it - a little clunky but works. :) $IPV6 = $false $arrInterfaces = (Get-WmiObject -class Win32_NetworkAdapterConfiguration -filter ""ipenabled = TRUE"").IPAddress foreach ($i in $arrInterfaces) {$IPV6 = $IPV6 -or $i.contains("":"")} write-host $IPV6"
763,A,"Buffer management for socket application best practice Having a Windows IOCP app............ I understand that for async i/o operation (on network) the buffer must remain valid for the duration of the send/read operation. So for each connection I have one buffer for the reading. For sending I use buffers to which I copy the data to be sent. When the sending operation completes I release the buffer so it can be reused. So far it's nice and not of a big issue. What remains unclear is how do you guys do this? Another thing is that even when having things this way I mean multi-buffers the receiver side might be flooded (talking from experience) with data. Even setting SO_RCVBUF to 25MB didn't help in my testings. So what should I do? Have a to-be-sent queue? ""orthogonal""? I'm limited with my English heh.. Well I thought that 'SO_RCVBUF' would allow more data to be received than my app could take i.e if I schedule a read of 100bytes to a buffer but the sender sent me 1MB then the socket's buffer (which I set the size to with 'SO_RCVBUF') would get filled. Orthogonal = perpendicular :) TCP provides *flow control* - (http://en.wikipedia.org/wiki/Transmission_Control_Protocol#Flow_control) that slows down fast sender if the receiver cannot read data fast enough to drain the receive buffer. In what way `SO_RCVBUF` did not help? With UDP this is a headache but with TCP - what's the issue? Also send queue is orthogonal to receiver side being flooded. I reference count the per connection (socket) and per operation (buffer) structures. This works very well and deals with the lifetime issues perfectly. Each time an overlapped operation is posted the reference count of the per connection is incremented and a new buffer is allocated from the pool. When the operation completes I process the results and release the reference on the socket and the buffer. If this is the last reference then the structure is cleaned up (buffers go back to the pool etc). You can see all of this in action in my free IOCP client/server framework which is available for download from here. +1 for using memory pool."
764,A,"Java network code not sending I'm writing my first non-trivial Java app that uses: networking a GUI threads It's a IM program. When I send a message the server doesn't output what it should. I'm sorry this description is so bad but I don't know how to narrow the problem down further. public class MachatServer { //snip public static void sendMessage(int targetId int fromId String message) { ConnectedClient targetClient = getClient(targetId); // Also runs System.out.println(""Sending message: "" + message + ""\n\nfrom "" + fromId + "" to "" + targetId); targetClient.addOutCommand(""/message:"" + fromId + "":"" + message + ""\n""); } } class ConnectedClient implements Runnable { public void run() { String contact; contact = s.getInetAddress().toString(); System.out.println(""Connected to "" + contact); try { out.write(""/connected"" + ""\n""); out.flush(); String command; while(true) { if(shouldExit) { s.close(); break; } if(in.hasNextLine()) { command = in.nextLine(); commandProcessor.addInCommand(command); } Thread.sleep(100); } } catch(Exception e) { e.printStackTrace(); } } // snip public void addOutCommand(String command) { commandProcessor.addOutCommand(command); // // My guess is that the problem is with this method as the next line // Does not print out. // // System.out.println("""" + thisId + "" recieved to send: "" + command); } } class CommandProcessor implements Runnable { // snip public void run() { String currentCommandIn; String currentCommandOut; while(true) { try { currentCommandIn = inQueue.poll(); if(currentCommandIn != null) { System.out.println(""Processing: "" + currentCommandIn); String[] commandArr = CommandParser.parseRecievedCommand(currentCommandIn); if(commandArr[0].equalsIgnoreCase(""message"")) { int target = Integer.parseInt(commandArr[1]); String message = commandArr[2]; // This definetly runs System.out.println(""Message sending to: "" + target); MachatServer.sendMessage(target this.conId message); } else if(commandArr[0].equalsIgnoreCase(""quit"")) { // Tell the server to disconnect us. MachatServer.disconnect(conId); break; } currentCommandOut = outQueue.poll(); if(currentCommandOut != null) { try { out.write(currentCommandOut + ""\n""); System.out.println(currentCommandOut + ""sent""); out.flush(); } catch (IOException e) { e.printStackTrace(); } } } catch(Exception e) { e.printStackTrace(); } public synchronized void addOutCommand(String command) { if(command != null) { try { outQueue.push(command); } catch(Exception e) { System.out.println(command); e.printStackTrace(); } // Does not print System.out.println(""Ready to send: "" + command); } else { System.out.println(""Null command recieved""); } //snip } The full source code is at my github in case I have narrowed the problem down incorrectly. The expected output should be when I telnet in and send ""/message:0:test"" it should send ""/message:myid:test"" to the client with ID 0. The actual output is nothing. Can you post the expected and actual output? This is probably not a complete answer but there are a few serious issues with your code that could be the cause of your problem so you should fix those first. First the loop in CommandProcessor.run is busy-waiting i.e. it runs constantly. You should use blocking operations. Also inQueue and outQueue are accessed from two different threads so you need synchronization on every access. I recommend using something implementing the java.util.concurrent.BlockingQueue interface to solve both issues. And finally when checking your full code it appears that you also need to synchronize access to the ConnedtedClient.shouldExit field (I believe you can use `java.util.concurrent.atomic.AtomicBoolean as a replacement but I'm not sure). And the reason why this could be the cause of your problem: Since CommandProcessor.run is not synchronizing on anything (or accessing anything volatile) the Java virtual machine can assume that nothing from outside can modify anything it examines so in theory when the run method first notices that inQueue and outQueue are both empty it can optimize the whole method into nothing as it can assume that it is the only thing that can modify them. But I don't know whether this can actually happen in practice as the JVM needs to know quite a bit about the LinkedList implementation and notice that the thread is just doing these two checks in a loop. But it's always best to be safe because that way your code is guaranteed to work. I know the code is pretty crap so I'll fix these points and post the results later. Yeah my laptop is still warm from running the server 10 minutes ago... Turns it is the error pointed out by Grandpa that was causing it to not work. I still need to fix all the problems you pointed out however.  The field outQueue is uninitialized in CommandProcessor and you commented out the printStackTrace() that would have helped you figure it out. Thanks. This fixed it.  Maybe the problem is that the data you send is to short.... A friend of mine had a similar problem a couple of years ago and it turned out the data was being buffered until it had enough data to send... It had something to do with optimizing the amount of network traffic... I believe he mentioned something called ""Nagle's algorithm"" when he finally solved it.... Hope this can be of some help... I had this problem earlier. The call to out.flush() should empty the buffer. Nagle's algorithm delays small packets by a few milliseconds; it doesn't stop them going out. TCP/IP Illustrated Vol. 1 chapter 19 covers this in some detail."
765,A,"Design question on Python network programming I'm currently writing a project in Python which has a client and a server part. I have troubles with the network communication so I need to explain some things... The client mainly does operations the server tells him to and sends the results of the operations back to the server. I need a way to communicate bidirectional on a TCP socket. Current Situation I currently use a LineReceiver of the Twisted framework on the server side and a plain Python socket (and ssl) on client side (because I was unable to correctly implement a Twisted PushProducer). There is a Queue on the client side which gets filled with data which should be sent to the server; a subprocess continuously pulls data from the queue and sends it to the server (see code below). This scenario works well if only the client pushes its results to the manager. There is no possibility the server can send data to the client. More accurate there is no way for the client to receive data the server has sent. The Problem I need a way to send commands from the server to the client. I thought about listening for incoming data in the client loop I use to send data from the queue: def run(self): while True: data = self.queue.get() logger.debug(""Sending: %s"" repr(data)) data = cPickle.dumps(data) self.socket.write(data + ""\r\n"") # Here would be a good place to listen on the socket But there are several problems with this solution: the SSLSocket.read() method is a blocking one if there is no data in the queue the client will never receive any data Yes I could use Queue.get_nowait() instead of Queue.get() but all in all it's not a good solution I think. The Question Is there a good way to achieve this requirements with Twisted? I really do not have that much skills on Twisted to find my way round in there. I don't even know if using the LineReceiver is a good idea for this kind of problem because it cannot send any data if it does not receive data from the client. There is only a lineReceived event. Is Twisted (or more general any event driven framework) able to solve this problem? I don't even have real event on the communication side. If the server decides to send data it should be able to send it; there should not be a need to wait for any event on the communication side as possible. Good questions but you accepted a non-answer. :-( I don’t care if you call it server/client/whatever. – P2P is not a twisted thing? ""I don't even know if using the LineReceiver is a good idea for this kind of problem because it cannot send any data if it does not receive data from the client. There is only a lineReceived event."" You can send data using protocol.transport.write from anywhere not just in lineReceived. Yes this is the problem I encountered with the `LineReceiver`. And ... I'm proposing a solution - call `protocol.transport.write` from somewhere other than `lineReceived`.  ""I need a way to send commands from the server to the client."" Don't do this. It inverts the usual meaning of ""client"" and ""server"". Clients take the active role and send stuff or request stuff from the server. Is Twisted (or more general any event driven framework) able to solve this problem? It shouldn't. You're inverting the role of client and server. If the server decides to send data it should be able to send it; False actually. The server is constrained to wait for clients to request data. That's generally the accepted meaning of ""client"" and ""server"". ""One to send commands to the client and one to transmit the results to the server. Does this solution sound more like a standard client-server communication for you?"" No. If a client sent messages to a server and received responses from the server it would meet more usual definitions. Sometimes this sort of thing is described as having ""Agents"" which are -- each -- a kind of server and a ""Controller"" which is a single client of all these servers. The controller dispatches work to the agents. The agents are servers -- they listen on a port accept work from the controller and do work. Each Agent must do two concurrent things (usually via the select API): Monitor a well-known socket on which it will receive work from the one-and-only client. Do the work (in the background). This is what Client-Server usually means. If each Agent is a Server you'll find lots of libraries will support this. This is the way everyone does it. If you don't like the words ""client"" and ""server"" then how about ""master"" and ""slave""? Or ""manager"" and ""worker""? There's nothing wrong with a design that many processes connecting to one and then receiving commands from it. Your ""accept meanings"" are needlessly restricted. S.Lott you might want to read up on the design and terminology of the X protocol. The words ""client"" and ""server"" do not always mean what you seem to think they mean. Having a clear semantics which side is supposed to do what actually helps in keeping the software implementation sane and problem free. Having a server push data to the client (vs. the client polling from the server) will bring you problems down the road. If you don't like the way I use servers and clients please tell me another way to solve my problem: A manager tells its workers to do a job and they continuously send it the results of the work until the manager tells them to stop or to do something else. Request from the manager a chunk of work do it and report back when you're done. The difference seems subtle but your approach will have scaling issues and hard to troubleshoot communication issues (been there done that) I actually this a bit back: it depends. Pull architectures are easier to build up I think. _The difference seems subtle..._ The difference to what? What is the difference between our approaches? I currently do exactly what you are describing except there is no _done_. The workers get jobs which are not done until the manager tells them to stop. @forest: The X client applications make requests of the X server. The X server (running on your desktop) has events queued which the X client application must accept. I think the meanings are the same. The X server can't communicate randomly to a client. My understanding is that the client -- effectively -- polls the X server for mouse and keyboard events. +1 for concise problem analysis Your answer is not to do that this way because no one does it this way? I have a problem and need a solution i don't think questioning the problem is straightforward here. I've thought about connecting two TCP sockets like FTP does. One to send commands to the client and one to transmit the results to the server. Does this solution sound more like a _standard_ client-server communication for you? How about P2P? Or let me rephrase the questions: Is Twisted good for P2P? Is any other event driven framework ideal for P2P?"
766,A,net/if_dl.h header file not found in Ubuntu 10.10 I'm learning network programming using the Unix Network Programming (3rd ed.). I got the source code from the web page and tried to compile the code. When I did a make in the libroute directory of the source code I got the following error: $ make gcc -I../lib -g -O2 -D_REENTRANT -Wall -c -o get_rtaddrs.o get_rtaddrs.c In file included from get_rtaddrs.c:1: unproute.h:3: fatal error: **net/if_dl.h: No such file or directory** compilation terminated. make: *** [get_rtaddrs.o] Error 1 As you can see I do not have the required header file. I tried to search for the header file using $ apt-file search if_dl.h libnewlib-dev: /usr/lib/newlib/i686-linux-gnu/include/net/if_dl.h I only get libnewlib-dev package which is the header file package for the embedded system devel. Where can I get the required header file(s) either as a debian package or source code. Please let me know. Thank you. See this - What package do i need to install for using routing sockets?
767,A,"Checking network status in C# How do I check that I have an open network connection and can contact a specific ip address in c#? I have seen example in VB.Net but they all use the 'My' structure. Thank you. Well you would try to connect to the specific ip and handle denies and timeouts. Look at the TcpClient class in the System.Net.Sockets namespace.  If you just want to check if the network is up then use: bool networkUp = System.Net.NetworkInformation.NetworkInterface.GetIsNetworkAvailable(); To check a specific interface's status (or other info) use: NetworkInterface[] networkCards = System.Net.NetworkInformation.NetworkInterface.GetAllNetworkInterfaces(); To check the status of a remote computer then you'll have to connect to that computer (see other answers)  If you want to monitor for changes in the status use the System.Net.NetworkInformation.NetworkChange.NetworkAvailabilityChanged event: NetworkChange.NetworkAvailabilityChanged += new NetworkAvailabilityChangedEventHandler(NetworkChange_NetworkAvailabilityChanged); _isNetworkOnline = NetworkInterface.GetIsNetworkAvailable(); // ... void NetworkChange_NetworkAvailabilityChanged(object sender NetworkAvailabilityEventArgs e) { _isNetworkOnline = e.IsAvailable; }  First suggestion (IP Connexion) You can try to connect to the IP using something like: IPEndPoint ipep = new IPEndPoint(Ipaddress.Parse(""IP TO CHECK"") YOUR_PORT_INTEGER); Socket server = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); server.Connect(ipep); I suggest you to check code for ""Chat"" program. These program manipulate a lot of IP connection and will give you a good idea of how to check if an IP is available. Second suggestion (Ping) You can try to ping. Here is a good tutorial. You will only need to do : Ping netMon = new Ping(); PingResponse response = netMon.PingHost(hostname 4); if (response != null) { ProcessResponse(response); } Thanks for the suggestions. The ping class is proving very useful. Ping packets (ICMP echo requests) are commonly blocked by firewalls therefore using this approach to test availability of a server on the other side of a firewall will fail in such a scenario.  If you're interested in the HTTP status code the following works fine: using System; using System.Net; class Program { static void Main () { HttpWebRequest req = WebRequest.Create( ""http://www.oberon.ch/"") as HttpWebRequest; HttpWebResponse rsp; try { rsp = req.GetResponse() as HttpWebResponse; } catch (WebException e) { if (e.Response is HttpWebResponse) { rsp = e.Response as HttpWebResponse; } else { rsp = null; } } if (rsp != null) { Console.WriteLine(rsp.StatusCode); } } } Regards tamberg"
768,A,"POSIX cancellation points Can anyone point me towards a definitive list of POSIX cancellation points? I was just about to answer a question on stackoverflow and realised I didn't know my stuff well enough! In particular are accept() and select() cancellation points? I have an old book that says no but I've seen sites on the internet claim that they are. Confused! I am also after a list of Linux cancellation points if that is different. Additional Info: since kernel 2.6 Linux has used the NPTL thread library which is POSIX compliant so cancellation points should be as above for recent Linux implmentations. http://www.ddj.com/linux-open-source/184406204  See the pthread_cancel man page for further and fast info.  The POSIX 1003.1-2003 standard gives a list in the System Interfaces section then General Information then Threads (direct link courtesy of A. Rex). (Added: POSIX 1003.1-2008 is now available on the web (all 3872 pages of it in PDF and HTML). You have to register (free). I got to it from the Open Group Bookstore.) Cancellation Points Cancellation points shall occur when a thread is executing the following functions: accept() aio_suspend() clock_nanosleep() close() connect() creat() fcntl() (When the cmd argument is F_SETLKW) fdatasync() fsync() getmsg() getpmsg() lockf() mq_receive() mq_send() mq_timedreceive() mq_timedsend() msgrcv() msgsnd() msync() nanosleep() open() pause() poll() pread() pselect() pthread_cond_timedwait() pthread_cond_wait() pthread_join() pthread_testcancel() putmsg() putpmsg() pwrite() read() readv() recv() recvfrom() recvmsg() select() sem_timedwait() sem_wait() send() sendmsg() sendto() sigpause() sigsuspend() sigtimedwait() sigwait() sigwaitinfo() sleep() system() tcdrain() usleep() wait() waidid() waitpid() write() writev() A cancellation point may also occur when a thread is executing the following functions: access() asctime() asctime_r() catclose() catgets() catopen() closedir() closelog() ctermid() ctime() ctime_r() dbm_close() dbm_delete() dbm_fetch() dbm_nextkey() dbm_open() dbm_store() dlclose() dlopen() endgrent() endhostent() endnetent() endprotoent() endpwent() endservent() endutxent() fclose() fcntl() (For any value of the cmd argument. [Presumably except F_SETLKW which is listed.] fflush() fgetc() fgetpos() fgets() fgetwc() fgetws() fmtmsg() fopen() fpathconf() fprintf() fputc() fputs() fputwc() fputws() fread() freopen() fscanf() fseek() fseeko() fsetpos() fstat() ftell() ftello() ftw() fwprintf() fwrite() fwscanf() getaddrinfo() getc() getc_unlocked() getchar() getchar_unlocked() getcwd() getdate() getgrent() getgrgid() getgrgid_r() getgrnam() getgrnam_r() gethostbyaddr() gethostbyname() gethostent() gethostid() gethostname() getlogin() getlogin_r() getnameinfo() getnetbyaddr() getnetbyname() getnetent() getopt() (if opterr is non-zero.) getprotobyname() getprotobynumber() getprotoent() getpwent() getpwnam() getpwnam_r() getpwuid() getpwuid_r() gets() getservbyname() getservbyport() getservent() getutxent() getutxid() getutxline() getwc() getwchar() getwd() glob() iconv_close() iconv_open() ioctl() link() localtime() localtime_r() lseek() lstat() mkstemp() mktime() nftw() opendir() openlog() pathconf() pclose() perror() popen() posix_fadvise() posix_fallocate() posix_madvise() posix_openpt() posix_spawn() posix_spawnp() posix_trace_clear() posix_trace_close() posix_trace_create() posix_trace_create_withlog() posix_trace_eventtypelist_getne posix_trace_eventtypelist_rewin posix_trace_flush() posix_trace_get_attr() posix_trace_get_filter() posix_trace_get_status() posix_trace_getnext_event() posix_trace_open() posix_trace_rewind() posix_trace_set_filter() posix_trace_shutdown() posix_trace_timedgetnext_event( posix_typed_mem_open() printf() pthread_rwlock_rdlock() pthread_rwlock_timedrdlock() pthread_rwlock_timedwrlock() pthread_rwlock_wrlock() putc() putc_unlocked() putchar() putchar_unlocked() puts() pututxline() putwc() putwchar() readdir() readdir_r() remove() rename() rewind() rewinddir() scanf() seekdir() semop() setgrent() sethostent() setnetent() setprotoent() setpwent() setservent() setutxent() stat() strerror() strerror_r() strftime() symlink() sync() syslog() tmpfile() tmpnam() ttyname() ttyname_r() tzset() ungetc() ungetwc() unlink() vfprintf() vfwprintf() vprintf() vwprintf() wcsftime() wordexp() wprintf() wscanf() An implementation shall not introduce cancellation points into any other functions specified in this volume of IEEE Std 1003.1-2001. The side effects of acting upon a cancellation request while suspended during a call of a function are the same as the side effects that may be seen in a single-threaded program when a call to a function is interrupted by a signal and the given function returns [EINTR]. Any such side effects occur before any cancellation cleanup handlers are called. Whenever a thread has cancelability enabled and a cancellation request has been made with that thread as the target and the thread then calls any function that is a cancellation point (such as pthread_testcancel() or read()) the cancellation request shall be acted upon before the function returns. If a thread has cancelability enabled and a cancellation request is made with the thread as a target while the thread is suspended at a cancellation point the thread shall be awakened and the cancellation request shall be acted upon. However if the thread is suspended at a cancellation point and the event for which it is waiting occurs before the cancellation request is acted upon it is unspecified whether the cancellation request is acted upon or whether the cancellation request remains pending and the thread resumes normal execution. Ugh! Can't get the table to work very well it looked OK in preview and nothing like a table afterwards. Look at the URL for the information! There are a lot of possible cancellation points. exactly what I was looking for. thanks! Here's a deep link to the right section: http://www.opengroup.org/onlinepubs/000095399/functions/xsh_chap02_09.html#tag_02_09_05_02 Ah thanks. Now how do you go about finding that? (I looked at page source - is that the best way? Only way?) Yeah that's what I did. (Well I first got out of their annoying frameset by right-clicking ""Open frame in new tab."" That way I get the HTML filename quickly.) You know I deep-link often enough that it might be useful to have a browser extension that shows me all the possible link points ... Alternatively we could have gone to the topical index http://www.opengroup.org/onlinepubs/000095399/idx/topic.html and it's a click away!"
769,A,"Flow based routing and openflow This may not be the typical stackoverflow question. A colleague of mine has been speculating that flow-based routing is going to be the next big thing in networking. Openflow provides the technology to use low cost switches in large application IT data-centers etc; replacing Cisco HP etc switch and routers. The theory is that you can create a hierarchy these openflow switches with simple configuration eg. no spanning tree. Open flow will route each flow to the appropriate switch/switch-port using only the knowledge of the hierarchy of switches (no routers). The solution is suppose to save enterprises money and simplify networking. Q. He is speculating that this may dramatically change enterprise networking. For many reasons I am skeptical. I would like to hear your thoughts. @Nathan: OpenFlow 1.1 actually adds some primitives that enable the use of multiple links via the Multipath Proposal. Yes it is possible to extend the OpenFlow spec indefinitely until all conceivable networking functions are included. And just because there is a later spec it does not mean that switch vendors will implement it. Switch vendors may view OpenFlow as a sort of trojan horse. Once they agree to implement a seemingly benign version 0.8.9 they will feel compelled to later implement version 1.0 1.1 2.0 3.0 etc. until what was once a narrow interface has now become a wide highway exposing every nook and cranny of their switch and eroding their competitive advantage.  Nathan Excellent historical account and overview of openflow. Thanks! You've hit on the points that I've been wrapping my head around as to why Openflow might not be widely adopted. Since it was designed to be open to allow researcher the ability to run experimental protocols and not necessarily be ""compatible with"" the big players Cisco/HP/etc. it puts itself into niche (although potentially big) market more on this later. And as you've stated it's recieved some adoption in the ""cloud data centers (CDC)"" e.g. google facebook etc because they need to exploit experimental protocols to gain a competitive advantage or optimize for their application. As you've stated some switch vendors have added openflow capability to capitalize on the niche need in academia and potentially sell into the CDC; google facebook. This is potentially a big market (or bubble if you're pessimistic). The problem that I see is that the majority of the market (80% or more) is enterprise IT data centers. The requirements here is for stable compatible networking. Open and less expensive would be nice but not at the cost of the former. One could think of a day where corporate IT is partially or completely cloud-sourced where QoS is maintained by the cloud provider. In this case experimental protocols could be leveraged to provide a competitive advantaged for speed or QoS. In which case; openflow could play a more dominant roll. I personally think this scenario is many years off. So the conclusion I come to is that other than in research and perhaps CDCs (google facebook) the market is pretty small. I suppose that if researchers use openflow to come up with a better protocol for say link aggregation or congestion management then eventually Cisco and HP will provide those in their standard offering because their customers will demand it. So openflow could be a market influencer (via the research community) but it would not be a market disruptor. Do you agree with my conclusions? Thanks for your input. @Nathan: Unless we invent a time machine it's impossible to predict the future :) Unfortunately for profit commercial software vendors have to build models of what the future may look like given various conditions in order to make rational decisions on where to invest their resources. Even if they have technology that could significantly enhance the adoption of a technology (e.g. openflow) there needs to be a path independent of their technology in which the market is a possible win. I would be interested in hearing opinions on how openflow could be widely adopted. I'm hesitant to predict the future. It is clear that OpenFlow has strong traction in the network research community based on the number of publications that have mentioned their use of OpenFlow for running experiments. I have heard anecdotes from a couple commercial switch vendors that OpenFlow has increased sales by dozens of switches per quarter so yes the market is currently very small. But if hundreds of smart researchers are exposed to OpenFlow then perhaps they will find more applications like Ethane that will create new markets.  OpenFlow is a research project from Stanford University led by professor Nick McKeown. In the original OpenFlow research paper the goal of OpenFlow was to give researchers a way ""to run experimental protocols in the networks they use every day."" For years networking researchers have had an almost impossible task deploying and evaluating their ideas on real networks with real Ethernet switches and IP routers. The difficultly is that real switches and routers from companies like Cisco HP and others are all closed proprietary boxes that implement standard ""protocols"" like Ethernet spanning tree and OSPF. There are business reasons why Cisco and HP won't let you run software on their switches and routers; there is no technical reason. OpenFlow was invented to solve a people problem: if Cisco is not willing to let you run code on their switch maybe they can at least provide a very narrow interface to let you remotely configure their switch and that narrow interface is called OpenFlow. To my knowledge more than a dozen companies are currently implementing OpenFlow support for their switches. Some like HP are only providing the OpenFlow software for research purposes. Others like NEC are actually offering commercial support. For academic researchers that want to evaluate new routing protocols in real networks OpenFlow is a huge win. For switch vendors it is less clear if OpenFlow support will help hurt or have no effect in the long run. After all the academic research market is very small. The reason why OpenFlow is most often discussed in the context of enterprise networks is that OpenFlow grew out of a previous research project called Ethane that used OpenFlow's mechanism of remotely programming switches in an enterprise network in order to centralize a security policy. Ethane and by extension OpenFlow has led directly to two startup companies: Nicira founded by Martin Casado and Big Switch Networks founded by Guido Appenzeller. It would be easier to implement an Ethane-like system if all of the switches in the network supported OpenFlow. Closely related to enterprise networks are data center networks the networks that interconnect thousands to tens of thousands of servers in companies such as Google Facebook Microsoft Amazon.com and Yahoo!. One problem with Ethernet is that it does not scale to this many servers on the same Layer 2 network. We attempted to solve this problem in a research project called PortLand. We used OpenFlow to facilitate programming the switches from a central controller which we called a Fabric Manager. We released the PortLand source code as open source. However we also found a limitation to OpenFlow's functionality. In another data center networking research project called Helios we were not able to use OpenFlow because it did not provide a mechanism for bonding multiple switch ports into a Link Aggregation Group (LAG). Presumably one could extend the OpenFlow specification indefinitely until it all possible switch features become exposed. There are other networks as well such as the Internet access networks Internet backbones home networks wireless networks cellular networks etc. Researchers are trying to see where OpenFlow fits into all of these markets. What it really comes down to is the question ""what problem does OpenFlow solve?"" Ethane makes a case for enterprise networks but I have not yet seen a compelling case for any other type of network. OpenFlow might be the next big thing or it might end up being a case of ""don't solve a people problem with a technical solution."" comment in new answer since I'm limited in char's here  In order to assess the future of flow-based networking and OpenFlow here’s the way to think about it. It starts with the silicon trends: Moore’s Law (2X transistors per 18-24 months) and a correlated but slower improvement in the I/O bandwidth available on a single chip (roughly 2X every 30-36 months). You can now buy full-featured 10GbE single chip switches with 64 ports and chips which have a mix of 40GbE and 10GbE ports with comparable total I/O bandwidth. There are a variety of ways physically connect these in a mesh (ignoring the loop-free constraints of spanning tree and the way Ethernet learns MAC addresses). In the high performance computing (HPC) world a lot of work has been done building clusters with InfiniBand and other protocols using meshes of small switches to network the compute servers. This is now being applied to Ethernet meshes. The geometry of a CLOS or fat-tree topology enables a two stage mesh with a large number of ports. The math is thus: Where n is the # of ports per chip the number of devices you can connect in a two-stage mesh is (n*2)/2 and the number you can connect in a three-stage mesh is (n*3)/4. While with standard spanning tree and learning the spanning tree protocol will disable the multi-path links to the second stage most of the Ethernet switch vendors have some sort of multi-chassis Link Aggregation protocol which gets around the multi-pathing limitation. There is also standards work in this area. Although it might not be obvious the vast majority of Link Aggregation schemes allocate traffic so all the frames of any given flow take the same path. This is done in order to minimize out-of-order frames so they don’t get dropped by some higher level protocol. They could have chosen to call this “flow based multiplexing” but instead they call it “link aggregation”. Although the devil is in the details there are a variety of data center operators and vendors that have concluded they don’t need to have large multi-slot chassis switches in the aggregation/core layer for server connect instead using meshes of inexpensive 1U or 2U switches. People have also concluded that eventually you need some kind of management station to set up the configuration of all the switches. Again drawing from the experience with HPC and InfiniBand they use what is called an InfiniBand Controller. In the telecom world most telecom networks have evolved to separate the management and part of the control plane from the boxes that carry the data traffic. Summarizing the points above meshes of Ethernet switches with an external management plane with multipath traffic where flows are kept in order is evolutionary not revolutionary and is likely to become mainstream. At least one major company Juniper has made a big public statement about their endorsement of this approach. I'd call all of these ""flow-based routing"". Juniper and other vendors’ proprietary approaches notwithstanding this is an area that cries out for standards. The Open Networking Foundation (ONF) was founded to promote standards in this area starting with OpenFlow. Within a couple of months the sixty+ members of ONF will be celebrating their first year anniversary. Each member has I am led to believe paid tens of thousands of dollars to join. While the OpenFlow protocol has a ways to go before it is widely adopted it has real momentum.  More context on SDN which discusses IETF's SDN initiative and ONF's Openflow. Working in conjuction is a powerful combination http://bit.ly/A8xYso  An excellent view of OpenFlow by Simon Crosby http://community.citrix.com/display/ocb/2011/03/21/The+Rise+of+the+Software+Defined+Network"
770,A,"How does the packets go out even behind Firewall or NAT with some application? Such as Skype/Team viewer/Logmein etc application which send audio/video behind NAT (behind firewall). But when i make a small tiny application which send text to another NAT location it failed to do the same. Example: Sender: -> Public ip: 91.1.2.3 My lan ip is: 192.168.1.2 with port 14446 udp -------> Data format: RTP packets Receiver: <------- Data received: 0 packets -> Public ip: 92.1.2.3 Friend lan ip is: 10.0.0.2 with port 14446 udp * same in both way How others does this? What is the way of doing peer 2 peer application development to overcome NAT issues? Always we have public ip's and mostly it has NAT issues. But how does then Skype works in such cases too? Do we have a audio/video port range for UDP or always UDP is open from anything? But mine does not work above range ports for UDP i also tried. What is the secret? that is making me curious!!. Note: My goal is audio packets handling where i believe too much filtering or firewall cause latency and delay and other issues gets involved relatively too. So i would like to know very clearly for my application that some of the ports (which port ranges?) can be used for such purposes where it really not blocking development stress. One common solution is that the client program connects outward to the server and thus establishes a connection. Most firewalls allow outward connections - the assumption being that you are trusted and can always connect to the outside. When the server then wishes to send a message to you it responds on the open connection. A (behind firewall) B (friend behind firewall) C (Server stun/nat mapper). A connects C and C connects A and A register to C? or A connects B but cant communicate because of behind NAT. or A and B in telephone talks about there negotiation information and try to establish behind NAT mapping? What exactly you mean is STUN which is only a public ip and public port maps but still its A to B. What I meant was: A connects to C and thus determines its external IP/Port. That's all STUN does. The C server owner can now declare that A is online and reachable on this external IP/Port. B can therefore connect directly to A. C would not be involved in the direct communication.  There are a number of types of NATs which vary in what traffic they'll allow in. See the Wikipedia article on NATs For most NATs STUN will let you open ports AND find out what port you opened (may be different than the port you sent from). In SIP and RTSP you'd typically provide the external IP and port determined by STUN to the other end. A fully-symmetric NAT means that STUN won't let you use a 3rd-party server to prop ports via STUN so you'll have to use UPnP (if enabled) or map ports in the router (or set up triggers) or you'll have to play evil games to make both sides think they initiated the connection. (Not easy and not guaranteed.) See the ICE & TURN specs (RFCs) from the IETF for detailed mechanisms to traverse NATs - though note that in some cases you must use an external proxy to forward packets. Great info. But still we are dealing with latency. The idea that i am trying to implement is A) Shake hands (for no firewall/NAT issues) B) by pass raw packets from user1 to user2 without involving a switch/proxy server to deal the Media packets. Because in my research and experiments it has been proved that peer 2 peer media should not involved third party filtering. And the main goal is lowest latency in media. Read the articles I referenced. You can't easily handle the case of a fully-symmetric router without control of the router; certainly not fully-symmtric routers at both ends without some type of RTP proxy/forwarder (see ICE/TURN). You can deal with symmetric routers via UPnP (if enabled) or manual port-mapping. I'm not 100% sure what you mean by ""Shake hands"". Low latency communication will be better if you can avoid a proxy - ICE lets you determine if there's a way to avoid relay. Are those IPv6 compatible? Or they will be removed and suspended technologies when IPv4 does not exist.  I believe the port that you use is what is usually used to determine if it should be allowed or not. Certain ports are always let through. I'm not sure of the exact ports but that will be different for all NATs and firewalls."
771,A,"How can I simulate TCP/IP errors? On a multi-tier application I need to simulate various TCP/IP errors to test some reconnection code. Does anyone know of any tools (Windows based) I can use for this purpose? Thanks. Got some great answers here. I'll post back as soon as I can try these out. Thanks everyone. Try netwox (formerly lcrzoex.) If it won't do it it can't be done. It contains >200 tools. This let me find what I needed the fastest. Thanks.  On FreeBSD the best tool by far is dummynet ""a tool originally designed for testing networking protocols and since then used for a variety of applications including bandwidth management. It simulates/enforces queue and bandwidth limitations delays packet losses and multipath effects."" On Linux you will have to use netem. (It seems there is now a port of dummynet but I never tried it.) More details (in French) in my article. Unfortunately neither of these would work as we're a Windows shop and I don't have time to try to port them. Thanks Though.  Scapy allows you to control every aspect of the packets and randomly modify (""fuzz"") the ones you don't want to control. If you're a command-line kind of guy it's a great tool.  No tools that I'm aware of but most of TCP errors can be emulated by a custom LSP filter. This article can get you started writing one"
772,A,how to export data to computer(PC) using wifi or http connection? I have made Iphone applicatio. In my application whatever data i have recorded; all that data i want to export to the Computer using the application. Is there any way available to sent data to computer using wifi using developed application? Can we use Bluetooth or Http connection to send application from device to the PC? if we can than how can we manage the bluetooth and how to use it without Jailbreaking? if any body has any solutionplease give any code or any link or any other solution which would appreciated. Thanks Mishal Please search this site before posting another duplicate. There have been many similar questions like this in the past few days. Including the three that were yours which were responded to. OK; intervention... I've merged 3 of these duplicates and deleted another. **DO NOT KEEP ASKING THE SAME QUESTION**. Either improve this question (edit) with anything that is missing or ask a *different* question. Your best bet is to send the data using HTTP or FTP to a server which could be your PC. Would that work for you? Here's one way to do it. Here's another. And one for FTP.  I dont think you have an option to gain control of a computer and toss a file in there from within an iPhone app. You cannot do it over the usb cord you cannot mount the drive unless you roll your own fs mounter (pretty difficult) and you cannot push a file over html or something and have it magically appear. The user would have to interact at some point. Many times this is done over html. In my apps I use CocoaHTTPServer to get local info into and off of the phone. You run the server and out-of-the-box it indexes all the files in the documents directory for you to download from any browser on the same wifi network. Give it a shot as it is a easy to implement solution for getting large files off the phone without having to resort to something clunky like email you *know* that this question has been asked the same way 3 times by the same user; please don't give the same reply each time. Duplicates should be closed.
773,A,"Upolading a file from server to client with servlet I'm trying to write a servlet that can upload a file from client to server and download a file from server to client from specific location to specific location. But two problems stopped me: 1. When uploading a file from client to server how to tell to the server where to store the file? 2. (and more important) How to do the downloading from server to client part? Here is the code so far: import java.io.FileOutputStream; import java.io.ObjectInputStream; import java.io.ObjectOutputStream; import java.net.ServerSocket; import java.net.Socket; public class Server extends Thread { public static final int PORT = 3333; public static final int BUFFER_SIZE = 100; @Override public void run() { try { ServerSocket serverSocket = new ServerSocket(PORT); while (true) { Socket s = serverSocket.accept(); saveFile(s); } } catch (Exception e) { e.printStackTrace(); } } private void saveFile(Socket socket) throws Exception { ObjectOutputStream oos = new ObjectOutputStream(socket.getOutputStream()); ObjectInputStream ois = new ObjectInputStream(socket.getInputStream()); FileOutputStream fos = null; byte [] buffer = new byte[BUFFER_SIZE]; Object o = ois.readObject(); if (o instanceof String) { fos = new FileOutputStream(o.toString()); } else { throwException(null); } Integer bytesRead = 0; do { o = ois.readObject(); if (!(o instanceof Integer)) { throwException(null); } bytesRead = (Integer)o; o = ois.readObject(); if (!(o instanceof byte[])) { throwException(null); } buffer = (byte[]) o; fos.write(buffer 0 bytesRead); } while (bytesRead == BUFFER_SIZE); fos.close(); ois.close(); oos.close(); } public static void throwException(String message) throws Exception { throw new Exception(message); } public static void main(String[] args) { new Server().start(); } } package com.filetransfer.web; import java.io.*; import java.net.Socket; import java.util.Arrays; import javax.servlet.*; import javax.servlet.http.*; public class FileTransfer extends HttpServlet { private static final long serialVersionUID = 1L; public static final int PORT = 3333; public static final int BUFFER_SIZE = 100; public static final String HOST = ""localhost""; public void doGet(HttpServletRequest request HttpServletResponse response) throws ServletException IOException { String action = request.getParameter(""option""); System.out.println(action); if (""upload"".equals(action)) { uploadFile(request); } else if (""download"".equals(action)) { downloadFile(request response); } } public void doPost(HttpServletRequest request HttpServletResponse response) throws ServletException IOException { doGet(request response); } public void reportError(HttpServletResponse response String message) throws IOException { response.sendError(HttpServletResponse.SC_NOT_FOUND message); } public void uploadFile(HttpServletRequest request) { String fileLocation = request.getParameter(""localfile""); File file = new File(fileLocation); Socket socket; try { socket = new Socket(HOST PORT); ObjectInputStream ois = new ObjectInputStream(socket.getInputStream()); ObjectOutputStream oos = new ObjectOutputStream(socket.getOutputStream()); oos.writeObject(file.getName()); FileInputStream fis = new FileInputStream(file); byte [] buffer = new byte[BUFFER_SIZE]; Integer bytesRead = 0; while ((bytesRead = fis.read(buffer)) > 0) { oos.writeObject(bytesRead); oos.writeObject(Arrays.copyOf(buffer buffer.length)); } oos.close(); ois.close(); } catch (Exception e) { e.printStackTrace(); } } public void downloadFile(HttpServletRequest request HttpServletResponse response) { File file = new File(request.getParameter(""remotefile"")); Socket socket; InputStream inputStream = null; OutputStream outputStream = null; try { socket = new Socket(HOST PORT); response.setContentLength((int)file.length()); outputStream = response.getOutputStream(); inputStream = new BufferedInputStream(new FileInputStream(file)); int nextByte; while ((nextByte = inputStream.read()) != -1) { outputStream.write(nextByte); } } catch (Exception e) { e.printStackTrace(); } finally { if (inputStream != null) { try { inputStream.close(); } catch (IOException e) { e.printStackTrace(); } } } } } Since you're using a HTTP servlet I strongly suggest to use a HTTP client. Don't pass around proprietary and self-invented request/response formats around but just construct requests and responses according the HTTP protocol. When you adhere a certain transport standard then there is no doubt that there are several API's and tools available which may ease the job. In the client side you can use java.net.URLConnection or Apache HttpClient for this. Sending files over HTTP from client to server (uploading) usually require a multipart/form-data request encoding. Sending files over HTTP from server to client (downloading) usually require just a correct Content-Type header and the entire file as response body. At the bottom of this answer you can find an example how to upload files by URLConnection (and in this answer an example with Apache HttpClient 4). In this answer you can find an example how to process the uploaded file in servlet. Saving the uploaded file is easy: just write the obtained InputStream to some FileOutputStream. In this article you can find an example how to send the file for download. Saving the downloaded file is also easy just write URLConnection#getInputStream() to some FileOutputStream. Ok if I want to download a file with filename specified in the URL I can use a URLConnection. But I want to download a random file from the remote machine on which my server is running. BalusC makes a very important point here. There is no need to re-invent the wheel. So your concrete problem is that you don't know how to let the server return a random file?"
774,A,"Non-Blocking UDP-Client Receive Thread-Safe Call I have been looking for a solution to this problem for ages. I have tried many things including BeginReceive() but all to no avail. There must be a way to do this make a UDP-Client receive call that is non blocking AND thread safe. I am trying to receive a message and write it to a Rich Text Box. using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Linq; using System.Text; using System.Windows.Forms; using System.Net; using System.Net.Sockets; using System.Threading; using System.IO; namespace Chat { public partial class Form1 : Form { public bool contfunctioning = true; public bool changedsplay = false; public string enteredchats msg chatstring Chats; UdpClient subscriber = new UdpClient(8899); UdpClient publisher = new UdpClient(""230.0.0.100"" 8898); public Form1() { InitializeComponent(); } private void btnConnect_Click(object sender EventArgs e) { string ConnectIPAddress; ConnectIPAddress = txtboxIP.Text; IPAddress addr = IPAddress.Parse(ConnectIPAddress); MessageBox.Show(""Subscribing to chat server on "" + ConnectIPAddress + ""."" ConnectIPAddress); EndPoint ep = null; // This is where The UDPClient subscriber needs to Begin.Receive() } private void btnSend_Click(object sender EventArgs e) { enteredchats = txtboxUsr.Text + "" "" + txtboxentertxt.Text; txtboxentertxt.Clear(); msg = String.Format(enteredchats); sdata = Encoding.ASCII.GetBytes(msg); publisher.Send(sdata sdata.Length); } } } Enough code yes? Thanks in advance for all responses. How have you tried `BeginReceive`? It's thread-safe if you use it correctly. What problem are you having exactly? Using BeginReceive would work perfectly here. Is it me or does the code snippet contain BeginReceive somewhere? It does not contain BeginReceive() as I stated in my question I tried it and it did not work. The tutorials I found for it were not extremely descriptive if someone could show me in an answer how to do it in this case that would be highly appreciated. I found that using a Timer() allowed me to circumvent this problem. using System.Data; using System.Drawing; using System.Linq; using System.Text; using System.Windows.Forms; using System.Net; using System.Net.Sockets; using System.IO; using System.Threading; using System.Diagnostics; namespace Chat { public partial class Form1 : Form { public bool contfunctioning = true; public bool changedsplay = false; public bool setupntwrk = false; public string enteredchats msg chatstring Chats lastchatstring; UdpClient subscriber = new UdpClient(8899); UdpClient publisher = new UdpClient(""230.0.0.100"" 8898); System.Windows.Forms.Timer timer1use = new System.Windows.Forms.Timer(); public Form1() { InitializeComponent(); } private void btnConnect_Click(object sender EventArgs e) { Thread rcvchats = new Thread(ReceiveChats); rcvchats.Start(); timer1use.Interval = 1000; timer1use.Start(); } private void btnSend_Click(object sender EventArgs e) { enteredchats = txtboxUsr.Text + "": "" + txtboxentertxt.Text; txtboxentertxt.Clear(); msg = String.Format(enteredchats); byte[] sdata = Encoding.ASCII.GetBytes(msg); publisher.Send(sdata sdata.Length); } private void timer1_Tick(object sender EventArgs e) { if (chatstring != lastchatstring) dsplay.AppendText(""\r\n"" + chatstring); lastchatstring = chatstring; } public void ReceiveChats() { while (true) { if (setupntwrk == false) { string ConnectIPAddress; ConnectIPAddress = txtboxIP.Text; IPAddress addr = IPAddress.Parse(ConnectIPAddress); MessageBox.Show(""Subscribing to chat server on "" + ConnectIPAddress + ""."" ConnectIPAddress); subscriber.JoinMulticastGroup(addr); setupntwrk = true; } IPEndPoint ep = null; chatstring = Encoding.ASCII.GetString(subscriber.Receive(ref ep)); Thread.Sleep(1000); } } private void btnHost_Click(object sender EventArgs e) { Process starthost = new Process(); starthost.StartInfo.FileName = ""C:\\ChatServ.exe""; starthost.Start(); } } } Why the downvote? Its legal to answer my own question in fact there happens to be a badge for it."
775,A,"MD5 purpose or uses If we can't decode the MD5 hash string then what is the purpose of MD5 where can we use MD5. I'm generally not a fan of link answers but: http://en.wikipedia.org/wiki/Cryptographic_hash_function#Applications If you add two numbers x and y you get a result. It is not possible to figure out what x and y were just by looking at the result. And yet addition is useful. then there may be the possibility that we are adding 5+2 or 4+3. In both the cases the answer will be same but x and y is different. Daniel's example was probably choosen to say that usefulness can't be determined by the fact the algorithm has no reverse function. A Hash algorithm by definition should produce different results for different inputs. Of course collisions exists but they **must** be rare or the hash algorithm is unefficient. yes.. but still.. adding is useful. MD5 is mainly used to maintain the integrity of files when it is send from 1 machine to another machineto detect whether any man in middle third party have not modify the contents of files. Basic example is : When you download any file from server server has MD5 calculated when it comes to you it again check for md5 values if md5 hash matches file is not corrupted or not modified by any third person.  Good hash functions like MD5 can be used for identification. See this question. Under certain conditions you can assume that equal hashes mean equal data blocks. *Good hash functions like MD5* seems a little bit outdated to me. @Georg: MD5 has been hacked so it should not be used for scenarios when someone would want to subvert it. Otherwise it is still good. Not really. DSPs (like say your video card's GPUs) can brute-force md5 in no time flat because it's too simple to calculate. The code at http://bvernoux.free.fr/md5/index.php for example claims to process 200 million md5 hashes per second on common consumer hardware (GeForce 8800GT and Core2Duo E6750 using one core). It's so easy to brute-force that there's no real point in making algorithmic attacks against it any more. @Dave Sherohman: Didn't know of those. Still this doesn't prevent from using MD5 in scenarios when there're noone to subvert it. @sharptooth: If there's no one to subvert it you are not paranoid enough ;) @Piskvor: Seriously I didn't mean cases when data protected is so useless noone wants to hack into it. I meant scenarios where there is no attacker - like this one http://stackoverflow.com/questions/862346/how-do-i-assess-the-hash-collision-probability @sharptooth: I see. I'd personally go with SHA-256 - not too much slower; but I guess MD5 would be good enough there.  To store data save in a database for example. If you save your password using md5 and you compaire it with the password you enter and encrypt it is still the same password but you cant see in the database what is the password. For example: password = 123 md5 of 123 = fkdjafkldjslghrioawegh if you try to log in and you enter 123 the md5 if it will still be the same and you can compair those. But if your database is hacked the hacker cant reed the password (123) +1. Note however that when storing passwords an additional random prefix should be added when computing the hash and stored alongside the hash. This way users choosing poor passwords won't be compromised by hash collisions. This is commonly known as a _salt_. E.g. if two users both choose ""secret"" as their password the hash will be the same. A cracker with access to a set of passwords could look for these common hashes and thus discover the password that produced it. The salt makes it extremely unlikely that the same password will produce the same hash for different users. See also this other question: http://stackoverflow.com/questions/536584/non-random-salt-for-password-hashes/ Keep in mind that passwords shouldn't be stored as a single MD5 hash (or any other single hash) since single hash operations are too fast to easily resist brute forcing: http://chargen.matasano.com/chargen/2007/9/7/enough-with-the-rainbow-tables-what-you-need-to-know-about-s.html  It allows you to determine whether the data you have (e.g. an entered password) is the same as some other data which is secret (e.g. the correct password) without requiring access to the secret data. In other words it can be used to determine ""is this user-entered password correct?"" while also keeping the correct password secret. (Note that there are stronger hashing methods out there which should be used instead of md5 for this purpose these days such as sha* and bcrypt. With modern hardware it's fairly easy to throw millions of passwords per second at an md5 hash until you find one that matches the correct password.) It allows you to verify the integrity of a transmitted file by comparing the md5 hash of the original file with the md5 hash of the data that was received. If the hashes are different the received data was not the same as the sent data so you know to re-send it; if they're the same you can be reasonably certain that the sent and received data are identical.  An decryptable file has the property that its always at least as big as the original file a hash is much much smaller. This allows us the create a hash from a file that can prove the integrity of the file without storing it. There are many reasons not to store the file in encrypted or plain text: As soon as an encrypted file falls in the wrong hands they could try to decrypt it. There's no chance that's going to happen with a hash. You simply don't need the file yourself but maybe you're sending it to someone and that person can proof it's integrity using the hash. +1. Are you sure about the *at least as big* statement ? Can't we imagine a cipher algorithm that use something like a ""compression method"" ? (I'm not talking of ciphering then compressing but of an hypothetical algorithm that could produce smaller ciphered data). Just wondering. In order to obtain meaningful compression you'd have to compress the data first then encrypt it. (Why? Because well-encrypted data is essentially random and compression relies on patterns in the data so it doesn't work on random data.) In that case the data being encrypted is the compressed form of the original data and Georg's statement that the encrypted data must be at least as large as the data being encrypted (which remember is now the compressed form) still holds. @ereOn: That would be a compression algorithm. But imagine you've got already compressed input data in that case you couldn't compress it even more. (If that were possible you could compress data indefinitely.) Yep it make sense. Thanks to both of you ;) There is in fact no such thing as a reversible compression function - *on average* the output of every reversible function is also at least as big as the original file. A ""compression algorithm"" is actually an ""expansion algorithm"" with some interesting failure cases :)  MD5 is a hash function and there are more like that such as SHA PBKDF bcrypt and scrpyt. I really prefer scrypt. Hash functions are used for integrity reasons in order to detect any manipulations that may occurred during the transmission of the actual message. The receiver is able to find if the received message has not not changed by checking the hash value of the message. These functions have three security properties: 1) It is difficult for someone to detect the actual message when it only has the h(m). 2) Given a message m and its hash function it is difficult to find another message with the same hash value. 3) Last it is difficult to find to different messages m1 m2 with the same hash value. Also it is important to know that hash function's algorithms are public and it is very easy to compute the hash value of a message. Moreover hashes are ""one-way"" functions meaning that is hard to find the message given the hash of the message. The actual security thus is based on that property."
776,A,"Help with this code ( gives a STATUS_ACCESS_VIOLATION ) Below is a code where I want to display the ip address of the address entered by the user but it gives me STATUS_ACCESS_VIOLATION and I am not understanding the reason for it. It compiles fine. Here is it #ifndef WIN32_LEAN_AND_MEAN #define WIN32_LEAN_AND_MEAN #endif // Link to ws2_32.lib #include <winsock2.h> #include <stdio.h> #include <stdlib.h> struct protoent *proto=NULL; int main(int count char *strings[]) { int pid; struct hostent *host; struct sockaddr_in addr; if ( count != 3 ) { printf(""usage: %s <addr> < Adressflag = 0 for name and 1 for IP > /n"" strings[0] ); exit(0); } if ( count == 3 ) { int flag = atoi(strings[2]); printf(""flag is %d""flag); if ( flag == 0) { pid = getpid(); proto = getprotobyname(""ICMP""); host = gethostbyname(strings[1]); bzero(&addr sizeof(addr)); addr.sin_family = host->h_addrtype; addr.sin_port = 0; addr.sin_addr.s_addr = *(long*)host->h_addr; //traceroute(&addr); printf(""IP of %s is %s""host->h_name  host-> h_addr); } else { } } return 0; } I am running this in cygwin. Is anyone aware of the error and can anyone tell what am I doing wrong ?? please chose a more informative title for your question. What does your debugger say? @Tomalak How to run it from cygwin ? @user506710: Read your toolchain's documentation. I would be really grateful if somebody can run this and tell me whether it's running fine at their end because then it would probably be a cygwin error in my desktop....Thanks a lot.. Do you really need the * on the front? addr.sin_addr.s_addr = *(long*)host->h_addr That means you are de-referencing some random address. You are also not checking host for null. No probs hope you manage to work the rest out If I don't It gives me a warning that assignment makes integer from pointer without a cast..... @user506710: That your code is broken without the cast does NOT mean that you should add the cast. The cast makes your code _more_ broken but hides the issue at compile-time. try removing all the addr code for now until you get the main bit working. What I mean is find the line that isn't working and fix that. @Sasquiha So I removed the address part but it does'nt really help as the error still remains.... Could it be because of cygwin also ? @user506710 have you check host for being null? @Sasquiha Nopes :) Turns out that was the error.... Thanks a lot..  addr.sin_addr.s_addr = *(long*)host->h_addr You can't just cast a char* to long* to get a number from a string and you probably shouldn't be dereferencing either."
777,A,route a custom url to a web-application I have a Vista Home PC with computer-name say *office_pc1* and with IP say 192.168.11.40. I have a web-application in that pc which can be successfully accessed via the IP itself. Now my question is how can I access that web-application if I'm entering the url as: http://support.somedomain.com/ in my local intranet. Is that possible without using any Server OS? Can I implement it by adding some entires in the etc/hosts file in the System32 folder of that machine. (The web-application is a third-party application. so i cannot touch the source-code) Is this a dreamy question? Thanks. thanks for the help. I actually solved the problem is an alternate way. As you know adding a host entry in every pc is not an ideal solution. By taking the risk I added the entry into my proxy machines hosts file. And Chanrmingly its working. Hope this helps. Thanks.  If you want to fake it then you can add a entry to your hosts file yes. Add 192.168.11.40 support.somedomain.com then run at an elevated command prompt ipconfig /flushdns Now try pinging that FQDN and you should see it resolve to your 192 address and away you go hi ive already added an entry in the host file. whenver i try to ping with FQDN i'm getting an error reply as: --- Ping request could not find host support.somedomain.com. Please check the name and try again. --- I tried the same from that local machine and from the nearby pcs. but getting the same error. Any idea? Did you flush the DNS first? Did you make sure notepad didn't save the file as hosts.txt? ah! thats a silly mistake. A small spelling-mistake. I eneterd 'supprt.somedomain.com' instead of 'support.somedomain.com'. Now i can successfully ping to 'support.somedomain.com' in the local machine. How can I ping from other pcs on the same network? Add the same entry in every hosts file. If you have an internal DNS server (which is unlikely for small offices or home) you could create a zone for that domain and add an A record to that zone but now we're dipping into server fault territory. You are right we dont have any internal DNS server. instead of adding in every host file is there any other workaround? Can i do anything on my proxy machine to acheive this? Nope. You don't want it to reach the proxy. it's lots of manual updates for you! ok thanks. let me try. thanks for your help.
778,A,"tcp underlying transmission mechanism/ network programming I have searched but I could not find the following: Process1 transmits data over TCP socket. The code that does the transmission is (pseudocode) //Section 1 write(sockdatalen);//any language.Just write data //Section 2 Process1 after the write could continue in section 2 but this does not mean that data has been transmitted. TCP could have buffered the data for later transmission. Now Process2 is running concurrently with Process1. Both processes try to send data concurrently. I.e. both will have code as above. Question1: If both processes write data to TCP socket simultaneously how will the data be eventually transmitted over the wire by IP/OS? a) All data of Process1 followed by all data of Process2 (or reverse) i.e. some FIFO order? or b) Data from Process1 & Process2 would be multiplexed by IP layer (or OS) over the wire and would be send ""concurrently""? Question2: If e.g. I added a delay would I be sure that data from the 2 processes were send serially over the wire (e.g. all data of Process1 followed by all data of Process2)? UPDATE: Process1 and Process2 are not parent child. Also they are working on different sockets Thanks A TCP socket is identified by a tuple that usually is at least (source IP source port destination IP destination port). Different sockets have different identifying tuples. Now if you are using the same socket on two processes it depends on the order of the write(2) calls. But you should take into account that write(2) may not consume all the data you've passed to it the send buffer may be full causing a short write (write()'ing less than asked for and returning the number of bytes written as return value) causing write() to block/sleep until there is some buffer space or causing write() to return an EAGAIN/EWOULDBLOCK error (for non-blocking sockets). I am talking about different socket per process. You mean the process could block the second start writing and occur the interleaving? @user384706: If they are different sockets it doesn't really matter. Every TCP segment contains the source and destination ports so even if your TCP/IP stack was sending 1 byte of data at a time there is no possible confussion. I know that they can not be mixed. My question is what is the message flow I will see over the wire.Multiplexing or serial transmission? @user384706: You never really know. Depends on lots of things mainly network conditions and OS's TCP/IP stack.  Answer 1: the order is unspecified at least on the sockets-supporting OS's that I've seen. Processes 1 & 2 should be designed to cooperate e.g. by sharing a lock/mutex on the socket. Answer 2: not if you mean just a fixed-time delay. Instead have process 1 give a go-ahead signal to process 2 indicating that process 1 has done sending. Use pipes local sockets signals shared memory or whatever your operating system provides in terms of interprocess communication. Only send the signal after ""flushing"" the socket (which isn't actually flushing). @larsmans:I read the link. What do you mean ""it explains that there is no flush operation on sockets""? It says that using flush data in buffer not yet send are send. How TCP_NODELAY fits in? It does not mention this @larsmans: So write flush signal.Flushing is guaranteed to transmit all data to destination or is like e.g. in C++ I/O operations? Only a signal to OS to flush. Not guarantee to happen yet @larsmans:Is flushing always used in network programs? That might depend on your OS but in any case the data is in the kernel buffer before process 2 starts writing so that will likely push the data over the wire. @user384706 no many don't they just wait for the OS to send data. Enabling TCP_NODELAY (prevent some buffering) is sometimes done to improve performance though it doesn't always help. @larsmans: TCP_NODELAY and flushing are different? @user384706 please read the post I linked to it explains that there is no flush operation on sockets.  write() is atomic; ditto send() and friends. Whichever one executed first would transmit all its data while the other one blocks. The delay is unnecessary see (1). EDIT: but if as I now see you are talking about different sockets per process your question seems pointless. There is no way for an application to know how TCP used the network so what does it matter? TCP will transmit in packets of up to an MTU each in whatever order it sees fit. You are saying that if Process1 calls write at e.g. timestamp 1:00:00 and Process2 calls write 2 seconds later it is still not certain if the packets will be multiplexed or the transmission of Process1 would have finished? Obviously there is a point with short transmissions or long intervals where the question becomes ridiculous. I still don't know why it matters to you. In a sense the question is *inherently* ridiculous.  Hmm are you are talking about single socket shared by two processes (like parent and child)? In such a case the data will be buffered in order of output system calls (write(2)s). If which is more likely you are talking about two unrelated TCP sockets in two processes then there's no guarantee of any order in which the data will hit the wire. The reason for that is sockets might be connected to remote points that consume data with different speeds. TCP flow control then makes sure that fast sender does not overwhelm slow receiver. But the order of output system calls must be explicitly controlled or it becomes indeterminate. Yes but that's a whole different discussion. @Nikolai N Fetissov:So the behavior of OS system calls is not deterministic to analyze? TCP stack is a very complex beast. From the application point of view you can just assume it's not deterministic :) @Nikolai N Fetissov: But is TCP doing this or IP? TCP passes data to IP. So IP is also black-box? It's TCP that puts pressure on the sender if receiver is slow. IP is packet at a time. Then at the link layer device drivers are free to drop output frames if hardware is busy."
779,A,"TCPClient vs Socket in C# I don't see much use of TCPClient yet there is a lot of Socket? What is the major difference between them and when would you use each? I understand that .net socket are written on top of WINSOCK and TCPClient is a wrapper over Socket class. Thus TCPClient is way up the chain and possibly inefficient. Correct me if I am wrong. Thanks Also you can access the socket directly from the TCPClient object it's under the property Client - so there is no performance difference.  The use of TcpClient and TcpListener just means a few less lines of code. As you say it's just a wrapper over the Socket class so there is no performance difference between them it's purely a style choice. 'style' is a big word for 'common sense' here. If you can use a prefab by all means do so :) The note here seems to contradict this. http://msdn.microsoft.com/en-us/library/system.net.sockets.socket%28v=vs.110%29.aspx ""If you are writing a relatively simple application and _do not require maximum performance_ consider using TcpClient"""
780,A,"Disable Carriage return I am using simple UDP connection. I would like to know if by default the connection has ""Carriage return"" enabled or disabled and how could I set that property? thanks ray. UDP traffic is session/connection less. So you can't have a ""connection"" on UDP. UDP is used to pass binary data rather than text and there is no way to disable carriage return or any other character. ""no way to disable carriage return"" so you say it's enabled in anyway? It not something you enable or disable. A UDP packet has no special handling for any particular byte value carriage return or any other byte value.  Eh that's not entirely accurate. UDP isn't differentiated by virtue of sending text vs. binary. All network protocols ultimately send data as bit streams (binary). What typically differentiates it is that unlike TCP there is no back and forth to establish sequence numbers for tracking packets and no ACK flag to signal that a packet was received. UDP will send packets with no regard to whether or not they get to the destination. Edit: Ray maybe you should provide a little more detail about what you're trying to do. Carriage Return is an ascii character just like any other. It has a numerical representation and occupies a byte of space just like the other ascii characters. So asking if it's ""enabled"" for UDP transmission isn't really a valid question. Any series of bits can be sent via UDP or TCP or any other protocol - which means UDP doesn't even understand what ASCII is or the letter ""b"" or a carriage return. It's all just a bunch of 1's and 0's and UDP is aware of IP addresses and Port numbers - just enough to send your bits of data somewhere. What your application does with those bits is the question. Thanks for your explanation I guess i didn't really understood what CR means. I thought it's some property which you set on the object. same as you can enable/disable it with regular windows TELNET program. Well you can prevent a Carriage Return from being used for something but that's an application level thing that you would have to code and doesn't have anything to do with UDP. If you elaborate maybe someone can help but I think your issue is specifically Java and not network related.  UDP broadcasts binary data - if you encode \r and/or \n to bytes and add it to the message it will be sent. No filtering no conversion on this protocol layer."
781,A,"Problem with IP selection Socket Programming C# I am creating a server (desktop based) which listens on a port 4504 using this bit of code  IPAddress[] AddressAr = null; String ServerHostName = """"; try { ServerHostName = Dns.GetHostName(); IPHostEntry ipEntry = Dns.GetHostByName(ServerHostName); AddressAr = ipEntry.AddressList; } catch (Exception) { } if (AddressAr == null || AddressAr.Length < 1) { return ""Unable to get local address ... Error""; } Listener_Socket = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); Listener_Socket.Bind(new IPEndPoint(AddressAr[0] Port)); Listener_Socket.Listen(-1); Listener_Socket.BeginAccept(new AsyncCallback(EndAccept) Listener_Socket); return (""Listening On "" + AddressAr[0].ToString() + "":"" + Port + ""... OK""); Now the problem is that I want to run this on my server and the value of AddressAr[0] I want to be is the public IP of my server but this snippet returns the local lan address of the server. Like I want AddressAr[0] = ""180.123.45.6"" something [which is the public IP of my server] but with this snippet I am getting AddressAr[0] = ""192.168.2.2"" PS: I am running this server as a desktop app and my trials were in the debugging mode. I'd appreciate any help. Thank You. You need to listen on your local host which will be your 192.168.2.2 (this is correct) and then forward the external packets/traffic from your public address to your local machine. If you're using for instance a linksys router you can go to NAT/QOS and forward any incoming traffic on a specific port e.g. 4504 to your local machine address 192.168.2.2 Any standard home router will have port forwarding built into the firmware. @Joy This is less of a programming question and more of a networking question but as it relates to listening for traffic via code it's at least educational.  When programing server you should always listen local ip ""127.0.0.1"" and all your connections from outside redirect to your local ip address where server is installed. Routing request from outside you should handle on your router. So basically that is it. I hope this helps. Do you have any clue if Socket Programming can be done using ASP.NET? So I can't expose my port on the server to the outside environment directly? Is the port forwarding on the router the only way to go? This server would be hosted on RackSpace cloud so you mean I would have to ask rackspace guys to adjust the router to forward all the port request form public IP like 184.123.4.4:4502 to localhost:4502? Yes its the only way. One way or another you need to setup this on your router. So basically what you need to do: Add port forwarding from publicip:port to your_computer_ip:port and that is all. If your server is on they by default transfer all trafic from your public IP to your server in theirs network only thing that you need to ask them to open you port. Okay. Thank you so much. I will work on it. Opening a port just means that unblocking it in the firewall or there is more to that. I kinda tried to unblock from firewall from my local machine but still when I use port scanner tools the desired port is blocked. Can you please help? Yes you need to add exception to your firewall on your local machine. I actually did that and then even disabled my firewall. Yet when I scan using http://www.yougetsignal.com/tools/open-ports/ I see it to be blocked! And there I can't help you start your server and than scan maybe its reporting blocked as closed  You can simply listen on any IP address that your machine has: listenerSocket.Bind(new IPEndPoint(IPAddress.Any port));"
782,A,"Convert urb to skbuff I am trying to write a kernel module which is the combination of the usb driver and a network driver now whenever the data is recieved by the usb driver it will be in struct urb but the data that is required by the net driver is sk_buff now how do i convert a urb to sk_buff i found something similar is being done in the phonet protocol but i couldnt understand howexactly this is being done. Can someone explain this code. drivers/net/usb/cdc-phonet.c static void rx_complete(struct urb *req) { struct net_device *dev = req->context; struct usbpn_dev *pnd = netdev_priv(dev); struct page *page = virt_to_page(req->transfer_buffer); struct sk_buff *skb; unsigned long flags; int status = req->status; switch (status) { case 0: spin_lock_irqsave(&pnd->rx_lock flags); skb = pnd->rx_skb; if (!skb) { skb = pnd->rx_skb = netdev_alloc_skb(dev 12); if (likely(skb)) { /* Can't use pskb_pull() on page in IRQ */ memcpy(skb_put(skb 1) page_address(page) 1); skb_add_rx_frag(skb skb_shinfo(skb)->nr_frags page 1 req->actual_length); page = NULL; } } else { skb_add_rx_frag(skb skb_shinfo(skb)->nr_frags page 0 req->actual_length); page = NULL; } if (req->actual_length < PAGE_SIZE) pnd->rx_skb = NULL; /* Last fragment */ else skb = NULL; spin_unlock_irqrestore(&pnd->rx_lock flags); if (skb) { skb->protocol = htons(ETH_P_PHONET); skb_reset_mac_header(skb); __skb_pull(skb 1); skb->dev = dev; dev->stats.rx_packets++; dev->stats.rx_bytes += skb->len; netif_rx(skb); } goto resubmit; case -ENOENT: case -ECONNRESET: case -ESHUTDOWN: req = NULL; break; case -EOVERFLOW: dev->stats.rx_over_errors++; dev_dbg(&dev->dev ""RX overflow\n""); break; case -EILSEQ: dev->stats.rx_crc_errors++; break; } dev->stats.rx_errors++; resubmit: if (page) netdev_free_page(dev page); if (req) rx_submit(pnd req GFP_ATOMIC); } The URB struct doesn't contain the actual transfered data rather it contains a pointer to the data the transfer_buffer element. The code basically copies data from the transfer buffer to the sk buff. The virt_to_page / page_address will return the virtual address of the start of the transfer_buffer page."
783,A,"Monitor traffic in Java I'm messing around with a program to track my internet browsing habits and I'm trying to think of the best way to do it. I have some ideas but I'm not sure how feasible they are. Somehow hook into firefox. I don't think there's an API that I can hook java into firefox with. I heard something about a firefox java rmi add on that I can access from java but I can't find anything. This would restrict me to firefox which is fine because I only use firefox. Emulate Windows ""netstat"" somehow. I don't want to call netstat because that limits me to Windows. This would be a last case scenario Socket programming. I can set up a ServerSocket to listen on port 80 but not if there is already something using that port so it doesn't do what I want it to do listen to what's already going on on port 80. Use a packet capture library. This seems like overkill. Any other ideas? A different approach to this problem might be to simply look at the title of the foreground window at regular intervals. I know this is easy to do in Win32 for example. Sure but I might as well just use the netstat command if I want to use Win32 calls Why does using netstat limit you to Windows? Wouldn't I have to call it via Runtime.exec? Isn't that windows call? netstat exists on pretty much all modern operating systems (Windows Mac OS X Linux etc). However I'm not sure what the output of netstat would give you anyway since it only shows *active* connections (and reading a page in a browser doesn't generally hold an active connection to the server after the page has loaded). good point might be something to try out though. Sure Check rabbit a java proxy with monitoringsniffing and filtering capabilities http://www.khelekore.org/rabbit/ I'll try that out. Have you used it with any success? yes depend what u state by ""any success"".:-) it's pretty simple to use and very robust and fast tanx to its nio implementation. you can grab the source where u can find out some examples to filterblock/redirect with many configurations options. Whatever your exact project is i assume it's better than other solution since u are not dependant of platform or presence and installation of low level network library. You can edit your previous posts to clarify your answer. SO is not like regular forums ;-)  Use a java local proxy to monitor any request from any browser I'm not familiar with the concept. Could you point me to a tutorial?"
784,A,"WCF net.tcp windows service - call duration and calls outstanding increases over time I have a windows service which uses the ServiceHost class to host a WCF Service using the net.tcp binding. I have done some tweaking to the config to throttle sessions as well as number of connections but it seems that every once in a while my ""Calls outstanding"" and ""Call duration"" shoot up and stay up in perfmon. It seems to me I have a leak somewhere but the code I have is all fairly minimal I'm relying on ServiceHost to handle the details. Here's how I start my service ServiceHost host = new ServiceHost(type); host.Faulted+=new EventHandler(Faulted); host.Open(); My Faulted event just does the following (more or less logging etc removed) if (host.State == CommunicationState.Faulted) { host.Abort(); } else { host.Close(); } host = new ServiceHost(type); host.Faulted+=new EventHandler(Faulted); host.Open(); Here's some snippets from my app.config to show some of the things I've tried <runtime> <gcConcurrent enabled=""true"" /> <generatePublisherEvidence enabled=""false"" /> </runtime> ......... <behaviors> <serviceBehaviors> <behavior name=""Throttled""> <serviceThrottling maxConcurrentCalls=""300"" maxConcurrentSessions=""300"" maxConcurrentInstances=""300"" /> .......... <services> <service name=""MyService"" behaviorConfiguration=""Throttled""> <endpoint address=""net.tcp://localhost:49001/MyService"" binding=""netTcpBinding"" bindingConfiguration=""Tcp"" contract=""IMyService""> </endpoint> </service> </services> .......... <netTcpBinding> <binding name=""Tcp"" openTimeout=""00:00:10"" closeTimeout=""00:00:10"" portSharingEnabled=""true"" receiveTimeout=""00:5:00"" sendTimeout=""00:5:00"" hostNameComparisonMode=""WeakWildcard"" listenBacklog=""1000"" maxConnections=""1000""> <reliableSession enabled=""false""/> <security mode=""None""/> </binding> </netTcpBinding> .......... <!--for my diagnostics--> <diagnostics performanceCounters=""ServiceOnly"" wmiProviderEnabled=""true"" /> There's obviously some resource getting tied up but I thought I covered everything with my config. I'm only getting about ~150 clients so I don't think I'm coming up against my ""300"" limit. ""Calls per second"" stays constant at anywhere from 2-5 calls per second. The service will run for hours and hours with 0-2 ""calls outstanding"" and very low ""call duration"" and then eventually it will shoot up to 30 calls oustanding and 20s call duration. Any tips on what might be causing my ""calls outstanding"" and ""call duration"" to spike? Where am I leaking? Point me in the right direction? It might be some inefficiency in the code you're executing when the service is called. Without seeing that it's tough to really diagnose. Better late than never but it did end up being exactly that."
785,A,IDE plugin's for developing multithreaded network applications I am looking for a plugin that helps developers create multithreaded network applications that works with either Eclipse and/or Netbeans. Should allow for functionality such as: Graphical modeling of callbacks Configuring executors Creating custom SSL factories Wizards for creating various filter streams How about a framework? JBoss Netty (http://www.jboss.org/netty/) and Apache MINA (http://mina.apache.org/) are very similar in function and will both facilitate the sorts of things you mention. As for an IDE plugin I don't know of anything but doubt the value of such a tool.
786,A,Does the .Net Framework Contain any Libraries for Monitoring Network Activity? I'd like to experiment with writing my own little monitoring/firewall app. I don't think .Net contains anything but there is the Network Monitor API as described here which should be possible to use from .Net using some PInvoke magic. Actually that link mentions a NetmonAPI.CS file that does the PInvoke for you... Thanks I'll check that out.
787,A,Broadcast-style Bluetooth using Sockets on the iPhone? Is there any way to open a broadcast bluetooth socket take a listen and send replies? I want a proper peer to peer system where I broadcast and listen for broadcasts in an area. That way variable clients can mingle. Is this possible? My theory is this: If GameKit can sit around wasting 25 seconds of the users time whilst having access to a broadcast socket can't I? Or must I be in kernel mode for such access? I'm not really sure where the proper bluetooth headers are as well. Thanks for reading! Bluetooth doesn't really have broadcast mode. The closest you can get to it is to have some protocol where one of the players becomes a master of the game and everybody else connects to him. After that the master must rebroadcast received packets to the rest of the clients. You can build on top of that to have some shared state and make the players re-elect the master if the original one disappears. Seems like we have no freedom as developers. I demand raw sockets! I'm sure I could make it connect way faster than GameKit does as well. It takes a total of 40 seconds between a 3GS and 3G. are you connecting to the known device by mac address? Or to unknown device. Bluetooth inquiry and discovery could take up to 2 minutes. Connection to the known address should take no more then 2 seconds.
788,A,"Net use: How can I connect to a network share? (using vb.net) How can I connect to a network share without using System.Diagnostics.Process.Start? I need to connect to a network share and get feedback if it already exists. Can I use some kind of API? define ""feedback"" Use a native call to mpr.dll to add a connection http://pinvoke.net/default.aspx/mpr/WNetAddConnection2.html You provide the function with the localname (aka diskletter) and remotename (the \server\sharename) in the NETRESOURCE class A return value of 0 means success a value of (among many others) const int ERROR_ALREADY_ASSIGNED = 85 (see http://msdn.microsoft.com/nl-be/library/aa385413(v=VS.85).aspx) means the localname is already in use. If you want to find out if the remotename is already assigned you have to enumerate over the current mappings with the WNetEnumResource http://pinvoke.net/default.aspx/mpr.WNetEnumResource Great - thank you !! :)"
789,A,"what does the number mean in the middle of the http header and content I'm writing a program that is similar to a browser but much more simple. The user types a link and the program saves it to a file in local. I sent an http header like this: GET /index.php HTTP/1.1\r\n Host: www.highradio.tw\r\n \r\n then I received: HTTP/1.1 200 OK Date: Sat 16 Apr 2011 14:16:29 GMT Server: Apache/2.2.17 (Win32) mod_ssl/2.2.17 OpenSSL/0.9.8q mod_wsgi/3.3 Python/2.7.1 PHP/5.2.5 (x64) X-Powered-By: PHP/5.2.5 (x64) Set-Cookie: PHPSESSID=d0ug0im94rg7lrjp40h38vo784; path=/ Expires: Thu 19 Nov 1981 08:52:00 GMT Cache-Control: no-store no-cache must-revalidate post-check=0 pre-check=0 Pragma: no-cache Vary: Accept-Encoding Transfer-Encoding: chunked Content-Type: text/html 204a <!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Transitional//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd""> <html xmlns=""http://www.w3.org/1999/xhtml""> .... What does the number (in HEX?) 204a mean? When I use Chrome or wget in terminal the 204a doesn't show up. I print out the first received 1024 bytes. I haven't parsed the header out yet. That hex value represents the number of bytes in the following chunk. Check out the Chunked transfer encoding page on Wikipedia it will tell you what that number is. It is possible to tell server not to send by this mode? (without chunked) apart from requesting via HTTP/1.0 I don't know a way to force non-chunked encoding. Wiki says ""For version 1.1 of the HTTP protocol the chunked transfer mechanism is considered to be always acceptable"". Thanks."
790,A,Android - find a server in the network I am currently writing a client-server application and I ask myself if there is a better way to find a server in the local network then going trough all the available IP addresses and see if the correct answer is supplied? You might want to look into UDP broadcasts where your server announces itself and the phone listens for the broadcasts. There is an example from the Boxee remote project quoted below. Getting the Broadcast Address You need to access the wifi manager to get the DHCP info and construct a broadcast address from that: InetAddress getBroadcastAddress() throws IOException { WifiManager wifi = mContext.getSystemService(Context.WIFI_SERVICE); DhcpInfo dhcp = wifi.getDhcpInfo(); // handle null somehow int broadcast = (dhcp.ipAddress & dhcp.netmask) | ~dhcp.netmask; byte[] quads = new byte[4]; for (int k = 0; k < 4; k++) quads[k] = (byte) ((broadcast >> k * 8) & 0xFF); return InetAddress.getByAddress(quads); } Sending and Receiving UDP Broadcast Packets Having constructed the broadcast address things work as normal. The following code would send the string data over broadcast and then wait for a response: DatagramSocket socket = new DatagramSocket(PORT); socket.setBroadcast(true); DatagramPacket packet = new DatagramPacket(data.getBytes() data.length() getBroadcastAddress() DISCOVERY_PORT); socket.send(packet); byte[] buf = new byte[1024]; DatagramPacket packet = new DatagramPacket(buf buf.length); socket.receive(packet); You could also look into Bonjour/zeroconf and there is a Java implementation that should work on Android. Thanks you! This was what I was looking for. @mark: did it work? [They](http://codeisland.org/2012/udp-multicast-on-android/) say that not all android devices handle non-unicast traffic correctly. In my case for example `socket.receive(packet);` returned exactly the same packet as it was send. Seems like it didn't even wait for an answer packet from server and just returned the same data. I have such UDP discovery in my application and about 1%-3% of users claims that the server is not found. This is because some routers block UDP broadcast. I am going to additionally use IP scan and/or Bonjour.
791,A,"Could somebody help me in spotting an error regarding uploading an image from iPhone to server here? I am strictly following the answers from http://stackoverflow.com/questions/125306/how-can-i-upload-a-photo-to-a-server-with-the-iphone. This part below is the part that get called when my button is pressed. NSLog(@""buttonPressed: %@"" [ConnectServerTryViewController getPathForFile: @""myPic.png""]); [[EPUploader alloc] initWithURL:[NSURL URLWithString:@""http://10.27.8.251/senior/uploader.php""] filePath: [ConnectServerTryViewController getPathForFile: @""myPic.png""] delegate:self doneSelector:@selector(onUploadDone:) errorSelector:@selector(onUploadError:)]; and here's my static getPathForFile method of ConnectServerTryViewController class: + (NSString*) getPathForFile: (NSString*) st{ NSString * path = [[NSBundle mainBundle] bundlePath]; NSString * finalPath = [path stringByAppendingPathComponent: st]; return finalPath; } I am very sure that my php is correct. I can upload an image from my mac to my server without any problem but below shows the output from XCode's console: 2009-11-24 15:43:31.177 ConnectServerTry[1645:20b] buttonPressed: /Users/myName/Library/Application Support/iPhone Simulator/User/Applications/4127CEB7-EFCA-4D84-B7CF-F78ED871A499/ConnectServerTry.app/myPic.png 2009-11-24 15:43:31.179 ConnectServerTry[1645:20b] Begin upload method 2009-11-24 15:43:31.180 ConnectServerTry[1645:20b] Trying urlRequest 2009-11-24 15:43:31.181 ConnectServerTry[1645:20b] Trying connection 2009-11-24 15:43:31.188 ConnectServerTry[1645:20b] Now wait for the URL connection to call us back. 2009-11-24 15:43:33.781 ConnectServerTry[1645:20b] -[EPUploader(Private) connection:didReceiveResponse:]: self:0x0x3d19760 [Session started at 2009-11-24 15:43:33 +0700.] 2009-11-24 15:43:33.781 ConnectServerTry[1645:20b] -[EPUploader(Private) connection:didReceiveData:]: self:0x0x3d19760 2009-11-24 15:43:33.783 ConnectServerTry[1645:20b] -[EPUploader(Private) connection:didReceiveData:]: data: C:\AppServ\www\senior There was an error uploading the file please try again! 2009-11-24 15:43:33.784 ConnectServerTry[1645:20b] -[EPUploader(Private) connectionDidFinishLoading:]: self:0x0x3d19760 2009-11-24 15:43:33.784 ConnectServerTry[1645:20b] *** -[ConnectServerTryViewController onUploadError:]: unrecognized selector sent to instance 0x3d21c70 2009-11-24 15:43:33.785 ConnectServerTry[1645:20b] *** Terminating app due to uncaught exception 'NSInvalidArgumentException' reason: '*** -[ConnectServerTryViewController onUploadError:]: unrecognized selector sent to instance 0x3d21c70' 2009-11-24 15:43:33.785 ConnectServerTry[1645:20b] Stack: ( 8307803 2435731003 8689723 8259190 8111810 12403 12513 3020196 3020051 13428152 13426200 13426914 13426914 13429398 13088725 8091873 8088648 87949 88146 23633923 8304 ) Do you appear to know what are some of the possible errors here? Thank you very much. onUploadError: is an invalid selector on ConnectServerTryViewController. How is onUploadError defined and is it defined in the class ConnectServerTryViewController?"
792,A,"Forwarding requests UDPSocket I have a basic ruby program that listens on a port (53) receives the data and then sends to another location (Google DNS server - 8.8.8.8). The responses are not going back to their original destination or I'm not forwarding them correctly. Here is the code. NB I'm using EventMachine require 'rubygems' require 'eventmachine' module DNSServer def post_init puts 'connected' end def receive_data(data) # Forward all data conn = UDPSocket.new conn.connect '8.8.8.8' 53 conn.send data 0 conn.close p data.unpack(""H*"") end def unbind puts 'disconnected' end end EM.run do EM.open_datagram_socket '0.0.0.0' 53 DNSServer end Any thoughts as to why or tips to debug would be most appreciated. The obvious problems are: UDP comms are usually connectionless use the 4 argument version of send instead of connect You're not receiving any data from the socket talking to 8.8.8.8 You're not sending any data back (#send_data) to the original client This seems to work: require 'socket' require 'rubygems' require 'eventmachine' module DNSServer def receive_data(data) # Forward all data conn = UDPSocket.new conn.send data 0 '8.8.8.8' 53 send_data conn.recv 4096 end end EM.run do EM.open_datagram_socket '0.0.0.0' 53 DNSServer end"
793,A,"How to check if a server is listening without exception handling I'm working on two apps that connect to eachother using TCP. At one point one of them is trying to connect using a TcpClient but the other app is not guaranteed to have started listening yet (using TcpListener). My first attempt was this: TcpClient c = null; while (true) { try { c = new TcpClient(); c.NoDelay = true; c.Connect( ip port ); break; } catch (SocketException ex) { Console.WriteLine(string.Format(""Could not connect to {0}:{1} retrying..."" ip port)); Thread.Sleep( 500 ); } } However the problem with this is that it relies on exception handling which is a bit of a nuisance for me because I have VS set up to catch any thrown exceptions (""Debug -> Exceptions..."" menu). So every time it tries to connect it breaks into VS and I have to press continue. The exception I get is: No connection could be made because the target machine actively refused it 127.0.0.1:50000 I would imagine that there would be a way to check if a server is listening on a specific port without actually trying to connect to it. I understand that just checking wouldn't be enough anyway - the server could go down between the check and the connect attempt but it would still be much better for me while I'm developing it. Or something like this: TcpClient tcpClient = new TcpClient(); while ( !tcpClient.TryConnect(....) ) { Thread.Sleep(1000); } Analogous to this: if (bool.TryParse( ""false"" )) { ... } I also tried using async methods (Begin/End Connect) and setting the timeout for the ManualResetEvent manually but that didn't work either. I've scoured the internets but I haven't been able to find a solution to this problem which is why I'm finally posting here :) The problem being that VS is breaking on the Exception? You you can always have VS ignore a specific family of Exceptions. In VS in the Debug Menu choose ""Exceptions..."" and in the dialog presented you can control this.  I was going to suggest not catching an exception so before I suggested that I test it myself and if you set it to throw all exceptions even if you don't throw an exception its all exceptions are still being thrown. I will have to agree with Shiv Kumar either adjust your settings while you debug your application or accept the limitations of what you are doing. The reason bool.TryParse works is it verifys each and every character similarly to how Int32.TryParse makes sure that each character in the string is 0-9 or any other valid numerical symbol. You could of course write your own networking class and not throw an exception when the connection fails. TryParse will not throw an exception you must catch any exception that is thrown if you use bool.Parse by try{}catch{} otherwise if you attempt to parse something that is not a boolean value it will throw an unhandled exception. TryParse was added later in the history of .NET Parse is more of the classic method allowing the programmer to handle all unexpected input and to validate input before trying to parse the data. I should add that TryParse will return false if its unable to parse the value at both the method's result is false and the out variable I do believe is false.This is at least the case with Int32 http://msdn.microsoft.com/en-us/library/system.boolean.tryparse.aspx I guess the point of pointing out how TryParse and Parse works is that they are entirely different beasts compare to TcpClient. I suppose I should clarify that the basic validation process is similar except one throws an exception and the other one doesn't and of course one returns what was actually parsed. (A bit off topic): I assumed that the internal workings of bool.TryParse and Parse were the same (i.e. they check the string in the same way) except that the latter one throws an exception if there's something wrong with the input. Are you saying that's not the case? @Srekel See my revised answer."
794,A,gethostbyname replacement for IPv6 addresses I have a program that uses gethostbyname (in Windows) in order to convert IP address to hostname. But it works only for IPv4... What is the correct replacement for IPv6? Thanks. Looking up gethostbyname in MSDN tells us that it's deprecated and we should look at getaddrinfo which has all kinds of options for dealing with other addressing families. Or if you're doing address to name translation you'll end up at getnameinfo Thank you. It works. Developers are encouraged to use the `GetAddrInfoW` Unicode function (http://msdn.microsoft.com/en-us/library/ms738519(v=vs.85).aspx) rather than the `getaddrinfo` ANSI function.  Use getaddrinfo which deprecates the old gethostbyname function. You **should** even use it for IPv4 addresses as well. You can produce protocol-independant code (same code that works for both IPv4 and IPv6) very easily. +1 good point @ereOn. Thank you. It works.
795,A,WebSocket Client in Lua Is there a WebSocket client (not a server) written for Lua. It can either be a C module or in pure Lua. Yesthere is https://github.com/lipp/lua-websockets. It does not add any C bindings but relies on luasocket lpack and depending on your usage scenario (lua-ev).  I gave up and switched to regular sockets on the server side. It was much easier.  Yep. http://w3.impa.br/~diego/software/luasocket/ Ok. That does raw sockets. I was hoping to avoid that. Is there no prebuilt api for WebSockets? None that I have found but I did find this: http://www.wellho.net/resources/ex.php4?item=u116/webclient  I could only find this pastebin: http://pastebin.com/f50d79dcd that doesn't seem to be a client it seems to be a server.
796,A,How to gain greater control of network packets on Android I'm looking to design an application that will require some deep control over IP packets. Looking over the reference guide on the developers site at Android I see very limited control over packets from java.net:SocketOptions and java.net:DatagramPacket. Specifically I'm looking to control the individual bits within the packet to set TCP Flags SYN/ACK/RST and so forth. Based on the docs I am assuming I cannot do this within the Java API provided by Android and I'm guessing I'll have to do it some other way? Anyone have any insight on this? I don't think this is possible. Maybe you can do this using the NDK but I have no experience using it so I can't even tell you whether the NDK supplies us with (Berkeley) sockets. Another question here is if this is possible without rooting? Usually raw packets requires root privs on most operating systems. So it appears the SDK cannot do as I had needed. The NDK may be able to but on further thought I believe even using that will require root privs to construct raw packets. As I have no intent to root my phone at this time this will all be put on hold.
797,A,"representation of 6 byte MACID I am working on Client Server application where I need to send 6 byte mac id (FFF000 - FFF1FF) to the server from the client. I wanted to know the how best to represent in c++. I thought of declaring it as unsigned long and then covert it in 6 bytes and send it to the server. Is this correct do this. Any Other alternative please suggest. Why would it have to be ""converted to hexadecimal"" to send to the server? Hexadecimal decimal octal base-64 etc. are just different ""views"" (representations with different symbols) of the same data. Sorry it should 6 bytes instead of hexadecimal Just use a 64-bit variable (an unsigned long is not necessarily it -- perhaps ""long long"" or ""int64_t""? See [size of int long etc](http://stackoverflow.com/questions/589575/c-size-of-int-long-etc)) and only send/use the low-6 bytes? I don't think I'd use an integral type for this since we would never treat the value as a number. I would represent it as a sequence of 6 bytes. Some possibilities are: unsigned char macid[6]; typedef unsigned char macid[6]; struct macid { unsigned char data[6]; }; But in the end I'd probably opt for: std::tr1::array<unsigned char 6> macid; send(serverFd &macid[0] macid.size()); `std::tr1::array<>` is cool for a short-term solution (or even internal implementation for `macid` class). If however there are named operations to perform on `macid` values I would definitely opt for the class solution."
798,A,"Multiple async webservice requests NSURLConnection iOS I have a problem that Im not sure how to tackle. I can without any problem make requests to the REST service when passing a single request. My problem now is at based on that respons i get some values including ID´s. All ID´s retrieved will then need make another request to collect new information. My guess is to load all the specific request in a array or dictionary and create request from that. Anyone have some useful tips regarding this? The information retrieved will then populate a UITableView. Is your requirement is something like this. req--->response(use it for another req and also store some info)--> response(again make new request and store/use some info)---> response.. and so on? Yes but the problem is that the first request retrieves everything I need in one request. For example I get back 20 people each with an ID. I want to take each ID and make 20 request to retrieve a schedule for each. The tricky is that the first response populates a UITableView with names. after each name I want to put their individual schedule. I dont know this made my problem more clear. I suggest you use a Sync-Async Pattern on this problem. You need to implement two synchronous methods: // Fetch the array of ID's -(NSArray*)fetchItemIDsWithError:(NSError**)error; // Fetch the item for a ID -(Item*)fetchItemForID:(NSString*)itemID error:(NSError**)error; Implementing these using synchronous code is easy and testable. You can use simple methods like dataWithURL… stringWithContentsOfURL… sendSynchronousRequest… or ASIHTTPrequest with ease and write straight forward unit tests for this. The code will also be extremely easy to maintain and extend compare to how concurrent code usually ends up. Now to step two create an asynchronous wrapper I would use a delegate and a method signature like this: @protocol FetchItemsDelegate <NSObject> -(void)didFetchItems:(NSArray*)array; -(void)failedFetchItemsWithError:(NSError*)error; @end -(void)fetchItemsWithAsyncDelegate:(id<FetchItemsDelegate>)delegate; You already have all the code that do what you need so all you have to do is impelemnt the asynchronious parts. This code will be well sepearated and straight forward. Probaly not more than this: -(void)fetchItemsWithAsyncDelegate:(id<FetchItemsDelegate>)delegate; { [self performSelectorInBackground:@selector(backgroundFetchItemsWithDelegate:) withObject:delegate]; } -(void)backgroundFetchItemsWithDelegate:(id<FetchItemsDelegate>)delegate; { NSAutoreleasePool* pool = [[NSAutoreleasePool alloc] init]; BOOL success = YES; NSMutableArray* items = [NSMutableArray array]; NSError* error = nil; NSArray* itemIDs = [self fetchItemIDsWithError:&error]; if (itemIDs) { for (NSString* itemID in itemIDs) { Item* item = [self fetchItemForID:itemID error:&error]; if (item) { [items addObject:item]; } else { success = NO; break; } } } else { success = NO; } if (success) { [delegate performSelectorOnMainThread:@selector(didFetchItems:) withObject:[NSArray arraiWithArray:items] waitUntilDone:NO]; } else { [delegate performSelectorOnMainThread:@selector(failedFetchItemsWithError) withObject:error waitUntilDone:NO]; } [pool release]; } I have written a longer blog post on this topic here: http://blog.jayway.com/2011/04/28/sync-asyn-pair-pattern-easy-concurrency-on-ios/ @PeyloW Thanks for your answer! I will give it a try. Im pretty new to Objective-C so if I now have a seperate class that retrieves the first reponse to populate the array of ID´s do I create a new class for these methods? The other class does a callback to my viewController and then populates the TableView. @Silversnail - I am a proponent of intelligent model objects. So if it was up to me I would add the methods as class methods on the `Item` class itself. Maybe in a category for separation in same header file but optionally in separate implementation files if the file grows to 500 lines of code or more. @PeyloW I think that I perhaps are little bit to unexperienced to get how I will get all this together I will take a look at your blog to gain some more understanding. This is how i have my async connection now... stackoverflow.com/questions/5975825/debugger-message if you are using NSURLConnection for the requests there's no need for the ""performSelector"" stuff as NSURLConnection is already asynchronous. @user68… - Yes it is but the resulting code will have allot more cruft and be ten times as hard to test/debug/extend/maintain. The Sync-Async-Pair pattern gives cleaner code. @PeyloW if you look at my link can you give a little more guidance towards how to fit into my structure? @PeyloW can you please elaborate a litte with on how to set this up do i put the above async part in the same protocol class? And in the same class as the sync methods? @PeyloW or do I put the sync methods in my viewController? Or in the same async class that i get the respons for the id´s to use? Thanks @Silversnail - Where and how you you implement a Sync-Asyn Pair is up to your application. A good rule of thumb that I have found works well is to put IO in the model layer. So not in your view classes and not in your view controller classes. Put the implementation is the model domain and only let your view controller class conform to the delegate needed to respond to the asynchronous results from the model. It is hard to give a more concrete example without knowledge of your actual application. If you did an RSS app then the RSSItem class would probably be the right place. I have the ids I need in my viewController..How do I pass this or loop it make a request for each id. I think that I need some more help in this issue stackoverflow.com/questions/5975825/debugger-message  Its solution depends on your existing implementation.(I am considering here only two cases.) Case I : If you are having a different class for managing connection related task a separate class that has NSURLConnection Delegate methods.(Asynchronous..) You will need to run a for loop for requesting all the 20 ID request with registering notification for them with ID as the name of notification. When connection will finish its loading then it will post a notification to your observing class and here using notification's name you can update the respective ID's related schedule. [here you will need to created different objects of that connection handler class]... here you will need to wait for schedule loading what you can do you can put an activity indicator in cell's accessary view till the schedule is loaded.(you can show names in this case) Case II : If its singleton or in the same class which you are using.(we can not create it's multiple objects)..(it will have the performance cost.) one by one request. You will need to send request one by one and update the cell's content until its done. It means you send request for ID 1 and when its response come's you can update cell and send next request. and so on. It will be helpful if you post us how you are handling connection activity. Thanks this is the structure of how i do the connections. http://stackoverflow.com/questions/5975825/debugger-message so in your cell do you have some view for your schedule display UILabel or something like that? Yes exactly! UIlabel you can refer http://stackoverflow.com/questions/5985477/how-to-handle-tiling-of-images-on-the-fly/5986993#5986993 link what you have to do is just replace the subclassing from UIImageView to UILabel and few other things If you still have some problem then please say. What we need to do is create our own custom UILabel for schedule which will handle its own loading request. this will also helpful (as we can call its startLoading method in cellForRowAtIndexPath) so only request for visible cells would be sent. Also we need few checks that we do make the request. Ok I have a look at that thanks a lot for helping me out!  your question is a bit vague but I think I may understand your problem. I usually implement a delegate pattern with a protocol when doing http requests: @protocol HttpDelegate -(void) httpDidFinish; -(void) httpError:(NSError *) error; @end and in an HttpClient class: -(void) connectionDidFinishLoading:(NSURLConnection *)connection { [self.delegate httpDidFinish]; } You controller (or another class) implements HttpDelegate and in httpDidFinish either make the second request or populate your table. In this case since this is a two step process instead of implementing HttpDelegate in a controller I'd probably add another class TwoStepProcessor and TwoStepProcessorDelegate. TwoStepProcessorDelegate is just like HttpDelegate except it has: -(void) secondStepFinished: that your controller implements."
799,A,"Identify all interfaces of computer by giving host doing little network projects for identification interfaces of computer. Id like to just use Ping DNS TCP to achieve it. Eg. I give google.pl as hostname I obtain ip. string host = ""google.pl""; IPAddress[] address = Dns.GetHostAddresses(host); Ok iv got IP. Now how can I check witch interface it use to communicate? Connect with TCP fails? Connect with UDP fails? .. etc. :)? This by no means is an easy problem to solve reliably so outsource the functionality to another program. The easiest way to accomplish this is to write an adaptor to use the NMAP shell program and then parse out the output.  If I understand what you want to do correctly then it's impossible. There are thousands of different protocols the computer could be using to communicate many of them proprietary. And even if you somehow managed to detect all of the protocols used today new ones are going to come into existence."
800,A,Caching Images? I am developing a Food Recommendation Engine. We have a lot of photos which has to be downloaded from the server during the usage of the Application. Do we need to cache or store the images locally just to reduce the network activity ? If the data from the server is not going to change then you should store them but if the data is changed then you can do two things On the server keep a last updated data date and depending on that always fetch that date from server check if the last updated data is not obsolete if yes then fetch again or there is no need Second approach is Store as cache and fetch every time you start your app lets say on an activity i have 15 images.. but when the second time the Activity is loaded - 5 new images have changed. What do u do in this situation ? so we have 10 images from the previous list and 5 that have to be newly downloaded. You mean to say not all the images only few of the images are changed if thats the case its totally depend upon the situation if you have 100 of images and only 10 are changed then create a web service that will also tell you about the updated images in conjunction with date and you can write simple AI(logic) on what situation what best suits you ok. What are the factors i need to consider when doing a local Cache dump on a mobile ? Can i do it on the SD card.. and upto what MB is it ok to utilize so that the app doesn't eat out all the memory :P Its general for cache its better to use /data/data/packagename/ but if the data is small otherwise in general for heavy data always use sdcard and delete the folder when you exit your app Oh you suggest to delete the data on exiting the app ? is it bad to have say 1-2mb of cache until the app is uninstalled ? like how the nokia maps does on the Nokia Phones ? I suggested that in caching and reason for that is look in days of nokia there are not so much apps out there but in case of Android there are tons of app in market and if every app store data on sdcard like this user must be frowned upon its sdcard usage and the amounts of folder created on sdcard if the data is really small as in your case 1 or 2MB then you can hide the folder by appending a dot in name folder that will make it hidden also during uninstall you were not we able to remove that folder Do u knw any resource where i can learn or see any such caching implementation ? Thanks for your time. Really helping me make the decision :) Nope right now its a big idea depending on your situation and I dont have any link right know but you can always start from achieving few things at intial and later concentarting on whole of the concept of caching and feel free to ask questions Why not give user a choice to decide if he/she wants to use image caching and if yes - then how much space to spare for it? Use sensible defaults in case user does not care about it. Thanks guys for the answer.
801,A,"Measuring upload/download rates with libpcap I'm using libpcap (and winpcap on Windows) in a C application to monitor network traffic. I need to differentiate between upload and download traffic on each network adapter to produce connection speed stats but the filter expressions used by the library don't seem to support this very easily (ie there are no 'incoming'/'outgoing' operators). One approach that I have considered is to query the IP address of each adapter and then use filters such as src host 1.2.3.4 (to measure uploads) and dst host 1.2.3.4 (to measure downloads). My questions are: Is there a better/simpler approach than the one above (something that would let me use the same filter expression for each adapter would be nice)? If the above approach is the way to go then is there any chance that a single adapter could have more than 1 IP address associated with it? The reason I ask is that the pcap_addr struct which holds the address details of a single adapter (in struct pcap_if) has a 'next' member suggesting that this is possible. Have you considered looking at pmacct - I have personally contributed to this in time past. This is a C tool that uses libpcap to passively monitor network traffic for accounting purposes. +1 I like the look of that library.  Try tcpdump This is really a comment not an answer to the question. Please use ""add comment"" to leave feedback for the author. While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.  Firstly remember pcap sees only packets. It doesn't see ""outgoing"" or ""incoming"" - simply packets. So yes you must filter using the src/dst in the ip headers. There is no other way to tell whether the packet is incoming or outgoing. Secondly yes there is nothing stopping an adapter having multiple IP addresses. So you need to grab the IP addresses configured from that adapter. pcap_findalldevs() (WinPCap Documentation) ought to help you here from which you should be able to deduce which devices you want to monitor."
802,A,multicast ip address - blocked in call to recvfrom i am writing a simple multicast application. i intend to run it on localhost. i have done the following: char *maddr; . . . sendfd = socket(...); struct sockaddr_in sasend; sasend.sin_family = AF_INET; sasend.sin_port = htonl(portno); inet_ntop(maddr &(sasend.sin_addr.s_addr)); struct sockaddr_in sarecv; memcpy(&sarecv &sasend); recvfd = socket(...); const int on = 1; setsockopt(recvfd SOL_SOCKET SO_REUSEADDR &on sizeof(on)); // can you explain why // this is needed bind(recvfd &sarecv); struct ip_mreq mreq; memcpy(&mreq.imr_multiaddr &(sasend.sin_addr)); mreq.imr_interface = htonl(INADDR_ANY); setsockopt(recvfd IPPROTO_IP IP_ADD_MEMBERSHIP &mreq sizeof(mreq)); char flag = 1; setsockopt(sendfd IPPROTO_IP IP_MULTICAST_LOOP &flag 1); if (fork() == 0) { while (recvfrom(recvfd)) { } } else { while (sendto(sendfd)) { sleep(3); } } in the actual code i am checking the return values of all system calls. the problem is that recvfrom does not return. the process stays blocked in the call to recvfrom. i have tried running two instances of the program on different shells. it does not help. also i tried setting loopback flag to 0 it does not help. i want to run both the programs from localhost. the multicast address i am using is 239.255.1.2 which i have seen from the book. i think we can use any class D address as we are making the required setsockopt call. connecting on port 1025 running linux kernel 2.6.25 also how do i check if multicasting support has been compiled into the kernel. Update: i did route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0 on the shell. still the problem exists. is the question too descriptive? ok i disabled the firewall and i could get the program running. got help from here now more problems: how to add rules to the firewall to specifically my code to run if i do not disable loopback then i do not get the messages from another process sending on the same multicast group and if i enable it then my process receives the messages it transmits. this is because i am running both the programs on localhost. is there a way around.
803,A,"Problem with a blocking network task I'm new in Java so please forgive any obscene errors that I may make :) I'm developing a program in Java that among other things it should also handle clients that will connect to a server. The server has 3 threads running and I have created them in the following way : DaemonForUI du; DaemonForPort da; DaemonForCheck dc; da = new DaemonForPort(3); dc = new DaemonForCheck(5); du = new DaemonForUI(7); Thread t_port = new Thread(da); Thread t_check = new Thread(dc); Thread t_ui = new Thread(du); t_port.setName(""v1.9--PORTd""); t_check.setName(""v1.9-CHECKd""); t_ui.setName(""v1.9----UId""); t_port.start(); t_check.start(); t_ui.start(); Each thread handles a different aspect of the complete program. The thread t_ui is responsible to accept asynchronous incoming connections from clients process the sent data and send other data back to the client. When I remove all the commands from the previous piece of code that has to with the t_ui thread everything runs ok which in my case means that the other threads are printing their debug messages. If I set the t_ui thread to run too then the whole program blocks at the ""accept"" of the t_ui thread. After reading at online manuals I saw that the accepted connections should be non-blocking therefore use something like that : public ServerSocketChannel ssc = null; ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(port)); ssc.configureBlocking(false); SocketChannel sc = ssc.accept(); if (sc == null) { ; } else { System.out.println(""The server and client are connected!""); System.out.println(""Incoming connection from: "" + sc.socket().getRemoteSocketAddress()); in = new DataInputStream(new BufferedInputStream(sc.socket().getInputStream())); out = new DataOutputStream(new BufferedOutputStream(sc.socket().getOutputStream())); //other magic things take place after that point... The thread for t_ui is created as follows : class DaemonForUI implements Runnable{ private int cnt; private int rr; public ListenerForUI serverListener; public DaemonForUI(int rr){ cnt = 0; this.rr = rr; serverListener = new ListenerForUI(); } public static String getCurrentTime() { final String DATE_FORMAT_NOW = ""yyyy-MM-dd HH:mm:ss""; Calendar cal = Calendar.getInstance(); SimpleDateFormat sdf = new SimpleDateFormat(DATE_FORMAT_NOW); return (sdf.format(cal.getTime())); } public void run() { while(true) { System.out.println(Thread.currentThread().getName() + ""\t ("" + cnt + "")\t (every "" + rr + "" sec) @ "" + getCurrentTime()); try{ Thread.sleep(rr * 1000); cnt++; } catch (InterruptedException e){ e.printStackTrace(); } } } } Obviously I'm doing something wrong at the creation of the socket or at the use of the thread. Do you know what is causing the problem? Every help would be greatly appreciated. Can you put some sort of debug statement after ssc.accept(); and tell us if it gets hit? Looking at the code it doesn't look like accept should block... please verify that it is actually blocking on accept and not some other place in your code. Don't use non-blocking I/O until you know you need it. Just start a new thread for every accepted socket as well as for the accepting threads.  Problem solved :) I looked at your suggestions and had a closer look at the code. It was a design error since I had a function that created a while(true) loop inside the constructor of DaemonForUI (and more specifically inside ListenerForUI()). It was causing the whole program to cycle through the while statement therefore stalling every other action. Silly mistake I must admit... :( Thanks for all the help everyone that answered my question. I will consider the mentioned idea of creating a new thread for every incoming connection. The duty that has to be performed for every incoming connection is not so heavy so I thought that one single thread could do the job."
804,A,"1 Linux PC with two network cards cannot produce network traffic for test system These questions relate to the basics of networking but I have not found an answer yet! Question 1: Can a 100Mbps switch handle 2 simultaneous TCP streams running at 100Mbps? To illustrate this if you have 4 PC's on 1 switch (100Mbps with no other connections) and you had two separate streams what speed would they reach? Would one stream impact on the others speed or is a switch fast enough internally to deal with many streams? Question 2: I have set up 1 PC (Linux) on a dedicated network switch the PC has 2 Network Interface Cards (NIC). Both NIC's have different IP address' and work fine. If I send any data from one NIC to the other absolutely no traffic is send on the network switch. It seams that the kernel is intelligent and works out there's no need to send anything on the network as the IP's are on the same PC. How do I turn this off? As I don't have access to 4 PC's I cannot test question 1 in the real world. These are very simple questions but at the same time very difficult to answer. Thanks! Yes - I'm actually using D-ITG traffic generator and binding server and client to a different network interface (has to be done using root user). Each network interface has a different IP address. Any ideas? Are you binding the test sockets to the IP address of one specific network card? The answer to question 1 depends upon the backplane bandwidth of the switch - this is the total rate of traffic it can handle internally. If it has at least 200Mbps of backplane bandwidth it can sustain two independent 100Mbps streams. Cheaper switches tend to have less backplane bandwidth than more expensive ""big iron"" switches. As for question 2 you could try and add manual routes to your own IP addresses explicitly specifying the external devices (if you check route -C you'll see that the routing cache has routes via the loopback device for your own IP addresses). That's a brilliant response :) I wish there was a way to find out a switch's backplane bandwidth. I have just tried 'route' but it doesn't like to tie the IP to the interface (invalid argument). Any ideas? @ross: I suspect you'll need to write a program using raw sockets as Jonathan suggests.  Question 1: Yes any 100Mb/s switch should be able to handle two independent 100Mb/s streams as each port is 100Mb/s and not the switch itself. Traffic does not hit the CPU unless it absolutely needs to at which point the switch may bottleneck and slow down. Normal traffic should be fine though. Question 2: You need to write a program to do it manually or find one online. I recommend looking at Libnet. The problem is that the kernel knows that it owns both of those cards so it doesn't bother actually sending the data out on the network. Using Libnet you can manually construct packets and send them out of a specific interface. The main trick though is that you need to send traffic in both directions (not much; you shouldn't need more than 1 packet a minute going in one direction with everything else going in the other direction). The problem is that the switch will not know where the packets are supposed to go and will flood them to all ports slowing everything down. I would hope that a switch could handle as much data as it has ports but I don't think its that simple. I think 'backplane bandwidth' is along the lines of what I need to find out. Although for cheaper switches this figure is not available (great). I have tried the network traffic generator which binds to an interface. It's called D-ITG http://www.grid.unina.it/software/ITG/ but the traffic still doesn't reach the network switch. Technically it's the backplane bandwidth that you need to worry about but if you're only talking about two streams then there should not be any problems. I'm not familiar with any traffic generators out there; last time I needed to do this I wrote my own using Libnet. I created two threads: one with a libnet instance bound to one interface and one with a raw socket bound to the other interface. The sender generated raw Ethernet packets with a custom protocol ID and a few header fields filling the rest of the space with random data. The receiver listened for packets decoded the headers to see what was received marked the packet as received and discarded it."
805,A,serialize any data type as vector - use reinterpret_cast? I didnt find anything directly related in searching so please forgive if this is a duplicate. What I am looking to do is serialize data across a network connection. My approach is to convert everything I need to transfer to a std::vector< uint8_t > and on the receiving side unpack the data into the appropriate variables. My approach looks like this: template <typename T> inline void pack (std::vector< uint8_t >& dst T& data) { uint8_t * src = static_cast < uint8_t* >(static_cast < void * >(&data)); dst.insert (dst.end () src src + sizeof (T)); } template <typename T> inline void unpack (vector <uint8_t >& src int index T& data) { copy (&src[index] &src[index + sizeof (T)] &data); } Which I'm using like vector< uint8_t > buffer; uint32_t foo = 103 bar = 443; pack (buff foo); pack (buff bar); // And on the receive side uint32_t a = 0 b = 0; size_t offset = 0; unpack (buffer offset a); offset += sizeof (a); unpack (buffer offset b); My concern is the uint8_t * src = static_cast < uint8_t* >(static_cast < void * >(&data)); line (which I understand to do the same as reinterpret_cast). Is there a better way to accomplish this without the double cast? My naive approach was to just use static_cast< uint8_t* >(&data) which failed. I've been told in the past that reinterpret_cast is bad. So I'd like to avoid it (or the construct I have currently) if possible. Of course there is always uint8_t * src = (uint8_t *)(&data). Suggestions? You can get rid of one cast by exploiting the fact that any pointer can be implicitly cast to void*. Also you might want to add a few const: //Beware brain-compiled code ahead! template <typename T> inline void encode (std::vector< uint8_t >& dst const T& data) { const void* pdata = &data; uint8_t* src = static_cast<uint8_t*>(pdata); dst.insert(dst.end() src src + sizeof(T)); } You might want to add a compile-time check for T being a POD no struct and no pointer. However interpreting some object's memory at the byte-level is never going to be save period. If you have to do it then do it in a nice wrapper (as you have done) and get over it. When you port to a different platform/compiler have an eye on these things. I have the `const` in there but elided for brevity. I do not however have the check for pointer and/or struct. This is used only by myself but it would probably be safest to add those checks to be sure. Thanks.  My suggestion is to ignore all the people telling you that reinterpret_cast is bad. They tell you it is bad because it's generally not a good practice to take the memory map of one type and pretend that it's another type. But in this case that is exactly what you want to do as your entire purpose is to transmit the memory map as a series of bytes. It is far better than using a double-static_cast as it fully details the fact that you are taking one type and purposefully pretending that it is something else. This situation is exactly what reinterpret_cast is for and dodging using it with a void pointer intermediary is simply obscuring your meaning with no benefit. Also I'm sure that you're aware of this but watch for pointers in T.  You're not doing any actual encoding here you're just copying the raw representation of the data from memory into a byte array and then sending that out over the network. That's not going to work. Here's a quick example as to why: struct A { int a; }; struct B { A* p_a; } What happens when you use your method to send a B out over the network? The recipient receives p_a the address of some A object on your machine but that object is not on their machine. And even if you sent them the A object too it wouldn't be at the same address. There's no way that can work if you just send the raw B struct. And that's not even considering more subtle issues like endianness and floating point representation which can affect the transmission of such simple types as int and double. What you are doing right now is fundamentally no different than just casting to uint8_t* as far as whether it's going to work or not is concerned (it won't work except for the most trivial cases). What you need to do is devise a method of serialization. Serialization means any way of solving this sort of problem: how to get objects in memory out onto the network in a form such that they can be meaningfully reconstructed on the other side. This is a tricky problem but it is a well-known and repeatedly solved problem. Here's a good starting point for reading: http://www.parashift.com/c++-faq-lite/serialization.html So yes misnomer. Regarding the rest of your comment: the question as posed is a simplification to inquire about whether or not to `reinterpret_cast` (or similar) - I'll rename to be more specfic. I'm aware of the subtleties in transferring data and internally everything has a pack/unpack which essentially does what I describe above for its own data.
806,A,How to find whether the MORE FRAGMENTS field is set or no? Given this header  how do I find if the MORE FRAGMENTS field is set or not.. struct sniff_ip { u_char ip_vhl; /* version << 4 | header length >> 2 */ u_char ip_tos; /* type of service */ u_short ip_len; /* total length */ u_short ip_id; /* identification */ u_short ip_off; /* fragment offset field */ #define IP_RF 0x8000 /* reserved fragment flag */ #define IP_DF 0x4000 /* dont fragment flag */ #define IP_MF 0x2000 /* more fragments flag */ #define IP_OFFMASK 0x1fff /* mask for fragmenting bits */ u_char ip_ttl; /* time to live */ u_char ip_p; /* protocol */ u_short ip_sum; /* checksum */ struct in_addr ip_srcip_dst; /* source and dest address */ }; struct sniff_ip ip_hdr; ... if(ip_hdr.ip_off & IP_MF) { //more fragments bit is set. } ...and if the fields in `ip_hdr` are in the order they're in on the network (for example if this is a packet captured from the network or read from a capture file using libpcap/WinPcap and not modified) remember to convert it from network byte order to host byte order with `ntohs()` first.
807,A,"why gethostbyaddr doesn't work for www.google.com /* As usual make the appropriate includes and declare the variables. */ #include <netinet/in.h> #include <arpa/inet.h> #include <unistd.h> #include <netdb.h> #include <stdio.h> #include <stdlib.h> int main(int argc char *argv[]) { char *host **names **addrs; struct hostent *hostinfo; /* Set the host in question to the argument supplied with the getname call or default to the user's machine. */ if(argc == 1) { char myname[256]; gethostname(myname 255); host = myname; } else host = argv[1]; /* Make the call to gethostbyname and report an error if no information is found. */ hostinfo = gethostbyname(host); if(!hostinfo) { fprintf(stderr ""cannot get info for host: %s\n"" host); exit(1); } /* Display the hostname and any aliases it may have. */ printf(""results for host %s:\n"" host); printf(""Name: %s\n"" hostinfo -> h_name); printf(""Aliases:""); names = hostinfo -> h_aliases; while(*names) { printf("" %s"" *names); names++; } printf(""\n""); /* Warn and exit if the host in question isn't an IP host. */ if(hostinfo -> h_addrtype != AF_INET) { fprintf(stderr ""not an IP host!\n""); exit(1); } /* Otherwise display the IP address(es). */ addrs = hostinfo -> h_addr_list; while(*addrs) { char *ip_str = inet_ntoa(*(struct in_addr *)*addrs); printf("" %s"" ip_str); /* <BEGIN> Get the host name by IP address */ struct in_addr addr = {0}; if (!inet_aton(ip_str &addr)) { printf(""Cannot parse IP %s\n"" ip_str); exit(1); } struct hostent *remoteHost = gethostbyaddr((void*)&addr sizeof(addr) AF_INET); if (remoteHost == NULL) { printf(""\nInvalid remoteHost\n""); exit(1); } printf(""\nOfficial name: %s\n"" remoteHost->h_name); char **pAlias; for(pAlias=remoteHost->h_aliases; *pAlias != 0; pAlias++) { printf(""\tAlternate name: %s\n"" *pAlias); } /* <End> */ addrs++; } printf(""\n""); exit(0); } /* Test Run for www.yahoo */ user@ubuntu:~/Documents/C$ ./getname2way www.yahoo.com results for host www.yahoo.com: Name: any-fp.wa1.b.yahoo.com Aliases: www.yahoo.com fp.wg1.b.yahoo.com 209.191.122.70 Official name: ir1.fp.vip.mud.yahoo.com /* Test Run for www.google.com */ user@ubuntu:~/Documents/C$ ./getname2way www.google.com results for host www.google.com: Name: www.l.google.com Aliases: www.google.com 74.125.225.83 Invalid remoteHost Why the code doesn't work for www.google.com? The code for gethostbyaddr is borrowed from http://www.unix.com/man-page/FreeBSD/3/gethostbyaddr/ The IP address 74.125.225.83 does not have a reverse PTR record. You can always test on the command line with nslookup or dig. can't find 74.125.225.83: Non-existent domain http://en.wikipedia.org/wiki/Reverse_DNS_lookup"
808,A,Reading Network Traffic I want to intercept all network traffic coming and going from a specific application on a Windows XP. How do I get started doing this? The goal would be to reverse engineer the API used between the local software and the remote software and implement the same API in my own software package. I can program in anything Java is the easiest however. http://www.winpcap.org/windump/default.htm To get all network traffic you could a tool called WireShark for example Just use Wireshark no need of programming anything.
809,A,"How to tunnel TCP over reliable UDP? Assume I have a reliable UDP library and want to tunnel arbitrary TCP connections over it. This is my current approach to doing so but I feel that it may not be very efficient. Any suggestions are very welcome. Client establishes a reliable UDP connection to server. Client runs a local SOCKS5 proxy which receives data from any app that connects to it and forwards it through the reliable UDP connection. Each packet includes a 4-byte id unique to each SOCKS connection. Server receives data. If the 4-byte id is new it makes a new connection to its local TCP socket and sends the data and spawns a new thread which receives any replies from the server and forwards them through the reliable UDP connection with the appropriate id. If the 4-byte id is old it simply sends the data over the existing TCP connection. Client receives data sending it over the existing SOCKS connection to whatever app started it. Right now this seems to work for making simple HTML requests from a browser but since the server isn't directly connected to the client it is unable to tell when the client terminates a connection. Is there a better way to do this? EDIT: No this is not homework. And please don't bother replying if you aren't aware of the advantages of reliable UDP libraries or for that matter haven't heard of them before. Thanks. I can't imagine why you would want to do this - why not use TCP directly? And I doubt your UDP library actually does provide reliable UDP connections - such things are a contradiction in terms. TCP *is* the reliable UDP. What are your goals in reinventing it? 1. there is no such thing as ""reliable UDP"". 2. there is no such thing as ""UDP connection"". Is this homework? Guys these replies aren't helpful at all. If you haven't heard of reliable UDP libraries before please don't reply. I suspect he's trying to create broadcast-capable TCP. Because any other explanation really doesn't make sense. @all: why the hate for UDP? there's nothing you can do on IP that UDP can't do including running TCP over it. Is this a trick question? There is a profound misunderstanding: a Client protocol that runs on top of UDP (whatever the characteristics of such protocol might be) cannot be termed ""reliable UDP"". Please use proper ""layering principles"" to describe the system. @jldupont: That's your opinion. http://www.javvin.com/protocolRUDP.html (based on RFC 908 and 1151) @Roboto: one can spec anything he/she wishes but that doesn't mean it has commercial relevance. RUDP isn't a networking reality. @jldupont: So? There is a draft IETF protocol called RUDP.. while experimental it exists so get off your high horse and help this guy out or have semantical arguments elsewhere @Roboto: you have a good point. Thanks for it. I'll be more constructive from this point on. Thanks to all the down-votes even on my revised contribution I am leaving this discussion. good idea.. you have effectively burned your bridges on this one jldupont It is unfortunate to see that folks can't seem to accept when somebody admits to his own fault repents and then contribute more constructively. I fear that some folks of this community might cause its down-fall. I just hope I am wrong. @jldupont: The answer you deleted wasn't an answer.. it was a comment. That's why it was getting downvoted into to oblivion. Don't take it personal. @Roboto: that's because some folks are very quick with the down-vote button... since there isn't a ""save as draft"" on SO I submit and continue editing... I type fast but it seems not fast enough. Anyhow see you around... hopefully in a better context. All the best. there are a few ready to use options: OpenVPN: tunnels either IP or ethernet Frames on top of UDP Teredo: tunnels IPv6 on top of UDPv4 manages both NAT traversing and full compatibility with IPv6 UDT: non-standard reliable high-performance multi-transport TCP-like protocol on top of UDP. Optionally lets you manage NAT traversal and then takes it from there Thanks but I am already aware of libraries related to what I'm doing. I'm specifically attempting to implement tunneling *using* a reliable UDP library.  You're going to have to communicate the loss of the client TCP connection to the server side across your UDP tunnel (and the opposite if the server should happen to close the connection first). Otherwise quite apart from the fact that the HTTP server doesn't know the client has disconnected you will be leaking connections on the side that didn't initiate the connection closure. One way to do this would be to reserve a special value of your 32 bit connection ID field - say 0x00000000 or 0xffffffff - as representing a control packet rather than connection data. Following that is another 4-byte field representing a connection ID and following that is an opcode field. The first opcode you could define is ""Connection terminated"". If your client-side of the tunnel detects that the client application has closed its TCP connection it sends a Connection-Terminated packet for the corresponding connection ID over the tunnel; If your client-side of the tunnel gets a ""Connection terminated"" opcode from the server side then it closes its connection with the client application; and similar for the server-side of the tunnel.  The most efficient way is when the two endpoints directly communicate to each other. If they communicate with different protocols you need at least one proxy / gateway / traffic converter / whatever. In this case there is no way around two of these converters as you now have 3 parts involved: End point client network traffic endpoint server. I don't see how you could make it more efficient under the given circumstances. As for the terminated connections if you use a tunnel then use it for all traffic i.e. communicate all kinds of requests of both client and server to the other side. If a termination can't be communicated to the server then the problem is sitting on the client side -- the client end point does not communicate its termination to the client tunnel entry. If it would then you can transfer this termination to the server. I'm not quite sure how to transfer infomation beyond simply the data contained within the packets. I use a simple process in which I recv() data with one protocol then send() it using the other protocol. Forwarding control information such as ACK packets aren't necessary because the UDP library I'm using provides that already. @oskar: if you are already maintaining a map of connections can't you send a 'connection abort' to clients that are indirectly connected? I was actually thinking about doing just that; essentially writing my own ""abort"" message. The only reason I haven't is because it seems inelegant to invent my own protocol but perhaps that's the best option. @oskar: I think it will work if you don't build in anymore ACK/NAK.. then you are essentially recreating TCP. I think the problem is that you have a half-baked mixed approach between a tunnel and a client-server communication (as for a database). It is not really a tunnel because it filters obviously important messages and it is not really a client-server approach because it is too strictly connected to the transport medium. Maybe you should decide for one side and stick to it?"
810,A,network routing paths for different tcp channels but for same machines Process1 runs in machineA. Process1 has opened server portX. Process2 runs in machineB. Process2 has opened server portY. Process1 & Process2 exchange messages over 2 different tcp channels. Process1 opens client socket to portY of machineB and start sending msgs to Process2. (tcp channel 1) Process2 opens client socket to portX of machineA and start sending msgs to Process1. (tcp channel 2) Question: Is the network path the same for channel1 & channel2 in all cases? I.e. go through same routers etc? UPDATE: I need this information to make some decision concerning synchronization of processes (so that 1 process is not faster than the other Thanks There is no guarantee about it. Is this related to your other question about interleaving? And in the real world asymmetric paths are sometimes seen. Yes.It seems that I could not express the other question in a manner that was clear to others  Think about this: If the path between two system is always the same ... why should someone cared about designing routing protocols. why would you expect that the routing algorithm of the routing protocol in the specific example I am asking would return different routes for each traversal side? @user384706 Because that's up to the routing algorithm to assign a path to each session. You may decide (just as an example) to assign your next hop in a round robin fashion selected from the list of possible routers (with a route to dest available). Or you may change next hop priorities depending on path load. That's the point on designing a layered protocol: upper layers don't need to bother on low level details. The cost to pay is nevertheles great: you can't design a real time protocol on these premises.
811,A,"How to speedup UDP communications in Windows XP applications I am doing some maintenance on software and have a problem that I do not understand. Application was developed using Microsoft Visual C++ 6 and runs on Windows XP. It consists of 21 applications that communicate to each other via UDP sockets. It is a simulation of an embedded avionics system used to debug the system in a PC environment. Each of the applications simulates a node in the embedded system and the embedded networked is simulated over UDP. The system originally ran on multiple PCs but can now runs on a single Quad core machine. The system is working but the communication is annoyingly slow. However opening up Internet Explorer and visiting a web site or two set something that would cause my applications to suddenly communicate very fast to each other. So my question is what did Internet Explorer set when visiting a web site so that my application can also set it? None of the original authors of the system is still around and I have very little windows programming experience. it might not be a windows problem after all check your API check your buffer and check for errormessages 'getlasterror()' that doesn't cause INVALID (-1) and stops the program try to use it even if your program runs perfectit might have useful warnings check ACK or attack speed it's a common issue at transfer large amounts of data over network connections ( <-- 90% it's your problem ) here is a useful topic on that subject ( support.microsoft.com/kb/823764 ) if neither of those solutions work try checking your driver version against the manfucturer website. last resort is those useful ideas: . use this program www.lvllord.de to increase max half/open connections from 8 to 50 . using a server edition windows can boost some internet based programs . using multi-threading with sockets API can cause some confusing to the API if you're using more than 2 sockets at different threads in multithreaded application try optimize performance by using async sockets or something like that ( msdn.microsoft.com/en-us/library/ms738551(VS.85).aspx ) So my question is what did Internet Explorer set when visiting a web site so that my application can also set it? None of the original authors of the system is still around and I have very little windows programming experience. may be ACK check it on wiki .. it means in other ways the attack speed .. if not then it will be the receive window size both settings are invisible to users .. but can be set via programs such as tuneup utilities or any general network hidden setting adjustors it might just do the trick .. Your suggestions are only valid for TCP connections not UDP connections which have no ACK and no ""half-open connections"" receive window size is last hope ? :D  If the protocol above UDP implements reliability the speed loss will be due to massive UDP packet loss on localhost. UDP performance on localhost is terrible your best bet is to wrap the socket API inside a TCP layer. If it's UDP broadcast or multicast you will have to look at implementing a broker process to multiplex the messages over TCP. Might be easier to look at existing messaging APIs that work well at the interprocess level such as ZeroMQ. I have given up on the problem but I will definitely try out ZeroMQ.  Try using Wireshark to see what Internet Explorer is doing."
812,A,"How'd I determine where one packet ends and where another one starts While sending packets across a network how can one determine where one packet ends and where another starts? Is sending/receiving acknowledgment one of the ways of doing so? Can you elaborate on what kind of packets or sockets? E.g. with TCP sockets you just have a datastream. You don't see where the packet boundaries were. With UDP socket you receive one packet with recv(). Then SCTP protocol has SEQPACKET that is somewhat like a TCP stream but retains the packet boundaries. Its case of TCP sockets @jkj With TCP sockets you just see the datastream where you can receive and send bytes. You have no way of knowing where a packet ends and another begins. This is a feature (and a problem) of TCP. Most people just read data into a buffer until a linefeed (\n) is seen. Then process the data and wait for the next line. If transferring chunks of binary data one can first inform the receiver of how many bytes of data are coming. If packet boundaries are important you could use UDP but then the packet order might change or some packets might be lost on the way without you knowing. The newer SCTP protocol behaves much like TCP (lost packets are resend packet ordering is retained) but with SCTP sockets you can send packets so that receiver gets exactly the same packet.  I assume here that you mean application-level 'packets'. If you use UDP you don't need to since it's a message protocol. TCP is a byte streaming protocol so it cannot send packets just bytes. If you need to send anything more complex than a byte-stream across TCP you have to add another protocol on top - HTTP is one such protocol. Text is fairly easy since lines have terminating characters usually CR/LF/CRLF. Sending non-text messages will require a different protocol. One approach that is often used with TCP is to connect stream a protocol-unit disconnect. This works OK but slowly because of the huge latency of continually opening and closing TCP connections. HTTP usually works like this in order to serve up web pages to large numbers of users who if left permanently connected while they viewed pages would needlessly use up all the server sockets. Waiting for an application-level ACK from the peer is sometimes necessary if it absolutely essential that peer receipt is known before the next message is sent but again this is slow because of the connection latency. TCP was not designed with this approach in mind. If the commonly available IP protocols cannot directly provide what you need you will have to resort to implementing your own. What sort of 'packet' are you sending? Rgds Martin  TCP is a stream-based protocol. That is it provides a stream vs. packet or message-based interface to the application. If using TCP an application must implement its own method of determining packets or messages. For example (a) all message are a fixed size or (b) each message is prefixed with its subsequent size or (c) there is a special ""end-of-record"" sequence in the data stream to indicate a message boundary. Search google for lots of information on how one can implement message boundaries in TCP."
813,A,Network Programming Low Level or Class Abstraction? I see a lot of questions on the topic of network programming. Despite all the questions and answers I just do not know which way is best to start. Is it better to start from the lowest level or immediately to work in .NET C # without going into details below abstraction? Is it better to go with Winsock or BSD Socket programming in Linux? What is the purpose of your system? I just want to learn it is my goal. if you want to learn for the sake of been able to do staff on the network you don't neet to know the low level you can use c# to do simple staff. if you want to implement a protocol then go read the rfc of the protocol and try to implement simple parts of it. You can still do low-level TCP or UDP programming in C# so at that point it is really just a matter of choice whether you want to write network code in C C# etc... If all you are trying to do is learn how to write network code I would consider the language more of a personal choice as the underlying network concepts remain the same.  .NET C# You have: low level API to work with TCP UDP etc. .NET Remoting WCF (http tcp named pipes MSMQ) I would recommended last option but depends on what exaclty you are trying to learn: to build distributed apps or the nitty gritty details of low level socket API. I have an idea to create a program for monitoring the network something like that. You could do it in C# mostly. For very specific situations in case you need a method that is not in the .NET Framework you can use PInvoke to call it.  It all depends on your existing programming skills. I would not start with the lower levels such as the Socket class (or TcpClient/UdpClient) without having at least basic understanding of asynchronous programming. A lot of people who starts with socket programming launches a separate thread for the reading since the Read method blocks. It's a very noneffective way to solve the problem especially in servers. BeingRead/EndRead is the way to go. Next up is designing a transfer protocol since TCP doesn't guarantee that the complete message is delivered at one. It only guarantees that your messages arrive in the correct order. The next big thing with socket programming is how to handle incoming data. A newbie mistake is to start appending strings which would result in a lot of memory usage in server applications. Use byte[] buffers and a buffer pool (flyweight pattern) to manage incoming data (should be a easy task if you've made a well designed protocol). As you can see it' quite a big task to take on with no prior experience. WCF is a much better option since it handles most of that stuff for you.
814,A,"Blocking a website from access for all browsers Greetings I would like to ask if there's a way to block website(s) from access on a computer(s) dynamically? I mean could this functionality be coded (on java native interface)? Your response is highly appreciated. Thanks Cyril H. which OS ?????? what do you mean by blocking website from access on computer? do you want to implement firewall software or somehing similar to parential control? You need to word this question better. Do you want something that will run on the user's machine or on the web server or on a router between them? Do you want to prevent Java from accessing the web site or prevent any web browser or any program at all? @Jigar: Ahhmm... Sorry about the insufficient data. The OS would be Windows XP Professional 32-bit SP3. @gigadot: Implementing a parental control using Java is my objective. @Dan R.: I would need to run it on the user's machine. I would want to prevent a website accessing to all browsers(example... facebook.com) I wouldn't like that computer accessing it... Yes you can code a simple HTTP proxy service with Java: http://www.java2s.com/Code/Java/Network-Protocol/Asimpleproxyserver.htm Alternatively there are plenty of existing proxy solutions out there might suit your needs out of the box: http://www.roseindia.net/opensource/freeproxyservers.php You would then configure the software/devices that access websites (e.g. your browser) to point to that proxy so that all HTTP communication passed through it. Your proxy could then restrict access to whatever URL(s) you wanted to based on whatever logic you wanted to code up. If you wanted to get really fancy/secure and require folks to use the proxy (and not to choose to bypass it) you could do that but that's probably more than you need to given your question. @Kelly: I am building a computer management system for internet cafe. This was a big problem for the cafe about some customer visits unwanted website. Banning the domain on the switch/router wasn't a use. If I'm going to use the proxy case would it be flexible to all browsers on a particular computer? Are they alternative other than using proxy server? Yes all of the major browsers support the notion of proxy-based HTTP because management of HTTP by an ISP (or the company itself) is a much requested feature. +1 but this would also rely on not allowing the user to change their browser settings or install a new browser (and probably lots of other circumventions) to get around it. @Qwerky: I can't to that to meet the objectives. Is there a way? @Cyril: Well there's no other alternative than the concept of ""intercepting network communications"". Whether you do that at the HTTP layer or below is another story. The software on your network card could conceivably even allow such a setting but I've never seen one. I'm a bit confused though -- if you're interested in alternatives to what is really not a common case why do you want to code it (as your question suggests)? Let me add one other option that is still in the realm of intercepting network communications -- implementing your own DNS server. In this case just like the proxy you would configure your software/devices (e.g. your O/S) to use your DNS server as a source for mapping URLs to IP addresses. You could then simply return a bogus or local IP address for any domains you want to block. But this isn't as flexible as the proxy case. You could code the DNS server in Java though too."
815,A,"Sockets Programming This is more of a general quick question. But in like C#PythonCC++.......etc Are all the basic ""Sockets"" network programming essentially the same. Like do they all use the Berkley Sockets (i think thats what they are called) or does each language have it's own way of doing sockets. Thanks Now a days we don't differentiate the language the class library that can be accessed from the language and the underlying operating system. Here is my explanation CC++C#Java - Are just the languages has no specific support regarding the network programming. Java class library .Net Framework C++ standard library - among this I think C# & Java provides some classes for network programming. C++ standard library does not provide any network programming classes (only iostreams for file stdinput & strings are available). But BOOST library for C++ provides classes for network programming. I am not aware of the other libraries. OS - The operating system proives a base api (mostly in C) that is utilised by the class libraries above. In case of windows it is the winsock api (WSA) and in case of unix it is the BSD socket api. I think windows also supports to some extent the BSD api so that the learning curve is less. But as @EnabrenTane said it is more than that.  Sockets are basically the same in C C++ Java Ruby. They are slightly easier (because the build in classes handle the boiler plate) in higher level languages. If you can write Socket code in C then you can do it anywhere if you have a reference to translate. @Kellogs brings up a good point Windows has their own Socket API which (typically) performs better (in my experiences) on Windows than the Posix implementations offered. The APIs are very similar. I would make the analogy of OpenGL to DirectX. If you really know one then you can figure out the other with a reference.  Agree with kellogs above. Windows & most major POSIX-compliant OSs support the basic BSD socket API. In addition to the Berkeley socket each platform provides additional proprietary API for performance & scalability enhancement. These extended APIs are not strictly confined to sockets only - they work on any I/O operations (i.e. disk files named pipes etc.). For example (in addition to traditional select() & poll()) Linux has it's own epoll mechanism BSDs have kqueue Windows has WSAevent* WSAaAsync* and I/O completion ports APIs. These differences are visible mostly in lower level languages like C C++ or Pascal. C# Java Python Ruby et al. are somewhat ""higher level"" compared to C - they provide robust technologies to insulate you from the low-level APIs. Instead of fiddling directly with the bare-bones socket API you can use the extensive run-time class libraries provided with each platform. For example to write a TCP/IP server in Python you could simply use the SocketServer class. Similarly in C# you can use WebClient to downlad files of the net. In C++ you have the Boost library. Of course there's nothing stopping you from using raw socket API directly in your app.  Sockets are platform-dependent not language dependent. So linux uses the BSD sockets alone Windows offers both BSD sockets (used almost exclusively) and a M$ flavour of sockets (called WSA) dunno about others. It all boils to what is found under the hood - more exactly at the kernel level of the OS. The socket implementation found there will offer a first set of API in order to make them accessible to kernel/user space - usually through a shared object / dynamic linked library and thus ""natively"" supporting the c/c++ languages. All the other languages are relying on language specific bindings to those SO / DLL files offered by the OS natively for C/C++. HTH I never heared about WPA do you mean WSA? IIRC those aren't really different sockets - you can connect to a socket and wouldn't see any difference if it's a WSA or BSD socket. WSA APIs are just convenient helpers e.g. to get windows messages if a socket event occurs. oops. WSA yes. Windows messages and a few more differences - check the dwFlags param from http://msdn.microsoft.com/en-us/library/ms742212%28v=vs.85%29.aspx. You won get theose flags for http://msdn.microsoft.com/en-us/library/ms740506%28v=vs.85%29.aspx Oh ok gotcha maybe I was looking at the wrong thing thanks. Ok then.....the ""raw"" socket API everyone talks about is that not the Berkley one? Btw I was mainly concerned with Unix.....what is their ""Raw socket"" API called. Either way Im assuming Syntax is pretty similar for all of them and hasn't changed much? Altho it seems BSD was replaced by POSIX for unix now correct? Uhm.. all I knew about POSIX was libpthread... And after having a look at http://en.wikipedia.org/wiki/POSIX#POSIX.1 it does not seem to me network sockets have anything to do with it. Another thing let's reffer to the native sockets API as ""native sockets API"" and not ""raw sockets API"" (where the raw sockets have a consacrated meaning (different) - they are related to IP sockets without anything from level 4 OSI stack on top). As far as I know Unix is on BSD sockets just like the rest of the world."
816,A,"How do I handle partial write completions from overlapped I/O using I/O Completion Ports On Windows I/O completion ports say I do this: void function() { WSASend(""1111""); // A WSASend(""2222""); // B WSASend(""3333""); // C } If I got a ""write-complete"" that says 3 bytes of WSASend() A were sent is it possible that right after that I'll get a ""write-complete"" that tells me that some or all of B & C were sent or will TCP will hold them until I re-issue a WSASend() call with the rest of A's data? Or will TCP complete it automatically? I've been developing client and server systems with IOCP for over 10 years now and I've never seen a partial write completion in normal usage. You CAN get them but if you do then the chances are your server is hosed anyway; you'll likely get a write completion with an error of ENOBUFS which tends to mean that you've exhausted non-paged pool or exceeded the locked pages limit. IMHO you should manage your resources such that you never hit these operating system limits. If a write completion returns less than the number of bytes that you think you should have written then there's not really much that you can do to recover if you have more writes pending. In the example in your question if A failed then you could only really shutdown the connection as B and C might succeed. In practice you don't have to worry about more than this. If you know that you don't have any more writes pending on that connection then you COULD issue another write to write the subsequent data. And TCP doesn't come into this at all. Take a look at the FTP RFCs. I've no idea how the protocol is structured for the data transfer side of things. They may have explicit support for flow control in the protocol. There's nothing to say that the server or client uses IOCP; they may use blocking writes and then there is no issue as the stack will simply block your call. And we've done the whole 'write completion driven pacing' to death in other questions. Crystal clear - I better put some limits before and not have to deal with it later. So for example in my LAN here I have two PCs - one is an FTP server (Serv-U) and the other is an FTP client (FlashFXP); When I upload a file from the client it fills the pipe in a speed which is relative to the LAN's current conditions - how does FlashFXP knows how much to push? Or does do a readl WSASend () one by one (doing the next one as soon as the current has finished)? What do you say?"
817,A,"socket ""read"" hanging if the MacBook sleeps more than 10 minutes I am writing an app where a socket is connecting to a host and downloading a file. The application runs in Mac. Now while the app is downloading if I put the MacBook in sleep mode for more than 10 minutes 60% of the time the app hangs when the computer wakes up. The stack trace shows that it has hanged in the ""read"" call. I am able to reproduce this with a sample program also. Below I have pasted the code of the sample program and the stack where it is hanging. How to solve this hanging? Also this is not just TCP/IP waiting that will come out in few minutes. I have waited for more than 12 hours it did not come out. The stack trace: - Call graph: 2466 Thread_2507 2466 start 2466 read$UNIX2003 2466 read$UNIX2003 The program :- #include <stdio.h> #include <string.h> #include <netdb.h> #include <sys/socket.h> #include <unistd.h> #define buflen 131072 unsigned int portno = 80; char hostname[] = ""192.168.1.9""; int main() { int sd = socket(AF_INET SOCK_STREAM 0); /* init socket descriptor */ struct sockaddr_in sin; struct hostent *host = gethostbyname(hostname); char buf[buflen]; int len; int ret; FILE *fp; int i; if(sd == -1){ printf(""Could not create client socket\n""); return 1; } /*set keep alive*/ int optval = 1; int optlen = sizeof(optval); ret = setsockopt(sd SOL_SOCKET SO_KEEPALIVE &optval optlen); if(ret != 0){ printf(""could not set socket option.\n""); return 1; } /*** PLACE DATA IN sockaddr_in struct ***/ memcpy(&sin.sin_addr.s_addr host->h_addr host->h_length); sin.sin_family = AF_INET; sin.sin_port = htons(portno); /*** CONNECT SOCKET TO THE SERVICE DESCRIBED BY sockaddr_in struct ***/ if (connect(sd (struct sockaddr *)&sin sizeof(sin)) < 0) { perror(""connecting""); return 1; } char *str = ""GET /general-log.exe / HTTP/1.0\n\n""; ret = write(sd str strlen(str)); if(ret < 0){ printf(""error while writing\n""); return 1; } fp = fopen(""downloaded.file"" ""wb+""); if(fp == NULL){ printf(""not able to open the file.\n""); return 1; } i = 0; while ((len = read(sd buf buflen)) > 0) { printf(""%d\t%d\n"" i++ len); fwrite(buf len 1 fp); //we should check for return } if(len < 0){ printf(""Error while reading\n""); } fclose(fp); close(sd); return 0; } Update apparently the SO_RCVTIMEOUT is solving the problem. struct timeval tv; tv.tv_sec=10; tv.tv_usec=0; setsockopt ( m_sock SOL_SOCKET SO_RCVTIMEO (char *) &tv sizeof ( tv ) ); Is it okay to use SO_RCVTIMEO? TCP/IP connections don't survive sleep mode. SO_KEEPALIVE doesn't help in this case since it has no effect on the server side. Just wait two minutes and the read will time out. After the timeout you can connect again. And that sleep(1) is unnecessary. The server will respond as soon as the data is available. If you don't fetch is right away you'll allocate a connection on the server for longer than you need. thanks. updated my question. SO_KEEPALIVE doesn't help in this case since it has no effect on the server side. I have waited more than 12 hours the program did not come out. What happens if you remove the SO_KEEPALIVE? Still hangs. Actually I added SO_KEEPALIVE later for testing only. And you put the mac to sleep while it downloads? Yes. I put it to sleep while the downloading is going on.  I couldn't solve it using blocking sockets. I had to change the IMAP library to non-blocking sockets."
818,A,"Is the received stream from a socket limited to a single send command? I currently work on a multithreaded application where I receive data using the following (simplified) code: private void BeginReceiveCallback(IAsyncResult ar) { bytesReceived = this.Socket.EndReceive(ar); byte[] receivedData = new byte[bytesReceived]; Array.Copy(buffer receivedData bytesReceived); long protocolLength = BitConverter.ToInt64(receivedData 0); string protocol = Encoding.ASCII.GetString(receivedData 8 (int)protocolLength); IList<object> sentObjects = ParseObjectsFromNetworkStream(receivedData 8 + protocolLength); InvokeDataReceived(protocol sentObjects); } I'm experiencing that receivedData contains not only the expected data but also a lot more. I suspect that this is data sent afterwards that has been mixed in with the previous in the stream. My question is what data can I expect to be stored in this buffer. Can it contain data from two different send operations from the client side? In this case then I suppose I will have to come up with a protocol that can differentiate between the data 'messages' sent from the client side. A simple approach would be to respectively start and end each stream with a specific (unique) byte. Is there a common approach to seperating messages? Furthermore I guess this means that a single receive call might not be enough to get all the data from the client which means I'll have to loop until the end byte was found? TCP/IP socket connections consist of two independent streams: one incoming and one outgoing. This is one of the key concepts of TCP/IP that is often missed. From the perspective of the application TCP/IP does not operate on packets; it operates on streams! There is no method to send a packet. The API simply does not exist. When you send data you just place those bytes in the outgoing stream. They are then read from the incoming stream on the other side. As an example one side can send 5 bytes and then send another 5 bytes. The receiving side can receive two batches of 5 bytes or one at a time or all 10 in a single read... To split the incoming stream of bytes into messages you need message framing. One of two solutions is commonly used. The one you suggested is the delimiter solution where SOT/EOT bytes are used to designate message boundaries. Another one (which I prefer) is the length prefix solution where the length of the message is prefixed to the message itself. A more thorough discussion is on my blog along with sample code for length prefixing. Thanks for the concise answer. My current solution uses length prefixing but is not able to distinguis between messages -- yet. What approaches can be taken to gracefully handled parts of the messages not arriving? Is a timeout based approach the best route to pursue? Furthermore what is half the messages - say the middle - gets lost. All the sudden I will be parsing half the next message instead causing a garanteed crash. Can I blindly trust in TCP to ensure this doesn't happen? TCP presents you with a reliable stream. It handles retransmission of lost pieces for you any bytes that you send from one peer will ALWAYS arrive (in the correct order) at the other peer unless the connection is reset (which you'll know about). @Qua: The sample code link I posted will accept partial messages and buffer them until the rest of the message arrives. As Len stated TCP streams are reliable so you don't have to worry about losing data. I do recommend that you send a ""heartbeat"" message from each end on a timer; this is [also covered](http://nitoprograms.blogspot.com/2009/05/detection-of-half-open-dropped.html) on my blog. Most custom TCP protocols are message protocols and length prefixing works great for them. A TCP/serial bridge protocol is different since you're wrapping a stream. In that case delimiters with escapes may be a better solution. You would just have to be sure to handle the [half-open scenario](http://nitoprograms.blogspot.com/2009/05/detection-of-half-open-dropped.html). BTW there's no way to get ""out of sync"" with TCP; length prefixing is just the easiest message framing *if* your protocol is message-based. Length-prefix with 8-bit-transparent data is good (perhaps optimal) for protocols which involve two devices which are communicating purely via TCP. If one is communicating with an embedded device via a TCP-to-serial bridge length-prefix is bad unless the maximum length is short enough that one can afford to send that many zeroes if the ends of a connection seem like they might be out of sync. Otherwise a delimiter-based approach with escapes or a combination of length-prefix and escapes/delimiters may be best.  With TCP/IP the data is typically considered a stream. You can receive as much as is sent (and you may need to ""receive"" again in order to get all that was sent). A common situation is that the two end points would have some kind of back-and-forth dialog. That does not sound like the situation that you are describing. A simple of handling what your application is doing is to send the length of data in the first few bytes (e.g. a 4 byte integer). The receiving end would receive 4 bytes to find the expected length and then receive that exact amount.  you are right the stream that the socket received can be either it could get truncated (forceful disconnect) or as you have seen it can contain additional data from another send operation. you can play around with the socket settings to specify immediate send (nagle? i think it's called) With the work I've done with TCP I have a communcation protocol that's used for binary data and another thats used for ascii when i'm sending binary data i have a known data lenght that all communication must start with (BlockType (2bytes) BlockLength (4bytes) Data (nBytes)) The receiving end then knows to read the 6bytes to work out the type and length. If i receive less than 6 bytes try until i do and buffer the previous values etc then once established size is known read until your have read from Data the BlockLength bytes (buffering as required). If you have a socket disconnect you need to deal with either resumption or restarting etc. If I am working with ascii data I use a unique wrapping method around the block of data being send (~~~start~~~) ....DATA.... (~~~end~~~) than just buffer the content into a stringbuilder or similar until you hit (~~~end~~~) and then continue with your operations. Hope this is of some help. Yes - it is called the Nagle algorithm. If it is enabled then (typically) the underlying stack will ""hold"" on to data for a few milliseconds before sending the data. If more data is given in that timeframe it will send all the data in a single send on the wire. It's important to *not* disable Nagle unless you know *exactly* what you're doing."
819,A,"Cant choose the right networking solution for my Java app I am writing a distributed Java application that will make heavy use of networking and that needs to be fast. Let's pretend that I have a class called Widget. Widgets are created client-side (most likely a Swing box) but will need to be persisted server-side and shared/distributed amongst all the other connected clients. So I need a way to serialize Widget instances send them to the server where the server will manage them and update all connected clients with state changes in the Widgets they are interested in. (So once a Widget is created and sent server-side it can theoretically be ""pulled down"" and modified by any other client. Performance is a must so this must be a binary protocol. Also would prefer non-blocking sockets and something that is very very scalable. So I'm looking at RMI NIO and Netty as feasible solutions. If I understand Java networking correctly then everything seems to indicate the RMI is considerably slower than NIO. I've also heard RMI is lacking in the scalability department. NIO on the other hand gives you more flexibility and thus is considerably more complicated. Netty seems to have the best documentation but seems slower than NIO and may not be capable of what I really need it to do. For Widget distribution I'm simply looking for the ability to send Widgets over a network and fast. Don't care what protocols are used. Anyone care to share their thoughts/input? Thanks! Both Google's Protocol Buffers and Apache Thrift meet those criteria. They are binary protocols for data serialization and they have RPC APIs for passing your widgets over the network so that you do not have to reinvent the wheel.  First of all RMI and NIO are apples to oranges; so don't mix up things that are not alternatives. Second make sure performance really truly matters; and what kind of performance (throughput & latency being usual first candidates; or maybe space efficiency). There is a chance you are overestimating significance of performance as with good solutions you are more likely to be bounded by network performance not by endpoint (CPU) performance. And if performance matters have a look at jvm-serializers to get one performance comparison of actual formats. Java serialization is used for RMI for example. Oh btw do not assume you must use a binary format. This is typically not a hard requirement; textual formats compress well and are easier to debug/troubleshoot and process by other platforms languages if need be. StaxMan - thanks for the input. I seem to be missing something basic when it comes to understanding the difference between RMI and NIO. As I understand it both are sockets-based. Whereas NIO allows you to implement anything on top of TCP or UDP RMI almost seems to have its own application-layer protocol for transporting serialized objects across a network. But doesn't this just boil down to 2 different socket-based protocols? What am I missing here? Unless the terms ""serialized"" and ""networked"" objects do not imply the same thing... NIO is the low-level alternative to older blocking I/O (""BIO""). NIO just allows accessing sockets (and files etc) in non-blocking manner. Netty is a library that makes it easier to use NIO. And RMI just implements ""black box"" of not just tcp based send/receive but also object serialization and other related pieces. Serialization with RMI does include full serialization from any Java object to byte[] and back; I think serialization has other meanings in other contexts? So: basically RMI implements ""all you need"" and it's hard to replace pieces. NIO (with or without Netty) is a lower-level building block. You can use any of dozens of serialization schemes with NIO (or blocking I/O). If you like idea of RMI you could consider RMI replacement DiRMI http://sourceforge.net/projects/dirmi/ written by a senior Amazon engineer; it's ""just like RMI just better"" addressing most known existing RMI issues. Ahhhh.... so is it safe to say that in order to pass Widget over a network it must first be serialized into bytes and then RMI uses TCP sockets (based on NIO?) to pass the bytes over the network. Then on the other end RMI has the ""black box"" magic to reconstruct the bytes into Java objects. Is this a fair assessment? Exactly. RMI implementation JDK uses may use NIO or blocking i/o (probably latter since NIO was added after RMI) but it is based on sockets either way.  You mentioned Netty. I would second that. My experience with Netty was great. It's a very efficient library with solid documentation and flexibility. It can do anything you can imagine with binary protocols yet it only does what you need so it's lightweight. It also scales very well. I've successfully load tested a single server with a simple job sustaining over 10k+ request/sec for 200-300 concurrent clients. That was on a fairly standard rack server. There is a bit of a learning curve but the javadoc is some of the best I've seen for dealing with that hurdle. The project is in active development and the lead on the project is extremely responsive to questions and defects. EDIT: For fast transfer of Widgets you could look into Protocol Buffers. They are pretty efficient for serializing/de-serializing objects over the wire. Thanks rfreak! I'll take that into consideration - those are some pretty big numbers. Also when I say that this is a networking application that uses distributed Widget objects am I using the phrase ""distributed"" correctly? Or do ""serialized"" ""distributed"" or ""networked"" have different implications? Thanks!  Have you looked at Jini and Javaspaces ? It's RMI so perhaps not suitable. But the space-based pattern is very powerful for distributing objects receiving notifications updating in a distributed fashion etc. and you can get something up and running very easily. Brian thanks for the suggestion - I'll take a look at Jini tonight."
820,A,"How do i monitor network traffic on Windows from the command line How do i monitor network traffic on Windows from the command line; specifically the download/upload speeds and amount of data uploaded/downloaded ? Is there a script /batch for doing that ? It might be better asking this on Serverfault While tshark is really powerful if you want to have fine grained statistics (according to hosts protocols ...) it has the main drawback to gather statistics during the time period it is running. As such it is only good at reporting ""instant"" statistics but not to report poll traffic at regular points in time to have a view of how your network traffic changes along the day week ... Moreover as tshark makes packets capturing there is some overhead. So according to your needs you might be interested in the MS Windows 'net' or 'netstat' commands ('netstat' has option to report statistics by protocol). 'net statistics [Server|workstation]' or 'netstat [-e|-s]' are as far as network traffic statistics are concerned the MS Windows equivalents of Linux 'ifconfig' (or 'cat /proc/net/dev' if you prefer). Note that as 'ifconfig' do 'net' or 'netstat' only report amount of data since the interface has been brought up. In order to obtain traffic rates you've got to timestamp your calls to those commands and do the computation yourself. AFAIK both commands are shipped with all recent MS Windows versions.  You can use tshark with -z <statistics> argument. Just search Wireshark. It is open source and multiplatform."
821,A,"How to make server accepting connections from multiple ports? How can I make a simple server(simple as in accepting a connection and print to terminal whatever is received) accept connection from multiple ports or a port range? Do I have to use multiple threads one for each bind call. Or is there another solution? The simple server can look something like this. def server(): import sys os socket port = 11116 host = '' backlog = 5 # Number of clients on wait. buf_size = 1024 try: listening_socket = socket.socket(socket.AF_INET socket.SOCK_STREAM) listening_socket.setsockopt(socket.SOL_SOCKETsocket.SO_REUSEADDR1) listening_socket.bind((host port)) listening_socket.listen(backlog) except socket.error (value message): if listening_socket: listening_socket.close() print 'Could not open socket: ' + message sys.exit(1) while True: accepted_socket adress = listening_socket.accept() data = accepted_socket.recv(buf_size) if data: accepted_socket.send('Hello and goodbye.') accepted_socket.close() server() EDIT: This is an example of how it can be done. Thanks everyone. import socket select def server(): import sys os socket port_wan = 11111 port_mob = 11112 port_sat = 11113 sock_lst = [] host = '' backlog = 5 # Number of clients on wait. buf_size = 1024 try: for item in port_wan port_mob port_sat: sock_lst.append(socket.socket(socket.AF_INET socket.SOCK_STREAM)) sock_lst[-1].setsockopt(socket.SOL_SOCKETsocket.SO_REUSEADDR1) sock_lst[-1].bind((host item)) sock_lst[-1].listen(backlog) except socket.error (value message): if sock_lst[-1]: sock_lst[-1].close() sock_lst = sock_lst[:-1] print 'Could not open socket: ' + message sys.exit(1) while True: read write error = select.select(sock_lst[][]) for r in read: for item in sock_lst: if r == item: accepted_socket adress = item.accept() print 'We have a connection with ' adress data = accepted_socket.recv(buf_size) if data: print data accepted_socket.send('Hello and goodbye.') accepted_socket.close() server() I'm not a python guy but the function you are interested in is ""select"". This will allow you to watch multiple sockets and breaks out when activity occurs on any one of them. Here's a python example that uses select. I am a Python guy and you need select: http://docs.python.org/library/select.html Thanks. Se my original post for an example code  Since Python's got so much overhead multithreaded apps are a big point of debate. Then there's the whole blocking-operation-GIL issue too. Luckily the Python motto of ""If it seems like a big issue someone's probably already come up with a solution (or several!)"" holds true here. My favorite solution tends to be the microthread model specifically gevent. Gevent is an event-driven single-thread concurrency library that handles most issues for you out of the box via monkey-patching. gevent.monkey.patch_socket() is a function that replaces the normal socket calls with non-blocking variants polling and sleeping to allow the switch to other greenlets as need be. If you want more control or it's not cutting it for you you can easily manage the switching with select and gevent's cooperative yield. Here's a simple example. import gevent import socket import gevent.monkey; gevent.monkey.patch_socket() ALL_PORTS=[i for i in xrange(1024 2048)] MY_ADDRESS = ""127.0.0.1"" def init_server_sock(port): try: s=socket.socket() s.setblocking(0) s.bind((MY_ADDRESS port)) s.listen(5) return s except Exception e: print ""Exception creating socket at port %i: %s"" % (port str(e)) return False def interact(port sock): while 1: try: csock addr = sock.accept() except: continue data = """" while not data: try: data=csock.recv(1024) print data except: gevent.sleep(0) #this is the cooperative yield csock.send(""Port %i got your message!"" % port) csock.close() gevent.sleep(0) def main(): socks = {p:init_server_sock(p) for p in ALL_PORTS} greenlets = [] for kv in socks.items(): if not v: socks.pop(k) else: greenlets.append(gevent.spawn(interact k v)) #now we've got our sockets let's start accepting gevent.joinall(greenlets) That would be a super-simple completely untested server serving plain text We got your message! on ports 1024-2048. Involving select is a little harder; you'd have to have a manager greenlet which calls select and then starts up the active ones; but that's not massively hard to implement. Hope this helps! The nice part of the greenlet-based philosophy is that the select call is actually part of their hub module as I recall which will allow you to create a much more scalable and complex server more easily. It's pretty efficient too; there are a couple benchmarks floating around.  If you really wanted to be lazy (from a programmer standpoint not an evaluation standpoint) you could set a timeout on your blocking read and just loop through all your sockets; if a timeout occurs there wasn't any data available. Functionally this is similar to what the select is doing but it is taking that control away from the OS and putting it in your application. Of course this implies that as your sleep time gets smaller your program will approach 100% CPU usage so you wouldn't use it on a production app. It's fine for a toy though. It would go something like this: (not tested) def server(): import sys os socket port = 11116 host = '' backlog = 5 # Number of clients on wait. buf_size = 1024 NUM_SOCKETS = 10 START_PORT = 2000 try: socket.setdefaulttimeout(0.5) # raise a socket.timeout error after a half second listening_sockets = [] for i in range(NUM_SOCKETS): listening_socket = socket.socket(socket.AF_INET socket.SOCK_STREAM) listening_socket.setsockopt(socket.SOL_SOCKETsocket.SO_REUSEADDR1) listening_socket.bind((host START_PORT + i)) listening_socket.listen(backlog) listening_sockets.append(listening_socket) except socket.error (value message): if listening_socket: listening_socket.close() print 'Could not open socket: ' + message sys.exit(1) while True: for sock in listening_sockets: try: accepted_socket adress = sock_socket.accept() data = sock.recv(buf_size) if data: sock_socket.send('Hello and goodbye.') sock.close() except socket.timeout: pass server() Why implement your own select? Isn't the built-in select module simpler than this? Thanks. I guess since the select uses less CPU. I went for that solution. I vaguely recall it *not* being simpler. But now that I think about it a little more there's a pretty good chance that I was working in C not Python when I was doing that :("
822,A,How to find which computer send the print job? I'm creating a postscript printer for windows 7 which will accept print jobs and forward them to real printers. It will be shared in the LAN and can receive print jobs from LAN computers. I want to find out from which computer a print job came from before forwarding them to a printer. How can I do this? Is the details available in the print job itself? Thanks Take a look at the GetJob and EnumJobs spooler api functions. They both return one of two structures JOB_INFO_1 or JOB_INFO_2. Each structure contains a pMachineName field which is the name of the name of the machine that created the print job. You may find the following links from pinvoke.net useful. http://www.pinvoke.net/default.aspx/Structures.JOB_INFO_1 http://www.pinvoke.net/default.aspx/winspool/EnumJobs.html Alternatively you may also want to look into using WMI and the Win32_PrintJob class. Specifically the HostPrintQueue property. Thank you very much I will check those and reply asap.
823,A,Why is my UDPClient null once in a while Can someone explain to me why this code fails once in a while with a null exception for udpLink while sending?  udpLink = new UdpClient(ipAddress 514); using (udpLink) { udpLink.Send(rawMsg rawMsg.Length); } This is the new code on how I fixed it.  udpLink = new UdpClient(ipAddress 514); using (udpLink) { if (udpLink != null) udpLink.Send(rawMsg rawMsg.Length); } Any ideas? What is that udp variable in your code? Did you mean to pass udpLink to the using statement? Martin ah...sorry just a typo when asking the question. Depending on whether or not this code segment is in a loop that executes thousands of times you might be maxing out on connections (speaking from experience). you can do a netstat -an and if it scrolls for more than a second chances are that could be your problem.  I don't see any reason why you should be getting a null pointer exception on udpLink. You are sure its udpLink that is null and not rawMsg? Also you're sure you are throwing a NullPointerException and not a SocketException or other exception?  I'm not sure if it really is the problem but I guess it's the way how you use the using statement. I would do it like this: using (UdpClient udpLink = new UdpClient(ipAddress 514)) { udpLink.Send(rawMsg rawMsg.Length); } Functionally that's the same all you did was remove a line of code. I wrote I'm not sure if it helps... I'm not sure why this worked but it did. I've been running the code for sometime now with not even one exception after I made your suggested code change. Thank you.
824,A,"Distributed system design using only C I have the work of implementing a distributed system of nodes (like p2p nodes) each of these nodes (lets say ABC and D) perform certain functions and need to interact with each other for various operations such as synchronize operations and other things like 15 A nodes interact with a group of 5 B nodes to get into the least loaded node's queue and get a token number and thereafter wait for C to redirect them to a free node D and so on. I am a bit lost as to how should i go about the design: The protocol that i have thought of is to encapsulate a struct of the type of operation and other things to be sent. Also this is done using an acknowledgment scheme so i can be sure the other side got the message. How do i go about the distributed mutual exclusion aspect as I do not have a central server. I am guessing each node replicates the data but this sounds a bit too expensive(not to mention stupid). What is the basic design methodology followed while implementing p2p systems ie how do i implement the program such that it is blocked on a receive but also can send further updates et al and at the same time get information from others about the 'state' of the whole system. How do i ensure total ordering of requests? Also what are the other issues that I may need to look at/face. I also would appreciate if you could point me towards some good online resources on implementing p2p and distributed systems. Thanks!! you are signing up for lots of pain: why not consider Erlang for example? This would help you solve lots of the problems e.g. networking distributed serialization marshaling etc. You say ""using only C"" but tag the question as both ""c"" and ""c++."" Which one is it? didn't know that. I just thought of the wiki concept and thought people may refine my query towards something more precise than what i described. How do i remove that attribute? and why is this community wiki? do know that this lowers your chances of getting decent answers... My bad. Only C no C++ The secret to prevent blocking is your end points must all be written as servers with threads for ""protocol"" processing that are separate from the threads for data processing. As for line protocol I've become enamored with JSON for line protocol. It is human readable. It is streamable without need for length bytes! It is easily extensible and mostly immune to protocol version change. Can you elaborate or give references to the **The secret to prevent blocking is your end points must all be written as servers with threads for ""protocol""** part of the answer. It would be of great help. Also How do i ensure total ordering of events? Look into the design of FTP. There is a protocol port where the client talks to the server. Only protocol data is sent over this connection. Data transfer takes place over a separate connection. This allows the server and client to react to changing conditions faster. As for total ordering of events that is impossible in a p2p design. It is also meaningless in a p2p protocol. If a sends to b and c sends to d does it matter which happens first?  I won't try to give a ""whole"" answer (because the question is way too large & vague anyways) but I could point you towards an interesting piece of the puzzle: You could use a Message Queuing system (e.g. AMQP RabbitMQ: there is an experimental C binding available) to implement reliable message delivery between your nodes. Mutual Exclusion: you can use a protocol such as Paxos +1 for message passing I will look at paxos. The requirement is to simply build a distributed serverless system of nodes which need to collaborate to process a job in a sequence.  Yet another piece of software worth a look could be KadC! http://kadc.sourceforge.net"
825,A,Python Pygame Pyro: How to send a surface over a network? I am working on a project in python using pygame and pyro. I can send data functions classes and the like easily. However I cannot send a surface across the wire without it dying on me in transit. The server makes a surface in the def __init__ of the class being accessed across the wire: self.screen = pygame.display.set_mode(SCREENRECT.size NOFRAME) On the server the screen prints as Surface(800x800x32 SW) but when retrieved by the client it is Surface(Dead Display). Something to note though. I get a dead display when I use an accessor function to get my screen. If I use print Player.screen to get the variable I instead get what seems to be a pyro pointer to the screen: <Pyro.core._RemoteMethod instance at 0x02B7B7B0>. Maybe I can dereference this? More than likely I am being thick does anyone have some insight? Thanks. :) Generally speaking you don't want to send a Surface (I'm assuming that a Surface is a device-dependent display) across the network. Most of the time your client will be responsible for managing the drawing on its local Surface and your server is responsible for telling the client what it needs to draw. A server may not even have a display that's capable of showing graphics!  A pygame Surface is a wrapper around an underlying SDL surface which I suspect can't be serialized by Pyro. If you want to copy its contents across the wire you would be better off doing something like this: on the server use Surface.get_buffer() to get access to the underlying pixels. make a note of the Surface's dimensions colour depth etc. send the data obtained from steps 1 and 2 over the wire to the client. on the client create a new surface using the dimensions colour depth etc from step 2. set the new Surface's pixels using Surface.get_buffer() and copying in the pixels from step 1. Edit: It just occurred to me that I'm overcomplicating it. To serialise your Surface use pygame.image.tostring() and to reload it use pygame.image.fromstring(). This might be exactly what I am looking for. I'll try to implement it tonight. Your edit to your post was what really helped me get to the bottom of my issue. Reading over pygame docs was also crucial! Thanks for linking to them in your post.  Try pickling the object and send the file...
826,A,LuaSockets POST data issue [LUA] I'm getting GET data without issue using:  local get err = client:receive() But I'm not sure how to receive POST data. I've tried:  local get err = client:receive('*a') But the server seems to always timeout on POST data (the test data is just an textbox with some words in it) Any help fully appreciated and as always please link me if this is a repeat question! Cheers! I would have done so but as a new member I was unable to do so until waiting 8 hours; with that time constraint in mind I edited the original post while the solution was still fresh and planned on re-posting it after the time limit expired. Please also note that I cannot make my own post as an answer until 24hrs aftert making said post. I think they're trying to get you to stick around and answer a few questions. :) (Well that and to prevent abuse.) go ahead and answer your own question below and then click the checkmark to mark it as answered. This helps when people are looking for unanswered questions. Lua is a word/name not an acronym so you don't write it as LUA. Seems to be a pet peeve around here. Are you developing a web server or client? @BMitch Oh sorry thanks for the edit! Also: Web server I need to receive the POST data from web browsers I've managed to find a solution to my problem seems that the: local get err = client:receive('*a') method only returns complete data. Headers seem to fall under partially received rather than fully received and as such gets placed to a third variable: local data emsg partial = client:receive(pattern prefix) This seems to be undocumented but I may just have missed it If you found the solution to your question accept your own answer.
827,A,iPhone - Browsing iPhone files through computer I want to be able to send files from an iPhone app to a computer. What would be the easiest way of doing this? I've made simple server client programs before but in those the client has always needed to connect to the server before being able to receive messages from it. There is an app for the iPhone called iSimulate where you put a server on a Mac (the iPhone simulator) and then you use the iSimulate app of an iTouch or iPhone to send touch events to the server. This app does not require you to type in an ip-address. Instead it presents a list of available computers that have this server up and running. How exactly is this being done? Can a server broadcast a message over a network w/o anyone being connected to the server? How does that work? How does a client listen for that broadcast? Here's a video of the app I'm talking about: http://www.youtube.com/watch?v=N3Qpd1ycZh4 I've seen similar apps use a peer-to-peer Bluetooth connection to exchange files between a iPhone and a desktop.  That app may be using Bonjour.
828,A,"Asynchronous network IO using r: Any existing packages Are there any R-project packages that facilitate asynchronous network IO? I'm think here along the lines of Ruby's Eventmachine or Python's Twisted. If there are several such packages/libraries which is the best in terms of: - performance - features First of all R is single-threaded so typically people try to use parallel computing approaches (see for instance the snow package). I don't think there's anything quite like Eventmachine or Twisted. Check out the following: The ""State of the Art in Parallel Computing with R"" paper describes most of the approaches to parallel computing in R (http://www.jstatsoft.org/v31/i01/paper). There are many useful packages in the HighPerformanceComputing view: http://cran.r-project.org/web/views/HighPerformanceComputing.html. Check out svSocket: http://cran.r-project.org/web/packages/svSocket/ You can try using NetWorkSpaces with R: http://cran.r-project.org/web/packages/nws/. There are several examples of R servers. RServe: http://www.rforge.net/Rserve/ The iBrokers packages is one of the only ones that I know which uses asynchonous requests. Have a look at the source code for that package (you can download it off R-Forge) and the related vignette: http://cran.r-project.org/web/packages/IBrokers/vignettes/RealTime.pdf The biocep project also includes many relevant features: http://biocep-distrib.r-forge.r-project.org/"
829,A,Android ConnectivityManager information I am trying to get a better grip on Android's ConnectivityManager and how it really works under the hood. Using the API is simple enough but I am looking for something lower level such as when it starts and the dirty details of what its doing in the background. I can't seem to find a good resource about it. Everything is just about using the API. Does anyone understand this or have any resources that they could point me to website book white paper or otherwise? I would really appreciate it. Thanks Maybe if you posted a particular question someone may be able to help you. Or you could look at the Android source. What is it you're trying to accomplish? The problem is that I can't even get into it enough to have specific questions. I was hoping for an overview of how it all goes down. I am trying to get the source now thanks for that suggestion. so you want to understand what the system service is doing? i think that abstraction is in place for a good reason and unless you plan on hacking your device there isnt much good reason to go mucking around in the OS. If you are really set on digging around i would suggest what CaseyB said: look at the Android source code. Besides that i doubt google is going to explain much beyond their APIs Yea I basically want to know more about what it's doing. So I guess the source is the best place to start. Thanks for the tips. I just want to close this one out. Source files are the best way to go on this one. Java.android.net is the directory that holds all the networking java services. To look at switching from network to network however one would have to look in the native layer.
830,A,"How are sockets implemented in JVM? I want to know how sockets are implemented in the Java Virtual Machine. Is there a native library included? And if is it a C library? Where can I find information about this topic? The offical Java tutorial on networking does not help me there. Some interesting links would help. Update: Are there any official information provided by Sun? Thanks in advance! Edit I found a proof I mark my answer as the correct one. Thanks to Oscar that was the perfect hint for me!!! THANKS! In the network guide of Java 1.4.2 an interesting piece of information is provided: The implementation details... ...that you don't need to know unless you subclass SocketImpl/DatagramSocketImpl. Every *Socket object has an underlying SocketImpl/DatagramSocketImpl that interfaces to native code. The Impl classes implement two methods to support options: void setOption(int optID Object val) throws SocketException; Object getOption(int optID) throws SocketException; that look much like C. These methods act as glue to the native methods and ensure type safety before native methods are invoked. So I think it's proofed: Java uses native libraries for sockets. Well from the source I can tell that method is ""native"" thus uses a native library that could be either in C C++ or assembly. As for the quote documentation is not running software. :) What is the rationale of your question anyway? I need to compare network programming techniques in Linux and Windows of Java and Python.  Probably using the system-V socket calls exposed by the underlying platform e.g. system-V under Unix WinSock under Windows. The specification only dictates how a virtual machine must behave so implementers are free to do as they wish.  Java is open source so you can grab the source code for a self paced deep examination of the class. As an start and quick answer to your question here's what I've found in a very quick scan: private native void socketConnect(InetAddress address int port int timeout) So yeap a native lib is included and my guess it is a C++ library. There are a number of implementations ( SSLSocket PlainSocket etc. ) The online source of JDK7 is here :S Not sure how up to date it is I've used my IDE to navigate the source code that comes with every JDK installation. It's hard to mark this question as the answer because I need an official citation o.s.e. +1 of course"
831,A,"Convert/cast CFReadStreamRef to NSInputStream (iOS5) I am trying to port my app to iOS5. I am using a TCP connection to a server via CFSockets. My problem now is the conversion (cast) from CFReadStreamRef to NSInputStream (same with write). With iOS4 I could use the toll-free bridging but with automatic reference counting of iOS5 this isn't possible anymore. This is what I get: error: Automatic Reference Counting Issue: Cast to 'NSInputStream *' of a non-Objective-C to an Objective-C pointer is disallowed with Automatic Reference Counting Code:  CFReadStreamRef readStream; CFWriteStreamRef writeStream; CFStringRef strRef = CFStringCreateWithCString(NULL [urlStr UTF8String] NSUTF8StringEncoding); CFStreamCreatePairWithSocketToHost(NULL strRef 4444 &readStream &writeStream); NSInputStream *iStream = (NSInputStream *)readStream; NSOutputStream *oStream = (NSOutputStream *)writeStream; Is there another way to pipe the socket out/input into an NSStream? Thanks for any hint! Managing Toll-Free Bridging states very clearly that you should use something like this: NSInputStream *iStream = objc_unretainedObject(readStream); That worked. Thank you. Found the Apple ""Programming With ARC Release Notes"" afterwards... My output stream still don't work :S Have you done something more to make it work?"
832,A,Mapping Port to PID for Transient Windows TCP Connections I am trying to reverse engineer a third-party TCP client / server Windows XP SP 3 app for which I have no source available. My main line of attack is to use WireShark to capture TCP traffic. When I issue a certain GUI command on the client side the client creates a TCP connection to the server sends some data and tears down the connection. The server port is 1234 and the client port is assigned by the OS and therefore varies. WireShark is showing that the message corresponding to the GUI command I issued gets sent twice. The two messages bear a different source port but they have the same destination port (1234 as mentioned previosuly). The client side actually consists of several processes and I would like to determine which processes are sending these messages. These processes are long-lived so their PIDs are stable and known. However the TCP connections involved are transient lasting only a few milliseconds or so. Though I've captured the client-side port numbers in WireShark and though I know all of the PIDs involved the fact the connections are transient makes it difficult to determine which PID opened the port. (If the connections were long-lived I could use netstat to map port numbers to PIDs.) Does anybody have any suggestions on how I can determine which processes are creating these transient connections? Best Dave I ended up creating a batch file that runs netstat in a tight loop and appends its output to a text file. I ran this batch file while running the system and by combing through all of the netstat dumps I was able to find a dump that contained the PIDs associated with the ports.  I can think of two things: Try sysinternals' tcpview program. It gives a detailed listing of all tcp connections opened by all the processes in the system. If a process creates connections you will be able to see them flash (both connect and disconnect are flashed) in tcpview and you will know which processes to start looking into. Try running the binary under a debugger. Windbg supports multi-process debugging (so does visual studio I think). You may have only export symbols to work with but that should still work for calls made to system dlls. Try breaking on any suspected windows APIs you know will be called by the process to create the connections. MSDN should have the relevant dlls for most system APIs documented. Start here... post a follow-up if you get stuck again.
833,A,"Getting started with client-server networking I'm a good programmer but I have zero network experience. Basically I'd like to get into client-server networking. For example I'd like to try getting a server process going which allows clients to connect over the internet and send pings to all of the other connected clients. Then maybe I'll try developing a simple chat client or some simple multiplayer game and I'll go from there. Languages I know very well that might be useful: Java C++ C. How do I get started? I want to learn best-practices up front so good learning resources you can recommend (eg books online materials etc) would be great. Edit: Should I also look into some kind of VM to emulate various machines interacting with each other? Edit 2: I've put up a 50-rep bounty. Some great answers have been put up so far - I'm looking for more detailed answers though so hopefully this will encourage that. For example an answer by someone with experience in this type of stuff that compares different learning approaches would be really helpful. Thanks! Also could I get some feedback on the whole VM thing? Out of curiosity only. How did you manage to ""know very well JavaC and C++"" but never get involved with networking? @user384706: By learning the languages' syntax most of their libraries and gaining lots of experience with them. I specified that I'm good with those languages to communicate that I don't need beginner tutorials that aim to teach programming AND networking together or that are geared towards beginner programmers. Like ""Now we'll learn the while loop - and then we'll move on to using the loop to send multiple pieces of data at once"" :) VMs aren't really necessary in my experience... you can simulate multiple machines simply by running multiple processes under a single OS. For Java I'd recommend to pass standard networking tutorial: http://download.oracle.com/javase/tutorial/networking/overview/networking.html. It describes basic networking concepts and provides number of examples.  I come from a mostly Linux network programming background. I've done a small bit of Windows socket programming from what I've seen the Windows socket APIs are pretty similar to the POSIX socket APIs and there is some pretty good information on MSDN for porting from POSIX to Windows sockets. Network programming isn't really difficult conceptually there are a few areas that you'll need to have an understanding of to be effective: First you'll need to have a basic understanding of networking. You'll need to at the least know about TCP and IPv4 since those are likely to be the default protocols that you'll end up using. Understanding a bit about DNS and ethernet at a conceptual level will probably also be useful but not strictly required at first. Once you have a basic background (reading a few wikipedia articles or a basic guide to networking will probably be enough to get you started at first) you can start looking at the POSIX socket APIs. These are pretty straightforward once you have the networking basics down and there are plenty of tutorials out there (Beej's network programming guide was listed by someone else it's pretty good and the man pages actually have some pretty useful examples as well). The biggest stumbling block you might run into here is knowing what needs to be in network byte order vs. host byte order but as a rule of thumb anything that you are writing to a socket should be put into network byte order first and anything you read from the socket should immediately be converted to host byte order. Where a lot of people stumble is that just learning the POSIX socket API isn't really enough to do effective network programming. It'll get you far enough to write a simple echo client but to move beyond that there are a few other things you need to be familiar with. The biggest ones are working with non-blocking file descriptors signals working with select/epoll/etc. and getting a handle on the child process workflow. At a high level it's pretty conceptually simple (create->bind->listen->select->fork->accept) but if you're not used to working with file descriptors select fork etc. then those are things you'll need to get up to speed on. One of the best programs to start with in my opinion is an IRC client. It will give you all of the basics you'll need (read/writing messages to a server concurrent connections dealing with different server implementations etc.) the protocol is pretty easy to parse and there are a lot of servers out there to connect to.  I prefer Java. I'm going to explain TCP: The basic concept is that you have to run a ""Server"" on a machine. That server accepts clients waiting for a connection. Each connection goes over a port (you know I hope...). Always use ports above 1024 because ports lower than 1025 are most of the time reserved for standard protocols (like HTTP (80) FTP (21) Telnet ...) However creating a Server in Java is done this way: ServerSocket server = new ServerSocket(8888); // 8888 is the port the server will listen on. ""Socket"" is the word you are probably looking for if you want to do research. And to connect your client to a server you have to write this: Socket connectionToTheServer = new Socket(""localhost"" 8888); // First param: server-address Second: the port But now there isn't still a connection. The server has to accept the waiting client (as I noticed here above): Socket connectionToTheClient = server.accept(); Done! Your connection is established! Communicating is just like File-IO. The only thing you have to keep in mind is that you have to decide when you want to flush the buffer and really send the data through the socket. Using a PrintStream for text-writing is very handy: OutputStream out = yourSocketHere.getOutputStream(); PrintStream ps = new PrintStream(out true); // Second param: auto-flush on write = true ps.println(""Hello Other side of the connection!""); // Now you don't have to flush it because of the auto-flush flag we turned on. A BufferedReader for text-reading is the good (best*) option: InputStream in = yourSocketHere.getInputStream(); BufferedReader br = new BufferedReader(new InputStreamReader(in)); String line = br.readLine(); System.out.println(line); // Prints ""Hello Other side of the connection!"" in this example (if this would be the other side of the connection. Hopefully you can start with networking with this information! PS: Of course all networking code have to be try-catched for IOExceptions. EDIT: I forgot to write why it isn't always the best option. A BufferedReader uses a buffer and read as much as it can into the buffer. But sometimes you don't want that the BufferedReader steals the bytes after the newline and put them into his own buffer. Short example: InputStream in = socket.getInputStream(); BufferedReader br = new BufferedReader(new InputStreamReader(in)); // The other side says hello: String text = br.readLine(); // For whatever reason you want to read one single byte from the stream // That single byte just after the newline: byte b = (byte) in.read(); But the BufferedReader has already that byte you want to read in his buffer. So calling in.read() will return the byte following on the last byte in the buffer of the reader. So in this situation the best solution is to use DataInputStream and manage it your own way to know how long the string will be and read only that number of bytes and convert them into a string. Or: You use DataInputStream. readLine() This method doesn't use a buffer and reads byte by byte and checks for a newline. So this method doesn't steal the bytes from the underlying InputStream.  Understand the basic concepts about networking. Layers IP Addresses Ports Packets [ Specifically UDP/TCP ] Learn programming abstractions about [1] like Sockets. Implement the ""Server"" and the ""Client"" yourself. Test it. Install Wireshark on your computer and look for the IP addresses Types of packets the port numbers etc that are sent for each type of activity. Build on the knowledge by using the Networking API's of Java/.Net/C++. It's probably a very bad idea to build everything from scratch. Java: http://download.oracle.com/javase/tutorial/networking/index.html .Net: http://msdn.microsoft.com/en-us/library/4as0wz7t.aspx C++ : Unix Network Programming by Richard Stevens Hope it helps.  If you're using C++ I would highly recommend that you look at using Boost.Asio it's a great asynchronous networking library that will have you reaching you're goals a LOT faster than using sockets directly. Obviously it's a great idea to learn the network stuff from the ground up but I think that it is highly beneficial to do that with a good (and easy to use) library to back you up so you can get results too. +1 for a C++ solution I highly suggest looking at Boost.Asio  IMHO socket programming in Java is much easier than in C/C++. You can start learning it from Java tutorial: http://download.oracle.com/javase/tutorial/networking/sockets/ For ""basic"" C/C++ socket programming you can learn from ""Beej's Guide to Network Programming"". It is worth to learn when I was in university :). Then you may learn about Windows Socket API for next starting point. To emulate the environment off course you can use VMs :)  Do start with the basics but then you should take a look at what is really used (in Java): For socket programming use Apache Mina. Modern client server apps use REST: Jersey RESTeasy (both JAX-RS complient) and Restlet (pioneer in this field).  I couldn't recommend book by Richard Stevens more.  Stevens' UNIX Network Programming has all the information you could wish for on using the BSD sockets API (which is now the standard sockets API) and how various UNIX systems interpret the functions in those APIs. It includes a lot of code demonstrating the topics under discussion. It is also very dense. Beej's guide is a good quick summary of how to use the sockets API. It might be one of the better places to start and just get the hang of the mechanics of putting together a server and a client. To understand why you're making those function calls with that data in that order and what exactly the function calls are doing you can turn to Stevens. If you want to pick up best practices take a look at production code: How does Mongrel 2 handle networking? How about lighttpd? Varnish? Why do they handle it the way they do? You can also look at the implementation of the networking stack for a platform you're interested in. Grab the kernel source code possibly a book discussing the kernel's design and dig in.  In my opinion you should start by learning how to use sockets let's say in C UNIX (any online tutorial will be suitable or use ""man""). And only after that you can google for language/OS specific libraries and choose whatever you'd like or whatever would fit better your needs. LE: You can always test your applications on the same machine.  Beej's guide to Network Programming is absolutely resounding. Used it at University. http://beej.us/guide/bgnet/ It covers the Sockets API and I remember it using C++ for the code examples. Also Computer Networks by Tannenbaum is an excellent read too. In fact I have some old client and server code...Its from my Uni days not the best code ever but might be helpful too! I had a brief look at this (web) book and while I recognised the content I wouldn't think it the best order for a beginner to learn about IP based client server networking. IMNSHO Some of the other aspects asked by the OP mentioned may be useful (some simple client and server programs for example) I did have the book mentioned in my answer as a companion though. Our class used this exact guide to build our first client/server programs in C. Really useful! @cam somewhere I read the implementation of tcp protocol in c language using structure .  Here's something not mentioned yet. Qt Qt (c++ framework) has a very good ""network"" class that I've used for TCP and UDP server/client type connections. There are several different examples that cover both: Blocking connections (connections that will make the current thread 'wait' for a return) Non-blocking connections (connections that set off call_backs once data is available) Check it out and you'll never look back. They are very well documented!!!  You may want to read Sockets programming in Java: A tutorial on javaworld. It explains the basics. Don't try to go too far in low-level networking initially. Code some simple client/server-s to see how it works.  If you know nothing about TCP/IP I'd start off with Douglas E. Comer's awesome book: TCP/IP Volume 1. You don't need to read all of it there's lots of useful stuff in there. Once you've covered that look at one of the several open source networking implementations such as boost asio. IMHO this is the easiest networking library to get up and running. Once this has piqued your interest further then start to investigate some of the lower level socket details. Btw brumScouse already mentioned Beej's guide this is also a very good resource. And then when you are further along and you want more: google C10K! ;) Let's just say that guide is about the fundamental sockets api; libraries like asio or ACE or whatever simply wrap the functionality exposed by the sockets api and give you nice abstractions to work with. So in that sense it is slightly ""lower level"" but at the same time it is very useful knowledge to have so that you know how the abstractions work. The reason I mention the abstractions first is that it hides a lot of implementation detail that you could get lost in... Would you say Beej's guide falls under the scope of 'lower level socket details'?"
834,A,Is it possible to program iphone to alter wireless router settings? I want to use my iphone to set alter my wireless router settings and I don't want to go through 192.168.1.1 - is there any security restrictions or SDK limitations I should be aware of starting off? -- t Aside from targeting specific devices and building an application for managing it you should check out Unpnp (http://www.gnucitizen.org/blog/hacking-with-upnp-universal-plug-and-play/) which would let you address device in a more uniform way. However what you can achieve with upnp is limited.  Wouldn't that depend mostly (well entirely) on the interface that the wireless router provides to the management of its settings? Yeah. Some expose telnet others only http.
835,A,Joining RTP multicast from the source This computer is taking SDI video as input and giving RTP stream as output. There is no problem joining this rtp multicast from another computer on the same network but if I run my software on the SDI to RTP machine I can't get any packets. There is no problem joining multicast but it acts like there are no packets. We have 2 identical networks I tried both no success. I also tried some other software like VLC to see if they can get any packets and it seems they don't have any problem at all. I checked resource monitor and saw that these software are listening ports without giving a local ip address. I am always setting computers local ip address before joining any multicast stream to select the network ( there are 2 ) For jrtplib you need to set acceptOwnPackets before creating a session RTPSessionParams rtp_sp; rtp_sp.SetAcceptOwnPackets( TRUE ); multiple personality disorder? :)
836,A,"Getting same information firebug can get? This all goes back to some of my original questions of trying to ""index"" a webpage. I was originally trying to do it specifically in java but now I'm opening it up to any language. Before I tried using HTML unit and other methods in java to get the information I needed but wasn't successful. The information I need to get from a webpage I can very easily find with firebug and I was wondering if there was anyway to duplicate what firebug was doing specifically for my needs. When I open up firebug I go to the NET tab then to the XHR tab and it shows a constantly updating page with the information the server is updating. Then when I click on the request and look at the response it has the information I need and this is all without ever refreshing the webpage which is what I am trying to do(not to mention the variables it is outputting do not show up in the html of the webpage) So can anyone point me in the right direction of how they would go about this? (I will be putting this information into a mysql database which is why i added it as a tag still dont know what language would be best to use though) Edit: These requests on the server are somewhat random and although it shows the url that they come from when I try to visit the url in firefox it comes up trying to open something called application/jos So the problem is?: With someone else's web-page hosted on someone else's server you want to extract select information? Using cURL Python Java etc. is too painful because the data is continually updating via AJAX (requires a JS interpreter)? Plain jQuery or iFrame intercepts will not work because of XSS security. Ditto a bookmarklet -- which has the added disadvantage of needing to be manually triggered every time. If that's all correct then there are 3 other approaches: Develop a browser plugin... More difficult but has the power to do everything in one package. Develop a userscript. This is much easier to do and technologies such as Greasemonkey deal with the XSS problem. Use a browser macro technology such as Chickenfoot. These all have plusses and minuses -- which I won't get into. Using Greasemonkey: Depending on the site this can be quite easy.   The big drawback if you want to record data is that you need your own web-server and web-application. But this server can be locally hosted on an XAMPP stack or whatever web-application technology you're comfortable with. Sample code that intercepts a page's AJAX data is at: Using Greasemonkey and jQuery to intercept JSON/AJAX data from a page and process it. Note that if the target page does NOT use jQuery the library in use (if any) usually has similar intercept capabilities. Or listening for DOMSubtreeModified always works too. I actually already am working with XAMPP for a local mysql database. But thanks for the answer and I will look more into how your suggesting. Is there anyway you can help me out a little further? Id be willing to pay if you could write the portion that I need(probably only take 15-20 minutes of your time)? It would be mainly for learning purposes and I would try to edit it further but I honestly have no idea where to start... If your interested you can contact me on msn at stormpsu10000@yahoo.com or if you have a preferred method of contact Id be willing to use whatever. Thanks Open another question and: (1) Clearly restate what you are trying to do using as much relevant detail as possible. Before-and-After code/pics work best.   (2) Link to the actual target page. If that is not possible save the page's source to pastebin.com and link to that.   (3) Appease the moderators by showing what you have done so far (code snippets). ... If you do all that and I have enough details to work with then I will give the issue a reasonable amount of my StackExchange time. You can also sometimes get help writing GM scripts at [userscripts.org](http://userscripts.org/forums/2). Yeah thats probably what I should have done to begin with however I didnt really want people to know what I was trying to do which is why I have been so vague in all of my questions because I didnt want to give it away. Ill try to learn how to make GM scripts first because thats the part that I have no idea where to start with. I think the links you provided me should be useful I just didnt spend very much time learning what they do and just tried to throw them into a script. Thanks for your advice/help though. Can i use a bookmarklet to simulate jQuery on the page? Like so? http://blog.reybango.com/2010/09/02/how-to-easily-inject-jquery-into-any-web-page/ Then do the exact same thing that you suggested? You could but it would be a pain because you'd have to click it with every load and THEN start whatever additional script.. Fortunately Greasemonkey can do anything a bookmarklet can do -- and do it automatically.  Jon I am fairly certain that you are confusing several technologies here and the simple answer is that it doesn't work like that. Firebug works specifically because it runs as part of the browser and (as far as I am aware) runs under a more permissive set of instructions than a JavaScript script embedded in a page. JavaScript is for the record different from Java. If you are trying to log AJAX calls your best bet is for the serverside application to log the invoking IP useragent cookies and complete URI to your database on receipt. It will be far better than any clientside solution. On a note more related to your question it is not good practice to assume that everyone has read other questions you have posted. Generally speaking ""we"" have not. ""We"" is in quotes because well you know. :) It also wouldn't hurt for you to go back and accept a few answers to questions you've asked. Yes I'm aware JavaScript is different then Java when did I mention JavaScript in this? And I was aware that firebug runs as part of the browser which is why i thought my best bet would be to try and emulate a browser which i attempted to do with java through both HTML unit and Selenium. However I didnt really have success with either which is why im taking a step back and asking what others would do. But thank you for an answer In the context and in the context of your other questions I was concerned there might be some confusion. I understand no one probably has read my other posts but I included it just incase someone who has thinks I am asking the same question again(which I dont believe I am) That was more of just short intro to where this question was coming from. ^^@ the above response Well thanks again for trying to help. As you can probably tell I am a beginning programmer and am probably trying to do some things that are over my head or level or however you would like to put it  If you're using a library such as jQuery you may have an option such as the jQuery ajaxSend and ajaxComplete callbacks. These could post requests to your server to log these events (being careful not to end up in an infinite loop). That can be done on a website that isnt mine? You could potentially do this as a browser plugin. Or you could use a bookmarklet that injects some code into the page you're viewing. It would require that the page is using a library which supports such callbacks however as there's no way to automatically intercept AJAX requests. Another completely different option would be to monitor network traffic leaving the machine in question for AJAX requests and responses although this is significantly more complicated."
837,A,"how to bind raw socket to specific interface My application is running on CentOS 5.5. I'm using raw socket to send data: sd = socket(AF_INET SOCK_RAW IPPROTO_RAW); if (sd < 0) { // Error } const int opt_on = 1; rc = setsockopt(m_SocketDescriptor IPPROTO_IP IP_HDRINCL &opt_on sizeof(opt_on)); if (rc < 0) { close(sd); // Error } struct sockaddr_in sin; memset(&sin 0 sizeof(sin)); sin.sin_family = AF_INET; sin.sin_addr.s_addr = my_ip_address; if (sendto(m_SocketDescriptor DataBuffer (size_t)TotalSize 0 (struct sockaddr *)&sin sizeof(struct sockaddr)) < 0) { close(sd); // Error } How can I bind this socket to specific network interface (say eth1)? Thanks Why would you want to do that? Your program will lose portability unless you are sure your machines will have interfaces named the predefined names. It is for embedded device the portability is not needed. I have 6 Ethernet ports and I need to send data using specific interface char *opt; opt = ""eth0""; setsockopt(sd SOL_SOCKET SO_BINDTODEVICE opt 4); First line: set up your variable Second line: tell the program which interface to bind to Third line: set the socket options for socket sd binding to the device opt and since opt is four characters long set 4 as the length. setsockopt prototype: int setsockopt(int s int level int optname const void *optval socklen_t optlen); Also make sure you include the socket.h header file Thanks it working but with one small modification: ifreq Interface; memset(&Interface 0 sizeof(Interface)); strncpy(Interface.ifr_ifrn.ifrn_name ""eth1"" IFNAMSIZ); if (setsockopt(sd SOL_SOCKET SO_BINDTODEVICE &Interface sizeof(Interface)) < 0) { close(sd); // Error } SO_BINDTODEVICE only works if you run as root right? (on Linux at least) Strange it seems to be requiring ifreq structure when socket(7) that the passed option is variable-length Zstring...  As mentioned earlier the correct thing to do is use the struct ifreq to specify the interface name. Here is my code sample. #define SERVERPORT 5555 ... struct ifreq ifr; /* Create the socket */ sd = socket(AF_INET SOCK_STREAM 0); if (sd < 0) { printf(""Error in socket() creation - %s"" strerror(errno)); } /* Bind to eth1 interface only - this is a private VLAN */ memset(&ifr 0 sizeof(ifr)); snprintf(ifr.ifr_name sizeof(ifr.ifr_name) ""eth1""); if ((rc = setsockopt(sd SOL_SOCKET SO_BINDTODEVICE (void *)&ifr sizeof(ifr))) < 0) { perror(""Server-setsockopt() error for SO_BINDTODEVICE""); printf(""%s\n"" strerror(errno)); close(sd); exit(-1); } /* bind to an address */ memset(&serveraddr 0x00 sizeof(struct sockaddr_in)); serveraddr.sin_family = AF_INET; serveraddr.sin_port = htons(SERVERPORT); serveraddr.sin_addr.s_addr = inet_addr(""9.1.2.3""); int rc = bind(sd (struct sockaddr *)&serveraddr sizeof(serveraddr)); I would also like to add that from a security perspective while it is good to bind the socket to an interface it does not make sense to use INADDR_ANY as the listening IP address. Doing so would make the port appear open in netstat on all network interfaces. Proto Recv-Q Send-Q Local Address Foreign Address State User Inode PID/Program name tcp 0 0 0.0.0.0:5555 0.0.0.0:* LISTEN 0 210898 26996/myserver Instead I specified an IP address specific to the interface being used (a private VLAN). This fixed the netstat output too: Proto Recv-Q Send-Q Local Address Foreign Address State User Inode PID/Program name tcp 0 0 9.1.2.3:5555 0.0.0.0:* LISTEN 0 210898 26996/myserver"
838,A,"What is a good method to handle line based network I/O streams? Note: Let me appologize for the length of this question i had to put a lot of information into it. I hope that doesn't cause too many people to simply skim it and make assumptions. Please read in its entirety. Thanks. I have a stream of data coming in over a socket. This data is line oriented. I am using the APM (Async Programming Method) of .NET (BeginRead etc..). This precludes using stream based I/O because Async I/O is buffer based. It is possible to repackage the data and send it to a stream such as a Memory stream but there are issues there as well. The problem is that my input stream (which i have no control over) doesn't give me any information on how long the stream is. It simply is a stream of newline lines looking like this: COMMAND\n ...Unpredictable number of lines of data...\n END COMMAND\n ....repeat.... So using APM and since i don't know how long any given data set will be it is likely that blocks of data will cross buffer boundaries requiring multiple reads but those multiple reads will also span multiple blocks of data. Example: Byte buffer[1024] = "".................blah\nThis is another l"" [another read] ""ine\n.............................More Lines..."" My first thought was to use a StringBuilder and simply append the buffer lines to the SB. This works to some extent but i found it difficult to extract blocks of data. I tried using a StringReader to read newlined data but there was no way to know whether you were getting a complete line or not as StringReader returns a partial line at the end of the last block added followed by returning null aftewards. There isn't a way to know if what was returned was a full newlined line of data. Example: // Note: no newline at the end StringBuilder sb = new StringBuilder(""This is a line\nThis is incomp..""); StringReader sr = new StringReader(sb); string s = sr.ReadLine(); // returns ""This is a line"" s = sr.ReadLine(); // returns ""This is incomp.."" What's worse is that if I just keep appending to the data the buffers get bigger and bigger and since this could run for weeks or months at a time that's not a good solution. My next thought was to remove blocks of data from the SB as I read them. This required writing my own ReadLine function but then I'm stuck locking the data during reads and writes. Also the larger blocks of data (which can consist of hundreds of reads and megabytes of data) require scanning the entire buffer looking for newlines. It's not efficient and pretty ugly. I'm looking for something that has the simplicity of a StreamReader/Writer with the convenience of async I/O. My next thought was to use a MemoryStream and write the blocks of data to a memory stream then attach a StreamReader to the stream and use ReadLine but again I have issues with knowing if a the last read in the buffer is a complete line or not plus it's even harder to remove the ""stale"" data from the stream. I also thought about using a thread with synchronous reads. This has the advantage that using a StreamReader it will always return a full line from a ReadLine() except in broken connection situations. However this has issues with canceling the connection and certain kinds of network problems can result in hung blocking sockets for an extended period of time. I'm using async IO because i don't want to tie up a thread for the life of the program blocking on data receive. The connection is long lasting. And data will continue to flow over time. During the intial connection there is a large flow of data and once that flow is done the socket remains open waiting for real-time updates. I don't know precisely when the initial flow has ""finished"" since the only way to know is that no more data is sent right away. This means i can't wait for the initial data load to finish before processing I'm pretty much stuck processing ""in real time"" as it comes in. So can anyone suggest a good method to handle this situation in a way that isn't overly complicated? I really want this to be as simple and elegant as possible but I keep coming up with more and more complicated solutions due to all the edge cases. I guess what I want is some kind of FIFO in which i can easily keep appending more data while at the same time poping data out of it that matches certain criteria (ie newline terminated strings). I thought this was an interesting problem too so I wrote up a post about solving it with the CCR that you can find at http://iodyner.spaces.live.com if you're interested... What you're explaining in you're question reminds me very much of ASCIZ strings. (link text). That may be a helpfull start. I had to write something similar to this in college for a project I was working on. Unfortunatly I had control over the sending socket so I inserted a length of message field as part of the protocol. However I think that a similar approach may benefit you. How I approached my solution was I would send something like 5HELLO so first I'd see 5 and know I had message length 5 and therefor the message I needed was 5 characters. However if on my async read i only got 5HE i would see that I have message length 5 but I was only able to read 3 bytes off the wire (Let's assume ASCII characters). Because of this I knew I was missing some bytes and stored what I had in fragment buffer. I had one fragment buffer per socket therefor avoiding any synchronization problems. The rough process is. Read from socket into a byte array record how many bytes was read Scan through byte by byte until you find a newline character (this becomes very complex if you're not receiving ascii characters but characters that could be multiple bytes you're on you're own for that) Turn you're frag buffer into a string and append you're read buffer up until the new line to it. Drop this string as a completed message onto a queue or it's own delegate to be processed. (you can optimize these buffers by actually having you're read socket writing to the same byte array as you're fragment but that's harder to explain) Continue looping through every time we find a new line create a string from the byte arrange from a recorded start / end position and drop on queue / delegate for processing. Once we hit the end of our read buffer copy anything that's left into the frag buffer. Call the BeginRead on the socket which will jump to step 1. when data is available in the socket. Then you use another Thread to read you're queue of incommign messages or just let the Threadpool handle it using delegates. And do whatever data processing you have to do. Someone will correct me if I'm wrong but there is very little thread synchronization issues with this since you can only be reading or waiting to read from the socket at any one time so no worry about locks (except if you're populating a queue I used delegates in my implementation). There are a few details you will need to work out on you're own like how big of a frag buffer to leave if you receive 0 newlines when you do a read the entire message must be appended to the fragment buffer without overwriting anything. I think it ran me about 700 - 800 lines of code in the end but that included the connection setup stuff negotiation for encryption and a few other things. This setup performed very well for me; I was able to perform up to 80Mbps on 100Mbps ethernet lan using this implementation a 1.8Ghz opteron including encryption processing. And since you're tied to the socket the server will scale since multiple sockets can be worked on at the same time. If you need items processed in order you'll need to use a queue but if order doesn't matter then delegates will give you very scalable performance out of the threadpool. Hope this helps not meant to be a complete solution but a direction in which to start looking. *Just a note my implementation was down purely at the byte level and supported encryption I used characters for my example to make it easier to visualize. Yes I've implemented an approach similar to this already but I don't like it. It's too messy and complex for my tastes that's why I'm asking for suggestions here. I like Noldorin's approach it has the elgance and reuse of existing framework code i desire.  That's quite an interesting question. The solution for me in the past has been to use a separate thread with synchronous operations as you propose. (I managed to get around most of the problems with blocking sockets using locks and lots of exception handlers.) Still using the in-built asynchronous operations is typically advisable as it allows for true OS-level async I/O so I understand your point. Well I've gone and written a class for accomplishing what I believe you need (in a relatively clean manner I would say). Let me know what you think. using System; using System.Collections.Generic; using System.IO; using System.Text; public class AsyncStreamProcessor : IDisposable { protected StringBuilder _buffer; // Buffer for unprocessed data. private bool _isDisposed = false; // True if object has been disposed public AsyncStreamProcessor() { _buffer = null; } public IEnumerable<string> Process(byte[] newData) { // Note: replace the following encoding method with whatever you are reading. // The trick here is to add an extra line break to the new data so that the algorithm recognises // a single line break at the end of the new data. using(var newDataReader = new StringReader(Encoding.ASCII.GetString(newData) + Environment.NewLine)) { // Read all lines from new data returning all but the last. // The last line is guaranteed to be incomplete (or possibly complete except for the line break // which will be processed with the next packet of data). string line prevLine = null; while ((line = newDataReader.ReadLine()) != null) { if (prevLine != null) { yield return (_buffer == null ? string.Empty : _buffer.ToString()) + prevLine; _buffer = null; } prevLine = line; } // Store last incomplete line in buffer. if (_buffer == null) // Note: the (* 2) gives you the prediction of the length of the incomplete line // so that the buffer does not have to be expanded in most/all situations. // Change it to whatever seems appropiate. _buffer = new StringBuilder(prevLine prevLine.Length * 2); else _buffer.Append(prevLine); } } public void Dispose() { Dispose(true); GC.SuppressFinalize(this); } private void Dispose(bool disposing) { if (!_isDisposed) { if (disposing) { // Dispose managed resources. _buffer = null; GC.Collect(); } // Dispose native resources. // Remember that object has been disposed. _isDisposed = true; } } } An instance of this class should be created for each NetworkStream and the Process function should be called whenever new data is received (in the callback method for BeginRead before you call the next BeginRead I would imagine). Note: I have only verified this code with test data not actual data transmitted over the network. However I wouldn't anticipate any differences... Also a warning that the class is of course not thread-safe but as long as BeginRead isn't executed again until after the current data has been processed (as I presume you are doing) there shouldn't be any problems. Hope this works for you. Let me know if there are remaining issues and I will try to modify the solution to deal with them. (There could well be some subtlety of the question I missed despite reading it carefully!) Ah thanks Noldorin. Works. =] Thanks for such a great solution. Answer is yours. I am interested in using this technique but am not familiar with iterators/IEnumerable. Can you please provide an example usage? Thanks. I point you to the MSDN docs on C# iterators: http://msdn.microsoft.com/en-us/library/65zzykke(VS.80).aspx. Hopefully that should make some sense. This is an interesting solution. I too have found Iterators to be useful but this was not a solution my mind would have come up with. I like it. Can you explain why you need to implement IDispose? I've been told that calling GC.Collect() is bad practice and can result in poor performance. Are you concerned about rapid allocations within a short time exhausting the heap? Yeah iterators are handy things. In this case you could just as well do it with a generic List though it may not look so nice of course. If you want to deal with the result as a List/Array it's trivial to convert to those types anyway and the implementation is still simpler. Regarding the use of IDisposable it is possibly not *necessary*. The null allocation followed by the GC.Collect is used to insure that the memory for its buffer is freed up immediately. Depending on how long lines can get this may or may not be much of an issue. (contd) The Dispose method (and thus GC.Collect) should only be called when the associated connection/NetworkStream is closed which shouldn't be too often so I wouldn't worry about performance. (There seems to be an alternative however: http://dotnettipoftheday.org/tips/dispose_stringbuilder.aspx) Also your comments indicate that the last line is guaranteed to be incomplete. This is not true since it is possible for it to be complete (including final newline). For instance after a batch of data is sent the last line will be complete. It's also possible it just might accidentally be. The code *seems* to work even if the last line is newline terminated but maybe i'm missing something. Is there a potential problem if the last line is newline terminated? @Noldorin I read up on iterators and understand how they work now. How would I connect my Stream instance to your ASyncStreamProcessor class though? That's the ""trick"". Because a new line is appended to all new data before processing it would treat new data ending in a line break by reading the last complete line then a zero-length (string.Empty) incomplete line which is stored into the buffer all finally followed by the null line. In fact to see truly see what's going on I recommend you create some test data (ending with a line break/an incomplete line) and step through the code of the method in the two cases. Anyway this approach has got me thinking - I'll have to test it against my thread-with-sync-reads at some point. You're welcome. Glad it did the job for you. You call BeginRead/EndRead in a loop calling the Process method on each callback. Read up on using asynchronous methods of TcpClient - it's not too complicated. Noldorin nice idea and good work on the class but you should really remove the IDisposable (unnecessary since it only manages a StringBuilder) and otherwise definitely remove the GC.Collect. Yeah so you may have a point there. I just thought it would be advisable to deallocate such a potentially large chunk of memory. Perhaps it would be better to use the solution in http://dotnettipoftheday.org/tips/dispose_stringbuilder.aspx or simply not bother even. Thanks for the comment. Yes instead of _buffer = null; you could set _buffer.Length = 0; this would recycle your (1) StringBuilder and be more efficient in all cases."
839,A,"How to create a Boost.Asio socket from a native socket? I am merely trying to create a boost ip::tcp::socket from an existing native socket. In the assign function the first parameter must be a ""protocol_type"" and the second must be a ""native_type"" but it never explains what these are or gives an example of its use. I'm guessing the second should be the socket descriptor but I'd really appreciate clarification. void SendData (int socket std::string message) { boost::asio::io_service ioserv; boost::asio::ip::tcp::socket s(ioserv); s.assign(/* what goes here? */ /* ..and here? */); s.send(boost::asio::buffer(message)); } ""Native type"" is just the socket handle in this case the int stored in ""socket"". ""Protocol type"" is the the protocol. For a TCP over standard IP using stream socket this would be the return value from boost::asio::ip::tcp::v4(). Substitute as appropriate for datagram sockets IPv6 etc. So: s.assign(boost::asio::ip::tcp::v4() socket); Adjusted as appropriate for what you're trying to do."
840,A,"Selection between ACE & Boost for learning I am an intermediate c++ programmer and done some work using ACE now I want to learn one of those Libraries thoroughly to progress in to my career. That why I need your kind help to make a decision that what should I learn first. And also please consider my destinations are to be an expert network programmer and Protocol designer. Thanks for your time and kind answers in advance. This might not be the answer you're looking for but I would strongly suggest that you don't artificially restrict yourself too much when it comes to career goals. Figure out how long you do expect your career as a programmer to continue and then ask yourself if you (a) can really see yourself doing network programming only for that amount of time and (b) if you well and truly believe that the one library you select for your in-depth knowledge will be able to fulfil the needs you have now for the rest of your career without stagnating your career. What will sustain your programming career in the long term is not the in-depth knowledge of a single library or two but your overall ability as a programmer. Libraries are tools (unless you are a library designer) in the same way that programming languages are tools (unless you are a language designer) and one mark of a good programmer is her or his abilities to select the appropriate tool for the task. With all that out of the way I do recommend that every C++ programmer is at least familiar with the fact that boost exists and some of the core libraries like the smart pointer library regular expressions etc. I would not expect anybody working for me to be an expert in all facets of Boost but I do expect even fairly inexperienced C++ programmers to know where they can find it and that they will be better off using code from Boost rather than trying to say write their own pooled memory allocator. The examples I gave might not look like they are directly applicable to network programming in the most narrow sense but they will certainly be needed in most programs of a non-trivial size. Another good reason to keep up with Boost is that a lot of the techniques that eventually will/might make it into the C++ standard library originate from Boost. Keeping an on where Boost is going will allow you to keep an eye on certain developments in the C++ community as new usage idioms are still being developed in C++; the language and its canonical usage is not ""fixed"" at least not as of now and again this is something you will have to keep up with if you are planning on a longer term career as a C++ programmer."
841,A,"Extending packet header from within a Netfilter hook I want to prepend IP header on an existing IP packet while inside NF_HOOK_LOCAL_OUT. The issue I face is that the skb expansion functions (such as copy/clone/expand/reallocate header) allocate a new sk_buff. We can not return this newly allocated pointer since netfilter hook function no longer (kernel version 2.6.31) passes the skb pointer's address (passes by value). How I solved the issue is as follows: 1. I got a new skb using skb_header_realloc(). This copies all the data from skb. 2. I modified the new skb (call it skb2) to prepend the new IP header set appropriate values in the new IP header. 3. Replace the contents of the original skb (passed in the Netfilter hook function) with the contents of the skb2 using skb_morph(). Returned NF_ACCEPT. Is this the only way of achieving what I intended to? Is there a more efficient solution? Are there other use cases of skb_morph (besides the IP reassembly code)? This works for me in 2.6 kernels: ... struct iphdr* iph; if (skb_headroom(skb) < sizeof(struct iphdr)) if (0 != pskb_expand_head(skb sizeof(struct iphdr) - skb_headroom(skb) 0 GFP_ATOMIC)) { printk(""YOUR FAVOURITE ERROR MESSAGE""); kfree_skb(skb); return NF_STOLEN; } iph = (struct iphdr*) skb_push(skb sizeof(struct iphdr)); //Fill ip packet return NF_ACCEPT; Hope it helps. I forgot to add something to this post. I used kfree_skb because I am returning NF_STOLEN. You can return NF_DROP but you cannot free skb. NF_DROP will tell the application that the packet failed to be sent. However NF_STOLEN does not. Thank you Fred. It works with a minor correction: pskb_expand_head retruns 0 on success. So the check should be changed to if (0 != pskb_expand_head(skb sizeof(struct iphdr) - skb_headroom(skb) 0 GFP_ATOMIC))."
842,A,tcp application exit will send FIN always? When a TCP application exits it will send a FIN packet. Consider a tcp client which get connected to a always listening server(server never exits). if the tcp client is exiting abruptly after few exchange of packets will it always send a FIN packet to the server? Thx! Under normal operation  a FIN will be sent yes. Here's a few cases where a FIN is not going to be sent. Someone yanks out the network cable of the client. The client gets nuked The FIN packets are dropped on the way. The OS on the kernel crashes hard. Yes even if the application does not close the connection the OS will do it for you when/after the application exits and a FIN will be sent(or a RST depending on the connection state e.g. if there's unprocessed incoming data ) I think close call is necessary to send a FIN correct? Even if the client does not call close call wil it be sent?
843,A,"Possible to sit on the network and receive a TCP stream/UDP datagrams? Has anyone out there done the work of sitting on top of a packet capture interface (like jpcap) with an implementation of UDPSocket (for UDP datagrams) and InputStream (for TCP streams)? I suppose it wouldn't be too hard to do given the callback API in jpcap but has anyone out there already done it? Are there any issues with doing this (do I have to figure out how to reassemble a TCP stream myself for example?) I have not done this particular thing but I do do a lot of work with parsing captured packets in C/C++. I don't know if there exist Java libraries for any of this. Essentially you need to work your way up the protocol stack starting with IP. The pcap data starts with the link-level header but I don't think there's much in it that you're concerned about other than ignoring non-IP packets. The trickiest thing with IP is reassembling fragmented datagrams. This is done using the More Fragments bit in the Flags field and the Fragment Offset field combined with the Identification field to distinguish fragments from different datagrams Then you use the Protocol field to identify TCP and UDP packets and the Header Length field to find the start of the corresponding header. The next step for both TCP and UDP is demultiplexing separating out the various connections in the captured packet stream. Both protocols identify connections (well UDP doesn't have connections per se but I don't have a better word handy) by the 4-tuple of the source and destination IP address and the source and destination port so a connection would be a sequence of packets that matches on all 4 of these values. Once that's done for UDP you're just about finished unless you want to check the checksum. The Length field in the UDP header tells you how long the packet is; subtract 8 bytes for the header and there's your data. TCP is somewhat more complicated as you do indeed have to reassemble the stream This is done using the sequence number in the header combined with the length. The sum of these two tells you the next sequence number in the stream. Remember that you're keeping track of the traffic in two directions. (This is a lot easier than writing an actual TCP implementation as then you have to implement the Nagle algorithm and other minutiae.) There's a lot of information on the net about the header formats; google ""IP header"" for starters. A network analyzer like Wireshark is indispensable for this work as it will show you how your captured data is supposed to look. Indeed as Wireshark is open source you can probably find out a lot by looking at how it does things I'll add one more thing: I find it easiest to pull the packet header into a C-style struct that replicates the layout of the packet. This requires a preprocessor directive to prevent internal padding of the struct and you have to remember to convert 16- and 32-bit values to host byte order using ntohs() and ntohl() respectively. I don't know if a similar approach is workable in Java  Tcp reassembly can be done with JNetPcap. Here is a complete example: final String SOME_PORT = 8888; StringBuilder errbuf = new StringBuilder(); Pcap pcap = Pcap.openOffline(""/dir/someFile.pcap"" errbuf); //Can be replace with .openLive(...) if (pcap == null) { System.err.printf(""Error: ""+errbuf.toString()); return; } //Handler that receive Tcp Event one by one AnalyzerListener<TcpStreamEvent> handler = new AnalyzerListener<TcpStreamEvent>() { @Override public void processAnalyzerEvent(TcpStreamEvent evt) { JPacket packet = evt.getPacket(); Tcp tcp = new Tcp(); if (packet.hasHeader(tcp)) { //Limiting the analysis to a specific protocol if (tcp.destination() == SOME_PORT || tcp.source() == SOME_PORT) { String data = new String(tcp.getPayload()); System.out.println(""Capture data:{""+data+""}""); } } } }; TcpAnalyzer tcpAnalyzer = JRegistry.getAnalyzer(TcpAnalyzer.class); tcpAnalyzer.addTcpStreamListener(handler null); //Starting the capture pcap.loop(Pcap.LOOP_INFINATE JRegistry.getAnalyzer(JController.class) null);"
844,A,"Need a good website URL to test against I need a URL to just test basic http connectivity. It needs to be consistent and: Always be up Never change drastically due to IP or user agent. (IE: 301 Location redirect/ huge difference in content... minor would be tolerable) The URL itself has a consistent content-length. (IE: it doesn't vary from by 2kb at most ever) A few examples yet none match all 3 criteria: One example of always up: www.google.com (yet it 301 redirects based on IP location). Another good one is http://www.google.com/webhp?hl=en. but the problem there is that based on a given holiday the content-length can really vary. I want to test if data can go over the pipe. And to be more specific if I can get at least 5kb. If you are testing http connectivity why not just ping www.google.com? Did you try my last suggestion? Will always be the same from anywhere in the world. A comment on my answer You could create your own website at least then you will know that the content will never change and you you will know when its down? Hm I suppose I would need to find a host with good uptime yes then that would match all criteria quite well. Relying on a third party for testing purposes is a bit selfish and rude unless you have an agreement with them. Your own site is definitely the way to go. I do not think it is rude if you only test connectivity. I think google can take it :) You could also just get the content from google.com (whether it redirects or not) and look for the existence some content that will be unique to google eg ""window.google"" (found doing a view source) Another idea could be to check more than one site that you would expect to be up?  It might not be obvious; but http://example.com http://example.net and http://example.org are actual real sites; the might suit your needs. Edit: I'm not sure about the specifics of their uptime stats and IP address but I imagine these things as well as the contents are completely and utterly static. I like the fact they exist! and one can assume that they will never change until they change. They don't look as if they've changed a lot in the last 10 years; no reason to think the next 10 would be different.  Why not go to something like http://www.google.com/ncr? Then you won't be redirected Looks good from here (America) but they do not redirect if you are from a non-english speaking country...? Try my edit! :) Always redirect to the english version of www.google.com (even in Sweden where I am) @Zombies Or you could try http://www.google.se/asdasdasdasd. I geuss that would always be the same unless google creates that page. Highly unlikely tho I think I will go with www.google.com/ncr . @>Zombies Great I think that my last comment is more fitting tho"
845,A,"how to capture packets in a Windows application that must be installed as non-admin? Is it possible to write a Windows app that can capture packets on the PC such that this application can be installed/run as non-admin? If yes what would be the approach e.g. which language which API/Library to use etc (e.g. would it be with the Windows Sockets 2 (Winsock) library?) I've looked at Network Monitor API's however the issue here is that NM needs to be installed and this requires ""admin"" access (to get the driver in place). Note what I'm after is to capture/monitor packets to sum frame sizes for all packets of a particular type [e.g. HTTP] going out to a specific set of IPs. thanks that's what I'm trying to ascertain - whether it's a hard constraint within Windows or not - or perhaps the answer may be different between XP Vista Windows 7? You would hope that an application without administrative rights could not gain access to the traffic other applications send. A serious security violation otherwise. Not sure this helps but you can run any application with different privileges than that of the currently logged in user. See ""Run as"" tools. thanks - but main issue here is for users with non-admin access and I know things like Network Monitor API does require admin access for the install of the drivers that get used to allow the packet capture"
846,A,"Why am I getting extra bits in my recv() calls with libnfqueue? I'm having problems with the following code. I receive my data with no problems however that data is loaded with extraneous bits! For example the following code grabs all traffic directed to it from nfqueue and prints out each byte followed by a newline. At first the data is exactly what I'd expect but then there are lines that have seeming 4 bytes on them! int main(int argc char** argv) { int fd; ssize_t rv; char buf[4096]; struct nfq_handle* h; struct nfq_q_handle* qh; h = nfq_open(); if (!h) { fprintf(stderr ""error during nfq_open()\n""); exit(1); } if (nfq_unbind_pf(h AF_INET) < 0) { fprintf(stderr ""error during nfq_unbind_pf()\n""); exit(1); } if (nfq_bind_pf(h AF_INET) < 0) { fprintf(stderr ""error during nfq_bind_pf()\n""); exit(1); } printf(""Binding to queue 0...\n""); qh = nfq_create_queue(h 0 &cb NULL); if (!qh) { fprintf(stderr ""error during nfq_create_queue()\n""); exit(1); } printf(""Copying packets...\n""); if (nfq_set_mode(qh NFQNL_COPY_PACKET 0xffff) < 0) { fprintf(stderr ""error during nfq_set_mode()\n""); exit(1); } fd = nfq_fd(h); memset(buf 0 4096); while ((rv = recv(fd buf sizeof(buf) 0)) && rv >= 0) { for (int i = 0; i < rv; i++) printf(""%02x\n"" *(buf+i)); printf(""\n\n""); nfq_handle_packet(h buf rv); } nfq_destroy_queue(qh); nfq_close(h); } Output: ... 50 10 39 08 48 ffffffa4 00 00 ... I couldn't find anything on the internet or in bug trackers similar to this problem. Where can I begin to diagnose it? Why are there all these extra bits in my data? How can I fix it? I did a comparison with the output from tcpdump and they aren't appearing there either. It seems that buffer contains char with value exceeding 128 and the signed/unsigned conversion error takes place. I've calculated the value of your byte - it is 164. Try to convert your bytes into unsigned chars before passing them to printf: printf(""%02x\n"" *(unsigned char*)(buf+i)); Wow silly me. Turns out this is exactly what the problem is. Thanks! While this works on twos complement machines the correct and portable way to do it is `*(unsigned char *)(buf+i)` i.e. cast the pointer type *before* dereferencing rather than dereferencing with the wrong type (signed `char`) and then casting that to the right type."
847,A,"What does InetAddress.isSiteLocalAddress() actually mean? Here is some code to determine the local host name that is supposed to work on a multi-homed box:  /** * Work out the first local host name by iterating the network interfaces * * @return * @throws SocketException */ private String findFirstLocalHostName() throws SocketException { Enumeration<NetworkInterface> ifaces = NetworkInterface.getNetworkInterfaces(); while (ifaces.hasMoreElements()) { NetworkInterface iface = ifaces.nextElement(); Enumeration<InetAddress> addresses = iface.getInetAddresses(); while (addresses.hasMoreElements()) { InetAddress add = addresses.nextElement(); if (!add.isLoopbackAddress() && add.isSiteLocalAddress()) { return add.getHostName(); } } } throw new RuntimeException(""Failed to determine local hostname""); } Does the call to isSiteLocalAddress introduce a bug? I can't find any useful information about this method but I have a feeling that it relates to IP v 6 only and is deprecated. For clarity I didn't mean that the method was deprecated... just the notion of ""site local"" address in IPv6 as per http://www.ietf.org/rfc/rfc3879.txt Looking at the implementation... For an Inet4Address it checks to see if it's one of the RFC1918 ""unrouteable"" addresses: 10.0.0.0/8 172.16.0.0/12 192.168.0.0/16. For an Inet6Address it checks the first two octets to see if it's a real ""site local"" address.  The method is definitely not deprecated and it's definitely not just used in IPv6. In IPv4 there are 3 network address ranges that are defined for site-local addresses: 10/8 172.16/12 and 192.168/16. Reading Inet4Address.isSiteLocalAddress() shows that addresses from exactly those 3 networks will return true on those methods. Effectively this tells you if the address you have is definitely not a public one (note that even if this method returns false the address might still not be public). see [here](http://books.google.co.in/books?id=NyxObrhTv5oC&lpg=PT187&dq=InetAddress.isSiteLocalAddress()&pg=PT186#v=onepage&q&f=false). it returns true if the address is an IPv6 site-local address. @John: yes I know that there is a similar meaning in IPv6 as well but I don't know the specifics of it. But since the question implied that it's IPv6-only I wanted to clarify that aspect. (By the way I can't read the page you linked to).  As far as I know the isSiteLocalAddress method is not deprecated. isSiteLocalAddress - Explanation indicating if the InetAddress is a site local address; or false if address is not a site local unicast address. The InetAddress even have two direct subclasses; Inet4Address and Inet6Address The best bet is to read the JavaDocs. Which version of the JDK are you using?  'Site local' is a deprecated name for private IP space. (Some nuances but basically right.) See RFC 1918."
848,A,"How to establish ethernet connection between laptop and embedded device I'm designing a device which will be connected to a computer using ethernet. It already has a mac controller built in. When I attach the device all that happens is the computer broadcasts a bunch of DHCP discover packets and some other packets I guess in an attempt to find the device and establish the connection. I assume I need to make my device respond at this point with some sort of acknowledgement packets but I'm not sure what? Has anyone done something like this before? Thanks I'm using wireshark to see the packets. The output looks something like this:  time source destination protocol info 1 0.000000 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xc82a69f 2 4.000064 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xc82a69f 3 10.688469 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xb452266b 4 14.690625 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xb452266b 5 22.690576 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xb452266b 6 38.690605 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xb452266b 7 62.652821 my_ip XXX.XXX.255.255 BROWSER Local Master Announcement MYLAPTOP Workstation Server Print Queue Server NT Workstation Potential Browser Master Browser 8 65.555281 my_ip XXX.XXX.255.255 BROWSER Domain/Workgroup Announcement MY NT Workstation Domain Enum 9 352.692192 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xa23d42a4 10 356.692376 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xa23d42a4 11 364.692421 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xa23d42a4 12 381.692442 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xa23d42a4 13 665.557507 my_ip XXX.XXX.255.255 BROWSER Domain/Workgroup Announcement MY NT Workstation Domain Enum 14 686.724951 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xbe3a7bdb 15 691.724307 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xbe3a7bdb 16 698.724276 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xbe3a7bdb 17 715.724291 0.0.0.0 255.255.255.255 DHCP DHCP Discover - Transaction ID 0xbe3a7bdb 18 783.295682 my_ip XXX.XXX.255.255 BROWSER Local Master Announcement MYLAPTOP Workstation Server Print Queue Server NT Workstation Potential Browser Master Browser 19 908.920831 my_ip XXX.XXX.255.255 BROWSER Get Backup List Request 20 908.920940 my_ip XXX.XXX.255.255 NBNS Name query NB MY<1b> Are you setting a static IP or using DHCP? I would disable DHCP and use a static IP to help eliminate packets that arent needed. It looks like what is happening is that your device is trying to get an IP number from a DHCP server but there is no DHCP server responding. EDIT: I say it is the device because if I am reading the trace right your computer already has an IP number. It might be instructive to look at the underlying Ethernet frames -- I believe Wireshark lets you do that.  ""DHCP Discover"" means your PC is asking any device connected to the Ethernet Link for an IP address it could use. Obviously your device is not a DHCP-server. Instead as a first step you should assign both your pc and your device static IP addresses preferably from the private IP ranges like 192.168.x.x. If your device has a working IP-stack it should then answer to pings. You can ignore the BROWSER and NBNS packets. Thats just Windows trying to talk to another Windows. Correction: It's not completely clear which device broadcasts the DHCP Discover it's most likely your PC but it could as well be the device (if it has a built-in DHCP-Client) or any other device on the same Ethernet link. You need to include the MAC Adresses in the Wireshark dump to be sure.  If your computer is broadcasting DHCP packets it's because it needs a DHCP server to get an address. It is not looking for your device. Did you take the computer off the main network and hook it up on a private network with your device? Your computer knows nothing about your embedded device and will not try and make a connection with it. The first thing you need to do is decide what the connection will do and then write or obtain software to create the connection. Typically the embedded device will create a server socket and wait for a client (you computer) to connect to it. Telnet is an example of a client/server tcp connection. It is apparent from your post that you are unsure about what your embedded device is doing. You need to elaborate more about your embedded device and how it should function. you are correct. I know what I want my device to do but I don't have a great understanding of transmitting data using ethernet. Basically I will be sending video from my device to be stored on a pc. Any suggestions wil be extremely appreciated. Sending video is a wide open question. You have many choices. 1) Streaming on demand using TCP and 1 stream per client. 2) UDP streaming either unicast or multicast. You can use either TCP or UDP to stream video data. You can stream using RTP or raw data. MPEG-2 can be streamed raw data like MPEG-2 Transport stream because the timing information is embedded. Video like MPEG-4 or H.264 can be streamed using RTP which adds time stamps. But they also require other mechanisms like SDP files or RTSP to get essential configuration data. A player such as VideoLan Client (VLC) is an excellent tool for the client side to test your embedded device. Search for RFCs that describe RTP and RTSP as a starting point."
849,A,AutoDisovery is selecting the wrong network interface I wrote a simple application using REALBasic The issue is that AutoDiscovery is binding to the wrong interface. The machine has 1 physical and two virtual NIC cards. The virtual ones are there because of VMWare How can I bind to the physical card? Are you looking at the available interfaces using NetworkInterface? You should be able to list and select the available interfaces. You should then be able to set the Socket.NetworkInterface property since the AutoDiscovery class is a sub of the SocketCore class. Would you setup the Interface in App.Open ? Probably as good a place as anywhere else.
850,A,Data structure to keep track of active connections When I want to keep track of active connections is it better to save them into linked list or directly to some array where index will represent ID of the connection? I want to prevent possible race condition issues for example: number of connections in the list is big someone start to search for a connection which is at the end of the list meanwhile during the search the connection is detached The structure should have a connection handle (SOCKET) a reference count and a flag denoting that connection should be closed and the object deleted as soon as possible. Whatever the container is it must have synchronization mechanism (critical section) for search/insert/remove and GetReference/Release logic. GetReference function should return NULL if closing flag is raised. You might need more that one GetReference depending on search conditions (and connection object should have all those values that can help it being found in container). One of GetReference functions can create new object if it does not exist in container. Release function should close the connection and delete the object from container if closing flag is raised and reference count is dropped to zero. However Release must not close the connection within critical section because that can be time consuming operation (depends on graceful shutdown and lingering option). Release should enter CS decrease reference count if it is zero then leave CS close connection enter CS again and remove object from container and delete it. Because the closing flag is raised the reference count will not raise between two CS's.
851,A,"Trying to anticipate behaviour of .NET TcpClient.GetStream() regarding ephemeral port usage in multithreaded environment I'm wondering if anyone has experience of this... I'm in the process of multithreading a currently serial process that makes request via a .NET TCPClient to a remote server. We know connect to the remote server on the same port via multiple threads as we have several instances of the serial processing application running connecting to the remote server on the same port. However I am less sure what will happen if I make multiple TCPClient.GetStream() requests using multiple threads in the same application - with regard to the local ephemeral (short lived) port. If the remote server knows that it has to send responses back to the ephemeral port from which the request originated are there any guarantees that each thread will use a differing ephemeral port for the request - or is there any chance of use of the same port? For context this is a card processing application. Hope that makes sense. If you are really trying to max the number of connections consider using the APM methods instead of allocating a thread per conversation. You don't need to worry about this unless you're making so many connections that you get port starvation. The Socket related APIs abstract away the issue of ephemeral ports and these need be of no concern to you (regardless of threads/processes) unless you've got thousands of connections that aren't clearing quick enough. In my experience it's only spiders that run into these kinds of problems. In this case it would be considerably easier to manage if all the connections were made from a single process. On newer Windows the port range is 49152-65535 (or ~16k concurrent connections) on earlier Windows considerably less. Thanks thats what I was hoping. Can you elaborate on your answer ""You don't need to worry about this"" Thanks very much."
852,A,"benefits of learning python network programming? I was looking at a book on python network programming and i wanted to know what would be the benefits to learning python network programming comprehensively? This would be in the context of being able to develop some really cool ground breaking web apps. I am a python newbe so all opinions woul be appreciated. Kind Regards Network programming != web applications. Newbie != cool ground breaking apps. newbee does not mean asking-clear-questions. what problem are you trying to solve? Is email yahoo messenger streaming of files not an example of using networking technology? The reason why i ask is because these tpoics are covered in the python network programming book. I don't have a specific project in mind i am just contemplating whether it will add anything to my ability to build web applications. ""Network programming"" isn't about ""cool web apps"". It's more about creating servers and creating clients that talk to servers. It's about sockets tcp/ip communication XMLRPC http protocols and all the technologies that let two computers talk to each other. If all you're interested in is web apps learning network programming won't benefit you a whole lot. You dont need to know how to design a drivetrain or the top/bottom end of an engine to slap a car together that would get you from point a to point b. But if you did you'd be able to build it way more efficient and/or squeeze better times off a quarter mile. I am basically interested in knowing if i will get any synergy from knowing web devlopment and network programming and how well can it be combined to develop something cool.  If you want to develop wed apps than you should rather focus on web frameworks like Django or Pylons. I already use web2py but i was just wondering wether going through a book on python network programming would be of any benefit as it does cover web programming and sockets transfering files etc.... If you want to make cool web apps then I think it's better to learn something about front end - maybe `jQuery` or something similar?  ""python network programming"" isn't any special kind of network programming. It sounds like if you had a better grasp on network programming you would be able to see where python would fit in to your overall design. And instead of reading a generic book about it you would dig through the python API's and go from there. The cool thing about python is that it's a huge collection of libraries which are optimized to each task. At work we use python to do all our server-side heavy lifting. We then use jquery and the Objective-J based Cappuccino to present an interface to the user."
853,A,"Variable sized packet structs with vectors Lately I've been diving into network programming and I'm having some difficulty constructing a packet with a variable ""data"" property. Several prior questions have helped tremendously but I'm still lacking some implementation details. I'm trying to avoid using variable sized arrays and just use a vector. But I can't get it to be transmitted correctly and I believe it's somewhere during serialization. Now for some code. Packet Header class Packet { public: void* Serialize(); bool Deserialize(void *message); unsigned int sender_id; unsigned int sequence_number; std::vector<char> data; }; Packet ImpL typedef struct { unsigned int sender_id; unsigned int sequence_number; std::vector<char> data; } Packet; void* Packet::Serialize(int size) { Packet* p = (Packet *) malloc(8 + 30); p->sender_id = htonl(this->sender_id); p->sequence_number = htonl(this->sequence_number); p->data.assign(size'&'); //just for testing purposes } bool Packet::Deserialize(void *message) { Packet *s = (Packet*)message; this->sender_id = ntohl(s->sender_id); this->sequence_number = ntohl(s->sequence_number); this->data = s->data; } During execution I simply create a packet assign it's members and send/receive accordingly. The above methods are only responsible for serialization. Unfortunately the data never gets transferred. Couple of things to point out here. I'm guessing the malloc is wrong but I'm not sure how else to compute it (i.e. what other value it would be). Other than that I'm unsure of the proper way to use a vector in this fashion and would love for someone to show me how (code examples please!) :) Edit: I've awarded the question to the most comprehensive answer regarding the implementation with a vector data property. Appreciate all the responses! Why do you need the `struct Packet`? This'll cause the compiler to crib about an already defined identifier. @dirkently It's only defined once and it's needed. Could you elaborate? Thanks! So `Packet` is a `typedef` for a `struct` and a `class` as well? This is definitely going to cause problems. @Charles Bailey Ah. I've modified the class name for this post it works OK. This cast is very dangerous as you have allocated some raw memory and then treated it as an initialized object of a non-POD class type. This is likely to cause a crash at some point. Packet* p = (Packet *) malloc(8 + 30); Looking at your code I assume that you want to write out a sequence of bytes from the Packet object that the seralize function is called on. In this case you have no need of a second packet object. You can create a vector of bytes of the appropriate size and then copy the data across. e.g. void* Packet::Serialize(int size) { char* raw_data = new char[sizeof sender_id + sizeof sequence_number + data.size()]; char* p = raw_data; unsigned int tmp; tmp = htonl(sender_id); std::memcpy(p &tmp sizeof tmp); p += sizeof tmp; tmp = htonl(sequence_number); std::memcpy(p &tmp sizeof tmp); p += sizeof tmp; std::copy(data.begin() data.end() p); return raw_data; } This may not be exactly what you intended as I'm not sure what the final object of your size parameter is and your interface is potentially unsafe as you return a pointer to raw data that I assume is supposed to be dynamically allocated. It is much safer to use an object that manages the lifetime of dynamically allocated memory then the caller doesn't have to guess whether and how to deallocate the memory. Also the caller has no way of knowing how much memory was allocated. This may not matter for deallocation but presumably if this buffer is to be copied or streamed then this information is needed. It may be better to return a std::vector<char> or to take one by reference or even make the function a template and use an output iterator.  i think you might want to do like this: ` struct PacketHeader { unsigned int senderId; unsigned int sequenceNum; }; class Packet { protected: PacketHeader header; std::vector<char> data; public: char* serialize(int& packetSize); void deserialize(const char* dataint dataSize); } char* Packet::serialize(int& packetSize) { packetSize = this->data.size()+sizeof(PacketHeader); char* packetData = new char[packetSize]; PacketHeader* packetHeader = (PacketHeader*)packetData; packetHeader->senderId = htonl(this->header.senderId); packetHeader->sequenceNum = htonl(this->header.sequenceNum); char* packetBody = (packetData + sizeof(packetHeader)); for(size_t i=0 ; i<this->data.size() ; i++) { packetBody[i] = this->data.at(i); } return packetData; } void deserialize(const char* dataint dataSize) { PacketHeader* packetHeader = (PacketHeader*)data; this->header.senderId = ntohl(packetHeader->senderId); this->header.sequenceNum = ntohl(packetHeader->sequenceNum); this->data.clear(); for(int i=sizeof(PacketHeader) ; i<dataSize ; i++) { this->data.push_back(data[i]); } } ` those codes does not include bound checking and free allocated data don't forget to delete the returned buffer from serialize() function and also you can use memcpy instead of using loop to copy byte per byte into or from std::vector. most compiler sometime add padding inside a structure this would cause an issue if you send those data intact without disable the padding you can do this by using #pragma pack(1) if you are using visual studio disclaimer: i don't actually compile those codes you might want to recheck it Thanks Uray for the example helped a lot! @uray This was a great tutorial and a great post. Thanks so much!!  I think the problem centres around you trying the 'serialise' the vector that way and you're probably assuming that the vector's state information gets transmitted. As you've found that doesn't really work that way as you're trying to move an object across the network and things like pointers etc don't mean anything on the other machine. I think the easiest way to handle this would be to change Packet to the following structure: struct Packet { unsigned int sender_id; unsigned int sequence_number; unsigned int vector_size; char data[1]; }; The data[1] bit is an old C trick for variable length array - it has to be the last element in the struct as you're essentially writing past the size of the struct. You have to get the allocation for the data structure right for this otherwise you'll be in a world of hurt. Your serialisation function then looks something like this: void* Packet::Serialize(std::vector<char> &data) { Packet* p = (Packet *) malloc(sizeof(Packet) + data.size()); p->sender_id = htonl(this->sender_id); p->sequence_number = htonl(this->sequence_number); p->vector_size = htonl(data.size()); ::memcpy(p->data data[0] size); } As you can see we'll transmit the data size and the contents of the vector copied into a plain C array which transmits easily. You have to keep in mind that in your network sending routine you have to calculate the size of the structure properly as you'll have to send sizeof(Packet) + sizeof(data) otherwise you'll get the vector cut off and are back into nice buffer overflow territory. Disclaimer - I haven't tested the code above it's just written from memory so you might have to fix the odd compilation error. Thanks for showing the code to use the variable array ""hack"" but I gave the ""answer"" to the solution with vectors.  This trick works with a C-style array at the end of the struct but not with a C++ vector. There is no guarantee that the C++ vector class will (and it most likely won't) put its contained data in the ""header object"" that is present in the Packet struct. Instead that object will contain a pointer to somewhere else where the actual data is stored. Thanks for the quick response. Are you forced to use C-style arrays for a variable payload struct then? Yes you can't use vector.  I think you need to work directly with byte arrays returned by the socket functions. For these purposes it's good to have two distinct parts of a message in your protocol. The first part is a fixed-size ""header"". This will include the size of the byes that follow the ""payload"" or data in your example. So to borrow some of your snippets and expand on them maybe you'll have something like this:  typedef struct { unsigned int sender_id; unsigned int sequence_number; unsigned int data_length; // this is new } PacketHeader; So then when you get a buffer in you'll treat it as a PacketHeader* and check data_length to know how much bytes will appear in the byte vector that follows. I would also add a few points... Making these fields unsigned int is not wise. The standards for C and C++ don't specify how big int is and you want something that will be predictable on all compilers. I suggest the C99 type uint32_t defined in <stdint.h> Note that when you get bytes from the socket... It is in no way guaranteed to be the same size as what the other end wrote to send() or write(). You might get incomplete messages (""packets"" in your terminology) or you might get multiple ones in a single read() or recv() call. It's your responsibility to buffer these if they are short of a single request or loop through them if you get multiple requests in the same pass. Thanks for the advice!"
854,A,"set MTU for device with multiple interfaces I am using ioctl(s SIOCSIFMTU (caddr_t)&ifr) to change the MTU for an interface. code is similar to this struct ifreq ifr; ifr.ifr_addr.sa_family = AF_INET; //iap->ifa_name is bond1:xx strncpy(ifr.ifr_name iap->ifa_name sizeof(ifr.ifr_name)); ifr.ifr_mtu = 1492; ioctl(s SIOCSIFMTU (caddr_t)&ifr) My problem is that the device has multiple interfaces and that the MTU is set to 1492 for all of these. I want to do it specifically for only one interface leaving all the others not impacted. How can I do it? bond1:43 Link encap:Ethernet HWaddr 00:0E:0C:E4:C5:45 inet addr:10.7.181.59 Bcast:10.7.181.255 Mask:255.255.255.0 UP BROADCAST RUNNING MASTER MULTICAST MTU:1492 Metric:1 bond1:48 Link encap:Ethernet HWaddr 00:0E:0C:E4:C5:45 inet addr:10.7.181.60 Bcast:10.7.181.255 Mask:255.255.255.0 UP BROADCAST RUNNING MASTER MULTICAST MTU:1492 Metric:1 bond1:49 Link encap:Ethernet HWaddr 00:0E:0C:E4:C5:45 inet addr:10.7.181.61 Bcast:10.7.181.255 Mask:255.255.255.0 UP BROADCAST RUNNING MASTER MULTICAST MTU:1492 Metric:1 If you want to update MTU for the specific interface you shallset ifr_name field of the struct ifreq to the name of the interface EDIT: You problem is in the name of the interface. The number after the column in the interface name is just an alias. Actually you don't have several different interfaces it's the same interface. That is why you setting is applied onto all interfaces with name ""bond:xx"" I am doing that. Check the strncpy(ifr.ifr_name iap->ifa_name sizeof(ifr.ifr_name)); but it still does not work Sorry for misunderstanding. The problem in the interface name. See the updated answer. OK. I see. How can I resolve the issue then? Unfortunately you can't fix it since it's the same interface with the same MAC address and other properties. The only thing is to use different physical interfaces. thanks for your help Eugene.  This may work . Get the Alias interface first and the set the MTU . ioctl(s SIOCGIFALIAS (caddr_t)&ifr)"
855,A,"c++ Hole punching UDP(RTP) I am doing a client-server voice chat program(unmanaged C++win32) in which clients connects to the server using TCP and textchat/chatroom functions are done in TCP while all audiotransmission is sent through a separate UDP/RTP socket (using the API from JRTPLIB). So the IP is known from the TCP connection and the port number of the RTP socket can be sent after connection is established. The problem is that in TCP only the server needs to do port forwarding for communications to work both ways since you establish a connection while in UDP you'd have to use recvfrom() -- which afaik needs the ports to be opened in the first place on the client side which I do not want (and is not needed if you look at any multiplayer game or VoIP client) Reading on sources that talk about UDP Hole Punching (for example http://en.wikipedia.org/wiki/UDP_hole_punching) for example they keep mentioning starting a udp conversation with the server. That's the thing - how do you actually start a udp conversation(both ways) with the server without the client having to open any ports? in TCP as I mentioned you just need to connect() to the server and communication is possible both ways. Also -- I know RTP builds on UDP but is there anything else I should know about the RTP hole punching (again using JRTPLIB) that makes it differ from UDP? Thanks in advance! @KaiserJohaan the client has to send the first UDP packet. That ""opens"" the port in the clients gateway so it can receive data from the server. I'm slightly confused - UDP Hole Punching is done for NAT Traversal i.e. when you need to connect peer to peer and both peers are sat behing a private network ( or router ). The server is only used as a way of 'matchmaking' the two clients. Is that what you're trying to do? I'm trying to achieve UDP/RTP connection between clients connected to a server without having the clients open any ports. As I understand it the clients exchange IP/Ports with the server who then shares it out between the clients so they can communicate to each other directly What do you mean by ""opening any ports"" ? There have to be ports opened for them to communicatethat's what udp hole punching does. How does online games and ventrilo/skype enable communication then? In those applications you don't need to open any ports for UDP communication @KaiserJohaan I see why you might think that. Each client opens a port to the server. The server now has a 'traversable' path back through the NAT to the internal client. The server then sends the endpoint details to each client who then use that IP/Port to communicate. So the port is not re-opened for the peer to peer comms. @zebrabox: Yes I see now I got the definition of hole punching wrong. The port used in the connection phase to the server is the same as the one used in transfer phase between clients. Anyway if I can salvage the thread the problem I described with applications(games VOIPetc) communicating over UDP without opening ports still puzzles me. @KaiserJohaan - I'd recommend looking at the code in libjingle which is Google's peer to peer lib http://code.google.com/apis/talk/libjingle/index.html @KaiserJohaan Many of them simply communicate through the server in which case there is no problem. It's when the clients have to communicate directly you have to start tricking the nat gateways. That's described in the wikpedia page you linked as well as e.g. http://www.brynosaurus.com/pub/net/p2pnat/  http://www.brynosaurus.com/pub/net/internet-drafts/draft-ford-midcom-p2p-01.txt. An actual implementation for doing this is non trivial. @nos Indeed - NAT Traversal is not possible for certain types of NAT conifguration so in those cases you either need to go through a relay server or not support those NAT types. As you say implementing it is very non-trivial! I see so the communication is to be relayed through the server. But again for the client how can they receive data with udp From the server without actually opening ports like its done through TCP with connect()? When the client talks to the server in order to exchange port information a connection is opened on both the client and NAT device. If you have a well-behaved NAT device that same connection can be used by the peer client to connect back. At that point communication is no longer relayed through the server. There are two possible definitions of ""opening a port"". One is opening a port with bind() for UDP or listen() for TCP another one is opening a port in a firewall. You need to open a port with an API call in order to receive something there is no way around it but you probably realize this so I think you mean opening a port in a firewall. But you don't need to do this on the side that initiates communication (the client). This applies both to TCP and UDP unless your firewall is set up in a very paranoid mode. Any reasonable firewall would allow a response from a server to a UDP port if there was a datagram sent from this port to the same server some time before. You only need hole punching if both sides are behind NATing firewalls/routers. That's how Skype does it. Moreover you don't even have to bother with recvfrom() and such stuff. You can just bind() a UDP socket then use connect() and recv()/send() or read()/write() exactly in the same way as you'd do with TCP. I see that makes sense. For the client does connect() setup this connection or do you actually have to send() something for the firewall to thereafter accept incomming packets on that port? @Kaiser no since UDP is a connectionless protocol all the connect() call does is just set up the default address to send or receive so you don't have to specify it each time. So you have to send() something first. I see. Thank you am going to try this out!"
856,A,"How to get my non-loopback network ip address in C? For a communication between two hosts I need to send the IP address of my host to the other site. The problem is that if I request my IP address it might be that I get back my local loopback IP addres (127.x.x.x)  not the network (ethernet) IP address. I use the following code: char myhostname[32]; gethostname(myhostname 32); hp = gethostbyname(myhostname); unsigned my_ip = *(unsigned*)(hp->h_addr); if( (my_ip % 256) == 127) { /* Wrong IP adress as it's 127.x.x.x */ printf(""Error local IP address!""); return; } The only way to solve it is to make sure my hostname in /etc/hosts is behind the real network address not the local loopback (the default for e.g. Ubuntu). Is there a way to solve this without relying on the content of /etc/hosts? Edit: I changed the above code so it makes use of getaddrinfo but I still get back the loopback device's number (127.001) instead of the real IP address: struct addrinfo hint = {0}; struct addrinfo *aip = NULL; unsigned ip = 0; struct sockaddr_in *sinp = NULL; hint.ai_family = AF_INET; /* IPv4 */ hint.ai_socktype = SOCK_STREAM; if(getaddrinfo(hostname NULL &hint &aip) != 0) { return 0; } sinp = (struct sockaddr_in *) aip->ai_addr; ip = *(unsigned *) &sinp->sin_addr; (I used to get back a list of 3 addrinfo's with the three SOCK_STREAMSOCK_DGRAM and SOCK_RAW but the hint prevents that) So my question still stands... gethostbyname is deprecated for many many years (one of the reasons being that it works with only one address family). As mentioned by qrdl you should use getaddrinfo Ok thanks for the info. This original code is 12+ years old. If /etc/hosts is still there and still the same looking for all entries of h_addr_list won't help. What do you mean wat should or shouldn't be placed in /etc/hosts ? The problem is that on our normal system (RHEL) the hostname is not placed after the 127.0.0.1 entry (but the real eth0 address) but on systems like Ubuntu they put the hostname after the 127.0.0.1 entry. Try get rid of (rename it if in doubt) the file /etc/hosts then you will get the eth0 IP directly if that is acceptable for you. Probably the worse choice but if you just have one machine to worry about...  Use getaddrinfo()  I just ran into a situation where when only /etc/hosts has information in it and when I used getaddrinfo to get the IP address list it returned 127.0.0.1 each time. As it turned out the hostname was aliased to localhost...something often easy to overlook. Here's what happened: The /etc/hosts file: 127.0.0.1 localhost.localdomain localhost foo ::1 localhost6.localdomain6 localhost6 172.16.1.248 foo 172.16.1.249 bie 172.16.1.250 bletch So now when you call getaddrinfo with host=""foo"" it returns 127.0.0.1 3 times. The error here is that foo appears both on the line with ""127.0.0.1"" and ""172.16.1.248"". Once I removed foo from the line with ""127.0.0.1"" things worked fine. Hope this helps someone. Yes that could have very well been the problem I found out myself also. Your system should take care of making sure your hostname is set to the right ip address. So it might be ok with no ethernet connection but once an extra connection is added the /etc/hosts should then be changed by the system.  Look at this: http://stackoverflow.com/questions/613471/discovering-public-ip-programatically Note that in some cases a computer can have more than one non-loopback IP address and in that case the answers to that question tell you how to get the one that is exposed to the internet. I need to get it from C not from a shell script or java The accepted answer at least is still relevant - you can access that website from C.  There is POSIX function getaddrinfo() that returns linked list of addresses for given hostname so you just need to go through that list and find non-loopback address. See man getaddrinfo.  Your new code hardwires the use of IPv4 (in the hint.ai_family field) which is a terrible idea. Apart from that you're close you just should loop through the results of getaddrinfo. Your code just gets the first IP address but there is an aip->ai_next field to follow... struct addrinfo { ... struct addrinfo *ai_next; /* next structure in linked list */ }; Putting a breakpoint after the getaddrinfo structure and browsing through the ai_next does not reveal all the different ip address I only get either the local address or the real ethernet address depending where the hostname in /etc/hosts is placed. As I said in a comment to the question the *algorith* is flawed anyway. My answer was just to explain how to implement the flawed algorithm :-)  Not an answer but a relevant comment: be aware that as soon as you start sending addressing information in the content of packets you run the risk of making your application unable to work across NAT:ing routers and/or through firewalls. These technologies rely on the information in IP packet headers to keep track of the traffic and if applications exchange addressing information inside packets where they remain invisible to this inspection they might break. Of course this might be totally irrelevant to your application but I thought it worth pointing out in this context. Yes it is irrelevant as it always runs on a local LAN however your point is relevant for larger scale applications indeed. Be careful: code has a way of lasting long beyond your original intent. Just be aware that someday someone is going to want to run it beyond the local lan. (This may not make any sense but they'll want to do it).  Even if the computer has only one physical network interface (an assumption that may or may not hold even netbooks have two - ethernet and WLAN) VPNs can add even more IP adresses. Anyway the host on the other side should be able to determine the IP your host used to contact it.  You're almost there. I'm not sure how you're getting my_ip from hp. gethostbyname() returns a pointer to a hostent structure which has an h_addr_list field. The h_addr_list field is a null-terminated list of all the ip addresses bound to that host. I think you're getting the loopback address because it's the first entry in h_addr_list. EDIT: It should work something like this: gethostname(myhostname 32); hp = gethostbyname(myhostname); unsigned my_ip = *(unsigned*)(hp->h_addr); for (int i = 0; hp->h_addr_list[i] != 0; ++i) { if (hp->h_addr_list[i] != INADDR_LOOPBACK) { // hp->addr_list[i] is a non-loopback address } } // no address found gethostbyname is deprecated since april 1997... (release of RFC 2133) I added the line that got the my_ip from hp now to the original question.  The originating address will be included in the packet sent... there's no need to duplicate this information. It's obtained when accepting the communication from the remote host (see beej's guide to networking specifically the part on accept())"
857,A,Testing network interrupts in software I have a network C++ program in Windows that I'd like to test for network disconnects at various times. What are my options? Currently I am: Actually disconnecting the network wire from the back of my computer using ipconfig /release Using the cports program to close out the socket completely None of these methods though are ideal for me and I'd like to emulate network problems more easily. I would like for sometimes connects to fail sometimes socket reads to fail and sometimes socket writes to fail. It would be great if there was some utility I could use to emulate these types of problems. It would also be nice to be able to build some automated unit tests while this emulated bad network is up. You can subclass whatever library class you are using to manage your sockets (presumably CAsyncSocket or CSocket if you are using MFC) override the methods whose failure you want to test and insert appropriate test code in your overrides.  You might want to abstract the network layer and then you can have unit tests that inject interesting failure events at appropriate points. This is the approach that I take and it works very well. Judicious use of interfaces means that I can mock out various parts of the system under test. For the tests that can't be done in this way I have the code under test connect to a test socket in the test harness which I then control in the test  There are some methods you can use it is depend on which level you want to test. For function level you can use XUNIT testing framework to mock a responce. For software level you can use a local proxy server to contral connection.  The closest I can think of is doing something similar with VEDekstop from Shunra.. Simulating High Latency and Low Bandwidth in Testing of Database Applications Shunra VE Desktop Standard is a Windows-based client software solution that simulates a wide area network link so that you can test applications under a variety of current and potential network conditions – directly from your desktop.
858,A,"getaddrinfo error: ai_socktype not supported struct addrinfo *myAddrinfo *curMyAddrinfo hint; memset(&hint 0 sizeof(struct addrinfo)); hint.ai_family = AF_INET; hint.ai_protocol = AI_PASSIVE; hint.ai_socktype = SOCK_STREAM; const int code = getaddrinfo(NULL SERVER_PORT &hint &myAddrinfo); if ((code) != 0) { printf(""getaddrinfo error occours: %s "" gai_strerror(code)); return 1; } this gives the error: ""ai_socktype not supported"" if i comment out the hint.ai_protocol = AI_PASSIVE; it will get through but i am wondering why it happens? thanks for your time That's because AI_PASSIVE is referred to ai_flags field (not ai_protocol). Try : hint.ai_flags = AI_PASSIVE; And have a look at addrinfo structure. oh yeah.. thanks.."
859,A,nginx websocket upgstream module development I'm trying to get nginx to reverse proxy websocket connections so that i can host a rails application on the same port. Considering we neglect the 8 byte content-length handshake issue I have a couple of questions on this topic: if i were to implement 1.1 keep-alive for current http upstream would that allow me to use nginx as a reverse proxy for a node.js websocket server instance at the backend? (and i'm not sure if i understand this correctly) considering i'm implementing upstream keepalive would that mean that every connection between client and nginx also results in a connection between nginx and the node.js backend for as long as the client is connected to the websocket? if that's the case is that in any way conflicting with nginx' way of connection handling? if 1. holds can you give me any hints on how i would approach implementing this? Thanks Reza P.s. There is an ngx-upstream-keepalive module which doesn't support http. http://mdounin.ru/hg/ngx_http_upstream_keepalive/ https://github.com/yaoweibin/nginx_tcp_proxy_module
860,A,"What types of apps are developed today using socket programming? I've worked in business application development for a while but have never done socket programming. I know that all HTTP transport implicitly involves socket communication but this is all abstracted when using most software frameworks. So I was curious what types of apps developed today involve socket programming? Here's what I developped in my own spare time (took me 2 years actually) : (1) program I called ""big chief"" (2) program I called ""the manager"" Here's how it works : First launch the managers on every machine that is configured for that. Once launched the big chief asks for dlls to create a list of sites to ""suck"". It cut them in ""packets"" and sends each packet to a ""(2) manager"" Each manager has a pool of ""workers"" (threads). As soon as it gets the list it activates each thread with one url to ""suck"". After some time once all the list is done the manager make a big ""results"" packet then send it back to the ""big chief"". It can go far further than just ""simply"" suck urls. (You can define a whole ""path"" with get and posts and the cookies follows the path which means stuff like ""going to xx.com simulate valid button then go to xx.com/valid.php (with all the cookies and so on set) then simulate something else.) Yep it's a mini-google. I used TCP for ""big chief"" and ""manager"" communication with my own protocol and compression before sending. One of its powerful feature is that you can extend it very easyli. I've used my PC for the ""big chief"" and 6 other Internet connexions for the managers (including a huge one from my old school). I am able to add as many ""managers"" as I want :). PS : Why am I talking about that ? Because I'm proud of it and it's not used at all. It's on my computer I've sucked a site that is hard to ... suck (pbase.com) and they've probably seen incoming connexions from the States China and so on (whereas I'm in France) (yep it does do support public proxies as well)... I'm so proud of a product that is not used at all... I think you mean to say you created a distributed web crawling bot that uses TCP to coordinate tasks. Is this correct? If so cool. Yep this is it. Thanks for clarifying. ;)  Any kind of proprietary communication protocol running over UDP or TCP would fit this description. We have a handful of applications that communicate with embedded systems using TCP and UDP all using specialized protocols.  An application involving networking or network protocols could involve socket programming. This would mean UDP TCP peer-to-peer etc.  Financial companies especially ones in algorithmic trading area rely on TCP/IP heavily. That ranges from third party communication products like Tibco to FIX over TCP sockets to in-house frameworks over UDP/multicast."
861,A,"List of all hosts on LAN network How can I get all the IP addresses and associated host names in a LAN? What is your platform (OS)? Do you want C++ or Cocoa? @Steve: My platform is Mac OS X 10.6 and a cocoa developer. But i am using RFB protocol. And i have to use those protocols which are written on C/C++ language. So a C/C++ solution on OS X would work for you? @Steve: Yes that is it. this is out of my domain but just wanted to clarify the question for possible responders To get the list of interfaces and IP addresses use getifaddrs(). Search for interfaces with ifa_addr->sa_family == AF_INET The IP address is in sin_addr.s_addr. You can then use gethostbyaddr() to look up the DNS name for that IP address. Update: It was pointed out to me that the OP was probably asking about discovering other hosts rather than the addresses of interfaces on the local machine. There is no reliable way to discover other machines on the local area network but there are a couple of tricks. Ping method: Use the ping utility (or a programatic equivalent) to ping the local broadcast address then see who responds. The broadcast address can be found by listing the interfaces as shown above. I believe ICMP does not require root access under OSX. Note that many systems may have ICMP ping disabled or firewalled so you will only get responses from the non-stealth ones. ARP method: Check the system ARP cache to see what IP addresses have been recently active. This will only show systems which have broadcast packets on the same network segment in recent minutes. Both methods can be blocked by firewalls routers and even switches so the exact borders of the ""LAN"" can be pretty narrow. Both methods can be implemented programmatically but it might be simpler and more portable to just call out to the command line ping or arp commands. Perhaps. I see now that the original question was not platform specific so you are probably correct. The question sounds more like the author wants to enumerate all the different hosts on the LAN not just information about the current hosts' uplinks."
862,A,"Most effective way to connect/retry connecting using C#? I am creating a little game client that will end up connecting to a server to gather some information on available games to play how many players are playing and all kinds of other stuff that you can imagine it ought to do. My difficulties come in finding an effective way in dealing with the connect/retry connect sequence upon first loading. I imagined my client would follow this process in trying to connect: Client application executed Try to establish connection If connection successful gather information - If not successful proceed to step 4 Display a new dialog/form that prompts the user that a connection is trying to be established Loop till a connection has been established I have been questioning my methods in trying to follow this sequence. I question if it is the right/most effective way to connect as well as to why my form I display in step 4 isn't working? try { sock.Connect(authenServerEP); // Once connected show our main client window this.Show(); // Create the LoginForm once a connection has been established and display LoginForm loginForm = new LoginForm(); loginForm.ShowDialog(); if (false == loginForm.Visible) { loginForm.Dispose(); } } catch (SocketException firstConnectException) { // Load retrying connection form EstablishingConnectionForm establishingConnectionForm = new EstablishingConnectionForm(); establishingConnectionForm.Show(); bool connected = false; // Loop until we are connected while (!connected) { try { sock.Connect(authenServerEP); connected = true; establishingConnectionForm.Dispose(); } catch (SocketException retryConnectException) { // Pass and retry connection } } } // end catch (SocketException firstConnectException) As you can see I am catching the SocketException that is raised when there is a problem connecting to the server (such as the server isn't running). I then go on to try to continuously loop till a connection is established. I dunno if I ought to be doing it this way. Are there better ways to do this? Also when I display establishingConnectionForm with Show() it doesn't look like it all of the forms/tools initialize (initialize could be misleading). The Label that is on the form is just shaded out in white as opposed to having its text displayed. Not only that but it seems like I can't select the form/dialog and actually move it around. It sits there with the ""Thinking/Working"" mouse icon. Now I presume this is because I am looping trying to reconnect and its blocking because of this (I could be wrong on the blocking?). Can this problem be solved with multithreading? If so do I need to multithread? Is there an easier way to show my form/dialog and be able to interact (IE movie it around and close it with the 'X' in the upper right corner) with it while I still try to reconnect? Thanks a bunch. I really appreciate you reading this post and am grateful for this community. :D Just an example below where I would handle any continuation logic in the catch and either break out or continue inside the while loop. Andrew  while (!connected) { try { sock.Connect(authenServerEP); connected = true; establishingConnectionForm.Dispose(); } catch (SocketException retryConnectException) { //Optional - add some wait time may be 5 seconds i.e. ""trying again in 5 seconds"" //System.Threading.Thread.Sleep(5000); //Here check the number of attempts and if exceeded: if(numberOfTimes == 5) break; else{ numberOfTimes++; continue; } } } This worked wonderfully. I appreciate your help. The one thing I have left to deal with is the fact that it seems when I bring up my establishingConnectionForm when I catch the SocketException I can never interact with it. It always shows the thinking icon and never lets me select it and move it around. Now I presume this is because I am looping trying to reconnect and its blocking because of this (I could be wrong on the blocking?). Is this a treahing problem? (IE all connecting should be done on threads so this connecting doesn't block - if it is blocking?) I second this answer. It actually follows along what is known as the circuit breaker pattern. Attempt something until either some condition occurs or a threshold is met. It is very handy when dealing with external connections that you have no control over. Yes the while loop will cause blocking on the thread that it is on. Kicking it off on a separate thread and kick off events to notify listening entities of updated statuses!"
863,A,What is a skeleton in a network? In a J2EE application clients (applications JSPs servlets JavaBeans) access entity beans via their remote interfaces. Thus every client invocation potentially routes through network stubs and skeletons even if the client and the enterprise bean are in the same JVM OS or machine. What's a network skeleton? Some kind of proxy? I understand a stub to be a single use connection is that correct? In RMI lingo the skeleton is the generated object that sits on the server accepts calls from the network unmarshals them and forwards them on to the business object. So....  Client Business Object -> Stub -> Network -> Skeleton -> Server Business Object It's a rather antiquated term though since this sort of stuff is much more automated than it used to be in early java where skeletons had to be explicitly generated. Good and short explanation.  When talking about RPC stubs and skeletons are proxies for the remotely called procedure/method on the client/server respectively. A client invokes a stub that stub sends a protocol request to the server. The request arrives to the skeleton at the server which then invokes the exported method. Typically stubs and skeletons are autogenerated from some IDL description by some tool
864,A,"Adaptive bandwidth allocation?  In our File Transfer application the network performance was fair but we want to get the maximum network performance so one way of achieving through adaptive bandwidth allocation .So the application will be forced to attain the available bandwidth.friends!!! if u have any white papers or code for reference it would be much helpful :) thanks krishna If you just throw it at the TCP session with no control it will transfer at full speed. You could also compact the file as you transfer. It will not accelerate the transfer but will optmize the use of the network at CPU coast. If it is not enough the only [software] way to improve that even more is by using multiple TCP sessions so you will reduce the speed delimitating effects of the latency over the TCP flow control. I beleave 5 concurrent transfers from different offsets of the same file will do the job faster impossible.  I don't think ""adaptive bandwidth allocation"" really means anything tangible (considering it's the #2 google hit for that expression!) but I'll try to give an answer that might help you ask a better question. If an application's network activity can be parallelised (bittorrent is a good example of this) then this is one way of achieving faster network transfers. In general though for user space applications the networking conditions are going to be outside the application's control for good reasons. If a userspace application considers it part of its mandate to adjust or affect external operating system-level networking conditions I would consider it malware. QoS for example could be used to prioritise the traffic associated with your application but that is something you might want to suggest and explain in a deployment guide and not try to manage from within your application."
865,A,"Basic client/server programming I am new to web programming...I have been asked to create a simple Internet search application which would allow transmit to the browser some data stored remotely in the server. Considering the client/server architecture (which I am new to) I would like to know if the ""client"" is represented only by the Internet browser and therefore the entire code of the web application should be stored in the server. As it's a very generic question a generic answer is also well accepted. Either you can put all the code on the server and have it generate HTML to send back to the browser. Or you can include JavaScript in the HTML pages so some of the logic runs inside the browser. Many web applications mix the two techniques.  You can do this with all the code stored on the server. 1)The user will navigate to a page on your webserver using an url you provide. 2)When the webserver gets the request for that page instead of just returning a standard html file it will run your code perhaps some PHP which inserts the server information perhaps from a database into a html template. 3) The resulting fully complete html file is sent to the client. To the client's browser it looks like any other html page. For an example of PHP the dynamically inserts information into HTML see: (this wont be exactly what you will do but it will give you an idea of how PHP can work) code: http://www.php-scripts.com/php_diary/example1.phps see the result (refresh a few times to see it in action): http://www.php-scripts.com/php_diary/example1.php3 You can see from this the ""code file"" looks just like a normal html file except what is between angled brackets is actually PHP code in this case it puts the time into the position it is at in the html file in your case you would write code to pull the data you want into the file.  As you note this is a very generic and broad question. You'd be well-served by more complete requirements. Regardless: Client/server architecture generally means that some work is done by the client and some by the server. The client may be a custom application (such as iTunes or Outlook) or it might be a web browser. Even if it's a web browser you typically still have some code executing client-side Javascript usually to do things like field validation (are all fields filled out?). Much of the code as you note will be running on the server and some of this may duplicate your client-side code. Validation for instance should be performed on the client-side to improve performance (no need to communicate with the server to determine if the password meets length requirements) but should be performed on the server as well since client-side code is easily bypassed."
866,A,"Which IP Framework (If Any) to use in C# .Net 4 It seems that System.net.Sockets had some performance problems in multithreaded applications in previous versions of .Net. and that SocketAsyncEventArgs was (At least partially) created to address these. However it looks like using SocketAsyncEventArgs and getting it right is a nontrivial process as shown in this article. Ive looked at some frameworksthat might be of use http://nitoasync.codeplex.com/ & http://supersocket.codeplex.com/ If I was looking for Maximum performance in an TCP application using managed code should I write around SocketAsyncEventArgs? and run the risk of discovering performance problems in production which are impossible to replicate in testing or should I use one of aforementioned frameworks?. I am not working with C# that long. Most of my experience has been in C++ but I have zero experience in this area. Im using .Net 4 If you're looking for something free I've had good experiences with SuperSocket. However if you need a bit more functionality...http://www.chilkatsoft.com/socket-dotnet.asp is really worth the extra dime. Regular updates and very easy to work with. Lots of examples and thorough/good documentation for the most part (some of the functions (very few) contain a ""To be documented"") The examples are really what sets them apart imo. Chilkat Library looks good. I have not purchased it yet as the developer did not respond to some simple questions which makes me wonder about support?"
867,A,Java or Ruby does it make a difference? I'm writing a large scale financial application that I use mostly Java for. Now to get some data I need to write a small script (<200 LOC) to download CSV files (over 20000 of them) and store them to disk. I need this to be fast but a few minutes doesn't make a difference to me. I was planning to write it in Java which isn't very hard but I would be done a lot faster if I wrote it in Ruby so I was wondering if there would be a large difference in speed between Ruby (or JRuby) and Java. The 20000 files are all about 1/2 a megabyte and the server I'm downloading from isn't keen to give away data (its completely legal don't worry about that) so my application has to randomly sleep in between and if the website denies a request it has to sleep for 3 minutes. Recommendations to any other easier-than-Java type of languages is welcome. Someone downvoted this after 4 months?! Sounds like you app will be I/O bound so the speed of the language is not terribly important In a language like Ruby or Python I would expect this to be more like 20 LOC or less. Especially since you have a limited request rate there is no point using simultaneous connections to try to speed things up If you have a bunch of machines with different ip addresses ( or one machine with several external addresses) you could split the job across those to speed things up since the rate limiting is usually by ip address Where do your urls come from? finance.google.com <- I've written a similar program in Python (admittedly it was terribly written) and it took around 9 hours.  Use whatever makes you comfortable. Language implementation speed probably won't be an issue there network speed and the sleeps you have to put in will be a bottleneck anyway. He's going to be sleeping between connections more than anything else. +1 If you're going to be downloading 20k+ files at around 500kB each you shouldn't be worried about language speed you're going to be waiting much longer for the connection overheads than anything else.
868,A,How to send pcap file packets on NIC? I have some network traffic captured pcap file and want to send its packets on NIC; is it possible? Is there any application to do this? Probably highly dependant on the OS. You should indicate which one. bit-twist can do this. just install it and inject your packet like this : # bittwist -i eth0 pcap-file.pcap  I use tcpreplay on Linux/Freebsd eg: #tcpreplay -l 0 -i eth1 path-to-your-captured-file.pcap -l loop how many times 0 for infinite -i interface where you want to send out  Yes there is a way - sending a packet to NIC means injecting it to an interface. You can do this with the help of libnet packege in linux. I myself am working on the same these days. Try Googling with this term you'll surely get some good stuff to share.  You should be able to use some kind of replay application like this one (tcpreplay).  There is a libpcap/winpcap library that allows the programmer to send/receive packets and work directly with NDIS-level driver. http://www.winpcap.org
869,A,"Fastest way to tell if SQL Server is available I have a winform program that uses Merge Replication to keep a local SQL Express sync'd with a Central SQL Server on our network. Due to the nature of this app the sync process runs automatically on App open and close. The way I am doing it (below) seems quite time consuming. What would be a faster way? Thinking outside the box it would probably be okay if I just checked for the Server itself though that could present a hole for an edge case where the Server is available but SQL is down. Alternately maybe just checking if our domain is available though that could leave the same hole I guess. Thanks! private static bool IsSqlAvailable() { try { var conn = new SqlConnection(""Data Source=WWCSTAGE;Initial Catalog=Connect;Integrated Security=True""); conn.Open(); conn.Close(); HUD.IsSQLAvailable = true; return true; } catch (Exception) { HUD.IsSQLAvailable = false; return false; } } As an additional note. The above is particularly slow when off network. I even set the Connection Timeout = 1 and it still sits at this point for quite some time. If you're setting a connection timeout to 1 second and it's still taking a while to fail maybe the connection attempt isn't to blame. Could it the DNS lookup? See http://www.eggheadcafe.com/conversation.aspx?messageid=31241642&threadid=31241641 for some interesting info on connection timeouts. I would wrap my Connection in a using clause and probably lower the connection timeout in my connection string (generally if the server is up it should connect more quickly than the default) but other than that it's fine.  My usual advice in this kind of scenario where you're checking for the existence/connectivity of something that is out of your control is: don't. Your check can pass and an instant later the result is no longer true. So just try to do whatever it is you want to do assuming that the SQL server is available and handle the inevitable errors gracefully. Except that doesn't answer my speed problem. If I let it try to run the sync process and wait for it to error out gracefully the user has a very slow load experience. If I was running the Sync any where other then Load and Close I might agree with you but as I am not I don't. Thanks for the reply though. @Paladin - Okay as long as you understand that there will be times when the Open and Close will be slow (because the check passed but then the server disappeared) and are prepared for that. (I'd suggest that in the Open you do the sync in a background thread if possible also do the sync periodically in background whilst the app is in use)  I just wrote a test connection method today in an app I am writing at work. I just use a simple try catch method on just a connection.open and connection.close and catch the exception if it fails. You dont have to set a variable if you dont need it. Just return true or false. It will hang for a few seconds if the connection is going to fail.  I think you can just ask the SQL Server TCP port. But there is a problem if used named instances.  You may be thinking about this too hard. The availability of your SQL server is essentially equivalent to you being able to connect to it so I would say your method of trying to open a connection is quite valid. My only suggestion would be to assign HUD.IsSQLAvailable once where you call the method since you are already getting a return value instead of assigning it twice inside IsSqlAvailable()."
870,A,"Catch network adapter up/down and address changed event (Windows C#) I am searching for a method to be notified when any windows network interfaces go up or down or the addresses of any network interfaces are changed. I would prefer solutions in C# C is also possible. Winpcap could be used. I am aware that there is an event for network adapter address changes in C# but this also possible without enumerationg all adapters and looking the changed settings? May some advanced programmer help a fellow newbie. with best regards Have a look at the System.Net.NetworkInformation namespace. The NetworkChange class allows you to register for NetworkAddressChanged and there are NetworkAvailability events and others that should cover most of what you're looking for. You will need to enumerate just like the sample shows- many machines have complex network setups that don't just boil down to ""hey something changed"" so you'll need to set up the baseline before you start. Thanks I will try that."
871,A,"Socket API or library for C++? I recently have been getting more into C++. I have done some (very minimal) socket programming with C but have been interested in doing some work with C++. I have only been able to find reference/tutorials on C based socket implementations. Is there a reason for this? I know or believe I know that you can use the C socket libraries for C++ but am not sure. Is there a C++ socket library that is used more often then others? This isn't a subjective question I am actually looking to find out what the Socket API's/libraries are for C++. I am fairly new at socket programming and C++ so please no answers that will throw me for a loop. Thanks An important test of any socket library is whether it exposes events. Qt does the native Win32 socket API does (see `WSAAsyncSelect`) BSD sockets make events possible without making them easy (`select` and `poll` which contrary to the name is event-driven not polling or `XtAppAddInput`). Many other libraries force you into a blocking one-thread-per-connection style which is not only inefficient but leads to bugs. The complicating thing here is that your socket events and UI or serial port or whatever other events your program deals with have to be compatible in a single event loop. I have only been able to find reference/tutorials on C based socket implementations. Is there a reason for this? Probably because all the socket implementations are based on the original C language berkeley socket api which defines functions like recv send listen accept select etc. I can highly recommend you look at the Boost ASIO. It's a cross platform C++ API so any code you develop will be portable. In fact a number of other Boost libraries you will find useful to you and all of them are cross platform. With reference to the basic API. You can use the original socket C functions in both windows and Linux. However be aware that under windows there are some slight differences. e.g. you have to call the WSAstartup function first. A really good reference to basic socket programming is Beej's guide to network programming. http://beej.us/guide/bgnet/ I'd recommend having a bit of a read over it even if you're using a C++ api as it gives you an understanding of what's going on. Edit: To be honest I don't use Boost ASIO anymore. I found it horribly slow. Use LibEV or similar or roll your own. Boost ASIO doesn't appear to use epoll on Linux.  Here I'm attempting to answer some of your specific factual questions to which I have something to contribute. Yes you can use any C socket library in C++. If it doesn't work out-of-the-box because the linker reports an undefined reference for the library functions you want to use then can fix it by editing the .h file(s) of the library adding extern ""C"" in front of all function and global variable declarations. To find libraries go to http://freshmeat.net/  and search for C++ socket or C socket. Here is what I've found for C++ socket: Socket++. ENUt. LibSylph. CPPSocket might also work for you although the last update was in 2003. SocketW is similarly old: last update in 2003. As Raphael has mentioned in his answer you might find the socket part of the Qt library useful. See QTCpSocket for reference and the fortune client for example code. Also Boost.Asio has popped to my mind but it might have too much abstraction and low-level details exposed for you. Do your search for C socket on freshmeat you may find a C library which suits better than any C++ library. libsylph doesn't appear to deal with sockets or networking at all. socketw is a very thin wrapper. socket++ makes using sockets badly really easy and appears not to provide any way to use sockets well (non-blocking).  I like to use Qt to program sockets. It provides an object-oriented implementation and it's multi-platform"
872,A,Comparison of P2P Technology I have gone through various udp based P2P Technology like Stun . I have implemented UDP/TCP hole punching recently for implementing p2p. I found there are other technology as for like ICEUPnP and teredo Can any body tell me what is the difference between these technology. Which one is the latest technology/protocol used for P2P in recent year. It will good If any can provide comparative analysis on various UDP based P2P protocols. Any link or suggestion will appreciated. ICE stands for Interactive Connectivity Establishment. It is a protocol for NAT traversal (i.e. punching holes) supported by the IETF. There has been several reviews and evolutions of the RFC. Some may find the specifications overkill in general or unclear when it comes to performing TCP NAT traversal. UPnP is a technology helping local devices finding each other and start communicating automatically. It implements IGD for NAT traversal which allows remote configuration of the NAT/Router (when possible) to redirect WAN traffic to the device. Unfortunately this method is a huge threat to security since any application could hijack NATs/routers to let any undesirable traffic come in. Teredo is not really related to P2P or NAT traveral. If you have an IPv6 device A on a ipv4 LAN (for example) it won't be able to connect using ipv6 to a remote ipv6 enabled device B located on the WAN. Teredo allows A to communicate with B with ipv6 by transporting ipv6 over ipv4. Teredo is massaging the frictions between ipv4 and ipv6 so to speak. None of these technologies is 'dominating' P2P for now. It is still a boiling environment. Thanks JVerstry
873,A,Java serialization over network Just want to know if there's a tutorial or an how-to for serializing object put it in a stream over network and deserialize it on the other point. I understand the principles of serialization I/O streams sockets and so on just want an example (client sending object to a server) to start with. Thank you. Java provides (binary) object serialization using the ObjectOutputStream (and ObjectInputStream). You can just writeObject() into the stream and readObject() on the other end. All you need to do for this to work is implement the Serializable interface. But rather than doing that manually you may be interested in taken it one level up and using Remote Method Invocation. With RMI you can call methods on objects that live in another JVM and all the serialization and networking happens under the hood. And for the sake of completeness there is also XML bean serialization if you cannot use the binary format. That XML format is very generic (read: verbose and ugly) but there are some popular libraries (like XStream) that create alternative XML serializations. XStream also offers JSON serialization (with Jettison) which can be very useful in cross-language projects: www.json.org Never heard about RMI seems like WCF in Microsoft technology hope I'm not wrong... Thank you for the idea !  Its pretty simple actually. Just make your objects serializable and create an ObjectOutputStream and ObjectInputStream that are connected to whatever underlying stream you have say FileInputStream etc. Then just write() whatever object you want to the stream and read it on the other side. Heres an example for you. You might want to be a little more careful about closing your resources than that example.  This (pdf) is a useful tutorial which walks you through the basics of serialisation and sockets then ties the two concepts together (about halfway through the slides) to show how to serialise an object and send it from client to server (no RMI). I think that's precisely what you want. Really show how to use serialization AND sockets thank you !
874,A,Sending movie stream from client to server. The python server should play the stream on the fly I would like to simulate some kind of camera on a UAV. The camera should provide a live stream and send the stream over a network connection to a server. The server should be able to play the stream on the fly for me to see. I was thinking the client(UAV) just read a moviefile and sent it to the server. But how can the server show the file on the fly? I suppose the simplest way would be to use xine or MPlayer to show the movie? But how? This is to be done in python and GNU/Linux. The client and server is both located on the same machine. The main issue is to get the server to play the file on the fly before it has the whole file available. Any ideas? EDIT: The server and client are connected with a standard TCP/IP connection. The video feed is not alone on the connection. When you googled streaming video servers or streaming video protocols what did you see? What specific questions do you have on the things that a Google search turned up for you? try 'webcam' sudo apt-get install webcam on debian. It will grab images from a USB camera and put them in a jpg file in /var/www/ then you make an html page that auto-refreshes as fast as possible that points to the image file. I know its not a very elegant solution but its the only one I know of. If you really are bent on writing it yourself You will need to read data from /dev/video0 (probably) and maybe encode it according to a standard video format open a socket with the client process and write the video data to the socket. There are some rules for the proper way to stream data over a socket though. check out http://en.wikipedia.org/wiki/Real-time_Streaming_Protocol this is also very informative http://www.jejik.com/articles/2007/01/streaming_audio_over_tcp_with_python-gstreamer/ I guess using stationary images would be easier and just use feh to show the image while the next one is downloaded. If I was to use a video it would already be encoded. So I the would not have to encode anything I decided to go for the webcam solution even looking at myself all day long was not so very interesting.
875,A,Internet Board Games Hello I'm looking to create a web-page where users can play Gomoku live with eachother kind of like instantchess.com or yahoo's online pool. I would like to be able to.. 1) Have the players to chat with each other while they play. 2) Have registered users where a ranking can be kept track of. 3) Users can create 'rooms' where another player and spectators can join. Should I be using java applets for the whole page Especially the networking? If so can someone point me in the right direction because I don't know how to have the applets interact with the server for database storage and live play/chat. Also assuming I would be using java applets what IDE do you suggest? Any Help is appreciated Thanks. 1) If you use Java Applets I would recommend building in Swing. I do all of my development in Eclipse using their visual editor for rich application development but I have heard positive things about NetBeans. 2) I would not recommend using applet technology. It seems currently that Sun is on the verge of retiring that technology. Instead I would recommend a page based architecture. You could use a framework like GWT or something similar. I am certain other Stack Overflow members who are more familiar with web technologies can point you in a better direction than I can. However this post has a pretty good run down of a number of different web-presentation layer technologies. Be aware you will need to talk to a database to do the behavior you have described. Just an FYI if you were not aware.  Also your users are going to hate applets....because pretty much everyone hates Applets.  audiodude is right your users will hate applets. Flash is defacto now.
876,A,"How the session cookie is maintained by browser Hey all I was trying to create a java program which will remain logged in on server whenever i browse on website no authentication will be required even if i run the program later on i.e. my program should store some cookie file on client side and also tell the browser that this session should lasts for quite a long time. And whenever i run the program this should send the cookie details that this is recognized and the user is logged on to the server. So how this should be done using HTTP protocol. Thanks. :) The browser (client) stores a session cookie containing a key-value pair. The key is usually JSESSIONID and the value is a unique identifier. It is received by the client as a response of his request to the server which initiates the session. Whenever a request is made to the server the browser sends that key-value pair in a special http header (""Cookie""). The server then reads that header obtains the identifier and finds the corresponding session.  Why don't you guys use Google before asking? See the following: HttpURLConnection class HTTP cookies description How to use HTTP Cookies in Java"
877,A,General network MVC framework for Python I have been using Django for web-development and have become quite fond of that framework. However I would like to use a similar framework but for more general network applications. Is there such a framework? Or is it possible to modify Django to be able to have a more general network/protocol backend? The standard library comes with `SimpleSocketServer` here: http://docs.python.org/library/socketserver.html . It might be what you need Twisted should be able to help you. Take a look at Twisted projects and decide on the protocol which you are going to use. I have looked into the named framworks and at the moment it looks like I will be using Twisted for network Storm for ORM and Jinja2 for templating. In terms of simplicity i found Storm to be on par with Django. Review this thread http://stackoverflow.com/questions/53428/what-are-some-good-python-orm-solutions. Autumn seems to be much simpler but pathetic documentation. Your best bet would be to go with Storm. Twisted seems to be a good general network framework but I also want a template engine and an ORM framework too and I like those parts in Django. For the template engine I found Jinja which seems like a good match but I still haven't found an ORM framework as simple as the one in Django.
878,A,c udp chat testing I am writing a udp chat application in c. I need to test if messages are received in an incorrect order. CAn anyone please tell me of a tool I can use to delay certain messages? also please tell me how to use it? thank you very much in advance! also I am using ubuntu x86_64 and OSX 10.6.4. A tool in either OS will work When I created a syslog server I needed to see if it was catching the messages. I used Wireshark from http://www.wireshark.org/ . This is a free tool that shows you all traffic passing your network cable. Even packages not intended for your computer. Have fun... I have used wireshark before however as far as I know it can't intercept and delay certain packets Then I'm not sure what you are trying to do. Nothing on the network can be guaranteed to be received in a cretin order. Even TCP/IP is not received in a guaranteed order. Are you on low level or application level? application level so far I haven't had messages come out of order so I need to simulate it by holding message n and allowing message n+1 to be received before allowing n to be received Do you know the format of the message? I have written a tool allowing you to by script send data over different communication ports such as UDP or TCP/IP. By the scripting you can create buttons to send messages when ever you want. I'm in the end process of releasing this tool officially. It is free to use and if you are interested I can let you download it somehow. My messages consist of only a string that contains all the necessary information. Your tool could be helpful for me if I cut out the client and used your script to send the messages. Is it able to send plain text strings? It can send whatever you like even stream a file from disk if you want to. This tool is developed for these kind of situations. There are 2 major companies using this tool already for their product development. The easiest way is if you go to my contact page and fill in your email. I will send you info how to download it since it is not official released yet. Go to http://www.mkdevelopment.se/kontakt.php The page is in Swedish (sorry) but the fields are Name mail phone (optional) category and message. Is it okay for you if I arrange it tomorrow? It is in the middle of the night here now. that would be perfectly fine I can't read swedish (sorry) so would you like me to have anything special under category and message? I have prepared everything for you just give me your information as mentioned above and I send you the information. No need to add anything there... The program itself is in English =) I just send the form THANK YOU FOR ALL YOUR HELP!! You should have the info by now. Only glad to help... Hmm oups just cam to mind you where not running Windows! Shoot this only works on Windows XP and up :( Hope you can find something to run it on. I have windows I have just been doing all my development in linux. Thank you for everything you are a life saver! Good to hear just drop a mail if you need help. Do you want me to put you on the notification list for this tool? There will only be notifications sent on new updates. yes please put me on the list @romejoe If you are intrested my little tool ComSpy2 can officially be found on my server http://comspy.mkdevelopment.se/ There is an update coming soon adding a Lua script object.  If you need to verify this you probably also don't want messages to disappear (or at least know if they do). UDP doesn't sound like what you want. Have a look at implementing this using TCP instead you get this behaviour with the protocol. thank you for your input I know that TCP would solve all of my problems however my homework assignment is to write it with UDP Tag this as homework then.
879,A,"Sending a file from server side to client side using TCP in C++ Now this is a bit more of a request since I am not able to find any simple and direct example for this. Issue: I want to send a file from the server side to the client side. When the server is already up and listening on a port and the client requests a file (file's name being accepted as a parameter with the server IP address such as 127.0.0.1 and port no.) and then the process of transfer starts till the file is copied. Also can someone incorporate how can i measure the average transfer speed on the server side? BTW: I am running Linux x86 Cheers echo9 Here's a simple protocol: CLIENT SERVER socket() bind() listen() socket() connect() accept() send(""GET filename\n"") recv(buffer) inspect buffer parse filename (stop at space or \n) open() file. sendfile(file socket) close(socket) close(file) recv(socket) close() This protocol has the advantage of being able to use your web browser as the client and your web server as the host assuming they each support HTTP/0.9. Here's a client.  You might want to consider using sendfile(2).  Check Beej's Guide to Network Programming. There are lots of examples there that shows how to implement a client/server architecture using sockets and send data between them. EDIT: Check items 8 and 9 from this tutorial for a complete example on client/server. Note that on item 8 the server sends a char* to the client: send(fd2""Welcome to my server.\n""220); /* send to the client welcome message */ On this case it's the ""Welcome to my server.\n"" string and the next parameter is the size of the string you want to send. When you need to send data from a file it's the same thing: first you need to read the data from the file and store it in a char* buffer; that you manually allocated through malloc(). Something like this: char* buffer; buffer = (char*) malloc(1024); // let's say your file has 1KB of data /* insert here the code to read data from the file and populate buffer with it */ send(fd2 buffer 10240); @echo9 Updated answer. I already did that but that wasn't too helpful :( @echo9 How can Beej's Guide not be helpful? It describes everything you need to know about sockets. Please describe how far you got with your implementation."
880,A,Bandwidth throttling in Python What libraries out there let you control the download speed of network requests (http in particular). I don't see anything built-in in urllib2 (nor in (Py)Qt which I intend on using). Can Twisted control bandwidth? If not how can I control the read buffer size of urllib2 or Twisted? sleeping to suspend network operations isn't an option. urllib2 doesn't offer a way to do this so you'd have to extend some of the classes it uses and implement rate limiting yourself. You might want to look at this question. If you decide to write a limiter read up on the token bucket and leaky bucket algorithms. Alternatively you could use pycurl along with the CURLOPTMAXRECVSPEEDLARGE option. EDIT: The urlgrabber package appears to support throttling as well and is probably easier to understand than pycurl. If you prefer to program using an event loop model there's the Twisted approach which has already been mentioned in another answer.  Of course twisted can. You want twisted.protocols.policies.ThrottlingFactory. Just wrap your existing factory in it before you pass it to whatever wants a factory. Can you post a complete example? I'm a bit lost by all the classes. Could this help: http://www.google.com/codesearch/p?hl=en#OmUp4SsZVxQ/apt_proxy/fetchers.py&q=ThrottlingFactory&sa=N&cd=5&ct=rc ?
881,A,Why would a browser make two separate requests for the same file? I'm debugging a program I wrote and noticed something strange. I set up an HTTP server on port 12345 that servers a simple OGG video file and attempted to access it from Firefox. Upon sniffing the network requests I found these two requests were made: GET /video.ogv HTTP/1.1 Host: 127.0.0.1:12345 User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 Accept: text/htmlapplication/xhtml+xmlapplication/xml;q=0.9*/*;q=0.8 Accept-Language: en-usen;q=0.5 Accept-Encoding: gzipdeflate Accept-Charset: ISO-8859-1utf-8;q=0.7*;q=0.7 Keep-Alive: 300 Connection: keep-alive GET /video.ogv HTTP/1.1 Host: 127.0.0.1:12345 User-Agent: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1.5) Gecko/20091102 Firefox/3.5.5 Accept: text/htmlapplication/xhtml+xmlapplication/xml;q=0.9*/*;q=0.8 Accept-Language: en-usen;q=0.5 Accept-Encoding: gzipdeflate Accept-Charset: ISO-8859-1utf-8;q=0.7*;q=0.7 Keep-Alive: 300 Connection: keep-alive Range: bytes=8122368- The video is almost 8 MB in size so the fact that the second request specifics 8122368 bytes which is 7932 KB suggests it is requesting the very end of the file for some reason. Anyone have ideas? I love the link to your localhost address. Amusing but not that useful. Let's see how many people report it as a broken link :-) Sorry I'm aware that it isn't useful I just wrote it without thinking. It's been removed. Some media format have meta data at the end of the file and this data is usually required to allow proper seeking of the video. Ogg files don't have their duration so this technique is used by the browser to find that out.  In order to support seeking and playing back regions of the media that aren't yet downloaded Gecko uses HTTP 1.1 byte-range requests to retrieve the media from the seek target position. So because Ogg files don't contain their duration the initial download connection is terminated. Then there is a seek to the end of the Ogg file and read a bit of data to extract the time duration of the media. Info from here and here.  Its actually requesting 8122368 bytes starting backwards from the end. Which is 7.74MB if I did my calcs correctly. it might be something in how the buffering for that file type is done. No it is requesting the bytes from 8122368 to the end which would be fileSize - 8122368 bytes. See: http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.35
882,A,"Emacs Lisp network connections - what to do with the process object? Suppose there is a TCP server up and running on localhost:8080 in another Lisp dialect that understands lists. Now I open a network connection in Elisp setq pserv (open-network-stream plisp ""test1.l"" ""localhost"" 8080) and successfully assign an open network process object to variable pserv. But then whats next how do I use this process object to send requests to the server? What I want to do is to send lists to the other server (code as data) that are evaluated and the results returned. In the above statement ""test1.l"" is the Emacs buffer associated with the process so the results should be printed in that buffer. What if I put nil there and the process is not associated with any buffer - how do I access the server results (possibly in list form too) from Elisp or from the process object? The Elisp manual seems to take that knowledge for granted but I'm a bit lost here. Every hints would be appreciated. In Emacs a network connection is treated like an asynchronous subprocess. So the instructions on sending input to or reading output from subprocesses also apply here. To send something to the server see Input to Processes and to read the results see Output from Processes. Information is transmitted over the network in string form so you might want to use read or read-from-string after receiving data from the server. See Input Functions. Your answer already gave the key info: treat network connections as asynchronous subprocesses. So I can read the Elisp manual for more info ... Thanks.  Client/server communication is handled via asynchronous processes. That means that unfortunately you don't have a function ""send"" that you pass your data to be sent to the server and that returns to you the server's reply. The reason for that is that network communication can be slow and that would block all other operations within Emacs which is a single-threaded program. To send data to your server use (process-send-string process string). The first parameter is your network connection which Emacs treats like asynchronous processes. Since you have that subprocess-object already stored in the variable pserv you could write: (process-send-string pserv ""(my data (can be (in) (list) form))"") to send a string to the server. To read the server's reply you use a process filter function which are callbacks that get invoked with whatever the server sends back to you. So you first have to define such a callback function say: (defun handle-server-reply (process content) ""Gets invoked whenever the server sends data to the client."" ... ) This function takes two arguments the network process and the content data that the server sends. There's one tricky point to this though: the server reply may be split up into sub-contents. That is when handle-serer-reply gets called the content argument may only contain parts of the server reply. It may get called again later with further content. So make sure you handle that correctly. To declare your function as a callback use: (set-process-filter pserv 'handle-server-reply) As always character encoding can be a pita so look into the following two functions and decide whether you might need them: (set-process-filter-multibyte pserv t) (set-process-coding-system pserv 'utf-8 'utf-8) Make sure to set these before assigning a process filter function. You might also be interested in surveying the status of your network connection for instance to handle cases where the connection gets closed unexpectedly. For that you can use so-called sentinel functions another type of callback through which you get informed about changes of the process status: (set-process-sentinel pserv 'sentinel-function) (defun sentinel-function (process event) ""Gets called when the status of the network connection changes."" ... ) The event parameter contains the information about the way in which the connection status changed. I think the Emacs process documentation has been pointed out before. It's definitely worth reading. Thanks Thomas a very detailed and helpful answer! Which is practically copied out of the manual. The manual is very good. You should read it. @nicferrier I agree that's why my answer contains multiple links to the manual."
883,A,"How to bind a Raw Socket to a specific port? I am currently working on a programming assignment. The assignment is to implement a clientnetwork emulator and server. The client passes packets to a network emulator and the network emulator passes to the server. Vice-versa applies as well. The prerequisite for the assignment is that I may only use raw sockets. So I will create my own IP and UDP headers. I have tested my packets with wireshark. They are all correct and in the proper format(it reads them properly). Another requirement is that the emulator client and server all have specific ports they must be bound to. Now I do not understand how to bind a raw socket to a specific port. All my raw sockets receive all traffic on the host address they are bound to. According to man pages and everywhere else on the internet including ""Unix Network Programming"" by Richard Stevens this is how they are supposed to work. My teacher has not responded to any of my emails and I probably will not be able to ask him until Tuesday.I see two options in front of me. First I can use libpcap to filter from a specific device and then output to my raw socket. I feel this is way out of scope for our assignment though. Or I can filter them after I receive them from the socket. This apparently has a lot of overhead because all the packets are being copied/moved through the kernel. At least that is my understanding(please feel free to correct me if i'm wrong). So my question is: Is their an option or something I can set for this? Where the raw socket will bind to a port? Have I missed something obvious? Thank you for your time. -- I would think you're expected to filter the packets in your software. It sounds like the exercise is to learn what the different components of the IP stack do by recreating a simplified piece of it in user space. Normally in the kernel the IP code would process all packets verify the IP headers reassemble fragments and check the protocol field. If the protocol field is 17 (udp) then it passes it to the UDP code (every UDP packet). It's up to the UDP code to then validate the UDP header and determine if any applications are interested in them based on the destination port. I imagine your project is expected to more or less mimic this process. Obviously none of it will be as efficient as doing it in the kernel but since the assignment is to write (part of) an IP stack in user-space I'd guess efficiency isn't the point of the exercise.  The man page for raw(7) says: A raw socket can be bound to a specific local address using the bind(2) call. If it isn't bound all packets with the specified IP protocol are received. In addition a RAW socket can be bound to a specific network device using SO_BINDTODEVICE; see socket(7). Edit: You cannot bind a raw socket to a specific port because ""port"" is a concept in TCP and UDP not IP. Look at the header diagrams for those three protocols and it should become obvious: you are working at a lower level where the concept of port is not known. So to make sure I am interpreting this correctly I am unable to bind to a specific port? If that is the case how would you implement the ""filter""? Libpcap to a specific device and filter the ports or since this is only for an assignment just filter them out after the socket receives them? Or do you perhaps have a better way to implement in mind? Thank you for your response and time by the way. and Santa thank you for the help. I'm going to go ahead and do it in the program with a comparison of the port field. I had another question as well. I'm new to the board and I was wondering do I mark this question answered or something? See the edit in my answer. I think you are on the right track. A ""port"" is a construct of the transport protocol. From the network level (IP) packets are addressed to addresses. It is the job of the higher level transport protocols to implement the ""port"" semantics. I'd imagine you have to decode the received packets yourself to determine which ""port"" it is addressed to and filter accordingly."
884,A,"Selector.close throwing a java.util.ConcurrentModificationException on AIX platforms I am using java nio selector and seem to hit the following issue randomly but consistantly in my application while calling the selector.close. The selector object is being accessed by a single thread in my application. The same application works fine on Solaris Linux and Windows. I feel that this is an issue is with the AIX implementation of the Selector java.util.ConcurrentModificationException at java.util.HashMap$AbstractMapIterator.checkConcurrentMod(HashMap.java:118) at java.util.HashMap$AbstractMapIterator.makeNext(HashMap.java:123) at java.util.HashMap$KeyIterator.next(HashMap.java:196) at sun.nio.ch.SelectorImpl.implCloseSelector(SelectorImpl.java:95) at java.nio.channels.spi.AbstractSelector.close(AbstractSelector.java:102) at org.beepcore.beep.transport.tcp.TCPSelector.close(TCPSelector.java:173) java -version java version ""1.6.0"" Java(TM) SE Runtime Environment (build pap6460sr5ifix-20090729_01(SR5+IZ55981)) IBM J9 VM (build 2.4 J2RE 1.6.0 IBM J9 2.4 AIX ppc64-64 jvmap6460sr5ifx-20090728_39709 (JIT enabled AOT enabled) J9VM - 20090728_039709_BHdSMr JIT - r9_20090518_2017 GC - 20090417_AA) JCL - 20090529_01 Any pointers are appreciated Thanks in advance Vijay The solution consisted of following fixes: Synchronize the operations which involved the modification of the Selection Keys. Cancel all the SelectionKeys registered with the selector before calling Selector.close(). Call Selector.wakeup() in the wrapper function of selector.close() so that the selecting thread exits as soon as close is called.  boolean isContinue = true; while(isContinue) { try { for(SelectionKey selectionKey : selector.keys()) { selectionKey.channel().close(); selectionKey.cancel(); } isContinue = false; // continue till all keys are cancelled } catch (ConcurrentModificationException e) { // This should not occur. But log a debug message in case this is encountered } }  Do you have another thread that is iterating/modifying the key set of Selector? From the Selector's java doc keys are NOT threadsafe. Concurrency Selectors are themselves safe for use by multiple concurrent threads; their key sets however are not. ... You may get CME exception If you have a thread working on the key set while Selector.close() is being called. Looking at the stack trace the exceptions are happening in Sun's common implementation codes so it shouldn't be AIX specific implementation. My suggestion would be identifying the thread that is add/removing selector keys and see if synchronized keyword needs to be applied or you need to make a sync-copy before working on the keys. If the modifying thread is not your thread/codes then it is AIX issue. However I can't tell without seeing the codes that modifying the key set. Good luck debugging. I hope it helps"
885,A,"Visual Basic - Checking if a port is open Ok I'm making a Visual Basic GUI application to display whether a number of my ports are open for people to know whether things like my website and my Minecraft server are open. My problem is I have absolutely no idea how to do this in Visual Basic. Basically I'm asking for something which sends a signal to an IP with a specific port if it is open then return true if it's closed return false. Similar to: http://www.canyouseeme.org/ What version of Visual Basic? VB.NET? See ""port scanner written in c# with asp.net"" at http://www.codeproject.com/KB/aspnet/WebSitePortScanner.aspx. It is C# but it is simple enough to convert to VB.NET. If you need VB it will be harder but searching on ""port scanner"" and ""visual basic"" should turn up something."
886,A,"C# Detect Localhost Port Usage In advance thank you for your advice. I am currently working on a program which uses Putty to create a SSH connection with a server that uses local port forwarding to enable a client running my software to access the service behind the SSH server via localhost. IE: client:20100 -> Internet -> Remote SSH server exposed via router/firewall -> Local Intranet -> Intranet Web POP3 Server:110. Cmd Line: ""putty -ssh -2 -P 22 -C -L 20100:intranteIP:110 -pw sshpassword sshusername@sshserver"" Client would use putty to create a SSH connection with the SSH server specifying in the connection string that it would like to tie port 110 of the Intranet POP3 Server to port 20100 on the client system. Therefore the client would be able to open up a mail client to localhost:20100 and interact with the Internal POP3 server over the SSH tunnel. The above is a general description. I already know what I am trying to do will work without a problem so am not looking for debate on the above. The question is this...How can I ensure the local port (I cannot use dynamic ports so it must be static) on localhost is not being used or listened to by any other application? I am currently executing this code in my C# app: private bool checkPort(int port) { try { //Create a socket on the current IPv4 address Socket TestSocket = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); // Create an IP end point IPEndPoint localIP = new IPEndPoint(IPAddress.Parse(""127.0.0.1"") port); // Bind that port TestSocket.Bind(localIP); // Cleanup TestSocket.Close(); return false; } catch (Exception e) { // Exception occurred. Port is already bound. return true; } } I am currently calling this function starting with a specific port in a for loop to get the 'false' return at the first available port. The first port I try is actually being listened to by uTorrent. The above code does not catch this and my connection fails. What is the best method to ensure a port is truly free? I do understand some other program may grab the port during/after I have tested it. I just need to find something that will ensure it is not currently in use AT ALL when the test is executed. If there is a way to truly reserve the localhost port during the test I would love to hear about it. Wondering if I should move on from Putty for this and maybe use SharpSSH. Any other good SSH source code for c#/vb out there? Here's the answer how to check if local port is free. I would recommend this way: bool IsBusy(int port) { IPGlobalProperties ipGP = IPGlobalProperties.GetIPGlobalProperties(); IPEndPoint[] endpoints = ipGlobalProperties.GetActiveTcpListeners(); if ( endpoints == null || endpoints.Length == 0 ) return false; for(int i = 0; i < endpoints.Length; i++) if ( endpoints[i].Port == port ) return true; return false; } Unfortunately all three examples on that site failed to detect uTorrent listening on the starting port. That's so weird. Any other ideas? Can you show the output of command ""netstat -anbp tcp"". I'm interested in the lines with uTorrent.exe Excellent thank you!"
887,A,"Writing to Socket outputStream w/o closing it I'd like to write some messages to the server. Each time for the tramsmitting only I'm closing the outputStream and reopen it when I have to send the next message. os.write(msgBytes); os.write(""\r\n"".getBytes()); os.flush(); os.close(); How Can I keep this Socket's OutputStream os open and still be able to send the message? Thanks. In most protocols the server accepts som kind of EOF symbol. Send such a symbol instead of closing the stream. For example IRC servers interpret ""\r\n"" as the end of a message. This would be 2 messages on one open OutputStream: PrintStream printStream = new PrintStream(socket.getOutputStream()); printStream.print(""JOIN #channel1\r\n""); printStream.flush( ); printStream.print(""JOIN #channel2\r\n""); printStream.flush( ); Also you should wrap your outputStream with DataOutputStream. This wrapper creates more portable output. Plain OutputStream can cause trouble with certain primitive datatypes if server and client have different computer architectures.  I am missing something here. If you don't call close it will not close. For example os.write(msgBytes); os.write(""\r\n"".getBytes()); os.flush(); // Do something os.write(""more message""); os.flush(); // When you are finally done os.close();  Wrap the Socket's OutputStream in a PrintWriter and call the PrintWriter's println method. PrintWriter pw = new PrintWriter(socket.getOutputStream()); .... pw.println(message); // call repeatedly to send messages. .... pw.close(); // when finished writing to server and want to close conn.  I have found the problem and it lays on the client's side. On the client I have used - count = inputStream.read(buffer)) > -1 That means the client waits till server's outputStream closed and then processing the incoming data."
888,A,Writing High Performance Server. Which method? if you would need to write a high performance server how would you do it? Using asynchronous blocking epoll / kqueue? But how to handle the blocking System call epoll / kqueue here? Perhaps with a Main thread which uses worker Threads which perform the blocking epoll/kqueue ? Using libevent? Does it even differs from epoll/kqueue as it only capsulates different mechanisms like select epoll kqueue etc? Or instead using asynchronous blocking epoll/kqueue using Asynchronous non blocking libaio? But does it even support Sockets or just Disk IO? Thank you for your answers. I guess that I would start with figuring out what it was supposed to serve and getting a *quantitative* definition of high performance. I would use boost::Asio It uses best non blocking IO per system epoll for linux kqueue for NIX like overlapped I/O on Windows It can handle other streams as well (COM files). Concept is simple create io_service class and write handle (event like) classes to manage stream.  I would use libevent. It uses the best available mechanism on the target platform whatever that mechanism may be; so without changes your program will use kqueue on BSD epoll(4) on Linux and whatever else is best on whatever else might exist while still falling back to select(2) on old platforms and somehow it also works on Windows. Magical software. :) Does it support multiple CPU-Cores? @Filipe yes but not very transparently; if you want to use the event loop in multiple cores you need to [start multiple event loops one per participating thread / process](http://www.mail-archive.com/libevent-users@monkey.org/msg00403.html) and pass around descriptors as necessary.
889,A,Send a raw ethernet packet from inside a kernel module I found out that I need to build a new sk_buff struct in the kernel module and pass it to my network device but what I can't figure out is how to set the struct variables for a simple raw ethernet packet. This has to be easy but i would really appreciate it if someone could give me a sample code of how to put the sk_buff together. Take a look at the function packet_sendmsg_spkt in net/packet/af_packet.c for inspiration. The hard part is getting a struct sock if you don't have a socket... Edit: Added a basic code shell: int sendpacket(struct socket *sock struct net_device *dev __be16 proto void *data size_t len) { struct sock *sk = sock->sk; struct sk_buff *skb; if (!(dev->flags & IFF_UP)) return -ENETDOWN; if (len > dev->mtu + dev->hard_header_len) return -EMSGSIZE; skb = sock_wmalloc(sk len + LL_RESERVED_SPACE(dev) 0 GFP_KERNEL); if (skb == NULL) return -ENOBUFS; /* FIXME: Save some space for broken drivers that write a * hard header at transmission time by themselves. PPP is the * notable one here. This should really be fixed at the driver level. */ skb_reserve(skb LL_RESERVED_SPACE(dev)); skb_reset_network_header(skb); /* Try to align data part correctly */ if (dev->header_ops) { skb->data -= dev->hard_header_len; skb->tail -= dev->hard_header_len; if (len < dev->hard_header_len) skb_reset_network_header(skb); } memcpy(skb_put(skb len) data len); skb->protocol = proto; skb->dev = dev; skb->priority = sk->sk_priority; dev_queue_xmit(skb); return len; }
890,A,Detecting Wireless computers which have not established any connection to a Wireless Network All I know is that it is not possible to detect any wireless computer unless they are connected together in a network. I want to know; Is there any possible way that I could turn on my Wireless Adapter and search what are other wireless device types around me and some sort of an ID such as MAC or computer name etc. without creating or connecting any Network such as Wireless Ad-Hoc network etc. like we do in Bluetooth? - If it is possible then how to implement using C# or any language. Depending upon the operating system and NIC drivers that you're using you can set the 802.11 NIC to Promiscuous Mode and Monitor Mode in order to capture all packets on a given channel. At least under Linux this functionality is supposed by most drivers in conjunction with LibPCap; although Windows is another matter altogether (you'd have to use a version of Windows featuring NDIS6 - i.e. Windows Vista or Windows 7 and an external packet capturing API such as the one provided by the Microsoft Network Monitor SDK). Thank you very much for the fast answer and details. Will this work if all the Wireless PCs around are only just switched ON and NOT connect to any sort of network? I mean are they broadcasting there status during that time or is there anyway I can send a request to broadcast details to idle wireless NICs around. Thanks You'll want to monitor for Probe Request packets (directed to access points from stations/clients) and Probe Response packets (directed from access points to stations/clients). You can also monitor for Beacons if you're interested in devices that are either operating in Ad-Hoc mode or as both an access point and a station (e.g. some portable game consoles such as the Nintendo DS). However there's no guarantee that a switched-on client will send Probe Requests (since it's likely that a device may have its radio disabled be misconfigured or be otherwise actively participating within a network). Thank you very much this is important information for me. Do you know any online website or document where I can learn these in detail? Sorry for the delay in responding. The best reference is probably the IEEE 802.11 specifications themselves (which are available free-of-charge from http://standards.ieee.org/about/get/802/802.11.html). @Tyson you're correct but for Windows there is no need in NDIS6 or Vista/Win7. Vista has introduced the NetMon but that's really just another sniffing software and definitely not the only one. You can use WireShark for example to capture the packets over WinXP.
891,A,"How to specify source port of a UdpPacket? I wanted to send UdpPacket to a specific remote host (I already know the public IP and Port). I wanted to use C#'s UdpClient class. static int Main() { UdpClient client = new UdpClient(); IPEndPoint remoteEP = new IPEndPoint(IPAddress.Parse(""1.2.3.4"") 9999); byte[] data = GetData(); client.Send(data data.Length remoteEP); } When sending a packet the UdpClient choose an available port automatically. I want to manually set the port from which I send the packets. Thanks for your help in advance! Try specifying the endpoint when you create the UdpClient: UdpClient client = new UdpClient(localEndpoint); EDIT: Note that you can also specify just the port number: UdpClient client = new UdpClient(localPort); That may be somewhat simpler :) Wow thanks!!! It works! I thought this constructor can only be used if you want to receive messages. UdpClient = new UdpClient(new IPEndPoint(IPAddress.Parse(""0.0.0.0"") 9999)); worked @youllknow: It was a bit of a guess admittedly :) I've just edited my answer with a possibly simpler way of doing it too. Worth a try..."
892,A,"C++ Add DNS entry into network adapter I have to programatically add DNS server address in network adapter settings on windows. Programming language is C++. hahhhaa .... no. i need to do it programmatically what have you tried so far? where are you stuck? what are you missing? Setting DNS using iphelp and register at CodeProject. bool RegSetDNS(LPCTSTR lpszAdapterName LPCTSTR pDNS) { HKEY hKey; string strKeyName = ""SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\\Interfaces\\""; strKeyName += lpszAdapterName; if(RegOpenKeyEx(HKEY_LOCAL_MACHINE strKeyName.c_str() 0 KEY_WRITE &hKey) != ERROR_SUCCESS) return false; char mszDNS[100]; strncpy(mszDNS pDNS 98); int nDNS; nDNS = strlen(mszDNS); *(mszDNS + nDNS + 1) = 0x00; // REG_MULTI_SZ need add one more 0 nDNS += 2; RegSetValueEx(hKey ""NameServer"" 0 REG_SZ (unsigned char*)mszDNS nDNS); RegCloseKey(hKey); return true; } @Vijay: See here for some hints - http://us.generation-nt.com/answer/programmatically-setting-dns-wins-address-help-28260902.html it would be great to know how to add WINS entry too?  You can take a look to the IP Helper API You can find how to use it There"
893,A,Which TLS library should I use portability is an issue We are looking to add TLS (transport layer security) support to our software and are looking for the best choice of a library. The software is written in c++ and is ported to Linux and Solaris (and Windows but who gives a crap?). Being able to adapt the code to both Linux and Solaris is a major concern and thus a library which is known to work well on Solaris would be preferred even at the cost of efficiency. Thanks in advance for any recommendation. Shai We've used OpenSSL on Windows AIX Solaris HPUX and Linux for quite a long time and never found any incompatibilities. As far as I remember no platform specific code was needed to make it work everywhere. Thanks I'd vote you up had I remembered the login credentials to my actual account.  libssl works on Solaris. It's not fun to use but it's the de facto implementation.  GnuTLS? An example. Do you have any experience running it on Solaris? @user864940: Unfortunatelly I have no experience on this library.  We've used OpenSSL on Windows with C++ but you'd have to check if this ports to all the Operating Systems that you need.
894,A,Estimating how processor frequency affects I/O performance I am doing research about dedicated I/O software that would run on consumer hardware. Essentially it boils down to saving huge data streams for later processing. Right now I am looking for a model to estimate performance factors on x86. Take for example the new Macbook Pro: high-speed Thunderbolt I/O (input/output) technology delivers an amazing 10 gigabits per second of transfer speeds in both directions 1.25 GB/s sounds nice but most processors of the day are clocked around 2 Ghz. Multiple cores make little difference as long as only one can be assigned per network channel. So even if the software acts as a miniature operating system and limits itself to network/disk operations the amount of data flowing to storage can't be greater than P / (2 * N)[1] chunks per second. Although this hints the rough performance limit I feel it's far from adequate. What other considerations should one take estimating I/O performance in regards to processor frequency and other hardware specifics? For simplicity's sake assume here that storage performs instantly under all circumstances. [1] P - processor frequency; N - algorithm overhead Interesting question. You also need to remember that a lot of high speed data transfers are using DMA(Direct Memory Access) through a custom hardware interface @Mark Hall - Good point thanks for the input. The hardware limiting factors are probably the I/O bus performance say PCIe and more recently the FSB clock-rates since memory controllers are moving from northbridge to the CPUs themselves. Then of course you have to figure out what sort of processing you need to do on the input and how much work it is to produce the output. These at least for conventional software running on a CPU are dependent on the processor clock but not only. Writing your code to take advantage of the hardware facilities like caches instruction-level parallelism etc. is still a black art but can give you an order of magnitude performance boost. Basically what I'm ranting about is that not all software is created equal and you probably want to take that into account.  Likely harddisk controllers will decide the harddisk I/O performance graphics cards will decide maximum resolution and refresh I/O performance and so on. Don't really understand the question the CPU is becoming less and less involved in these kinds of things (well has been for the last 10 years). I doubt the question will even have bearing on CPUs with integrated GPUs since the buffer to be output to screen is in external memory sharing a bus with (again) a controller on the motherboard. It's all buffered so I can only see CPUs affecting file performance if you somehow force the hardware buffer size to something insanely puny. Edit: and I'm pretty sure Apple will prevent you from doing such things. ;) For Thunderbolt specifically it's more about what the minimum CPU model is that supports the kinds of bus speeds required by the Thunderbolt chip set version that is in the machine in question. Thunderbolt is a raw data traffic system and performance specs are potential maximums hence all the asterisks in the Apple specs. I believe it will indeed alleviate bottlenecks and in general give lag-free intelligent data shuffling doing many things simultaneously. The CPU will idle-wait a shorter time for needed data but the processing speed of the data is the same. When playing or creating a movie codec processing time will be the same but you will still feel a boost/lack of lag because the data is there when it needs it. For the I/O the bottleneck will become the read/write speed of your harddisk instead and the CPU bottleneck (for file copy operations likely at least some code in Finder) will stay the same. In other words only CPU-intensive tasks such as for example movie encoding will benefit significantly from a faster CPU while the benefits of Thunderbolt vs. a mix of interfaces will boost machines with both slow and fast CPUs. I am not aware of any network interfaces writing directly to disk. Even with DMA it takes a CPU and some software to read from RAM and write to disk. This is where CPU frequency architecture and other factors come in. Any thoughts on those? @Saul see the edit. All true. However I was thinking more along the lines of a **dedicated system**. In other words no iOS no graphics no sound no peripherals no nothing. Just bare I/O (network and disk). What would the performance factors be in **that** case? Obviously a 64-bit processor has **higher** data throughput per second than 32-bit since the former operates on longer data chunks. Etc. I'd like to reserve HDD issues for a separate question and assume here that storage performs instantly under all circumstances.
895,A,"what ip address does accept return see the following code: accept(sockfd (struct sockaddr*)&cliaddr &slen); cout << inet_ntop(AF_INET cliaddr.sin_addr ipv4addr 100); my client connects from localhost. i get an absurd address in the output. this is not my ip address. everytime i run the code i get a different ip address. when i ping that ip address i don't get any response. what is the reason. i am running suse linux on a virtual machine in windows vista. Update: bzero(&cliaddr sizeof(cliaddr)); int connfd = accept(sockfd (struct sockaddr*)&cliaddr &slen); if (sem_wait(&mutex) < 0) err_sys(""sem_init error""); char ipv4addr[100]; cout << inet_ntop(AF_INET &cliaddr.sin_addr ipv4addr 100) << endl; //const char* p = inet_ntop(AF_INET &cliaddr.sin_addr ipv4addr 100); //cout << p << endl; //cout << (void*)p << "" "" << (void*)ipv4addr << endl; this returns address as 0.0.0.0 if i uncomment the lines i get the correct address in all the lines 127.0.0.1 I see a semaphore in your code - make sure other threads don't use the same address structure at the same time. I.e. this might be a race condition. That code doesn't initialize the 'slen' parameter to the accept call. Make sure you set slen to sizeof(cliaddr). @Nikolai: only one thread running @nos that answers my question. i initialized slen to sizeof(cliaddr) and the code works. but if we do not initialize slen then how do we explain the behavior. My next guess:  On success inet_ntop() returns a non-null pointer to dst. NULL is returned if there was an error with errno set to indicate the error. Is cout.<< clever enough to dereference the pointer that's being returned or are you printing out the pointer? why else would it print 232.76.213.191 i think it has something to do with the virtual machine. OK I give up. Good luck!  You are missing the 4th parameter in your call to inet_ntop(). Here's a working example:  int sockfd fd; struct sockaddr_in saddr; socklen_t len = sizeof( saddr ); char addr_buf[INET_ADDRSTRLEN]; /* defined in <netinet/in.h> */ /* ... socket() bind() listen() */ bzero( &saddr len ); if (( fd = accept( sockfd ( struct sockaddr* )&saddr &len )) == -1 ) { perror( ""accept"" ); exit( 1 ); } /* watch out for EINTR */ if ( inet_ntop( AF_INET &saddr.sin_addr addr_buf INET_ADDRSTRLEN ) == NULL ) { perror( ""inet_ntop"" ); exit( 1 ); } printf( ""accepted connection from [%s:%d]\n"" addr_buf ntohs( saddr.sin_port )); ... Always check for errors when talking to network. in my program len was not initialized. hence the error.  My unsubstantiated guess is that you're getting IP v6 addresses back instead of v4 so your conversion is off. You might want to try using netstat to find out the client's port (you usually get a sort-of-random port number between 1025 and 65535) and see if the hex value of that appears somewhere in the hex representation of cliaddr. If there's a correlation between client port and what you believe to be the client address then your conversion is incorrect. the client connects using a socket belonging to the AF_INET family and is running on localhost. also inet_ntop should return an error in this case. I may have another explanation. See my other answer."
896,A,"How to get Domain Name of ipAddress and ipAddress from Domain Name in objective-c I am able to get the current ip address of my device/machine that I am using - by using this question's answer. I have gone through this question. Java allows to get the IPAddress from a domain name. Is it possible in objective c ? How ? The second question is How to get the name of device/machine by using it's IPAddress. Say for example I have an ipAddress 192.168.0.74 = What is the device name ? in Objective c ? How ? Thanks in advance for sharing your knowledge. You need to read the routing table - basically the same way as ""netstat -r"" command does. The netstat implementation is opensource - it's in the package network_cmds-396.6 at http://www.opensource.apple.com/release/mac-os-x-1082/ so you can check what it does. The default gateway contains the ""G"" flag but shouldn't connect the ""I"" flag (when both wifi and cell are active wifi is used for internet connection - the cell gateway is not used and is given the ""I"" flag). Hope it helps.  I'm not sure if this is the best way to do this but it works for me mostly. I put in StackOverflow's IP addresses (69.59.196.211) and it gave me back stackoverflow.com but I put in one of Google's IP addresses (210.55.180.158) and it gave me back cache.googlevideo.com (for all results not just the first one). int error; struct addrinfo *results = NULL; error = getaddrinfo(""69.59.196.211"" NULL NULL &results); if (error != 0) { NSLog (@""Could not get any info for the address""); return; // or exit(1); } for (struct addrinfo *r = results; r; r = r->ai_next) { char hostname[NI_MAXHOST] = {0}; error = getnameinfo(r->ai_addr r->ai_addrlen hostname sizeof hostname NULL 0  0); if (error != 0) { continue; // try next one } else { NSLog (@""Found hostname: %s"" hostname); break; } } freeaddrinfo(results); There can be multiple names for the address so you might not want to stop at the first one you find."
897,A,Releasing connection in connectionDidFinishLoading cause bad instruction According to URL Loading System Programming Guide NSConnection sample code I can release connection in connectionDidFailWithError and connectionDidFinishLoading. However releasing connection in connectionDidFinishLoading causing objc[19685]: FREED(id): message releaseDelegate sent to freed object=0x3b41630 Program received signal: “EXC_BAD_INSTRUCTION”. Why? If you doesn't retain the object you receive from [NSConnection connectionWithRequest:] then you must not call release on it.  This is a double release error. It means either you didn't retain an object enough times or you sent it too many release messages. I understand the nature of error but all I do is just create `NSConnection` with `connectionWithRequest` and then release in those 2 delegates. The rest of the bodies of delegate methods are empty. The general rule is: If you don't create objects using alloc and don't retain objects you don't have to release them. They will magically disappear when no-one needs them anymore (with NSAutoReleasePool being the magician and autorelease being the magic spell)
898,A,Socket buffer binding to socket In the linux kernel upon packet [sk_buff] receive when is the packet bound to its socket ? i.e. when is the sk_buff's sk field populated ? skb->sk is set on receive by skb_set_owner_r() for example in net/ipv4/tcp_input.c. In general the networking core will dispatch packets based on ethertype to the appropriate protocol handler and it is there that the socket is looked up and the packet put into the socket backlog for processing.
899,A,"python2.6 with MySQLdb NameError 'MySQLdb' not defined from the interpreter i can issue >>> from MySQLdb just fine. so I'm assuming the module did actually load. My source looks as follows: from Tkinter import * from MySQLdb import * """""" Inventory control for Affordable Towing Functions: connection() - Controls database connection delete() - Remove item from database edit() - Edit item's attributes in database lookup() - Lookup an item new() - Add a new item to database receive() - Increase quantity of item in database remove() - Decrease quantity of item in database report() - Display inventory activity transfer() - Remove item from one location receive item in another """""" def control(): ....dbInfo = { 'username':'livetaor_atowtw' 'password':'spam' \ ....'server':'eggs.com' 'base':'livetaor_towing' 'table':'inventory' } ....def testConnection(): ........sql = MySQLdb.connect(user=dbInfo[username] passwd=dbInfo[password] \ ........host=dbInfo[server] db=dbInfo[base]) ........MySQLdb.mysql_info(sql) ....testConnection() control() this gives me: brad@brads-debian:~/python/towing/inventory$ python inventory.py Traceback (most recent call last): ..File ""inventory.py"" line 53 in ....control() ..File ""inventory.py"" line 26 in control ....testConnection() ..File ""inventory.py"" line 22 in testConnection ....sql = MySQLdb.connect(user=dbInfo[username] passwd=dbInfo[password] \ NameError: global name 'MySQLdb' is not defined 1) where am I going wrong? 2) any other gotcha's that you folks see? 3) any advice on how to check for a valid connection to the database (not just the server)? This is 110% due to the way you're importing the module and then referencing it. You want to change: from MySQLdb import * to import MySQLdb if you plan on referencing it the fashion that you are. Anyway here’s how these statements and functions work: From: http://effbot.org/zone/import-confusion.htm import X imports the module X and creates a reference to that module in the current namespace. Or in other words after you’ve run this statement you can use X.name to refer to things defined in module X. from X import * imports the module X and creates references in the current namespace to all public objects defined by that module (that is everything that doesn’t have a name starting with “_”). Or in other words after you’ve run this statement you can simply use a plain name to refer to things defined in module X. But X itself is not defined so X.name doesn’t work. And if name was already defined it is replaced by the new version. And if name in X is changed to point to some other object your module won’t notice. from X import a b c imports the module X and creates references in the current namespace to the given objects. Or in other words you can now use a and b and c in your program. my week feeble mind forgot about the import and the way it affects namespace. Thanks for reminding me of how much of an idiot I am. Your explanations were far above average! I'll *try* not to forget this lesson again.  from MySQLdb import * and import MySQLdb do very different things."
900,A,set MTU in C programmatically A client requested that the MTU limit should be 1492. Is there a way to do it in the source code (program in C)? Is there any other way to do it in general? (ifconfig?) Why does somebody needs to modify MTU to a certain limit? What is the benefit? And the most important: By changing the MTU is there any risk to break the code? The MTU is a number which defines the maximum transmission unit per packet. The bigger it is the faster your data will be sent. Assuming you can send npackets/s of msize if m grows m*n grows too. I think your client wants that MTU because of its network equipment (maybe ethernet 802.3). Some equipment handel biggest frames size than others. You can use ifconfig with the option mtuto change its value: ifconfig eth0 mtu 1492. so is it only a matter of speed and configuration (ethernetpppoE etc)? Is there a risk to break the existing code? as far as I know it is more and hardware configuration issue than a software one.  It's not about speed directly; By increasing MTU you're reducing overhead which is data that is responsible for the proper delivery of the package but it's not usable by the end user; This can have a increase in speed but only for heavy traffic; Also if you increase MTU you're prone to increase the number of packets that are dropped (since you have a fixed bit error probability and more bits in a packet) eventually causing a decrease in performance with resent packets etc... So it's a compromise between overhead and data integrity; I guess that it is more of a interface configuration than something you control with a program; So it's better to stick with 'ifconfig' command or find the equivalent solution for Windows. Is it possible to set it through programming? Is there an API to set the MTU of a connection or interface? I do believe it's ~possible~ but not really sure how. Problably something very platform specific...
901,A,"Support for https using QNetworkAccessManager. Hitting SslErrors at runtime I am performing a https get operation with QNetworkAccessManager. I am hitting SSLErrors at runtime. After researching for a while I was able to get my program running after installing OpenSSL. I required two dlls: libeay32.dll and ssleay32.dll. Is it to say that I cannot perform https ""get"" operation using QNetworkAccessManager without OpenSSL ?? Doesn't Qt support native https support using QNetworkAccessManager. Thanks De Costo. Had to install the following dll's found at the following website to my machine: http://www.slproweb.com/products/Win32OpenSSL.html The encryption is guarded closely by the nationality. So More info at: http://developer.qt.nokia.com/faq/answer/how_can_i_add_ssl_support_to_my_qt_application"
902,A,"Does make-network-process work in windows? I'm having trouble debugging a program that uses emacs lisp to act as a simple tcp client. I've stripped it down to the minimum to present as a question here. This code works fine on Max OSX and on a Linux Centos machine but on windows (all with Gnu emacs recent version) the callback functions for the sentinel and filter process are never called. The blocking connect works whereas the non-blocking connect says non supported on windows which I can live with. The server is showing that the client connects and disconnects fine; but no output seems to make it to the server. Eventually the server disconnects. Any ideas? (defvar *url-process* nil) (defvar *url-state* nil) (defvar *url-response* nil) (defun url-buffer-message(process message) ""print a message to the url process buffer"" (save-excursion (set-buffer (process-buffer process)) (insert message))) (defun url-sentinel(process event) ""sentinal function for url network process which monitors for events"" (url-buffer-message process (format ""sentinel event %s"" event)) (cond ((string-match ""open"" event) (setq *url-state* 'open)))) (defun url-filter(process string) ""filter function for url network process which receives output"" (url-buffer-message process (format ""filter %s"" string)) ; debug (setq *url-response* string)) (defun url-new-process(p) (url-delete-process) (setf *url-process* p)) (defun url-delete-process() (when *url-process* (delete-process *url-process*) (setq *url-state* 'closed))) (defun url-hai(host port blocking) (setq *url-state* 'opening) (url-new-process (make-network-process :name ""url"" :host host :service port :nowait (not blocking) :filter #'url-filter :sentinel #'url-sentinel :buffer (get-buffer-create ""*url*"")))) (defun url-kthxbye() (url-delete-process)) (defun url-ping() (if (and *url-process* (eq *url-state* 'open)) (process-send-string *url-process* ""PING\r\n""))) (defun url-get() (if (and *url-process* (eq *url-state* 'open)) (process-send-string *url-process* (format ""GET /\r\n"")))) (defun url-info() (if (and *url-process* (eq *url-state* 'open)) (process-send-string *url-process* ""INFO\r\n""))) ; usage ; (url-hai ""127.0.0.1"" 80 t) ; (url-hai ""127.0.0.1"" 80 nil) ; (url-get) ; (url-kthxbye) Shouldn't `:nowait blocking` read `:nowait (not blocking)`? Yes you're right about that. I switched it in the code above The problem here was related to make-network-process When calling this function with :nowait set to nil to make the call synchronous/blocking the sentinel function is never called with the state 'open'. Sadly I was relying on that behavior and in my functions which send data I check for whether the connection is open yet. Once I discovered that the fix was obvious; just set the state to 'open if the make-network-process function succeeds and it's a blocking call. Working code below: (defun url-delete-process() (when *url-process* (delete-process *url-process*) (setq *url-state* 'closed))) (defun url-hai(host port blocking) (setq *url-state* 'opening) (url-delete-process) (let ((p (make-network-process :name ""url"" :host host :service port :nowait (not blocking) :filter #'url-filter :sentinel #'url-sentinel :buffer (get-buffer-create ""*url*"")))) (if p (progn (if blocking (setf *url-state* 'open)) (setf *url-process* p))))) (defun url-kthxbye() (url-delete-process))"
903,A,How to program RS232 over Ethernet? An existng system with a server and terminals communicating over RS232. Now the terminals are to be replaced with Android slate PCs and minimal changes should be made to the server. I presume they will use RS232 to Ethernet convertors at the server but how will the traffic look to the Android? Which protocol? How to I read and write an Ascii string? This depends entirely on the hardware you buy but generally this is done over Telnet or similar. While Telnet does have a basic protocol that sometimes gets followed for terminals generally it is as connect on a specific port (such as 23) and send and receive data. That data is then relayed to RS232. Yeuck! but thanks for the answer (+1). Any pointers to how implement this?
904,A,"SAXParser says:""java.net.SocketException: Network is unreachable"" but xmlint or curl are OK I can download the following URL: ""http://genome.ucsc.edu/cgi-bin/das/hg18/dna?segment=chr1%3A10%2C20"" with xmllint : xmllint ""http://genome.ucsc.edu/cgi-bin/das/hg18/dna?segment=chr1%3A10%2C20"" <?xml version=""1.0"" standalone=""no""?> <!DOCTYPE DASDNA SYSTEM ""http://www.biodas.org/dtd/dasdna.dtd""> <DASDNA> <SEQUENCE id=""chr1"" start=""10"" stop=""20"" version=""1.00""> <DNA length=""11""> ccctaacccta </DNA> </SEQUENCE> </DASDNA> or curl curl ""http://genome.ucsc.edu/cgi-bin/das/hg18/dna?segment=chr1%3A10%2C20"" <?xml version=""1.0"" standalone=""no""?> <!DOCTYPE DASDNA SYSTEM ""http://www.biodas.org/dtd/dasdna.dtd""> <DASDNA> <SEQUENCE id=""chr1"" start=""10"" stop=""20"" version=""1.00""> <DNA length=""11""> ccctaacccta </DNA> </SEQUENCE> </DASDNA> But the following java program raised an exception: import javax.xml.parsers.*; public class Test { public static void main(String args[]) { try { SAXParserFactory f=SAXParserFactory.newInstance(); f.setNamespaceAware(false); f.setValidating(false); f.newSAXParser().parse( ""http://genome.ucsc.edu/cgi-bin/das/hg18/dna?segment=chr1%3A10%2C20"" new org.xml.sax.helpers.DefaultHandler() ); System.out.println(""OK""); } catch(Exception err) { err.printStackTrace(); } } } error: java Test java.net.ConnectException: Connection timed out at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333) at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195) at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366) at java.net.Socket.connect(Socket.java:525) at java.net.Socket.connect(Socket.java:475) at sun.net.NetworkClient.doConnect(NetworkClient.java:163) at sun.net.www.http.HttpClient.openServer(HttpClient.java:394) at sun.net.www.http.HttpClient.openServer(HttpClient.java:529) at sun.net.www.http.HttpClient.<init>(HttpClient.java:233) at sun.net.www.http.HttpClient.New(HttpClient.java:306) at sun.net.www.http.HttpClient.New(HttpClient.java:323) at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:860) at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:801) at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:726) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1049) at org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source) at org.apache.xerces.impl.XMLEntityManager.startEntity(Unknown Source) at org.apache.xerces.impl.XMLEntityManager.startDTDEntity(Unknown Source) at org.apache.xerces.impl.XMLDTDScannerImpl.setInputSource(Unknown Source) at org.apache.xerces.impl.XMLDocumentScannerImpl$DTDDispatcher.dispatch(Unknown Source) at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source) at org.apache.xerces.parsers.XMLParser.parse(Unknown Source) at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source) at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source) at javax.xml.parsers.SAXParser.parse(SAXParser.java:395) at javax.xml.parsers.SAXParser.parse(SAXParser.java:277) at Test.main(Test.java:11) Why ? PS: there is no proxy here. (later) OK  I've got the anwser: The SAX parser tried to load this ""http://www.biodas.org/dtd/dasdna.dtd"" and this site is down. But how can I prevent the SAX parser to load this URL ??? DTD is loaded even if no DTD validation is done (which can be disabled) to load possible entity definitions. But you can write and hook up custom EntityResolver -- it is the thing that tries to load DTD; and if you implement one that returns empty String (which is valid DTD just contains nothing) it will prevent further processing. That's what I wrote; Many thanks for your comment."
905,A,"Send data back to client with UDP using Apache Mina I am using Apache Mina to create a server to accept UDP Client requests. I have read the Official documentation provided by Apache Mina regarding UDP Server & UDP Client. However I wished to know when the server receives a message can I write back to the UDP Client using the same session(I know UDP is connectionless at Network Layer however I can get the IP and PORT of the remote host at Application Layer) such that UDP Client receives a message. I know this is possible is TCP but am a little confused about UDP. I know this may not exactly be Java based but more Network Layer based question. Would appreciate if somebody could clear this for me. @Override public void messageReceived(IoSession session Object message) throws Exception { for (int i = 0; i < session.getService().getManagedSessions().values().toArray().length; i++) { IoSession aSession=(IoSession) session.getService().getManagedSessions().values().toArray()[i]; aSession.write(""Any Message""); } }  I got the answer to the same and thought i would share. UDP is connectionless however I can use the same session which I have in Apache Mina to write to the session. I tried it as a sample also and it worked."
906,A,"Get data from the web server and sent them to the client c++ I am trying to get some data from the webserver as a proxyand then sent the data to the client. Webserver -> Me -> Client This is my main code: int main() { Initialize(); ServerSocket s; s.Bind(); s.Listen(); while(true) { TCPSocket* sock = s.Accept(); ClientSocket client; string addr = handle.getAddress(); short prt = handle.getPort(); client.Connect(addr prt); char data[5000]; client.Send(""GET /index.html HTTP/1.0\r\n\r\n"" 28) ; while(true) { int numbytes=client.Recv(data 5000); if(numbytes == 0) { break; } cout <<""Received from web server: "" << numbytes << endl; int numbytes2 = sock->Send(data numbytes); cout << ""Sent to client: "" << numbytes2 << endl; } client.Close(); } s.Close(); return 0; } But when I browse the netlets say google.com I get ""302 Moved The document has moved here."" In every site I get different behaviorbut the site won't load up. Am i doing something wrong when I send and receive bytes ? Thanks in advanced. handle its a class that take care of authenticatuon between client and proxythat works good.Thats why i didnt include the code here Where have you defined 'handle'? Your code is working just fine. Its just that modern day webservers won't allow you to connect without properly authenticating yourself using user-agen headers etc. Try the following commands on your shell. telnet yourserver.com 80 <some messages from server> GET /index.html HTTP/1.0 If the response is same as what you get using C++ program then basically all you need to do is send more info to properly authenticate yourself.  user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)' headers = {'User-Agent': user_agent} And send this data along with your GET request. if i use operawhat should i need for Version token(MSIE 5.5) and platform token for windows 7 should be windwos nt 6.0 ? nevermindits 6.1 :)"
907,A,"float buffer casting to unsigned char* losing data on socket send Is there a better way to send float data over a socket? Below is an implementation that sends the data. static float theOUTPUT[THE_FLOAT_DATA_BUFFER_SIZE]; int size = 2048; int tb = 0; int numbytes = 0; int cs = 256; unsigned char* buf = (unsigned char*)theOUTPUT; while(tb < size) { numbytes = send(sock buf cs 0); printf(""bytes sent: %i\n"" numbytes); tb+=numbytes; buf+=numbytes; if(tb >= size) { break; } } I suggest you post a complete example that still exhibits the problem (though you might just find your error in the process of producing one :) You've set `size` to 2048. Is there a reason for this? Shouldn't it be something like `sizeof(theOUTPUT)`? If this is going from one computer to another realize that even if both server and client are using C no specific float internal representation is guaranteed by the C standard. What are the type and value of `ch`? is there a reason you are converting the array into an unsigned char* array? int ch =1; Yes I am casting the float buffer to unsigned char* to send it over the network. In this code there's no guarantee that size matches sizeof(theOUTPUT). If it less then you will lose data.  Thanks for the answers. It was not a very good question. I discovered the issue and found that I was sending more data than I thought over the socket. This what was corrupting the data. @user470379 C programming language standard is unrelated to floats but I think that is what you mean. If you build the server and the client then you have control over how you encode your data regardless of the programming language."
908,A,"Qt - QTcpserver is not working properly I am doing a simple server-client application. But the client gets some undefined behaviour from the server side. After I retrieved the error code I came to know that the server cut the connection. This is server-side main.cpp #include <QApplication> #include <QFile> #include <QFileDialog> #include <QMessageBox> #include <QtNetwork/QTcpServer> #include <QtNetwork/QTcpSocket> class MyMessageBox:public QMessageBox { public: MyMessageBox(std::string messageQWidget *parent=0):QMessageBox(QMessageBox::NoIconQString(""ErrorMessage"")QString(message.c_str())QMessageBox::OkparentQt::Widget) { } }; class My_Server:public QTcpServer { Q_OBJECT public: My_Server(); public slots: void on_Connection(); }; My_Server::My_Server():QTcpServer() { connect(thisSIGNAL(newConnection())thisSLOT(on_Connection())); } void My_Server::on_Connection() { MyMessageBox mm(""Connection Established""); mm.exec(); QTcpSocket * my_Socket = this->nextPendingConnection(); my_Socket->waitForBytesWritten(30000); QByteArray block(""Hi all""); my_Socket->write(block); } int main(int argcchar * argv[]) { QApplication app(argcargv); My_Server tcp_Server; tcp_Server.listen(QHostAddress(""127.0.0.1"")15000); return app.exec(); } #include ""main.moc"" This is client-side main.cpp #include <QApplication> #include <QDataStream> #include <QFile> #include <QFileDialog> #include <QHostAddress> #include <QMessageBox> #include <QtNetwork/QTcpServer> #include <QtNetwork/QTcpSocket> class MyMessageBox:public QMessageBox { public: MyMessageBox(std::string messageQWidget *parent=0):QMessageBox(QMessageBox::NoIconQString(""ErrorMessage"")QString(message.c_str())QMessageBox::OkparentQt::Widget) { } }; int main(int argcchar * argv[]) { QApplication app(argcargv); QTcpSocket client_Socket; client_Socket.connectToHost(QHostAddress(""127.0.0.1"")15000); QDataStream in(&client_Socket); in.setVersion(QDataStream::Qt_4_7); client_Socket.waitForReadyRead(30000); char buf[100]={'\0'}; client_Socket.read(buf(quint16)sizeof(buf)); QString nothing(buf); MyMessageBox mm((QString(""++ "")+nothing+"" ++"").toStdString()); mm.exec(); MyMessageBox mn(QString::number(client_Socket.error()).toStdString()); mn.exec(); return app.exec(); } This is the pro file( same for both) TEMPLATE = app TARGET = DEPENDPATH += . INCLUDEPATH += . # Input SOURCES += main.cpp QT += network I can't find out that why the server side is cutting the connection. If anybody help me to find out the cause I will be thankful to them. Note: I am using Qt-4.7.2 in windows platform The connection is getting closed because your server is exiting right after it's sent that message. This is probably because you're using QApplication but don't actually have a long-lasting GUI widget. So the event loop stops short once you've finished displaying that first dialog box. Try either: using QCoreApplication on the server side instead. (And don't start a message box - no GUI allowed with QCoreApplication.) give your server a main widget that is responsible for starting the TCP server. Your answer is ""bull's eye"". I will use some other mechanism like ""waitForReadyRead"" in the server side QTcpSocket to avoid that immediate exit. Because GUI is the heart of this application.  Check quitOnLastWindowClosed property of QApplication this will prevent your application to quit after closing the dialog. Also your server is ""waiting for bytes to be written"" before actual write. You should first do write(block) than call waitForBytesWritten"
909,A,It is possible to figure out which internet connection is correlated with which networkInterface? I am a noob in netoworks stuff but I must to get to know something. When I have list o networkInterface rmnet0 eth0 lo //etc. How can I tell which kind of connection (wifi/bluethoof/UMTS) use which networkInterface? You can use the command 'lshal'
910,A,"how can asn1 notation and libraries help me? Abstract Syntax Notation I'm in the process of writing a network client and server in the process of research i came across asn1 notation. How does asn1 notation help me program network services and what problems does it solve? Is it worth using? does it simplify network programming or is it something i can do without? Are there alternatives? I'm looking for tutorials on asn1 how and why i should implement it? Are you using it? If so how are you using it. It's a binary serialization format. You'd use it for the same sort of things you use XML for or to interoperate with ASN.1 based services such as X.500 DAP. @user499211: have you decided to use asn.1? I have the exactly same questions. What have you found out? Please share!! You should not implement ASN.1 (unless that is explicitly your goal). Rather most ASN.1 users find a library and use the library. (in my experience any library that is good also costs serious money). I would not use ASN.1 unless you need to. Interoperating with something that already specifies ASN.1 is a good reason to use it. (example: I think that Kerberos transactions are encoded in ASN.1) Otherwise I'd look at any other format such as XML or JSON. ASN.1 is horrible. It is hard to imagine anything worse. LOL @GregS.... it is very good at the specific task it was created for: to express limited data in an extremely precise unambiguous and efficient way. Given that at the time it was created 2400bd modems were ""cutting edge"" and ASCII was not universal I understand why it is that way. Still on modern systems there is less and less need for it and popular sentiment is with you."
911,A,"Framework for Java RPC server I'm planning on writing an RPC server in Java. The server needs to accept incoming RPCs - probably over HTTP - and answer them. Fairly basic stuff. Support for 'long polling' or 'hanging' RPCs isn't necessary so a thread-per-request model ought to be perfectly adequate. If I were writing this in Python I would probably use a framework like twisted. In C something like glibc. In each case the framework provides an implementation of the general 'select loop' core of handling IO and invoking higher level constructs that deal with it eventually leading to my application being called for events such as receiving an RPC. It's a long time since I wrote anything substantial in Java though so I don't know what the state of the art or the suggested solutions are for this sort of thing. Possibly there's even parts of the standard library I can easily use to do this. Hence my question to StackOverflow: What frameworks are there out there that would be suitable for a task like this? Note that although I may use HTTP for the RPCs this is emphatically not a web application - and as such a web framework is not appropriate. You have multiple choice: Roll your own solution with the existing SDK for socket programming. Java RMI the remote method invocation framework. Java CORBA bindings is no longer considered current. Java Web Service frameworks are quite complex. Look at Apache CXF and the different J2EE products. Then you have different systems running above HTTP transport like JSON/XML-RPC where you need a Web server. Even though you rule them out. Why does use of JSON or XML require a web server? I already have (most of) the wire format of the RPCs decided for me so I'm not looking for an RPC library per-se - just a general network programming framework. @tweakt you are right. I was specifically thinking of JSON over HTTP or XML over HTTP. That's why...  You could consider using some thing as simple as Jetty the internals of jetty are very stable and can handle quite silly number of connections. If you implement the jetty specific handler interface you can also do with out all of the servlet and JSP support libraries making it quite a small embedable app server.  Apache MINA is a very well designed asynchronous non-blocking networking framework. It provides byte-oriented access to read and write packet data. Building atop that it has a filter system where additional layers can be added providing things like line-oriented text parsing encryption (via TLS) compression etc. PS: The 2.0 release series is highly recommended even though it's still in ""milestone"" form it's proven very stable and is nearing a final release. This looks excellent. I'll hold off marking it as 'the' answer in case someone comes up with something even better though. ;) One question though: The HTTP example they provide implements the HTTP protocol itself and the API only seems to contain support for proxying HTTP. Do you know if there's a built-in or well-used MINA library for handling HTTP requests? @Arachnid AsyncWeb (http://mina.apache.org/asyncweb/) is an HTTP layer built atop MINA. Though if you are indeed serving this RPC protocol over HTTP requests then you might be better off just writing a servlet and hosting it in any of the web-app containers out there. A better link: http://docs.safehaus.org/display/ASYNCWEB/Home See also: https://grizzly.dev.java.net/ Thanks again. I'd give you a +2 if I could. ;)"
912,A,"Encoding decoding an integer to a char array Please note that this is not homework and i did search before starting this new thread. I got http://stackoverflow.com/questions/1522994/store-an-int-in-a-char-array I was looking for an answer but didn't get any satisfactory answer in the above thread. Here's my requirement: I want to encode my data(say an integer) in a byte array and then transfer over the network and then decode at the other end and process it. Here's the encoding part: const int MAX=5; uint32_t a = 0xff00ffaa; char byte_array[1024]; // this is the array to be transferred over the network char buff[MAX]=""""; sprintf(buff""%4d""a); memcpy(byte_arraybuff4); // fill remaining stuff in the byte array and send it over the network Here's the decoding part: const int MAX=5; char buff[MAX]=""""; strncat(buffbyte_array4) int i=atoi(buff); // Work with i Here are my questions : 1) Is the above code portable? I guess it is( please correct me) 2) Now I wish to encode the byte array with 3 bytes (but the integer size is 4) i.e say the integer stores 0x00ffaabb and i just want the byte array to have ff int 0th index aa in the 1st index and bb in the 2nd index. How to do that? snprinf doesn't seem to work or may be i am missing something. A person who has implemented any network protocol can easily help me out. Decoding logic would still work i guess. (strncat(buffbyte_array3) followed by atoi function call). Here's what the protocol says :  --------+--------+--------+--------+------------------------------ |Version| 3 byte length | Remaining stuff --------+--------+--------+--------+------------------------------ Version is 1 byte followed by 3 byte length of the message. I hope I could clarify my problem Thank you all for your prompt help. I got it. I am kind of implementing a protocol from an RFC and so needed all this information. Thanks a tonne :) Use XDR (RFC 4506).  The use of atoi function is only justified when the string that you are expecting to decode was build by your own code and no further than a couple of lines above. I.e it is only usable in sketch-like code. Otherwise especially in your case when the data arrives from the network atoi function cannot be meaningfully used to perform decoding since it provides no usable error handling mechanism and absolutely no protection from overflow (undefined behavior on overflow). The only function that can be meanigfully used for string-to-integer conversion is a function from the strto... group strtol in your case.  You're storing as ASCII where you should be storing the bytes themselves. The encoding should be something like: uint32_t a = 0xff00ffaa; unsigned char byte_array[1024]; Notice how I made your target array unsigned to indicate that it's ""raw bytes"" and not actually characters. byte_array[0] = a >> 24; byte_array[1] = a >> 16; byte_array[2] = a >> 8; byte_array[3] = a >> 0; This serializes the variable a into the four first bytes of byte_array using big-endian byte ordering which is sort of the default for many network protocols. You may also want to see my answer here: question 1577161.  1) it sort of work since you use an array of characters for the transportation I would use a binary protocol personally. If you can use the 4 bytes of your variable I would take a look to htonl/ntohl functions (they are on virtually every unix and on windows since w2k) else see below 2) with a binary protocol encoding would be uint32_t a = 0xff00ffaa; char byte_array[1024]; // this is the array to be transferred over the network // leave byte_array[0] for version byte // leave the high order byte in a since you want only the 3 lowest byte_array[1] = (char)((a & 0x00FF0000)>>16); byte_array[2] = (char)((a & 0x0000FF00)>>8); byte_array[3] = (char)(a & 0x000000FF); and decoding would be uint32_t a = 0; a |= byte_array[1]<<16; a |= byte_array[2]<<8; a |= byte_array[3]; Ugh - so much wrong with this example. 1. If you're going to use the network translation routines (htonl() and friends) then you MUST transfer the entire variable. On a little-endian machine this example will chop off the least-significant byte (0xaa in this case). 2. If you're going to transfer by stripping out the bytes yourself then there is no need to use htonl() - that function ensures that the binary endian format matches the network format. When stripping out the bytes you're DEFINING an endian format. You are right about the choping I merge a first example with htonl but taking the high order byte and another without htonl but without the high order byte as requested in the question. I'll edit my answer to remove the calls to htonl.  What you're doing will sort-of work. You're not transferring the bytes of the data - you're transferring the numeric value of the data. As a result a buffer of size 5 is way too small for the data you're sending (0xFF00FFAA has a numeric value of 4278255530 - 10 bytes). To transfer the bytes you need to do something like the following (assumes little endian): Encode: char array[1024]; // outgoing network data int next = 0; array[next++] = value & 0xFF; array[next++] = (value >> 8) & 0xFF; array[next++] = (value >> 16) & 0xFF; array[next++] = (value >> 24) & 0xFF; These statements strip off the bytes of the value and assign them to successive values in your array. Decode: char array[1024]; // incoming network data int next = 0; value = 0; value |= (int)*((unsigned char*)array)[next++]; value |= (int)*((unsigned char*)array)[next++] << 8; value |= (int)*((unsigned char*)array)[next++] << 16; value |= (int)*((unsigned char*)array)[next++] << 24; These statements pull the bytes out of the array and push them back into the value. If you want to try to optimize your network format and not transfer bytes you can eliminate some of the data. But remember that your sender and receiver need to know from each other what to expect - so there needs to be some communication of what the type or length of data elements being passed is.  It's probably best to use some existing tool. If you can't - do you care about endianness (i.e. is this a cross platform protocol?) Otherwise you can simply do something like... unsigned char msg[1024]; int writeIndex = 0; [...] int mynum = 12345; memcpy(msg + writeIndex  &mynum sizeof mynum); writeIndex += sizeof mynum; and to decode //[...] also declare readIndex; memcopy(&mynum msg + readIndex sizeof mynum); readIndex += sizeof mynum; (you could replace the notion of msg + index with an unsigned char pointer though this is unlikely to matter). Using memcpy like this is liable to be slower but also more readable. If necessary you could implement a memcopy clone in a #define or inline function - it's just a short loop of assignments after all.  Maybe you need to make this work with an existing protocol in which case ignore my answer. Rather than reinvent the wheel here why don't you use Google's Protocol Buffers library to do this job? Simpler more flexible and very efficient.  At least to be portable you should think about possible different byte order on encoding. Do you really need to implement new networking messaging protocol? Don't NASA IPC or Sun RPC suit you? They both are stable enough NASA is simpler to startup RPC seems available more widely (yes it is ready to use and library is available for most popular systems). For RPC try 'man rpc' For NASA IPC look here  What you have will not work in the manner in which you have it. For example a is 32 bit and in your example you the high order bits set which means it cannot fit into a 4 digit number with your printf statement. (0xff00ffaa = 4278255530 which is more then 4 digits) I believe it will overflow the buffer. I believe printf will convert it and overflow the field but it depend on how your compiler/C implements the printf function when there is not enough buffer space. For the printf statement you have the maximum value you could pass in would be 9999 for 4 characters. Likewise in you example of transferring the data with the 3 byte length field you would have a maximum length of 999. In theory your length could be 1000 if you added 1 to the length but the buffer you have declared is 1024 where the maximum buffer length you would need would be 1004 bytes long. Using ASCII characters does make messages/data portable across the system but it is at the expense of using more bandwidth/space and programming time and effort to convert back and forth from ASCII to transfer the data. It seems like you have a good idea but it still needs a bit of work."
913,A,"Is it possible for me to accept a connection and have it die withouit my knowing then accept antoher connection on the same socket number? Is it possible for me to accept a connection and have it die withouit my knowing then accept another connection on the same socket number? I've got a thread to do protocol parsing and response creation. I've got another thread to handle all my network IO and one more thread to handle new incomcing connection requests. That makes three threads total. Using select in the IO thread I get a failure and have to search for the dead socket. I am afraid there is the case that accept might want to accept a new connection on a socket number that was previous dead. I'd assume this can't happen until I ""shutdown() || close();"" the socket that may be dead on the server side. If it could happen is the only solution to setup mutexes to halt everything while I sort out what sockets have gone bonkers? Thanks Chenz A socket descriptor wont get reused until you close it. Assuming we're talking TCP then if the remote side closes its send side of the connection then you'll get a recv() returning 0 bytes to tell you of this. Since TCP support half closed connections you could still be able to send data to the remote side of the connection (if your application level protocol is made that way) or you might take the fact that the remote side has closed its send side as an indication that you should do the same. You use shutdown() to close either your send side or your recv side or both sides of the connection. You use close() to close the socket and release the handle/descriptor for reuse. So in answer to your question. No you wont be able to accept another connection with the same socket descriptor until you call close() on the descriptor that you already have. You MAY accept a connection on a new socket descriptor; but that's probably not a problem for you."
914,A,"Binding UDP sockets where port<1024? I think I already know the answer but just to make sure. I am trying to listen for UDP broadcasts on a port below 1024 under iOS and bind() returns permission denied. On OS X I can get it to work if I run as root. Is there a way around this problem On iOS devices (without jailbreak)? Why do you need a low port? I am working with a proprietary protocol that sends UDP broadcasts on a port below 1024. Changing that is going to be difficult. Could you talk to a computer on the same network that you put a relay software on? @thejh That would be a serious compromise for our customer. I think the solution will be to rework the backend to use a higher-numbered port. Short answer: You can't. Long answer: On most UNIX systems you need root privileges to listen on ports below 1024. Since you can not obtain root privileges on iOS using the normal SDK and APIs this is simply impossible to do. The only way to do this would be to jailbreak the device.  I partly agree with badcat's answer. But it is not all the truth and kind of confusing. On MAC OSX you need root privileges to use port below 1024 while it is fine for using it on IOS. I did run an app on my ipad and iphone using port below 1024. It works fine without jailbreak. You can see complains from Android community ""https://code.google.com/p/android/issues/detail?id=4039"". So I think Apple did something smart!"
915,A,"Inconsistent packet delivery by NIC NIC performance measurement One of our customers experiences problem with our streaming application (win32). It seems like UDP (RTP) packets that should be sent by the application with some constant interval (say 20 ms) are actually sent with a greatly variable deltas (say 15ms - 25ms - 10ms - 30 ms). This is the only customer that experiences the problem so network card or other OS network related infrastructure is our primal suspect. The question is what kind of network configuration may introduce such problem (AV?QOS?) And how can I measure the time between actually calling ""send"" function and the moment the packet was actually delivered to the network? Is there any tool available for it. If you are saying that you are measuring this directly on the NIC of the computer actually generating the packets (i.e. so can discount all network influences) then a possible cause is the load on the computer itself. If there are many applications running on the computer especially interactive ones and ones with a strong user interaction bias (which tend to get priority from most schedulers) then you may find that your application creating the messages is simply finding it hard to compete for the time it needs. Even if all your customer computers have the same software loaded what applications they are actually running and what they are doing with them may have an influence.  I suspect any network issue can cause this problem. There's no concept of QoS (quality of service) with basic UDP (even to the point that you can lose packets have duplicates etc.). Your network card has to queue up packets to write to the network and so you can't guarantee deliveries since it's queueing up packets from different applications. Routers can prioritise as well and that will affect the regularity of these packets. EDIT: You've pointed out the local NIC so the above re. routers doesn't apply in this situation. In short there's no reason at all to expect that the above is anything other than acceptable. This is the only customer that experiences the problem. No one talks about perfect timing but variability of packet sending times with this customer is way beyond normally observed Pay attention we are talking about capturing local NIC so no router or networking problem is involved.  Guys the problem was actually the timing functions of windows indeed it turn out that Sleep() may have resolution of more than 15 ms. unless you are programtically set it to one 1ms. So no relation what' so ever to NIC."
916,A,"How do I calculate network speed: either ""goodput"" Bandwidth or real Bandwith? I'm trying to determine the latency and bandwidth between the server and various clients in a multi-tier system. I believe the easiest item to get is the ""goodput"" or the effective Application-layer rate but the latency is a bit harder to get at yet harder still is the true bandwidth that includes ""window size"" and other technical factors. I'm looking for either a description of how to most accurately calculate this from either the client or server perspective... as long as I am able to run the server on Azure. I'm open to (but not limited to) using IIS to stream a series of bytes a TCP-based WCF service but the solution needs to be .NET based. UPDATE I added a bounty to this question since there is so much information out there... and yet much seems to conflict each other or have missing data especially in a .NET implementation. I'm most interested in having 2 implementations C# or Silverlight. Some networking Gurus have said I need to take the ""slow start"" of TCP window size into consideration. I hate to see a good bounty go to waste so I'll attempt to answer my own question with the information I have available. [Latency] Measuring actual (on the wire) latency and speed using Silverlight is not possible since we don't have access to TCP sockets and can't access such low-level interfaces from the sandbox. That also kills the ""TCP SlowStart"" as well.That being said it's possible to infer WCF latency and Goodput bandwith. WCF Service 2 using System; 3 using System.Net.NetworkInformation; 4 using System.Text; 5 6 public class Service : IService 7 { 8 public bool PingNetwork() 9 { 10 return true; 11 } 12 } And the client side:  using System; using System.Windows; using System.Windows.Controls; using PingDemo.PingServiceReference; namespace PingDemo { public partial class MainPage : UserControl { private ServiceClient client = new ServiceClient(); private DateTime PingStart = new DateTime(); public MainPage() { InitializeComponent(); client.PingNetworkCompleted += new EventHandler(client_PingNetworkCompleted); } void client_PingNetworkCompleted(object sender PingNetworkCompletedEventArgs e) { if (e.Error == null && e.Cancelled == false) { MessageBox.Show(""Ping Status: "" + e.Result); } } private void btnPing_Click(object sender RoutedEventArgs e) { if (client == null) { client = new ServiceClient(); } PingStart = DateTime.UtcNow(); client.PingNetworkAsync(txtHostIP.Text.Trim()); TimeSpan Est_RoundTripTime = DateTime - PingStart; //may need to look up exact syntax } } } [Upload Download] In a similar manner that Latency is determined by using a WCF service host upload and download speed can be estimated by downloading a known (uncompressable) set of data to the client and back. The following blog discusses an approach to send large data over WCF. This can be used to calculate the speed in which data is sent/transmitted using a similar manner as above. http://kjellsj.blogspot.com/2007/02/wcf-streaming-upload-files-over-http.html  Did you try WCF tracing and message logging? If you turn on tracing ane message logging on the service and also on clients you can open all logs in Service trace viewer and check all end to end information. My server and one client lives in Azure. I'd like to get the result from the trace logs programatically and use that to adjust application behavior at runtime. I'm not sure how to accomplish this in my situation based on this Sorry I don't have any experience with Azure so my answer is probably pointless. I guess you should tag this question with Azure tag because it can have big impact on a solution for your problem. Good idea re: tagging; Also thank you for the tip to check the trace viewer. I'll +1 for it's general helpfulness it once more replies come in The Azure is not changing the tracing at all. You can still enable WCF Tracing while in Windows Azure (its just a matter of configuration setting in web.config before all and Windows Azure runs Full IIS) and monitor the trace files if this is required.  I've been considering a technique for a while that could produce decent results with regard to determining latency. If the client and the server have synchronized clocks with very high accuracy (against a common low-latency source; i.e.: time.nist.gov) simply sending the current timestamp on the server to the client would allow the client to compare its local time with the server's time. The difference in timestamps would yield the latency. This could be used in combination with large ""dummy"" file transfers to determine latency during sustained transfers and under different network loads."
917,A,How to communicate without prior knowledge of IP Im developing an app which uses the tcp connection. currently im communicating using hard coded IP addresses as a sample but in the real world this is not the case i think. We come across any mobile and start communicating/sharing etc without having any prior knowledge of other person's IP. In such a case how to get the IP address of those who are using my application. How can i communicate without the prior knowledge of the IP address. How to implement this. Help me in sorting it. Thanks in advance. EDIT: And in case if the user connects the internet thro' GPRS/3G connection then his IP address will be changing based on the service provider. How can i find that. Are you trying to connect your users through the Internet or is it enough to connect them on the local LAN (with WIFI for example)? @Alejandro Mezcua: Im trying to connect the user thro' internet. You should _really_ learn the basics of computer networking. Comparing TCP and DNS only shows that you have a bigger problem than what you have asked in the question. @Octavian: Maybe im not good in networkingbut i need to start with this. So i started learning the basic stuff. provide me some ideas to move towards the app. (Some samples for android and procedures to start withetc..). Can you use DNS? You can still hard-code the domain name if you want and the domain name can be configured to point to any IP addresses anytime later. Wats the advantage of using DNS over TCP. then how can i know who all are using my app using DNS. Wats the procedure. Can u give some code samples which explain this. Thanks  Your app must use DNS. You will first need get your IP into the DNS system. You do this by signing up for a domain name & setting up A / AAAA records for hostnames with a hosting provider (or you could set up your own DNS server). You may even find some free DNS providers. In your app you can hard-code a fully qualified domain name that you previously set up say - app.foo.com and use the android gethostbyname library call to fetch the IP address for you. The local DNS resolver will then go to its DNS server and fetch the IP address corresponding to app.foo.com.  Given what you are commenting on other answers if what you want are the IP addresses of other Android devices to do some kind of P2P game you should note that there is no reliable way to do that directly. If the users are connected in the same LAN you could provide in your application some kind of discovery service using UDP broadcasts. If the users are connected to the internet and have public IP addresses then you could use some intermediate server to register the users at startup and have them discover other users using that server. But (and this is the most common case) if the users are connected to the Internet and have private IP addresses (like when they are connected with WIFI on their LAN) you need some kind of server that acts as a proxy for their requests because there is no way to make a direct TCP connection from a natted IP to another natted IP. There are a few solutions to solve this problem you could start for example by learning something about XMPP and how it works. thanks for your help. U mentioned some intermediate servercan u elaborate on that. Thanks again Not really because it greatly depends on your application and protocol. I'd first take a look at how others do it so that you can learn and the adapt your app to whatever setup you think is best for your application. Okay.Thanks a lot for your great help man.
918,A,"How to make a server send a processed data back to the invoking iPhone? I am sorry if the title is unclear. Allow me to elaborate further. Firstly I have an application on an iPhone that will allow the user to upload a data to server. We simply use php to allow the user to upload the data. Secondly the server will process this data and then return the processed data back to this iPhone. This part is the part that I am still having trouble. My question is ""how to make the server send the processed data back to the invoking iPhone?"" Are there some sort of ""listening to server"" method in iPhone SDK or there are other ways to consider? In the case of some errors in connection is it possible for the server to know which iPhone to send the data back to? Well the iPhone is able to make HTTP requests as you will have to do to send the data to your PHP app on the server. In the HTTP response is where you'll put the processed data. You'll want to look at The URL Loading System doc for info on how to use it. The response you get back will be in something like XML or maybe JSON in which case you can use a 3rd party library. If you can't do this in one request - for example if your server side processing takes hours - the only way you'll be able to send the data back is with a Push Notification. There's no listening web server that is running on the iPhone that you would be able to send requests to. Can you provide an example of how this polling works? Isn't that for media playing or something and doesn't it lock up the main thread? thats (Push Notification) not the only wayyou can use polling for example to get back the response and there is probably a way of programing a socket to get data pushed in not to mention making an async request in which the data will come back eventually if they have programmed the web service to reuturn a response (as long as the app is open)"
919,A,Is that possible to create our own server using TcpListener class with IIS being uninstalled? Is that possible to create our own server using TcpListener class with IIS being uninstalled? Yes. You can listen for incoming TCP/IP connections with TcpListener and implement any protocol you want. If you want to handle HTTP yourself a better approach might be to use HttpListener rather than implementing the HTTP protocol yourself.  Yes TcpListener uses TCP/IP stack and has nothing to do with IIS but if you are trying to create a WebServer using TcpListener it'd be kind of waste of effort don't you think? +1 for answering. The reason is that to overcome hidden weaknesses in IIS. Could you elaborate? What is the hidden weakness in IIS? I can't reveal the weaknesses openly but a report being submitted to MS.
920,A,How to receive UDP packets from any ip and any port? I wanted to use C#'s UdpClient to listen to any incomming UDP packets. I want to receive packets from any IP and any port. I tried the following: UdpClient udpClient = new UdpClient(0); IPEndPoint ep = new IPEndPoint(IPAddress.Any 0); byte[] data = udpClient.Receive(ref ep); but without success. Does anyone know whats wrong? Thanks in advance! Pretty sure that you cannot listen to 'every' port like that... don't see why you would want to either. @Fosco: Network sniffer? @MPritch: Then that's not really UDP. You'd be listening on raw ethernet packets. Fosco and Steven Sudit (below) are correct. You can not not listen to 'every' port by specifying a port of 0. What happens is that the system picks an ephemeral port for you. You are now listening for UDP packets on a port you didn't pick and no application knows to send to. It does make sense to specify port 0 and use an ephemeral port if you are sending UDP datagrams or if you have a separate channel (of any kind) for sending the ephemeral port number to the sending applications. Your best idea would be to identify specific ports you would like to listen to and start listening on those. Depending on what is done with received datagrams it might be best/simplest to create a new Thread for each port you are listening on and process it there or enqueue it on a synchonrised (with lock) queue or list for processing on a central thread. You should limit the ports though; it would not be possible to listen to them all. That said you could use something like Wireshark or the Winpcap SDK/API to 'sniff' UDP packets right from the network adapter. I have had it working within a .NET application before without too much difficulty. Hope that helps. Example API: pcapdotnet http://pcapdotnet.codeplex.com/  RECEIVE on any port? That's insane. You would be flooded with messages from other applications (try TcpView for an idea of how many messages get passed on your system per second!) You must specify a port! Port is sort of like an identifier -- this packet is meant for THIS program (identified by port #) Send on any port is sensible as it asks the system to choose a port send OUT port for you -- which isn't really that important to your application as the sender sometimes  You need to listen on a specific port. By passing in zero you are assigned an arbitrary port so you will receive only UDP datagrams addressed to it. In other words you will receive nothing. If you did receive something the IPEndPoint would be filled in with information about the sender. The initial value could be used to constrain the sender.
921,A,getting remote endpoint for asynchronouse receive When I want to received UDP datagrams asynchronously I write call BeginReceiveFrom of a System.Net.Sockets.Socket class. The BeginReceiveFrom method expects an Endpoint as the remoteEP parameter. public IAsyncResult BeginReceiveFrom ( byte[] buffer int offset int size SocketFlags socketFlags ref EndPoint remoteEP AsyncCallback callback Object state ) This Method starts an asynchronous receive and cannot return any result in remoteEP since it returns immediately. Is this parameter for a kind of filtering or will it be modified on receive completion? In the receive handler I call EndReceiveFrom where I also have to pass a reference to an Enpoint object as the endPoint parameter. public int EndReceiveFrom ( IAsyncResult asyncResult ref EndPoint endPoint ) The EndReceiveFrom indicates with this Endpoint object the sender of the UDP frame. Why must I pass a remoteEP to the BeginReceiveFrom and can I avoid this? There's no overloads that do not require an endpoint but that doesn't mean you have to specify specific endpoints. You can specify any ip address as the remoteEP. EndPoint remoteEP = (EndPoint)(new IPEndPoint(IPAddress.Any 0)); EndReceiveFrom expects a pass by ref variable that will be populated by the method call itself. You can instantiate the endPoint similarly. EndPoint receivedFromEP = new IPEndPoint(IPAddress.Any 0); After the call is made simply ignore the value of receivedFromEP if you do not need it. I do not have any documentation to provide a definite answer to your question perhaps somebody else does. My guess is that it has to do with binding to network interfaces security and convention. Oftentimes applications send data bidirectionally so it makes sense to know where data came from. You will likely have more than one client as well so a seperation of data and maybe even ordering of packets could be necessary (subsets of TCP functionality). I believe not needing any client information is more the exception to the rule. So it makes sense to populate the EndPoint by default. So what is the remoteEP parameter in BeginReceiveFrom for? Why do I create an IPEndPoint that's ignored anyway?
922,A,"Why can't a bind linux service to the loop-back only? I am writing a server application that will provide a service on an ephemeral port that I only want accessible on the loopback interface. In order to do this I am writing code like the following: struct sockaddr_in bind_addr; memset(&bind_addr0sizeof(bind_addr)); bind_addr.sin_family = AF_INET; bind_addr.sin_port = 0; bind_addr.sin_addr.s_addr = htonl(inet_addr(""127.0.0.1"")); rcd = ::bind( socket_handle reinterpret_cast<struct sockaddr *>(&bind_addr) sizeof(bind_addr)); The return value for this call to bind() is -1 and the value of errno is 99 (Cannot assign requested address). Is this failing because inet_addr() already returns its result in network order or is there some other reason? I should have done some more experiments before posting. It would appear that inet_addr() already returns the address in network order. instead of making experiments you could read the man page. It says that inet_addr doesn't do any decent error checking and that there is already a constant defined in netinet/in.h called INADDR_LOOPBACK that is exactly what you want. @BatchyX: That constant is nonstandard (not specified in POSIX). @R.: it is nonstandart but it exists on BSD Linux Mac OSX Solaris and Windows and only the ""linux"" tag was set by the asker. inet_addr should be avoided for there is a much saner method of constructing struct sockaddrs (which means it also obsoletes gethostby*): #include <netdb.h> /* Error checking omitted for brevity */ struct addrinfo hints = {.ai_flags = AI_PASSIVE}; struct addrinfo *res; getaddrinfo(""::1"" NULL &hints &res); /* or 127.0.0.1 if you are 60+ */ bind(fd res->ai_addr res->ai_addrlen); freeaddrinfo(res); +1 for avoiding inet_addr  Is this failing because inet_addr() already returns its result in network order Yes. So remove the htonl call. `INADDR_LOOPBACK` is a nonstandard extension."
923,A,"setsockopt fails for IPPROTO_TCP IP_TOS in C My code fails. I am running as root (same behavior as normal user) First I want to set the TOS and then get the value. int tos_local = 0x28; if (setsockopt(sockfd IPPROTO_TCP IP_TOS &tos_local sizeof(tos_local))) { error(""error at socket option""); } else { int tos=0; int toslen=0; if (getsockopt(sockfd IPPROTO_TCP IP_TOS &tos &toslen) < 0) { error(""error to get option""); }else { printf (""changing tos opt = %d\n""tos); } } the printf prints changing tos opt = 0 I would expect to print 0x28 (40). What is the problem? The correct answer:  if (setsockopt(sockfd **IPPROTO_IP** IP_TOS &tos_local sizeof(tos_local))) { int tos=0; int toslen=sizeof(tos); //that line here if (getsockopt(sockfd IPPROTO_IP IP_TOS &tos &toslen) < 0) { IP_TOS has level IPPROTO_IP not IPPROTO_TCP. See the documentation. This affects both setting and getting the option. Also what Seth said about initializing the length parameter which affects only getsockopt. Indeed. And IP_TOS happens to equal 1 as does TCP_NODELAY so this code actually does something... Just not what was intended. @Nemo: Yeah they should have designed these as bitfields instead of separate parameters. But that's water under the bridge.  When calling getsockopt you pass in the size of the memory pointed to by &tos. In other words initialize toslen to sizeof(tos). I get compiler warning error: warning: passing argument 5 of getsockopt makes pointer from integer without a cast when I pass sizeof(tos). Additionally the getsockoption fails with ""Bad Address"" @cateof: passing `&tos_len` as the parameter is correct but you need to declare it as `int tos_len = sizeof (tos);` and not `0`. @cateof: @Ben Voight: I think you mean passing &tos_len as the parameter is correct but you need to initialize it to sizeof(tos) and not 0. just caught that myself ;)"
924,A,"how can i choose a particular process to run in perl Is there a command or function that i can create to choose to run anyone of these nmap scans that i desire instead of how my code runs now which is the first ip address scanning and the rest follow? Here is my code: (`nmap -v -r 99.xxx.xxx -p 1-200`); (`nmap -v -r 98.xxx.xxx -p 1-200`); (`nmap -v -r 96.xxx.xxx -p 1-200`); Insufficient data for meaningful answer. How should it choose which one to run or whether to run one of them at all? What do you mean by ""instead of"" how you code runs now? How *does* your code run now? You need to use @ARGV: system(""nmap -v -r $ARGV[0] -p 1-200""); And call your script with the address in the CLI argument such as perl foo.pl 99.1.1.1 And if not `$ARGV[0]` then at least some variable. However the code should also assign the results to a string variable. With the backticks as shown (in Perl) the output is captured and thrown away. So: `my $scan = qx(nmap -v -r $ip_address -p 1-200);`. I see you added the back-ticks to the question - was there a reason for that? It might be that it is just a case of 'send output to stdout` in which case `system` might be a better choice or something roughly equivalent. I just copied the OP's code verbatim other than adding `@ARGV`... thank you for catching the `system()` optimization"
925,A,"Do I have to enter src and dst mac address in forged packet? I have started coding a packet injector and read up a bit on what one has to do. One thing I'm wondering though is when I create the IP header the h_source and h_dest fields should contain the mac address of the sender and receiver. Do I have to do this and is there a quick way to find out the mac address of the destination? Let's say I craft a ICMP ping packet or some arbitrary TCP packet. Would be nice to just be able to say ""send it to 192.168.0.10 from 192.168.0.1"" and not having to care about the mac addresses. I guess the kernel normaly fills this in but letting it interfere here would not allow me complete control over the packet. I'm afraid you've got that wrong IP has no knowledge of the MAC adress only the ethernet layer knows that. That's why you need the ARP protocol to determine which mac adress to send an IP packet to. Normally you know which subnet you belong to if your destination IP is local you ARP for the MAC and send it diretly (this is usually done at a much lower level though). If it's not on your local subnet you ARP for the gateway IP and send it there instead which will forward it somehow. The only source and destination present in the IP header are the source and destination IP addresses. HTH. EDIT: For clarification. When sending via the gateway the IP packet is normally not touched (except TTL and because of that the checksum). There are situations where the IP packet must be fragmented but that's a different issue. The point is the source and destination addresses remain the same it's only on the layer below where we're actually saying it should be sent via some gateway or router. @Nikolai; how is that an exception? The computer on the inside doesn't care about that? Or you mean exception to the ""normally not touched""? Yeah that's what I meant by normally there are situations where you do want to change the IP header (e.g. NAT or something else you might want to tweak such as flags or if you need fragmentation). But I was trying to make a point regarding protocol layer separation.. :) @roe you might want to mention NAT which is so common with home and corporate IPv4 networks now as an exception to your statement about gateways. I meant to say the ethernet header... I mixed it up with the ip header... sorry Well @inquam you don't have to create the mac header if you just want to send IP packets. If you want to forge the mac-source and destination then quite naturally you'd have to fill those in yourself as well I really don't see your question then anymore. If I want to craft a packet by hand I thought I had to do all the headers myself... But can I get the kernel to do the ethernet header for me and I can muck around as much as I want with the ip header and tcpheader and it's options? @inquam; that's what raw sockets are for see this http://www.tenouk.com/Module43a.html for example. You do need to specify the protocol in the `socket` call in this case to let the kernel know where it's supposed to look for the destination address (IP in this case)  Yes you will need to fill this in. You can use ARP to determine the MAC address of a given IP address: http://en.wikipedia.org/wiki/Address_Resolution_Protocol"
926,A,"Can select() be used for clients not just servers? I'd like to make a TCP client that makes multiple connections while a select() loop that receives data from them is running in a separate thread. I'm not sure this is possible though because the select() loop is already running and thus I don't see how it would ""notice"" a new socket was added even if the thread-safety issues are dealt with. Is there a way to do this or must I spawn a new thread and use recv() every time I make a new connection? (Edited for clarity.) One simple way to do this is to also select on a pipe. After you arrange things so that the thread will also select on the new connection you write a single byte on the pipe. This causes the thread to come out of select. When it notices the pipe is readable it reads the bytes to 'reset' the pipe so it's ready for use again updates its file descriptor sets and goes back to selecting.  Of course it is possible. The select() function accepts file handles in three sets one for read one for write and one for errors. Just add your socket to the read set and you'll be noticed when the server has sent you something. This page has code showing how this is done. Would it be safe to assume this is impossible? I could always just make a new thread for each socket and use recv() but I plan to make a lot of connections during the course of the program so it may turn out to be a lot of threads. Ok thanks so even though select() is running in a loop and a socket is added from a totally different thread where connect() is used it will still notice that the read set just received a new addition and will begin receiving data on it? It's a bit counterintuitive but I'll try it out. @oskar: I see ... I don't think it's impossible (few things are after all :) but it might be complicated. You would need some sort of ""incoming connection queue"" in which the threads that accept the connections can place the new sockets. The main thread then before each call to select() needs to go through and clear the queue adding all new socket(s) to the FD_SET. Something like that. Huh? Threads? I thought you said ""single thread"" in the question. Please edit your question to more clearly describe how you use threads. The FD_SET is (very probably) not thread-safe so you need to take care if adding sockets to a shared set from many threads. Sorry I assumed that was clear because select() has to run in a loop thus unless I make all my connections before getting into the loop I'm sort of forced to make my connections in a separate thread. By ""single thread"" I meant that I want to receive all my _data_ from all my current connections in a single thread but I still need a separate thread to establish those connections. I've edited the question for clarity. If you call `connect()` on a non-blocking socket the call returns immediately and `select()` returns the socket as ""writable"" when the connection is established. So you could open the connections in the ""receiving"" thread.  Another good reason to select() on client sockets is to track outgoing TCP connections progress. This allows for example to setup connection timeout. Set client socket to be non-blocking. Call connect(). Probably it would return with EINPROGRESS error set (connection is in progress you are not blocked because socket is non-blocking). Now select() with FD_SET configured to track client-socket as 'write-ready'. Also you can set timeout. Analyze select() result. Analyze if last client socket operation was failed or succeed. The most useful thing is you can use this on several sockets in different state. So you get truly non-blocking handling of number of sockets (client server outgoing listening accepted ...). And all of this with only one thread."
927,A,"How do I fix this blocking issue? 119 while(remainLength > 0){ 120 if(remainLength >= MAX_LENGTH){ 121 log(""WorkHandler::workLoop remain %d > max %d \n"" remainLength MAX_LENGTH); 122 currentSentLength = send(client->getFd() sBuffer MAX_LENGTH MSG_NOSIGNAL); 123 log(""currentSentLength %d \n"" currentSentLength); 124 } 125 else{ 126 log(""WorkHandler::workLoop remain %d < max %d \n"" remainLength MAX_LENGTH); 127 currentSentLength = send(client->getFd() sBuffer remainLength MSG_NOSIGNAL); 128 log(""currentSentLength %d \n"" currentSentLength); 129 } 130 131 132 if(currentSentLength == -1){ 133 log(""WorkHandler::workLoop connection has been lost \n""); 134 break; 135 } 136 sBuffer += currentSentLength; 137 log(""sBuffer %d\n"" sBuffer); 138 139 remainLength -= currentSentLength; 140 log(""remainLength %d \n"" remainLength); 141 142 } I have this code and the send function sometimes gets stuck. Some people points out that using non bloking I/O. I am using epoll so I think it is a little bit hard to change the whole design to non blocking mode. Is there any way to prevent blocking the send function? Thanks in advance.. If you don't want send to block you'll need non-blocking I/O. There's no way around that. You don't have to put the socket in non-blocking mode though the MSG_DONTWAIT flag can be used on a per-call basis. But you will need to deal with EAGAIN/EWOULDBLOCK error codes. From the man page linked above: The flags argument is the bitwise OR of zero or more of the following flags. so you can combine that with MSG_NOSIGNAL. I am already using MSG_NOSIGNAL. Can I use both MSG_NOSIGNAL and MSG_DONTWAIT? Could you please show some sample code ? Thanks in advance Please read the man page linked it answers that question. You can find sample non-blocking code pretty easily with your favorite search engine (or this site's search feature for that matter)."
928,A,Load testing strategies for network server applications I am building a Windows service that listens for connections over TCP and once a connection is established sends a steady stream of data. It is very simple... once the initial request is made the communication is one-way (on the application layer of course). What is a good way to test this with significant load say up to 3000 simultaneous connections? Are there standard tools for this kind of thing or should I just write my own? I have the feeling you have written your own protocol so I think you're like stuck to implementing your own load testing client. Writing a load testing client for something like this isn't hard. The only thing I would advise though is to really think about whether you're going to test on a connection from one physical machine or multiple. I'm not saying you should test from 3000 physical machines but something like 10 or 20 machines would be a good idea. Otherwise you have a good chance the client machine being the bottleneck. Also to really test the server you have to have a good load test implementation. If you see the processor usage of your load tester staying at 100% you're not testing the server; you're testing the client. Thanks for the advice. I think I will likely go this route. Perhaps when I am done I will build a generic test utility and release it for others to use. Something that can send a pre-defined chunk of data to a server and accept a response. I am not-so-confident in my optimization abilities which is why I was hoping a standard tool already exists but checking for the condition of the tester being overloaded is easy enough. Thanks again. Well the best advise I can give is to go for an async implementation (`BeginRead`/`BeginWrite`) instead of multiple processes. This is one of the most important points and will increase your throughput of the client.
929,A,"What is easiest way to send structured data via tcp using java? I have to connect to a existing C system and the tcp packet looks like typedef struct exampleDataPacket{ int messageType; float dataValue; char dataDesc[100]; } So to send this kind of data using java I can't find anything in my books. What would be the best way to send/receive this kind of data? Thanks CP Remember that Java is going to have things in network byte order; it is likely the C programmer was lazy and is not expecting that (at least in my experience) and you may have to compensate for their mistake and do some byte swapping.  You can use a DataOutputStream if the data you're trying to write is compatible with how it's going to be read on the C side. Check the documentation on writeFloat and writeChar carefully to see if that will actually work. Otherwise you'll have to use a raw OutputStream and encode the data into bytes yourself. Edit: Just wanted to add - my best guess is that for your purposes writeFloat will work but writeChars will not due to encoding issues. Instead you'll have to use the getBytes method on your Java String to get a byte array in whatever single-byte character encoding your C code is expecting then write those bytes to the DataOutputStream. I would emphasize ""Check the documentation ... carefully"". When moving primitives across the wire you're very likely to run into problems with encodings and different byte ordering. Not only that but everything in Java is signed while many C/C++ variables tend to be unsigned."
930,A,Please suggest other ways of communicating between server & client I'm writing a TCP chat server ( programming language does not mather ). It's a school project for my nephew so it won't be released and all questions I'm asking are just for my knowledge :). . Some of the things it will support: chatting between users ( doh ) it will be multithreaded sending each other files I know I could easily get away with all the stuff above if I go with serialization and just send objects from client to server and back. But if I do that it will be limited to a specific programming language ( meaning clients written in other programming languages may not be able to deserialize the objects ). What would be the way to go so that other clients written in other languages could be supported? One way to go off the top of my head would be to go in this direction: the server & the client communicate by sending messages & chunks ( in lieu of other names ). Here's what I mean by this: every time the client/server wants to send something ( text message or file ) it will first send a simple text message ( newline terminated ) with the number of the chunks it will send. Example: command 420304050 Where command would be something like instant-message or file4 would be the number of chunks to be sent 20 would be the size in bytes of the first chunk 30 of the 2nd and so forth. after the message was sent the client/server will start sending chunks ( of sizes mentioned in the sent message ). What do you think about implementing the client/server communication this way? What better options are there? TCP/IP over Avian Carrier as per RFC 1149: http://www.rfc-editor.org/rfc/rfc1149.txt That sounds fine basically you need to define a message header (as you have done) so that a receiver will know how many bytes to read for each message. I suggest placing a message size as the first parameter instead of a variable-length command string. That way there is no ambiguity in case the command happens to arrive across 2 separate TCP packets.  A pretty typical solution here is Web Services using SOAP.  What you say about serialization isn't true. You can use a cross-platform serialization protocol like protocol buffers. This will make your life easier and save you from implementing your own communication protocol. In my opinion it will be better to find an existing protocol and implement this instead of trying to make your own. Something as simple as xmodem could do. Also to avoid client software having to act both as server and as client (meaning having to solve peer identification issues) you could have all clients communicating through a centralized server.  I might pass XML between the clients; that'd work. For passing files perhaps just pass a byte buffer; only problem there is if you're going between high endian and low endian machines I think. In fall of 2003 a classmate of mine in a graduate systems modelling class did a simulation study. The result was that using XML to encapsulate data was good for two orders of magnitude of performance hit. The problem is that when you take all the coding into effect XML tends to wrap about 800 bits (100 bytes) of verbiage around every actual 8 bits (1 byte) of actual data. @John R. Strohm: Create your own DTD and keep it short. I've found that most of the heavily verbose XML DTDs are a disaster but This is the message would be valid as would a7x@dz...  Silly question: Why not use TELNET protocol over TCP/IP for the chat stream and something like FTP over TCP/IP for file transfers?
931,A,"How do i check the data sent to the IP Address on particular port? I am sending data to the IP address 127.0.0.1 on port number 5152. Through socket programming i am sending the data (""Hello world"") . I am receiving a acknowledgement as sent 51 Bytes. But how do i know the data received is correct or not in the above IP address. I have created a server application  and i am using a same IP and port number here. // Create our listening socket // sListen = socket(AF_INET SOCK_STREAM IPPROTO_IP); if (sListen == SOCKET_ERROR) { printf(""socket() failed: %d\n"" WSAGetLastError()); return 1; } // Select the local interface and bind to it // if (bInterface) { local.sin_addr.s_addr = inet_addr(szAddress); if (local.sin_addr.s_addr == INADDR_NONE) usage(); } else local.sin_addr.s_addr = htonl(INADDR_ANY); local.sin_family = AF_INET; local.sin_port = htons(iPort); if (bind(sListen (struct sockaddr *)&local sizeof(local)) == SOCKET_ERROR) { printf(""bind() failed: %d\n"" WSAGetLastError()); return 1; } Bind fails with 10048 error. You sent your data - now you need to receive it ! You'll need to create a server that listens on port 5152 on the same box and have it display what it receives from your client. i have given the same IP address and port number in the server. Is that a problem? Bind is failing in the server application. Code i have used is if (bind(sListen (struct sockaddr *)&local sizeof(local)) == SOCKET_ERROR) { printf(""bind() failed: %d\n"" WSAGetLastError()); return 1; } listen(sListen 8);  Your error is ""address already in use"". That means some other program has the port open. You can see a list of programs listening using ""netstat"" at the command line. Perhaps that will help you figure it out. Wireshark is a handy application to see what's being sent via the network. Very handy for debugging these kinds of programs.  Depend on what you're connecting to. If it's your application the fact that the socket opens proves that you already have a listening socket : just display what it receives (or configure the traces on the server to display it). Another idea when dealing with network development is to monitor actual network traffic using wireshark."
932,A,.NET Communications Component I'm looking for a component for C#.NET 2008 Professional that is capable of doing the majority of the network communications work that is required of our application so our programmers don't have to. This component should function similarly to the way RealThinClient (RTC) does. The component must be able to gracefully lose and regain connections. It must have encryption built-in. It must have compression built in. RTC is capable of doing connection pooling as well and this is something that we would also like to have. A little background about us: we were previously a Delphi 7 shop and are moving to using Visual Studio .NET 2008. We just located RTC before we made the decision to move to .NET and are wishing that there was something that duplicated the functionality available. :-) Thanks for your help and links are much appreciated. Michael Beck You should look at IPWorks from www.NSoftware.com Not only do they have a Dot Net library they have a Delphi component as well. This might help you convert. If you buy their Red Carpet Subscription you'd get all libraries.  WCF Windows Communication Foundation is a set of libraries in the .net framework 3.0 that abstracts application communication from your code. All of your communications are abstracted so that you use your business objects as if there was no actual communication going on. It also allows you to fully configure what types of communication protocols security etc. you want to use without you changing your code. This may not be exactly what you are looking for but WCF should always be considered when doing application communication in a .net system. Start here http://social.msdn.microsoft.com/content/en-us/msft/netframework/wcf/GettingStarted
933,A,"Linux network programming. What can I start with? I've recently got interested in Linux network programming and read quite a bit (Beej's Guide to Network Programming). But now I'm confused. I would like to write something to have some practice but I don't know what exactly. Could please recommend me a couple of projects to start with? Thanks. The book to read is Unix Network Programming by R. Stevens: http://www.kohala.com/start/unpv12e.html The project to do is a TCP Proxy or a Web Proxy. Make it single-threaded but of course handle multiple connections. You should be able to finish it in one week of spare-time work.  I suggest finding any computer networks laboratory course page and solve the relevant assignments. There you'll find detailed explanation of problem and assignments will be in the increasing order of complexity. You can find such webpages by searching this in Google ""inurl:edu computer networks lab assignments"". (without qoutes) Here are some excellent assignments (with very detailed explanations). Try to implement all of them in this order http://www.facweb.iitkgp.ernet.in/~agupta/netlab/Assgn1.pdf http://www.facweb.iitkgp.ernet.in/~agupta/netlab/Assgn2.pdf http://www.facweb.iitkgp.ernet.in/~agupta/netlab/Assgn3.pdf http://www.facweb.iitkgp.ernet.in/~agupta/netlab/Assgn5.pdf http://www.facweb.iitkgp.ernet.in/~agupta/netlab/Assgn6.pdf http://www.facweb.iitkgp.ernet.in/~agupta/netlab/Assgn7.pdf  Write a very simple stupid web server which will accept connections on port 80 and serve back pages. Then you can extend it to support other media download (images etc.). Then you can add some (prebuilt) script language interpreter which will process pages in PHP Perl etc. You'll learn a lot in the process. Except don't try to listen on port 80. That requires privileges. List on port 8000. Slightly simpler for beginners.  I'm not sure how in-depth you want to start your Linux Network Programming career but if you want to just get started with dealing with sockets probably the easiest examples are a Producer/Consumer pairing or an Echo Server. Another good source would be looking at some of the examples/assignments from any number of University/College courses on Distributed computing. Producer/Consumer This could be run in a pair of terminals on your computer to test. Create two applications: Producer program starts with a hostname and port takes a line of input from the user connects to the Consumer sends it the input asks for another line of input and finishes when it reaches EOF (Ctrl-D). Consumer program starts with a listening port waits for a connection from the Producer reads the message sent by the Producer outputs that message closes the connection with the Producer exits gracefully when sent an interrupt (Ctrl-C). Echo Server Similar idea to Producer/Consumer. Echo Server starts with a listening port waits for a connection reads a message from the Client sends that same message back to the Client and exits gracefully when sent an interrupt (Ctrl-C). Echo Client starts with a hostname and port takes a line of input from the user connects to the Server sends the input reads back the response compares the two to verify it was an echo asks for another line of input and finishes when it reaches EOF. I like the `Echo` idea a lot. You could simplify further by making it a `Ping` like pair :)  I can recommend grabbing Wireshark - it will help you understand the to-and-fro of network traffic. Although Wireshark is a great product how it could possibly help someone in learning network PROGRAMMING ? I'd say learning about network flows is vital to network programming. Plus it can easily help with debugging. And it's really really interesting :-)  I would start by developing a simple multiplayer game like tic-tac-toe: use ncurses (to easily program your gui) manage a lobby in which players can join manage games (started by a players that asks to play to another one) manage ladder or rankings manage a global chat for everyone that joins and a local chat for people that are playing of course I suggested tic-tac-toe but you could choose another similar game (with simple rules).. the important part is to have to care about many clients that are also playing in couples (to handle data forwarding and game managing) and sending states (like games list) to players. The good thing of this example is that you have a two-level protocol: first level to handle global action second level to handle a single game I had to make a Boggle game like this in school. Learned a lot."
934,A,Pinging nodes in Network I have a network consisting nodes(computers with ubuntu) with 1 node as root. I am building a fault management system for it for which lets say root sends IP address of a node A to node B(through sockets). Now node B has to ping node A repeatedly to check the network. How should I do this. I mean is there a way to write C/C++ program which given an IP address pings the node and report the problem. Or do I have to write some script or anything? You could use Perl and the Net::Ping module a short script could do this. Or use something like Big Sister for network monitoring. I think that would work
935,A,How can I access network device counters such as tx/rcv packets/bytes from kernel code? I'm writing a driver and I want to get network counters of some device within it. What data structure holds that information and how can I access it? First use dev_get_by_name to get a pointer to the struct net_device of the interface. Then the net_device structure has a pointer to the stats function which is declared as: struct net_device_stats *(*get_stats)(struct net_device *dev); The net_device_stats structure contains the information you want.
936,A,"Advice for program with network/automation/authentication purpose I have an idea for a program that will do a particular job for me; a job which I keep forgetting to do. I am looking for some advice on how to get started with it i.e. language recommendations that will allow me to complete the program. The chore is as follows: I open up a web browser visit the web page log in click a tab in my account and ""sign on"" log out close browser. Due to having so much on at the moment with work I forget about this and it is important that I do it before a particular time in the morning. Aside from alleviating me of the chore I am keen to learn about the software technologies that will allow me to automate this and work on my novice programming skills some more. The way I perceive this of working would be: Write program that runs as a daemon checks date and time every 30 minutes (to ""sign on"" half an hour before deadline) either calls another program or methods/functions within its own programming that opens up a socket port and IP before deadline automates text entry of username and password fields within the HTML of log-in page's source automate/action the Javascript ""submit"" button accept/store cookie action the appropriate radio button's associated Javascript function for ""sign on"" action ""log off"" Javascript function nullify cookie/session end. I am unfortunately for me a little hazy in the technical aspects that this would involve. I run Linux so perhaps the daemon will be a cron job and the actual main bulk of the program would be methods/functions called up at the appropriate time before the deadline? Is this better accomplished in Ruby Perl Python or Java? Or Perhaps the cron-job associated with a simple script that calls a Java program? I apologise but I am a little naive here as software isn't my subject so some advice would be greatly appreciated. So too would be if you could point me in the direction of the right resources to read as well. I am aware of Ruby's Mechanize library but I am only familiar with Java. However willing to pick up another language. This sounds like something that depending on the structure of their website could easily be handled by curl. man page: http://linux.die.net/man/1/curl In essence you can write a cron job that calls a little bash script. Your bash script will just be a couple curl statements that log you on and request the ""sign on"" page."
937,A,Mobile peer-to-peer connections I want to make some software that would be great if it do not needed a server at all. It is possible to make an internet connection between two mobile devices? Does someone have a clue on what I need to research for iPhone Android and Blackberry? You can do a direct connection with UDP or TCP but you will encounter logistical problems. For example if you are on the same wifi network all will be good. But if one phone is on a wifi network with a NAT'd address and the other is on a public address you will have problems since the private NAT'd address is not route-able. If the phone with the private knows the ip of the public address phonethe private ip phone can make the initial contact. This is why people use server to have one central location for everyone to meet and connections can be established. What is both phones are behind a NAT and not on the same network? for example in an office your may have a 10.x.x.x private network if both phones are on that network communication between them is not a problem.
938,A,"How to uniquely identify a network? Let's say I want to make an application where stored data is encrypted so only my application is able to read it. But I want the application to be accessed only if the user is on a particular network. For instance the application is an Android app that deals with medical records in a hospital. How to be sure that the device is connected to the network of the hospital ? The idea is that outside this network the app won't work. The question is not particularly related to wireless networks wireless devices or Android this is general to programming and network identification. Could a certificate do that ? I'm new to this. Does a network ""identifier"" could be faked ? For instance I'm pretty sure that a WiFi SSID is easy to fake. Cheers. More details: Let's assume that the point of the local data is not for an ""offline mode"" but to avoid network latency. In that case the data needs to remain accessible only if connected to a particular network in case the device is stolen. But if there is no way to be sure of the network's identity... What about a server that would answer to the question ""Heya am I on the right network ?"" and if no response comes out I know that I'm not on the right one ? (Or that the server just does not respond...) But again if the app is hacked that can be faked too. Yeah thanks @blowdart and sorry about that. Perhaps you could use IPSec tunnels. Many routers and firewalls support IPSec. What I'm thinking is something like this:  ----------------------------------- / IPSec tunnel \ +---------+ \ A | IPSec | B Untrusted \ trusted network -------| capable |------- Networks ----------- Your application | router | Internet etc. +---------+ The gateway router/firewall that provides access for the trusted network has an IPSec tunnel configured between itself and your application. On both the router and your application server the tunnel looks like another network interface. A route on the router directs traffic for your application to the tunnel interface. A filter can be used on the router to ensure traffic is forwarded to the tunnel only if it arrives on interface A (i.e. the trusted network). Traffic arriving on interface B destined to your application can just be dropped by a filter on the router since it's obviously going the wrong way. If your application binds its listening socket to just the tunnel interface you'll know you're only accepting connections received over the tunnel. You can use whatever combination of encryption and authentication mechanisms you want to to ensure the traffic is secure. Most IPSec implementations support just about anything you could want. +1 for the drawing :)  Interesting problem. Generally speaking the purpose of storing data locally is so that it can be accessed while ""offline"". However I think there may be some underlying misconceptions here. Presumably the only reason you'd want to do this is to try and prevent a stolen device from giving up it's secrets. Fact of the matter is you can't. If the device is no longer under your physical control then it's just a matter of time before it can be hacked. If we are talking about sensitive data it shouldn't be stored on the devices. Instead the device should retrieve the data it needs from your server when it needs it and delete it locally when no longer necessary. The fact that you want the device to only work when connected to your local network implies that you can accomplish this goal. As a side note this is why things such as ""remote wipe"" exist. It's also why every time the device connects to your network it needs to test it's authentication and authorization. Point is if someone reports the device lost or stolen then you need to be able to ban it from your network AND if the device supports this remotely disable it. Bearing in mind that it is entirely possible to pull a device from the network and therefore disable a remote wipe from executing. With that out of the way there is absolutely no way you can ensure the device is on a given network. All of that can be faked. It's kind of trivial to setup a router of a given name and change it's MAC to masquerade as whatever and assign it certain IP addresses. For all intents and purposes it could be made to look exactly like an access point you have... And that's just with normal run of the mill wireless routers you can buy at your local computer store. Agree with the misconception fact. If the point of the local data is that it can be accessed while offline trying to identify the network is pointless... Thanks for the ""remote wipe"" I will take a look at that too.  You could write your program so that the key to decrypt the data is stored on a server on the hospital network. If your program never stores the key it makes it harder (although not impossible) for someone to access the device's data outside of the network. As Chris pointed out a remote wipe would definitely be desirable. You could put in logic so that if the device ever attempts to read the data while not connected to the network it wipes the data (this might lead to unintended wipes). Blacklisting is good too so that if the device tries to reconnect to the network you can essentially brick it. One thing that would be really bad is if you have a network outage and all your devices accidentally get wiped. The first paragraph has the right idea - the only way to do this securely is to ensure that something that is technically required to read the data is only available on the right network. You should though use many keys - one per record say - and have the application forget the key immediately after using it. +1. With the idea of erring on the side of caution I for one would like to have the devices accidentally wiped in the event of a network outage. But when the key is retrieved from the server I'm subject to a Man-in-the-middle attack. Right ? I'm definitely not a security expert but you could use a secure method of transferring the key (SSL). Even if someone can sniff the key they would need to also access the device and your decryption methods program to get the data.  Any network can duplicate another's SSID so that's not reliable. You could start using a combination of SSID and a MAC address of a known router but MAC addresses can be duplicated (although not on the same network) so that doesn't work either. Frankly unless the wireless network in question is using certificates to identify devices you're going to have no reliable way do it it and even then this supposes you have a way in your application to get the certificate used the wifi network returns during network authentication."
939,A,nmap ports scan closes udp socket on windows open by ipworks v4 Upon receiving UDP packets from nmap (port scanner) UDP socket that were open by ivrworks (v4) is being closed without any notification. Has anyone encountered such a problem seems like ipworks closes socket upon empty udp packet. ipworks8 solved the problem
940,A,"How to handle pending connections to a server that is designed to handle a limited number of connections at a time I wonder what is the best approach to handle the following scenario: I have a server that is designed to handle only 10 connections at a time during which the server is busy with interacting with the clients. However while the the server is busy there may be new clients who want to connect (as part of the next 10 connections that the server is going to accept). The server should only accept the new connections after it finishes with all previous 10 agents. Now I would like to have an automatic way for the pending clients to wait and connect to the server once it becomes available (i.e. finished with the previous 10 clients). So far I can think of two approaches: 1. have a file watch on the client side so that the client will watch for a file written by the server. When the server finishes with 10 clients it will write the file and the pending clients will know it's time to connect; 2. make the pending clients try to connect the server every 5 - 10 secs or so until success and the server will return a message indicating whether it is ready. Any other suggestion would be much welcome. Thanks. Of the two options you provide I am inclined toward the 2nd option of ""Pinging"" the server. I think it is more complicated to have the server write a file to the client triggering another attempt. I would think that you should be able to have the client waiting and simply send a READY signal. Keep a running Queue of connection requests (from Socket.Connection.EndPoint I believe). When one socket completes accept the next Socket off the queue."
941,A,"Not able to receive or send entire packet in socket programming using C Since two days I am working on my code for sending a packet from client to server via sockets in C. With the help of some of the tips I have made quite bit of progress in my code. But now with my below code I see that I am able to communicate between client and server. I have told my server to print the number of bytes it received. It shows me that only 8 bytes were sent by Client and prints them out. But at my client side it prints that it has sent everything. Now I am not sure whether the problem is at sending side or receiving side. My hunch is that something is wrong at my client side. It would be really of great help if someone sees something drastically wrong in my code which I am not able to see right now. Client: int main(int argc char *argv[]) { int sockfd portno n; struct sockaddr_in serv_addr; struct hostent *server; data_struct client_data; struct packet { long int srcID; long int destID; int pver; int profiles; int length; long int data; }; if (argc < 3) { fprintf(stderr""usage: %s hostname port\n"" argv[0]); exit(0); } portno = atoi(argv[2]); //Convert ASCII to integer sockfd = socket(AF_INET SOCK_STREAM 0); // socket file descriptor if (sockfd < 0) error(""ERROR DETECTED !!! Problem in opening socket\n""); server = gethostbyname(argv[1]); if (server == NULL) { fprintf(stderr""ERROR DETECTED !!! no such server found \n""); exit(0); } bzero((char *) &serv_addr sizeof(serv_addr)); //clear the memory for server address serv_addr.sin_family = AF_INET; bcopy((char *)server->h_addr (char *)&serv_addr.sin_addr.s_addr server->h_length); serv_addr.sin_port = htons(portno); printf(""Client 1 trying to connect with server host %s on port %d\n"" argv[1] portno); if (connect(sockfd(struct sockaddr *)&serv_addrsizeof(serv_addr)) < 0) error(""ERROR in connection""); printf(""SUCCESS !!! Connection established \n""); char buffer[256]; struct packet *pkt = (struct packet *) buffer; char *payload = buffer + sizeof(struct packet); long double packet_size; printf(""Started Creating packet\n""); pkt->srcID = 01; pkt->destID = 02; pkt->pver = 03; pkt->profiles = 01; pkt->length = 16; pkt->data = 1; 2; 3; 4; 5; 6; 7; 8; { if (send(sockfdpktsizeof(packet_size)0) <0) printf (""error\n""); else printf (""packet send done""); } return 0; } Server: int main(int argc char *argv[]) { int sockfd newsockfd portno clilen; struct sockaddr_in serv_addr cli_addr; int n; char wish; long int SrcID; long int DestID; int Pver; int Profiles; long int Data; int Length; char bytes_to_receive; int received_bytes; struct packet { long int srcID; long int destID; int pver; int profiles; int length; long int data; }; if (argc < 2) { fprintf(stderr""usage: %s port_number1""argv[0]); exit(1); } sockfd = socket(AF_INET SOCK_STREAM 0); if (sockfd < 0) error(""ERROR DETECTED !!! Problem in opening socket""); bzero((char *) &serv_addr sizeof(serv_addr)); portno = atoi(argv[1]); serv_addr.sin_family = AF_INET; serv_addr.sin_addr.s_addr = htonl(INADDR_ANY); serv_addr.sin_port = htons(portno); if (bind(sockfd (struct sockaddr *) &serv_addr sizeof(serv_addr)) < 0) error(""ERROR DETECTED !!! There was a problem in binding""); listen(sockfd 10); clilen = sizeof(cli_addr); printf(""Server listening on port number %d...\n"" serv_addr.sin_port); newsockfd = accept(sockfd(struct sockaddr *) &cli_addr &clilen); if (newsockfd < 0) error(""ERROR DETECTED !!! the connection request was not accepted""); char buffer[256]; struct packet *pkt = (struct packet *) buffer; char *payload = buffer + sizeof(struct packet); long double packet_size; bytes_to_receive = sizeof(packet_size); received_bytes = 0; int rc =0; while ((rc = recv(newsockfdpktsizeof(packet_size)0)) > 0) { received_bytes+=rc; SrcID = pkt->srcID; DestID = pkt->destID; Pver = pkt->pver ; Profiles = pkt->profiles; Length = pkt->length; Data = pkt->data; printf(""Data Received from Client_1 are :\n""); printf(""Source ID: %ld\n"" SrcID); printf(""Destination ID: %ld\n"" DestID); printf(""profile Version: %d\n"" Pver); printf(""No of Profiles: %d\n"" Profiles); printf(""Length: %d\n"" Length); printf(""data : %ld\n"" Data); } if (rc == 0) { printf(""Connection closed by Server\n""); printf(""Bytes received: %d\n""received_bytes); } if (rc == -1) { perror(""recv""); } { if (close(newsockfd) == -1) { error(""Error closing connection with client 1""); } printf(""Connection with client 1 has been closed\n""); } return 0; } The output that I am getting are: Client Side: Client 1 trying to connect with server host 130.191.166.230 on port 1234 SUCCESS !!! Connection established Started Creating packet packet send done and Server Side: Data Received from Client_1 are : Source ID: 1 Destination ID: 2 profile Version: 0 No of Profiles: 1074462536 Length: 0 data : 0 Connection closed by Server Bytes received: 8 Connection with client 1 has been closed Please look at how the code got formatted - the indentation is totally unreadable. I answered one like this just yesterday: http://stackoverflow.com/questions/6791022 because the number of bytes you are sending is 8 bytes. This line `send(sockfdpktsizeof(packet_size)0)` is equivalent to `send(sockfdpkt 8 0);` what is the meaning of this line `pkt->data = 1; 2; 3; 4; 5; 6; 7; 8;` but i defined packet_size as long double...which is 16 bytes right?? i want numbers 123 inside tha data field of packet. @Nikolai: I read your reply for somewhat similar question so what you suggest that I should buffer the received data or send in some other fashion.I am confused as where exactly i am wrong...while sending or receiving the size of long double depends on the platform. It could be 8 10 12 or 16 (as far as I know). This is why we told you to worry about serialization. Anyhow declare packet_size as `int` and then assign a value for `packet size = sizeof(struct packet)` in both the client and the server. BUT remember this code will run only on the same type of machines. @badawi: Yes that has solved the problem. Now in my data field I want to send numbers 123456.......these can be random and max can be 1000. Right now I just see Number ""1"" at the o/p. How to achieve this?? any idea?? @badawi let us [continue this discussion in chat](http://chat.stackoverflow.com/rooms/1770/discussion-between-user537670-and-badawi) It looks like the OP needs to read a basic C programming book about the syntax of the language and how to assign to fields in a struct. This hash nothing to do with socket programming really. @Chris Dodd: I am amateur with C programming.....can you guide how to proceed it would be of great help....thanks A simple search http://www.google.com/search?q=learn+C or http://stackoverflow.com/search?q=learn+C will give you lots of pointers... There is an informal rule that nobody is allowed to write any software that uses TCP until they memorize this sentence and can fully explain what it means: ""TCP is a byte-stream protocol that does not preserve application message boundaries."" What that means is that TCP only ensures that you get out the same bytes you put in and in the same order. It does not ""glue"" the bytes together in any way. Before you write any code that uses TCP you should either use a protocol that is already designed (such as IMAP or HTTP) or design one yourself. If you design one yourself you should write out a protocol specification. It should specifically define what a protocol-level message will consist of at the byte level. It should specifically state how the receiver finds the ends of messages and so on. This may seem a little silly for simple applications but trust me it will pay off massively. Otherwise it's almost impossible to figure out why things aren't work because if the server and client don't quite get along there's no arbiter to say what's right.  When you call send the function returns the number of bytes actually sent and this number can be less than the number of bytes you wanted to send. So every time you want to send something there must be a loop like the following bool sendBuffer(SOCKET s unsigned char *buf int size) { while (size > 0) { int sz = send(s buf size0); if (sz < 0) return false; // Failure size -= sz; // Decrement number of bytes to send buf += sz; // Advance read pointer } return true; // All buffer has been sent } and a similar loop must be done when receiving (in other words recv can return less bytes than what you are asking for). If you don't make these loops the risk is that everything apparently will work anyway (until the size of an ethernet packet) when you work on your local machine or even over a LAN but things will not work when working across the internet. Note also that as other answers pointed out you asked to send sizeof(packet_size) i.e. the number of bytes required to store that variable not the size of the structure. @dreamlax: typo thanks Are you sure your function should return `bool*`? @6502: thanksOne more thing in the data field of my packet i want to send the numbers 123456......this can be random and max limit of numbers is 500. Here it shows that it just received number ""1"". How to achieve this?? Is it because I am not to doing the loop thing that you suggested.  First of all recv(newsockfdpktsizeof(packet_size)0)) /* What is packet_size ? */ recv(newsockfdpktsizeof(struct packet)0)) /* You probably mean this. */ That might solve your problems but there are a few issues with the way you are using TCP sockets. But at my client side it prints that it has sent everything Where ? I don't see you actually checking the number of bytes sent. send(2) can return after sending less that you asked it to. It shows me that only 8 bytes were sent by Client and prints them out. TCP is a stream-oriented protocol. You send bytes and they arrive in the same order. So when you recv(2) something you might get less (or more than you wrote). So the following can be true: client: send 100 bytes send 400 bytes server: recv 50 bytes recv 150 bytes recv 250 bytes recv 50 bytes The number of send and recv calls need not be identical when using TCP. so you mean to sayclient might be sending all the bytes but my recv is recving only a few so how can i gurantee that sending and receiving is proper.... Refresh the page and see my answer (what is `packet_size`?). You can keep receiving until you get the entire `packet`. packet_size is defined as ""long double"". @user537670 `struct packet` is bigger than that thanks now it shows that it received 24 bytes...One more thing in the data field of my packet i want to send the numbers 123456......this can be random and max limit of numbers is 500. Here it shows that it just received number ""1"". How to achieve this?? just a small note for the example: if the client sent 100 bytes (first send) that will fit in one segment and you can read the whole thing on the server side. @yi_H: so you mean to say it is not able to send the entire data?? @yi_H Yes it does fit in a segment. But the receiver is free to split it whichever way it likes (although in most cases it will just `recv 100`). @user537670: that was for the answerer. it looks like you have prodigious amounts of questions you should read Steven's networking bibles (TCP Illustrated vol1 UNIX Network programming). @yi_H: I am on it...will look for that book....But any idea @ my question how to send different numbers in the data field and server receiving it??The data field of my packet i want to send the numbers 123456......this can be random and max limit of numbers is 500. Here it shows that it just received number ""1"". How to achieve this?? There are dozens of problems with your code solving that one problem won't help you. You need to allocate memory (or use a large enough pre-allocated array) fill it with the numbers for efficiency send with writev so it's one call then fix the receiver side to handle partial ""packets"" and so on... this is not a horse race first try to understand the subject then write the code. then if you don't understand *one* thing feel free to ask.  I don't specialise in socket programming but there are a few things I've noticed. As far as I'm aware I don't think you can send structs over sockets that easily. You may wish to consider a different method. NB when using send/recv you're also determing the sizeof packet_size and not the sizeof the struct. Googling brought up this about sending structs over sockets: http://ubuntuforums.org/showthread.php?t=613906"
942,A,"How do you throttle the bandwidth of a socket connection in C? I'm writing a client-server app using BSD sockets. It needs to run in the background continuously transferring data but cannot hog the bandwidth of the network interface from normal use. Depending on the speed of the interface I need to throttle this connection to a certain max transfer rate. What is the best way to achieve this programmatically? The problem with sleeping a constant amount of 1 second after each transfer is that you will have choppy network performance. Let BandwidthMaxThreshold be the desired bandwidth threshold. Let TransferRate be the current transfer rate of the connection. Then... If you detect your TransferRate > BandwidthMaxThreshold then you do a SleepTime = 1 + SleepTime * 1.02 (increase sleep time by 2%) Before or after each network operation do a Sleep(SleepTime) If you detect your TransferRate is a lot lower than your BandwidthMaxThreshold you can decrease your SleepTime. Alternatively you could just decay/decrease your SleepTime over time always. Eventually your SleepTime will reach 0 again. Instead of an increase of 2% you could also do an increase by a larger amount linearly of the difference between TransferRate - BandwidthMaxThreshold. This solution is good because you will have no sleeps if the user's network is already not as high as you would like. @Brian: Why is that '1 +' necessary? Won't SleepTime = 1.02 * SleepTime by itself increase the value by 2%? I just added it so that if your SleepTime gets to 0 it will be able to grow again. Also so that it will always grow by at least 1 when it has to grow. If the protocol utilises acknowledgements then the delay before a response is received could also be used to provide end-to-end pipeline delay and suggest the rate at which to transmit messages. This would provide an adaptive solution that would permit self-tuning in multiple installations.  I've had good luck with trickle. It's cool because it can throttle arbitrary user-space applications without modification. It works by preloading its own send/recv wrapper functions which do the bandwidth calculation for you. The biggest drawback I found was that it's hard to coordinate multiple applications that you want to share finite bandwidth. ""trickled"" helps but I found it complicated.  The best way would be to use a token bucket. Transmit only when you have enough tokens to fill a packet (1460 bytes would be a good amount) or if you are the receive side read from the socket only when you have enough tokens; a bit of simple math will tell you how long you have to wait before you have enough tokens so you can sleep that amount of time (be careful to calculate how many tokens you gained by how much you actually slept since most operating systems can sleep your process for longer than you asked). To control the size of the bursts limit the maximum amount of tokens you can have; a good amount could be one second worth of tokens."
943,A,Hex values in Protocol headers Why are the protocol headers like TCP or UDP normally using Hex representation while filling the particular fields in the protocol header ? Is there any specific advantage ? well that's for convenience in reality if you inspect the actual packet they are all binary data which for sake of readability is shown as hex  Depending upon the fields in question (flags such as SYN FIN ACK RST URG PSH ..) it is easiest to set the fields using bitshifting arithmetic (0x1 < TCP_OFFSET_SYN) and OR | or AND & the results with the existing field. Bitshifts just go easier with hexadecimal than decimal and is often more convenient to read than octal. It boils down to whoever wrote the code you're reading probably thought hex was more understandable than decimal in that instance but this is obviously subjective. Your opinion may vary. :)
944,A,"Standard way of using a single port for multiple sockets? Hey I am writing an app in Twisted and as it stands I have 4 servers bound two different ports all communicating with the client via JSON. Is there anyway to bind these 4 servers to the same port and have the interactions remain the same? For instance say the client subscribes to two different feeds transmitted via a direct socket. Right now I just do like server1.read_string() server2.read_string() and it will read the correct JSON string from the respective feeds. Is there anyway to maintain this type of functionality but contact my server on the same port? I do not want to throw all of the server functionality into one massive server and partition the data by header prefixes. I don't want to do something like s = server.read_string() header = s.split(//some delimiter)[0] if (header == ""SERVER1"") { // Blahh } In order to have multiple servers running on the same machine all bound to the same port they need to be bound to different IP addresses. The only way to bind to the same port on the same IP is to enable the socket's SO_REUSESOCKET option but then multiple servers would be able to receive each other's inbound data really messing up your communications. Otherwise having a single server that uses headers to identifies the particular feeds is best. Why do you not want to do that? I would need the headers on the client side as well yes? Is there a better way to transmit header data than directly in the body of the message like I have shown above? The header has to be in the message data itself. When a message is received on either end the receiver can identify which feed the message relates to.  It sounds like you have many clients interacting with your servers via HTTP. The standard solution is to throw a reverse proxy between the client and your servers - that proxy then forwards connections to the appropriate server depending on the URL. The reverse proxy can run on any one of your existing servers or on its own server to lighten the load. If your data is cachable the reverse proxy can do caching on your results too. There are many reverse proxies available and you will want to choose one based on what sort of workload you have. Do you need it to be highly configurable? Is the data public or based on logins? How long does each connection last / how many connections to you want to hold open at once? Squid Varnish HAProxy are good reverse proxies and even Apache could do this for you. I plan to use HAProxy for Gridspy my project as I have many ongoing connections with my clients and want to place an orbited server in the same URL path as my django server. See This tutorial for more information on how to forward many connections on port 80 from one server to many. This tutorial is focused on Comet but your problem is even simpler than that. If you are considering an ongoing tcp/ip connection from the browser back to your servers seriously consider Orbited. See this tutorial about graphs via orbited and morbidQ. Orbited will also punch through firewalls and proxies better than most custom solutions will as it looks like normal HTTP traffic."
945,A,"How to wake a thread after receiving 2 packets I'm using libnetfilter_queue for my project. From C app queue is accessible by ""queue file descriptor"". I have 5 queues and 5 threads to handle them. What I want to achieve is to wake thread when there is exactly 2 packets in queue. I came up with idea to use select function and array of ints indicating how many packets were queued in each queue. After select exit with > 0 code I check which queue has received a packet and increment value in array if it's bigger than 2 I wake up a thread. Everything would be fine but select indicate that queue has data to read until I call recv and I can't do that because separate thread should handle these packets. Anyone has idea how to solve this issue? I know I can set SO_RCVLOWAT but it does not solve my problem because I don't know what size will be those 2 packets. why would you need anything like this? As recommended by Tobu epoll is a better choice and it performs better than select. However most of these polling functions will indicate there is an event (data available) unless someone reads. If possible use the following model: Use epoll/select to watch for the incoming data wake up the worker thread. Let the worker thread decide what to do with the data (one packet two or more) before actually doing the work. OR: One Reader thread-N Worker threads: Will use epoll to wait and read all the incoming data and post it to the corresponding worker thread's queue. Once the # of packet reaches the threshold wake up the Worker thread (using a semaphore).  You are looking for edge-triggered event notifications — notifications that are sent when the quantity of available data changes. epoll works like that when using the EPOLLET flag and by default will rearm the notification so that you keep being notified of new packets. Please note that you will be notified only once if several packets arrive between two epoll_wait calls. i see your point. If I spent too much time between epoll_wait call and in that time more than 1 packet was recived i can't determine how many were received. Thanks for that information Is it really _when the quantity of available data changes_ or when it goes from empty to non-empty (for receive buffer)? I'm pretty sure it's the former: whenever a packet arrives. The kernel code justs sets a flag when ep_poll_callback is called and doesn't track if the buffer was full or not at the time."
946,A,"getaddrinfo and IPv6 I'm trying to understand what the getaddrinfo function returns : #include <stdlib.h> #include <sys/types.h> #include <unistd.h> #include <sys/socket.h> #include <netdb.h> int main (int argc char *argv[]) { struct addrinfo *res = 0 ; getaddrinfo(""localhost"" NULL NULL&res); printf(""ai_flags -> %i\n"" res->ai_flags) ; printf(""ai_family -> %i\n"" res->ai_family) ; printf(""ai_socktype -> %i\n"" res->ai_socktype) ; printf(""ai_protocol -> %i\n"" res->ai_protocol) ; printf(""ai_addrlen -> %i\n"" res->ai_addrlen) ; struct sockaddr_in* saddr = (struct sockaddr_in*)res->ai_addr; printf(""ai_addr hostname -> %s\n"" inet_ntoa(saddr->sin_addr)); freeaddrinfo(res); return 0 ; } results : ai_flags -> 40 ai_family -> 2 ai_socktype -> 1 ai_protocol -> 6 ai_addrlen -> 16 ai_addr hostname -> 127.0.0.1 In /etc/hosts I 've got : 127.0.0.1 localhost ::1 localhost Getaddrinfo returns only 127.0.0.1 and not ::1 ? I don't understand why ? The second question is where can I find the meaning of those ints (40216 etc) ? I've read the man but there is nothing about that. I also wanted to know if it's possible to give a IPv6 adress (for example ::1) and the function returns the name : localhost ? Thanks a lot !! According to the manual page getaddrinfo allocates space and fills a linked list beginning at &res. I think it would return more than one entry and you need to iterate through the linked list by res->ainext extern struct sockaddr_in6 create_socket6(int port const char * address) { struct addrinfo hints *res *resalloc; struct sockaddr_in6 input_socket6; int errcode; /* 0 out our structs to be on the safe side */ memset (&hints 0 sizeof (hints)); memset (&input_socket6 0 sizeof(struct sockaddr_in6)); /* We only care about IPV6 results */ hints.ai_family = AF_INET6; hints.ai_socktype = SOCK_STREAM; hints.ai_flags = AI_DEFAULT; errcode = getaddrinfo (address NULL &hints &res); if (errcode != 0) { perror (""[ERROR] getaddrinfo ""); return input_socket6; } resalloc = res; while (res) { /* Check to make sure we have a valid AF_INET6 address */ if(res->ai_family == AF_INET6) { /* Use memcpy since we're going to free the res variable later */ memcpy (&input_socket6 res->ai_addr res->ai_addrlen); /* Here we convert the port to network byte order */ input_socket6.sin6_port = htons (port); input_socket6.sin6_family = AF_INET6; break; } res = res->ai_next; } freeaddrinfo(resalloc); return input_socket6; } Here is some code that explains it. Basically unless you give getaddrinfo some hints to tell it to only work with IPV6 it will also give IPV4 results. That is why you have to loop through the results as shown. I fixed your memory leak.  @jwodder and @onteria_ covered the IPv6 portion well so I'll just tackle the numbers portion: ai_flags -> 40 Probably this is going to be the sum of the following two in /usr/include/netdb.h: # define AI_V4MAPPED 0x0008 /* IPv4 mapped addresses are acceptable. */ # define AI_ADDRCONFIG 0x0020 /* Use configuration of this host to choose This is the protocol family inet inet6 apx unix etc.: ai_family -> 2 bits/socket.h:78:#define PF_INET 2 /* IP protocol family. */ bits/socket.h:119:#define AF_INET PF_INET This is the socket type stream dgram packet rdm seqpacket: ai_socktype -> 1 bits/socket.h:42: SOCK_STREAM = 1 /* Sequenced reliable connection-based The higher-level protocol TCP UDP TCP6 UDP6 UDPlite ospf icmp etc: ai_protocol -> 6 Funny enough in /etc/protocols: tcp 6 TCP # transmission control protocol The size of the struct sockaddr. (Differs based on the address family! Ugh.) ai_addrlen -> 16 This is because you're getting back a struct sockaddr_in see linux/in.h: #define __SOCK_SIZE__ 16 /* sizeof(struct sockaddr) */ struct sockaddr_in { sa_family_t sin_family; /* Address family */ __be16 sin_port; /* Port number */ struct in_addr sin_addr; /* Internet address */ /* Pad to size of `struct sockaddr'. */ unsigned char __pad[__SOCK_SIZE__ - sizeof(short int) - sizeof(unsigned short int) - sizeof(struct in_addr)]; }; And the last one from /etc/hosts :) ai_addr hostname -> 127.0.0.1 It's really interesting ! thanks for your answer !  res also contains a field struct addrinfo *ai_next; which is a pointer to additional entries found by getaddrinfo or NULL if there were no other entries. If you examine res->ai_next you should find the IPv6 entry. As for the integer fields in a struct addrinfo they correspond to predefined constants with implementation-defined values and the integer values themselves are not of general interest. If you want to know what a given field means compare it against the constants that can be assigned to that field (SOCK_STREAM SOCK_DGRAM etc. for ai_socktype; IPPROTO_TCP IPPROTO_UDP etc. for ai_protocol; and so forth) or for ai_flags test each bit corresponding to a predefined constant (e.g. if (res->ai_flags & AI_NUMERICHOST) {printf(""ai_flags has AI_NUMERICHOST\n""); }).  Other answers have been given to most parts but to answer this final part: I also wanted to know if it's possible to give a IPv6 adress (for example ::1) and the function returns the name : localhost ? The function you want there is getnameinfo(); given a socket address it returns a string name."
947,A,Networking for games? I want my game to be entirely server side. Meaning the client only sends their keystrokes. They then are given updates of the changed objects' positions from the server. The client then renders everything each frame. This is a 2D game I was thinking of something like this: calculate the inbetween frame using Box2D and do not try to predict where the server is in fact going to be. ServerPos -> ClientPos -> ServerPos -> ... If by a certain time we have not gotten a packet (dropped or something) then we just simulate the next frame on client. The idea is to avoid the server always correcting our position. We want the client to fill the inbetweens but not try to predict where the server is going to be. We want to avoid at all costs moving the player in the opposite direction because the client over simulated so we could multiply the resultant vector by a scalar like 0.98 which would mean the client would be slightly slower than the server and would help ensure a smooth transition to the server position. Thanks No. Network latency will make the game neigh unplayable if you control everything server-side. You need only to look at a game that already does this like LaTale. The input delay is horrible.  Just a thought but I think you might be forgetting network latency. Assuming you're really sending the data 60 times per second in response to something the client sends. Than your maximum latency can be 1 second / 60 = 17ms. That will be a problem for just about any internet connection since your application is bound to introduce some latency aswell. So... if you want something like this to work you'll have to have a little buffer window to catch this delay. And that will make it feel less responsive. There's a reason that most online games have some prediction algorithms in place just in case the connection drops/stalls a bit. I think the idea is nice but in practice it probably won't work that well. +1 -- beat me to it =) I was saying pretty much the same thing but backwards lol. @Milo -- I specifically share the same opinion on **latency** being your primary enemy here rather than bandwidth or the server/client being able to handle it. @Milo -- keep in mind: the _best_ latency you can achieve for the _longest_ communication line from A to B on Earth is 66ms if we assume the data travels at the speed of light -- that's very limiting! +1 beat me too. I was going to suggest **ping***'ing from the client to the server -- That will give you the hard fps limit that you cannot exceed. Also you are overlooking issues of UDP dropping packets or TCP getting congesting and lagging as you suddenly get the last 10 seconds worth of packets all at once. -- Edited to add: I suppose you could to some predictive work and send all possibilities out for some time into the future. But then you are talking some serious bandwidth very quickly! Hmm. Upon reflection I suppose you could just have a long delay from when someone clicks until it takes effect. You could send out 60 frames per second even if the response time to a mouse/keyboard click is on the order of 0.25 to 0.5 seconds... But the game... The playability... Ouch!  That will and can work only on LAN or exceptionally on some low-hop server-to-client connections. Try TRACERT to some public server - here is an example: C:\Users\mosh>tracert -d -w 3000 www.google.com Tracing route to www.l.google.com [209.85.149.104] over a maximum of 30 hops: 1 46 ms 99 ms 99 ms 192.168.1.1 2 * * * Request timed out. 3 10 ms 9 ms 10 ms 172.29.16.133 4 10 ms 9 ms 10 ms 195.29.110.230 5 41 ms 41 ms 95 ms 80.81.193.108 6 46 ms 47 ms 127 ms 209.85.255.176 7 49 ms 47 ms 47 ms 216.239.48.11 8 47 ms 47 ms 47 ms 216.239.48.5 9 54 ms 53 ms 54 ms 209.85.254.21 10 51 ms 42 ms 40 ms 209.85.149.104 Trace complete. Every router you have between your server and your clients will add few milliseconds and depending on the 17ms (one frame) delay would make the game unusable.  Sync as few as possible. 60 times per second is too much and not necessary. For LAN sycn once per 100ms is enough; for WAN once per 200ms is a normal case. And which kind of game are you making? The policy is very different for different games. You may need to customize a sync policy for your game. Making a 2D sandbox game with Box2D Physics @Milo - have a look at this http://gafferongames.com/game-physics/networked-physics/  You can't do it this way because you don't know how long it will take for the keystrokes to reach the server. And given that physics changes the state of objects over time that unpredictable timing aspect means you can't guarantee that the server's representation will continue to match the client's. One side has to be authoritative - the other has to predict wait or both.
948,A,How to solve '...is a 'type' which is not valid in the given context'? (C#) i want to call this class from my Winform.But if i write this codes. Compiler send me error. What is solution. Please help me Error : 'CERas.CERAS' is a 'type' which is not valid in the given context using System.Drawing; using System.Linq; using System.Text; using System.Windows.Forms; namespace WinApp_WMI2 { public partial class Form1 : Form { public Form1() { InitializeComponent(); } private void Form1_Load(object sender EventArgs e) { CERas.CERAS = new CERas.CERAS(); } } } CERAS is a class name which cannot be assigned. As the class implements IDisposable a typical usage would be: using (CERas.CERAS ceras = new CERas.CERAS()) { // call some method on ceras }  You forgot to specify the variable name. It should be CERas.CERAS newCeras = new CERas.CERAS();  Change private void Form1_Load(object sender EventArgs e) { CERas.CERAS = new CERas.CERAS(); } to private void Form1_Load(object sender EventArgs e) { CERas.CERAS c = new CERas.CERAS(); } Or if you wish to use it later again change it to using System.Drawing; using System.Linq; using System.Text; using System.Windows.Forms; namespace WinApp_WMI2 { public partial class Form1 : Form { CERas.CERAS m_CERAS; public Form1() { InitializeComponent(); } private void Form1_Load(object sender EventArgs e) { m_CERAS = new CERas.CERAS(); } } }
949,A,"vb6 winsock control RemoteHostIP truncates the last digit from IP address Hi i am writing a socket client/server application in VB6. i have the following code Private Sub sockMain_ConnectionRequest(ByVal requestID As Long) If sockMain.State <> sckClosed Then sockMain.Close End If sockMain.Accept requestID Debug.Print ""Accepted connection from: "" & sockMain.RemoteHostIP & vbCrLf End Sub its printing the IP but the last digit is missing example if my connection is from ""192.168.1.123"" then it shows ""192.168.1.12"" only If socket connection is established are you sure the connection is established from 192.168.1.123. Did you check what IP is shown as the remote IP in tcpview software. If the IP shows 192.168.1.123 then i dont there is any problem with the socket APIs. The problem will be with the way you are printing the IP. I am able see the active connection and able to transfer data and in the debug mode if i check the connection state the socket is in Connected state only. there is no issue with the connection. but the control RemoteHostIP is always shows the IP by truncating the last digit is i connect client from local i am getting 127.0.0. instead of 127.0.0.1 Strange indeed. I've tried the exact same code and it works on my machine. I tried using telnet from the same machine and also from a laptop and the correct IP address was printed in both cases. I have to agree with ckv and say that its the way you are printing RemoteHostIP. Hi CKV and quamrana still i am not getting why in my system the last digit is missing. even if i set a watch for the socket object and checked during debug still shows the RemoteHostIP by truncating the last character. @Dharma: OK that's bad. Can you try a minimal version of the program on some different hardware/OS? If you used exact same code you must have used same function to print the IP also right? So you mean with same code as Dharma has mentioned above you are getting correct IP. Yep I just pasted the code from the OP into my form. Currently i am tied with other schedules..i will check this latermay be this week end. @Dharma: Any chance of trying it on Vista or Windows 7? btw My machine is Windows XP SP3. @Dharma: So what hardware/OS *are* you using to get the symptoms you describe in your question? Except vb6 winsock control other programming languages line VC++ C# and VB.Net i am able to get the RemoteIP address complete. Pentium(R)D CPU 2.8 GHz/Windows XP sp2  This is a known bug in KB957924 Microsoft Visual Basic 6.0 Service Pack 6 Cumulative Update (link) in both v1 and May 2009 v2. This is why some people can duplicate it and some can't. It also is limited to the second and subsequent usage of the control. It is discussed here. As a really ugly workaround you can call recvfrom in the wsock32.dll lib with the sockMain.SocketHandle a small buffer and the MSG_PEEK parameter (&H2) to retrieve the socket address directly. This must be done before calling sockMain.GetData(). Then you have to parse out the IP address yourself. I can post code that does this for the specific case I'm using (UDP) if requested. I'm not sure it will work in your case since it looks like you're using TCP and Accept."
950,A,"How to read tcp packets which have been redirected to localhost? I use iptables (PREROUTING) to redirect all TCP Traffic to a local port. Now I want to capture these packets using a C program. I tried lots of socket variations (UDP / TCP / ...) but I cannot make a connection to localhost using the port I specified in iptables. I can see all the packets being redirected but how can I capture this traffic? These are my rules (its Android but should´t make any differences...): Chain PREROUTING (policy ACCEPT 32 packets 5675 bytes) pkts bytes target prot opt in out source destination FIX ME! implement getprotobynumber() bionic/libc/bionic/stubs.c:384 0 0 DNAT tcp -- any any anywhere anywhere tcp dpt:5512 to:192.168.1.107 Chain OUTPUT (policy ACCEPT 56 packets 3433 bytes) pkts bytes target prot opt in out source destination FIX ME! implement getprotobynumber() bionic/libc/bionic/stubs.c:384 0 0 DNAT tcp -- any any anywhere anywhere tcp dpt:5512 to:127.0.0.1 I already tried creating TCP/UDP/RAW Socket (I also thought about ""local"" / UNIX-Sockets but what´s the address for it?) and ServerSocket / DatagramServer - but I received nothing... Thanks!! If you simply want to read the packets then the ULOG target should probably be your first choice. You can configure netfilter to send packets to userspace and the ulogd daemon can save the packets to a file or database so presumably it can be configured or modified to send packets to your program directly. If on the other hand you're trying to do some clever interpositioning to create a transparent VPN or something similar ULOG would probably be a little too much work.  You could just use libpcap which will capture any traffic occurring on the ethernet device and then just filter out what you want/need. You cant make a connection to a port if there is no service listening on it even with DNAT. You need to explain exactly what your trying to accomplish explain your network setup and what data your trying to capture. My Problem is that all the received packets will be processed by some filter and I can only receive the processed packets using libpcap. So what I want is reading the raw packets before being filteres. My filter works as I can see on the counters - but how can I process these raw packages which are redirected to the specified port? Then you need to do what sarnold said and use ULOG or you could use the NFQUEUE filter which allows you to write a service that inspects/modifies/saves them. See here for a NFQUEUE example - http://www.debian-administration.org/article/Filtering_traffic_based_on_thousands_of_IPs_efficiently"
951,A,"Matching an ip address to a table of subnet masks... What is the best approach in Java? Here's the scenario... I have a table of subnets. (see below) I have an ip address. I would like to find out what subnet the ip address belongs to based on a lookup in the table. This association will then be used to determine what location the user is at. It's a private network space so the standard internet to location lookups wouldn't apply. What would be the best approach? Would I need to break the ip address into it's numeric parts and to a bitwise comparison against all the subnets.. or are there built-in tools in Java API that could make my life easier for comparing IP address to subnet mask? I'm mainly looking for best way to compare ipaddress to a given subnetmask and determining yes this matches or no it doesn't.. Optionally. Any tips on how to store the list and search with minimal operations would be appreciated also. Ideally I'd be doing something similar to this: List subnetInfo = null; subnetInfo = findSubnet('192.168.0.1'); //value null if nothing found .... //return null if nothing found List findSubnet(String ipaddress) { List subnetDetails = null; .... code here ... return subnetDetails; } Table 1: Sample list of subnets dk-ballerup-gen-off-v411 10.172.80.0/21 NANR-denmark-ballerup-metallbuen66-ground-first-floors-incl-dhcp-(sr14585203) ae-dubai-ofssl-gen-off-v410 10.172.88.0/24 NANR-arab-emirates-ofssl-iflex-general-office-v410-(sr12781477) ru-stpetersburg-gen-off-v411 10.172.89.0/24 NANR-russia-stpetersburg-general-office-incl-dhcp (bsteinba) I'm facing the same question than you but limited to private networks (192.168.X.X 10.x.x.x ... & IPv6). If you're limited to this case too you should use the built in InetAddress.isSiteLocalAddress()  It's an old question but I thought it worth saving somebody else the pain it caused me. Steer clear of the Apache Commons Net SubnetUtils class it's buggy: http://en.newinstance.it/2009/03/25/checking-if-an-ip-is-in-a-subnet-range-subnetutilssubnetinfo-isinrange-bug/  If your table is a SQL table AND your database is PostgreSql you can use PostgreSQL types and functions. Well that's a lot of conditions ...  Apache Commons Net has a SubnetUtils for this sort of thing including determining if a given IP address is within a given subnet. Trivial example:  String[] subnetsMasks = { ... }; Collection<SubnetInfo> subnets = new ArrayList<SubnetInfo>(); for (String subnetMask : subnetsMasks) { subnets.add(new SubnetUtils(subnetMask).getInfo()); } String ipAddress = ...; for (SubnetInfo subnet : subnets) { if (subnet.isInRange(ipAddress)) { System.out.println(""IP Address "" + ipAddress + "" is in range "" + subnet.getCidrSignature()); } } Why would you need an API for something you can check with the & operator? ... which is why netmasks are *called* netmasks ... Because turning a CIDR mask into a number is the sort of task that you can easily get wrong. WHy re-implement it when you can get it off the shelf? And if the CIDR mask crosses the byte boundary? More lines of code. And more bugs. This problem has been solved in libraries there are better things to spend our energy on. You're kidding right? You don't even have to convert the bytes into an integer you can just mask the bytes in a loop. About 4 lines of code.  Turn the IP address and the netmask into ints and AND them together. If the result equals the IP address it is an address in that netmask. Note that you may get more than one match if your netmasks aren't distinct. This is only true for IPv4. The same technique works for IPv6 but the addresses are not ints. They're not even longs. They're 128-bit numbers which Java represents as a byte array.  Remember that IP address is just an int value for historic reasons represented as 4 octets in decimal form. For the same token the subnet is really a range of consecutive ints from network address to a broadcast address. Therefore if your IP address object has an int converter you can simply check if that integer is in range of the subnet by doing simple int comparisons."
952,A,"Nagle-Like Problem so I have this real-time game with a C++ sever with disabled nagle using SFML library  and client using asyncsocket also disables nagle. I'm sending 30 packets every 1 second. There is no problem sending from the client to the server but when sending from the server to the clients some of the packets are migrating. For example if I'm sending ""a"" and ""b"" in completly different packets the client reads it as ""ab"". It's happens just once a time but it makes a real problem in the game. So what should I do? How can I solve that? Maybe it's something in the server? Maybe OS settings? To be clear: I AM NOT using nagle but I still have this problem. I disabled in both client and server. For example if I'm sending ""a"" and ""b"" in completly different packets the client reads it as ""ab"". It's happens just once a time but it makes a real problem in the game. I think you have lost sight of the fundamental nature of TCP: it is a stream protocol not a packet protocol. TCP neither respects nor preserves the sender's data boundaries. To put it another way TCP is free to combine (or split!) the ""packets"" you send and present them on the receiver any way its wants. The only restriction that TCP honors is this: if a byte is delivered it will be delivered in the same order in which it was sent. (And nothing about Nagle changes this.) So if you invoke send (or write) on the server twice sending these six bytes: ""packet"" 1: A B C ""packet"" 2: D E F Your client side might recv (or read) any of these sequences of bytes: ABC / DEF ABCDEF AB / CD / EF If your application requires knowledge of the boundaries between the sender's writes then it is your responsibility to preserve and transmit that information. As others have said there are many ways to go about that. You could for example send a newline after each quantum of information. This is (in part) how HTTP FTP and SMTP work. You could send the packet length along with the data. The generalized form for this is called TLV for ""Type Length Value"". Send a fixed-length type field a fixed-length length field and then an arbitrary-length value. This way you know when you have read the entire value and are ready for the next TLV. You could arrange that every packet you send is identical in length. I suppose there are other solutions and I suppose that you can think of them on your own. But first you have to realize this: TCP can and will merge or break your application packets. You can rely upon the order of the bytes' delivery but nothing else.  You have to disable Nagle in both peers. You might want to find a different protocol that's record-based such as SCTP. Using Nagle's algorithm or not isn't negotiated so even if the client sends immediately the server need not. EDIT2 Since you are asking for a protocol and I understand switching to sctp isn't easy (it's a beautiful protocol though) here's how I would do it: Define a header for the message. Let's say I would pick a 32 bits header. Header: MSG Length: 16b Version: 8b Type: 8b Then the real message comes in having MSG Length bytes. So now that I have a format how would I handle things ? Server When I write a message I prepend the control information (the length is the most important really) and send the whole thing. Having NODELAY enabled or not makes no difference. Client I continuously receive stuff from the server right ? So I have to do some sort of read. Read bytes from the server. Any amount can arrive. Keep reading until you've got at least 4 bytes. Once you have these 4 bytes interpret them as the header and extract the MSG Length Keep reading until you've got at least MSG Length bytes. Now you've got your message and can process it This works regardless of TCP options (such as NODELAY) MTU restrictions etc. Hmmm OK well you are right. Thank you. OK I am considering changing my protocol. I did changed my code but now I have HUGE gaps between any packet. I don't know why. So what would be the best for positions? UDP? Can you explain a bit about SCTP? @Eli I once again edited my answer :-) I did disabled both. I checked for SCTP and TCP seems better for MMO real time games. @Eli how does it seem better ? That's not the question here right now this is my problem maybe if I will decide to change it it will take me a lot of time to change all my code so right now I want to solve that problem because it should not be like that. @Eli So it **doesn't** seem better. I knew it's not convenient but I was trying to provoke you into admitting you don't actually have arguments :-) I don't understand. Most of the MMO real time games uses TCP without having this problem so I need to use SCTP? Sure I can edit my code for this TCP problem to work better - but it really should be like that? @Eli I'm on your side but why can't you just receive 'ab' and treat it as if there's 2 messages ?! What do you mean? How can I know that ab is 2 messages and not one? @Eli Use a delimiter ? `|a||b|` Use some protocol with a header length ? The options are endless."
953,A,"Strange net paths in WHOIS As a self-learning project I'm implementing a C++ whois client. I'm using a couple of ancient C implementations for reference e.g. koders.com. One thing puzzles me. As well as the expected path format like example.com example.co.uk 192.0.32.10 or 2620:0:2d0:200::10 the old C implementations accept some other formats: Paths beginning with one of the following strings: ""net-"" ""netblk-"" ""asn-"" ""as-"" ""lim-"" ""coco-"" ""coho-"" or ""core-"". I presume these look something like ""netblk-example"" but I don't have a working example. Paths ending in one of the following strings: ""-au-dom"" ""-dom"" ""-org"" ""-hst"" ""-arin"" ""-ripe"" ""-mnt"" ""-gandi"" ""-ap"" ""-au"" ""-ti"" ""-is"" ""-6bone"" ""-norid"" ""-ripn"" ""-sgnic"" ""-metu"" ""-cknic"" or ""-kg"". Again I presume these look like ""example-arin"" but I'm not certain. To (finally) get to my question: does anybody know what these paths are? Are they still used? Does anybody have legit examples of valid paths like these? Update 23 May 2011 Added C++ tag (my implementation language) as nobody seems to even view ""network-programming"" without a language. These extra strings are (mostly) for recognising the object types maintained by the Regional Internet Registries such as RIPE ARIN APNIC. Their databases are also accessible via the whois protocol even though the entries within them are not domain names. I had guessed that much but I want to know if they are still in use and preferably some real-life examples that I can use in testing. after more research I now think I understand and realise your answer is better than I originally thought."
954,A,"Sending Data in socket programming using ""C"" I had earlier posted a question regarding same but over here i want guidance for my code. Using the tips from people I have tried to create for sending a packet. My max packet structure alongwith header and payload is of 16 bytes.Kindly if possible glance through the sending and receiving code and suggest where i am going wrong. Basically my client keeps sending data to serverit just doesn't end and server doesn't show results. Client: int main(int argc char *argv[]) { int sockfd portno n; struct sockaddr_in serv_addr; struct hostent *server; struct packet { long int srcID; long int destID; long int pver; long int profiles; char length; long int data; }; if (argc < 3) { fprintf(stderr""usage: %s hostname port\n"" argv[0]); exit(0); } portno = atoi(argv[2]); //Convert ASCII to integer sockfd = socket(AF_INET SOCK_STREAM 0); // socket file descriptor if (sockfd < 0) error(""ERROR DETECTED !!! Problem in opening socket\n""); server = gethostbyname(argv[1]); if (server == NULL) { fprintf(stderr""ERROR DETECTED !!! no such server found \n""); exit(0); } bzero((char *) &serv_addr sizeof(serv_addr)); //clear the memory for server address serv_addr.sin_family = AF_INET; bcopy((char *)server->h_addr (char *)&serv_addr.sin_addr.s_addr server->h_length); serv_addr.sin_port = htons(portno); printf(""Client 1 trying to connect with server host %s on port %d\n"" argv[1] portno); if (connect(sockfd(struct sockaddr *)&serv_addrsizeof(serv_addr)) < 0) error(""ERROR in connection""); printf(""SUCCESS !!! Connection established \n""); char buffer[128]; struct packet *pkt = (struct packet *) buffer; char *payload = buffer + sizeof(struct packet); long int packet_size; printf(""Started Creating packet\n""); pkt->srcID = 0x01; pkt->destID = 0x02; pkt->pver = 0x01; pkt->profiles = 0x01; pkt->length = 128; pkt->data = 1; 2; 3; 4; 5; 6; 7; 8; if (send(sockfdpktsizeof(packet_size)0) <0) printf (""error\n""); else printf (""packet send done""); return 0; } Server: int main(int argc char *argv[]) { int sockfd newsockfd portno clilen; struct sockaddr_in serv_addr cli_addr; int n; char wish; long int SrcID; long int DestID; long int Pver; long int Profiles; long int Data; char Length; char bytes_to_receive; char received_bytes; struct packet { long int srcID; long int destID; long int pver; long int profiles; char length; long int data; }; if (argc < 2) { fprintf(stderr""usage: %s port_number1""argv[0]); exit(1); } sockfd = socket(AF_INET SOCK_STREAM 0); if (sockfd < 0) error(""ERROR DETECTED !!! Problem in opening socket""); bzero((char *) &serv_addr sizeof(serv_addr)); portno = atoi(argv[1]); serv_addr.sin_family = AF_INET; serv_addr.sin_addr.s_addr = htonl(INADDR_ANY); serv_addr.sin_port = htons(portno); if (bind(sockfd (struct sockaddr *) &serv_addr sizeof(serv_addr)) < 0) error(""ERROR DETECTED !!! There was a problem in binding""); listen(sockfd 10); clilen = sizeof(cli_addr); printf(""Server listening on port number %d...\n"" serv_addr.sin_port); newsockfd = accept(sockfd(struct sockaddr *) &cli_addr &clilen); if (newsockfd < 0) error(""ERROR DETECTED !!! the connection request was not accepted""); char buffer[128]; struct packet *pkt = (struct packet *) buffer; char *payload = buffer + sizeof(struct packet); long int packet_size; bytes_to_receive = sizeof(pkt); received_bytes = 0; if (recv(newsockfd pkt sizeof(pkt) 0) < 0) error(""ERROR DETECTED !!! There was a problem in reading the data""); else { do { received_bytes += (buffer + received_bytes bytes_to_receive - received_bytes); } while (received_bytes != bytes_to_receive); SrcID = pkt->srcID; DestID = pkt->destID; Pver = pkt->pver ; Profiles = pkt->profiles; Length = pkt->length; Data = pkt->data; printf(""Data Received from Client_1 are :\n""); printf(""Source ID: %l\n"" SrcID); printf(""Destination ID: %l\n"" DestID); printf(""profile Version: %l\n"" Pver); printf(""No of Profiles: %l\n"" Profiles); printf(""Length: %l\n"" Length); printf(""data : %l\n"" Data); } if (close(newsockfd) == -1) { error(""Error closing connection with client 1""); } printf(""Connection with client 1 has been closed\n""); return 0; } The server is not showing any o/p. Client says it has send the packet. While compiling the server code i see four warnings saying unknown conversion type characters 0xa in format for all the printf statements in server code. I guess I am going wrong somewhere in the server code side but I am not able to follow the ""serialization"". Please update me with your inputs it would be of great help. Here is couple of issues that I found: Your client keep sending packages because it is in infinite while loop. You passed wrong len parameter of recv function. Right now you pass sizeof(packet_size) which is equal to sizeof(long int) (4 bytes on 32 bit OS) but probably your intension was to use sizeof(packet) (16 bytes). You don't check how many bytes were truly read by recv function. With TCP you don't have guaranties that you read all 16 bytes of struct packet. So from time to time you could read less bytes and your packet will be incomplete. Here is an example in some pseudo code how you should receive whole packet: bytes_to_receive = sizeof(packet) received_bytes = 0; do { received_bytes += recv(buffer + received_bytes bytes_to_receive - received_bytes) } while (received_bytes != bytes_to_receive) Your struct packet in client and server is different. In one you use char length; in second long int length; I think also this kind of assignments in server make no sense pkt->srcID = SrcID; and should be something like this SrcID = pkt->srcID; +1. He also cannot possibly detect EOS unless he checks the result of recv() for zero. after incorporating your suggestions I see an error: server.c:85:5: error: too few arguments to function ârecvâ /usr/include/sys/socket.h:148:16: note: declared here line 85 of my code is do { received_bytes += recv(buffer + received_bytes bytes_to_receive - received_bytes); } while (received_bytes != bytes_to_receive); In the pseudo code the recv function has less arguments right....that is what I am getting as error. Where should this code be placed or what all modififcations need to be done..  The problem with the client continually sending is because you simply have it in a loop. With indentation fixed it becomes clear what has happened: while (1) { if (send(sockfdpktsizeof(packet_size)0) <0) printf (""error\n""); else printf (""packet send done""); } i don't see anything printing on server side........previously i was told 2 apply serialization can u guide me vid my code which parts r not in proper place.......thanks  firs mistake in client program ""packet_size"" not defined. use struct packet instead; try to use some integer value to send instead 0x10.........etc. in server module in printf us %ld instead of %l. Avoid unused variables like payload in server module. i got the output............... i mean packet_size not used and u r sending data of this length."
955,A,Qt POP3 and SSL? If Gmail is using SSL does this mean that I have to implement the algorithms for encrypting and decrypting all the traffic by myself in order to download the mail? If so is there a fast workaround for doing this ? I should mention that I'm making a POP3 client from scratch in Qt Creator and I want to implement all the communication by myself. Is this project you're doing open-source? I'd be interested to take a look at the code because I've been thinking of implementing exactly the same thing. You don't need to implement encryption algorithms by yourself. Qt includes SSL support under QSsl namespace which was introduced in Qt 4.3. You should consider looking under QSsl namespace for required classes and enumerations.
956,A,"Security implications to be aware of when writing a lightweight network file transfer protocol I have a desktop application that will 'sync' with an iPhone application. The sync is actually a simple file copy — the iPhone app is a viewer so I don't have to deal with the complexities of actually syncing the data in a two-way manner. I've written my own protocol since I need it to be lightweight and I'm sending files that may be bigger than the RAM available on the device — therefore the files are being streamed straight from the network to disk. The files consist of one 'data' file (an SQLite database) and zero or more resources which are pictures PDFs etc. Once the connection has been accepted the client and the server communicate in chunks which start with the following struct: struct ChunkHeader { UInt32 headerLength; UInt32 totalChunkLength; UInt32 chunkType; UInt32 chunkNameLength; UInt32 chunkDataLength; } __attribute__ ((packed)); After the struct comes a UTF-8 filename string immediately followed by the data for that file. The header struct contains the length of the filename string (chunkNameLength) and the length of the binary data length (chunkDataLength). chunkType contains the ""type"" of the chunk so the client knows what to do with the data and can be one of the following: typedef enum { kChunkTypePreSyncInfoDictionary = 0 kChunkTypeDataFile = 1 kChunkTypeResource = 2 kChunkTypeEndOfData = 3 kChunkTypeSyncCancelled = 4 } ChunkType; Up to now I haven't taken security into account at all. The sync will take place over a local WiFi network which may or may not be encrypted. The data being copied doesn't contain sensitive information by its nature in the way a word processing document doesn't — there's nothing stopping the user entering sensitive details if they want to but it isn't asked for. So what steps should I take to ensure the data is adequately secure? I'm working in Objective-C but I'm not looking for language-specific answers just concepts. Things I'm worried about: Should I take steps to prevent data spoofing? Should I take steps to actually encrypt the data? Should I take steps to ensure another application doesn't pretend to be my app and replace either the client or server with something else? Should I take steps to check the received data for something that causes damage? What are your actual threat scenarios? What do you want to protect against? Security isn't a vague ""blanket"" you throw over your application. Security is specific responses to specific use cases. What are your threat use cases? Snooping? Spoofing? Corruption? ""Should I take steps to check the received data for something that causes damage?"" Always. Unconditionally. Without Exception. For your other questions: Should I take steps to prevent data spoofing? Should I take steps to actually encrypt the data? (snooping) Should I take steps to ensure another application doesn't pretend to be my app and replace either the client or server with something else? (spoofing again) You said ""The data being copied doesn't contain sensitive information"" so snooping seems not to be a threat scenario you care about. So it looks like you're worried about some spoofing threat scenario (you mentioned it twice). This is usually handled with some kind of exchange of credentials between client and server. Passwords work for user authentication. Some kind of certificate might work for client-server mutual trust. The idea is to have ""enough"" information for the client and server to know the other party is trustworthy. Some folks are happy with a known URL. Other folks want some ""shared secret"" (like a password). Some folks need even more and include numerous details in the protocol. Some folks include algorithmic details in the protocol (HTTP Digest Authentication for example requires the client to do a little algorithmic dance on data provided by the server.)"
957,A,"Get all IPs in the same network as my computer Is there any way to programatically (in c#) get all the other workstations that are in the same domain/subnetwork as my computer and display some information about them (if they are active what kind of OS they have installed their IP etc)? The simplest approach that I can think of (which is far from fool proof) is to send an ICMP echo request (defined in RFC 792) to 224.0.0.1. C# provides the Ping class to do this. Keep in mind though that you run the risk of dropped packets there is also the question as to whether or not all the machines on the network support multicast.  nmap does this plus a ton more. Its open source. nmap's the only solution that will work... Can be called from C# just fine! (Surely .NET programmers can call other programs) Thanks but I have to implement this functionality in an existing program so I cannot use nmap You can however look at how Nmap does it and reimplement that method in C#...  This is a VB solution but I'm pretty sure you'll be able to make the changes you need to make this work! There's probably a better way but this was a first cut. Imports System.Net.NetworkInformation Imports System.Directory Services Class NetworkInfo Function GetComputers() as list(Of String) dim List as new list(of String) Dim DomainEntry as new DirectoryEntry(""WinNT://"" + DomainInfo.GetDomain.Trim()) DomainEntry.Children.SchemaFilter.Add(""computer"") For Each Machine as DirectoryEntry in DomainEntry.Children List.Add(Machine.Name) Next return List End Function End Class There are all sorts of useful tools knocking about in the System.Net.NetworkInformation namespace to let you capture things like the IP address etc. Does that work for hosts with a static IP address and have not registered a dynamic DNS address? This works for both static and dynamic IP addresses. Thanks that was exactly what I needed How? The only possible way I can think of that this would be true is if _all_ of those machines were part of the domain which is not necessarily the case. Read the question - all the machines are on the same domain. This assumes a DNS server is available and contactable."
958,A,"Inform me when site (server) is online again When I ping one site it returns ""Request timed out"". I want to make little program that will inform me (sound beep or something like that) when this server is online again. No matter in which language. I think it should be very simple script with a several lines of code. So how to write it? Don't forget the notify sound like echo""^G""! Just to be different - here's Windows batch: C:\> more pingnotify.bat :AGAIN ping -n 1 %1% IF ERRORLEVEL 1 GOTO AGAIN sndrec32 /play /close ""C:\Windows\Media\Notify.wav"" C:\> pingnotify.bat localhost :)  Some implementations of ping allow you to specify conditions for exiting after receipt of packets: On Mac OS X use ping -a -o $the_host ping will keep trying (by default) -a means beep when a packet is received -o means exit when a packet is received On Linux (Ubuntu at least) use ping -a -c 1 -w inf $the_host -a means beep when a packet is received -c 1 specifies the number of packets to send before exit (in this case 1) -w inf specifies the deadline for when ping exits no matter what (in this case Infinite) when -c and -w are used together -c becomes number of packets received before exit Either can be chained to perform your next command e.g. to ssh into the server as soon as it comes up (with a gap between to allow sshd to actually start up): # ping -a -o $the_host && sleep 3 && ssh $the_host Is `-o` a standard option? I don't see it on Linux or Windows Ah my bad. It seems to only be avail on mac. I guess he can wrap it in a loop. Confirm that `-o` is not available on Linux. Too bad would be very handy for this use. +1 anyhow for showing me the option on my Mac. =)  One way is to run ping is a loop e.g. while ! ping -c 1 host; do sleep 1; done (You can redirect the output to /dev/null if you want to keep it quiet.) On some systems such as Mac OS X ping may also have the options -a -o (as per another answer) available which will cause it to keep pinging until a response is received. However the ping on many (most?) Linux systems does not have the -o option and the kind of equivalent -c 1 -w 0 still exits if the network returns an error. Edit: If the host does not respond to ping or you need to check the availability of service on a certain port you can use netcat in the zero I/O mode: while ! nc -w 5 -z host port; do sleep 1; done The -w 5 specifies a 5 second timeout for each individual attempt. Note that with netcat you can even list multiple ports (or port ranges) to scan when some of them becomes available. Edit 2: The loops shown above keep trying until the host (or port) is reached. Add your alert command after them e.g. beep or pop-up a window."
959,A,Creating basic file transfer between 2 applications on 2 computers using bluetooth in C# First I don't want to pair the 2 computers manually. I don't want to use inbuilt bluetooth software to transfer file I don't won't windows to control my Bluetooth All done on Windows 7 computers (32Feet.net can be also applied if necessary) I want to create two applications say each knows of the other. Put them in two different computers and make them run. so each will turn on their Bluetooth antennas and then the Two applications will connect using the bluetooth and a file is sent from one to the other. thats really all. How can I basically achieve this can someone guide me? Also I got 2 questions: How can i set one application to broadcast a message that it exists over bluetooth. How can the other detect this and connect to that computer using Bluetooth One idea i have is to connect to each nearby computer iteratively and then search if the application exists and then pair them automatically through the application itself (not by windows manually) and send the file. You can use my library 32feet.NET. :-) To send the file use the class ObexWebRequest or for more advanced OBEX use see the partner library Brecham.Obex. To discovery devices in range use BluetoothClient.DiscoverDevices or BluetoothComponent.DiscoverDevicesAsync. To respond to any authentication requests use class BluetoothWin32Authentication. :-) See the user guide at 32feet.NET User Guide Thanks I already went through the 32Feet.Net user guide few days ago. But my problem is in order to use the suggested functionality the two computers must be paired right? How can I make this **pairing** automatic through C#. The 2 applications I create & run on the two machines should identify each other and establish the pairing. How can I achieve this? probably in C#. Thanks for the promptly reply.  I would suggest that if both computer are on the same network. That you simply create a tcp listener on the receiving computer and send the file in byte array to it from the sending computer. I want to use bluetooth and there is no other network connection.
960,A,"Can NIC send n network messages(to n destinations) simultaneously? Can a machine send n network messages/packets (to n destinations) simultaneously? Is there a upper bound on level of parallelism and what affects network's parallelism. More specific say there are 2 packets and four event s1 r1 and s2 r2 denotes send/receive packet 1 and send/receive packet 2. When we send asynchronously (like s1 s2...r1r2) and synchronously (s1...r1s2...r2) does it matter? Could total latency be shorten in the case of asynchronous send. Yes they can. A NIC just transmits the frames the driver tells it to and does it as soon as it can. NICs don't care about destinations. Higher layers (e.g: TCP) are responsible for retransmissions and have their own buffering. NICs usually can have several frames ready to be sent but they stay little time in the NIC as soon as the medium is free and the NIC has transmitted a frame without collisions it can take another frame ready for transmission.  You basically have three choices: Point to point Multicast Broadcast A point to point message goes from one source to one destination. A multicast message goes from the source to the router and from there gets distributed to the multiple recipients. There are some relatively intelligent switches (usually called ""layer 2+"" or something on that order) that can handle multicasting as well but your average ""garden variety"" switch usually can't/won't handle it. A broadcast goes from the source to everything else on the local subnet. There are a cople of ways this can be done. Probably the most common is to send to the address 255.255.255.255. Broadcasting is supported directly by Ethernet which has a ""broadcast"" MAC address of FF:FF:FF:FF:FF:FF. Note that IPv6 no longer supports broadcasting but does have an ""all hosts"" multicast group that's vaguely similar. Which switches don't support multicast? Transparent bridges (i.e: garden-variety) just have to flood the frame to all ports except the originating port. Also point-to-point is usually called unicast. Not sure if this make difference. but the question is more interested in sending multiple different messages not one message to multiple destinations. @Richard: In that case it's simple: only one message can be sent at a time (ultimately a network connection is a really fast serial port so it really only sends one bit at any given time). Sending two different messages (whether to the same or different destinations) always has to be done serially."
961,A,"Netty vs Apache MINA They both provide roughly the same functionality. Which one should I choose to develop my high-performance TCP server? What are the pros & cons? Would also be interesting to add Grizzly to the comparison. Grizzly is a completely different beast. There was even the idea of Grizzly support for MINA when the both groups talked. @Hardcoded you say grizzly is a completely different beast I'm a new comer to this can you please point out the differences or give me an article to read on that? I'd really appreciate it. Grizzly has a different background and the last time I look at it it was mostly suited to HTTP-based applications. I just looked at the examples and was surprised to see that they are using very similar structure as with MINA or Netty. So the beast isn't that different anymore MINA has more out-of-the-box features at the cost of complexity and relatively poor performance. Some of those features were integrated into the core too tightly to be removed even if they are not needed by a user. In Netty I tried to address such design issues while retaining the known strengths of MINA. Currently most features available in MINA are also available in Netty. In my opinion Netty has cleaner and more documented API since Netty is the result of trying to rebuild MINA from scratch and address the known issues. If you find that an essential feature is missing please feel free to post your suggestion to the forum. I'd be glad to address your concern. It is also important to note that Netty has faster development cycle. Simply check out the release date of the recent releases. Also you should consider that MINA team will proceed to a major rewrite MINA 3 which means they will break the API compatibility completely. Oh @trustin is the author of both MINA and Netty.  In Netty site you can find some performance reports. As expected :-) they point out Netty as the framework with the best performance. I never used Netty but I already used MINA to implement a TCP protocol. The implementation of encoding and decoding was easy but the implementation of the state machine was not so easy. MINA provides some classes to aid you when implementing the state machine but I found them kind of hard to use. In the end we decided to ditch MINA and implement the protocol from scratch and surprisingly we ended with a faster server.  MINA and Netty were initially designed and build by the same author. That's why they are so similiar to each other. MINA is designed at a slightly higher level with a little more features while Netty is a little faster. I think that there's not much difference at all the basic concepts are the same.  I've only ever used MINA to build a small http like server. The biggest problems I've run into with it so far: It will hold your ""request"" and ""response"" in memory. This is only an issue because the protocol I choose to use is http. You could use your own protocol however to get around this. No option to provide a stream off disk in case you want to serve up large files. Again can be worked around by implementing your own protocol Nice things about it: Can handle a lot of connections If you choose to implement some sort of distributed work system then knowing when one of your nodes goes down and loses connection is useful for restarting the work on another node. When you say ""stream off disk for large files"" don't people normally use UDP for that? No. They use kernel sendfile (exposed in Java as FileChannel.transferTo) over TCP.  I performance tested 2 ""Google Protobuffer RPC"" implementations where one was based on Netty (netty-protobuf-rpc) and the other based on mina (protobuf-mina-rpc). Netty ended up being consistently faster ( +- 10% ) for all message sizes - which backs up the overall performance claim on the Netty web site. Since you want to squeeze every bit of efficiency out of your code when you use such an RPC library i ended up writing protobuf-rpc-pro based on Netty. I have used MINA in the past but find their documentation of the 2.0 stuff has big holes and the breaking of API backward compatibility a big minus.  While MINA and Netty have similar ambitions they are quite different in practice and you should consider your choice carefully. We were lucky in that we had lots of experience with MINA and had the time to play around with Netty. We especially liked the cleaner API and much better documentation. Performance seemed better on paper too. More importantly we knew that Trustin Lee would be on hand to answer any questions we had and he certainly did that. We found everything easier in Netty. Period. While we were trying to reimplement the same functionality we already had on MINA we did so from scratch. By following the excellent documentation and examples we ended up with more functionality in much much less code. The Netty Pipeline worked better for us. It is somehow simpler than MINAs where everything is a handler and it is up to you to decide whether to handle upstream events downstream events both or consume more low-level stuff. Gobbling bytes in ""replaying"" decoders was almost a pleasure. It was also very nice to be able to reconfigure the pipeline on-the-fly so easily. But the star attraction of Netty imho is the ability to create pipeline handlers with a ""coverage of one"". You've probably read about this coverage annotation already in the documentation but essentially it gives you state in a single line of code. With no messing around no session maps synchronization and stuff like that we were simply able to declare regular variables (say ""username"") and use them. But then we hit a roadblock. We already had a multi-protocol server under MINA in which our application protocol ran over TCP/IP HTTP and UDP. When we switched to Netty we added SSL and HTTPS to the list in a matter of minutes! So far so good but when it came to UDP we realised that we had slipped up. MINA was very nice to us in that we could treat UDP as a ""connected"" protocol. Under Netty there is no such abstraction. UDP is connectionless and Netty treats it as such. Netty exposes more of the connectionless nature of UDP at a lower level than MINA does. There are things you can do with UDP under Netty than you can't under the higher-level abstraction that MINA provides but on which we relied. It is not so simple to add a ""connected UDP"" wrapper or something. Given time constraints and on Trustin's advice that the best way to proceed was to implement our own transport provider in Netty which would not be quick we had to abandon Netty in the end. So look hard at the differences between them and quickly get to a stage where you can test any tricky functionality is working as expected. If you're satisfied that Netty will do the job then I wouldn't hesitate to go with it over MINA. If you're moving from MINA to Netty then the same applies but it is worth noting that the two APIs really are significantly different and you should consider a virtual rewrite for Netty - you won't regret it! re: earlier comment by Josh on lack of support for UDP in Netty: I don't understand why you couldn't use a few pages of hand-crafted code to do what you need rather than abandoning Netty. UDP is listening on a different port anyway. I have been testing Netty vs. Nginx and am quite impressed (Netty scoring about the same or better under load).  I prefer Netty. Twitter also chose Netty to build its new Search System and sped it up to 3x faster. Ref: Twitter Search is Now 3x Faster We chose Netty over some of its other competitors like Mina and Jetty because it has a cleaner API better documentation and more importantly because several other projects at Twitter are using this framework."
962,A,"Can you send images without stalling the other data going through the same java.net.socket? This is the situation: I'm working on a project where I need to be able to send one or more images once in a while to/from the server as well as a lot of other types of data represented with text. The way it is currently done is by sending a message with that says ""incoming image of size x to be used as y"" (It's not ""formulated"" that way of course) and then I call a method that reads the next x bytes through a DataInputStream. At first I met some problems with latency screwing things up but I made the server spawn a new thread to send the ""incoming image"" message and then wait for a flag that is set when the client responds with a ""I'm ready for the image"" message. It works in a way now but if anything else for instance a chat message is sent while the image is being transfered that message meant for a BufferedReader will be read as raw bytes and used as part of the image. So I will have to block all outgoing data (and add it to a queue) when there is an image that is being sent. But this seems very wrong and annoying as users of the application will not be able to chat while receiving/ uploading a big image. This is what I need: So I either need to set up an independent channel to use for raw data. Which as far as I understand from some tinkering I will have to set up a new socket over a new port which seems unnecessary. The other way I can see to solve this would be to somehow use tag each packet with a ""this is text/raw data"" bit but I have no idea how to do this with java? Can you add information to the packet-header when you write something to the stream (that every packet containing that info will contain) and then read the packet data on the other end and act accordingly? As you can see I do not have much experienced with networking nor have I used Java for a long time. This is also my first post here so be kind. If anything was unclear please ask and I'll specify. All ideas are welcome! (There is possibly a standard way to do this?) Thanks a lot! Java should have a standard support for HTTP protocol - use HTTP to do your picture transfers as you can set the type of data being transmitted in the header. Basically you would have your client/server architecture establish a separate request for each new data transfer (be it text or image) that way enabling you to do processing in a simple loop. This might be of some help to you : How to use java.net.URLConnection to fire and handle HTTP requests? Thanks I'll look into it. Do you suggest that I change my whole architecture to HTTP protocol from TCP? As far as I understand that would make it possible to make it a web applet as well? You don't change to HTTP from TCP because HTTP runs ON TOP of TCP (it's an application level protocol). It has nothing to do with your app being a ""web applet"" you can have a desktop app acting as a http client - one of these actually is your browser. And web applets are pretty much a thing of ancient past. I don't know anyone today who is seriously working on web applets anymore. Yeah I am aware of how it works :) But from the programmers perspective (me) I would have to change the code to send it over HTTP even though it is still technically TCP. And I also know that it will still work as a desktop application but I was thinking that it could be nice to have an applet version of the client but since I was using sockets it wasn't possible. I haven't really looked into it but it sounds like what you suggest is the same way you make applets communicate to your server? Anyway that's not my main concern would just be sweet to have the option. Thanks a lot. HTTP is not a bad idea but will have to worry about authentication.  There is nothing in TCP protocol itself that can help you. You either open a new socket connection (can be to the same server port) or you split your images in smaller chunks and wrap these chunks in envelopes saying what type of message it is: image or chat. And then reconstruct the image on the receiving end from these chunks. But this will waste bandwidth and add complexities of its own (e.g. how big do you make a chunk of that image?). I would go with the separate binary data connection. Ok thanks a lot :) I will watch this question for a while and probably go for a separate data connection? *Looking up how to set up multiple connections on the same port* You just open the 2nd connection same as the first one. Server side can accept on a server socket in a loop. Yeah I got it to work :) Thanks again. I'll still have to use some time to make it all work but I can see how I'm going to do it now."
963,A,"Bi-directional communication with 1 socket - how to deal with collisions? I have one app. that consists of ""Manager"" and ""Worker"". Currently the worker always initiates the connection says something to the manager and the manager will send the response. Since there is a LOT of communication between the Manager and the Worker I'm considering to have a socket open between the two and do the communication. I'm also hoping to initiate the interaction from both sides - enabling the manager to say something to the worker whenever it wants. However I'm a little confused as to how to deal with ""collisions"". Say the manager decides to say something to the worker and at the same time the worker decides to say something to the manager. What will happen? How should such situation be handled? P.S. I plan to use Netty for the actual implementation. I think you need to read up on sockets.... You don't really get these kinds of problems....Other than how to responsively handle both receiving and sending generally this is done through threading your communications... depending on the app you can take a number of approaches to this.  If you feel like reinventing the wheel and don't want to use middleware... Design your protocol so that the other peer's answers to your requests are always easily distinguishable from requests from the other peer. Then choose your network I/O strategy carefully. Whatever code is responsible for reading from the socket must first determine if the incoming data is a response to data that was sent out or if it's a new request from the peer (looking at the data's header and whether you've issued a request recently). Also you need to maintain proper queueing so that when you send responses to the peer's requests it is properly separated from new requests you issue.  I'd go for a persistent bi-directional channel between server and client. If all you'll have is one server and one client then there's no collision issue... If the server accepts a connection it knows it's the client and vice versa. Both can read and write on the same socket. Now if you have multiple clients and your server needs to send a request specifically to client X then you need handshaking! When a client boots it connects to the server. Once this connection is established the client identifies itself as being client X (the handshake message). The server now knows it has a socket open to client X and every time it needs to send a message to client X it reuses that socket. Lucky you I've just written a tutorial (sample project included) on this precise problem. Using Netty! :) Here's the link: http://bruno.linker45.eu/2010/07/15/handshaking-tutorial-with-netty/ Notice that in this solution the server does not attempt to connect to the client. It's always the client who connects to the server. If you were thinking about opening a socket every time you wanted to send a message you should reconsider persistent connections as they avoid the overhead of connection establishment consequently speeding up the data transfer rate N-fold. Wow! Thanks! This is really great :)  ""I'm also hoping to initiate the interaction from both sides - enabling the manager to say something to the worker whenever it wants."" Simple answer. Don't. Learn from existing protocols: Have a client and a server. Things will work out nicely. Worker can be the server and the Manager can be a client. Manager can make numerous requests. Worker responds to the requests as they arrive. Peer-to-peer can be complex with no real value for complexity. I see. But how should one deal with situations in which both client and server need to initiate a communication? @Zwei Steinen: introducing a Message Queue is not ""warranted"". It's ""essential"" it's ""the best practice"" it's ""the standard solution"". Go for it. Thanks for you advice! Right now the client has to periodically poll the server (and it does not work the other way around). Do you think in that situation introducing a Message Queue is warranted (to avoid the periodical polling)? @Zwei Steinen: First don't allow client and server to initiate. Client initiates; Server is passive. This leads to the server saving things until the client requests them. If you think you want ""real time"" you have a very complex problem and need a proper middleware to handle Message Queues.  The correct link to the Handshake/Netty tutorial mentioned in brunodecarvalho's response is http://bruno.factor45.org/blag/2010/07/15/handshaking-tutorial-with-netty/ I would add this as a comment to his question but I don't have the minimum required reputation to do so."
964,A,"Ensure that root user is running the client program that is trying to connect the server program I have a server program which listens on a particular port. I have a requirement where client program that tries to connect to my server must be initiated by a root user. How do I ensure this in the server program? How do I ensure [anything about the client program] in the server program? You can't. If your security model requires the server to know whether client is root you don't have security. Let's consider one possibility: your network protocol includes a notification like this: My-Uid-Is: 0 Your client the perfectly secure version that you wrote might implement this notification like this: fprintf(socketFd ""My-Uid-Is: %d\n"" getuid()); // send server my identity But my client the one what I wrote without your knowledge or consent will implement the notification like this: fprintf(socketFd ""My-Uid-Is: 0\n""); // lie to server about my identity Pop quiz: how can your server know whether it is talking to your truthful client or my lying client? Answer: it can't. In fact if you generalize this concept you realize that the server can't rely upon the validity (whether that means the truthfulness the format the range-checking etc.) of anything the client says. In this specific case using the clients source port number is as unreliable as any other choice. Yes many operating systems require root privileges to bind to low-numbered source ports. But my PC might not be running your favorite operating system. I might be connecting from my own PC running my own OS which doesn't have that feature. Remember: you can't trust anything the client says. There are techniques involving public-key encryption that can be used to guarantee that the program you are talking to has access to specific secrets. That assuming that the secrets are adequately protected can be used to guarantee that a specific person computer or account generated the request. I'll let someone else discuss PKI and how it might apply to your situation. Very nicely said... thanks... but the solution given by Erik provides sufficient enough security as of now... will think of the public-key encryption as TODO  The client should bind to a port below 1024 before connecting. This port range is reserved for root. but is not a denial of service for the port which client binds. Say the client wants to initiate a service for which my client prog has already binded. Yes the port will be blocked for others. This is however the traditional way of ensuring something is root. You can always bind connect to server from <1024 then also connect from >1024 and tell the server over the root connection that the other connection is OK. That entirely depends on your setup. You could use kerberos if that applies to the environment you could use username/password you could use key-based authentication. But the ""standard"" way is to use this portrange seems fair enough.. thanks.."
965,A,multiple IP addresses chooce the sender I have eth0 and eth1. I am creating a simple tcp program with gsoap. the endpoint is 1.2.3.4. The endpoint receives my data but the IP of eth0 is shown in the sender details. I would like my receiver to see eth1 instead of eth0. Is this possible? Use the bind(2) call to select the source IP or create an explicit route(8) to the server via eth1. Second option is probably preferable since you don't need to modify the client source code. Yes or create a route to the server via `eth1`. Nikolai bind needs an IP and a port. How do I select the port? How do I know if it is available or not? Set the port to `0` the kernel will select an available ephemeral port for you. even if I am the client? does the route(8) works both ways? do I need to add a route from eth1 to eth0 for the reply from the server? That's the upstream router job to route through appropriate link on the way back. Usual setup for non-router machines is to have a single *default* route that all traffic follows.
966,A,Can I wake a thread that has blocked because of a call to DatagramSocket.receive()? I have a thread that blocks on a UDP packet and I need to be able to tell it to forget about that packet and do something else all before the receive timeout happens. Is there any way to do this? JB has posted one part of the solution. But if in case you are not using NIO channels the solution AFAIK here would be to close the socket in consideration and handle it likewise in your runnable/callable. I did something similar a while back with TCP sockets in case if you are interested. The feasibility of the solution again depends on whether closing the socket would be acceptable in your case or not. In that case going with the NIO solution would make much more sense.  Use a DatagramChannel to read your UDP packets and interrupt the reading thread. As per the documentation of Thread.interrupt (and DatagramChannel) the read operation will then throw a ClosedByInterruptException. You might want to set the DatagramChannel [to blocking-mode](http://download.oracle.com/javase/1.4.2/docs/api/java/nio/channels/spi/AbstractSelectableChannel.html#configureBlocking%28boolean%29) so that it behaves more like a DatagramSocket. In non-blocking mode the DatagramChannel.receive() might just return null immediately which will be confusing if you have never used non-blocking IO before. The javadoc of DatagramChannel will tell the same of course but sometimes you stumble over this anyway  Set a much shorter read timeout and have your read method loop the correct number of times before it considers a read timeout to have happened. In the other (n-1) cases have it check Thread.isInterrupted().
967,A,"Monodevelop Proxy Settings How do I set the proxy settings of Monodevelop? I tried to modify the Monodevelop.exe.config with the following code <system.net> <defaultProxy useDefaultCredentials=""true""> <proxy proxyaddress = ""http://172.0.0.18:8080"" bypassonlocal = ""true""/> </defaultProxy> </system.net> But its not taking useDefaultCredentials=""true"" attribute it is saying Unrecognized attribute 'useDefaultCredentials' in the exception box. Moreover my proxy settings need userid and password and this credential is different from Windows Logon credentials. So how do I specify those for Monodevelop? This is the same as configuring the proxy for any .NET app. IIRC if autodetection is enabled then .NET will use the WinHTTP proxy settings which are set through IE or proxycfg.exe. Re. your main question the useDefaultCredentials attribute should on the <proxy> element not the <defaultProxy> element."
968,A,Network: How redirect a hostname to a ip? i have this hostname ( valterhenrique.sytes.net )  i created it in No IP. But i need to redirect this hostname to my machine which ip is 192.168.1.149 . How can i do this ? Best regards Valter Henrique. 192.168.1.149 is a private IP. It will not be routed outside of your network. Have a look in your router configuration interface. Usually there is a setting for updating DynamicDNS like No-IP automatically to your current WAN IP and a setting to forward a port from your local machine (192.168.1.149) to the outside world. the solutions to my problem was : i set up a DMZ to the machine that is running what i want to access from outside. then i set up the DDNS in my router to access my ip from the hostname. that's all folks =) glad it worked. i already have a client application that keeps my outside ip updated with no-ip but i don't get it how i gonna do the forward. cause its requires the port and the ip of my LAN. You are behind a router. You need to configure the router to accept connections on for example port 80 of your WAN and redirect them to for example port 8080 on your local machine and vice versa. now i found it the DynamicDNS as you said in my router options.
969,A,"How can I execute certain commands on a separate thread? What is the best solution to allow an object to execute methods on a thread? The object is the owner of the TThread and the thread contains only a TidHTTP (blocking socket) to post request and parse the response. Example : Object > Execute Request on the Thread Thread > Send request via idHTTP wait for response send the result to the Object Thread > Wait for another request Object > Update the UI depending of the result of the request A relative safe way to communicate with threads is using command queues. The object post a request in the queue (using semaphores). The tread checks the queue (using semaphores) and if it is filled executes the oldest request (you can introduce priorities if you want). If the task is finished the object is signalled (for example with a callback function). The thread normally sleeps and only awakes to check the queue. If there is nothing to do it ""presses the snooze button"" and sleeps again. Be sure to guard the access to the queue with semaphores. Else there is a chance of data corruption and you have a hard bug to find. Thanks for your answer :)  Another method worth mentioning is to use Async Calls by Andreas Hausladen. It is a simple to use and well written thread wrapper that works very well in a functional environment.  I don't know about ""best"" - depends what your criteria are. If you expand a little on your requirements we could maybe offer more specific help. In the meantime... The simplest method is to allow the owning object to write the request to the thread either into one or more properties or by a public method. The data fields behind the properties/method are not accessed directly by the main Execute routine: use a method called by Synchronize() to copy these data fields into variables that can be used by the Execute() routine. I use this method when speed is not the main objective and the owning object does not need to queue multiple requests. Many people disparage using Synchronize but it depends on the functionality you are trying to achieve. I try to keep things simple until the requirements demand otherwise. If throughput is more of an issue or if you need to have overlapping requests you could use a queue to store the requests with access to the queue controlled by a TCriticalSection. You could also use TThreadList either directly or as a basis for your own typed storage - I am not aware of a generic equivalent of TThreadList though there may well be one. Thanks for your help ! I need to use a queue to store the request and the thread can only execute one request at the same time. I can't spawn more thread that will use the same idHTTP so even if all requests are independent they must be executed one by one only. The application I'm working on is more explained on this post : http://stackoverflow.com/questions/1506212/complex-software-architecture"
970,A,"real time network usage statistics monitoring/capture? I would appreciate any advice from experience from the community regarding the following challenge I've given myself - i.e. any pointers re best approach / direction here? Requirements Allow collection / real-time-monitoring of network usage from a users Windows PC to a specific set of IP addresses (or DNS names) on a per application/process running on the PC point of view differentiating between ""up"" and ""down"" traffic. For example: show how much network traffic has been used (sending to the configured set of IP addresses/DNS names) for each PC process/application for the day so far. Solution should run on the PC that the user is utilising (i.e. not require setup of software on a separate PC) For Windows PC (e.g. XP Vista Windows 7) Shouldn't cause noticeable performance hit for the users (e.g. slow down internet browsing) Would want the data collected stored on the PC in a manner that a GUI program (e.g. C# WPF app) could access for displaying to user. For network related stuffs you should go for Microsoft Network Monitor tools. Also WinPcap and Ethreal are some tools available. Since WinPcap and Network Monitor both are network based also NetWork Monitor is MS Core Product so you can explore a wide range network related stuffs as compared to Ethreal or WinPcap. Thats what I thought. Also I am agree with Lieven. thanks Amit - is there a reason you have in mind re why Network Monitor would be better than WinPcap for my requirements? (noting I was hoping to wrap everything up I needed into the one app)  Microsoft Network Monitor can do all that. A new high performance capturing feature allows you to capture on faster networks without dropping frames. Parser profiles provide a simple way to increase filtering/parsing speed and allow you to switch quickly between various parser sets. And UI updates like Color Rules Windows Layouts and Column Management give you flexibility to do cool customizations to help you work the way you want. and Script-based parser model with frequent updates Concurrent live capture sessions Support for Windows Vista Support for 32-bit platforms and for 64-bit platforms Support for network conversations and process tracking API to access capture and parsing engine Wireless Monitor Mode Capturing thanks also Lieven - don't suppose you have a comment re WinPcap versus Network Monitor? in some distant past I have tried WinPcap. It didn't work for me then (note the then part). Network Monitor worked out of the box. You also should take into account that I was trying these tools merely out of curiousity not because I strictly needed them to solve a problem. As such I can not comment on wich one provides you with the better info to debug a problem. That said just looking at the output of Network Monitor taught me quite a few things. Actually *seeing* an ip packet wrapped in a tcp packet (or is it the other way around :) is enlightening."
971,A,How to control Download rate/ Upload rate in file-transfer using socket programming I am new to network programming and am trying to develop an application for rate controllable file transfer using socket programming in C++ for my networks' course assignment. I would like to know how to control the download rate/upload rate in file transfer. In fact is there a way to measure the bandwidth of the host? (so that we can know the time for a data sent with the send() to be received). TCP/IP doesn't allow rate control. Call sleep() to artificially slow down. The most common method of rate limiting is to use a token bucket. Basically you increment a counter at the rate you want to send then when you send the data you decrement that counter and only send as many bytes as the counter says is available. Thanks for getting back quickly. That's precisely what i need.
972,A,"Retrieve PAC script using WPAD on OSX How do I retrieve the PAC script using WPAD on OSX? is it enough to fetch the contents of ""http://wpad/wpad.dat"" in hopes that the DNS has ""wpad"" pre-configured for this convention? is there a more ""formal"" method of doing this? Here is how to get PAC proxies for a given URL: #import <Foundation/Foundation.h> #import <CoreServices/CoreServices.h> #import <SystemConfiguration/SystemConfiguration.h> CFArrayRef CopyPACProxiesForURL(CFURLRef targetURL CFErrorRef *error) { CFDictionaryRef proxies = SCDynamicStoreCopyProxies(NULL); if (!proxies) return NULL; CFNumberRef pacEnabled; if ((pacEnabled = (CFNumberRef)CFDictionaryGetValue(proxies kSCPropNetProxiesProxyAutoConfigEnable))) { int enabled; if (CFNumberGetValue(pacEnabled kCFNumberIntType &enabled) && enabled) { CFStringRef pacLocation = (CFStringRef)CFDictionaryGetValue(proxies kSCPropNetProxiesProxyAutoConfigURLString); CFURLRef pacUrl = CFURLCreateWithString(kCFAllocatorDefault pacLocation NULL); CFDataRef pacData; SInt32 errorCode; if (!CFURLCreateDataAndPropertiesFromResource(kCFAllocatorDefault pacUrl &pacData NULL NULL &errorCode)) return NULL; CFStringRef pacScript = CFStringCreateFromExternalRepresentation(kCFAllocatorDefault pacData kCFStringEncodingISOLatin1); if (!pacScript) return NULL; CFArrayRef pacProxies = CFNetworkCopyProxiesForAutoConfigurationScript(pacScript targetURL error); return pacProxies; } } return NULL; } int main(int argc const char *argv[]) { NSAutoreleasePool *pool = [[NSAutoreleasePool alloc] init]; CFURLRef targetURL = (CFURLRef)[NSURL URLWithString : @""http://stackoverflow.com/questions/4379156/retrieve-pac-script-using-wpad-on-osx/""]; CFErrorRef error = NULL; CFArrayRef proxies = CopyPACProxiesForURL(targetURL &error); if (proxies) { for (CFIndex i = 0; i < CFArrayGetCount(proxies); i++) { CFDictionaryRef proxy = CFArrayGetValueAtIndex(proxies i); NSLog(@""%d\n%@"" i [(id)proxy description]); } CFRelease(proxies); } [pool drain]; } For the sake of simplicity this code is full of leaks (you should release everything you got through Copy and Create functions) and does not handle any potential error.  See section 8 of the WPAD draft on compliance. Using only DNS as you suggest would make you ""minimally compliant"". To be fully compliant you should check to see if the host has received WPAD configuration from DHCP prior to using DNS. You should be able to use the System Configuration framework to see if the host received an option 252 parameter from the DHCP server. EDIT: Actually you can get the WPAD URL directly from the system configuration framework. Looks like you'd be interested in kSCPropNetProxiesProxyAutoConfigEnable and if that's set to 1 the WPAD URL should be in kSCPropNetProxiesProxyAutoConfigURLString."
973,A,"How to Intercept connection requests? I'm working on an app that needs to intercept when a program tries to connect to an specified IP address and redirects it to another IP. Is this possible? I'm using .net for this. thanks! Your best bet is to use EasyHook project to hook the connect function.  It might not fit the bill but you could use the ""hosts"" file. (""hosts"" file will only work for this purpose if the program is not connecting by IP address but a host name)"
974,A,Detecting IP addresses of network cards and ethernet devices with Windows API I am using the Microsoft code here to learn how to detect IP addresses of cards and devices: http://msdn.microsoft.com/en-us/library/aa365949%28v=VS.85%29.aspx I notice some strange behavior. I have a system with two ethernet cards; one is connected to the internet and one is connected to an ethernet device. When I run the sample code it will always give an IP address for the card that has the internet connection but the other card will come up as 127.0.0.1 with a subnet mask of 255.0.0.0 unless I have the ethernet device plugged in and powered. But the card should have a default IP address whether its actually connected to anything right? How can I modify this code to detect that? There's a third IP address detected that appears to be just empty data. I tried this on another computer with a single network connection and it also detected a second non-existent connection. Each time this connection has an IP address of 127.0.0.1 and a subnet mask of 255.0.0.0. What does this represent? Given the demo code would this be easy to edit to be able to detect IP addresses of devices on the network that any card is connected to? I really just want to detect the IP address that a single ethernet device is set to. The device is directly connected to the card. The reason I want to do this is because the device and card obviously don't play nice when their subnets are different and I want to detect when this is the case. Thanks! R Dunno exactly you should at least specify your configuration is it DHCP or static IP or something else? It's the loopback interface Some broadcasting may be required. ARP is the link-layer protocol so it can be used without the IP address to broadcast a link to find the devices and then to detect their address. Don't know about windows precisely but on Unix an arping command is present for this. 1. Both static IP and DHCP. Preferably static IP but the user can set everything up either way. 2. Thanks! 3. Thanks! @Awesomania Sorry I meant configuration for that second interface that gets disconnected. Is it said to get its address by DHCP or something else. Actually I tend to agree with selbie you could've just mistaken it for loopback again 'cause it looks unlikely that some real NIC will have this IP.  That address of 127.0.0.1 is not the address of the other card. It is the address of the loopback adapter - a virtual IP address that can only send and receive data with itself. The other NIC (that isn't plugged into a network) is simply not in the address table. You may just want to call GetAdapterAddresses and filter out all adapters with an IFType of IF_TYPE_SOFTWARE_LOOPBACK. If you want to see use existing tools that provide the same thing type either of the following from a command prompt: route print (This will dump the routing table) ipconfig /all (this will show you the state of ALL adapters including the loopback) +1 backing up with the `route` and `ipconfig` is a good idea for debugging!
975,A,"Is there a technical reason for applications to hang on DNS lookups? If I try to quit Firefox when it's ""Looking up domain.com..."" it beachballs (hangs) goes into ""not responding"" status then finally quits. It does this without fail. This is on Mac with the latest FF but it's always been this way for me even on Windows with FF and I've noticed it with other applications. Is there any technical reason why this can't be handled better? I haven't noticed anything with FF/windows XP but this might help: http://installingcats.com/2008/06/05/slow-internet-with-leopard/ Leopard had some well-documented DNS problems earlier this year you can search for blog posts /forums http://www.mac-forums.com/forums/airport-networking-wireless-technology/99001-internet-connection-slow-leopard-dns-airport-issue.html#post607373  There is a bug report for this problem. It has been around since at least 2004.  No. You can use threads or even seperate processes to do this asynchronously (non-blocking). This is just poorly written software. Aside from that I don't have this particular problem with FF... If I remember the current state of the code correctly the majority of DNS requests are serviced in a non-blocking fashion. There are some exceptions but that has little to do with the problem described. The browser is crashing (and it might not even have anything to do with the DNS service itself)  You should check bugzilla.mozilla.org to see if there are any recent bugs regarding the DNS service. Historically this type of problem is very rare in Firefox and other mozilla based products but there have been times where specific problems caused the DNS service to die or wait until a timeout. The other important question is: are you sure it is DNS? A packet trace or necko debug logs might be useful. The way the status bar works what is says is not necessarily what is keeping it from quitting."
976,A,"Get a remote MAC adress via IPv6 Hey guys i searched the web for a solution but i found nothing! :( Is it possible to get the MAC from an another PC in the same Network via IPv6 (without WMI)? With IPv4 it is easy (ARP). IPv6 uses the ""Neighbor Discovery Protocol"" (NDP) to get the MAC adress. Are there any methods in .Net for this? Any suggestions? Thanks for your help! Regards Fabian what's the point of OSX tag here? the question and and the accepted answer look completely irrelevant to OSX... You can run the external command ""netsh int ipv6 show neigh"" and filter out the host you are interested in. You should have contacted it just before that so you know it is in the NC. If you want an API for that use GetIpNetTable2 or more directly ResolveIpNetEntry2. I doubt there is a .NET API for this so you'll have to use PInvoke. Thank you. I think this should solve my problem. I tried it but it isn't simple to get it work in 5 minutes ;). If I have a result I contact over again  Martin's answer was for Windows but this is for if you are on a GNU/Linux or other *nix box. You want to use the neigh function of the ip command to show IPv6 neighbours like so: $ ip -6 neigh fe80::200:ff:fe00:0 dev eth0 lladdr 00:0e:54:24:23:21 router REACHABLE fe80::202:b0ff:fe01:2abe dev eth0 lladdr 00:02:b0:02:2a:be DELAY (Pro tip: you can leave the -6 off and view IPv4 ARP as well as IPv6 ND in the same list.) Also if you want to find out the MAC addresses of all the IPv6 machines on the LAN and not just those you already know about you should ping them first then look for neighbours: $ ping6 ff02::1%eth0 64 bytes from fe80::221:84ff:fe42:86ef: icmp_seq=1 ttl=64 time=0.053 ms # <-- you 64 bytes from fe80::200:ff:fe00:0: icmp_seq=1 ttl=64 time=2.37 ms (DUP!) 64 bytes from fe80::202:b0ff:fe01:2abe: icmp_seq=1 ttl=64 time=2.38 ms (DUP!) 64 bytes from fe80::215:abff:fe63:f6fa: icmp_seq=1 ttl=64 time=2.66 ms (DUP!) $ ip -6 neigh fe80::200:ff:fe00:0 dev eth0 lladdr 00:0e:54:24:23:21 router REACHABLE fe80::202:b0ff:fe01:2abe dev eth0 lladdr 00:02:b0:02:2a:be DELAY fe80::215:abff:fe63:f6fa dev eth0 lladdr 00:02:15:ab:f6:fa DELAY # <-- a new one! Many thanks for your response. Though she does not fit completely because I work in C# and .Net. But she has helped me nevertheless.  The answer by @Alex would be better if the line parsing code were improved. Here is one way: public static string netsh(String IPv6) { IPAddress wanted; if (!IPAddress.TryParse(IPv6 out wanted)) throw new ArgumentException(""Can't parse as an IPAddress"" ""IPv6""); Regex re = new Regex(""^([0-9A-F]\S+)\s+(\S+)\s+(\S+)"" RegexOptions.IgnoreCase); // ... the code that runs netsh and gathers the output. Match m = re.Match(output); if (m.Success) { // [0] is the entire line // [1] is the IP Address string // [2] is the MAC Address string // [3] is the status (Permanent Stale ...) // IPAddress found; if (IPAddress.TryParse(m.Groups[1].Value out found)) { if(wanted.Equals(found)) { return m.Groups[2].Value; } } } // ... the code that finishes the loop on netsh output and returns failure }  Here is my code: public static string netsh(String IPv6) { Process p = new Process(); p.StartInfo.FileName = ""netsh.exe""; String command = ""int ipv6 show neigh""; Console.WriteLine(command); p.StartInfo.Arguments = command; p.StartInfo.UseShellExecute = false; p.StartInfo.RedirectStandardOutput = true; p.Start(); String output = ""go""; while (output!=null){ try { output = p.StandardOutput.ReadLine(); } catch (Exception) { output = null; } if (output.Contains(IPv6)) { // Nimmt die Zeile in der sich die IPv6 Addresse und die MAC Addresse des Clients befindet // Löscht den IPv6 Eintrag entfernt alle Leerzeichen und kürzt den String auf 17 Zeichen So erschein die MacAddresse im Format ""33-33-ff-0d-57-00"" output = output.Replace(IPv6 """").Replace("" "" """").TrimToMaxLength(17) ; return output; } } return null; } Why the downvote? I suspect the down vote was because this is very fragile parsing code. If the IPv6 argument is in a different case from the netsh output it will fail. If the IPv6 argument is perfectly legal and correct but makes a different choice on leading zero or collapsing zero fields then it will fail. If the IPv6 argument is a subset of what happens to appear on a line it will give a false positive match."
977,A,"Sending UDP datagram without fragmentation I want to send broadcast message to all peers in my local network. Message is a 32 bit integer. I can be sure that message will not me fragmented right? There will be two options: - peer will receive whole message at once - peer will not receive message at all Going further 4 bytes is maximum size of data that can be sent in one UDP datagram? I use Ethernet based network in 99%. Ethernet packets can be up to around 1500 bytes (and that's not counting jumbo frames). If you send broadcast messages with a payload of only 4 bytes they shouldn't get fragmented at all. Fragmentation should only occur when the packet is larger than the Maximum Transmission Unit (so about 1500 bytes over Ethernet). I have often set MTU much smaller than 1500. Especially on modem or ISDN/IDSL links combined with QoS smaller packets help responsiveness. So don't *assume* 1500 is valid everywhere.  IPv4 specifies a minimum supported MTU of 576 bytes including the IP header. Your 4 byte UDP payload will result in an IP packet far smaller than this so you need not fear fragmentation. Furthermore your desired outcome - ""peer will receive whole message at once or peer will not receive message at all"" is always how UDP works even in the presence of fragmentation. If a fragment doesn't arrive your app won't recieve the packet at all. The rules for UDP are ""The packet may arrive out-of-order duplicated or not at all. If the packet does arrive it will be the whole packet and error-free."". (""Error-free"" is obviously only true within the modest limits of the IP checksum). +1 for fragmentation not an application issue. Fragmentation is what the ""IP"" part of UDP/IP is dealing with. It is handled in the lower part of the network stack and not something you don't have to bother with in your application."
978,A,process termination I have 2 processes and they send UDP messeges. They don't read the messages immeditely and sometimes when they read the messages they can read a bulk of them. I saw that when one of the processes is down the other one is still getting those messages. How come ? it is down. Thanks You keep getting packets after one process crashes because the OS buffers the incoming packets. See for example this discussion on UDP Buffering. When working with unconnected UDP sockets there is no way for either pier to know if the other pier has crashed other than sending round trip keep alive packets. OK. If it was TCP the buffer would still exists ? Yes. But you may see different behavior with TCP since if your pier closes / crashes the OS will send a FIN or RST packet indicating that the connection is closed and after you consume the remaining data in the buffer you will get a 0 length read or read error .
979,A,"Get the number of packages transmitted per connection How do I get the number of packages transmitted per TCP connection? I am using Java but i know I will have to fetch the number from the underlying OS so this quastion applies to Linux and Windows operating systems and will have different answers for each of them I assume. I need this information to profile the network load of an application which seems to send too many small packages by flushing the socket streams too often. Why is the number of packets on the network important? @Steve-o: Because too many (useless) packages reduce performance. If you send a single request in 20 packages instead of 1 you will decrease the performance significantly. Of course it is a programming problem and I am just intereseted in a way to monitor this. I already tested it. Performance has increased by a factor of 2.5 in my app when I improved the package creation (It was really bad before :))! hardware acceleration and receive interrupt coalescing pretty much negate any performance penalty. Plus you cannot just guess at performance results you need to test. I think that pure java API does not allow you to do this. As a workaround I'd recommend you to try JPcap. +1 looks interesting I will try it.  However I would start by checking your code uses buffering correctly. Is Nagle is left on the OS will coalesce these writes together even if you send one byte at a time. If Nagle is off your NIC driver could still coalesce data together. Using tcpdump/windump will confirm the actual packet sizes sent. How often the application flushes the stream will have little impact on how often the OS sends the data. You could also use a profiler like yourkit or a system tool like strace which can monitor the number of OS calls. Thanks for pointing out nagle. I will have to ensure nagle is off and try to better use appropriate buffering. @Dnaiel I would leave Nagle on until you have buffering you are happy with. Even then I would leave it on unless a few milli-seconds delay is too much.  Access to a TCP connection profiler would give this info at the local or server connection level. The number of packets is normally higher than expected for several reasons: header packets and frame packets connection establish and ack or response packets secure protocol negotiate and control packets packets small enough to be processed through POTS phone line equipment in some types of connection cryptography identity and algorithm control packets the data transmit protocol in use going through the tcp connection acting as a lower transport layer actual user or application data in packetised and framed form connection disconnect packets and responses. The streams may be flushed often if there is a multithread or multiuser situation and every connected user cannot recieve any data characters from the output stream which have just been transmitted to another user. Correct. This is why I am analyzing our application on the packet level currently to be able to see if there are only the packages send which are really needed.  Looks like you can get it (as root) by either examining /proc/net/nf_conntrack or running the ""conntrack"" utility with appropriate parameters. This assumes that netfilter connection tracking is enabled and you have the utility. You'll have to find the right connection yourself - but you should be able to identify it by the src/dst/sport/dport This is exactly what I was looking for. I am still waiting for an answer on windows so I don't accept this answer yet."
980,A,"Persistent HttpURLConnections on Android I've got an issue trying to get the Android application (well Service it case it makes any difference) to use persistent HTTP 1.1 connections. The following loop (simplified test case) works through a single TCP session on a desktop JRE but on an Android device results in the whole socket creation/teardown cycle.  while (true) { URL url; try { url = new URL(""http://10.0.0.125:8080/SRV?""); URLConnection connection = url.openConnection(); HttpURLConnection httpConnection = (HttpURLConnection) connection; int responseCode = httpConnection.getResponseCode(); } catch (MalformedURLException e) { } catch (IOException e) { } } The Oracle's JDK describes something called 'system properties': http.keepAlive= default: true http.maxConnections= default: 5 Is there something similar in Android's runtime that prevents persistent connections from being maintained? I was seeing the same issue with persistent HTTP 1.1 connections not being established. I wrote a quick test app to obtain more details. First I performed a TCP dump of traffic from my app to see what was happening. ""Connection:keep-alive"" is being sent properly to my server. My server was then responding with ""Connection:keep-alive"". However after my app closed its connection's InputStream the underlying socket was closed by Android as well...instead of being persisted. To dig deeper I wrote my app to connect using two different approaches: HttpURLConnection con = (HttpURLConnection) url.openConnection(); AND HttpClient client = new DefaultHttpClient(); It turn out that HttpClient was not persisting the underlying sockets but HttpURLConnection does. So if you want the best performance use HttpURLConnections until Android resolves this bug in DefaultHttpClient. Seems like a bug in Android HTTP 1.1 implementation? Use HttpURLConnection for applications targeted at Gingerbread and higher. https://developer.android.com/training/basics/network-ops/connecting.html  Android's JVM uses the Apache HTTP Components library under the hood for HTTP connections (even those that are done using the java.net interface): as such the behavior is subtly different from the Oracle JVM. In theory the underlying Harmony code respects the http.keepAlive system property but whether Google's copy preserves that behavior isn't certain to me. If you want to be absolutely certain what's happening you have to use the HttpComponents code. It is long and painful but if you look at http://hc.apache.org/httpcomponents-client-ga/tutorial/html/connmgmt.html it outlines the connection management approach for http components. Look at section 2.11 which details how to explicitly control the connection management using HTTP components. Good luck. As a quick test one might try also [URLConnection.setRequestProperty(""Connection"" ""keep-alive"")](http://developer.android.com/reference/java/net/URLConnection.html#setRequestProperty%28java.lang.String%20java.lang.String%29) if setting required header did the trick. Thanks for the pointer in the right direction - the Google documentation is not particularly straightforward about the fact that java.net internally uses Apache's HttpClient. I'll rewrite my code using Apache interfaces (and the information provided in 2.11) and would keep you updated. @Femi - as bizzare as it is using HttpClient seems to respect the ""http.keepAlive"" - perhaps the translation layer is setting something up in the wrong fashion (and doesn't accept the fact that I've changed the system property from default values) That is indeed odd. I've never tried to explicitly set the keepAlive bits but you are right: its entirely possible that one slipped through the cracks. @harism - it doesn't seem to change anything. @qdot - I see thanks sharing the information."
981,A,"Is there a way to forbid connection from outside of the local session in Windows? Let's consider I have a service in my user session that listens on some TCP port. Is there a way on Windows to only authorize processes from the same session to connect and to deny connections from the ""outside"" ? (""outside"" means another computer and another user session on the same host). I will also accept any alternative to TCP that allows only two process within the same session to communicate. This has to work for Windows Vista Seven and Server 2008. Many thanks. You can bind your listening socket on localhost (127.0.0.1) which limits connections to only internal local connections. If you then want to limit it further on the originating session you should build some form of authentication into the protocol running over the TCP connection. For example: the client must send the sessionname of it's own session before the server is going to communicate further. Can a user with limited privileges mess with the routing table or do you need administrator privileges? If you need administrator privileges to mess with the routing table the (possibly) built-in ""local"" authentication could also be circumvented by messing with the TCP/IP stack or replacing system files. Actually if someone mess up with the routing table an external host can under some circumstances access to `127.0.0.1`. Anyway I'd like to avoid building my own authentication scheme if the system can provide an alternative."
982,A,Bytes alignment when sending packets If I send a packet containing address port and other stuff would there be any alignment problem on the other side? (using socks 5 protocol) You don't mention which language you're programming in. If you're using C or C++ there are functions like htons htonl ntohs ntohl and more to standardize network data elements into a transportable format. This guide (among many others) will tell you how to write network portable code so differences in endianness will not bite you in the @ss. yesi am using c++ i use htons for example when i set the port to initialize the socketbut my packet looks like this: packet[20] how do i set all packet using thoose functions ? You have to apply those functions on an item by item basis to normalize each address port or whatever in the packet not the packet as a whole.
983,A,"BeginReceiveFrom callback method called only when running a packet sniffer I'm using asynchronous sockets to send and recieve UDP packets. The sending part of things works fine but the receiving side of things is not working with the callback method never called. However when i ran Wireshark to check that the UDP datagrams were arriving suddenly the callback is executed. Could anyone shed some light on why this should be? Thanks! some code could help to answer your question Wireshark puts your NC into promisuous mode - you are sth like sniffer. Then you can get packets that are not intended to you. Perhaps you send packet with some changed fields that makes NC in ""normal"" mode can't see him? E.g ICMP Echo: ICMP is the protocol behind the ping command. To ping a machine you send an ICMP Echo request packet to it and wait for an ICMP response one. Usually the ICMP request is embedded in an Ethernet packet to be delivered across the network. A standard Ethernet packet would include the MAC address of the addressed network card as well as the IP address of that machine in the embedded ICMP packet. The packet would be detected by the appropriate card and that machine would respond to the ping. This is the standard process. Now let's see what happens if we sent a ping packet (ICMP Echo request one) with the IP address of the suspected sniffer address but with a different faulty MAC address in the Ethernet envelope. If the network card in the sniffer machine is not on promiscuous mode then the packet will not be received by that machine. Naturally the machine wouldn't respond. The ping attempt would fail. If the network card in the sniffer machine is on promiscuous mode then the machine will see all packets in the network. The TCP/IP stack on that machine would thus accept the ping packet by identifying the received packet IP address. The stack would thus send a response. The ping attempt would succeed. Put your code here it will be easier :) This is indeed what was happening. The UDP packets were coming from a development board using a Microchip Ethernet controller. The firmware was putting the wrong MAC address into the UDP header so without Wireshark running and my NIC in promiscuous mode the packets were being rejected. Thanks for the pointer!"
984,A,"get LAN ip and print it Does anyone know how do I get my LAN IP and print it on the screen. *I don't mean on shell but in c programming. **I will appreciate if you'll post me a sample code. What have you tried so far? With which bit are you stuck? There's a few approaches; first you could set up a connection to a known peer using connect(2) and then read the local socket 'name' with getsockname(2). This is a pretty poor mechanism but it is easy. But getsockname(2) will only report one IP address when a machine may have thousands of IP addresses and which IP is returned will depend in part upon the peer that you chose! So not very reliable. A much better answer is to use rtnetlink(7) to read the IP addresses for the machine directly from the kernel: you would send RTM_GETADDR messages to the kernel for each interface on the machine and read the answers back. Your best bet for understanding how to use this is probably to read the source code for the ip program. Another option is to use the SIOCGIFCONF ioctl(2) on an IP socket. This interface isn't as flexible as the rtnetlink(7) interface so it might not always be correct but it'll be a mid-way point. The ifconfig(8) utility uses this approach for displaying ifconfig -a output. Again your best bet would be to read the source. (There is some slight documentation in ioctl_list(2).) `getifaddrs()` is an easier to use interface (which uses netlink underneath). thanks.. do you know where I can find sample codes..?  The getifaddrs() function from <ifaddrs.h> is the simplest way to get the current interfaces and corresponding addresses: #include <stdio.h> #include <ifaddrs.h> #include <netinet/in.h> #include <arpa/inet.h> int main() { struct ifaddrs *iflist *iface; if (getifaddrs(&iflist) < 0) { perror(""getifaddrs""); return 1; } for (iface = iflist; iface; iface = iface->ifa_next) { int af = iface->ifa_addr->sa_family; const void *addr; char addrp[INET6_ADDRSTRLEN]; switch (af) { case AF_INET: addr = &((struct sockaddr_in *)iface->ifa_addr)->sin_addr; break; case AF_INET6: addr = &((struct sockaddr_in6 *)iface->ifa_addr)->sin6_addr; break; default: addr = NULL; } if (addr) { if (inet_ntop(af addr addrp sizeof addrp) == NULL) { perror(""inet_ntop""); continue; } printf(""Interface %s has address %s\n"" iface->ifa_name addrp); } } freeifaddrs(iflist); return 0; } Thanks `getifaddrs(3)` is a much nicer interface. thanks man excellent solution :)  gethostbyname should help you to do this. Example: GetLocalAddress() { char ac[80]; // Get my host name if (gethostname(ac sizeof(ac)) != -1) { printf(""Host name: %s\n"" ac); // Find IP addresses struct hostent* p_he = gethostbyname(ac); if (p_he != 0) { for (int i=0; p_he->h_addr_list[i] != 0; ++i) { struct in_addr addr; memcpy(&addr p_he->h_addr_list[i] sizeof(struct in_addr)); printf(""IP address %d: %s\n"" i inet_ntoa(addr)); } } } You might want to filter to remove 127.0.0.1 from the list. I don't think 'C programming' has `std::cout` ;-) Thanks I edited to replace by printf. i'm using 4.4.5. thanks buddy you example did help me to understand linux-network programing. thanks buddy though I get segmentation fault when I run the code. I get warning from the compiler..for the line ""printf(""IP address %d: %s\n"" i inet_ntoa(addr));"" warning: format ‘%s’ expects type ‘char *’ but argument 3 has type ‘int’ This runs for me. No warnings or segfaults. GCC 4.3.3."
985,A,Beginner: Sending data over sockets Can anyone find an example of a simple server/client thing? I'm willing to use any C++ library or even Winsocks it self. I've Googled around but want some opinion on a good article for beginners/sites. see http://beej.us/guide/bgnet/output/html/singlepage/bgnet.html +1 Best guide ever written imho.  I like ACE. It is a pretty large library but there are some core classes that are really easy to use for beginning network programming. Here's a really good book for ACE too. Ace Programming Guide  If you are looking for a C++ networking library I suggest Asio which is now part of Boost. Start with the Tutorial. Also have a look at the examples.  JFGI http://www.adp-gmbh.ch/win/misc/sockets.html http://tldp.org/LDP/LG/issue74/tougher.html  If you're willing to use a C++ library I heartily recommend Qt. It gives you an easy way to communicate with sockets and much more. In particular see the QtNetwork module - a few of its relevant classes for your cause: QTcpSocket QTcpServer QUdpSocket. Thanks for the link but I think the first one links to the wrong thing. I think this is also a good idea to start with a lib first then go deeper into the sockets and some day write my own. @kooo: tx fixed the link Qt is nice and will allow you to things quickly. But if he really wants to understand how things work I believe it might more interesting for him to use the native socket directly. @ereOn: fair enough. My suggestion is with the assumption that he wants to write a real application. For learning Beej's guide is a much better idea
986,A,two certificates sent during peer verification. Why? def cert_check(conncerterrnumdepthok): print 'Got cert'cert.get_subject() return ok Server: ctx = SSL.context(SSL.TLSv1_METHOD) ctx.set_verify(SSL.VERIFY_PEERverify_cb) ctx.use_private_key_file('server.key') ctx.use_certificate_file('server.crt') ctx.load_verify_locations('ca.crt') Client: ctx = SSL.context(SSL.TLSv1_METHOD) ctx.set_verify(SSL.VERIFY_PEERverify_cb) ctx.use_private_key_file('client.key') ctx.use_certificate_file('client.crt') ctx.load_verify_locations('ca.crt') How is it that on both client and server side I get two certificates. One with no CommonName and one with the correct CommonName= myownserver.com/myownclient.com All the aforementioned files have just one key/certificate. Also I am guessing that the first printed certificate is the ca.crt because it is the only certificate without any CommonName. But why would that happen? This depends on the depth that has been set for verification. From the man pages the maximum depth for the certificate chain verification that shall be allowed for the current context. Also depth's default value is 9.
987,A,Checking for open ports in C/C++ There have been other questions regarding the subject of verifying the accessibility and accessibility of socket ports. How would one go about looking for a port to listen on dynamically in C/C++? The basic process I'm trying to accomplish is this: Client starts Client finds open port XYZ and listens on it. Client transmits a basic 'I Am Here' message via UDP Datagrams to a server with the port information Client and Server can communicate. I know you can accomplish something like this if you pick an arbitrary port number and try binding to it. If it fails increment the number and try again until you get a successful 'bind'. Is there a more elegant way to do this? It seems kind of hacky. Sending port numbers inside the message will break clients behind a NAT firewall so don't do it. Just have the server use the port that the message arrived from. Which OS is this for? @David Windows right now but hopefully it will eventually be portable to linux as well. If you bind to port 0 a random port will be allocated. Then getsockname() may be used to find out the actual port used. That will work. But Sean do not send the port number in a UDP message. That will break NAT. Just have the server respond to the port that the message arrived from. @Zan NAT isn't going to be an issue. It's only going to be used between elements on the same network segment. +1 this is also used by FTP servers when client is in passive mode
988,A,"join/leave multicast group using libpcap I need to receive a multicast stream but filter incoming packets by source MAC address on CentOS 5.5. I'm planning to use libpcap library. Is it possible to join/leave multicast group using libpcap? If yes how to do that? Thanks Sure just construct and send the appropriate IGMP packets. The answer above is better than my suggestion. But if you can relate source MAC addresses to IP addresses you would be better off doing it *all* in the application no libpcap at all. Just connect the MC socket to the desred source IP address that will filter out everything else. If there are more than one desired sources you can run multiple MC sokcets on the same port. There's a socket option for that. You're really doing this the hard way! Do it all with a UDP socket it's easy. Well easier. I need not to receive a packets that my application is generating. I created the filter that pass all packets except the packets that coming from my own MAC address What the option? Do you mean to use setsockopt()? I checked all possible options and did not found anything that can help IP_MULTICAST_LOOP or IPv6_MULTICAST_LOOP. But even if you don't have it it would be a lot simpler to check the IP address the incoming datagram was received from in your application code and forget about libpcap altogether. Multicast is hard enough already ;-) How to do that? What API I need to use? thanks it's working!  1.Create dummy socket: sd = socket(AF_INET SOCK_DGRAM IPPROTO_UDP); 2.Bind it: rc = bind(sd (sockaddr*) &addr sizeof(sockaddr_in)); 3.Join multicast group: ip_mreq mreq; mreq.imr_interface.s_addr = htonl(InterfaceIp); mreq.imr_multiaddr.s_addr = htonl(DestIp); if (setsockopt(sd IPPROTO_IP IP_ADD_MEMBERSHIP &mreq sizeof(mreq)) < 0) { close(sd); // Error handle... } Don't send or receive packets using dummy socket 4.open pcap using pcap_open_live() The general idea is use regular socket in order to ""tell"" kernel to send IGMP join packet and after use pcap in order to capture packets."
989,A,"Running a simple TCP server with poll() how do I trigger events ""artificially""? I have a fairly basic TCP server keeping track of a couple connections and recv'ing data when it's available. However I'd like to artificially trigger an event from within the program itself so I can send my TCP server data as if it came from sock1 or sock2 but in reality came from somewhere else. Is this possible or at all clear? struct pollfd fds[2]; fds[0].fd = sock1; fds[1].fd = sock2; while (true) { int res = poll(fds 2 timeout); if ((fds[0].revents & POLLIN)){ //ready to recv data from sock1 } if ((fds[1].revents & POLLIN)){ //ready to recv data from sock2 } } Do you mean put data into the queue of sock1 or sock2 without connecting to localhost with another socket ? Exactly I don't want to connect to localhost with a new socket -- I actually want to ""spoof"" one of the existing sockets. Create a pair of connected sockets (see socketpair(2)) and wait for events on one of the sockets in your poll loop. When you want to wake up the poll thread write a single byte to the other socket. When the polling loop wakes up read the byte do whatever was required and continue. Do you mean use the write() function to write to its buffer then make an if-statement saying ""if you receive a 1-byte message with this value do this""? Why is the socketpair necessary -- is there a reason I can't write directly to a socket's buffer that I'm waiting on? As you suspect you need socketpair() because you cannot write to the socket buffer you are waiting on. So you need the pair of sockets to give other parts of your program a way to wake up the poll loop. The poll loop needs to notice that it has been woken up in this way and then do whatever it is you want to do. The next question is how you tell it what to do: you wither need an internal data structure or you could write a complete message to the write end of the socket pair. If you write complete messages and you have multiple writers be careful about non-atomic writes corrupting messages. ""wither"" -> ""either"" Yay self-pipe trick :)  I would consider something like this: struct pollfd fds[2]; fds[0].fd = sock1; fds[0].events = POLLIN; fds[1].fd = sock2; fds[1].events = POLLIN; for (;;) { int result = poll(fds 2 timeout); if (result) { if ((fds[0].revents & POLLIN)){ /* Process data from sock1. */ } if ((fds[1].revents & POLLIN)){ /* Process data from sock2. */ } } else { /* Do anything else you like including processing data that wasn't from a real socket. */ } } Notes: don't forget to initialise your events field for(;;) is more idiomatic C than while(true) and doesn't require true to be defined How it answers the question? I was hoping for a way to externally trigger the poll() function whereas I think your code simply waits for it to time out and then provides a way to so something else.  This is more like a design question -- your polling loop should probably abstract the poll method to allow trapping on other external signals like from kill -USR1. If you really want to trigger port traffic you'll likely want to use netcat to send data to the socket."
990,A,Client connections with epoll I'm programming an application(client/server) in C++ for linux using epoll y pthreads but I don't know how to handle the connect() calls for attach a new connection in the descriptor list if a loop with epoll_wait() is running(Edge-triggered) How to can I do it?... I could to use a dummy file descriptor to trigger an event and scape of wait? or a simple call to connect() could fire the event??... Sorry for my bad english... Yes you can use another file descriptor that's just for waking up your epoll_wait() loop. Use pipe() to create the file descriptor. Add the reading end of the pipe to your epoll list and write a single byte to the writing end when you want to wake it up. The reading side can just read that byte and discard it. Thanks I will use `pipe()` to old kernel versions and `eventfd()` to new kernel versions(I have found it in man pages.) adew
991,A,"C/C++ detect network type I need to write a win32 c/c++ application which will be able to determine whether the PC it's running on is connected to one of 2 networks. The first network is the company LAN (which has no internet connection) and the second network is a standalone switch with a single PC connected to it (the PC that the program is running on). I'm pretty new to network programming but so far I have tried testing to see if a network drive which is held on our LAN can be mapped. This works fine if the PC is connected to the LAN the drive mapping succeeds so so LAN detection is successful. However if the PC is connected to the switch this results in a VERY long timeout which is not a suitable as it will delay the program so much as to make it unusable. Does anyone have any alternative suggestions? I'm using c/c++ in VS 6.0 [Update] Whilst trying a few different ideas and looking at some of the suggestions below I thought I should update with some additional information as many (if not all) of the suggestions I don't think will work. (1) The aforementioned LAN has no external connections at all it is completely isolated so no resolving of external DNS or pinging websites is possible. (2) Hostname MAC address IP Default Gateway Subnet etc etc (basically everything you see in ipconfig -all) are all manually configured (not dynamic from the router) so checking any of these settings will return the same whether connected to the LAN or the switch. (3) Due to point (2) any attempts to communicate with the switch seem to be unsuccessful in fact almost all networking commands (ping arp etc) seem to fail - I think due to the machine trying to connect to the LAN when it isn't there :-( One thing I have found which works is pinging the default gateway IP which times out when connected to the switch. This is sort of ok as I can reduce the timeout of ping so it doesn't just hang for ages but it feels like a bit of a hack and I would certainly appreciate any better solutions. Thanks unrelated to the problem but: you are really using the old compiler from 1998? So what you have is two different networks that are configured identically so that software can't tell the difference and now you want to tell the difference with software. @akira Unfortunately yes. This is legacy code and due to the complexity of the software and our testability constraints the company are reluctant to update compiler. If they did it would result in having to do a full system test which takes a team of 3 people upwards of 6 weeks to complete. As far as TCP/IP is concerned there is no such thing as a LAN on WAN. There are a set of non-internet routable addresses like 192.168.x.x and 10.x.x.x but these are sometimes used by ISP short of IP addresses. You best bet is to use Asynchronous APIs when making TCP/IP connections. WIN32 defines a whole buch of OVERLAPPED APIs for this purpose. This will prevent your application from grinding to a halt while waiting for a remote connection. Alternatively put the socket stuff into another thread and then only notify the UI when the operation is done. I'm not sure what you mean by the LAN on WAN comment. To clarify my description I just meant that our LAN is isolated so I cannot do something like ping Google for example (this seemed to be a commonly offered solution on forums I searched for network detection issues). Also putting the socket stuff in another thread will not work as the application cannot run until it knows which network it is connected to so it would have to wait any way.  Various things you can look at for differentiation: DNS domain name (GetComputerNameEx) MAC address of gateway (ping it then GetIpNetTable) Routing table(do you have a gateway and default route on the company LAN) WNet discovered network resources (WNetOpenEnum WNetEnumResource) Ability to resolve external hostnames (try a 5-10 names like www.google.com www.microsoft.com and so on if one resolves you should have internet) You'll have to decide how many indicators are ""enough"" to decide you're on one or the other LAN though if tests fail. Then keep retrying until you have a definite result. http://msdn.microsoft.com/en-us/library/aa366071%28v=VS.85%29.aspx has a lot of network related functions that you can experiment with to create further indicators. I just updated my question - bearing in mind the new information will any of the above still work?  I would first try to differentiate between the two using information available locally--that is from your computer. Does the output of ipconfig /all differ depending on which network you're connected to? If so exploit that difference if you can. Is it possible to get the MAC address of the standalone switch? Of the switch that controls the company LAN? That would be a sure way to tell. Unless somebody cloned the MAC address. If you try using the existence or non-existence of some network service to determine which network you're connected to you can never be sure. For example if you failed to map that network drive all you know is that the network drive isn't available. You can't say for certain that you're not connected to the company LAN. Same is true if you use ping. Lack of response from a particular machine means only that the machine didn't respond. Some of these suggestions won't work (please see updated question) I do know the MAC address of the standalone switch though how can I use this? Sorry for being a noob I do appreciate the help! @Gavimoss: The Windows `arp` command will show you the MAC address of the machine connected at a particular IP address. That is `arp -a 10.77.76.1` will show you the IP address and the MAC address. You can access that data in your program through the IP helper APIs. http://msdn.microsoft.com/en-us/library/aa366073(v=VS.85).aspx If you know the MAC address of the standalone switch then you can compare that against the value returned by `arp` (or the IP Helper function that you call). Apologies for the delay in accepting this was a side project that I just came back to finish off and your solution works perfectly - thanks!"
992,A,extracting public key from certificate and encrypting data This is for a homework assignment! I get the server's certificate using get_peer_certificate() and the calling dump_certificate to dump the certificate in a variable. The format is PEM and looks right to me. -----BEGIN CERTIFICATE----- GIBBERISH................ ...................... ........................ -----END CERTIFICATE----- How do I extract the server's public key from this file ('server.pubkey') and encrypt plaintext using RSA algorithm and any python library. At the time of writing this I am using pyOpenSSL I'd recommend using a more broad crypto library such as M2Crypto which has the X509 certificate functions as well as RSA encryption: from M2Crypto import RSA X509 data = ssl_sock.getpeercert(1) # load the certificate into M2Crypto to manipulate it cert = X509.load_cert_string(data X509.FORMAT_DER) pub_key = cert.get_pubkey() rsa_key = pub_key.get_rsa() cipher = rsa_key.public_encrypt('plaintext' RSA.pkcs1_padding)
993,A,How to call accept() with non-blocking sockets c++ This my be a silly question but I'm trying to use non-blocking sockets for the first timebut what I noticed is that when I run the server program it imediately gives me error in accept()(WSAEWOULDBLOCK what i have read this is an usual thing). My question is thenhow can I connect to the server then ? I am using ioctlsocket in Windows. Thanks. By using non-blocking sockets accept() will immediately return if there's no client connection waiting. You need to check if the error is WSAEWOULDBLOCK ignore it if it is and use a polling loop to check again later. The select function may be useful.
994,A,"Sniffing detection Can someone tell me how exactly works ""test ICMP""? (One of methods to detect sniffing in local network) Your phrasing doesn't make any sense to me. Maybe you can clarify? I heard about sending packet ICMP to host that is suspected to be sniffer. This packet should have some changes (MAC address of sth) but I'm not sure what and why. Sniffing detection is basically detecting if there are any sniffers in your network. The main feature of sniffers that is used to detect them is that they place the network card in promiscuous mode listening for all traffic. Typically a sniffer is placed on a machine with a full TCP/IP stack which will be affected by this mode. ICMP is the protocol behind the ping command. To ping a machine you send an ICMP Echo request packet to it and wait for an ICMP response one. Usually the ICMP request is embedded in an Ethernet packet to be delivered across the network. A standard Ethernet packet would include the MAC address of the addressed network card as well as the IP address of that machine in the embedded ICMP packet. The packet would be detected by the appropriate card and that machine would respond to the ping. This is the standard process. Now let's see what happens if we sent a ping packet (ICMP Echo request one) with the IP address of the suspected sniffer address but with a different faulty MAC address in the Ethernet envelope. If the network card in the sniffer machine is not on promiscuous mode then the packet will not be received by that machine. Naturally the machine wouldn't respond. The ping attempt would fail. If the network card in the sniffer machine is on promiscuous mode then the machine will see all packets in the network. The TCP/IP stack on that machine would thus accept the ping packet by identifying the received packet IP address. The stack would thus send a response. The ping attempt would succeed. Similar to other methods of detection this has false positives as well as false negatives. The sniffer machine may be instructed to ignore all ICMP requests. Detecting promiscuous mode is not exactly detecting sniffers though it is a very significant clue. Thank you very much. That's exactly what I need :) Do you know maybe how to send some icmp packet like this (to suspected ip but different faulty mac address) in .NET/C#?"
995,A,"How to implement dead reckoning when turning is involved? ""Dead reckoning is the process of estimating one's current position based upon a previously determined position and advancing that position based upon known or estimated speeds over elapsed time and course."" (Wikipedia) I'm currently implementing a simple server that makes use of dead reckoning optimization which minimizes the updates required by making logical assumptions on both the clients and the server. The objects controlled by users can be said to be turning or not turning. This presents an issue with dead reckoning (the way I see it.) For example say you have point A in time defined by [position velocity turning: left/right/no]. Now you want point B after t amount of time. When not turning the new position is easy to extrapolate. The resulting direction is also easy to extrapolate. But what about when these two factors are combined? The direction of the velocity will be changing along a curve as the object is turning over t amount of time. Should I perhaps go with another solution (such as making the client send an update for every new direction rather than just telling the server ""I'm turning left now"")? This is in a 2D space by the way for the sake of simplicity. Well I think ""turning: left/right/no"" is insufficient to determine position B - you also need to know the arc at which the turn is being made. If you are turning left along a circular path of radius 1 you will end up at a different place than if you are turning along a circular path of radius 10 even though your initial position velocity and direction of turn will all be the same. If making the client send an update for every new direction and treating them as linear segments is an option that is going to be a much easier calculation to make. You can simply treat each new report from the client as a vector and sum them. Calculating a bunch of curves is going to be more complex. Ah yeah I do of course know the speed at which the direction changes as well but it can be assumed to be a constant.  For simplicity let's say that your vehicles have a turning radius r that's independant of speed. So to compute the new position given the initial coords and the time: compute the distance (that's velocity * time) compute how much you turned (that's distance / (2*pi*r)) add that arc to the original position. The last steps needs elaboration. Given the angle a computed in step 2 if you started at (00) with a due north heading (i.e. pi/2 radians) and are turning left then your new positions is: (r*cos(a)-1 r*sin(a)). If your original heading was different say it was ""b"" then simply rotate the new position accordingly i.e. multiply by this rotation matrix:  [ cos b  -sin b ] [ sin(b) cos(b) ] Finally add the initial position and you're done. Now you only need to send an update if you change the velocity or turning direction."
996,A,How to create a duplex connection using TcpListener and TcpClient? I have a situation where I need to send and receive information parallelly. My protocol can define a read port and a write port. I currently have the following code: public void Listen() { ThreadPool.SetMinThreads(50 50); listener.Start(); while (true) { var context = new ListeningContext(listener.AcceptTcpClient() WritePort); } } How can I create another listener from the TcpClient I am passing? A TcpClient object wraps a NetworkStream object. You use the GetStream() method of TcpClient to access the NetworkStream object which is then used to read data from and write data to the network. The MSDN article for NetworkStream says the following: Read and write operations can be performed simultaneously on an instance of the NetworkStream class without the need for synchronization. As long as there is one unique thread for the write operations and one unique thread for the read operations there will be no cross-interference between read and write threads and no synchronization is required. Use the TcpListener object to listen for incoming connections. Use the TcpClient object returned from the call to AcceptTcpClient() to communicate (read and write) with the remote endpoint. That's the answer I was looking for. Thank you. So that means I can't use the TaskFactory? It might run operations on the same thread. Long story short how can I gaurentee that read and write both run on different threads with the Task API?  TCP connection is a full-duplex pipe take a look here. You don't need a separate port or anything else. As far as I know you can only send or receive because the socket might be a file right? I can't find where it says that it's full duplex. No sockets are sockets files are files. @Nikolai N Fetissov: Linux implements sockets using file descriptors. Am I missing something? Not only Linux sockets are represented as file descriptors everywhere. You cannot call `accept()` on a regular disk file though. The kernel knows the difference. @Nikolai N Fetissov: I didn't say it's a regular file. It uses the hard drivce right? File descriptor is the offset into process open file table that references kernel file table that holds function pointers to processing routines. Those are different for each file system and for sockets and for shared memory segments etc. And no sockets do not use hard drives that'd be painfully slow networking :)
997,A,"Hold a network connection although IP address change Is it possible to hold an open TCP connection with a client while the IP address of the client is externally changed? For example the connection is establishes against address X but somewhen while the connection is open the client-side user asks for IP renew and gets another IP address. Can the connection remains alive in this case? Thanks in advance. No it cannot. Even if the local side could be massaged to understand that the connection is suddenly between different addresses the remote side will not understand and will refuse to work with it. You'd need to re-add the old IP address to continue using the connection. To do so: Linux: ip addr add 172.16.10.20/22 dev bond0 Windows: do some pointy-clicky or**fill in command here** Thanks. What do you mean in ""You'd need to re-add the old IP address to continue using the connection.""? How?  What I know is when using UDP the connection can be remained one of my application is to change a remote network adapter address via UDP. For TCP I agree with Soonts you should design your server and client application to allow the reconnect in a transparent way.  This is possibe with tcp v6 connections if you're using e.g. tunnelbrocker.net. Every time your IPv4 address changes the connection between your IP4 address and the tunnelbrocker's IP4 address is dropped then re-established however your IP6 attress is the same and all the TCP/IPv6 conections from your IP6 address to your destination IP6 addresses are still here. Or if you're designinng both your client & server your could design your protocol allowing the client to reconnect in a transparent way.. IPv6 can't be changed? The address can be changed manually. The IP6 address wont change if you'll just switch to another ISP and/or move to another continent: the ISPs are currently only providing IPv4 connectivity."
998,A,Implementing a Network Stream for FTPClient from a Read buffer(VC++) I have implemented a read/write stream to read a buffer manipulate the data(like adding headers and footers during output file creation) and write it to a file. What should I do to instead of writing to a file locally to write to a file in a remote location but I have only FTP access to the remote server. I wrote a client using POCO to transfer the file to the ftp server but it is a two step process. How can I implement a solution which directly writes to the FTP server? I am not able to get how to connect a source stream(which is actually a ReadFile call) to the FTP network stream? Thanks. You need an FTP client library that you can call directly from your app to avoid the need to write the file to disk and then send it via a separate process. This earlier question has some useful info. I got some good info. Thanks. Great good luck
999,A,How to get MAC address in windows7? Possible Duplicates: Getting Machine’s MAC Address — Good Solution? How do I get the MAC address of a network card using Delphi? I am using MAC address as hardware id for protection(ofcourse I have encrypted this data) I am using below code to get MAC address on user computer function MacAddress: string; var Lib: Cardinal; Func: function(GUID: PGUID): Longint; stdcall; GUID1 GUID2: TGUID; begin Result := ''; Lib := LoadLibrary('rpcrt4.dll'); if Lib <> 0 then begin @Func := GetProcAddress(Lib 'UuidCreateSequential'); if Assigned(Func) then begin if (Func(@GUID1) = 0) and (Func(@GUID2) = 0) and (GUID1.D4[2] = GUID2.D4[2]) and (GUID1.D4[3] = GUID2.D4[3]) and (GUID1.D4[4] = GUID2.D4[4]) and (GUID1.D4[5] = GUID2.D4[5]) and (GUID1.D4[6] = GUID2.D4[6]) and (GUID1.D4[7] = GUID2.D4[7]) then begin Result := IntToHex(GUID1.D4[2] 2) + '-' + IntToHex(GUID1.D4[3] 2) + '-' + IntToHex(GUID1.D4[4] 2) + '-' + IntToHex(GUID1.D4[5] 2) + '-' + IntToHex(GUID1.D4[6] 2) + '-' + IntToHex(GUID1.D4[7] 2); end; end; end; end; above code works perfectly on windows XP but its giving different values in windows7 the value changing every time after computer resratred :( is there any chance of getting MAC address thats constant (unless user changed his MAC address) or is there any good code which retrvies constant data on all OS ? thanks in advance What makes you think that [this code](http://msdn.microsoft.com/en-us/library/aa379322%28VS.85%29.aspx) retrieves your computer's MAC address? Ack! That function doesn't return a MAC address. It returns a value that sometimes just happens to contain a MAC address. @steve0 to retrieve the mac address of an Network Adapter you can use the WMI and the Win32_NetworkAdapterConfiguration Class and check the MACAddress property. Check this code: program WMI_MAC; {$APPTYPE CONSOLE} uses SysUtils ActiveX ComObj Variants; function VarToStrNil(Value:Variant):string; //Dummy function to onvert an variant value to string begin if VarIsNull(Value) then Result:='' else Result:=VarToStr(Value); end; Procedure GetMacAddress; var objWMIService : OLEVariant; colItems : OLEVariant; colItem : OLEVariant; oEnum : IEnumvariant; iValue : LongWord; wmiHost root wmiClass: string; function GetWMIObject(const objectName: String): IDispatch; var chEaten: Integer; BindCtx: IBindCtx;//for access to a bind context Moniker: IMoniker;//Enables you to use a moniker object begin OleCheck(CreateBindCtx(0 bindCtx)); OleCheck(MkParseDisplayName(BindCtx StringToOleStr(objectName) chEaten Moniker));//Converts a string into a moniker that identifies the object named by the string OleCheck(Moniker.BindToObject(BindCtx nil IDispatch Result));//Binds to the specified object end; begin wmiHost := '.'; root := 'root\CIMV2'; wmiClass := 'Win32_NetworkAdapterConfiguration'; objWMIService := GetWMIObject(Format('winmgmts:\\%s\%s'[wmiHostroot])); colItems := objWMIService.ExecQuery(Format('SELECT * FROM %s'[wmiClass])'WQL'0); oEnum := IUnknown(colItems._NewEnum) as IEnumVariant; while oEnum.Next(1 colItem iValue) = 0 do //if VarToStrNil(colItem.MACAddress)<>'' then //uncomment if you only want list the interfaces with mac adress //if colItem.IPEnabled then // uncomment if you only want list the active interfaces begin WriteLn('Card Description '+VarToStrNil(colItem.Caption)); WriteLn('MACAddress '+VarToStrNil(colItem.MACAddress)); end; end; begin try CoInitialize(nil); try GetMacAddress; Readln; finally CoUninitialize; end; except on E:Exception do Begin Writeln(E.Classname ': ' E.Message); Readln; End; end; end.  Here is some code working well for any computer on your network - may try it to get your own using '127.0.0.1' as IP: function GetRemoteMacAddress(const IP: AnsiString): TSockData; // implements http://msdn.microsoft.com/en-us/library/aa366358(VS.85).aspx type TSendARP = function(DestIp: DWORD; srcIP: DWORD; pMacAddr: pointer; PhyAddrLen: Pointer): DWORD; stdcall; const HexChars: array[0..15] of AnsiChar = '0123456789ABCDEF'; var dwRemoteIP: DWORD; PhyAddrLen: Longword; pMacAddr : array [0..7] of byte; I: integer; P: PAnsiChar; SendARPLibHandle: THandle; SendARP: TSendARP; begin result := ''; SendARPLibHandle := LoadLibrary('iphlpapi.dll'); if SendARPLibHandle<>0 then try SendARP := GetProcAddress(SendARPLibHandle'SendARP'); if @SendARP=nil then exit; // we are not under 2K or later dwremoteIP := inet_addr(pointer(IP)); if dwremoteIP<>0 then begin PhyAddrLen := 8; if SendARP(dwremoteIP 0 @pMacAddr @PhyAddrLen)=NO_ERROR then begin if PhyAddrLen=6 then begin SetLength(result12); P := pointer(result); for i := 0 to 5 do begin P[0] := HexChars[pMacAddr[i] shr 4]; P[1] := HexChars[pMacAddr[i] and $F]; inc(P2); end; end; end; end; finally FreeLibrary(SendARPLibHandle); end; end; This code is extracted from our freeware and open source framework unit SynCrtSock.pas. See http://synopse.info/fossil  Check out How do I get the MAC address of a network card using Delphi? - the questions already been asked and answered a few times on SO.
1000,A,Is SCTP good for peer-to-peer apps? I am considering using SCTP instead of TCP for a p2p app written in C. Should I do it? Also how does the speed of SCTP compare to the speed of TCP? EDIT: I found that SCTP can be tunneled over UDP with the only problem being tunneled SCTP is not interoperable with untunneled SCTP. How does it help you? You're P2P so every peer must have at least one socket open to every other peer. If you've got a socket open then you can do everything you need to do over that. If you've taken the approach of one socket per file and you have multiple files being tranferred concurrently between two given peers then SCTP will save you one socket per file. However on a normal P2P network of any size you will almost never have multiple files being transferred concurrently between two peers. Just have one socket and have your own little protocol; send a packet with a header the header indicates content type e.g. a command or part a file - and if so which file and which byte range. Of course you get a little overhead for that whereas if you have one socket for commands and one per file you're more efficient. Is saving one socket per peer (assuming one download at a time) worth the time/hassle/complexity of using SCTP? Once I learn all the ins and outs of SCTP I think it would be simpler to use SCTP because it takes care of many issues for me such as fault tolerance. SCTP has smaller packet headers (to my knowledge) so this would reduce the overhead of making many small requests. Packet loss is so rare an event it does not justify using SCTP over TCP; all you gain is a couple of seconds of continued throughput on the other connections *if* you're concurrently transferring multiple files to the same peer. As for smaller headers - I suspect you will end up using UDP anyway. If you have five or ten thousand queued peers you won't be holding a TCP/SCTP socket open to them all. You'll merely keep in touch with the odd UDP packet now and then.  Have you considered whether your target systems will all have SCTP pre-installed on them or whether your application will need to include SCTP itself? In my experience I would not expect all systems to have SCTP installed on them and I would expect them not to if it were Windows. If you include SCTP in the application itself then that will more than double the number of messages being passed into an out of the Kernel which will impact performance when compared with using the pre installed TCP. Have you considered what benefits you want from SCTP? You mentioned fault tolerance but for this to work with SCTP it requires the application to have multiple ethernet ports and and IP addresses. Is this likely on your app? As much as I love SCTP (!) I would seriously consider sticking with TCP unless you are sure SCTP is needed or unless you control the hosts your app is deployed on. Regards  If it's for a local area network sure go for it. Note however that if you plan to use it on the open internet many consumer grade firewalls aren't flexible enough to permit unrecognised IP protocols through them. OP asked also about SCTP tunneled over UDP does this resolve the problem with firewalls? what about hole punching for NAT traversal? is it possible in case of SCTP? Tunnelling would solve the firewall problem. Hole punching would depend on the tunnel running on a consistent well-known port. See http://tools.ietf.org/html/draft-ietf-behave-sctpnat-05 for the current state of SCTP over NAT and the proposal to fix it. SCTP has poor protocol support at the moment and actual implemented support is worse -- to the extent of not working with many consumer grade devices.
1001,A,"Problems creating and binding port in unix. (New to network programming) I'm having trouble creating a port in Unix. This code keeps returning ""Error creating socket (3)"" which makes no sense whatsoever since sockfd must be smaller than 0 to print the error line. server.c: int main (int argc char **argv) { int sockfd; struct sockaddr_in server_sockaddr; char buff[TRANSFER_BUFFER_LEN]; /* check number of command line arguments */ if (argc < 2) { fprintf (stderr ""Usage: %s <port>\n""); exit (EXIT_FAILURE); } /* create server socket */ if ((sockfd = createUdpSocket (&sockfd &server_sockaddr atoi (*(argv + 1))) ) < 0); { fprintf (stderr ""Error creating socket (%d).\n"" sockfd); exit (EXIT_FAILURE); } ... return 0; } socket_utils.h: int createUdpSocket (int *sockfd struct sockaddr_in *client_sockaddr int port) { /* create socket */ if ((*sockfd = socket (AF_INET SOCK_DGRAM 0)) == -1) return -1; memset (client_sockaddr 0 sizeof (struct sockaddr_in)); client_sockaddr->sin_family = AF_INET; client_sockaddr->sin_addr.s_addr = htonl (INADDR_ANY); client_sockaddr->sin_port = htons (port); /* bind socket */ if (!bind (*sockfd (struct sockaddr*) &client_sockaddr sizeof (sockfd))) { close (*sockfd); return -2; } return *sockfd; } Any ideas? This same function works fine in my client program that doesn't take a port value (instead takes 0). Cheers Rhys Which port # are you trying to open? Ports <= 1024 require privileges. Looks like you have too many parentheses which is confusing the code and yourself....Also here is a useful guide to understanding network programming with sockets courtesy of beej... Cheers this looks good.  if ((sockfd = createUdpSocket (&sockfd &server_sockaddr atoi (*(argv + 1))) ) < 0); Whoops where'd that semi-colon come from? You've got a stowaway! Also this line won't do what you expect: if (!bind (*sockfd (struct sockaddr*) &client_sockaddr sizeof (sockfd))) The bind() function returns 0 on success -1 on failure. Your test is backwards and will error out when bind succeeds rather than fails. It's also got an incorrect & where you say &client_sockaddr. And the third argument needs to be the size of the sockaddr_in struct not sizeof(sockfd). Try: if (bind (*sockfd (struct sockaddr*) client_sockaddr sizeof (struct sockaddr_in)) != 0) And heck while we're here your usage printout is missing its argv[0] argument: fprintf (stderr ""Usage: %s <port>\n""); Oh the shame! Thanks very much."
1002,A,For distributed applications which to use ASIO vs. MPI? I am a bit confused about this. If you're building a distributed application which in some cases may perform parallel operations (although not necessarily mathematical) should you use ASIO or something like MPI? I take it MPI is a higher level than ASIO but it's not clear where in the stack one would begin. I know nothing about ASIO but from a quick Google it looks to me to be a lot lower level than MPI. For me the whole point of MPI is so that I can program against a higher level of abstraction from the messaging than it seems ASIO provides. Where you begin depends on your needs. For mine parallelising scientific codes for high-performance the obvious answer is MPI. I'm not sure I'd use it or at least not sure it would be my default choice if I were writing more general-purpose distributed as opposed to parallel applications. Well actually it probably would be my default choice to avoid learning another approach (most of which are less portable and less long-lived than MPI) but I'll admit it might not be the best choice if starting from an equal footing. Yes that's probably fair. That's the domain that MPI came from and where it's most used. And it's more tailored for parallel computations than distributed; it really doesn't for instance cope well with the failure of a process in a gang which is something a lot of distributed computations require. So MPI is really geared towards scientific parallelism and math and not general purpose distributed programming. Fair to say that?  As far as I know MPI is currently incapable of handling the situation when the new distributed nodes want to join the already started group. The problems also may occur if one of the nodes goes offline. MPI does not reveal any network related machinery that is underneath. Thus if you would ever need something on the lower level -- you're in trouble. If you on the other hand do not aticipate such a need then you'll save yourself a lot of time using MPI.
1003,A,"Bufferwriting/sending messages problem in javaNIO My problem is concerning JAVANIO client server message passingi m unsure about defining the problem technically but: it seems that buffer is caching the data and when it is done then it is sending all together which is disturbing logic: private void sendCreate(String lineSocketChannel from) /* A new client wishes to join the world. This requires the client to find out about the existing clients and to add itself to the other clients' worlds. Message format: create name xPosn zPosn Store the user's name extracted from the ""create"" message */ { StringTokenizer st = new StringTokenizer(line); st.nextToken(); // skip 'create' word userName = st.nextToken(); String xPosn = st.nextToken(); // don't parse String zPosn = st.nextToken(); // don't parse // request details from other clients sendBroadcastMessage( ""wantDetails "" + achannel.socket().getInetAddress() + "" "" + portfrom); // tell other clients about the new one sendBroadcastMessage( ""create "" + userName + "" ""+xPosn+"" ""+zPosnfrom); } // end of sendCreate() method responsible for broadcasting messages from server: private void sendBroadcastMessage(String mesg SocketChannel from) { prepWriteBuffer(mesg); Iterator i = clients.iterator(); while (i.hasNext()) { SocketChannel channel = (SocketChannel) i.next(); if (channel != from) channelWrite(channel writeBuffer); } } i m assuming that this should send the first message i.e sendBroadcastMessage( ""wantDetails "" + achannel.socket().getInetAddress() + "" "" + portfrom); but this is notit seems that it is waiting for other method call i.e sendBroadcastMessage( ""create "" + userName + "" ""+xPosn+"" ""+zPosnfrom);and then sending both message as one message which is affecting application logic.ideally it should or it should send the first message after first call to sendBroadcastMessage and then when client recive the first then other call should be processed. these are methods which are using in sendBroadcastMessage(): private void prepWriteBuffer(String mesg) { // fills the buffer from the given string // and prepares it for a channel write writeBuffer.clear(); writeBuffer.put(mesg.getBytes()); writeBuffer.putChar('\n'); writeBuffer.flip(); } private void channelWrite(SocketChannel channel ByteBuffer writeBuffer) { long nbytes = 0; long toWrite = writeBuffer.remaining(); // loop on the channel.write() call since it will not necessarily // write all bytes in one shot try { nbytes += channel.write(writeBuffer); } catch (ClosedChannelException cce) { cce.printStackTrace(); } catch (Exception e) { e.printStackTrace(); } // get ready for another write if needed writeBuffer.rewind(); } please suggest some solution. thanks jibby lala Edit : what about thisi got this patch from some chat app: private void prepWriteBuffer(String mesg) { // fills the buffer from the given string // and prepares it for a channel write writeBuffer.clear(); writeBuffer.put(mesg.getBytes()); writeBuffer.putChar('\n'); writeBuffer.flip(); } // called needs to remove the channel if it fails otherwise it will fail forever. private void channelWrite(SocketChannel channel ByteBuffer writeBuffer) { long nbytes = 0; long toWrite = writeBuffer.remaining(); // loop on the channel.write() call since it will not necessarily // write all bytes in one shot try { while (nbytes != toWrite) { nbytes += channel.write(writeBuffer); try { Thread.sleep(CHANNEL_WRITE_SLEEP); } catch (InterruptedException e) { } } } catch (ClosedChannelException cce) { } catch (Exception e) { } // get ready for another write if needed writeBuffer.rewind(); } perhaps you intended  while(writeBuffer.remaining()>0) channel.write(writeBuffer); However you problem appears to be that you assume there is some type of magic marker between messages. However no such divider exists. A stream is just a stream of bytes. When you read in a blocking mode you will get atleast one byte you may get more this may span what was multiple writes but unless you include in the stream where you expect a message to start and end you will have no way of knowing. A simple approach is to write the length of the message at the start of the message and read at most single message until you get all of it. Something like. private void prepWriteBuffer(String mesg) { // fills the buffer from the given string // and prepares it for a channel write writeBuffer.clear(); byte[] bytes = mesg.getBytes()); writeBuffer.putInt(bytes.length); writeBuffer.put(bytes); writeBuffer.flip(); } // called needs to remove the channel if it fails otherwise it will fail forever. private void channelWrite(SocketChannel channel ByteBuffer writeBuffer) throws IOException { while(writeBuffer.remaining()>0) channel.write(writeBuffer); writeBuffer.rewind(); } Thanks for the answer but this is inserting 4 additional characters in between those messages and still concatenating them and then sending. correct. that way to receiver can read the 4 bytes to determine the length and know in advance the length of the message. If you need a pure text format you can do that but decoding it to find the message boundaries with NIO is trickier."
1004,A,Can Socket.Receive() return 0 if the connection is still open? I am writing a C# .NET server application that sends and receives data over Sockets and I am having some issues if the client application crashes without closing the socket the right way. I have set the 'receive timeout' to a given value and I expected the Socket.Receive() to throw an exception after that amount of time. But instead the method just returns 0. So my question is: Is it possible that the socket is still open if the Socket.Receive() returns 0? Or can I safely assume that it is down? (Might be a bit tricky to understand. If so please let me know in the comments) Your question is a little confused. The Socket is open until you close it. The connection is open until either end closes it. The read side of a connection can be closed by the sender shutting it down for output. So when reading a connection you will receive zero if the peer has either closed his socket or shut it down for output.  Nope. It's down if you receive 0. From MSDN: If the remote host shuts down the Socket connection with the Shutdown method and all available data has been received the Receive method will complete immediately and return zero bytes. http://msdn.microsoft.com/en-us/library/8s4y8aff.aspx Thank you for the rapid response! I read that article but I couldn't figure out if that is the only case when the method can return 0. But you are sure of this? +1 - yup only when it's close will it return 0 Thanks guys! :) @csharptest.net this is also my experience. but do you have any reference for this (official documentation)? @RobertHegner: It's the way that sockets work. 0 can only be received if the socket is down. Isn't the MSDN article that I linked to official enough? Here is the one for the C method: http://msdn.microsoft.com/en-us/library/windows/desktop/ms740121(v=vs.85).aspx
1005,A,"socket programming problem with select I have two nodes communicating with a socket. Each node has a read thread and a write thread to communicate with the other. Given below is the code for the read thread. The communication works fine between the two nodes with that code. But I am trying to add a select function in this thread and that is giving me problems (the code for select is in the comments. I just uncomment it to add the functionality). The problem is one node does not receive messages and only does the timeout. The other node gets the messages from the other node but never timesout. That problem is not there (both nodes send and receive messages) without the select (keeping the comments /* */). Can anyone point out what the problem might be? Thanks. void *Read_Thread(void *arg_passed) { int numbytes; unsigned char *buf; buf = (unsigned char *)malloc(MAXDATASIZE); /* fd_set master; int fdmax; FD_ZERO(&master); */ struct RWThread_args_template *my_args = (struct RWThread_args_template *)arg_passed; /* FD_SET(my_args->new_fd &master); struct timeval tv; tv.tv_sec = 2; tv.tv_usec = 0; int s_rv = 0; fdmax = my_args->new_fd; */ while(1) { /* s_rv = -1; if((s_rv = select(fdmax+1 &master NULL NULL &tv)) == -1) { perror(""select""); exit(1); } if(s_rv == 0) { printf(""Read: Timed out\n""); continue; } else { printf(""Read: Received msg\n""); } */ if( (numbytes = recv(my_args->new_fd buf MAXDATASIZE-1 0)) == -1 ) { perror(""recv""); exit(1); } buf[numbytes] = '\0'; printf(""Read: received '%s'\n"" buf); } pthread_exit(NULL); } You must set up master and tv before each call to select() within the loop. They are both modified by the select() call. In particular if select() returned 0 then master will now be empty. yes. that solved the problem. thanks!"
1006,A,"how to get IP address in C#? Assume that a computer is connected to many networks(actually more than one). I can get a list of IP addresses which includes all IP addresses the computer have in networks but how can I know that an IP address belongs to which network? First there are some terms you need to know. These example numbers presume an IPv4 network. ip address (192.168.1.1) subnet mask (255.255.255.0) network address (192.168.1.0) network interface card nic (one hardware card may have several of these) To see which network an ip address belongs to requires you to calculate the network address. This is easy if you take your ip-address (either as an Byte[4] or an UInt64) and bitwise ""and"" it with your subnet mask. using System; using System.Linq; using System.Net; using System.Net.NetworkInformation; using System.Net.Sockets; namespace ConsoleApplication { public static class ConsoleApp { public static void Main() { var nics = NetworkInterface.GetAllNetworkInterfaces(); foreach (var nic in nics) { var ipProps = nic.GetIPProperties(); // We're only interested in IPv4 addresses for this example. var ipv4Addrs = ipProps.UnicastAddresses .Where(addr => addr.Address.AddressFamily == AddressFamily.InterNetwork); foreach (var addr in ipv4Addrs) { var network = CalculateNetwork(addr); if (network != null) Console.WriteLine(""Addr: {0} Mask: {1} Network: {2}"" addr.Address addr.IPv4Mask network); } } } private static IPAddress CalculateNetwork(UnicastIPAddressInformation addr) { // The mask will be null in some scenarios like a dhcp address 169.254.x.x if (addr.IPv4Mask == null) return null; var ip = addr.Address.GetAddressBytes(); var mask = addr.IPv4Mask.GetAddressBytes(); var result = new Byte[4]; for (int i = 0; i < 4; ++i) { result[i] = (Byte)(ip[i] & mask[i]); } return new IPAddress(result); } } } Note that you can have several ip addresses on the same network that vpn connections may have a submask of 255.255.255.255 (so the network address == ip address) etc."
1007,A,"Boost.Asio documentation is non-existent. What do these errors mean? I'm struggling with two errors with Boost.Asio. The first occurs when I try to receive data on a socket: char reply[1024]; boost::system::error_code error; size_t reply_length = s.receive(boost::asio::buffer(reply 1024) 0 error); if (error) cout << error.message() << endl; //outputs ""End of file"" The second occurs when I try to create an ip::tcp::socket from a (valid!) native socket: boost::asio::io_service ioserv; boost::asio::ip::tcp::socket s(ioserv); boost::system::error_code error; s.assign(boost::asio::ip::tcp::v4() nativeSocket error); if (error) cout << error.message() << endl; //outputs ""The parameter is incorrect"" With all these troubles an no documentation to turn to I am tempted to go back to BSD sockets but I'm having my own problems there...so if anyone can help I'd really appreciate it. EDIT: Regarding number 2 nativeSocket is declared thusly: SOCKET nativeSocket = INVALID_SOCKET; nativeSocket = accept(svr_sock (struct sockaddr*)&sin &size); After that a few other things are done to the socket -- namely setting it as non-blocking using ioctlsocket and using setsockopt for SO_LINGER and SO_OOBINLINE. Just added it in. For your first question: ""End of file"" normally indicates that the connection was closed by the peer. For your second question: Could you check what is returned by `boost::system::error_code()` **before** calling `assign()`? Can you give the declaration/initialization for nativeSocket? I think the issues with prob 2 lie with that. This is not a complete solution to your second problem by any means. Any errors that it generates should be mapped into a boost::system::error_code but I don't find anything like it in boost/system/error_code.hpp so I'm at a loss as to what exactly it is supposed to mean. But after looking through the code for boost 1.39 assign is eventually handed off to either detail::reactive_socket_service< Protocol Reactor >.assign (or detail::win_iocp_socket_service<Protocol> if you're using windows). It can only be producing an error in two places in boost/asio/detail/reactive_socket_service.hpp: if (is_open(impl)) { ec = boost::asio::error::already_open; return ec; } or if (int err = reactor_.register_descriptor( native_socket impl.reactor_data_)) { ec = boost::system::error_code(err boost::asio::error::get_system_category()); return ec; } Since you're not getting an already_open error the error must from the second bit of code. The reactor type comes from a sequence of ifdef/elif pairs in boost/asio/stream_socket_service.hpp and of those available only the register_descriptor function in epoll_reactor can throw any error (and of course detail::win_iocp_socket_service<Protocol>.assign can also). The error in epoll_reactor comes from sys/epoll.h specifically: int result = epoll_ctl(epoll_fd_ EPOLL_CTL_ADD descriptor &ev); if (result != 0) return errno; In the windows implementation the related code is if (iocp_service_.register_handle(native_socket.as_handle() ec)) return ec; I think this is the origin of your error but honestly I can't trace it past this point."
1008,A,"Sources for news about network security I am a software developer that wants to stay up-to-date on network security news. What are some of the best sources online for not only keeping tabs on newly discovered security vulnerabilities that may affect projects I'm working on but also best-practices when developing network software. Please keep in mind that I am looking for sources that cater to the software developer not IT administration. For the parallel sysadmin's question: http://serverfault.com/questions/34523/security-resources All very good answers so far. I'll wait a few days to let the community vote and help me decide on the best answer. This will also give me time to try out all the resources mentioned. sans.org has many mailing lists which will keep you up to date on a multitude of different security issues SANS NewsBites will keep you up to date on cyber security news from around the world and how they pertain to the industry @RISK is a compendium of newly discovered vulnerabilies in the wild and what they are doing. They also provide links to many of them and almost always there is a link to security focus finally SANS OUCH is a more generic security newsletter with tips and tricks and interesting security stories. SANS is a world renowned post-secondary security college.  Bruce Schenier blog! http://www.schneier.com/ If you don't know him: http://en.wikipedia.org/wiki/Bruce_Schneier As a developer you might be also interested in the publications of the IEEE and ACM IEEE Security and Privacy http://www.computer.org/portal/web/security/home ACM Special Interest Group on Security Audit and Control http://www.sigsac.org/ I think that is a good starting base. I don't want to give you a longer list so you are not overwhelmed. Use those links to get you started. Once you read them you will get to know other people and then you might decide better which authors/places you want to learn from. can you track the latest vulnerabilities with that blog? @Berkay he asked for references for software developers and not IT administration. Latest vulnerabilities tracking is more for the later. Crypto-Gram newsletter is a great away for a developer to keep updated with security trends @berkay check out my answer. That might help you to get latest vulnerability data. Its in initial stage but will be improved more as per the user requirements.  Go to security focus you can find all security vulnerabilities and you can sign different kind of mail lists to get the latest bugs etc. i don't think there is better option. The SecurityFocus Vulnerability Database provides security professionals with the most up-to-date information on vulnerabilities for all platforms and services. SecurityFocus Mailing Lists allow members of the security community from around the world to discuss all manner of security issues. There are currently 31 mailing lists; most are moderated to keep posts on-topic and to eliminate spam +1. Was about to recommend the same. Security focus moved their news segment to a new site that is not good. BugTraq is still the same. @The Rook which site? i'm still following securityfocus and no problem still up to date? Security Focus use to have news and columnists and it was awesome now its just bugtraq. The news moved to (http://www.symantec.com/connect/) which i don't like. The CVE aggravate has all of the vulns on bugtraq and a whole lot more.  For the very latest of what happening on the net... The Internet Storm Center http://isc.sans.org/diary.html SANS has the top 25 Programming Errors report http://www.sans.org/top25-programming-errors/  I have a few more i can add to this list. vulnerabilities/threats: For vulnerabilities there by far the best is the CVE RSS: http://nvd.nist.gov/download/nvd-rss-analyzed.xml For emerging threats the Internet Storm Center is the best: http://isc.sans.org/ Bugtraq is 2nd rate and nearly every post is incorrect: http://www.securityfocus.com/archive/1 (they also have rss: http://www.securityfocus.com/rss/vulnerabilities.xml) The ""old"" SecurityFocus.com is now ""Symantec Connect"": http://www.symantec.com/connect/ Security news aggregate: http://www.reddit.com/r/security (freakn' awesome :) http://www.hackinthebox.org (high traffic not that great) http://governmentsecurity.org (Meh some people like it) Blogs: http://hackaday.com (Sweet hardware hacks) http://www.schneier.com/blog/index.xml (of course ;) http://blog.metasploit.com/feeds/posts/default (low traffic but awesome) http://securestate.blogspot.com/feeds/posts/default (low traffic) http://ha.ckers.org/blog/feed/ (web apps low traffic)  http://security.didici.cc/ is all I use nowadays. It's a news aggregator which includes many high quality news sources as well as podcasts videos events CVEs Tweets as well as stackexchange posts. Full disclosure: I'm the author of the site.  I have recently released an App. It might help you Please feel free to utilise it. It's free in AppStore. https://itunes.apple.com/au/app/vulhunter/id742110071?mt=8"
1009,A,C++ networking simple send and receive I'm trying to link 10 computers together the program I would like to write would have one 'control' computer. From what I've looked up this computer would take all the packets sent over the network and do a echo with them... right? The other computers would need to be able to send information (then echoed to the others) to the 'control' ... is there a easy! or simple way to do this? From what I've seen I want a non-blocking socket? I have looked into sockets and such but for an amateur programmer like me this seems like a daunting task :) I'm kind-of looking for an easy class implication that has a send() and an event driven recv(). I'm not going to be sending that much information over the network. Doing it in C++ won't really be easy espeically if you haven't worked with sockets before. Have you considered using .net? It's a lot easier there. http://beej.us/guide/bgnet/ In my opinion the unchallenged best guide to socket programming. so there's no 'easy' way out? :P Using a library like http://qt.nokia.com/doc/4.6/qtcpsocket.html it would be easier. I don't know if you have the freedom to do so or if you want to include a big framework like Qt just to have easy access to sockets. --- I would go for the no to answer your question. :) - my wisdom is limited maybe there is sleek library to do so.  Any communications over a network requires you to have some understanding of networking in general. Even an echo server will have to decide to block in a single thread or provide multiple threads up to a point. What protocol? What is the target space (traditional Internet isolated LAN etc)? W. Richards Stevens wrote good books on the subject (UNIX-based). Beej is another good online resource. If you are really looking for easy why not just use a scripting language such as Ruby or Python to do this? I have never coded in Python... i have used C++ and C# though.... I will look into it though thanks! If you know nothing about networking and nothing about Python you will make better progress learning Python to do the networking.
1010,A,Delphi - How to monitor the network Can someone give me some indications about how to log the web pages accessed(using any web browser)? Should I make a global proxy.... hook the network....? All that I need to log is the page address not the info contained in it. I am using Delphi. Thank you I am looking if is possible for a solution without using Winpcap Edit: I just found Internet Packet Monitoring Components by Magenta Systems. :) I am not feeling guilty accepting it(although I feel some guilt 'cause I gave up easily on searching on Google for the answer). But I can't accept my answer now. When I am trying to accept it a orange message box appears telling me that I can accept my own answer... tomorrow :D cool! If the monitoring components have resolved your question then mention them as an *answer* to this question; don't edit the *question* to add an answer. Even though you asked the question you're allowed to answer it too. Welcome to Stack Overflow. And since your answer seems to be a good one don't feel guilty about accepting it. I found the answer: Internet Packet Monitoring Components(free) by Magenta Systems
1011,A,"Operation now in progress error on connect( function) error I want to set timeout value of function connect but I get this error: ""Operation now in progress"" My code: if ((he = gethostbyname(authdefhost)) == NULL) { snprintf(errbuf CERRBUFSIZ - 1 ""cannot resolve %s: %s\n"" authdefhost hstrerror(h_errno)); return -1; } sin.sin_family = AF_INET; memcpy(&sin.sin_addr he->h_addr_list[0] sizeof(struct in_addr)); if ((sd = socket(AF_INET SOCK_STREAM 0)) < 0) { snprintf(errbuf CERRBUFSIZ - 1 ""cannot create client socket: %s\n"" strerror(errno)); return -1; } if ((fcntl(sd F_SETFL O_NONBLOCK) < 0)) printf(""error on setting socket flags.""); if (connect(sd (void *) & sin sizeof(sin)) == -1) { snprintf(errbuf CERRBUFSIZ - 1 ""cannot connect to server %s: %s\n"" authdefhost strerror(errno)); close(sd); return -1; } FD_ZERO(&fdset); FD_SET(sd &fdset); int rv; if ((rv = select(sd + 1 NULL &fdset NULL &tv)) == -1) { printf(""error occurred on select function.""); return -1; } else if (rv == 0) { printf(""time out occurred.""); return -1; } else { printf(""connection established""); return sd; } When you call connect() on a non-blocking socket you'll get EINPROGRESS instead of blocking waiting for the connection handshake to complete. Then you have to select() for writability and check the socket error to see if the connection has completed. From Linux's connect() manpage: EINPROGRESS The socket is nonblocking and the connection cannot be completed immediately. It is possible to select(2) or poll(2) for completion by selecting the socket for writing. After select(2) indicates writability use getsockopt(2) to read the SO_ERROR option at level SOL_SOCKET to determine whether connect() completed successfully (SO_ERROR is zero) or unsuccessfully (SO_ERROR is one of the usual error codes listed here explaining the reason for the failure). In short: This is expected. You need to specifically check for `errno != EINPROGRESS` in your error-checking path for `connect()`. *Chapter 16 Nonblocking I/O* of **UNIX Network Programming (Volume 1)** describe this feature in detail and present a demo."
1012,A,"how to manage qdisc from kernel I have trouble to find the kernel API to enable some qdisc policies ""tc"" utility run from user-space. I found the modules in net/sched/sch_*.c but I'm not sure how to use them. For instance if I want to enable TBF should I do something like the following in my code?  static struct Qdisc_ops tbf_qdisc_ops __read_mostly = { .next = NULL .cl_ops = &tbf_class_ops .id = ""tbf"" .priv_size = sizeof(struct tbf_sched_data) .enqueue = tbf_enqueue .dequeue = tbf_dequeue .peek = qdisc_peek_dequeued .drop = tbf_drop .init = tbf_init .reset = tbf_reset .destroy = tbf_destroy .change = tbf_change .dump = tbf_dump .owner = THIS_MODULE }; register_qdisc(&tbf_qdisc_ops); Is that enough? How do I attach qdisc to a net device? qdisc modification is exposed only to netlink API. That means that it's designed to be changed from the userspace. However if qdisc is defined using tc its' parameters can be changed within the kernel by calling .change() function."
1013,A,"Client use high port number Why does the client end of a connection use high port number(ephemeral ports) whereas the applications listen on typically small port numbers ? Thx in advans Karthik Balaguru Servers listen on a fixed port number so that clients will know where to connect. Clients do not need to use a fixed port number since no one is initiating a connection to them and in fact they cannot use a fixed port number if there may be more than one client running on the same machine (e.g. a web browser) connecting to the same server. IANA has designated ports in the range 0..49151 as fixed port numbers for specific services and ports in the range 49152..65535 as dynamic (ephemeral) ports which are not assigned to any service and can be used when a fixed port number is not required. The port range 0..49151 is further divided into the well known range 0..1023 which only a privileged process can bind to (at least on Unix/Linux) and the registered range 1024..49151. Ports in the range 1024..49151 can be used by server processes that may run as an unprivlieged user and it is also possible for clients to use ports in this range if they are not being used by a server (e.g. dynamic ports on Linux and Solaris start at 32768 by default rather than 49152).  Because server ports are usually well known ports. On a Unix box you will see their assignment in /etc/services file. The client ports on the other hand are usually picked by TCP/IP stack from the specific high range. So servers know what ports to listen on clients know what port to connect to and nobody cares what port the connection is made from.  Lower port numbers (< 1024) are reserved to privileged processes. In addition many of these ports are assigned to specific services by the Internet Assigned Number Authority. Clients establish connections to servers listening on these well-known ports but use dynamically assigned port numbers in the higher ranges according to the ports that are available to them.  Ports numbers under 1024 are called ""registered"" while those above (limit of 65535) are called ""unregistered"". All these two terms mean is that ports under 1024 have standard services associated with them. IE: 53 for DNS 80 for HTTP 25 for SMTP etc. Note they are associated - there is nothing to stop you from setting your application to use port 53 25 etc but it's not recommended because other services will attempt to connect and or operate on these ports so it could cause problems. The unregistered port region is dynamically used by client applications. IE: You are reading this answer while connecting to port 80 of the StackOverflow.com webserver(s) but your browser is using an unregistered port to initiate that request. Actually IANA calls ports 1024..49151 ""registered"" -- see the link in my answer for details. Also there is something stopping you from using port 53 or 25 if you are not root. You can't use ports below 1024 unless as root or proxy service on *nix systems only. This is not the case on Windows.  Lower port numbers are typically reserved for common applications. Shorter = easier to remember."
1014,A,"How to choose which Network interface to use? I use Qt for my TCP communication. If my PC has 2 network interfaces (say eth0 eth1) and say I want to explicitly use eth1 how do I do that in Qt? QTcpServer::listen takes address of the interface you want to listen as the first argument. If you have IP address 192.168.0.1 on eth0 and 10.0.0.0.1 on eth1 then QTcpServer serv0; QTcpServer serv1; serv0.listen( QHostAddress(""192.168.0.1"") 8080 ); serv1.listen( QHostAddress(""10.0.0.0.1"") 8080 ); serv0 will listen only port 8080 on eth0 and serv1 will listen only port 8080 on eth1. There is no way to specify which interface should QTcpSocket use since it is decided by operation system according to the kernel routing table. You can use QNetworkInterface::allAddresses() to get list of interfaces addresses available.  It doesn't look like Qt's network API supports binding to interfaces for TCP connections. There is a QUdpSocket::bind function but apparently no corresponding function for QTcpSocket objects. If Qt doesn't give you the control you need you may need to drop down to the OS level socket functions instead. Fortunately the Berkeley Socket API is mostly cross-platform with only minor differences between operating systems.  That's not a Qt question that's a socket question. Bind() to the address on eth1. You have to hope that the routing table will actually do what you expect when you do that."
1015,A,bluetooth connection in vc#.net I want to create an Bluetooth chat room for pc i will use visual studio 2005 as the front end i just want to know how do i connect one system to another system with each other and send data or message using an Bluetooth in vc#.net software like intranet chatting use's LAN as connection medium traditional way to connect to one system to another system is using LAN i want to create intranet chatting using Bluetooth as connection medium I wonder if you mean that you will use VS2005 to *create* the front end or if you really mean you want to chat from VS2005 itself. i will use vs2005 to create the front end The best method to do is to connect the two systems via bluetooth using just windows. And then you can add a serial port in the bluetooth manager of each computer and connect your application via Serial Port. http://www.conniq.com/Windows-networking/Bluetooth_PAN_xp-setup_01.htm http://www.acroname.com/garcia/tutorials/bluetooth/connection/connection.html
1016,A,"TCP secured connection - only via my client so I have this TCP connections between my server and client and anyone can connect to my server. But I want to make sure that the client is really using MY client application and not just faking messages from a fake TCP client. What would be the ways to do that check that the connection really is from my game client? Thanks! EDIT If I'm gonna use TLS can I solve that problem? Impossible to do with 100% reliability. If it's worth someone's time to do so they'll bypass any check you could think to do. Whatever solution you end up with just make it be more expensive to do than whatever it is you're using the connection for. Wow that is so right. Thanks!!!! There will probably not be a complete solution to your problem since whatever you do the other party might always take your program run it in a monitored environment manipulate the runtime data and let it use its ""secure"" network protocol. Since the client application is in uncontrollable hands you can never be sure that it is your own program. Baby example: My application runs your application and plays back the data to your server and forwards your response back to the application. How can you tell? That said it might be a very promising ""99%"" approach to use SSL and hardcode the client's private key into the application -- with some trickery you can try and make it hard to find (e.g. see how Skype does it). If you then also build integrity checks into your program that figure out whether anyone is manipulating the memory or debugging into your program you can try and make it a bit harder for a potential adversary. (But note that you will always have to ship the private key with your application so it isn't really safe from discovery.) How does Skype do it? @user: [Here check it out](http://www.blackhat.com/presentations/bh-europe-06/bh-eu-06-biondi/bh-eu-06-biondi-up.pdf) -- I'm sure if you put an extra afternoon in you can rig up something like that for your client ;-) I don't understand. If I'm gonna use TLS is that gonna solve my problem? @Eli: As I said your problem *has* no complete solution. But using TLS (or rather SSL?) allows you to do some PKI-based authentication which is a reasonably good start. Hmmm OK.. But is it right? -> Logging via SSL. After login the client starts sending and getting TCP messages from the server using a key that they agreed in SSL message. You should read up some basics about security and cryptography. Try [Schneier's book](http://www.schneier.com/book-ce.html) for an extremely clean exposition. The crux is the *authentication* part by which you verify that the client app holds a private key that (in principle) only you could have given it. The encryption afterwards is optional you could go for 0-bit encryption if eavesdropping isn't a problem. I will. But in the time I need to start with something. Do you have a better idea that I can use? I think SSL is reasonably straight-forward to use you just have to know what you want. Armed with the understanding of the cryptographic ideas using SSL in your program shouldn't be terribly hard at all. Look I know it sounds awful but I have to release my game. I'm still not sure about the secuirty but I need to start with a good thing. Right now I don't have a time to read a book like that and understand it - my English is not good. So in my situation what do you think would be the best way to try ensuring the client? First off this really shouldn't be a major obstacle since you should never see the effect of this extra layer in most of your code so you can just keep programming the rest of your game. Ask someone who knows about SSL and they can probably write the session wrapper for you in an hour or so. Or post a question on the ""Programmers"" sister site -- or even here on SO if you show us your existing networking code and ask ""how to I wrap this in an authenticated SSL connection"" or something. Big thanks. :).  Others have suggested useful answers to your question but I'm going to suggest another approach. Re-examine your requirements. Ask yourself why you want to know the identity of the client program. Is it so that you can trust your client program more than you trust 3rd-party client programs? If you need to trust the identity or integrity of software that you have already shipped to your customers I claim your security model is broken. Once the software runs on a client's PC you should assume it is evil even if you originally wrote it. Any status any command any data whatsoever that comes from the network must be checked before it is relied upon. How can I do that? The client is an iOS device. I thought that checking the client would be the best way since the only thing can happen is me to lose money - if the player could break into my system he can add himself items that costs money and I don't want that to happen. Maybe 1 or 2 is OK but if they will share the crack into the internet I will lose a lot of money.  You could use a public/private key pair in order to verify that you are who you say you are. http://en.wikipedia.org/wiki/RSA#Signing_messages And to authenticate on the server the client needs to sign the message using it's private key. The problem of storing the private key in the way that the user of the application is not able to access it is not solved on general-purpose systems. Can't I store it in the code..? iOS application is the client - I don't think it's really possible to access the code . . It's not possible to access your source code but it is certainly possible to access a key contained in the client. iOS devices can be jail broken and that means that anything you hard code in your app can be read by a third party. Hard code - what is that means? You wanna tell me that a jailbroken user can access all of my codes? It means that you have to embed the key in your program. When someone downloads your program they are getting a copy of the key and if they know what they are doing they can extract the key and use it make an app which can emulate yours. See Kerrek's answer. There is no way that I am aware of to do this 100% securely. OK so I want to make it as hard as possible. What do you suggest? Here's what I thought -> Logging via SSL. After login the client starts sending and getting TCP messages from the server using a key that they agreed in SSL message.  My default response is to use a challenge/response authentication. After connection send a random number from the server to the client The client then computes using a hash/key/.... a response message and returns that to the server If the response matches the servers computation your chances of authenticity are better. Note though that a reverse engineer of your client will leave this method open to fraud. Also note that this is subject to man-in-the-middle attacks. The attacker lets that part of the protocol run the completion and then starts injecting his own commands into the now-trusted connection. Absolutely. MitM is one of many potential pitfalls. If the OP wants SECURITY as opposed to security I feel his studies will be far deeper than this query."
1017,A,"On Linux: how can I programmatically determine if a NIC interface is enabled and plugged in? I want to determine if a network card is enabled up and plugged in. Basically I want to know if the network card will work. I need this information from with a C++ program and would like to display an error message when the network isn't working properly. If possible I would like to avoid using shell commands to determine this information. -- Dylan How do you want to identify the network card? You might try taking a look at /etc/udev/rules.d/70-persistent-net.rules which maps hardware MAC addresses into nice names (like eth0). Then when you have the nicer name you can run things like ethtool eth0 to determine if it is [physically] connected (last line) ifconfig eth0 to determine if it is up (look for ""UP BROADCAST..."") and if it has an IP address. I'm willing to guess there are automatic libraries for this though; have you looked around? I'm not sure if there's easily accessible code in NetworkManager but that should be a good first place to look.  Run through the output of getifaddrs you can use the link layer for the MAC address to identify an adapter and check the ifa_flags for IFF_UP. Use AF_NETLINK for notifications about interface changes.  You can look at /sys/class/net/eth0/operstate where eth0 is your interface to see if it's up. Look at /sys/class/net/eth0/carrier to see if there is a carrier. Though I guess executing ifconfig and friends will give you more compatibility to *BSDs. Is there something similarly elegant for older kernels? On my old CentOS 4 box with kernel 2.6.9 I have no ""operstate"" file under /sys/class/net/eth?. Do you know how to check the link status? Is there a file for that? As in when you do `ethtool eth0` output will show you link detected: YES  open AF_NETLINK socket bind it to sockaddr_nl with nl_groups = RTMGRP_LINK send message RTM_GETLINK to kernel make poll/epoll on socket to read RTM_NEWLINK and RTM_DELLINK messages you will receive initial interfaces list and its changes in future  Remember on Linux ""everything"" is a file. The best way would be to use the approved kernel<->userspace communication namely sysfs mounted at /sys. Network devices are linked at /sys/class/net If you wish to use the ioctl interface look at man netdevice"
1018,A,"wget/curl in C# I'm writing a scraper in C# and I'd like to download some data to files and submit some forms. I've been using wget and curl so far for that. How would I do that in C# (on Linux)? (I mean a library for that not calling shell commands via system() or whatnot). WebRequest is one of the .NET classes for retrieving web content. A good library to parse the HTML is the HTML Agility Pack that can also directly download the page.  You can use System.Net.WebClient which is the simplest interface for downloading resources in .NET. If you need more control on the requests look at HttpWebRequest. For WebClient just instantiate an instance and call one of the Download methods which fits your needs: var cli = new WebClient(); string data = cli.DownloadString(""http://www.stackoverflow.com"");"
1019,A,"Functions to manipulate IPv4 addresses in C#? Given a IPv4 address in the form of a string (ex. ""10.171.24.69"") and a netmask (ex. ""255.255.255.128"" or ""25"" for number of bits in network part) I need to compute the broadcast address which may be by either zeroing or one-ing the bits in the host part (depending on the IPUseZeroBroadcast property which I can query via WMI). I'm examining the System.Net.IPAddress class but it looks insufficient to the task. Any suggestions? I don't know of any built in functions but you could calculate it yourself easily enough static IPAddress ComputeBroadcastIP(IPAddress ip IPAddress netmask) { byte[] ipBytes = ip.GetAddressBytes(); byte[] maskBytes = netmask.GetAddressBytes(); byte[] broadcastBytes = new byte[ipBytes.Length]; for (int i = 0; i < broadcastBytes.Length; i++) { broadcastBytes[i] = (byte)(ipBytes[i] | ~maskBytes[i]); } return new IPAddress(broadcastBytes); } You could also do IPAddress broadcast = new IPAddress((ip.Address | (~mask.Address)) & 0xffffffff); but the .Address property is deprecated. Note that to convert a string like ""1.2.3.4"" to an IPAddress you can call IPAddress.Parse e.g. IPAddress foo = IPAddress.Parse(""1.2.3.4"");"
1020,A,"Vb.net - Display each network adapter enabled or disabled I'm trying to display the network adapters on the computer running this application. For that I want to be able to display all network adapters no matter which status they have. Then I want to be able to enable and disable the adapters by pushing a button. I've tried using System.Net.NetworkInformation but it seems as though it only contains the one that's active. ' NETWORK ADAPTERS ' Create label Dim LabelNetworkAdapter As New Label Dim old As Padding = LabelNetworkAdapter.Margin LabelNetworkAdapter.Margin = New Padding(old.Left 8 old.Right old.Bottom) ' CreateButton Dim BtnConnectButton As New Button BtnConnectButton.Height = 23 BtnConnectButton.Width = 60 For Each nic As NetworkInterface In NetworkInterface.GetAllNetworkInterfaces() Dim strText As String = nic.Description.ToString TableLayoutPanel_Nettverkskort.Controls.Add(LabelNetworkAdapter) LabelNetworkAdapter.Text = strText TableLayoutPanel_Nettverkskort.Controls.Add(BtnConnectButton) BtnConnectButton.Text = ""Koble fra"" Exit For Next That's what disabling an interface means apps can't see it anymore. Use WMI Code Creator to play with WMI queries. What you're trying to do is not possible. When you disable a network interface the operating system hides it from applications. Which makes sense since you can't actually use it. The only way you're going to see it is in the shell's ""My Network Places"" folder (or whatever they've decided to call it now). This bug report on MSDN seems to aptly describe your problem. It was closed as ""by design"". The documentation even explains that the return value is: A NetworkInterface array that contains objects that describe the available network interfaces or an empty array if no interfaces are detected."
1021,A,"C# network programming -- listening I'm writing a client-server utility application. They communicate through TcpClient/Stream (client) and TcpListener/Socket (server). I send bytes back and forth between these two then I come to the part where I have a problem; the client sends a signal to the server (say StartCounting). The client starts doing some stuff (say counts up from zero). The server does the same as soon as it receives the StartCounting. Now while both sides are counting each side may interrupt the counting (say by pressing return). When one side stops it sends a message to the other side to stop. Here's some attempted code (for the client but the only difference is how they read the byte array): private void WaitForBye(object source ElapsedEventArgs e) { byte[] responseBytes = new byte[1000]; int k = stream.Read(responseBytes 0 1000); String response = ASCIIEncoding.ASCII.GetString(responseBytes); response = response.Substring(0 k); String from to message; extractData(response out message out from out to); Console.WriteLine(++counter); if (message == ""BYE"") isBye = true; } bTimer = new Timer(); bTimer.Elapsed += new ElapsedEventHandler(WaitForBye); bTimer.Interval = 1000; bTimer.Enabled = true; Console.WriteLine(""Press return when finished.""); while (Console.ReadLine() != """" && !isBye) ; How would I do that? Preferably without using threads and with a Timer instead. Don't think you could be counting and listening at the same time on the same thread so my guess you'll have to look into threading for this one. C# 4.0 makes this kind of stuff a lot easier with Tasks. A bit off topic but why are you using sockets? Wouldn't WCF work jus fine here? It would save you a truckload of typing. Good luck GJ @offtopic: This is my first networking program and I want to work my way up from the lowest level. Ok going this route (bottom up) will provide with a lot of low level skills and understanding that's for sure. It's not that scary if you want to understand it. WCF would enable you to get stuff into production a lot quicker and shield you from beginnner pitfalls. It does add some overhead that is true. But there are other communication frameworks that are better at that like MassTransit or Protocol Buffers.  You could do it with stream.peek to check if there is data to be read. Otherwise your code would wait until there is data and the counting would not work. Remember that the message you read may not be complete so don't use this approach for mission critical situations. Using WCF would save typing but would add to the payload of the communications. Succes. The `Stream` doesn't support a `Peek()` method. And what about the `Socket`?"
1022,A,Identify user and machine on the local network In my company we use small application called IPMsg a messenger kind of tool to pass messages and file to other fellows in company even it allows to multicast the message. And also it lists the user name host name and IP addresses of users. How can it do that? There is no server present for message routing and when checked through netstat command in CMD it does not show any details like what protocol and port it is using to communicate. There is source code also available on the same site which is in VC++. I didn't understand a line of code... (I'm a C# guy) Can anyone explain me how it can do that? IPMsg is a daemon which listens to incoming connections on a specific port which is the connection port. You can find out which port it used by using Wireshark. Start wireshark start listening on the interface where you have connected to LAN and then start sending any message wireshark will show you the message on the screen with the port number also. The application is a peer-to-peer software and doesn't require a central server software to route messages. it only has a small daemon which accepts incoming connections. This is the way Jabber Instant messaging protocol also works. As you said it lists username hostname and ip address of users do you mean it pings the network and finds it? If yes then it is actually possible to find the IP addresses of computers on the Local Network which requires you to know the subnet on which you are connected. You can use ARP/ICMP Ping to know the hosts present on your network provided you enter the correct subnet information Multicasting a message is also nothing special. It is a feature provided with all Networking Stacks. If you want mutlicasting in .NET it is allowed. Check this page on Code Project which gives a nice example  IPMsg probably multicasts a request for all clients to report their user and host details. A similar mechanism is used when Windows Explorer attempts to find other machines on a network. A good description of this type of multicasting discovery is described here.  One simple way would be to let the application listen on a certain network port and when you start your instance of it it tries to connect to that port on each computer on the same network. If that other computer has that port open and answers correctly then you have found another instance of the application. In the case of an instant messenger that doesn't have a server it *has* tolisten on a port to accept the incoming messages
1023,A,"Behavior of shutdown(sock SHUT_RD) with TCP When using a TCP socket what does shutdown(sock SHUT_RD); actually do? Does it just make all recv() calls return an error code? If so which error code? Does it cause any packets to be sent by the underlying TCP connection? What happens to any data that the other side sends at this point - is it kept and the window size of the connection keeps shrinking until it gets to 0 or is it just discarded and the window size doesn't shrink? shutdown(sock SHUT_RD) causes any writer to the socket to receive a sigpipe signal. Any further reads using the read system call will return a -1 and set errno to EINVAL. The use of recv will return a -1 and set errno to indicate the error (probably ENOTCONN or ENOTSOCK). why would SHUT_RD affect writers? I don't see any obvious reason for it to do so (though I suppose I always use MSG_NOSIGNAL so that may have an impact). Because (quoting from Wikipedia) ""On POSIX-compliant platforms SIGPIPE is the signal sent to a process when it attempts to write to a pipe without a process connected to the other end."" And the shutdown(sock SHUT_RD) statement disconnects the reader process from the pipe. To add injury to insult the default action upon receipt of a SIGPIPE signal is for the process (i.e the writer) to terminate. This is not correct. Any further reads will cause recv() to rerun zero indicating end of stream.  I test shudown(sockSHUT_RD) on Ubuntu 12.04. I find that when you call shutdown(sockSHUT_RD) if there are no any type of data(include FIN....) in the TCP buffer the successive read call will return 0(indicates end of stream). But if there are some data which arrived before or after shutdown function read call will process normally as if shutdown function was not called. It seems that shutdown(sockSHUT_RD) doesn't cause any TCP states changed to the socket  It has two effects one of them platform-dependent. recv() will return zero indicating end of stream. Any further writes to the connection by the peer will either be (a) silently thrown away by the receiver (BSD) (b) be buffered by the receiver and eventually cause send() to block or return -1/EAGAIN/EWOULDBLOCK (Linux) or (c) cause the receiver to send an RST (Windows).  Shutting down the read side of a socket will cause any blocked recv (or similar) calls to return 0 (indicating graceful shutdown). I don't know what will happen to data currently traveling up the IP stack. It will most certainly ignore data that is in-flight from the other side. It will not affect writes to that socket at all. In fact judicious use of shutdown is a good way to ensure that you clean up as soon as you're done. An HTTP client that doesn't use keepalive can shutdown the write-side as soon as it is done sending the request and a server that sees Connection: closed can likewise shutdown the read-side as soon as it is done receiving the request. This will cause any further erroneous activity to be immediately obvious which is very useful when writing protocol-level code.  shutdown(SHUT_RD) does not have any counterpart in TCP protocol so it is pretty much up to implementation how to behave when someone writes to a connection where other side indicated that it will not read or when you try to read after you declared that you wont. On slightly lower level it is beneficial to remember that TCP connection is a pair of flows using which peers send data until they declare that they are done (by SHUT_WR which sends FIN). And these two flows are quite independent.  Looking at the Linux source code shutdown(sock SHUT_RD) doesn't seem to cause any state changes to the socket. (Obviously shutdown(sock SHUT_WR) causes FIN to be set.) I can't comment on the window size changes (or lack thereof). But you can write a test program to see. Just make your inetd run a chargen service and connect to it. :-)"
1024,A,"Why does UnicastIPAddressInformation.IPv4Mask return null on an IPv4 Address? The following is a section of code which builds a list of IP addresses and their subnet masks from the local system however the Warn function seems to get triggered regularly which should in theory be impossible - as it should not be possible to have an IPv4 address without the associated subnet mask[?].  static NetworkUtil() { foreach (NetworkInterface ni in NetworkInterface.GetAllNetworkInterfaces()) { foreach (UnicastIPAddressInformation address in ni.GetIPProperties().UnicastAddresses) { if (address.Address.AddressFamily == AddressFamily.InterNetwork) { if (address.IPv4Mask != null) { m_subnets.Add(address.Address address.IPv4Mask); } else { m_log.Warn(""[NetworkUtil] Found IPv4 Address without Subnet Mask!?""); } } } } } That is rather odd; obviously the semantics of this are not quite what one could reasonably expect. Can you check to see if the subnet masks of the interfaces in question are classful? I.e. for those interface addresses where IPv4Mask is null are they class A B and C addresses with a network mask of /8 /16 and /24 respectively? Perhaps this function is not really a member of the CIDR world and they really do only ""subnet"" masks; an address without one is assumed to use the classful network mask.  It is because you have come across a loop back adapter. ie 127.0.0.1 These have the IPv4Mask set to null. Refer to ni.Name to see if I am right. To check for a loopback adapter do... if (ni.NetworkInterfaceType != NetworkInterfaceType.Loopback) ..."
1025,A,"How to request a URL that requires a client certificate for authentication I need to request a URL from a server that uses client certificates for authentication and can't find a way to do this for my application. My problem is that the Java client I'm working on has the certificate file available locally but due to restrictions on the PCs it will be running on it cannot install the certificate in a keystore. In short I just want to be able to explicilty specify the certificate to use for the URL I need to retrieve. Any suggestions? Do you mean you are restricted against installing the cert into a java keystore or an OS keystore? I meant that I can't create a keystore on the clients PC that I can import the key from - I can only access it as a .cer file on the file system. You can change the default JSSE keystore through a property when you invoke your app -Djavax.net.ssl.keystore=somefile. So you could import your cert with the keytool command into a keystore and invoke your app pointing to that e.g. keytool -importcert -file mycert -keystore mystore java -Djavax.net.ssl.keystore=mystore ... Importing the cert on its own will not import the private key: you need that if you want to use the cert for client-certificate authentication. Thanks but I can't use a keystore on the file system.  It's not clear what the restrictions you're talking about are. More specifically I'm not sure what you consider the difference between the local certificate file and a keystore. Most keystores are file-based so you might be able to load the file this way directly without needing an installation process. Are the restrictions related to the security policies used by the JVM itself (which may prevent you from instantiating KeyStores)? First it's not just the certificate you need on the client side but its private key. Often people use the word ""certificate"" in this context to mean both but you do need to make sure your file doesn't contain the certificate without the private key. Typically you'll find the combination of private key + certificate in a PKCS#12 file (.p12/.pfx) a lot of tools import/export in this format; it's also a keystore format natively supported by the Sun JVM (type PKCS12). To make this work you need to configure what makes the connection with the appropriate keystore. SSL/TLS client-certificate authentication is always initiated by the server: the client responds with a certificate if it has one (and wants to use it). To configure it for a specific URL you need to find out what makes the connection (perhaps HttpsURLConnection) and set it there (unless it's set up in the default context -- even if it's set up in the default context it will only be used for servers that request it). To set up the keystore globally on the JVM (which may be what your restrictions prevent you to do) you can set the javax.net.ssl.keyStore javax.net.ssl.keyStorePassword (and related) system properties. (Because the password could be visible it's better not to do it on the command line). These system properties are used for the configuration of the default SSLContext (which is used often transparently by libraries or classes such as HttpsURLConnection to build the SSLSocketFactory and then SSLSocket initialized with those properties). You could build SSLContext from your file specifically for use for that connection. The SSLContext is effectively a factory for the SSLSocketFactory or SSLEngine and you can set the SSLSocketFactory in a given HttpsURLConnection. The following would build an SSLContext using ""/path/to/file.p12"" as your keystore (that is the one with your private key and the certificate you're going to send) and keep the default settings for the truststore (you'd need to catch the exception for the input stream too). KeyStore ks = KeyStore.getInstance(""PKCS12""); FileInputStream fis = new FileInputStream(""/path/to/file.p12""); ks.load(fis ""password"".toCharArray()); KeyManagerFactory kmf = KeyManagerFactory.getInstance(""SunX509""); kmf.init(ks ""password"".toCharArray()); SSLContext sc = SSLContext.getInstance(""TLS""); sc.init(kmf.getKeyManagers() null null); From there you can configure the connection like this (if this is what you're using): HttpURLConnection connection = (HttpURLConnection) url.openConnection(); if (connection instanceof HttpsURLConnection) { ((HttpsURLConnection)connection) .setSSLSocketFactory(sc.getSSLSocketFactory()); } Some libraries will let you pass an SSLContext directly (Apache HTTP Client 4 supports that and this can be done with Apache HTTP Client 3 using this.) Note that you don't need to provide the password as a direct parameter when loading the keystore you could also use a callback (maybe better from the GUI point of view). Perhaps this library could help (but it's not necessary): you could use the KeystoreLoader for its helpers to do this. There are also SSLContextFactories in this libraries (but you would probably not need any of the wrappers as they tend to be for customizing the trust management or key selection). This is generally how using a client-certificate is configured but it's difficult to provide more details without clarifications regarding what your restrictions exactly are (and which libraries you're using). Thanks for the comprehensive repose Bruno. I have a .cer file to authenticate the client with. From what I can gather this is just the public key right? If so then your answer will mean that this is not going to work. However I've imported this into a browser and can access the Tomcat server using that. Any thoughts? Usually .cer files (ultimately the extension is just an indication) only contain the certificate which contains the public key) but not the private key and you need to have the private key to use client-certificate authentication (otherwise anyone could send the certificate and that wouldn't be authentication). It's possible that you Tomcat server uses optional client-cert authentication in which case it would let you in even without a cert. Where did you import this file into your browser? How was this client-certificate issued to you? What OS are you using? The client cert was issued by the company I work for and they have signed it. When I access a Web app within the organization I'm automatically logged in. The login on the Tomcat server I'm using required CLIENT-CERT authentication. I should add that the realm used for the authentication in Tomcat is a custom one. It's possible that the realm just looks for a valid username in the certificate and allows access. OK for having your cert issued by your company but where was the private key generated? In the browser? On a smart card? Regarding Tomcat is the connector (in server.xml) configured with clientAuth=""want"" or clientAuth=""need""? I guess the private key was created by the browsers when they downloaded the certificates. I hadn't realised that these were required until you mentioned them. In Tomcat clientAuty='false' and I can't change this setting. OK so you have two potentially big problems here unfortunately. (1) if you've lost your private key you're probably going to have to re-emit a certificate (it might be worth asking assistance within your company). (2) With clientAuth=""false"" this implies a re-negotiation per webapp (otherwise the client-cert is requested initially). The problem with this is that renegotiation has been disabled (J2SE 1.6_19 I think) following a security issue and the fix (RFC 5746) doesn't seem to be in 1.6_21. There is an unsafe workaround: http://java.sun.com/javase/javaseforbusiness/docs/TLSReadme.html Thanks so much for your advice. I think I'll try another approach to this problem then."
1026,A,Developing client app for proprietary server I was thinking about developing an app that enables the user to remotely check the progress of a longrunning task. The server application running the task is an existing commercial tool and comes with a proprietary client to connect to the server to manage it. However the client is available only for windows computers and not for mobile devices hence my desire to fill the gap. The communication between client and server is neither encrypted nor password protected in any way. What would be the best way to analyze or reverse engineer such a proprietary protocol? Are there any legal implications (I know this is not the place to ask legal stuff but if you happen to know how to reverse engineer stuff you maybe know whether it is legal or not too)? I'm a fan of http://www.wireshark.org/ for protocol analysis. Free powerful extensible cross-platform. As regards legal stuff: It depends on jurisdiction - and each country's courts seem to enjoy not coming up with consistent precedents. The general rule is reverse-engineering is okay for 'interoperability'. You'd really have to ask a lawyer for more info though. Personally if something is running on my machine and I want it to behave in a different way I have no ethical issues forcing it to. That's just me though. I can fully imagine a virus writer ringing me up and making some kind of legal threats that I breached his EULA... Hi Rushyo thanks for pointing me towards wireshark. I installed it and did a few quick tests. It appears there's a lot I have to learn ;-) Can you recommend any wireshark tutorials that deal with the kind of thing I'd like to do to help me get into the matter? Thanks alot! I can't really recommend any unfortunately as I was entirely self-taught. It does assume you are familiar with protocol you are sniffing. If you don't know that then you need to read the spec first! Wireshark themselves have a business model based around training if you want to take advantage of that: http://www.wireshark.org/docs/ Otherwise there's tons of tutorials on Google.
1027,A,"How to make readInt() block on input byte stream? I have an input stream coming form a blackbox (say B). All the messages coming in from this stream are serialized binary data and each message starts with a four byte int. Most of it is logging data and runs 24 hrs a day. I read these four bytes using readInt() method. Now ocasionally the main thread would exit with EOFException and crash the program. After researching on this I found that it happens when there are less than four bytes in the input stream at the time of readInt(). My guess is that the buffer is not filling in fast enough between successive reads. Some of the possible solutions I am thinking of include checking available() before reading (consumes too many cycles considering the amt of data) or restart when exception occurs (sounds like poor programing). If only I could block using readInt() it would be the best way I think. I've looked at implementation of readInt() but again it boils down to blocking with read(). Anyone knows of a better solution? You seem to be blocking the reading-flow by producing a wall-of-text ;-) Introducing some paragraphs might make this more readable (and will help get answers). `readInt()` method of what? What kind of stream are you reading from? You have a point there Joachim. I'll be more careful. I am using DataInputStream as Stas guessed.Having an embedded device on the other end makes it difficult to change stuff there. My guess is that the blackbox may be sending intermitten chunks of bytes not caring wether the message is sen in entirity. The stream may go dry at times without having complete contents. Any blocking call down the call hierarchy is ""bound"" to make all the calls up the chain blocking as along as both calls are part of same thread of execution. The readInt method of DataInputStream makes four calls to the read method of the underlying input stream which will surely block as long as the data is not made available hence your fear of ""buffer doesn't fill in fast enough"" doesn't seem to be logical. I have encountered these kind of exceptions in cases where either the server process dies or or drops the connection in which case the client ends up reading a -1 and throws an exception. Are you gobbling any sort of exceptions in your client/server code? Do your logs show anything suspicious? You are right about the blocking. The server in my case is an embedded device that might sent intermitten chunks of data. When things aren't available on the stream read() returns -1 and readInt() returns EOFException. This is expected behavior. Is there a way to circumvent this? It can be assumed that the data will never stop flowing and I am not worried about closing the stream. So for practical purpose it can be assumed that the connection is never closed on the server side and an actual end of stream is never reached. Like already mentioned there is no ""when things aren't available on the stream"" since the client would ""wait"" for that as long as the server doesn't decide to return -1. Since this data from the server is stream and flows in chunks have you made sure that you have an appropriate timeout set for your client/server socket?  The basic InputStream interface requires blocking reads the EOFException you get is thrown when the readInt() encounters the end of stream marker since returning the incomplete int would be a bad idea it throws the End Of File Exception. The EOFException is thrown because the other end of the stream reached its endhas been closed or is no longer connected. You should check if the blackbox terminates the connection. Since the stream is network based your socket may have a timeout set if this is the case try changing the SOTimeout value of the socket. The black box is an embedded device and may send intermittent chunks of data. This may cause only <4 bytes at the other end and not send anyhing for a while. I am looking to block till all 4 bytes are received. SOTimeout is a good idea. I'll look into it right now and see if that helps.  I believe that you are using DataInputStream. That class throws EOFException in situation when the stream which it wraps returns -1 from the read() method (which actually blocks until input data is available). I suppose you should have a look why the main stream's read returns with -1 in the first place."
1028,A,"How vulnerable is a persistent unencrypted Internet connection between 2 trusted networks? Scenario: 2 network devices each on separate private LANs. The LANs are connected by public Internet. Device on network A is listening on a socket; network A firewall has NAT port forward setup for external access from network B's port range. Device on network B makes outgoing connection to network A to connect to the listen socket. Is there any difference in vulnerability of short-term connection made for a data transfer then dropped when complete (e.g. few seconds) and a persistent connection which employs a keep-alive mechanism and reconnects when dropped (hours days ..)? The security of actually making the connection is not part of my question. Why aren't you using a VPN and SSL? I have suggested that as an alternative but have been asked to respond to the question of security of this mechanism as well. If all they have is FUD why respond in detail? Just pitch the alternative and move on? Because I'm not certain enough of my own knowledge of network infrastructure to be sure of my conclusion that it is just FUD. @boycy: It doesn't matter. You have a better solution which is to use a VPN. Don't address the bad solution with a detailed explanation of why it's bad. Just use the standard good solution -- VPN with SSL. It matters indeed -- alternative solution aside I'm interested in any real-world practical vulnerability of a long-term open connection to bolster my own knowledge. the client will maintain a persistent connection to server No such thing exists. Each connection -- no matter how long it's supposed to last -- will eventually get disconnected. It may be seconds before the disconnect or centuries but it will eventually get disconnected. Nothing is ""persistent"" in the sense of perpetually on. There is no such thing as a ""keep-alive mechanism"". It will get disconnected. ""Assume the server authenticates the client upon connection"". Can't assume that. That's the vulnerability. Unless you have a secure socket layer (SSL) to assure that the TCP/IP traffic itself is secure. If you're going to use SSL why mess around with ""keep-alive""? When it gets disconnected how does it get connected again? And how do you trust the connection? Scenario One: Denial of Service. Bad Guys are probing your socket waiting for it to accept a connection. Your ""persistent"" connection goes down. (Either client crashed or you crashed or network routing infrastructure crashed. Doesn't matter. Socket dead. Must reconnect.) Bad Guys get your listening socket first. They spoof their IP address and you think they're the client. They're in -- masquerading as the client. The client host attempts their connection and you reject it saying they're already connected. Indeed this is the exact reason why folks invented and use SSL. Based on this you can dream up a DNS-enabled scenario that will allow Bad Guys to (a) get connected and then (b) adjust a DNS entry to make them receive connections intended for you. Now they're in the middle. Ideally DNS security foils this but it depends on the client's configuration. They could be open to DNS hacks who knows? The point is this. Don't Count On A Persistent Connection It doesn't exist. Everything gets disconnected and reconnected. That's why we have SSL. The client can simply reconnect the server must respond to the user request with the appropriate error. False. The client cannot ""simply"" reconnect. Anyone can connect. Indeed you have to assume ""everyone"" is trying to connect and will beat the approved client. To be sure it's the approved client you have to exchange credentials. Essentially implementing SSL. Don't implement your own SSL. Use existing SSL. would they have to break into a switch site? Only in the movies. In the real world we use packet sniffers. Not sure I understand; the distinction is important surely? If you connect out to a hub you will receive packets not destined to/from your public IP. If it's a switch then you will only receive packets to/from your IP. SATAN seems to be a port scanner with greatly extended functionality which would allow you to scan someone but not to receive packets from a switch which aren't destined for your network. No my original analogy was actually closer to what I was asking than yours. But thanks. @boycy: ""the distinction is important"". Not really. The script kiddies can't all be buying proper routers from telcos. (I suppose they **all** could be buying routers but I doubt it.) This leads me to conclude that at least one cable provider allows script kiddies an unreasonable level of access. Additionally there are any number of tunneling protocols as well as unsecured routers. The ""hub/switch"" business doesn't have any **practical** significance since hacking goes on irrespective of that distinction. Sorry I should've clarified that -- I was using 'persistent' with an implicit caveat of 'within the bounds of possibility'. I will create (a) a keep-alive mechanism for the socket (empty TCP payload every 5 mins perhaps) and (b) a handling mechanism for the 'no active connection' eventuality. The client can simply reconnect the server must respond to the user request with the appropriate error. I'll add some clarification as I think my question's not quite pointing in the right direction. Thanks for the responses so far :-) Ok that provides an alternative (for which I am grateful - and your good justification of it as a better solution will strengthen the case for it) but for my own education I'd still be interested in the answer to the original question - practically is anyone /likely/ to be able to eavesdrop on Internet traffic outside a LAN environment? @boycy: Yes. (1) When your connection is dropped and reconnected it can be spoofed. (2) All unencrypted data is completely visible in the TCP/IP packet stream -- every router sees all of your data in the clear. @boycy: The question (above) says ""spoof"". Your comment says ""listen"". Which is it? By 'eavesdrop' I meant the attacker would be able to see and hence manipulate data passing through a network element. I guess my question is about the physical security of Internet network elements. I wanted to ignore the security of the connecting itself (because I feel comfortable with my knowledge there) and look at the security of a longterm connection compared to an on-demand connection. Sorry I'm not trying to be a pain here though I think I've achieved it. @boycy: There is **no** physical security on The Internet (in the large). Everything is readable (and indeed actually read) by every router on the net. I'll repeat that since it seems to confuse you. Everything is readable (and indeed actually read) by every router on the net. What's your question? Please **update** the question so people don't have to integrate a bunch of comments to figure out what you're talking about. I am not confused about the concept that every router on the net can read the data and am somewhat offended by the patronising repetition. I do know that a router can read the packets and am not completely new to network programming. I'm (a) sorry that this question is frustrating you and (b) very grateful for your time. I'm obviously finding it difficult to phrase it in a way which elicits the information I'm asking for. Despite rephrasing the question in the last comment I will do so above as you request. @boycy: There is no such thing as a ""long term"" connection. Why do you keep trying to focus on that? So you wouldn't define a network connection which is maintained open and reestablished if dropped longterm? @boycy: It's ""on demand"". The demand is -- theoretically -- less frequent but there's no ""persistent"". All you have is on-demand frequently (i.e. HTTP) and on-demand rarely (your ""keep-alive"" thing). It's still TCP/IP and it's still on demand. I know it's not a technical term but I'm trying to contrast between a connection open for a short amount of time and one open for a long period of time. Regarding the packet sniffing mechanism where can you sniff the packets if you don't break into a switch site? @boycy: There's no distinction between short and long duration. They're both ""on-demand"" with the exact same vulnerabilities. The Same. I would sniff packets through my Cable TV provider because I'm too cheap to actually contract with a Telco for a proper T1 or DS1 connection. So you're saying that if you have a cable connection then the immediate network element outside your house is a hub not a switch? And thanks I guess ""The Same"" answers my question though I feel far more stupid than when I asked it.. And as far as the distinction goes I know the protocol doesn't give two hoots about it but let me explain why I thought it might be important by way of analogy. Compare the difference in home security between someone who only unlocks their door when they want to go through it and someone who leaves their door unlocked all the time. Not exactly a watertight analogy but it may help you understand why I asked the question ""is a hub not a switch"" too subtle a distinction for me. Download `satan` or other Black-Hat network analysis/hacking software. The Internet itself is very wide open. The individual sites (behind firewalls) are not so open. But sniffing unencrypted traffic is done by kids with open source software. They're called ""http://en.wikipedia.org/wiki/Script_kiddie"" and the net's full of 'em. @boycy: ""Not exactly a watertight analogy"". Not even close. The analogy is someone who opens their door frequently vs. rarely. Either way **the door must be opened**. You can try to do it less often but that does not change the essential vulnerability. It only changes the frequency. `Each connection -- no matter how long it's supposed to last -- will eventually get disconnected. It may be seconds before the disconnect or centuries but it will eventually get disconnected. Nothing is ""persistent"" in the sense of perpetually on.` What nonsense. ""Persistent"" is not the same as ""permanent"" and this is precisely why. `There is no such thing as a ""keep-alive mechanism"". It will get disconnected.` Similarly a life support machine is still a life support machine even if you don't keep it plugged in for infinity years."
1029,A,"read xml data over tcp i'm developing an application that is listening to tcp to get some xml data coming from other devices. i'm use sniffing c# code and i can sniff all the packets. my problem is that in every packet i can find a Piece of the data in every packet. like this: 1 packet from ip41 data:< 2 packet from ip41 data:?xml versi 3 packet from ip41 data:on=""1.0"" 1 packet from ip35 data:< ?xml 4 packet from ip41 data:encoding=""UTF-8 the real data looks like this: <?xml version=""1.0"" encoding=""UTF-8""?><alarm><datetime>2010-07-18T11:14:22Z</datetime><textch><textchid>020</textchid></textch><rule>DIR-020</rule><text>020-DIR-Intersection3_Magles_TCS6</text></alarm> i want to be able to get the data in a string like the real data not in pieces. is there is a method or a library in .net that can do that? possible duplicate of [read sniffing data over tcp](http://stackoverflow.com/questions/3345171/read-sniffing-data-over-tcp) i did it by monitoring one port by thread..and Assembly them by sequence number. thanks for all your help  Are you sniffing or do you just want to connect to the device and grab the data? If the latter you can use the TcpClient class to do what you need. using System.Net.Sockets; TcpClient tcp = new TcpClient(AddressFamily.InterNetwork); tcp.Connect(IPAddress.Parse(""192.168.0.1"") 12345); And then tcp.GetStream() will get you something you can feed into your XML parser of choice. Edit: Here's a slightly more detailed sample. using System; using System.Collections.Generic; using System.Text; using System.Net; using System.IO; using System.Net.Sockets; using System.Threading; namespace ConsoleApplication1 { class XMLBlaster { Thread myThread; public XMLBlaster() { myThread = new Thread(Start); } public void Begin() { myThread.Start(); } //This will listen for a connection on port 12345 and send a tiny XML document //to the first person to connect. protected void Start() { TcpListener tcp = new TcpListener(IPAddress.Any 12345); tcp.Start(1); TcpClient client = tcp.AcceptTcpClient(); StreamWriter data = new StreamWriter(client.GetStream()); data.Write(""<myxmldocument><node1><node2></node2></node1></myxmldocument>""); data.Close(); client.Close(); } } class Program { static void Main(string[] args) { //this sets up the server we will be reading XMLBlaster server = new XMLBlaster(); server.Begin(); //this is the important bit //First create the client TcpClient tcp = new TcpClient(AddressFamily.InterNetwork); //Next connect to the server. You probably will want to use the prober settings here tcp.Connect(IPAddress.Loopback 12345); //Since byte manipulation is ugly let's turn it into strings StreamReader data_in = new StreamReader(tcp.GetStream()); //And just read everything the server has to say Console.WriteLine(data_in.ReadToEnd()); //when we're done close up shop. data_in.Close(); tcp.Close(); //this is just to pause the console so you can see what's going on. Console.WriteLine(""Press any key to continue...""); Console.ReadKey(false); } } } Note that this ignores the problem of any protocols you need to follow (for example if you're communicating via HTTP (port 80) there's a lot of work involved in talking to the server before getting the data (and there's another class that does this properly ;)) ok thats look good ...i want to test it but i don't know how to parse the data from tcp.GetStream i tried this NetworkStream networkStream = tcpClient.GetStream(); byte[] buffer = new byte[180000]; string d= networkStream.Read(buffer0Convert.ToInt32(networkStream.Length-1)).ToString(); i have this error...""This stream does not support seek operations."" i'm new in networking programming and i don't know if this can work at all can u tell me how to get the data from the network stream in a textbox or string?? I've added a fairly detailed example of using TcpClient. As a bonus you also get to see the server equivalent but you probably don't need to pay any attention to it :) 0 down vote accept i'm not getting any data at all all what i got is Press any key to continue... any help please ?"
1030,A,What is a good tutorial/howto on .net / c# socket programming I'm porting old VB6 code that uses the Winsock control to C#. I haven't done any socket programming and I wonder if anyone has a good reference/tutorial/howto that I can use to start getting up to speed. I'm appealing to the hive mind while I proceed with my generally unproductive googling. I'm using UDP not TCP at this time. MSDN is a good place to start Are you working on: a client (TCPClient) or a server (TCPListener)  here are a few: http://www.michaelzammuto.com/writing/socket1.jsp http://www.developerfusion.co.uk/show/3918/ http://www.csharphelp.com/archives/archive133.html  I recommend the asynchronous model for most applications especially if you want performance or applications that don't hang as soon there is a network problem. For this the MSDN articles on Socket.BeginConnect and Socket.BeginReceive are good places to start. The following link is not .NET but many of the recommendations still hold: http://tangentsoft.net/wskfaq/articles/lame-list.html  Code Project has a great tutorial  Just a heads up: I would recommend first working with TCP rather than UDP. UDP doesn't automatically redeliver lost packets like TCP so it will add another element to the equation that will probably just confuse you as you're just starting out. Building a socket client is relatively easy using the TCPClient class available in the .Net library. TCPListener is easy enough to use for a single client but if you're hoping to develop some server type application (IE: Handling multiple connections.) the real hurdle you'll have to overcome is understanding multithreading. Once you've played around with single connection sockets I suggest you read up on multithreading.  The August 2005 MSDN Magazine had an article about System.Net.Sockets and WinSock: http://msdn.microsoft.com/en-us/magazine/cc300760.aspx wow. I remember that one that was a really good article.
1031,A,"recv receiving not whole data sometime i have following issue: here is the chunk of code: void get_all_buf(int sock std::string & inStr) { int n = 1; char c; char temp[1024*1024]; bzero(temp sizeof(temp)); n = recv(sock temp sizeof(temp) 0); inStr = temp; }; but sometimes recv returning not whole data (data length always less then sizeof(temp)) only it's part. write side always sends me whole data (i got it with sniffer). what matter? thx. P.S. i know good manner suggests me to check n (if (n < 0) perror (""error while receiving data) but it doesn't matter now - it's not reason of my problem. P.S.2 i've forgot - it's blocking socket. Good manner also suggest to check your inputs. if there is no \0 in what you are receiving then at best your program may crash at worse you can get an crafted invalid string which exploits the program and pwn the system for fun and profit. `std::string`? Then this is a C++ question not a C one. A much better way is to use following: void get_all_buf(int sock std::string & output) { char buffer[1024]; int n; while((errno = 0 (n = recv(sock buffer sizeof(buffer) 0))>0) || errno == EINTR) { if(n>0) output.append(buffer n); } if(n < 0){ /* handle error - for example throw an exception*/ } }; Also note that the buffer allocated on the stack is much smaller. Having 1M buffer on stack may cause stack overflow. Additional note: You probably don't want to read until the socket is closed so you may need to add another termination condition to the while loop. EINTR is a errno value. That mean you should set errno to 0 do the recv call and then compare errno to EINTR to see if you have been interrupted by a signal recv will return -1 not 0 on EINTR. @AlastairG: Yes I have just figured it out from a man page. It's long since I have used BSD sockets last time. If you receive a EINTR without receiving any data then you should ignore it and continue anyway.  TCP works as a layer on top of other layers: IP and Ethernet. IP allows data fragmentation and Ethernet allows some data to get lost over the wire. That leads to data loss and it's reflected on your calls to recv. When you call recv the underlaying operating system will try to read as much data as it can up to the size you specified but might return the call having read less bytes even one single byte. You need to create some protocol of your own to keep reading data up to finishing your data piece. For example you can use ""\n"" as a delimiter. This code can be improved but I hope will get you the idea: void get_all_buf(int sock std::string & inStr) { int n = 1 total = 0 found = 0; char c; char temp[1024*1024]; // Keep reading up to a '\n' while (!found) { n = recv(sock &temp[total] sizeof(temp) - total - 1 0); if (n == -1) { /* Error check 'errno' for more details */ break; } total += n; temp[total] = '\0'; found = (strchr(temp '\n') != 0); } inStr = temp; }  The TCP standard allows for fragmentation of data packets. In practice this doesn't happen with small data packets of a few hundred bytes or so but a megabyte of data is almost certain to get fragmented. Secondly when you say the sniffer says all the data gets sent in one packet or in many? Good network programming practice requires you to not assume that messages arrive in singular chunks. Two sequential messages can arrive as one packet (in theory but almost never in practice) and even if they arrive in multiple packets can be read as a single read. One message can get fragmented into multiple packets and they might not all arrive at once which is probably what you are seeing. Your program should buffer all its reads and have a mechanism to determine when a whole message has arrived either via a delimiter (e.g. HTTP headers which are delimited with CRLFCRLF) or by a byte count (e.g. HTTP bodies where the length is specified in the header) or by closing the connection to indicate the end of the data (e.g. HTTP bodies when the content length isn't specified in the header). There may be other mechanisms too. @AlastairG : True I have not read that comment carefully. @BatchyX: true but since signals do prevent effective use of MSG_WAITALL it probably shouldn't be used except in very particular circumstances. If it is not being used then as I said earlier in almost every case recv() returns a short count due to fragmentation rather than to signals. A short count from recv() can also be due to the size of the receive buffer but again this is rarely the case so I didn't mention it. Was it you downvoted me? Thanks because it encouraged someone to up vote me so I gained 8 points :) i've received about 30 kb data successfully. it was fragmented. but sometimes i cannot receive 7-8 kbytes of data. It will depend on how the sending side fragments the data how busy the network is how the packets arrive and probably several other variables. I have updated my answer with suggestions as to how to cope with it but just a brief outline. I suggest you search the internet for articles on network programming and study them. It's not that difficult but there are a lot of things to consider. How you write your program depends very much on what you are doing. Most articles on socket programming give bad examples of very noddy applications and the code is little use in real life. @milo since you know the size of structure you are expecting keep calling recv until you have read that many bytes. `for ( int total (0); total < sizeof ( temp ); ) { int n = recv ( sock temp + total sizeof ( temp ) - total 0 ); if ( n < 0 ) abort(); total += n; }` i understand what are you talking about but it's important to me to know how to do it properly. because i saw in books examples of use `recv` it looks like this: `n = read(newsockfdbuffer255); if (n < 0) error(""ERROR reading from socket"");` so... does it correct? TCP has nothing to do with this. The problems lies in the recv() function. recv can return as many bytes as it wants because of signals and such. @Pete Kirkham: Calling recv() in a loop is exactly proposed in my answer. thnx i will be waiting for end of packet signature. but it's interesting to me to know other opinions if anybody does have - you are welcome :) @Juraj you hadn't answered when I wrote the comment @BatchyX: That is true (although usually recv returns -1 with errno set to EINTR if interrupted by a signal) but in the vast majority of cases like 99.9999% of the time or more the problem is caused by fragmentation. @Juraj actually your answer is not the same as Pete's. Pete is suggesting reading until an expected amount of data is read. Your solution will read until the socket is closed (or at least shutdown for writing on the far end). If fragmentation is your only problem use the posix MSG_WAITALL flag. but fragmentation is not your only problem so you have to loop-and-check anyway."
1032,A,"Java Network / Socket programming tutorial I'm going to create a project in some month in a course I'm having and for that purpose I would like to read and try making some small test programs regarding the network / socket programming in Java. Ideally what I'm looking for is a tutorial with description and a end program to create where I can between two computers send and resive small and simple text messages. Don't have to be any GUI or something like that just the simple version. I've read some articles from Sun's own homepage and in one of my Java Book but I seem to be missing the last piece of the puzzle to really understand that ""network programming"". In addition I don't know if I'm mixing things now but to my understanding can the multithreading paradigm also be applied to the network / socket programming. Therefore my question is if anyone knows about a good tutorial or educational link for me to get an overview of this topic. I’m not totally new to Java but haven’t got any experience in this type of programming. Thanks - Emil There is no universal book for this kind of learning. The best bet is to start with java's own tutorial on sockets/threads etc here. By encountering problems once you start the implementation it is then you will start learning preferably by visiting SO again and asking a bunch of specific code questions.  Here is a nice sample chapter from Java Network Programing. There's also some other parts available here and also sample sources are available. If you'd decide to buy this book I'd say these chapters are enough for now: Chapter 2. Basic Network Concepts Chapter 3. Basic Web Concepts Chapter 4. Java I/O Chapter 5. Threads Chapter 6. Looking Up Internet Addresses Chapter 7. URIs and URLs Chapter 9. Sockets for Clients Chapter 10. Sockets for Servers Don't bother about NIO yet; it's quite advanced. In addition I don't know if I'm mixing things now but to my understanding can the multithreading paradigm also be applied to the network / socket programming. Yes indeed it's a very important aspect. You'll find out why once you've read the sample chapter. Have fun! I actually own that book. It explains the different terms a lot in easy to understand language. But there is also the problem instead of explaining the needed stuff it goes on and has too much unnecessary information. Each time I needed something after reading in the book I ended on java's tutorial page or somewhere else where the information was much better and more ""to the point"".  I've only read the C version of this book but likely Java version is going to be equally short and good: Calvert and Donahoo's TCP/IP Sockets in Java: Practical Guide for Programmers. Even if you learned socket programming in C you could probably adapt to Java's implementation fairly quickly.  For basic steps you might wan't to read this tutorial Sockets programming in Java. In this model the most people used one thread to handle one network connection. In java 1.4 NIO was introduced this makes intros a bit more complex: Java NIO Tutorial. NIO allows to use a Selector which allows you to handle many connections in one thread which leads to higher performance.  from my experience and in my opinion when you want to learn socket programming at the level of abstraction provided by the Java Network API and for any language for that matter you also have to be familiar with i/o and threads. there are a lot of tutorials out there i suggest you grab a book. just a thought!"
1033,A,Multiple Socket Connections I need to write a server which accepts connections from multiple client machines maintains track of connected clients and sends individual clients data as necessary. Sometimes all clients may be contacted at once with the same message other times it may be one individual client or a group of clients. Since I need confirmation that the clients received the information and don't want to build an ACK structure for a UDP connection I decided to use a TCP streaming method. However I've been struggling to understand how to maintain multiple connections and keep them idle. I seem to have three options. Use a fork for each incoming connection to create a separate child process use pthread_create to create an entire new thread for each process or use select() to wait on all open socket IDs for a connection. Recommendations as to how to attack this? I've begun working with pthreads but since performance will likely not be an issue multicore processing is not necessary and perhaps there is a simpler way. I developed a library and server that implements the behavior requested above using C++ and a select()/single-threaded structure; It's BSD licensed so you're welcome to use it or just crib example code or ideas from it. If you're interested you can find it here: https://public.msli.com/lcs/muscle/ As a note I'm working in C / C++ I should also mention the number of connections to this system are static. This is not a public facing machine (connections will not change over time -- i.e I don't see performance issues being a problem). Personally I would multiplex using select() or preferably kqueue() if running on mac or bsd. Either of the three options would work however and the pthread/fork options are super easy to implement if your server is going to have a low client count and low active client contention. And in the case that no clients were communicating with the server? (As in all communication comes from the server to the client). What would be the best option then? As a note there is no language called C / C++. In which language are you working? C++ as tagged? C++ although if someone references C code I can use it as an example Child processes are not nice because you just move the goalpost. You will need to make your child processes communicate between each other then you are back to the same problem. It is possible to use threads but you will have other problems if your threads keep blocking on socket receive. select() (or poll() on newer (POSIX) Unixes) is still the best solution. You tell either select() or poll() which sockets or descriptors you want to monitor for events (probably just input (read) events is enough for you) then you do the read only on that socket or descriptor that was flagged by select()/poll(). It is guaranteed that recv() will not block.  Read the C10K page for whole bunch of options. Then read the High Performance Server Architecture article. These will answer lots of questions for you. I would go with the third option. For that look into epoll(4) and/or kqueue(2) facilities for modern performant select/poll replacements. +1 for epoll(). You might also look at boost::asio which will abstact away a lot of the work. Yes `boost::asio` is one of the options for C++. It's mentioned on the pages I linked.
1034,A,"How to get domain controller IP Address Hi All I want to know how I can get Domain Controller IP address programatically using C# anyone tried that? Thanks Here's how I would do it. You'll need to use System.Net and System.DirectoryServices. // get root of the directory data tree on a directory server DirectoryEntry dirEntry = new DirectoryEntry(""LDAP://rootDSE""); // get the hostname of the directory server of your root (I'm assuming that's what you want) string dnsHostname = dirEntry.Properties[""dnsHostname""].Value.ToString(); IPAddress[] ipAddresses = Dns.GetHostAddresses(dnsHostname);  Well here is the general workflow of how to get it as described at the MS site: http://support.microsoft.com/kb/247811 Here is the link from PInvoke.net to call the referenced DsGetDcName function: http://pinvoke.net/default.aspx/netapi32/DsGetDcName.html You could go down and dirty and do a raw DNS A Record query as described in the first link but I think the PInvoke call should do the trick. Hope that helps Mike Haven't tried it in C# but in Java it's real easy to go the DNS A record route.  Thanks All I done it as in this code  using System; using System.Collections.Generic; using System.Text; using System.Net; using System.Net.Sockets; using System.DirectoryServices.AccountManagement; using System.DirectoryServices.ActiveDirectory; public doIt() { DirectoryContext mycontext = new DirectoryContext(DirectoryContextType.Domain""project.local""); DomainController dc = DomainController.FindOne(mycontext); IPAddress DCIPAdress = IPAddress.Parse(dc.IPAddress); } Thanks again To retrieve this across a different domain you'd need to do: `new DirectoryContext(DirectoryContextType.Domain""project.local""""validUserInDomain""""validUserPassword"");`"
1035,A,What can I do to avoid TCP Zero Window/ TCP Window Full on the receiver side? I have a small application which sends files over the network to an agent located on a Windows OS. When this application runs on Windows everything works fine the communication is OK and the files are all copied successfully. But when this application runs on Linux (RedHat 5.3 the receiver is still Windows) - I see in Wireshark network trace messages of TCP Zero Window and TCP Window Full to appear on each 1-2 seconds. The agent then closes the connection after some minutes. The Windows - Linux code is almost the same and pretty simple. The only non-trivial operation is setsockopt with SO_SNDBUF and value of 0xFFFF. Removing this code didn't help. Can someone please help me with this issue? EDIT: adding the sending code - it looks that it handles properly partial writes: int totalSent=0; while(totalSent != dataLen) { int bytesSent = ::send(_socket(char *)(data+totalSent) dataLen-totalSent 0); if (bytesSent ==0) { return totalSent; } else if(bytesSent == SOCKET_ERROR){ #ifdef __WIN32 int errcode = WSAGetLastError(); if( errcode==WSAEWOULDBLOCK ){ #else if ((errno == EWOULDBLOCK) || (errno == EAGAIN)) { #endif } else{ if( !totalSent ) { totalSent = SOCKET_ERROR; } break; } } else{ totalSent+=bytesSent; } } } Thanks in advance. More details? Is the file being transferred successfully only at a slower rate or is the transfer failing? If it's failing where is it failing? Is anything getting across or is it failing half way through? @Robert thanks. The transfer fails. If I transfer a folder contains for example 2 GB of 3 KB - 50 KB files it transfers sometimes ~0.5 GB sometimes ~1.3 GB of data and then fails. What error messages are you getting and which side is shutting down the connection? Are you using blocking or non-blocking I/O. Do you have a dedicated thread doing I/O? The more details the better and if you could post code fragments that would be the best. What is `::send(...)`? Is this a member of your class which wraps the standard `send(...)` function? Can you post the receiving code too? It sounds like data may not be getting pulled off at the receiving end. The most likely problem is that you have a bug in your code where you don't handle partial reads or partial writes correctly. TCP between Linux and Windows is known to work.  A read() returning 0 indicates a closed connection. A send() returning 0 however just indicates 0 bytes sent. You have to continue the loop. To avoid busy waiting you can use select to wake up when the socket is writeable again. Sorry Peter must have misread you post and switched what you said about send and read in my own head. You're wrong. A return of 0 on send has no special meaning. A return of 0 only indicates a closed connection for reads. On send a closed connection is indicated by a return of -1 and `errno` set to `EPIPE` if the `MSG_NOSIGNAL` flag is passed other wise a signal is thrown and the program is terminated. @Robert I fail to see where you prove me wrong. I especially pointed out that sending 0 bytes is not special beyond the fact that 0 bytes where sent.  I tried to disable Nagle's algorithm (with TCP_NODELAY) and somehow it helped. Transfer rate is much higher TCP window size isn't being full or reset. The strange thing is that when I chaged the window size it didn't have any impact. Thank you. That's really odd. Typically disabling Nagle is only useful for real time apps where you want to have very low latency at the expense of wasting allot of bandwidth. Disabling it for bulk file transfer seems counter-intuitive. Have you actually tested and seen objectively that disabling Nagle is what makes the difference? Maybe some other change you made could be responsible? @Robert S. Barnes: That's really odd I agree. But this is the only change that was made and it helped. Moreover the receiver side has already disabled Nagle. I know that it may refer to an underlying fundamental problem that is hiding somewhere waiting to jump out and bite at another time. But as a workaround it is good enough.  Not seeing your code I'll have to guess. The reason you get a Zero window in TCP is because there is no room in the receiver's recv buffer. There are a number of ways this can occur. One common cause of this problem is when you are sending over a LAN or other relatively fast network connection and one computer is significantly faster than the other computer. As an extreme example say you've got a 3Ghz computer sending as fast as possible over a Gigabit Ethernet to another machine that's running a 1Ghz cpu. Since the sender can send much faster than the receiver is able to read then the receiver's recv buffer will fill up causing the TCP stack to advertise a Zero window to the sender. Now this can cause problems on both the sending and receiving sides if they're not both ready to deal with this. On the sending side this can cause the send buffer to fill up and calls to send either to block or fail if you're using non-blocking I/O. On the receiving side you could be spending so much time on I/O that the application has no chance to process any of it's data and giving the appearance of being locked up. Edit From some of your answers and code it sounds like your app is single threaded and you're trying to do non-Blocking sends for some reason. I assume you're setting the socket to non-Blocking in some other part of the code. Generally I would say that this is not a good idea. Ideally if you're worried about your app hanging on a send(2) you should set a long timeout on the socket using setsockopt and use a separate thread for the actual sending. See socket(7): SO_RCVTIMEO and SO_SNDTIMEO Specify the receiving or sending timeouts until reporting an error. The parameter is a struct timeval. If an input or output function blocks for this period of time and data has been sent or received the return value of that function will be the amount of data transferred; if no data has been transferred and the timeout has been reached then -1 is returned with errno set to EAGAIN or EWOULDBLOCK just as if the socket was specified to be nonblocking. If the timeout is set to zero (the default) then the operation will never timeout. Your main thread can push each file descriptor into a queue using say a boost mutex for queue access then start 1 - N threads to do the actual sending using blocking I/O with send timeouts. Your send function should look something like this ( assuming you're setting a timeout ): // blocking send timeout is handled by caller reading errno on short send int doSend(int s const void *buf size_t dataLen) { int totalSent=0; while(totalSent != dataLen) { int bytesSent = send(s((char *)data)+totalSent dataLen-totalSent MSG_NOSIGNAL); if( bytesSent < 0 && errno != EINTR ) break; totalSent += bytesSent; } return totalSent; } The MSG_NOSIGNAL flag ensures that your application isn't killed by writing to a socket that's been closed or reset by the peer. Sometimes I/O operations are interupted by signals and checking for EINTR allows you to restart the send. Generally you should call doSend in a loop with chunks of data that are of TCP_MAXSEG size. On the receive side you can write a similar blocking recv function using a timeout in a separate thread. Thanks for this post. It is very informative especially the `MSG_NOSIGNAL` which I think is my problem on one of my applications.  A common mistake when developing with TCP sockets is about incorrect assumption about read()/write() behavior. When you perform a read/write operation you must check the return value they may not have read/write the requested of bytes you usually need a loop to keep track and make sure the entire data was transfered.
1036,A,"Properly handling NSURLConnection errors I have a simple form interface set up that send username and password information to a server: (working)  NSString *postData = [NSString stringWithFormat:@""user=%@&pass=%@""[self urlEncodeValue:sysUsername][self urlEncodeValue:password]]; NSLog(@""Post data -> %@"" postData); /// NSData* postVariables = [postData dataUsingEncoding:NSASCIIStringEncoding allowLossyConversion:YES]; NSMutableURLRequest* request = [[[NSMutableURLRequest alloc] init] autorelease]; NSString* postLength = [NSString stringWithFormat:@""%d"" [postVariables length]]; NSURL* postUrl = [NSURL URLWithString:@""http://localhost/~csmith/cocoa/test.php""]; [request setURL:postUrl]; [request setHTTPMethod:@""POST""]; [request setValue:postLength forHTTPHeaderField:@""Content-Length""]; [request setValue:@""application/x-www-form-urlencoded"" forHTTPHeaderField:@""Content-Type""]; [request setHTTPBody: postVariables]; NSData *returnData = [NSURLConnection sendSynchronousRequest:request returningResponse:NULL error:NULL]; NSLog(@""Post data SENT & returned -> %@"" returnData); How do I handle connection errors such as no internet connection firewall etc. Also does this method use the system-wide proxy settings? Many of my users are behind a proxy. Thanks a lot! You should use asynchronous request to handle proxy and network errors. This is more efficient. To add extra check you can add reach-ability test in your code before communications. you can find reach-ability test code here  First you shouldn't use synchronous requests use asynchronous request instead and indicate activity using indeterminate progress indicators. When using asynchronous requests you have to set a delegate implement delegate methods notably: -connectionDidFinishLoading: -connection:didFailWithError: From the docs: Unless a NSURLConnection receives a cancel message the delegate will receive one and only one of connectionDidFinishLoading: or connection:didFailWithError: message but never both. In addition once either of messages are sent the delegate will receive no further messages for the given NSURLConnection. As for the last question: Also does this method use the system-wide proxy settings? Yes NSURLConnection uses them automatically."
1037,A,"What is the xmlsocket:// protocol? This SO answer says ""The Flash Player now looks for a policy file on port 843"".. I have a policy server that is listening on port 843 which sends out an XML policy to Flash clients. Is this enough? What is the ""xmlsocket://"" protocol? Is it just a way to tell Flash player that we're talking about sockets when using the loadPolicyFile() method? As I understand it there are two different types of policy files cross domain policies for loading data from an external domain and policy files regarding socket connections. Both policy files are quite similar in that they both use a similar structured XML data structure. I believe that the xmlsocket:// protocol is merely a pseudo protocol in that it instructs the Security.loadPolicyFile method to load a policy regarding sockets rather than a cross domain policy for accessing external data. More information can be found on this Livedoc page."
1038,A,Detecting internet Inactivity in VB.NET Detecting internet Inactivity in VB.NET Here are three approaches: Use PerformanceCounters to monitor Packets/sec. Use System.Net.NetworkInformation to read packet counters. Use GetIpStatistics to read the packet counter. A VB example. I'll look into the coding and then decide on a further action! Thanks for your help! :D This was the best:- http://www.ex-designz.net/apidetail.asp?api_id=444
1039,A,Does recv remove packets from pcaps buffer? Say there are two programs running on a computer (for the sake of simplification the only user programs running on linux) one of which calls recv() and one of which is using pcap to detect incoming packets. A packet arrives and it is detected by both the program using pcap and by the program using recv. But is there any case (for instance recv() returning between calls to pcap_next()) in which one of these two will not get the packet? I really don't understand how the buffering system works here so the more detailed explanation the better - is there any conceivable case in which one of these programs would see a packet that the other does not? And if so what is it and how can I prevent it? AFAIK there do exist cases where one would receive the data and the other wouldn't (both ways). It's possible that I've gotten some of the details wrong here but I'm sure someone will correct me. Pcap uses different mechanisms to sniff on interfaces but here's how the general case works: A network card receives a packet (the driver is notified via an interrupt) The kernel places that packet into appropriate listening queues: e.g. The TCP stack. A bridge driver if the interface is bridged. The interface that PCAP uses (a raw socket connection). Those buffers are flushed independently of each other: As TCP streams are assembled and data delivered to processes. As the bridge sends the packet to the appropriate connected interfaces. As PCAP reads received packets. I would guess that there is no hard way to guarantee that both programs receive both packets. That would require blocking on a buffer when it's full (and that could lead to starvation deadlock all kinds of problems). It may be possible with interconnects other than Ethernet but the general philosophy there is best-effort. Unless the system is under heavy-load however I would say that the loss rates would be quite low and that most packets would be received by all. You can decrease the risk of loss by increasing the buffer sizes. A quick google search tuned this up but I'm sure there's a million more ways to do it. If you need hard guarantees I think a more powerful model of the network is needed. I've heard great things about Netgraph for these kinds of tasks. You could also just install a physical box that inspects packets (the hardest guarantee you can get).
1040,A,"Pattern/technique for sending ‘updated only fields’ from server to client for a given object? I have some serverside data that I need replicating (pushed in real-time) from a server app to around 100 wpf clients. My problem is when a given Order object changes it typically only changes a 1 or 2 fields so I only want to send those changes over the wire Not the whole object – thus decreasing the wire payload processing time etc as the whole Order object has around 50 fields. The data is a Dictionary of Order objects keyed on OrderId. I use protobuf-net to seralise the data and send over the wire to the wpf clients. Has anyone dealt with this patterm/problem before? Or have any ideas on who to achieve this? Thanks a lot. Create a simple proxy using Castle.DynamicProxy which saves the name of all properties that have been changed.  protobuf-net supports a number of patterns to aid this type of scenario the simplest being (to share the pattern used by System.ComponentModel): [ProtoMember(1)] public string Foo { get;set; } public bool ShouldSerializeFoo() { /* return true if Foo is ""dirty"" */ } This assumes you have some mechanism for tracking the changes yourself (for hooking into the ShouldSerialize* method); protobuf-net doesn't do change tracking itself. If you don't currently have any change tracking you might be able to use something from this answer: Comparing 2 objects and retrive a list of fields with different values I meant a Dictionary (hashtable) (key: PropertyName Value = theValue) to store the property name and latest values @arkina fine whatever works; as long as you can tell whether it has changed or not. Sorry how would this work? I can track changes using INotifyPropertyChanged and store the last change in a Hashtable (key: PropertyName Value = theValue). But in your example how would ShouldSerializeFoo be used? Inside the property? Also do you have any System.ComponentModel examples? I can't find anything on this usage pattern? @arkina - [see here](http://msdn.microsoft.com/en-us/library/system.componentmodel.propertydescriptor.shouldserializevalue.aspx) for MSDN discussing that pattern. Re your earlier comment I might use a `HashSet` and just store the names; ""as is"" the object would need to be able to return `true`/`false` on a member-by-member basis i.e. `return deltas.Contains(""Foo"")`"
1041,A,"How can I learn _really_ low-level network programming? So I want to learn all about networks. Well below the socket down to raw sockets and stuff. And I want to understand hubs routers access points etc. For example I'd like to be able to write my own software to do this kind of stuff.* Is there a great source for this kind of information? I know that I'm asking a LOT here and that to fully explain it all requires from high level down to low level. I guess I'm looking for a source similar in scope and depth to Applied Cryptography but about networks. Thanks to anyone who can help to point me (and others like me?) in the right direction. * Yes I realize using any of my hand-crafted network stack code would be a huge security issue and am only looking to do it to learn :) Similar Question: here. However I'm looking for more than just 'what's below TCP/UDP sockets?'. Edited for Clarification: The depth I'm talking about is above the driver level. So assuming that the bits can make it to and from the other end of the wire what next? How low-level you want to go; do you want to learn to: * write a TCP implementation using only raw sockets? * Write a new TCP/IP stack for your OS or for an embedded system? * Do the frame encapsulation too? (which will be different for connecting to each of Ethernet wired networks Gig-E dialup ATM Token Ring wifi bluetooth etc) * write a network card driver to take frames and send them to the NIC? This course worked for me: COS 461 at Princeton. Note that it assumes system-level programming experience with C. Pretty much all the readings and lectures are available online under ""Syllabus"". And you can try the assignments too (unfortunately you won't have access to the Virtual Network System). Just out of curiosity where does this fall under Princeton's undergrad course sequence? It's generally taken by CS majors in the 3rd or 4th year but some people just take them because they are interested.  CISCO CCNA materials contain a great network fundamentals but does not affect programming aspect. I'm not sure that there is an official free link but you can try to find them.  Hmmm ... have you looked into Computer Networks by Tanenbaum ?  Check this.. it is a good collection of information: http://www.tcpipguide.com/free/t_toc.htm  This may not help you learn it but a packet sniffer like Wireshark will give you some insight into what the data looks like at a pretty low-level protocol (TCP/IP).  You should equip with a c compiler and the necessary libs and headers for your OS and play around. You may want to read for example: http://snap.nlc.dcccd.edu/learn/fuller3/chap13/chap13.html I had some more links in my delicious account but they all went down the digital drain ;-)  I learned IP networking from TCP/IP Illustrated. Highly recommended. I'd also recommend Unix Network Programming http://www.amazon.com/Unix-Network-Programming-Addison-Wesley-Professional/dp/0131411551/ref=sr_1_1?ie=UTF8&s=books&qid=1244659765&sr=1-1  Have you any embedded programming experience ? If so I recommend you buy one of these development boards. They are cheap and allow you work on every part of the networking stack plus all the software tools required are free. Note that getting going on it isn't easy and I ended up reading the CS8900 IC datasheet to learn how to make it communicate with the ARM7 based processor. But if you enjoy that sort of thing (as I do) then they are great fun.  The TCP/IP Guide  As you have obviously recognised the universe does not start and end with the IP Protocol. Take a look at the OSI 7 Layer Model where IP is a Layer 3 (Network) protocol. Common IP Routers will operate at this level but there is more complexity you probably should understand in the Data Link and Physical layers before you start coding your own network stacks. Start with the fundamentals of data communications in all its myriad forms and work your way up the stack until you get to where you need to stop. Data Communications Computer Networking and Open Systems is a good foundation text and then look for more detail on each area you need to focus on. Previous answers include good links for IP and TCP/IP and as mentioned Wireshark will let you look down through some of the layers  I have found the networking chapter in ""understanding the linux kernel"" and ""understanding linux network internals"" from oreilly to be very helpful. The TCP/IP stack is a very good start but there is a lot more and a good understanding of how ethernet works and how ethernet != IP != the-interweb will go a long way. books on network security often do a decent if not goos job explaining how networks work in a concise context. what really did the trick for me was taking a job implementing NAT :)"
1042,A,"What network library on what IRQL? I somewhat confused because I've read that ""everything"" should be possible at IRQL_PASSIVE but I am not so sure whether this includes winsock2 or other userland libraries. My normal understanding would be to use the WSK interface. But it would be much more comfortable if I could use normal sockets. As I am running into builder errors a lot while trying to include winsock I am a little unsure. And as searching documentation provides no authoritative answer I'd like to state this as a question: What kind of libraries can I really access at IRQL_PASSIVE? Is the IRQL the only limiting parameter? You cannot access (most) user land libraries from kernel mode. Either you get an kernel mode interface for that library or you have to use a user mode service (inverted calls). Winsock has a kernel interface See this MSDN article. The IRQL level determines which 'kernel-services' you can use. So for KeGetCurrentIrql() >= IRQL_DISPATCH you cannot depend on paging (This produces the IRQL_NOT_LESS_OR_EQUAL bugcheck) and cannot (should not?) call functions which use paging. It does not prevent usage of libraries."
1043,A,udp packet loss and recovery I am working on both udp/tcp based P2P for file and real time video streaming application.the application will be developed for both Linux and windows platform using c++. We are using ICE(TCP/UDP hole punching) to implement the P2P. while TCP ensure the packet loss but for UDP I need a decent approach to make sure packet must be delivered to the other peer. I wanted to know the algorithm or technique to do this. Is there any free thord party tool/libraries to do. Any link and suggestion will be appreciated? You might find the answers to this question helpful: What do you use when you need reliable UDP?  A simple approach would be to have a monitoring thread for each packet -- public void run() { int tries = 0; do { send(); try { Thread.sleep(1000); } catch (InterruptedException e) {} } while (!acknowledged() && ++tries < maxTries); } If performance is important a single thread (or small number of threads) could monitor a queue of messages. This just seems a VERY unlikely method of ensuring UDP data delivery that's all... Why do you say it's unlikely? Are you serious? I didn't get you Len!  You need to cover for 4 main issues: Data slicing - UDP datagrams cannot contain an infinite amount of information. Therefore you will (often) need to slice your information into multiple datagrams and reconnect the puzzle pieces at the other end. For a given 'slicing' you need unique identifier and puzzle piece number. Never Reached - UDP datagrams can sometime get lost on the net. If a target peer does not receive an expected datagram there should be a mechanism letting him request it again. Another method is to send a confirmation upon reception. Replay - Sometimes you may receive the same UDP datagram twice (for complex reasons). The target peer should detect this. Out-of-order - The order of sending is not always the order of receiving. A target peer needs to handle this situation. There is a protocol called slicing window which you could implement. I don't think you'll find a 3rd party library for this (though someone may prove me wrong here) because all the above is typically implemented by TCP itself.
1044,A,streaming to correct network interface I have IP cam that supports RTSP streaming. It's connected to router with 2 network cards with IP1 and IP2 addresses. I make 2 connections to IP cam by IP1 and IP2 addresses from the same IP and I need to receive corresponding streams thru correct network card but both streams (RTP over UDP) go thru IP1. How this can be resolved? I don't know if RTSP server binds UDP sockets to corresponding IP and I don't know what IP stack is in IP cam (weak end system or strong end system). I haven't found anything interesting in router configuration. As I understand routing table cannot help me cos I'm connected from the same IP is it right? Also Sorry for incomplete info but it's all I have at the moment. Thanks for your time. Try 'nmap' (http://www.nmap.org) it's got OS/tcp stack fingerprinting capabilities. It could identify whatever's running inside the camera. actually received reply from router vendor and no it's not possible to send UDP packet over particular network interface. it uses load balancing that cannot be turned off
1045,A,"How to handle buffering data read from the network? When reading data over the network you specify a buffer to receive the data into: byte[] b = new byte[4096]; socket.Receive(b); Now my first thought is of course to reuse the receive buffer by declaring it as a member variable of the class. My next issue is that I have not received all of the data that I am expecting so I need to buffer my data. This is easy to accomplish by keeping track of the count of bytes received and specifying the offset: socket.Receive(m_ReceiveBuffer count m_ReceiveBuffer.Length - count); Now the issue here is that if it is still not enough I am guessing that I need to grow the buffer which means copying memory and continue to receive into this buffer. Assuming that something went wrong this buffer would continue to grow and if big enough messages are received would run the system out of memory. Any ideas how to properly handle this? Is there a better way of receiving the data than just fill copy grow fill copy grow that I am talking about? First separate the code into receiving the data and processing the data. The receive buffer should only hold the data until the code gets a chance to copy it out to the processing area. The processing area is where you will determine if you have received enough data to do something useful. Don't leave the data in the network receive buffer until this happens. For the network receive buffer I think using a circular buffer will help you with your idea of reusing a buffer. Hopefully you have an idea of the message sizes. That will help in determining the size of the buffer. If you assert (or something similar) when the read and write pointers of the circular buffer meet then increase the size of the buffer. Once the size of the buffer is large enough you should be able to read enough data out of the buffer into the processing code at a rate fast enough that the circular buffer doesn't overflow.  read in chunks: const int ChunkSize = 4096; int bytesRead; byte[] buffer = new byte[ChunkSize]; while ((bytesRead = socket.Receive(buffer 0 ChunkSize SocketFlags.None)) > 0) { byte[] actualBytesRead = new byte[bytesRead]; Buffer.BlockCopy(buffer 0 actualBytesRead 0 bytesRead); // Do something with actualBytesRead // maybe add it to a list or write it to a stream somewhere } This is still not solving the issue where I can't process the data right away it will still continue to fill up memory. Could you explain what you mean by ""process"" the data? Can't you ""process"" the data in chunks or do you need to have read all the data in which case of course you will need to have it in memory?  Before starting with SYstem.Net.Sockets.Socket are you sure that you can't use System.Net.Sockets.TcpClient (or UdpClient) that does all the messy buffer work for you and transforms it to an easily managed stream? If not remember that the amount of data you recieve doesn't have to be equal to what you request so you should always look at the return value from the recieve function. And the only way to not run out of memory is by actually processing what you recieve. Yes I only use Receive as an example but I am using ReceiveAsync API to read the data in an asynchronous fashion. For this a network stream will not work for my case. And my issue arises that yes it works great if you can process all of the data immediately but sometimes it is just enough to push it to be a little bit bigger."
1046,A,"Keeping sync in multiplayer RTS game that uses floating point arithmetic I'm writing a 2D space RTS game in C#. Single player works. Now I want to add some multiplayer functionality. I googled for it and it seems there is only one way to have thousands of units continuously moving without a powerful net connection: send only the commands through the network while running the same simulation at every player. And now there is a problem the entire engine uses doubles everywhere. And floating point calculations are depends heavily on compiler optimalizations and cpu architecture so it is very hard to keep things syncronized. And it is not grid based at all and have a simple phisics engine to move the space-ships (space ships have impulse and angular-momentum...). So recoding the entire stuff to use fixed point would be quite cumbersome (but probably the only solution). So I have 2 options so far: Say bye to the current code and restart from scratch using integers Make the game LAN only where there is enough bandwidth to have 8 players with thousands of units and sending the positions and orientation etc in (almost) every frame... So I looking for better opinions (or even tips on migrating the code to fixed-point without messing everything up...) Does every player's computer have to simulate everything or only things within their view? They have to simulate everything. What are you using doubles for specifically? Could you use decimal instead? Generaly the server would store the state(position/oriantaion/type) of all players units. When player1 moves a unit either... the instuction to move is sent to the server or... the updated state is sent to the server When the player client needs to render the scene the server sends back state info on the location of all the units within a requested scope.  I'm a little late responding to this but from a Game Security point of view the simulation should only be running on the server/host (i.e.: don't trust the clients they could be cheating): The clients should only be sending their movements/commands to the server (which discards bad inputs or clamps them within game limits so a client saying ""I'm running at 10000m/s"" gets clamped by the server to say 10m/s). The server/host only tells clients about things happening within their field of view (i.e.: a player at co-ordinates 00 doesn't get told about two AIs fighting each other at 2000 if he can only see a radius of 50 units around him/herself). It's the second part that saves the bandwidth - the simulation on the server/host may have thousands of objects to manage but the clients only need to know about 100 or 200 things within their own field of view. The only wrinkle in the situation is things like dynamic fire (bullets missiles etc) whose range may be greater than a client's view radius. The server/host tells the clients their origin and initial trajectory/target object the clients then simulate their path according to the same rules but the kills are only valid in the simulation on the server/host. Serializing the client-specific world state and compressing it before transmission can also be a huge win especially if your class properties are only Public where needed. (I normally avoid XML but we significantly improved compression ratios in one application by serializing to XML and compressing it versus serializing to a binary format and compressing that. I suspect the limited range of ASCII characters used had a hand in it YMMV.) Actually it is pretty easy to ensure security in P2P games where each client simulates the world on their own. All you have to do is send turn checksum of simulation data to each other (like resource count building count etc) and if someone sends a different checksum then he's either desync or cheating and you simply drop him out of the game.  A common technique is to have all clients describe their current state to the other clients periodically. When two computers disagree about the state of an object presumably due to floating point error the game has some rule to determine which is correct and all clients adjust to match it. As he stated in the original post the game state will consist of thousands of objects making entire state synchronisation infeasible. I do not know of a single real time strategy game that uses that approach -- they all synchronise by syncing input and ensuring that the simulation is deterministic.  Surely all your client will be using the same binary so compiler optimisations have no effect on synchronisation issues. Also if you are only planning on targeting one architecture (or are at least only allowing people to play against each other if they are on the same architecture) then that doesn't matter either. I've done exactly the same thing using floating points in C# developing games for the iPhone and desktop and they both give the same results even though the iPhone is ARM and desktop is x86. Just make sure the game does exactly the same calculations and you will be fine. If all else fails just replace all instances of float in your game to a standard fixed-point arithmetic class. That way you can be 100% sure that your calculations are deterministic across architectures although the nature of fixed-point arithmetic may adversely effect your game. Even 2 instances of the exact same architecture can have different floating point results due to things like the FPU control word CPU instruction reordering based on pipeline usage etc. It read somewhere that floating point operations are not associative (due to rounding) and one bit difference can matter an can cause desync. Even different versions of .NET can have different JIT optimizers which can cause different results. And I just can't ask my users to have .NET 2.0 (because I want it work with Mono on Linux too). So I decided to create a fixed-point class and replace (almost) all double with according to this: http://stackoverflow.com/questions/605124/fixed-point-math-in-c (he probably faced exactly the same problem) I'm aware that CPU will reorder instructions but I don't believe that it will reorder the operations of floating point operations in such a way that would break determinism. fixed-point arithmetic worked everything in sync. thx"
1047,A,"CFReadStreamHasBytesAvailable polling - best practices I'm currently polling my CFReadStream for new data with CFReadStreamHasBytesAvailable. (First some background: I'm doing my own threading and I don't want/need to mess with runloop stuff so the client callback stuff doesn't really apply here). My question is: what are accepted practices for polling? Apple's documentation on the subject doesn't seem too helpful. They recommend to ""do something else while you wait"". I'm currently just doing something along the lines of: while(!done) { if(CFReadStreamHasBytesAvailable(readStream)) { CFReadStreamRead(...) ... bla bla bla } else { usleep(3600); // I made this up sched_yield(); // also made this up continue; } } Is the usleep and the sched_yield ""good enough""? In there a ""good"" number to sleep for in usleep? (Also: yes because this is running in my own thread I could just block on CFReadStreamRead - which would be great but I'm also trying to snag upload progress as well as download progress so blocking there wouldn't help...). Any insight would be much appreciated - thanks! you don't have a separate thread for upload and download? Each request effectively has its own thread. maybe there is some requirement I'm not understanding but if upload and download are assigned to their own threads then does the problem go away? i.e. then you just have one thread reading the stream one writing it each blocking. however you need to synchronise them. There's no need for two threads I can get all the info I need from the ReadStream (I don't need/have a write stream). I'm polling the ReadStream from a background thread the question is what is a ""good"" way to poll? When doing manual CFStream based connections on a separate thread (for custom things like bandwidth monitoring and throttling) I use a combination of CFReadStreamScheduleWithRunLoop CFRunLoopRunInMode and CFReadStreamSetClient. Basically I run for 0.25 seconds and then check stream status. The client callback also gets notified on its own as well. This allows me to periodically check read status and do some custom behavior but rely mostly on (stream) events. static const CFOptionFlags kMyNetworkEvents = kCFStreamEventOpenCompleted | kCFStreamEventHasBytesAvailable | kCFStreamEventEndEncountered | kCFStreamEventErrorOccurred; static void MyStreamCallBack(CFReadStreamRef readStream CFStreamEventType type void *clientCallBackInfo) { [(id)clientCallBackInfo _handleNetworkEvent:type]; } - (void)connect { ... CFStreamClientContext streamContext = {0 self NULL NULL NULL}; BOOL success = CFReadStreamSetClient(readStream_ kMyNetworkEvents MyStreamCallBack &streamContext); CFReadStreamScheduleWithRunLoop(readStream_ CFRunLoopGetCurrent() kCFRunLoopDefaultMode); if (!CFReadStreamOpen(readStream_)) { // Notify error } while(!cancelled_ && !finished_) { SInt32 result = CFRunLoopRunInMode(kCFRunLoopDefaultMode 0.25 NO); if (result == kCFRunLoopRunStopped || result == kCFRunLoopRunFinished) { break; } if (([NSDate timeIntervalSinceReferenceDate] - lastRead_) > MyConnectionTimeout) { // Call timed out break; } // Also handle stream status CFStreamStatus status = CFReadStreamGetStatus(readStream_); if (![self _handleStreamStatus:status]) break; } CFRunLoopStop(CFRunLoopGetCurrent()); CFReadStreamSetClient(readStream_ 0 NULL NULL); CFReadStreamUnscheduleFromRunLoop(readStream_ CFRunLoopGetCurrent() kCFRunLoopDefaultMode); CFReadStreamClose(readStream_); } - (void)_handleNetworkEvent:(CFStreamEventType)type { switch(type) { case kCFStreamEventOpenCompleted: // Notify connected break; case kCFStreamEventHasBytesAvailable: [self _handleBytes]; break; case kCFStreamEventErrorOccurred: [self _handleError]; break; case kCFStreamEventEndEncountered: [self _handleBytes]; [self _handleEnd]; break; default: Debug(@""Received unexpected CFStream event (%d)"" type); break; } } Isn't handling the status yourself kind of antithetical to the purpose of runloops? It's supposed to notify you if case of events right?  I think this question is a bit of a paradox because you're asking what the best practices are for doing something that's intrinsically not a best practice ;) When there's a perfectly good method for blocking on network I/O any compromise that causes you to poll instead is by definition not the best practice. That said if you do poll I think it might be more appropriate to ""run the runloop until date"" on your thread instead of using whatever posix sleep or yield method you're imagining. Remember that each thread gets its own runloop so essentially by running the runloop you're allowing Apple to employ its concept of best practices for blocking until a future date. As for the time delay I don't know if you'll get a definitive answer for what a good time is. It's a tradeoff between peppering the CPU with polling cycles vs. being stuck in the runloop for a little while when I/O is ready to be read from the network. Ideally I think I would refocus your efforts on making this work using I/O blocking calls but if you stick with the poll & idle technique don't fret too much about the specific delay time. Just pick something that works and doesn't seem to impact performance negatively in either direction. (Also I'd like to clarify that I'm not too religious about the polling vs. blocking thing I'm only stressing its value because you're obviously in search of an elevated solution). Yep you're right. I suppose the best way to do this then is to use a runloop and schedule a timer to periodically check upload status. (Or use CFRunLoopRunInMode to force the runloop to hand back control to me periodically - overhead difference?). Either way the whole reason I'm doing this is completely up in smoke (unless I can come up with a fix). No way to accurately grab upload progress for small transfers: https://devforums.apple.com/thread/9352?tstart=0 You're certainly not wrong :). I'm using plain ol' threads no runloops associated with them. I figured I'd avoid the overhead of a runloop (negligible?). I also figured polling was going to happen at some level in the system so I might as well do it myself - though I might be wrong about that. I think you are wrong about the presumption that polling will happen at some lovel in the system. The way the runloop-based CFStream stuff works is at some point where the network I/O is actually being waited on it literally becomes a hardware-driven wakeup. So there's no polling just waiting."
1048,A,How do I construct the packets for this UDP protocol? Valve Software's Steam Server Query protocol as documented here allows you to query their game servers for various data. This is a little out of my depth and I'm looking for a little guidance as to what I need to learn. I'm assuming I'll need socket and struct correct? I'm comfortable with basic UDP tasks like these so I guess my main question is how do I construct my data with struct as I'm completely unfamiliar with it.? I found an answer to my own question. Yay. SRCDS.py has this implemented already and I figured it out by looking it over.
1049,A,perl readdir() on network directory - scalar vs list context and network performance We have a network share with a large number of files and are going to iterate over all files via perl readdir(). Which is more efficient with network communication (or are they the same)? 1) readdir in list context? 2) readdir in scalar context? Usually I would avoid the list context to avoid having to store the entire list in memory but if it's going to hit the share once in list context and many times in scalar context then I'd rather go with the list context. Thanks! -Peter Below is some code to benchmark the difference: use Benchmark; my $dir_name = 'a network path'; timethese(-2 { scalar => sub { opendir(my $dir $dir_name); while(my $entry = readdir($dir)) { } } list => sub { opendir(my $dir $dir_name); my @entries = readdir($dir); } }); The results were quite similar on my machine/network (WinXP and a distant share) however a bit in favor of scalar version.
1050,A,Patterns for Multithreaded Network Server in C# Are there any templates/patterns/guides I can follow for designing a multithreaded server? I can't find anything terribly useful online through my google searches. My program will start a thread to listen for connections using TcpListener. Every client connection will be handled by it's own IClientHandler thread. The server will wrap the clientHandler.HandleClient in a delegate call BeginInvoke and then quit caring about it. I also need to be able to cleanly shutdown the listening thread which is something I'm not finding a lot of exampes of online. I'm assuming some mix of lock/AutoResetEvents/threading magic combined with the async BeginAceptTcpClient and EndAcceptTcpClient will get me there but when it comes to networking code to me it's all been done. So I have to believe there's just some pattern out there I can follow and not get totally confused by the myriad multithreaded corner cases I can never seem to get perfect. Thanks. Take a look at this previous question: http://stackoverflow.com/questions/32198/how-do-you-minimize-the-number-of-threads-used-in-a-tcp-server-application It's not strictly C# specific but it has some good advice.  Oddly enough you may find something on a Computer Science Assignment CSC 512 Programming Assignment 4: Multi-Threaded Server With Patterns. Altough it's C++ voodoo but the theory is quite understandable for someone who can do C#. Acceptor/ Connector Monitor Object Thread Safe Interface Wrapper Facade Scoped Locking Strategized Locking Reactor Half Sync/Half-Async Leaders/Followers Altough you can get the whole list of nice readings on the main page. This looks to be very helpful. Thanks! Looks like I'll be spending some time reading today.
1051,A,"Private Network (as in IPv4) question I am working on an embedded TCP/IP4 stack and HTTP/SNMP/SMTP stuff. It functionally works but I want to make it work faster on LAN. Because of the Nagle's Algorithm and the delayed TCP-ACK The HTTP application seems to work slow even on LAN. As can be seen on http://en.wikipedia.org/wiki/IPv4#Private_networks  There are 3 different Private Networks with different bit-block values. What I will do is that: I will first be sure that I am a LAN member by looking at my own IP I will look at the dst_ip and check if it belongs to the same LAN as me Are these enough to prove that me and the other party belong to the same LAN ? Then of course I will use a simple hack like sending the same packet twice to speed up the communication. I already tested this and it works but it's optional right now. I want to turn it into a built-in feature. Thanks in advance... Although answers that suggest measuring latency are very promising; for this particular system it is not feasible (very low-end embedded system barely functioning). Thank you for your answers... You need to check your subnet mask to know if a destination IPv4 address is on the same network as you. Only addresses with the same prefix will share the same network for sure. For instance if your IP address is 10.2.3.87 and your subnet mask is /26 that means that addresses between 10.2.3.64 and 10.2.3.127 are on the same subnet as you because they will all use the same prefix 10.2.3.64. The /26 subnet mask is 255.255.255.192. Of course you will do all this in binary so you will not be checking to make sure the address is between a min and a max value instead you will use the subnet mask to get the prefix of your address and the destination address then check that they are equal. In binary: 10.2.3.87 00001010 00000010 00000011 01010111 10.2.3.64 00001010 00000010 00000011 01000000 255.255.255.192 11111111 11111111 11111111 11000000 As you can see the AND mask has 26 one bits (/26) and therefore the two IP addresses share the same prefix when the subnet mask is applied. It is possible but not common for other subnets to reside on the same LAN broadcast domain so in odd circumstances your performance trick will not work. If you use the subnet mask to identify the closeness of the destination IP address then you may not need to worry about RFC 1918 private addressing. Thank you using subnet_mask looks better...  It might be more robust to detect ""low latency"" connections rather than ""LAN connections"". Maybe your application/stack could measure latency in some way then switch to the alternative algorithm.  Those Addresses are in use in Internal Networks (Businesses and homes). they can't be use in the Public Internet. I think it will be ok. In fact RFC 1918 addresses can be used on the Public Internet and are in use on the Public Internet. They shouldn't be of course but the only thing that stops them from being used are traffic filters on every network that uses them. It seems that lots of networks don't filter the traffic and it leaks onto the Internet. See this URL for an example of some of the networks leaking this so-called Martian traffic: http://www.ris.ripe.net/historic-reports/martians/2009/01/20090101.html @Michael Dillon: This is the first time I hear this ""Martian Traffic"". I think this is caused by low-quality adsl-modems. No big network admin would do such mistake right ?  You're (probably) better off measuring latency than looking at IP addresses. Just because two IPs do not live within the same subnet does NOT mean they have high latency between them. Just because two IPs happen to live within the same IP range does not mean taht the have low latency (consider two ethernet segments linked by a WAN bridge across a 64 kbps line; maybe unlikely in this day and age but I have certainly worked with networks where links like that were common). However checking that two IPs are within the same subnet is certainly a good approximation.  As long as you're sure nobody's using VPN connections you're probably fairly safe. A computer connected via a VPN connection however will (at least normally) have an IP address that's local to your network even though their connection may have quite (or even extremely) high latency. Thanks for the extra info..."
1052,A,Finding the type of data in IPv4 packet Excluding options field in IPv4 header after 20 bytes of header data follows. That data may be TCP packet or UDP etc. Now given a IPv4 packet (with header and data) How to find out which type of transport layer packet (TCP/UDP/etc.) is present in data? Actually I am parsing a IPv4 packet so I need to understand this. The protocol field of the IPv4 header (see RFC791) will tell you:  0 1 2 3 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ |Version| IHL |Type of Service| Total Length | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Identification |Flags| Fragment Offset | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Time to Live | Protocol | Header Checksum | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Source Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Destination Address | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ | Options | Padding | +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+ The protocol numbers are assigned by IANA and are listed here: http://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml Some example protocol numbers are: 1 ICMP 6 TCP 17 UDP  Deep packet inspection? Ipoque release some open source code for this task: opendpi. Actually I just found out that in the header of IPv4 there is a field called protocol. In that the protocol is specified. Anyways I got to know a new library.
1053,A,"What is ""Sim blocking"" (seen in tomcat doc)? I saw the following description in the official tomcat configuration documentation (APR connector description omitted):  Java Blocking Connector Java Nio Blocking Connector Classname Http11Protocol Http11NioProtocol Tomcat Version 3.x 4.x 5.x 6.x 6.x Support Polling NO YES Polling Size N/A Unlimited - Restricted by mem Read HTTP Request Blocking Non Blocking Read HTTP Body Blocking Sim Blocking Write HTTP Response Blocking Sim Blocking SSL Support Java SSL Java SSL SSL Handshake Blocking Non blocking Max Connections maxThreads See polling size What does ""Sim Blocking"" mean? Just a guess but it could stand for simulated blocking meaning a blocking api wrapped around the underlying non-blocking nio api. While the answer seems true I'd rather vote up the post with the reference ;-)  According to Filip Hanik a Tomcat committer it means ""simulated blocking"". (Reference: Tomcat User Mailing list post) So it is the servlet spec which forces this: ... ""simulated blocking"" to accomodate the current servlet spec... @ChristopheRoussy For the 2.x specs which Tomcat is the reference implementation yes. The ""current"" spec 3.x allows for async but Tomcat is not the reference implementation there. Note the Filip Hanik quote is from early 2008 and JSR 315 (the 3.x spec) was not released until late 2009."
1054,A,"What is a socket physically? I always prefer the pyhsical meaning of a programming concept to its logical meaning. So here comes this question. As I review the socket programming paradigm I noticed that what the bind() connect() functions do are just like tuning the socket created by the socket() function. So I guess that what the socket() function does is just creating a data structure (and possibly a data structure in the kernel space) to hold the details about the end-to-end communication settings between the client and the server. And bind() connect() just fill in that data structure. I am not familiar with the implementation of the socket API so I hope someone could address my concern. It's the hole in the skull where your eyeball goes. Oh were you talking about a different kind of socket? Never mind... I am not a native english speaker what does ""hole in the skull"" mean? Thanks. ""skull"" is the part of the body above the chest and torso. It houses the brain. The Eye sockets are the two openings on the face that house the eyes. He was making a joke. @San Jacinto thanks I thought it was some kind of proverb. you've put a bounty on this question but San Jacinto has given you a link to the actual implementation of the IP stack for Linux systems and I've explained how what is physically put on the wire corresponds to what is passed to these functions and we have both explained that the implementation details will vary from system to system. If neither of these are what you are looking for perhaps you should add more detail about what exactly you are looking for. @BoltClock _physically_ not _physiologically_ ;) You don't appear to know what ""physically"" means. Physically it doesn't exist at all other than perhaps being represented by some electrons on a wire. This is why you got the joking responses. Logically you have detailed answers below. My 2 cents worth: A socket is basically consistent of a source IP source Port destination IP and destination Port. (Physically it really isn't anything this is a software based concept) This way the operating system can tell which Application (through it's port number) needs to receive the packets or is sending out the packets. The network stack (TCP/IP or OSI model) is implemented differently depending on your OS. If you want to learn more how packets are transferred and processed you study the OSI model or TCP/IP stack. This will tell you what happens to information as it leaves your application to be sent across a network. The OS deals with handling the packets so if you're a programmer as said before you're normally not interested in these details. If you're curious and want to know just like I do I suggest you start up and reading. :) Someone downvoted and did not say why? A connected socket is that 4 tuple but a generic ""socket"" isn't necessarily. It sounds like you're thinking mostly in terms of TCP and in that context everything here is right – except maybe that TCP does specify several parameters that aren't supposed to vary by system. (Wasn't my downvote of course but perhaps they were thinking along those lines?)  This is highly platform-dependent. The point of the API is so that you DON'T need to know these details. If you're really interested in learning this (which you shouldn't be for just applications and system applications programming) you can download a linux kernel source archive from kernel.org and examine Linux's TCP/IP implementation by looking under net/ipv4 To add some clarity To transport data across the network we usually adhere to standards defined by the International Standards Organization. They have a standard called the OSI or Open Systems Interconnection model. This model defines 7 layers of abstraction for applications to move data across a network. I'll only talk about the first 4 as they are the the pertinent ones for your question. Physical Layer: This layer defines how the data is actually transmitted over the media. Hardware vendors adhere to defined standards on how to move the data. The standards agree on electrical signals and the electronic aspects of the data moving. How it fits into the system: Hopefully there's very little software support required for this layer. Whatever programming is done here is likely to be done on-module and not in the kernel or application. Data Link Layer: This is the first layer that arguably involves some sort of programming. This layer defines the line-level protocols that operate on the physical links. Ethernet is one protocol. Frame relay is another. Token Ring is another. Each end of the link must be running the same data link protocol. This layer combines a compatible physical layer standard to give a means to actually transfer data from one host to another. In many regards it can be thought more of an appendix to the physical layer rather than its own layer but because link-level protocols are defined here that's not a great analogy. This layer gives physical addresses to nodes on a network. How it fits into the system: You'd need to write a driver to talk to the interface module that runs these data-link protocols. Depending on the module and the system the module may have all that it needs to actually work or it may need some system-level help. Ideally you just create a set of code interfaces (perhaps implemented as structs that contain function pointers for the appropriate handling of I/O.. I don't really know) and when you install a new physical module a driver need only to implement those code interfaces and now your physical module is usable. Network Layer This is the layer that provides the ability to move data between networks (in the case of TCP/IP). The Internet Protocol is defined at this layer. This layer gives logical addresses to nodes so that they can be grouped into networks. By knowing what network (also called a subnet determined programatically using the subnet mask) the host is on we run algorithms that correctly move data from one network to another. If one host is on network A in China and one host is on network B in Australia algorithms at this level are in charge of providing a path that links these networks and therefore these hosts. An important thing about programming for this layer is that you should be able to just ""plug in"" any data link layer to transmit over. This means that once you create code on your system to transmit over Ethernet Token Ring 3G or Frame Relay that you should be able to use all of them without the network layer needing to know what data link technology it is using. The logic of moving data between networks should not depend on the actual physical link it is operating on. This layer puts your data into packets and packets are what are routed over the internet. How it fits into the system: All of this layer must be coded as part of the system. It is entirely a software construct and should be isolated as much as possible from the data link layer. I am not enough of an expert to say in practice how well this is accomplished. Because the functionality of this layer is system-defined we have total control over what the software must support. This makes the construction of the code interfaces that allow using this layer by higher-layer protocols rather simple compared to the ones in the data link layer. Transport Layer: This layer defines segmentation of data (because if you just sent giant pieces of data all at once hardly anything would make it in order). This layer also defines TCP which provides hand-shaking checksums packet ordering variable data window sizes and guaranteed reliabilty. TCP gives you the ability to create multiple logical channels of communication over the same physical link. It differentiates one coversation on a link from another conversation on the same link. UDP is also defined at this level and can be thought of as an extremely light-weight TCP. UDP provided almost none of the beneifts of TCP but still provides the physical channel multiplexing. If your transport layer is written well your applications don't need (speaking from a code architecture standpoint) to worry about whether the transport layer is using TCP or UDP (just mentioning these two b/c the yare most popular on IP). While you may pick one or the other based on timing performance needs or reliability needs (and in practice applications often make an assumption about which one they are running) your application doesn't need to have exact knowledge of which one is running. Because this layer is built on top of the network layer we don't need to worry about how our data will get from one host to another if they are on different networks. If a router is running a standard routing protocol augmented by some statically-defined routes we don't need to worry about that. It's all taken care of for us by the network layer. If the network-layer configuration changes on the host that we are running it doesn't matter. We don't need to change our entire application to account for this. How it fits into the system: Very similar to network layer except it provides different functionality than does the network layer. Additionally these interfaces are used more in user-space than are the network layer interfaces. This is the layer that actually defines the sockets that you use in TCP/IP networking. Hope this helps and you can understand why your question is a little confusing to most of us. Sometimes programmers are just curious. @the_drow No doubt. It's more of a warning not to rely on such structures than it is a ""don't look here!!"" type of thing. @AndreasGrech updated. The link is not valid anymore.  Are you familiar with the OSI model? bind() specifies the local IP address and port (layer 3) to use so when the packet is physically sent out it specifies that IP address as the sender and connect() specifies the remote IP address and port to physically place in those packets. As an aside a lot of programming is pure ""logic"" and doesn't really have a ""physical"" meaning unless by ""physical"" you actually mean ""implementation detail"" which will vary from platform to platform. If you're actually asking about the physical implementation meaning how ""meaning"" is transformed into electrical signals you would probably be happier as a computer engineer than as a programmer."
1055,A,"Read function in Network Programming in C I use this code for reading from socket : int n ; char buffer[256]; n = read(newsockfd buffer 255); if (n < 0) { error(""ERROR reading from socket""); } if the data that must be read bigger than 255 byte (for example 1000) which change must be occured ? I know change char buffer[1000] I need different solution . I think the correct code depends on how would you like to process incoming data how to handle signals how socket timeouts and is `newsockfd` in blocking or nonblocking mode? Does changing the third parameter of read to 1000 work or not? i.e. char buffer[1001]; read(newsockfd buffer 1000); If the read function does not accept a count that large you may read it multiple times. In the loop keep track of how much characters you have read so far and use read(newsockfd buffer+m 1000-m);  Have you checked errno? http://linux.die.net/man/2/read If you need more data use a loop no?  Use something like: char buffer[256]; memset(buffer 0 sizeof(buffer)); char* pBuf = buffer; int bytesLeft = sizeof(buffer) - sizeof(char); while(bytesLeft > 0) { int n = read(newsockfd pBuf bytesLeft); if (n < 0) { error(""ERROR reading from socket""); break; } if(n == 0) { error(""peer shutted down""); break; } pBuf += n; bytesLeft -= n; } +1 but...it would be better if there were a 'sizeof' in sight - 255 ==> sizeof(buffer)-1 I assume (consistent with the question). I'm not clear why there's a byte left unused; presumably for a terminal null in a string. Also the Q asked about 'more than 255 bytes' but the answer doesn't do that. The 'while(true)' condition would be better as 'while (bytesLeft > 0)' and you can avoid the last condition in the loop. Should you worry about 'n == 0' (EOF - other end of socket gone away)? How will this read more than 255 bytes? This code will read a maximum of 255 bytes; just that that it will read it in chunks. May be I am missing something. Please blast me if I am :) `char pBuf = buffer;` should be `char *pBuf = buffer;` or I don't think it would even compile. @Jonathan Leffler You're right on all accounts. I'll correct code. @billyswong yes that was a typo. thx  Just read several times from the socket until you got all the data you want to receive. For example to receive 1000 bytes it could look like this (on success read returns the number of bytes read): int received = 0; while (received < 1000) { n = read(newsockfd buffer 255); // error checking... // do something with the partial data in ""buffer""... received += n; } in a loop perhaps? How to use loop what is loop's variable I added an example. What exactly to do with the data in `buffer` in each iteration depends on how you want to process further what you are receiving.  Adding to the solutions above you can even read any number of bytes (assuming you don't have an idea beforehand how many Bytes you are expecting to be sent on the socket) by allocating some more memory to your buffer in which you are reading whenever you reach the maximum limit of the same. Yes  can reallocate the same buffer array. you mean with a realloc()?"
1056,A,"iPhone: Looking for an API to store survey results In my iPhone app I am trying to gather feedback on my game via an optional survey. I would like to send this data to a central server where I could gather all the statistics and process them to have a bar chart or other convenient presentation. The data being sent an integer representing the user's preference. ie) Given ""How did you like our game?"" the user could select from a button from :) :\ :( . Eventually I would also like to have audio feedback where the user could record their voice. The variable length sound clip would be sent and stored from an iPhone to some server for playback later. Has anybody done something similar? Google Spreadsheets offers an API which I can't make heads nor tails of but I think it will support a tally of responses. ie) I received 10 goods 4 neutral and 7 poor ratings. As far as I can tell I wouldn't be able to store a sound clip in a spreadsheet. I also looked into Google App engine but if possible I'd like to avoid the networking code to transmit the survey data and the server side scripts to interpret the data. Can somebody recommend a simple API that I could use to store integers for viewing? Ideally I'd like to say something simple like if (response == ""Good"") MyGoogleSpreadsheet.MyAppStats.Increment(positiveResponseTallyCell); Later on I'd just log into Googlespreadsheets and the results would be there for me to read. By the way I don't have to necessarily use a spreadsheet. I just want this to be easy. Bonus points if the API supports the storage and playback of sound but integers will suffice for now. I know you said you've already taken a look at the Google Spreadsheets API but you may not have seen the GData Objective-C framework which includes support for reading/writing to and from Google Docs spreadsheets (via the GDataServiceGoogleSpreadsheet class).  You can do this with the Flurry API (flurry.com) which lets you specify which custom events to log and gives you charts and downloadable CSV. Very easy to use: [FlurryAPI startSession: flurryID]; [FlurryAPI logEvent:@""QUESTION1"" withParameters:@""Good""]; Although Flurry is made for general usage statistics gathering I think it would be a quick and easy way to accomplish what you want."
1057,A,Static matrix or dynamic allocated matrix I want to model a purification of the Petri Network. And i was given the suggestion to use a matrix which is allocated dynamically. After thinking about the problem i came up with a different approach like so: A static matrix of n transitions and p locations and a function which returns the purified matrix from the static matrix. Which approach is the safest and the best? The static implementation or the dynamical one? In terms of safety the static implementation is much less likely to leak memory. The dynamic one is more flexible though. If you're thinking about this issue it's probably best to just go with the dynamic solution.  A dynamically allocated matrix is better: can be resized to suit the input dimensions you pay only for what you need though you have the burden of managing the memory all by yourself.
1058,A,"How Many Network Connections Can a Computer Support? When writing a custom server what are the best practices or techniques to determine maximum number of users that can connect to the server at any given time? I would assume that the capabilities of the computer hardware network capacity and server protocol would all be important factors. Also would you consider it to be a good practice to limit the number of network connections to a certain maximum number of users? Or should the server not limit the number of network connections and let performance degrade until the response time is extremely high? In general modern servers can handle very large numbers of concurrent connections. I've worked on systems having over 8000 concurrently open TCP/IP sockets. You will need a high quality servicing interface to handle that kind of load check out libevent or libev.  This really depends on your operating system. Different Unix flavors will support ""unlimited"" number of file handles / sockets others have high values like 32768. A typical user limit is 8192 but it can usually be set higher. I think windows is more limiting but the server version may have higher limits.  I think you shouldn't limit the number of connections your server will allow - just catch and handle properly any exceptions that might occur when accepting and closing connections and you should be fine. You should leave that kind of lower level programming to the underlying OS layers - that way you can port your server easier etc.  Dan Kegel put together a summary of techniques for handling large amounts of network connections from a single server here: http://www.kegel.com/c10k.html  That is a good question and it definitely is situational. What is your computer? Do you have a 4 socket machine filled with Quad Core Xeons 128 GB of RAM and Fiber Channel Connectivity (like the pair of Dell R900s we just bought)? Or are you running on a p3 550 with 256 MB of RAM and 56K modem? How much load does each connection place on your server? What kind of response is acceptible? These are the questions you need to answer. I guess the best way to find the answer is through load testing. Create a unit test of the expected (and maybe some unexpected) paths that your code will perform against your server. Find a load testing framework that will allow you to simulate 10 100 1000 10000 users performing those tasks at the same time. That will tell you how many connections your computer can support. The great thing about the load/unit test scenario is that you can put in response time expectations in your unit tests and increase the load until you fall outside of your response time. If you have a requirement of supporting X number of Users with Y second response you will be able to demonstrate it with your load tests.  One of the biggest setbacks in high concurrency connections is actually the routers involved. Home user oriented routers usually have a small NAT table preventing the router from actually servicing the server the connections. Be sure to research your router/ network infrastructure setup just as well."
1059,A,Java me : Can we retrieve bluetooth address of connected device from an open slave connection? Here is a typical sequence of events that occur : Host device opens a service ( Host device accepts and opens all incoming connections) A remote device connects to host device. Now we have a slave connection open at host device. At host device I want to know the bluetooth address of remote device. I can always pass it as data from remote to host device but can I extract it from connection object somehow without any data transfer? Thanks in advance... Vivart's answer is right however it implies you haven't looked at the JSR82 docs at all. There are relatively few methods in this API but there is a method called RemoteDevice.getBluetoothAddress() fairly obvious what this is going to do no? i think this will help you // retrieve the device that is at the other end of // the Bluetooth Serial Port Profile connection // L2CAP connection or OBEX over RFCOMM connection RemoteDevice remote = RemoteDevice.getRemoteDevice( javax.microedition.io.Connection c); // retrieve the Bluetooth address of the remote device String remoteAddress = remote.getBluetoothAddress(); // retrieve the name of the remote Bluetooth device String remoteName = local.getFriendlyName(true);
1060,A,"What is the Difference Between read() and recv()  and Between send() and write()? what is the difference between read() and recv()  and between send() and write() in socket programming ? performance and speed and other behavior. I just noticed recently that when I used write() on a socket in Windows it almost works (the FD passed to write() isn't the same as the one passed to send(); I used _open_osfhandle() to get the FD to pass to write()). However it didn't work when I tried to send binary data that included character 10. write() somewhere inserted character 13 before this. Changing it to send() with a flags parameter of 0 fixed that problem. read() could have the reverse problem if 13-10 are consecutive in the binary data but I haven't tested it. But that appears to be another possible difference between send() and write(). +1. See also [winsock not supporting read/write](http://stackoverflow.com/q/4778043)  Per the first hit on Google read() is equivalent to recv() with a flags parameter of 0. Other values for the flags parameter change the behaviour of recv(). Similarly write() is equivalent to send() with flags == 0. This isn't the whole story. `recv` can only be used on a socket and will produce an error if you try to use it on say `STDIN_FILENO`. This thread is now the first hit on Google Google loves stackoverflow  ""Performance and speed""? Aren't those kind of ... synonyms here? Anyway the recv() call takes flags that read() doesn't which makes it more powerful or at least more convenient. That is one difference. I don't think there is a significant performance difference but haven't tested for it. Perhaps not having to deal with flags may be perceived as more convenient.  read() and write() are more generic they work with any file descriptor. However they won't work on Windows. You can pass additional options to send() and recv() so you may have to used them in some cases.  The only difference is that recv/send let you specify certain options for the actual operation . read/write are the 'universal' file descriptor functions while recv/send are slightly more specialized (for instance you can set a flag to ignore SIGPIPE or to send out-of-band messages...)."
1061,A,"Best way to send continuous data in Java using Netty I'm planning to use Netty to design a TCP Server. When the client connects I have to immediately start pumping XML data to the client continuously...for hours/days. Its that simple. So I override ""channelConnected"" method and send data from that method right?...thats great. I will be using the following ChannelFactory ChannelFactory factory = new NioServerSocketChannelFactory( Executors.newCachedThreadPool() Executors.newCachedThreadPool()); NioServerSocketChannelFactory documentation says A worker thread performs non-blocking read and write for one or more Channels in a non-blocking mode. Good. According to effective Java Item 51: Don't depend on the thread scheduler I want the worker thread to do a ""unit of work"" and then finish/return. So in my case though I have to send data continuously I want to send some chunk (lets say 1 MB) and then be done (unit of work completed) so that worker thread can return. Then I'll send another 1 MB. Below example is from the official guide of Netty HERE. I guess the question is then in this scenario if i had to unconditionally keep sending time to the client how would I do it considering each send as a unit of work. One way of doing it would be to just put a while loop and do a Thread.Sleep. Any other way?  package org.jboss.netty.example.time; public class TimeServerHandler extends SimpleChannelHandler { @Override public void channelConnected(ChannelHandlerContext ctx ChannelStateEvent e) { Channel ch = e.getChannel(); ChannelBuffer time = ChannelBuffers.buffer(4); time.writeInt(System.currentTimeMillis() / 1000); ChannelFuture f = ch.write(time); f.addListener(new ChannelFutureListener() { public void operationComplete(ChannelFuture future) { Channel ch = future.getChannel(); ch.close(); } }); } @Override public void exceptionCaught(ChannelHandlerContext ctx ExceptionEvent e) { e.getCause().printStackTrace(); e.getChannel().close(); } } Doing a while/sleep would work but would not be in the Netty super-scalable style. It would be thread-per-connection programming. Instead schedule a periodic job on an Executor that writes a message to the channel."
1062,A,"Is the packets loss caused by LAN or WAN? Its very confusing any idea? I have a ISP provider (Telenet). Who provides via DHCP public ip to there DSL modem. From that modem we have RJ45 cable connected to a 24 port switch. Now in our local network: - from switch port 1 we have one voip phone - from switch port 2 we have another voip phone Each has default gateway 78.21.232.1 and subnet mask 255.255.240.0. But different public ip such as 78.21.235.x or 78.21.232.x series. Question/confusion: When i send packets from our local network to our local public ip's is the traffic going to ISP default gateway? Or its straight inside our Switch network? But i saw many times i gets packet loss in those voip phone diagnostics. Making me completely confused now. You've got several places to look for more information about where your packets will go. The first is your routing tables: $ ip route 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.121 metric 1 169.254.0.0/16 dev eth0 scope link metric 1000 default via 192.168.0.1 dev eth0 proto static This shows which interfaces (virbr0 eth0) will be used for packets destined to which CIDR ranges. The default entry at the bottom is used for everything that doesn't match one of the more specific routes. Since my LAN is 192.168.0.0/16 then I would expect all packets sent to hosts on my LAN to not go through a gateway device (the via ... in the last entry). The arp tool can also help you find out where your packets will go: $ arp -n Address HWtype HWaddress Flags Mask Iface 192.168.0.146 ether 00:06:7f:27:45:80 C eth0 192.168.0.1 ether 00:0f:66:4c:01:f8 C eth0 My machine knows two other MAC addresses on the network right now and can send packets to them directly. If the HWaddress is duplicated for several entries they are probably on the other side of a gateway or it is a machine with multiple IP addresses advertised on a single NIC. I'm not sure how to help with the packetloss problems; try moving switch ports swapping cables etc. Maybe you've got a bad port that can happen from time to time. Try to isolate which pairs of machines have packetloss problems. (And don't try to ping -f to Mac OS X machines they rate limit ICMP replies. It's funny once you know about it...) Packet loss is fixed by just enabling the embedded Video conferencing device Ethernet interface to settings to ""100Mb full duplex"" or ""Auto"". arp -a shows 78.21.232.1 (dynamic ISP default gateway) 78.21.235.2 (video device 1 type dynamic) 78.21.238.3 (video device 2 type dynamic) 224.0.0.2 (one of the video device  second lan interface type static). I am still confuse when video device 1 send/receive packets to video device 2 are they sending straight or there packets being switched from ISP default gateway to end points? @Iam if I read that correctly it looks like packets can be routed directly between the two VoIP devices. Of course they could be implemented to always send traffic upstream; can you do a quick test initiate a call between them and then yank the upstream cable for a few seconds? >:-> still works when i yank the ISP cable to the switch."
1063,A,"How can I solve an application networking issue? I have assisted with an application that has a couple of thousand deployments. This application is a .NET 2 Winforms application. There are a couple of features that require the application to ""phone home"" to some custom web services that we have written to update product information. For several years this has worked fine. The occasional support call we would get would be firewall related which we would fix by adding exceptions for out application. This year we are getting some calls where our application is unable to connect to our web services but is FOR SURE not a firewall related issue. Domain Name resolution is occurring and the page for the Web Service can be loaded into a browser. Nevertheless our APPLICATION is completely unable to connect to the web service. For the life of me I do not know how to solve this problem. Can anyone offer some suggestions on how to troubleshoot and debug this issue. Any guidance is welcome. I AM able to copilot in to some of these customers to get remote access to their machines. Seth So...any luck in figuring out the issue? Just curious... curtisk Thanks for asking. As of now...no luck. I even did the wireshare thing with one customer...but since I did it while connected to copilot the traffic was encrypted. At least there were NO references to the IP of our web services. Still working on it though. Thanks for your help. SEth This sounds like a pickle as other have mentioned you can always create a diagnostic app and see what kind of returns/ exceptions are being thrown. Here's a few thoughts... Is the application even calling out to the app server hosting the webservices? Does the services host see the incoming connections from these client PCs (check access logs)? Let's check from the client side you can simply use a command window on the client machine and use netstat -a command to see all TCP/UDP connections to and from the machine look for your webservice host in there ideally run this command close to when the app attempts it's call home.(can you trigger these ""phone home"" calls manually?) Another more involved approach is Wireshark setup a capture filter there to pick up only traffic to your backend server and if its not SSL you can even go as far as seeing the SOAP and/or XML that's going across the wire.(which helps if you are connecting but there is an issue with the envelope/message/format) Another possibility is proxy issues but if you are using default and they are getting to it through browser that's not as likely. Depending on how this app is coded can you show some code around where these calls are getting made?  When you say it cannot connect do you mean it reaches the webservice (verified by IP/connection logs) but isn't able to retrieve the page or that it never reaches the webservice? If it is repeatable what about throwing together a little test app that just calls your webservice doing as much debug logging as possible and run that from one of these remote clients? (not sure what to tell you if it works.. but that could indicate something in itself. Perhaps test on another port.) No proxy within these client's network that IE is using and your service is not?  Could you build a small test application with a ton of debugging/logging information that you could get one of your clients that is having the problem run? If you write a test application that connects in the same way as your real application you could log every single step and exception to a log file and that might help you figure out what is going on. You'll need to find a customer that is willing to work with you but with a couple of thousand deployments there should be someone that is willing to give you a hand. If you post the piece of code that is doing the phoning home we might come up with some more specific ideas."
1064,A,"What is the purpose of the AI_V4MAPPED flag in getaddrinfo? The getaddrinfo call has many interesting flags. I'm wondering what the purpose of the AI_V4MAPPED flag is. On no system do I seem to be able to get getaddrinfo to produce ::ffff:n.n.n.n form addresses as I would expect when I set this flag. Am I expecting the wrong thing? Am I seeing bugs? In particlar if I ask for AF_INET6 family addresses and specify AI_V4MAPPED I would expect to see ::ffff:n.n.n.n addresses for hosts that only have DNS A (IPv4 address) records. And I would also generally expect that if I specified AI_ALL that I would get both a host's DNS AAAA (IPv6 address) records and DNS A records in ::ffff:n.n.n.n form. Again am I expecting all the wrong things here? I've tested this on Fedora 11 - glibc 2.10.1 and OS X 10.4. In my experience AI_V4MAPPED does not work on Mac OS X 10.6. If you provide hints.ai_family = AF_INET6 and hints.ai_flags = AI_V4MAPPED it will always return EAI_NONAME and gai_strerror() prints ""nodename nor servname provided or not known"". It works properly on OS X 10.7. Posting this here in case it helps someone even though you're on Fedora.  I get exactly what you're expecting to get on Debian Lenny (glibc 2.7) - with one exception - if I specify AI_V4MAPPED without AI_ALL and the hostname I look up has CNAMEs pointing at A records I don't get those returned. It works fine if AI_ALL is also specified or if the hostname is directly associated with A records. I don't know why - perhaps that's a glibc bug? Here's my test program: #include <stdio.h> #include <sys/types.h> #include <sys/socket.h> #include <netdb.h> #include <arpa/inet.h> int main(int argc char *argv[]) { struct addrinfo hints = { 0 }; struct addrinfo *res *res_c; int err; char name[INET6_ADDRSTRLEN]; if (argc < 2) { return 1; } hints.ai_family = AF_INET6; hints.ai_flags = AI_V4MAPPED | AI_ALL; err = getaddrinfo(argv[1] NULL &hints &res); if (err) { printf(""getaddrinfo: %s\n"" gai_strerror(err)); return 1; } for (res_c = res; res_c; res_c = res_c->ai_next) { const void *addr; int port; struct protoent *proto; switch (res_c->ai_family) { case AF_INET6: addr = &((struct sockaddr_in6 *)(res_c->ai_addr))->sin6_addr; port = ((struct sockaddr_in6 *)(res_c->ai_addr))->sin6_port; printf(""AF_INET6\t""); break; case AF_INET: addr = &((struct sockaddr_in *)(res_c->ai_addr))->sin_addr; port = ((struct sockaddr_in *)(res_c->ai_addr))->sin_port; printf(""AF_INET\t""); break; default: addr = NULL; printf(""(%d)\t"" res_c->ai_family); } proto = getprotobynumber(res_c->ai_protocol); if (proto) { printf(""%s\t"" proto->p_name); } else { printf(""(%d)\t"" res_c->ai_protocol); } switch (res_c->ai_socktype) { case SOCK_STREAM: printf(""SOCK_STREAM\t""); break; case SOCK_DGRAM: printf(""SOCK_DGRAM\t""); break; default: printf(""(?socktype?)\t""); break; } if (addr && inet_ntop(res_c->ai_family addr name sizeof name)) printf(""addr = %s"" name); if (addr) printf(""%d"" port); printf(""\n""); } return 0; } I've been doing a lot more research on this. getaddrinfo seems to be plagued with any number of bugs partly because it has a lot of weird cases to get around broken handling of AAAA requests. You answer is the closest to a real answer so you get the answer credit. AI_V4MAPPED without AI_ALL is a glibc bug see http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=503912"
1065,A,"How to delete a TcpChannel object in .NET I'm having some troubles with TcpChannel. I want to create a channel give remote access to an object let's say a server and after doing all this close the channel. The problem is that I might need to re-open the same channel later in the same port and I'm having some hard time trying to do this. for connection purposes I only do: var channel = new TcpChannel(port); Console.WriteLine(""Start Connection received at Server""); ChannelServices.RegisterChannel(channel false); //Initiate remote service as Marshal RemotingServices.Marshal(this ""Server"" typeof(Server)); then to close it i just do: Console.WriteLine(""Stop Connection at Server""); channel.StopListening(null); RemotingServices.Disconnect(this); ChannelServices.UnregisterChannel(channel); channel = null; After this if I try to create a new tcpChannel instance i get an exception saying that the tcpChannel connections are unique they must be on different ports. So how can i close the tcpChannel? :S Thanks in advance. @Paul Sasik I am having troubles because the tcpChannel is only closed after all connections are closed as well so I need to end all acesses to my remote object.I tought i could do that by doing Remoting.Disconnect(object) but it doesn't seems to work with already shared remoted objects i must do that by hand. I'll try to do that now. Thanks for all your help I was amased by all your support and how fast you helped us. Are you having trouble reopening it between different sessions of the same application or in the same session? If it's the latter you could just open the channel when the app starts and then close/unregister it when the app shuts down. your close code are working. recheck the logs you miss the ""Stop Connection at Server"" somewhere. Update: there the my log (no errors): Start Connection received at Server Stop Connection at Server Start Connection received at Server Stop Connection at Server there the implementation code:  private void button1_Click(object sender EventArgs e) { channel = new TcpChannel(port); Trace.WriteLine(""Start Connection received at Server""); ChannelServices.RegisterChannel(channel false); //Initiate remote service as Marshal RemotingServices.Marshal(this ""Server"" typeof(Server)); } private void button2_Click(object sender EventArgs e) { Trace.WriteLine(""Stop Connection at Server""); channel.StopListening(null); RemotingServices.Disconnect(this); ChannelServices.UnregisterChannel(channel); channel = null; } The code is working and the TcpChannel is being ""closed"". However when I try to do channel = new TcpChannel(oldPort); it throws a exception saying that ports in tcpChannel are exclusive. Somehow the tcpChannel ain't being closed... @matutano : no errors i closed and reopened without error (and when i am trying to open twice i am receiving your error) @matutano i mean its working when no remote servers connected so the problem is to disconnect remote servers first. I was just thinking about that that code really works and thats because there isn't any remote servers connected to it. I've tryied to do the same by running the set of servers with only one connected (so there wasn't any entity remotly connected) and it worked perfectly wasn't suposed RemotingServices.Disconnect to end all remote services?  If you just want to stop and start listening on the same port you need to explicitly call start listening. You can loose the last three lines of the code after StopListening and preserve and reuse the object until your application goes down. channel = new TcpChannel(port); channel.StartListening(data) Thanks for your reply.I really must close the tcp connection. I have a set of servers that are controled by another entity. Is this entity that orders this set of servers to open and close some connections. My problem appears when that ""master"" entity orders a server to open a tcpChannel that had been previously opened. I really must close connections.  You will need to set the channel property: exclusiveAddressUse to false."
1066,A,"How send raw ethernet packet with C#? Is there a way to send raw packet Ethernet to other host via C#? In Windows 7 if it makes difference. I just want send ethernet packet with some changed fields like MAC address What do you expect to happen on the other end? Raw too? Making your own protocol? What's wrong with the ones we all use your hardware knows how to route and your LAN admin knows how to support? SharpPcap and Pcap.net are the way to go. You need a WinPcap wrapper framework because Windows doesn't allow access to lower level protocol headers for 'security reasons'. WinPcap provides its own networking driver that allows you to bypass that restriction. Based on suggestion by Saint_pl: I found probably better solution - similar to SharpPcap. It's Pcap.Net - .NET wrapper for WinPcap. Now I can modify my packets whatever I want. I have some resources for you that maybe helpful. I don't try that solutions in Windows 7 but maybe it contains some good info to start. Raw Ethernet Packet Manipulation or mirror on CodeProject This purpose of this article is to explain how to send a raw Ethernet packet using C# on a Microsoft platform. A raw Ethernet packet is the complete Layer 2 network frame that is sent to the physical wire. Sending a frame like this allows you to manipulate the target and source MAC addresses and the Layer 3 protocol fields. Also some info on raw sockets (just in case you interesting too): Client (and Server) Sockets Communication take a look on whole chapter but here key parts: C# Raw UDP Socket Program Example C# Raw Socket Ping Program Example part A | part B All examples Not sending packets but maybe interesting: A Network Sniffer in C# SharpPcap - A Packet Capture Framework for .NET I think Raw Ethernet Packet Sending from CodeProject will help me. And btw I suppose it's one and only way that I was found to send ethernet packet with e.g changed mac address via C#. I found probably better solution - similar to SharpPcap. It's http://pcapdotnet.codeplex.com/ - .NET wrapper for WinPcap. Now I can modify my packets whatever I want. Thanx :) Raw ethernet manipulation is pretty low level. To complete your task at good level you certainly need some knowledge about Network Driver Interface Specification and understanding device drivers (and its API). Don't skip it. C# alone don't give you full power in that task. Welcome :) Glad to see that you have nice solution for your task. @NickMartyshchenko do i have to install winpcap to use pcap.net? @Gobliins Pcap.Net is a .NET wrapper for WinPcap so it's a managed interface to WinPCap and requires WinPcap installed to function properly. Here some details how to run it http://pcapdotnet.codeplex.com/wikipage?title=Using%20Pcap.Net%20in%20your%20programs&referringTitle=Pcap.Net%20User%20Guide  Socket server = new Socket(AddressFamily.InterNetwork SocketType.Stream ProtocolType.Tcp); IPEndPoint ip = new IPEndPoint(IPAddress.Parse(""10.25.184.11"") 4456); server.Connect(ip); byte[] sendData = new byte[] { 0 8 32 64 }; server.Send(sendData); //done. now let's listen for data byte[] receiveData = new byte[1024]; int receivedDataLength = server.Receive(receiveData); //if the response is a string message string stringData = Encoding.ASCII.GetString(receiveData 0 receivedDataLength); Console.WriteLine(stringData); Ah I see. Comprehension fail. This is a TCP packet not an ethernet packet... You're 2 layers too high on the stack ;) Also this would not work on Windows 7. [Only Server versions of windows allow use of raw TCP sockets](http://msdn.microsoft.com/en-us/library/windows/desktop/ms740548%28v=vs.85%29.aspx#LIMITATIONS_ON_RAW_SOCKETS) due to malware abuse in the past.  iphelper API has some low level stuff - but probably not quite as low as you want to get"
1067,A,"Java Socket class is lying regarding connection status We have a Java client that keeps a persistent socket connection open to a remote server. The client polls a DB table every 15 seconds and if there a new item it serializes and writes it to the socket. Before writing to the output stream I would like to check whether socket connection is still good. For the specific application logic doing this proactive check is simpler than catching an exception and reconnecting reactively. I used following code figure out which method can let me know when the connection is broken: LOG.debug(""Socket status: bound="" + _socket.isBound() + "" closed="" + _socket.isClosed() + "" connected="" + _socket.isConnected() + "" outputShutdown="" + _socket.isOutputShutdown() + "" inputShutdown="" + _socket.isOutputShutdown()); I briefly disable my network adapter and during the next polling as expected there was an exception while writing to the socket. However the debug statement printed the following: ""Socket status: bound=true closed=false connected=true outputShutdown=false inputShutdown=false"" I expected either closed to be true or connected to be false. What actual values I get seem to be a lie. Is there a way to reliably check the connection status of a socket? Not sure if it relates to the issue but in the sample code you're checking `isOutputShutdown()` twice and not checking `isInputShutdown()` at all They are not a lie you are just misinterpreting them. Java Network Programming by Elliot Rusty Harold offers the following gem: ""...To tell if a socket is currently open you need to check that isConnected() returns true and isClosed() returns false. For example: boolean connected = socket.isConnected() && ! socket.isClosed();"" Hope this helps. It is a good answer but bear in mind that the peer may become disconnected without sending a proper TCP closure. This is neither common nor exceedingly rare. Thus `connected` may be true but the next socket operation will fail. Thus you must still have some mechanism to deal with failure so the above line will probably not save you any coding. @GregS: you are quite right. I wonder why the authors of Socket's API chose not to provide a convenience method to help sort out whether a Socket is connected at all. Back to your answer in the meantime! It is because TCP has no way of supplying the answer. If the underlying transport can't provide the info then no API built on top of it can. @EJP You are right it is wrong and perhaps the correct answer is there is no way to tell due to limitations in TCP itself apart from failed read or write attempts. Elliotte Rusty Harolde is completely wrong about this and he is not in general reliable about aspects of this sort. `isConnected()` tells you whether you have ever connected the Socket. `isClosed()` tells you whether you have everclosed it. Neither of them changes in response to changes in the state of the *connection*. They are both about the *socket* not the *connection.* @Arvanem Exactly so fix your answer. Remove the wrong information and put in the right information.  I'm not 100% sure about this but I'm pretty certain that the underlying BSD Sockets API doesn't actually have a mechanism to determine whether or not a TCP stream is still open; having a read() or write() fail is a pretty reliable way to tell that the stream has been torn down.  Read the Socket class Javadoc carefully. isConnected is true if the socket was able to connect. The method name is a misnomer it would more accurate if it was hasConnected. Once the socket successfully connects it becomes true and stays true. Same thing for isBound. You have to try a socket operation and check for failure."
1068,A,how to create a vpn software I want to create an application which creates a VPN between some endpoints something like hamachi and i do not have a starting point. I haven't found any resource to explain how to create such a network application.I want to use c# because i have some experience with it. I really need some help anything that can put me on the right way. Thanks. Well what do you think you have to do already? This is a really broad question. :) Try this first http://en.wikipedia.org/wiki/Vpn for options then select which one you want then explore that option. This smells almost like a homework/class project sort of question to me. The complete lack of expressed initiative doesn't help. You might want to take a look at SSH tunneling and see if it solves your needs. or something else that is already made.. there is tons of VPN software that is free/opensource  There are a number of distinct elements of VPN software that you'll have to figure out: What technology/standard will your program use to provide the privacy? Some common ones are IPSEC L2TP PPTP SSH and SSL. Web searches ought to turn up rich information (including RFCs) on all of these. If you're doing this as a learning exercise rather than needing actual security you could also design your own. Are you implementing a client a server or both? What operating system(s) will you support? This affects what you need to do to convince it to route packets through your application. Do you plan to interoperate with software implementing some standard? Well i am doing this as a learning exercise. So choosing the standard to provide the privacy seems to be the first stage if i get that right. I will create(hopefully) both client and server and it will be made for windows. Thanks for your reply
1069,A,"Clients with Multiple IP Addresses - How? I have a squid transparent proxy that authenticates client based on IP address (the only way available). The intended setup is my client would sign up my php page logs the IP to MySql and Squid would authenticate based on the Ip in the database. This method works well under normal scenario but I noticed there is a problem because I keep seeing ""access denied"" in my Squid log. I realize some client would sign up using IP A but have a different IP at the same time. For example one client has an Ip of xxx.255.1.58 in Livezilla but xxx.255.1.30 in my Mysql database. This occurred in a single instance (Less than 2 minutes apart). How is this possible? I understand some clients might have dynamic IP but is this the explanation? Edit: I've added a sample Squid Log of an user Within 149 seconds the person used 10 Ip addresses to access my server and obviously got denied...Could this possibly be the case for dynamic IP? I don't believe dynamic IP changes this frequently. From my understanding IP address changes only if you reset the connection. xxx.255.1.20 xxx.255.1.62 xxx.255.1.75 xxx.255.1.86 xxx.255.1.41 xxx.255.1.48 xxx.255.1.75 xxx.255.1.32 xxx.255.1.92 xxx.255.1.65 There are lots of reasons why a client might have multiple IPs. It depends entirely on how they access the internet (Their ISP). HI my client had 10 different IPs within 149 seconds. Is there an ISP setup out there that gives up new IP on every HTTP request?  Any corporate user is likely sitting behind several proxies therefore potentially every request will appear from a different source IP address. Usually for HTTPS requests the proxy system is configured to use the same source address however for regular HTTP it is a free for all. @Nick Currently not so popular but expect it to rapidly rise with scarcity of IPv4 addresses. It could be then simply ISP level NAT rather than proxying which would also affect HTTPS. It is possible iPad/iPhone users on certain networks might be in a similar situation. Thanks Steve Such setup is not common within residential Internet users is it? My target audience is mainly residential users not corporate."
1070,A,Checksum validation in intranet vs internet applications While looking at messages in Wireshark I have noted that Checksum validation is always disabled. Is it like an obsolete requirement or does it apply to internet traffic only which goes outside the firewall of a company network? Also can someone please advice how it is set (eg. whether from an application or network card setting etc.) PS: The question might not be of practical significance. I am asking this to fill the large missing gaps in my poor network programming skills. I had heard that checksum validation is a major bottleneck for tcp communication but am surprised that it is disabled for all messages that I have seen This question is answered in the Wireshark FAQ. The upshot is that checksums are generally calculated by network cards and Wireshark often intercepts packets before they hit the hardware that does the actual calculation. Enabling validation for those packets results in a large number of errors so they disabled validation by default. More info is available via the link. Edit: just to address fruit's comment below I screenshotted a couple of TCP packets for comparison. The first one is a TCP packet without validation: You can see that there is a non-zero checksum there so it might appear that Wireshark (or some other pre-hardware app) has done the checksum for you. However when we turn validation on for this same packet.. Now we can see that this checksum wasn't valid in the first place. I can't find a source for this info but I think it's strong evidence that Wireshark is not populating that field for us; doing so would go against Wireshark's nature anyway. Instead I expect that this is just an uninitialized field in the packet - it takes more work to set a field to zero than to omit setting it at all. It's also worth noting that as time goes on more and more network stacks will be offloading checksumming to the hardware so there will be fewer and fewer cases of valid checksums coming from the local machine. as I understand now the checksum validation shown in wireshark is the validation done by wireshark application. checksum validation might still be happening on the network card hardware for tcp communications. please advice if my understanding is not correct When you send if your NIC supports checksum offload your TCP/IP stack may decide not to do work that the NIC can do later. @fruit that's correct. Checksums are always going to be happening on the TCP packets but because it's done in the hardware Wireshark doesn't always get to see it.
1071,A,Get port of the incoming tcp connection how could I get the port of the client that is connecting to the c# application? Assuming you have a System.Net.Sockets.Socket object Type-cast its RemoteEndPoint property to an IPEndPoint and then read its Port property.  To listen on a socket don't you need to know the port number first? sure but I need the client port for NAT punchtrough a server. I don't need the port where the client program is binded but the one of the connection aasigned by the router Then I'd suggest you go to http://www.rfc-editor.org/rfcsearch.html and search for `NAT` and/or `Network Address Translation`. And the work through the RFCs. Also see http://en.wikipedia.org/wiki/NAT_traversal
1072,A,"Buffer size for capturing packets in kernel space? Going through the man page of tcpdump here It seems kernel can drop the packets if the buffer is full. I was wondering if 1) that size is configurable and/or 2) where can I see the size for my distro? From the man page (for easy reference): packets ``dropped by kernel'' (this is the number of packets that were dropped due to a lack of buffer space by the packet capture mechanism in the OS on which tcpdump is running if the OS reports that information to applications; if not it will be reported as 0). 1) It's configurable but not precisely as it would decide a proper size from your request. 2) Use setsockopt / getsockopt with SO_RCVBUF / SO_SNDBUF I'm not familiar with linux but it seems this link explains it well. http://linux.die.net/man/7/socket Thanks for your answer. I tried your suggestion and the answer I got was around 48kb. Which does not seem right. It is size of the socket buffer not the buffer size which kernel keeps for receiving packets. Thank you for your hard work.  There are several areas you might check to mitigate packets dropped by kernel: Look at configuring /proc/sys/net/core/netdev_max_backlog and /proc/sys/net/core/netdev_budget. The default is probably pretty low; try setting each to something like 2000. Writing to the output device screen may be blocking/slowing the tcpdump process long enough to fill the recv buffer Use -nn to turn off DNS lookups and port naming Write to file instead of the screen Try a tool such as gulp If you have a multi-processor machine look at using taskset Use nice to set the priority of the process Even with those settings it may just be that you can not keep up with the speed of the traffic you are trying to capture. Look at the details of your NIC and machine and ensure that what you expect is even possible. Thanks. Modifying /proc/sys/net/core/netdev_max_backlog did the trick for me. Off topic question here in case you happen to see this comment Why would writing to file be faster/better than writing to screen when in unix both are practically files? I'm not an expert but I'd imagine it boils down to the buffering techniques that can be used in either case. I'm sure you'd get a more definitive answer if you asked a formal question ;) I can ask a formal question but I need to be sure that this is the case. Do you have any first hand experience / case study at hand right now  where you observed this? I would really love to know the answer if writing to file instead of screen is faster. Thank you for taking time out to answer my question. It is going to very dependent on the output device. A quick run on my machine using `gnome-terminal` vs `aterm` set to capture 100000 packets on a heavy download: `gnome-terminal` (to file) : 20 dropped `gnome-terminal` (to screen) : 27217 dropped `aterm` (to file) : 0 dropped `aterm` (to screen) : 0 dropped @Anon: ezpz is right writing to a file can be much faster than writing to screen. Writing to ""screen"" tends to actually mean writing to a PTY and PTYs have a very limited buffer size. If the displaying program on the other side of the PTY isn't reading fast enough then that buffer will fill and writes to the PTY will start to block. Most displaying programs are slower than disk (and disk has much larger buffers too)."
1073,A,Socket.select not returning readable sockets in c# I'm building a server that serves two clients using streamreaders/writers (using networkstreams) using newline symbols to indicate the start/end of a new command. Because readline (and writeline too but that's not an issue) will block until there actually is a line to read I use socket.select in order to determine when there is a line to read. However because I don't know how many lines there are to read I just read one and loop back using socket.select again. Which according to the documentation: http://msdn.microsoft.com/en-us/library/system.net.sockets.socket.select.aspx says that if it's still readable it will make the list to the readable sockets and continue or if you've read everything it won't. Here is my code: DateTime startTime = DateTime.Now; DateTime endTime = startTime.AddSeconds(Double.Parse(playTime)); List<Socket> clients; double secondsLeft; while ((secondsLeft = (endTime - DateTime.Now).TotalSeconds) > 0) { clientSockets = new List<Socket>() { clientSocket1 clientSocket2 }; Socket.Select(clientSockets null null (int)(secondsLeft * 1000000)); if (clientSockets.Contains(clientSocket1)) { handlePlayer(reader1 writer1 writer2); } if (clientSockets.Contains(clientSocket2)) { handlePlayer(reader2 writer2 writer1); } } However in a unit test when the clients send a lot of words in rapid succession it will only read the first word (from each player) and then scoekts.select will freeze until the timout is reached. So is this the right way to see if there's more data in the network stream? (As a streamreader). are you sure you parse the stream correctly ? (i.e. one call to Socket.Receive() might read *many* words if you supply a buffer that can hold a lot of data. You'll not get just 1 word even if 1 word was sent with 1 Send() call.) Oops.reader1/2 and writer1/2 are just streamreaders and streamwriters made like this: StreamReader reader1 = new StreamReader(new NetworkStream(clientSocket1) Encoding.UTF8)) and the only way I get stuff out of the streamreader is by reader1.ReadLine() right so the StreamReader might read serveral lines from the underlying NetworkStream but you only read one of them. Why on earth use Socket.Select here? we are not on BSD Unix any more. Use async I/O via Socket.BeginReceive or similar to handle both streams concurrently in an event-driven way. If you really want to use streams I imagine StreamReader.EndofStream could be useful here.
1074,A,Source IP from UDP packet in c under window OS I want to get the source IP of UDP packet kindly guide me so that I can make it possible. I am working in c under windows platform. Use the recvfrom function. It has a from parameter that points to a sockaddr structure that will receive the source address. @ULysses now Ok?? Actually my application is in sniffer mode so that I can't use the recvfrom function. What does that mean - what API are you using for sniffing ? I am using winsock. @Arman wait so what's the matter with the recvfrom in your case? I believe that the link in the post you marked as an answer also suggests using the function @Arman :) yepp  recvfrom is what you need  EDIT: have a look at this tutorial: http://www.sockets.com/ch16.htm There you should find some code that can guide you in the right direction! This should help you: sockaddr structure: http://msdn.microsoft.com/en-us/library/ms740496(VS.85).aspx in_addr structure: http://msdn.microsoft.com/en-us/library/ms738571(VS.85).aspx
1075,A,"UDP Load Testing: How do you simulate many UDP clients? I am developing a tool to perform load testing on a UDP server (using C#/.NET 4.0 running on NT 6.x although that is less relevant). The server talks to tens of thousands of clients where the communication between each client is very low traffic and infrequent. All communications follow a request-reply pattern where one of the sides initiates communication with the other side who then replies. When the server needs to send something to the client he looks up the last known endpoint (IP + port) of the client and sends a UDP packet and listens for a reply on a single known port which is used to receive communications from all clients. When the client initiates communication it already knows the endpoint of the server and simply sends a packet from an ephemeral port and waits for a reply on the same port. The same ephemeral port is used for the lifetime of the client. The design of the load testing tool is pretty simple; emulate the behavior state and decision making of each client to a low yet sufficient complexity. Since the communication with each client is only occasional (every few seconds) and the amount of processing required for each communication is very minimal the best approach I can think of is to use a single thread with a single socket to perform all the communication for a large number of simulated clients which most likely still won't keep the thread fully busy and socket saturated. Unfortunately I have encountered two problems with that approach arising from the fact that each client sends and receives from his own port: A socket will only allow sending a UDP packet from either a system-allocated ephemeral port or a specific port the socket is bound to. A socket will only receive UDP packets from the port it is bound to. One socket per client This above two constraints seems to mean that I must create a socket for each client since the UDP packet must originate from a certain port and a reply will be sent to that port. So the first possible solution is to do just that create a socket per simulated client. Let's say we're simulating 30000 clients on a single machine: Is creating 30000 sockets even feasible? Is it a best practice? Is it performant? Will Windows even let you bind 30000 sockets to 30000 different ports? How do I check across 30000 client sockets if any data was sent by the server? Do I poll all the sockets periodically to see if any data was received? Is there a way to wait on all 30000 sockets and get the first packet that arrives to any of them? What resources does the operating system allocated for each socket? What are the limits of each of those resources and what are the implications of reaching them? A single socket for all clients A different approach would be to use a single socket but the two problems I mentioned earlier must first be solved somehow: The first problem of having the UDP packet originate at a port other than the Socket's port is solvable. It involves creating a raw socket and constructing the UDP header yourself which means you can specify any source and destination port you'd like. The only difficulty is with calculating the optional yet important UDP checksum which requires not only the UDP header and payload but also the source and destination IP address the former is problematic since it requires calling Win32 APIs to obtain (GetBestInterface and GetAdaptersInfo) which involves several native structures and lots of unmanaged memory allocations a potential reliability pitfall from a .NET perspective but it can be done. The second problem using a single socket to receive UDP packets from a list (or range) of ports remains unsolved by me. Even with raw socket the OS demands I bind the socket to a specific single port before allowing me to perform receive operations. Is there a way to do that? There's always the packet sniffing techniques but I'd rather avoid them unless it can be done from managed code in a reliable and a somewhat straightforward way (in which case I'm open for suggestions). Other approaches? Is there another approach I haven't thought of? Am I on the wrong track? Can you think of a better solution? I'd love to hear any suggestions you might have. Yes you can easily create > 30000 sockets on a windows machine but you may need to tune MAXUSERPORT (see here). Use I/O completion ports or async I/O and you don't then need to worry about 'polling' and the scalability 'just works'. The main resource issue is non-paged pool but this has become far less of an issue on Vista or later (see here) the next issue is the I/O locked pages limit which may be an issue if you are posting very large buffers for your reads but given this is UDP I assume you'll have 'sensible' sized datagrams and the locked pages limit is then unlikely to be an issue. I've blogged about some of the scalability issues here: http://www.serverframework.com/asynchronousevents/2010/12/one-million-tcp-connections.html I've written tools similar to what you're trying to do using my C++ socket server framework. There's a UDP test tool example that ships as part of the framework that sends a configurable number of datagrams from unique client ports and waits for replies and which is easy to adjust to deal with your specific datagram format and response requirements (see here). Thank you very much for your question very informative and helpful. It seems that sockets aren't the heavyweight mechanisms I thought they were. Does the `MaxUserPort` registry change require a restart to take effect? I don't know... It seems the `MaxUserPort` registry key works up to NT 5.x (XP/Server 2003) and beginning with NT 6.x (Vista/Server 2008) the correct (and immediate) way of setting the ephemeral port range is via the [`netsh int ipv4 set dynamicport` command](http://support.microsoft.com/kb/929851). After getting ""An operation on a socket could not be performed because the system lacked sufficient buffer space"" error when trying to bind 25k ports I performed a `netsh int ipv4 set dynamicport udp start=1024 num=60000` and it instantly solved the issue (no restart)! useful to know thanks."
1076,A,Debugging server script like python? I'm new to 'network' programming. I've done throughout the Google app engine tutorial and I'm trying to make my own application which is guestbook. It has a server script on Google app engine and client app is running on iPhone. But how do I check if the server is received my message from iPhone? or How do I debug the script I wrote on server side? (it's an python script) I want to make sure the data is in right format and other stuff. Please somebody give me a link if there's good tutorial or good place to start. thanks. In your code you should be using tons of logging calls. In the app engine dashboard for your app there is an option to view logs and all of your logging calls will show up in there (and can be filtered in various ways). There are also ways to run dev_appserver.py and debug locally using software like PyDev but you'd have to make your iPhone app send its messages to the address of your local server. WOw this is a good tip! I'm going to try it right now. thanks ;)
1077,A,"There's a black hole in my server (TcpClient TcpListener) I'm trying to build a server that will receive files sent by clients over a network. If the client decides to send one file at a time there's no problem I get the file as I expected but if it tries to send more than one I only get the first one. Here's the server code: I'm using one Thread per connected client public void ProcessClients() { while (IsListening) { ClientHandler clientHandler = new ClientHandler(listener.AcceptTcpClient()); Thread thread = new Thread(new ThreadStart(clientHandler.Process)); thread.Start(); } } The following code is part of ClientHandler class public void Process() { while (client.Connected) { using (MemoryStream memStream = new MemoryStream()) { int read; while ((read = client.GetStream().Read(buffer 0 buffer.Length)) > 0) { memStream.Write(buffer 0 read); } if (memStream.Length > 0) { Packet receivedPacket = (Packet)Tools.Deserialize(memStream.ToArray()); File.WriteAllBytes(Path.Combine(Environment.GetFolderPath(Environment.SpecialFolder.DesktopDirectory) Guid.NewGuid() + receivedPacket.Filename) receivedPacket.Content); } } } } On the first iteration I get the first file sent but after it I don't get anything. I've tried using a Thread.Sleep(1000) at the end of every iteration without any luck. On the other side I have this code (for clients) . . client.Connect(); foreach (var oneFilename in fileList) client.Upload(oneFilename); client.Disconnect(); . . The method Upload: public void Upload(string filename) { FileInfo fileInfo = new FileInfo(filename); Packet packet = new Packet() { Filename = fileInfo.Name Content = File.ReadAllBytes(filename) }; byte[] serializedPacket = Tools.Serialize(packet); netStream.Write(serializedPacket 0 serializedPacket.Length); netStream.Flush(); } netStream (NetworkStream) is opened on Connect method and closed on Disconnect. Where's the black hole? Can I send multiple objects as I'm trying to do? Thanks for your time. I am guessing that if clients upload multiple files your loop is reading the whole stream as a single file on the server side. Where's the ""delimiter between files""? How does the server know where one ends and another begins? You know I've found this other site (http://devblog.antongochev.net/2008/07/01/sending-data-via-tcp-using-tcpclient-and-tcplistener/) and he uses a ""delimiter"" as you said even with serialized objects. Gonna try that. if that were the case how would you explain the first file being successfully transferred? The first deserialization seems to be working okay. Am I correct? Is its size correct? Or is it the sum total size of all the files the client sent? How are you validating the file is received correctly? yes its size is correct and the md5 is also the same as the first transferred file. The files are images so it's pretty easy to check but i'm also using an external tool to verify the md5 checksum. Yep that was the problem. Thank you!."
1078,A,"How can I programmatically detect a Window's network adapter's duplex state? I am using WMI to detect a number of items about a network adapter's state. Among the things I need to find out are (a) speed and (b) duplex. I have been able to detect the speed of a network adapter by using WMI and the following Python code: from pycom.client import wmi dev_name = r""\\DEVICE\\{287EB4BB-5C2A-4108-B377-15E1D0B0E760}"" query1 = """""" SELECT * FROM MSNdis_EnumerateAdapter WHERE DeviceName = '%s'"""""" % dev_name wmi_ndis = wmi.WMI(""root\\WMI"") results = wmi_ndis.ExecQuery(query1) instance_name = results[0].InstanceName del results query2="""""" SELECT * FROM MSNdis_LinkSpeed WHERE InstanceName='%s'"""""" % instance_name results = wmi_ndis.ExecQuery(query2) linkspeed = results[0].NdisLinkSpeed del results print instance_name linkspeed del instance_name del linkspeed del wmi_ndis There appears to be a perfect class for the data I want: MSNDis_LinkParameters. However this table does not appear to be populated. There are values in Win32_NetworkAdapter as well but they are also not populated. I would be happy to use a native C API or WMI but I can't do screen scraping because the application needs to work with arbitrary languages. Thanks! Apparently the underlying issue here is that WMI provider implementation is handled by the NIC vendor not the OS-- so some NICs may support some settings while (as you've discovered) others don't. For link speed check out this for some WMI scripts which may work on most NICs. For duplex I think you're out of luck at least according to topic. Look at the last post in that thread-- it seems pretty specific about how to work around the limit in some cases but won't work for all NICs."
1079,A,"What do I need to fill in when using IP_HDRINCL I'm awaiting the arrival of ""Linux Network Programming"" but in the meantime I thought I'd ask my brothers (and sisters) here for some info. If I have constructed a raw packet structure containing the ethernet header ip header and tcp/udp/icmp header. What do I actually have to fill in when using the option IP_HDRINCL? At first I thought that I had to do everything but now I understand that the src and dest mac addresses of the ethernet header could be handled by the kernel. But what is required that I fill in and what values should the other fields have for the kernel to understand that it should fill those in? Thanx in advance boys (and girls)! You start with the IP header. But set all members of the IP and TCP/UDP frames correctly. Your assumptions is correct. Don't include the Ethernet header when you send raw IP frames. Usually Ethernet is used as a low level communication but there are others. I use the write function to write my packet to the network. If I set the macaddresses in my ethheader it is reflected in my sent packet when I look at it with wireshark. This allows my to spoof my src mac address. The det macaddress if overwritten by the on retrivet by the kernel if I send tha packet as a TCP packet or similar though. I have build like a hirarchy (don't wanna use the word inheritance in C... hehe) where the lowest level packet is an ethernet packet. Then an IP packet contains the stuff of an ethernet packet plus the ip components and so on. So because of this I DO include the ethernet header myself. But I noticed that if I just filled in the src and dst ip and port I could set the src and dst mac to anything and the kernel would update the mac of the dst with the one used by my dst ip's mac address. But you pass the pointer to the IP header to the send() function. If there are some other bytes (like the Ethernet header) before this address it won't care."
1080,A,"Parsing Binary Data in C? Are there any libraries or guides for how to read and parse binary data in C? I am looking at some functionality that will receive TCP packets on a network socket and then parse that binary data according to a specification turning the information into a more useable form by the code. Are there any libraries out there that do this or even a primer on performing this type of thing? In my experience the best way is to first write a set of primitives to read/write a single value of some type from a binary buffer. This gives you high visibility and a very simple way to handle any endianness-issues: just make the functions do it right. Then you can for instance define structs for each of your protocol messages and write pack/unpack (some people call them serialize/deserialize) functions for each. As a base case a primitive to extract a single 8-bit integer could look like this (assuming an 8-bit char on the host machine you could add a layer of custom types to ensure that too if needed): const void * read_uint8(const void *buffer unsigned char *value) { const unsigned char *vptr = buffer; *value = *buffer++; return buffer; } Here I chose to return the value by reference and return an updated pointer. This is a matter of taste you could of course return the value and update the pointer by reference. It is a crucial part of the design that the read-function updates the pointer to make these chainable. Now we can write a similar function to read a 16-bit unsigned quantity: const void * read_uint16(const void *buffer unsigned short *value) { unsigned char lo hi; buffer = read_uint8(buffer &hi); buffer = read_uint8(buffer &lo); *value = (hi << 8) | lo; return buffer; } Here I assumed incoming data is big-endian this is common in networking protocols (mainly for historical reasons). You could of course get clever and do some pointer arithmetic and remove the need for a temporary but I find this way makes it clearer and easier to understand. Having maximal transparency in this kind of primitive can be a good thing when debugging. The next step would be to start defining your protocol-specific messages and write read/write primitives to match. At that level think about code generation; if your protocol is described in some general machine-readable format you can generate the read/write functions from that which saves a lot of grief. This is harder if the protocol format is clever enough but often doable and highly recommended. You mean *vptr++ don't you also cast of buffer to char* is missing.  The standard way to do this in C/C++ is really casting to structs as 'gwaredd' suggested It is not as unsafe as one would think. You first cast to the struct that you expected as in his/her example then you test that struct for validity. You have to test for max/min values termination sequences etc. Checkout http://mixter.void.ru/rawip.html for a quick example using raw sockets. What ever platform you are on you must read Unix Network Programming Volume 1: The Sockets Networking API. Buy it borrow it steal it ( the victim will understand it's like stealing food or something... ) but do read it. After reading the Stevens most of this will make a lot more sense. I'm skeptical of the method ""cast then check"". If you don't check you risk getting invalid data. And if you check what's the point of casting? Checking will be as slow as traditional parsing. As Casey Barker wrote below things are not that simple. You can fix byte alignment and padding most of the time (and you need to be aware of this and test it thoroughly with each new system) but once you get into endian order problems you will be forced to fix each struct individually before checking for validity. And if you are checking for validity then you may as well check it during parsing. Parsing individual tokens also allows fine grained subclassing and versioning. Indeed Office File Validation introduced in Office 2010 and later backported to Office 2007 and Office 2003 basically checks the file for validity to prevent vulnerabilities from being exploited.  Let me restate your question to see if I understood properly. You are looking for software that will take a formal description of a packet and then will produce a ""decoder"" to parse such packets? If so the reference in that field is PADS. A good article introducing it is PADS: A Domain-Specific Language for Processing Ad Hoc Data. PADS is very complete but unfortunately under a non-free licence. There are possible alternatives (I did not mention non-C solutions). Apparently none can be regarded as completely production-ready: binpac PacketTypes DataScript If you read French I summarized these issues in Génération de décodeurs de formats binaires. @bortzmeyer These are all news to me. Thanks for the info!  Basically suggestions about casting to struct work but please be aware that numbers can be represented differently on different architectures. To deal with endian issues network byte order was introduced - common practice is to convert numbers from host byte order to network byte order before sending the data and to convert back to host order on receipt. See functions htonl htons ntohl and ntohs. And really consider kervin's advice - read UNP. You won't regret it!  Parsing/formatting binary structures is one of the very few things that is easier to do in C than in higher-level/managed languages. You simply define a struct that corresponds to the format you want to handle and the struct is the parser/formatter. This works because a struct in C represents a precise memory layout (which is of course already binary). See also kervin's and gwaredd's replies.  You don't really need to parse binary data in C just cast some pointer to whatever you think it should be. struct SomeDataFormat { .... } SomeDataFormat* pParsedData = (SomeDataFormat*) pBuffer; Just be wary of endian issues type sizes reading off the end of buffers etc etc Or different compilers etc. That's *really* fragile code IMO. Agreed. I think all the many etc's are why he wants a library to do it. yeah -.- while that approach is reasonable as long as you are on the same machine doing that in network programming should really be avoided. the data structure should be defined as packed on both ends and after taking care endian issues your are perfectly safe. At least some of the time the structures you need are already defined---and in a way that is locally correct----in a set of API headers somewhere. This is common for OS services etc. If you must write it yourself looking into what #pragma's your compiler provides to insure that you get the packing and endian right. Extremely dangerous method when you read packets on the network: it would make very easy for an attacker to send you severly invalid data. I've used this on multiple projects and never had any problems. You need to watch for packing and alignment issues - easy fix with a pragma. Endian and invalid/dangerous data you need to check for whatever method you use. Serialisation is (a) slower (b) over engineering imo (in most cases) @Gwaredd - Did you use the same method even for network programming ??? Please do not just cast structs it is VERY fragile see Casey Barker's response for the proper way of doing it.  You might be interested in Google Protocol Buffers which is basically a serialization framework. It's primarily for C++/Java/Python (those are the languages supported by Google) but there are ongoing efforts to port it to other languages including C. (I haven't used the C port at all but I'm responsible for one of the C# ports.) There are many ways to serialize data (Protocol Buffers is nice but it is just one of it there is also XML JSON ASN/1+BER etc). They work only if you control the specification of the protocol. If it is not the case your method does not work. Absolutely. If you're not in control of the protocol you basically have to do it manually.  I'm not really understand what kind of library you are looking for ? Generic library that will take any binary input and will parse it to unknown format? I'm not sure there is such library can ever exist in any language. I think you need elaborate your question a little bit. Edit: Ok so after reading Jon's answer seems there is a library well kind of library it's more like code generation tool. But as many stated just casting the data to the appropriate data structure with appropriate carefulness i.e using packed structures and taking care of endian issues you are good. Using such tool with C it's just an overkill.  I have to disagree with many of the responses here. I strongly suggest you avoid the temptation to cast a struct onto the incoming data. It seems compelling and might even work on your current target but if the code is ever ported to another target/environment/compiler you'll run into trouble. A few reasons: Endianness: The architecture you're using right now might be big-endian but your next target might be little-endian. Or vice-versa. You can overcome this with macros (ntoh and hton for example) but it's extra work and you have make sure you call those macros every time you reference the field. Alignment: The architecture you're using might be capable of loading a mutli-byte word at an odd-addressed offset but many architectures cannot. If a 4-byte word straddles a 4-byte alignment boundary the load may pull garbage. Even if the protocol itself doesn't have misaligned words sometimes the byte stream itself is misaligned. (For example although the IP header definition puts all 4-byte words on 4-byte boundaries often the ethernet header pushes the IP header itself onto a 2-byte boundary.) Padding: Your compiler might choose to pack your struct tightly with no padding or it might insert padding to deal with the target's alignment constraints. I've seen this change between two versions of the same compiler. You could use #pragmas to force the issue but #pragmas are of course compiler-specific. Bit Ordering: The ordering of bits inside C bitfields is compiler-specific. Plus the bits are hard to ""get at"" for your runtime code. Every time you reference a bitfield inside a struct the compiler has to use a set of mask/shift operations. Of course you're going to have to do that masking/shifting at some point but best not to do it at every reference if speed is a concern. (If space is the overriding concern then use bitfields but tread carefully.) All this is not to say ""don't use structs."" My favorite approach is to declare a friendly native-endian struct of all the relevant protocol data without any bitfields and without concern for the issues then write a set of symmetric pack/parse routines that use the struct as a go-between. typedef struct _MyProtocolData { Bool myBitA; // Using a ""Bool"" type wastes a lot of space but it's fast. Bool myBitB; Word32 myWord; // You have a list of base types like Word32 right? } MyProtocolData; Void myProtocolParse(const Byte *pProtocol MyProtocolData *pData) { // Somewhere your code has to pick out the bits. Best to just do it one place. pData->myBitA = *(pProtocol + MY_BITS_OFFSET) & MY_BIT_A_MASK >> MY_BIT_A_SHIFT; pData->myBitB = *(pProtocol + MY_BITS_OFFSET) & MY_BIT_B_MASK >> MY_BIT_B_SHIFT; // Endianness and Alignment issues go away when you fetch byte-at-a-time. // Here I'm assuming the protocol is big-endian. // You could also write a library of ""word fetchers"" for different sizes and endiannesses. pData->myWord = *(pProtocol + MY_WORD_OFFSET + 0) << 24; pData->myWord += *(pProtocol + MY_WORD_OFFSET + 1) << 16; pData->myWord += *(pProtocol + MY_WORD_OFFSET + 2) << 8; pData->myWord += *(pProtocol + MY_WORD_OFFSET + 3); // You could return something useful like the end of the protocol or an error code. } Void myProtocolPack(const MyProtocolData *pData Byte *pProtocol) { // Exercise for the reader! :) } Now the rest of your code just manipulates data inside the friendly fast struct objects and only calls the pack/parse when you have to interface with a byte stream. There's no need for ntoh or hton and no bitfields to slow down your code. Does this code work even for passing a structure via sockets >>> It's expressly good for sockets -- especially when you don't want to make assertions about the endianness/bus width/alignment of the processes on either end of the socket. I agree with your comments completely but the code itself should have been more explicit on that matter. The part where you convert the raw bytes into a word should be done using an instance of some Endian converter so that it can be easily switched with a different implementation when needed. Hence my code comment ""You could also write a library of ""word fetchers"" for different sizes and endiannesses."" So what you want me to write the whole library? :) But this code works on any core of any endian orientation. It's completely portable so long as you properly typedef ""Byte"" and ""Word32"" so I don't see why you'd need to ""switch the implementation."" This question came up again recently and re-reading it I think Groo missed the point of this implementation. So I want to say more specifically: This code works for ANY processor of ANY width or endianness so long as the PROTOCOL is big-endian. Protocols don't usually change after they're defined so it's really a non-issue."
1081,A,"ServerSocket reuseAddress allow bind to an already bound port? When using Netty I was surprised that if I use reuseAddress option it allows a ServerSocket to bind to the same address without raising an ""already bind exception""  ServerBootstrap bootstrap = new ServerBootstrap( new NioServerSocketChannelFactory(Executors .newCachedThreadPool() Executors.newCachedThreadPool())); bootstrap.setPipelineFactory(new ChannelPipelineFactory() { @Override public ChannelPipeline getPipeline() throws Exception { ChannelPipeline p = pipeline(); p.addLast(""handler"" new DummyHandler()); return p; } }); bootstrap.setOption(""reuseAddress"" true); bootstrap.bind(new InetSocketAddress(2000)); bootstrap.bind(new InetSocketAddress(2000)); I just thought that reuseAddress allows a new socket to reuse a close-wait socket but this is different. The following is the result of a netstat command  C:\Users\secmask>netstat -a -n|grep 2000 TCP 0.0.0.0:2000 0.0.0.0:0 LISTENING TCP 0.0.0.0:2000 0.0.0.0:0 LISTENING Am I missing something? What's going on? What you are seeing is what reuseAddress is supposed to do. Multiple sockets can be bound to the same IP/Port at the same time regardless of their states. when new income client connect to this port which of that duplicate socket will accept the client? could you provide some documents links about this? When multiple sockets are bound to the same IP/Port at the same time their behavior is undefined. Anything can happen. There is no way to predict it. It is VERY bad practice to do this for exactly this reason. `reuseAddress` should only be used when you have bound your own socket then closed it and need to reopen the same IP/Port for a new socket before the old socket has finished any wait states when you are sure that no more data will arrive for the old socket.  I assume that Windows allows this due to history. It is a bit of a security issue. See http://msdn.microsoft.com/en-us/library/ms740618 for some information about how the involved options interact. Which socket gets a connection is undefined. Maybe if you narrow down the version of Windows you are using you could narrow down what the response will be although it is probably just to not depend on it. yes it only on windows not linux."
1082,A,"Best architecture for an iOS application that makes many network requests? I'm in the process of rethinking my approach to the request architecture of a large app I'm developing. I'm currently using ASIHTTPRequest to actually make requests but since I need many different types of requests as a result of many different actions taken in different view controllers I'm trying to work out the best system of organizing these requests. I'm currently building singleton ""requesters"" that are retained by the app delegate and sit around listening for NSNotifications that signal a request needs to be made; they make the request listen for the response and send out a new NSNotification with the response data. This solves most of my problems but doesn't elegantly handle failed requests or simultaneous requests to the same singleton requester. Anyone have any success devising a clear OO architecture for making many different types of requests in an iOS app? Good question: I've been researching for an answer to this as well as someone who has created a connection manager for NSURLConnection that works well for one connection but not so well for several. After having tried several approaches this is one architecture that is giving me excellent results is easy to document understand maintain and extend: I have a single object taking care of network connectivity let's call it a ""network manager"". Typically this object is a singleton (created using Matt Gallagher's Cocoa singleton macro). Since you use ASIHTTPRequest (which I always do wonderful API) I add an ASINetworkQueue ivar inside my network manager. I make the network manager the delegate of that queue. I create subclasses of ASIHTTPRequest for each kind of network request that my app requires (typically for each backend REST interaction or SOAP endpoint). This has another benefit (see below for details :) Every time one of my controllers requires some data (refresh viewDidAppear etc) the network manager creates an instance of the required ASIHTTPRequest subclass and then adds it to the queue. The ASINetworkQueue takes care of bandwidth issues (depending on whether you are on 3G EDGE or GPRS or Wifi you have more bandwidth and you can process more requests etc). This is done by the queue which is cool (at least that's one of the things I understand this queue does I hope I'm not mistaken :). Whenever a request finishes or fails the network manager is called (remember the network manager is the queue's delegate). The network manager doesn't know squat about what to do with the result of each request; hence it just calls a method on the request! Remember requests are subclasses of ASIHTTPRequest so you can just put the code that manages the result of the request (typically deserialization of JSON or XML into real objects triggering other network connections updating Core Data stores etc). Putting the code into each separate request subclass using a polymorphic method with a common name accross request classes makes it very easy to debug and manage IMHO. Finally I notify the controllers above about interesting events using notifications; using a delegate protocol is not a good idea because in your app you typically have many controllers talking to your network manager and then notifications are more flexible (you can have several controllers responding to the same notification etc). Anyway this is how I've been doing it for a while and frankly it works pretty well. I can extend the system horizontally adding more ASIHTTPRequest subclasses as I need them and the core of the network manager stays intact. Hope it helps! good answer! how do you test your system? another big concern of mine is coming up with an architecture that's easy to unit test. thanks excellent explanation. One quick thing that springs to mind you could possibly use blocks instead of notifications if you only have one class that is interested in your response. @Michael yes but only if your app runs exclusively in iOS 4.x. As I have several projects backwards-compatible with 3.2 for the moment I stick with plain'ol notifications. @kevboh glad to hear that! Any feedback comment or critics would be greatly appreciated. As an update I'm now using the architecture fairly heavily (we have 20-30 different types of requests) and it scales admirably. It's even works fine with cookies. Am I right? I've just published a blog post that shows a sample implementation of this architecture: http://akosma.com/2011/09/20/a-proposed-architecture-for-network-bound-ios-apps/ Given the recent announcement that ASIHTTPRequest is not going to be supported anymore I'm going to adapt Senbei to support AFNetworking in the near future. @akosma: I've done the same to my setup. @kevboh: Can you provide a URL? @BillCraun A URL for what? @akosma Quick question but first thank you for your contribution. After it having been nearly 3 years I wanted to get an update hoping for you to have finally supported Senbei to AFNetworking. Could you let me know the update on that please?  Here is how I generally do it. I too have a singleton object used for making network requests. For requests that have to be made often I have an NSOperationQueue that accepts AFHTTPRequestOperations (or AFJSONRequestOperations) since I generally use AFNetworking for making requests. For these there is a completionBlock and failureBlock property that is executed upon success or failure of the request. On my singleton object I would have a method for initiating a particular network request and as parameters to that method I would include a success and failure block which can be passed into the blocks defined in the method. This way the entire application can make a network request and the scope of the application at that point is available to the singleton in the block that is passed to the method. For example...(using ARC)  @implementation NetworkManager -(void)makeRequestWithSuccess:(void(^)(void))successBlock failure:(void(^)(NSError *error))failureBlock { NSURL *url = [NSURL URLWithString:@""some URL""]; NSURLRequest *request = [NSURLRequest requestWithURL:url]; AFHTTPRequestOperation *op = [[AFHTTPRequestOperation alloc] initWithRequest:request]; [op setCompletionBlockWithSuccess:^(AFHTTPRequestOperation *operation id responseObject) { [responseObject doSomething]; if (successBlock) dispatch_async(dispatch_get_main_queue() successBlock); } failure:^(AFHTTPRequestOperation *operation NSError *error) { if (failureBlock) dispatch_async(dispatch_get_main_queue() ^{ failureBlock(error); }); }]; [self.operationQueue addOperation:op]; } @end And you can always make the success block take whatever parameters you need to pass around.  The fully-loaded project is a good read. Interesting. I'll have to check this out. I've seen fully-loaded before- it handles a specific problem well but I'm looking for a solution that treats with different types of requests that have different responses. The way fully-loaded handles requests is similar to what I've detailed in my question with nsnotifications and asihttprequest doing the heavy lifting. I'm looking for something more extensible and robust even if it's just a creative way to organize notifications and requests. The other code example in my mind is the `Networking` folder in the MVCNetworking sample from Apple which I do not have any experience to make any comment though."
1083,A,"Writing a simple email server What would be a good starting point for me to learn about creating an email server? Basically what I want to do is have a server (such as foo.com) recieving mail for me so if I send an email to (blah@foo.com) it will dump the contents of the email into /mail/blah/subject and then send it off to my REAL email account. I'm looking to do this as a programming exercise so links to RFCs etc. would be nice. Reinventing the wheel is a good way to learn about wheels. EDIT: Feel free to retag this appropriately. Wouldn't be easier to type ""SMTP RFC"" into Google than type that whole question? But I don't know if that's all I need to receive and forward emails. While I don't wish to discourage you in your quest to learn the only thing ""simple"" about a mail server is in the name of the protocol (Simple Mail Transfer Protocol - SMTP) Edit: I provided some headings and divided RFCs by topic. I hope it's more accessible now. It's quite a list and I wish I could format it any better but unfortunately that's about it. Since you mentioned you don't really know what you need let me clarify: If you only want to implement a simple ""proxy"" server that sits in between your MUA (email client) and ""real"" server you can probably get away with only implementing basic SMTP functionality. This will allow you to send messages i.e. to submit messages to an MTA. POP3 is for email clients to pull messages off of your server while IMAP is an alternative to POP3 with a somewhat different feature set mainly providing an on- or offline mode which can be thought of like managing remote folders (i.e. mailboxes). MIME specifies the format of the contents of e-mail messages in presence of multi-part messages attachments etc. Internet Message format (also defines e-mail address format) http://www.faqs.org/rfcs/rfc822.html http://www.faqs.org/rfcs/rfc2822.html SMTP: http://www.faqs.org/rfcs/rfc821.html Update to SMTP/RF821: http://www.faqs.org/rfcs/rfc5321.html SMTP-AUTH: http://www.faqs.org/rfcs/rfc2554.html Message submission (i.e. for the application to be acting as a MUA): http://www.faqs.org/rfcs/rfc2476.html IMAPv4: http://www.faqs.org/rfcs/rfc1730.html IMAPv4rev1: http://www.faqs.org/rfcs/rfc2060.html POP3: http://www.faqs.org/rfcs/rfc1081.html http://www.faqs.org/rfcs/rfc1939.html http://www.faqs.org/rfcs/rfc1957.html POP3 extensions: http://www.faqs.org/rfcs/rfc2449.html Authorization for POP/IMAP: http://www.faqs.org/rfcs/rfc2195.html TLS for POP3 and IMAP: http://www.faqs.org/rfcs/rfc2595.html AUTH-RESP-CODE for POP3: http://www.faqs.org/rfcs/rfc3206.html POP3 simple authentification: http://www.faqs.org/rfcs/rfc5034.html MIME which is composed of 5 RFCs: http://www.faqs.org/rfcs/rfc2045.html http://www.faqs.org/rfcs/rfc2046.html http://www.faqs.org/rfcs/rfc2047.html http://www.faqs.org/rfcs/rfc4288.html http://www.faqs.org/rfcs/rfc4289.html http://www.faqs.org/rfcs/rfc2049.html Can you split that up into sending email/receiving email sections? Thanks for the headings looks good. Thanks!"
1084,A,"Information about remote IP addresses I want to track all IP addresses with which my computer is communicating. I am using C# and Visual Studio. There is one listBox in which I will put all remote IP address. I am new to network programming so I need some direction about that kind of problem. My question is how to do this? How to get information about remote IP addresses which which my computer make connections? Are you sitting behind a router? Or do you have a direct connection to the Internet? Do you care about *remote* servers or only those on your local network? I am using Aolynk Router. For now I just want logic how to get information about any remote IP address whit which my computer get in touch. Do you want only the remote addresses connected to your program or all connections of your computer(like the `netstat` command) @Cody why do you think a router is relevant? If the remote host is behind a router you will get the address of his router. But a router on your side of the internet should have no effect. @CodeInChaos: Strictly speaking perhaps it may not be. But the vast majority of routers now have NAT turned on by default (and if not they *should*). That could definitely complicate things because it's doing IP address translation beyond the client machine. For example running `netstat` on the machine I'm sitting at *everything* is a private IP even though I'm obviously connected to the Internet. @Cody Your computer won't notice the NAT your router does. It only notices the NAT the remote router does. My `netstat` does show public IPs while I'm behind a router that does NAT. Most socket programming applications talk to the TCP socket API that's roughly standardized between operating systems. If you just want to write a simple server and visualize the clients talking to your server running in your process that's pretty easy. However your process can't see the socket connections talking to other processes on your machine with the typical socket API. If you want to get access to all IPs the entire machine is talking too you'll have to use a lower level API. That's not standardized between OS. For Windows I'd look at Winsock API to see if you can find how to look at the tcp/ip stack of the OS. It gets more complex because TCP connections ride on top of IP packets. At what level do you want to look at? Just TCP or all protocols that also ride on top of IP. ARP UDP TCP ICMP to name a view standard ones. If you're just learning then doing just a simple server and all the connections talking to that server in your process is much simpler. Going lower is going to be jumping into the deep end quickly. Not that I'm saying you shouldn't this. Would you like to keep it simple and stay with the connections your process is talking to or would you like to go into the deep dark Window's forest? Choose your adventure. Edit: Better option for lower level access is WinPcap with a C# wrapper. That will let you get the raw packet information. Even let you put your PC in promiscuous mode and sniff all traffic on your subnet not just your PC. Not only IP addresses whit which my program is communicating. Every connection on my hole computer. Something like log information about connection activity. Well if you're just learning it's easier to do the just your program that's all I'm pointing out. You'll have to go lower with something like WinPcap or Winsock to see that information. Or writing a Network Driver Filter that gets inserted into the TCP stack. The OSI network model has levels so doing your process stays at the application layer at layer 4 the next layer down is TCPUDP etc at layer 3 then IP ARP at layer 2 and ethernet at layer 1 (this is hardware so you can't write software in this level but you can read from it). It gets lower level as you go down in the layers.  On Windows there's an IP helper library iphlpapi which can discover information about open connections on your machine. In particular you want to look at the GetTcpTable and GetUdpTable functions. The problem is this is a C API. To use these from C# requires you to use the P/Invoke API. It's a trivial task but can take some time to do. Each of the C structs used need rewriting in C# making use of the System.Runtime.InteropServices namespace to marshal them. Much of this can be automated or done by decorating struct fields with the correct attributes but there may be cases where you need to marshal a structure manually. The C functions you import need marking with [DllImport(""iphlpapi.dll"")] with any other relevant attributes depending on the function you're importing. You can read more about PInvoke at msdn and perhaps find some already written imports at the pinvoke wiki. In addition there is a tool which can attempt to automate much of it for you the Pinvoke interop assitant  Assuming you have ObservableCollection<string> mAccessInfo somewhere in your code as a publicly visible variable bound to your list you could place this in the part of the code where you listen for connections: Socket socketForClient = tcpListener.AcceptSocket( ); if (socketForClient.Connected) { mAccessInfo.Add(socketForClient.RemoteEndPoint.ToString()); //do other stuff... } Since it's ObservableCollection on each new connection that enters the list the list you're bound to is updated. Edit: As Cody Gray pointed out this won't work if you have IP's in between as proxy/router/NAT etc...  You could leverage the netstat command to get information about which IP addresses and ports your machine is communicating with. You could run this command on a regular basis and parse the text output. For example the following command will list the state of all connections including local and remote connection details: C:\Users\chibacity>netstat -a -n Active Connections Proto Local Address Foreign Address State TCP 10.0.0.107:32495 209.85.229.125:5222 ESTABLISHED TCP 10.0.0.107:32507 209.85.229.125:5222 ESTABLISHED TCP 10.0.0.107:32520 209.85.229.18:443 ESTABLISHED TCP 10.0.0.107:32755 209.85.227.109:993 ESTABLISHED //etc... Or if you have admin privilleges you can get executable details too: PS C:\Windows\system32> netstat -a -n -b Active Connections Proto Local Address Foreign Address State TCP 10.0.0.107:32495 209.85.229.125:5222 ESTABLISHED [googletalk.exe] TCP 10.0.0.107:32507 209.85.229.125:5222 ESTABLISHED [chrome.exe] TCP 10.0.0.107:32520 209.85.229.18:443 ESTABLISHED [chrome.exe] TCP 10.0.0.107:32755 209.85.227.109:993 ESTABLISHED //etc... You can call this command from .Net using System.Diagnostics.Process and parse the text output e.g: http://msdn.microsoft.com/en-us/library/system.diagnostics.process.standardoutput.aspx Reliably parsing the text output is close to impossible. It can change with every windows version. And it's localized. On a german windows XP it looks like this: ""Aktive Verbindungen Proto Lokale Adresse Remoteadresse Status TCP MyComp:4201 123.456.789.123 HERGESTELLT TCP MyComp:4206 stackoverflow.com:http WARTEND"" @CodeInChaos I see your point but describing pulling IP addresses out of that as ""close to impossible"" is over exaggerating. It would be **easy**. Surely you cannot be serious. The IP address format is not localized. I'm exaggerating a bit but that code won't be very reliable and I'd avoid it in my program if there is any other way. @CodeInChaos Again you are exaggerating. All versions of Windows produce an output that can be parsed with a simple Regex. Yes it's very hacky - I admit that! But it's not that problematic."
1085,A,"RSA encryption and decryption of a long message in java I want to use java standard library and as much as I know its functions inputs are limited. So I implemented two methods to this purpose. Here they are: private byte[] RSAenc(String in) throws Exception { Cipher c = Cipher.getInstance(""RSA""); c.init(Cipher.ENCRYPT_MODE privKey); int l = in.length(); byte[] part; byte[] result = new byte[(int)(64*java.lang.Math.ceil(l/20.0))]; int i = 0; while(i*20+20<l) { part = c.doFinal(in.substring(i*20i*20+19).getBytes(""UTF-8"")); System.arraycopy(part 0 result i*64 part.length); i = i+1; } part = c.doFinal(in.substring(i*20l-1).getBytes(""UTF-8"")); System.arraycopy(part 0 result i*64 part.length); return result; } private String RSAdec(byte [] in) throws Exception { Cipher c = Cipher.getInstance(""RSA""); c.init(Cipher.DECRYPT_MODE privKey); String result = """"; byte[] part = new byte[64]; int l = in.length; int i = 0; while(i+64<=l) { System.arraycopy(in i part 0 part.length); result = result + new String(c.doFinal(part) ""UTF-8""); i= i+64; } return result; } They work in this manner: for encryption I break the string to at most 20 size substrings and then use the Cipher to encrypt them. For decryption I break byte array to 64 byte blocks and apply Cipher decryption to them. Is there already a function which do this? Or at least is there a neater solution? How safe is my approach? Is encryption result length (result.length) always 64 on all distributions of JRE? Thanx See also [How can I use asymmetric encryption such as RSA to encrypt an arbitrary length of plaintext?](http://crypto.stackexchange.com/questions/14/how-can-i-use-asymmetric-encryption-such-as-rsa-to-encrypt-an-arbitrary-length) on crypto.stackexchange.com. RSA is suited to key encipherment not bulk data encryption. Instead of encrypting messages with RSA most protocols generate a key for a symmetric cipher like AES and encrypt the message with that. Then RSA is used to encrypt that symmetric key so that only the message recipient can recover it. The RSA-encrypted symmetric key is sent with the AES-encrypted message to the recipient. thanks ... is the symmetric key (for AES or DES) always smaller than 64 (the maximum for RSA encryptor) bytes? @kvphxga: Well a good RSA key should be at least 2048-bits so it should be able to encrypt a lot more than 64 bytes. But yes all of these ciphers use keys that are 64 bytes or less. @kvphxga: DES has 56 bit keys - this is quite small and should not be used today (since brute-forcing does not take that much time). AES has three variants with key sizes 128 bits (16 bytes) 192 bits (24 bytes) or 256 bits (32 bytes) and is considered secure."
1086,A,"Send and Receive a file in socket programming in Linux with C/C++ (GCC/G++) I have a client-server architecture where I want both the client and the server to send and receive files. How do I do that using sockets programming on Linux using C/C++? Is there a library that would make this easy? One solution that I am thinking of is partitioning the file and send the segments individually. This file will serve you as a good sendfile example : http://tldp.org/LDP/LGNET/91/misc/tranter/server.c.txt  The most portable solution is just to read the file in chunks and then write the data out to the socket in a loop (and likewise the other way around when receiving the file). You allocate a buffer read into that buffer and write from that buffer into your socket (you could also use send and recv which are socket-specific ways of writing and reading data). The outline would look something like this: while (1) { // Read data into buffer. We may not have enough to fill up buffer so we // store how many bytes were actually read in bytes_read. int bytes_read = read(input_file buffer sizeof(buffer)); if (bytes_read == 0) // We're done reading from the file break; if (bytes_read < 0) { // handle errors } // You need a loop for the write because not all of the data may be written // in one call; write will return how many bytes were written. p keeps // track of where in the buffer we are while we decrement bytes_read // to keep track of how many bytes are left to write. void *p = buffer; while (bytes_read > 0) { int bytes_written = write(output_socket p bytes_read); if (bytes_written <= 0) { // handle errors } bytes_read -= bytes_written; p += bytes_written; } } Make sure to read the documentation for read and write carefully especially when handling errors. Some of the error codes mean that you should just try again for instance just looping again with a continue statement while others mean something is broken and you need to stop. For sending the file to a socket there is a system call sendfile that does just what you want. It tells the kernel to send a file from one file descriptor to another and then the kernel can take care of the rest. There is a caveat that the source file descriptor must support mmap (as in be an actual file not a socket) and the destination must be a socket (so you can't use it to copy files or send data directly from one socket to another); it is designed to support the usage you describe of sending a file to a socket. It doesn't help with receiving the file however; you would need to do the loop yourself for that. I cannot tell you why there is a sendfile call but no analogous recvfile. Beware that sendfile is Linux specific; it is not portable to other systems. Other systems frequently have their own version of sendfile but the exact interface may vary (FreeBSD Mac OS X Solaris). In Linux 2.6.17 the splice system call was introduced and as of 2.6.23 is used internally to implement sendfile. splice is a more general purpose API than sendfile. For a good description of splice and tee see the rather good explanation from Linus himself. He points out how using splice is basically just like the loop above using read and write except that the buffer is in the kernel so the data doesn't have to transferred between the kernel and user space or may not even ever pass through the CPU (known as ""zero-copy I/O""). Brian I'm guessing you and florin will not get the check mark as this is almost certainly homework :( What happened to your original answer? You first suggested the 'sendfile' command on the *nix box now you've given a programmatic solution whilst removing all vestiges of your original answer. I didn't remove all vestiges of it. I just added the skeleton of a standard `read`/`write` loop above it. I figured that I should explain that first because even using `sendfile` you will need to use such a loop to receive the file and since I'd mentioned `splice` later on which uses the same pattern. Ah somehow missed the retention of 'sendfile'. Sorry. It should be pointed out that `splice`/`vmsplice` is mostly something that looks good in theory. In practice the buffer size is limited to 64k and there is no single good (or safe) way to send data exceeding that since there is no reliable way of knowing when exactly a buffer is not needed any more. There's a kernel developer discussion on that somewhere too (with some excursion into using 3 buffers) though I can't find it right now. Also for zero copy to truly work you must sometimes do obscure things (GIFT flag) which are not well-documented or conclusive. @Damon Do you have any references for more information? I'm curious about what exactly the gotchas are; is it less efficient than the standard userspace `read`/`write` loop or just not much more efficient? I don't actually have much experience with them myself so I'd love to learn more about the gotchas.  Do aman 2 sendfile. You only need to open the source file on the client and destination file on the server then call sendfile and the kernel will chop and move the data. How about receiving files in the same program ? Saved me lots of trouble thanks!"
1087,A,"Writing a dummy Network Shared Printer I want to write a dummy printer driver which appears as a shared printer on a LAN and can accept print jobs; so when its installed on a computer other computers in the LAN can browse and add it as a usual shared network printer and send print jobs to it. I want to do this in c# are there any better suggestions? Can anyone tell me any information regarding this tips tools reading material etc. anything! Thank you very much for educating me on that! there's no such thing as ""C#.Net"". It's just ""C#"". If you just want a printer that will accept jobs and not do anything with the data you don't need to write your own printer driver. Just create a local printer and when it asks you for the port create a new local port and give it the name NUL. From the Windows XP Professional Product Documentation: If the printer is physically attached to the print server select the appropriate local port. LPT1 through LPT3 represent parallel ports; COM1 through COM4 represent serial ports When a client prints to a printer port denoted as FILE the client is prompted for the file name. If you decide to add a new local port you can enter one of the following: A file name such as C:\Dir\Filename. All jobs sent to this port are written to the named file and each new job overwrites the last one. The share name of a printer such as \Server\Printer (URLs are not accepted). Jobs sent to this port are transferred over the network to the named share by the network redirector. NUL. This specifies the null port which you can use to test whether network clients can send jobs. Jobs sent to NUL are deleted without wasting paper or delaying real print jobs. IR. Use this port to connect to infrared-enabled printers meeting Infrared Data Association (IrDA) specifications. If your hardware does not support IR it will not be listed on the Ports tab. thank you very much I appreciate your answer very much. That fulfilled many of my questions. And yes I don't want to print the print jobs just need to accept the print jobs and direct them to a desired real printer according to a print job routing table. Imagine 3 PCs. one is sending a print job to a dummy printer job acceptor software on another PC and that pc then transfers it to a PC with a real printer. Thanks for your response can u suggest me any reading materials or even topics coz I have no idea only the passion to do it is all I have. Also any suggestion is welcome! Thanks"
1088,A,"Can I sniff UDP packets addressed another Linux machine using Python? I have a Python process on one Linux machine server1 that receives and processes raw UDP packets. I want to have another Linux machine server2 capable of listening to the same UDP packets server1 is receiving. Is there any Python solution capable of sniffing UDP packets addressed to the another (Linux) machine? I don't have control over the sending end. All that comes from a data provider. Is this from a stock exchange by chance? No it's just some embedded devices on the field. Oh ok- i know exchanges LOVE multicast. Anyway are you trying to get a second machine to process the data for redundancy or for more throughput? I'll take a look into it. I'm building a new python process that will do other things with the same data. If you want the processes on different machines (you think one machine can't do it all) I would have a Linux machine receive the data and using iptables send it to multiple other machines. Maybe to a different socket on the same machine. This is possible because its UDP. If you want it all on the same machine I would have a single process that spawns subprocesses with connected `PIPE`s binds the UDP socket and copies the data to each subprocess' pipe; maybe after some input validation. Any idea how can I use the iptables to send data to other machines? I didn't read though it all but this might help: http://www.bjou.de/blog/2008/05/howto-copyteeclone-network-traffic-using-iptables/ I think you're looking for the `--tee` flag / target. The traffic goes through a couple of unmanaged switches so I can't do much from the networking side. How about adding a python process on the same server: server1? Is that possible to have multiple Python processes listening to the same UDP packets on the same port number on the same machine? No- only a single process can listen to a socket. but you can have something listen to the socket and send the data to multiple other subprocesses. either on the same machine or other machines. What kind of data is it? It's a UDP packet with about 150 text characters inside it. did you write/own the sending end? I have had a similar problem and wrote a small python script to forward incoming udp packets to multiply hosts. A drawback here is ofcourse that you loose the source IP of the originating udp packets. import socket import sys time string def sendUDP(remotehostremoteportUDPSockdata): UDPSock.sendto( data (remotehostremoteport)) def serverLoop(listenportremotes): # Set up socket UDPSock = socket.socket(socket.AF_INETsocket.SOCK_DGRAM) UDPSock.bind( (""0.0.0.0""listenport) ) while 1: data addr = UDPSock.recvfrom(1024) if not data: pass else: sys.stdout.write(""."") ; sys.stdout.flush() # Send udp packet to remotes... for remote in remotes: sendUDP(remote[0]remote[1]UDPSockdata) time.sleep(0.001) if __name__ == ""__main__"": if len(sys.argv) < 3: print ""%s listenport remotehost1:port1 remotehostN:portN ..."" % sys.argv[0] sys.exit(-1) listenport = int(sys.argv[1]) print ""Local foward port %d"" % listenport remotes = [] for pair in sys.argv[2:]: hostport = string.split(pair"":"") remotes.append( (hostint(port)) ) print ""Adding remote forward %s:%s"" % (hostport) print ""Starting serverloop"" serverLoop(listenportremotes) I wrote something similar and I did lose the originating IP as you said. In my case I need the originating IP since I need to communicate back. Any ideas how to solve the IP issue? Just noticed a vulnerability in this code. You have to check if the originating ip is one of the address:port in remotes or else you might have a feedback loop which will exhaust your resources @itgorilla you put the originating ip address in the data; data = struct.pack(""!LH""ip2long(addr)port) + data then unpack it on the receiving part. Another way is to spoof the source address but require superuser privileges. That's what I thought I should do also.Thank you! take a look at http://www.sk89q.com/2008/10/spoofing-a-udp-packet-in-python/ Be aware that you might have some trouble with ip-spoofing filtering routers/firewalls.  If you want more than one machine to process the same data you'd be better off going to mulitcast (if you can control the sender and the infrastructure) Else http://sourceforge.net/projects/pylibpcap/ will enable packet capture via python. You will still have to configure the infrastructure to get the packets to the machine you want to sniff them. Either by iptables (if is a Linux machine) or a mirror port on the switch etc. Edit: If you want the processes on different machines (you think one machine can't do it all) I would have a Linux machine receive the data and using iptables send it to multiple other machines. Maybe to a different socket on the same machine. This is possible because its UDP. If you want it all on the same machine I would have a single process that spawns subprocesses with connected PIPEs binds the UDP socket and copies the data to each subprocess' pipe; maybe after some input validation +1: I was forgetting that he's using UDP. Multicast could be a good option  This not depends on Python but on your network architecture. If server1 and server2 are connected (probably they are) through a switch then you can't do it because the packet passing through the router will be sent only to the requested IP. So first of all tell us how is composed your network architecture. Where are server1 and server2? How the reach each other? Your problem solution won't depend neither on your OS nor in the language used. Anyway you tagged your question ""linux"" so I think you are familiar with that OS. If this is the case and server1 and server2 access the LAN through the same router you can evaluate the possibility of installing linux on your router (have a look at openwrt) and perform the sniffing and whatever from the router itself. +1 since any solution is really going to be about network architecture not programming... Although ARP spoofing can be used to sniff on switches. If its a managed switch most of them have a mirror port option. You setup a mirror port and can sniff all the traffic from the configured mirror port. @Douglas Leeder. yes it can but it isn't a proper solution but just an hack. That way one of the two server will act like the router itself..so.."
1089,A,"Why am I having ""Invalid argument"" while trying to accept conections? In the next code while I try to connect a client the server shows the following error: ""invalid argument"" I can't see the error. if((l_sock=socket(AF_INETSOCK_STREAM0))!=-1) { struct sockaddr_in srv_dir; srv_dir.sin_family=AF_INET; srv_dir.sin_port=8500; srv_dir.sin_addr.s_addr=INADDR_ANY; if((bind(l_sock(struct sockaddr *)&srv_dirsizeof(struct sockaddr_in)))!=-1) { if(!(listen(l_sock5))) { signal(SIGINTcerraje); int t_sock; struct sockaddr_in cli_dir; socklen_t tam; time_t tstmp; struct tm * res; res=(struct tm *)malloc(sizeof(struct tm)); while(!key) { if((t_sock=accept(l_sock(struct sockaddr *)&cli_dir&tam))!=-1) { tstmp=time(&tstmp); res=gmtime(&tstmp); send(t_sockressizeof(struct tm)0); wr_hora(*rescli_dir.sin_addr); } else perror(""Petición no atendida"");//The error is printed here. Thank you very much. Read the documentation on accept(2): The addrlen argument is a value-result argument: it should initially contain the size of the structure pointed to by addr; on return it will contain the actual length (in bytes) of the address returned. When addr is NULL nothing is filled in. So you need to initialize the value of tam passed into accept with sizeof(cli_dir). You're fortunate that the socket library was able to catch your error because you're passing in uninitialized memory which results in undefined behavior. That's it thank you very much. I supposed that the variable tam was only an output value so I don't even think that it needs an initial value."
1090,A,"Wormhole Attack Implementation -Sensors I am trying to implement a wormhole attack detection program for network sensors. I have looked around the internet for source code on this but have found none. Does anyone know where to find source code for Wormhole sensor Attacks (for simulation purposes) or wormhole detection? If so where?. It can be in any programming language. Thanks everyone. Note: I know there are documents out there with Psuedocode on them that I can recreate but I was wondering if there is source code already out there in order to save me some time. If notthen I'll code it. Edit: I'll even take ones with Psuedocode right now. I just have to say this might be the single best question title I've ever seen. hahahahahaha thanks @shobhonk I guess I mean we have a test data generator.the problem is on our project we need on that works and is post detection which runs into a lot of problems. Get a wired router? Seriously though this is a massive topic and project that people write thesis about. You aren't going to find readily available complete code examples as the problem hasn't really been adequately solved yet. I know I'm working on one I just wanted to ask the community if anyone knew of any readily available code available. I have searched and found nothing I just didn't want to recreate the wheel if I didn't need to. Also a wired router doesn't work in a mobile sensor network sorry.... Inverted tachyon beam from deflector dish tbh Thanks dude but I am being serious about this. Do some lit review read how people went about this problem. Choose best methods i know its hard what others did by reading academic paper. I would use Opnet first to try to simulate some of the ideas used by various authors. @TheChes44 I would recommend contacting the authors of published academic papers. They may be willing to share their simulation code. At least it's worth a try. I mean I am but some will not send it. Just how authors are but I will keep that in mind though. Thanks. It looks like you can ask these students for their implementation. http://www.wings.cs.sunysb.edu/~ritesh/wormhole.html  The wormhole attack has a lot of cases we should keep in mind. The network is set up and has no plans to add anymore nodes (easy dont let anything connect but i guess with my limited understanding of ah hoc this maybe pointless) The network is setup and nodes are added often (they look like a normal computer i think changes in the tables they collect should point to a wormhole set up) the wormhole is not seen and mirrors the other side of the network with out changing the packets at all only copying them (in this case you have to use the time the packet gets to it's target and that is very ""resource heavy"") The network does not have two sides it has many or one. I am sure missed other cases but I think it maybe best to explain what your looking into. I would think that if you have a network with a bridge you can just check to in sure all packets are somehow sent to it. In the paper you have listed they list forbidden routes. When they find that one is made they know there is a wormhole. say that there is a node on A side named D and it should never link to Q in less then 4 hops that shows us a wormhole. I think this is easier to answer if taken in parts then built in to one problem. so if you want try to explain what you're looking for and i'll see what i can dig up or code very poorly. I think finding a program that will sim a network will be nice then add a random wormhole and see what changes. logically we know the layout of the network but we need a case that will always work and that's hard to come by. Maybe there is a base case I am over looking? do you understand how it works in C? I am looking to rewrite the code they have into java right now. Any suggestions? Hell no. The program doesn't even work. The guy who made this will not offer tech support so I am lost I am a c++ guy and only limited to the class room so far but I will see what I can do. I think if we simplify the problem it will be easyer. example(we write a java program that shows us a ""ad hoc"" nextwork) then we add on the ability to add a wormhole to the next work before and after ""ad hoc set up"" @TheChes44 let us [continue this discussion in chat](http://chat.stackoverflow.com/rooms/1851/discussion-between-dennis-hayden-and-theches44) you have the code they used? if you dont mind can you post it? I keep thinking about this problem thought out the day. but i'm not getting far i can understand the context. But i feel like i am missing something about the idea. They have it on their website the source code. Trust me in the fact that it doesn't help.I am looking at it and there are no comments at all. I am struggling thru the code. As there is not a correct impmenentation I might just roll with this one and convert it to java.But.....converting C to Java is ahrd http://www.wings.cs.sunysb.edu/~ritesh/code/ If I can get this to work in Java I can tweak it the way I want.  I'll try and find anything which might be useful: A Study on Wormhole Attacks in MANET (uses ns-2): http://www.mirlabs.org/ijcisim/regular_papers_2011/Paper32.pdf Wormhole Attack Detection in Wireless Sensor Networks: http://www.indiastudychannel.com/attachments/Resources/112464-22934-Wormhole-Attack.pdf Psuedocode of the wormhole implemented would be appreciated. Thanks."
1091,A,"Preferred Placement of a Network Collector in a Switched Environment I'm not a network specialist so my apologies if i've used some of the domain terminology incorrectly etc. For web metrics/analytics we currently use both client-side (js page tags) and server-side (log files) data. Neither gives us ""delivery"" information (e.g. connection speeds) hence the interest in Network Collectors. We are in a switched environment so installing the N/C as if it were a web server i.e. on a switch port won't allow it i don't think to see the web server traffic. After some research i've learned how to place the N/C by configuring a monitoring port. What concerns me about this is the m/p appears work by duplicating the traffic within the switch. Is there are better solution for N/C placement in this type of network environment? I think this type of questions would better be addressed at serverfault.com @ mjv : i thought about that and decided against it because the information is most relevant to Web Analytics people and from comparing the frequency of the relevant tags most of those types are at SO. Don't worry Doug switches nowadays won't falter under this sort of load. The way you have explained is quite OK. Of course you could buy a more expensive switch with ""NetFlow"" sort of support... and have the switch collect the data for you.... @ jl: nice one thanks."
1092,A,"Tcp connections hang on CLOSE_WAIT status Client close the socket first when there is not much data from server tcp connection shutdown is okay like: FIN --> <-- ACK <-- FIN ACK ACK --> When the server is busying sending data: FIN --> <-- ACKPSH RST --> And the server connection comes to CLOSE_WAIT state and hang on there for a long time. What's the problem here? client related or server related? This happens on Redhat5 for local sockets. This article talk about why ""RST"" is sent but I do not know why the server connection stuck on CLOSE_WAIT and do not send a FIN out. [EDIT]I ignored the most important information this happens on qemu's slirp network emulation. It seems to be a problem of slirp bug for dealing with close connection. This a known defect for qemu. Better URL: http://lists.gnu.org/archive/html/qemu-devel/2008-06/msg00372.html  This means that there is unread data left in in the stream that the client hasn't finished reading. You can force it off by using the SO_LINGER option. Here's relevant documentation for Linux (also see the option itself here) and [here's the matching function2] for Win32. It's the server side that is remaining open so it's on the server side you can try disabling SO_LINGER. If the server's hanging on a write call then you've probably filled the TCP window and the stack is waiting for ACKs from the client before it can accept more data to send... It seems that the SO_LINGER only influence the close() call but the server hang on write() call instead?  It may mean that the server hasn't closed the socket. You can easily tell this by using ""lsof"" to list the file descriptors open by that process which will include TCP sockets. The fix is to have the process always close the socket when it's finished (even in error cases etc) The problem is that the server is hung on write call and I can not detect error."
1093,A,"Fastest communication between a Android and PHP Hi all Im building a android application that requires data from a PHP server. Lets say i have a text input on my web application. Whenever the users trigger a ""onkeydown"" i want to send the character to the Android application and my Android application will just output the current character. The web application will constantly send small ""commands"" to myAandroid application. I would like to know what is the fastest possible way to receive those send / receive those commands. I see a few options on how this could be implemented. 1) My android application constantly polls the server to see if a message has been written. 2) My Android application would setup a socket server that the PHP script would connect and push the message to. 3) I guess this is extremely slow and inaccurate but i guess i could use push notifications. Is there any other alternatives ? and what would be the best solution?. Push message is not slow just inaccurate (ie. no guarantee that it will be sent). Still it's probably faster to receive a push message than to poll the server every - say - five seconds. If you can afford the time it's worth combining the two approach ie push messages and regular pooling as fall back. That said you don't provide enough info on what you're doing so it's difficult to suggest anything. The fastest way would be to connect to the server leave the connection open and have PHP slowly stream little quantities of gibberish data until it has a real message to send. That's the fastest thing but of course is a no-no in terms of battery usage and so on. So please explain what you're trying to do. UPDATE to remote command a phone you need an open connection. There are many ways to do it I'll post just a proof of concept php. <?php //to send a command to the phone just create a file //named command.txt in this directory //from any other script set_time_limit(0); while (true) { if (file_exists('command.txt')) { $command = file_get_contents('command.txt'); rm( 'command.txt' ); echo $command; flush(); unset ($command); } else { echo asc(0); //prevent the connection to be dropped flush(); usleep(200); //sleeps for 200 ms } } ?> This example lacks many things it's just a proof of concept on how to handle an open connection with PHP. The client side would need to strip all zero bytes and keep the rest of data downloaded. This is in absolute the fastest way to handle a 1 way remote interaction on the other hand will suck battery pretty fast :). Hi tacone. Ive updated my example to better reflect what im trying to archive :) I dont see why push notification would be faster because i would have to add an addiotional ""step"" for my message ""web"" -> ""push service"" -> ""device"". If you poll the server every 5 seconds on the average you may have to wait 2.5 seconds until you discover the server has a new message. A push notification may take less than a second to arrive. Is using push notifications this heavy really a way i need to investigate? I would think using the push service like this could be considered spam?. Let say battery isnt of any concern for this application would a socket solution then be the ideal choise? push notifications would not fit your requirements they would be too many. i wrote about it before your last update. I'll update my answer now. Thank you very much for your help!"
1094,A,"Web server on port 80 on iPhone I've been working on a project for awhile and it's got a built-in HTTP server which runs on port 8080. The users are told to access the device via e.g. http://192.168.1.4:8080/ -- works great. Recently I realized that applications CAN use port 80 to remove the need for "":8080"" though if I try to set the port to 80 I get a crash with ""General CFSocket error"". Any ideas how to enable port 80 for a web server on an app? A few screenshots where this is happening: First -- on the iPad the app is showing the URLs where you can access it. Second -- Firefox by IP: The above are from a real app on the store it's not jailbroken magic or anything. I know that ports < 1024 are reserved for the admin on UNIX systems so the above app is obviously doing something specific to get access to the port. Any chance another application is using that port or you opened the port in the past with your application and forgot to clean up properly? Tried powering off and back on the phone then trying fresh? I open my app crash open other app works open my app crash. Nope not a cleaning up problem. Oh and this is not iOS 4 so no other apps are running. And I think keeping a network socket in iOS 4 while in background results in immediate termination so it shouldn't be the case even then. iPhone is unix based. Ports below 1024 are reserved for the root/superuser. You need to be root in order to use those ports. Yes. In other words you can't open ports below 1024 except when running as root on a jailbroken phone. I presumed this was the case which it obviously is not because apps are accessing port 80. Check out Air Sharing. It's a little expensive but it does it. at least the HD version for iPad.  You can bind to port 80 on the device's IPv4 interface but not the IPv6 interface and not in the simulator. You'll need to modify your socket code to only listen on the IPv4 interface for the simulator you can conditionally use a different port: #if TARGET_IPHONE_SIMULATOR [httpServer setPort:8080]; #else [httpServer setPort:80]; #endif I'm not running on the Sim so that's not the prob -- I guess the socket code is the culprit then. Will poke and comment again! The user would need to be on an IPv6-only network for the connection to fail I don't think you'll find too many in that environment. It would be a good idea to file a radar on this the behavior should be consistent. Holy... it worked. You rock Mr. Matt! :) This of course leaves the problem: what about users who WANT IPv6? I guess you might put in some configuration value somewhere which enables IPv6 if the port is > 1024 but hm preferably Apple should just let you use port 80 period. It seems like it isn't working anymore with 4.0.2"
1095,A,"to find a list of connected server in android device? Programmability i want monitor IP ever connected to my Adnroid device. My initial thought is i can write a background service which will run tcpdump command and forward its output to inputStream. By putting any regular expression i can retrieve list of connected IP to my device. I think that would be bulky to continually run command like tcpdump. any better solution ?? Edit - typo You'll have to choose how often to run tcpdump and that'll be tricky - running it more frequently will have a performance impact but running it less frequently may mean that you miss short-lived connections. Perhaps you could set up a logging ""firewall"" on your device using iptables ( http://www.linuxquestions.org/questions/linux-security-4/iptables-logging-385165/ ) and then parse the output of THAT whenever you actually need the output? TCPDump needs you to root your Android device and I dont want to do that. Anything way around ??"
1096,A,How long after locking the screen on an iphone does the app keep running? I've been trying to find the documentation to say how long an app keeps running with an iPhone locked and how long a network connection stays active. I'm trying to verify that one of our network connections gets handled correctly once the app has been put to sleep and lost network connectivity Say your are playing an audio in background and if you set the number of loops to -1 it will play forever.  Apps stop in approximately 10 minutes with battery > 20% and in 5 minutes with battery > 5%. If iPhone is hooked up to a power source (USB / AC) the app will not be auto-stopped (unless it uses too much memory etc). You could make your app use background services at locking time and continue to run without being terminated.  It's not based on when the phone locks it's based on when your application goes in to the background (which locking the phone will do but so will lots of other things). short answer: 10 mins. long answer: there's a variety of special background behaviors (for VOIP music streaming and a few others). Since you didn't say what you're doing i'm going to assume they don't apply. The assumption is true no special use of background apis
1097,A,Ubuntu background task sockets running on Mono Basically I have this server app I built in vc# and for some reason when I run the task like this mono oserver.exe & It continues to run but I can't connect to it. But if I just call it normally like this mono oserver.exe it works fine? Is there a better way to do this that works? Or maybe certain things that mono can't do in the background? Not sure. Any ideas? Actually what I ended up finding out was that the working directory of the app changed as I moved the app from forground to background with fg and bg. I have no idea why this would happen but making absolute paths in my app allowed my app to run in the backgound. I also used mono oserver & > /dev/null.  Did you mean mono oserver.exe & some-another-command ? Actually what I ended up finding out was that the working directory of the app changed as I moved the app from forground to background with fg and bg. I have no idea why this would happen but making absolute paths in my app allowed my app to run in the backgound. I also used mono oserver & > /dev/null.
1098,A,"Java: How send and receive data using socket? I'm building a middleware where I need to constantly read what is happening in my device so I build this class: import java.io.DataInputStream; import java.io.DataOutputStream; import java.io.IOException; import java.net.Socket; import java.net.UnknownHostException; /** * * @author Valter */ public class Middleware { public static void main(String args[]) { try { // ip and port where is my device Socket socket = new Socket(""192.168.1.4"" 2001); DataInputStream dataInputStream = new DataInputStream(socket.getInputStream()); DataOutputStream dataOutputStream = new DataOutputStream(socket.getOutputStream()); // i need send this parameter to my device dataOutputStream.writeUTF(""}Rv!""); String answer = dataInputStream.readUTF(); System.out.println(""Answer:""+answer); dataInputStream.close(); dataOutputStream.close(); socket.close(); } catch (UnknownHostException ex) { System.out.println(""UNKNOW HOST EXCEPTION""); } catch (IOException ex) { System.out.println("" IOEXCEPTION""); System.out.println(ex.getMessage()); } } } Output: IOEXCEPTION Connection reset What's wrong with my class? Have you tested this device by any other means? Maybe your java code is doing what it's supposed to be doing and your device is misbehaving ? i'm using putty but i'm not receiving any information from my device.. i don't know if it working properly.. do you know any software that i can use it to test it ? I don't know if this is what is causing your problem but you are not flushing the output stream before attempting to read a response from the input stream. Try: dataOutputStream.writeUTF(""}Rv!""); dataOutputStream.flush(); I agree with that. it keeps return the exception. You don't need to flush a DataOutputStream unless there is a buffered stream underneath it. @EJP: Correct but `Socket.getOutputStream()` does not specify whether the returned output stream is buffered or not. My understanding is that in practice it is not (the returned stream is a non-buffered extension of `FileOutputStream` I think) but there is no guarantee. Regardless flushing a request before attempting to read the response is a matter of best practice. I was attempting to read data back from a LWIP implementation on a micro and I needed to call the flush function before I could read the buffer. Thanks @NathanD.Ryan  Does your device really understand the output of writeUTF()?and produce the correct input for readUTF()? Check the Javadoc. I don't think it likely. 'Connection reset' usually means you have written to a connection that was already closed by the other end which is already an application protocol error and it may indicate prior application protocol errors."
1099,A,Can traffic to multiple services on the same port be identified and re-directed? Think of the following services on one box: SOCKS proxy HTTP proxy SSH service VPN service I have found a case where it would be beneficial to run all of these services on the same box (save on high server costs with low usage) but they all need to listen on port 80 (network security restrictions require it). I'm a proficient Java developer. What I am brainstorming is whether it's realistic to consider a simple Java app listening on port 80 determining which service a new connection is bound for and then redirecting traffic from that connection to a local port where the service is listening. Is there something in the initial packets after the connection that I would be able to queue off of to determine the appropriate service? Creative thoughts are most welcome. I don't know the structure of all of those protocols but I would think that the easiest way to find the answer to your question would be to simply write a program that listens on port 80 and writes the initial data to log files and then connect with each of the above protocols and see if there are obvious patterns. Running a network analyser on either the server or the client like WireShark would also work and you don't have to write any code. Once you know the patterns you probably should look up the protocol documentation to verify whether it is really reliable.  I agree with Luke's answer and I think that such a creature is within the realm of possibility. Other factors to consider: If the server receives heavy traffic there may be some performance impact to running this java redirection service especially if your heuristics for determining the appropriate destination service are complex. For the HTTP service you may want the java redirector to issue something like a 301 Moved Permanently to the new port.
1100,A,"TERMINATED message in thread join I am using pthreads on unix in socket programming. When the parent thread is waiting for it's child to join I receive a message ""Terminated"" at the very end this is not a statement that I am printing out from my program. Could anybody help me and can tell what could be the reason for this message? This happens when parent is waiting on join and child threads starts to exit but eventually I am getting this message. Thanks to all Terminated is often the output when a program is sent the SIGTERM signal: Start this in one terminal: $ sleep 10 Terminated From another terminal: $ killall sleep -SIGTERM I'm not entirely sure why your process is receiving a SIGTERM but I wonder if your child is calling exit(2) or exit(3) directly rather than calling pthread_exit(3) or falling off the end of its function.  The ""Terminated"" message is printed by your shell since it noticed that your application was killed with a SIGTERM. This is likely due to a bug in your code - we'd need to see source in order to find it. Thanks a lot for the information i got the hint from the SIGTERM and I was able to debug since I was sending the kill() with SIGTERM but wasn't using the signal() command to catch that signal.....thanks again :)"
1101,A,Is there a way to get destination mac addres in ANSI C Is there a way to get the destination MAC address if the destination IP is known using ANSI C? I would preferably do it with a system call so the kernel deals with it all and I can take advantage of the ARP cache. I know I can build and send my own ARP request but then I would not have the caching functionality etc unless I implement it myself so it feels like letting the kernel handling it is the way to go. What I decided to do was to send my own ARP packets and caching it internaly in my application. If the destination is outside my local network I parse /proc/net/route and extract the gateway for the given interface and send an arp packet to the gateway to get it's macaddress (since that is the destination macaddress of packets destined outside the local network). Since you apparently have access to `/proc/net` why take this complex route instead of just looking up the arp table? Beacuse in my arp table there were several enteries eth0. 192.168.0.1 192.168.0.2 and 192.168.0.236. But I actually only have one gateway and that's 192.168.0.1. Since that was the only one listed in /proc/net/route as the default gateway it felt safer to use that since in /proc/net/arp there were no sign to tell which one was the default gateway. In the arp table I even had entries for co-workers computers on the network. What I could have done was to extract the ip to the gateway from /proc/net/route and get the mac from /proc/net/arp.  Not an exact answer because it's not ANSI C but you can read the arp table from /proc/net/arp (in Linux that is). That's where arp looks. For any other OS the easiest way is to use strace or an equivalent on the equivalent arp-cache-showing utility.
1102,A,TCP implementation in Java using UDP (Datagrams) For a file sharing application I would like to use TCP/IP Sockets. But this doesn't work because both clients are behind NATs. Therefore I have to use UDP and the hole punching method. Since UDP is not reliable I have to implement a TCP-like protocol. I don't need streams. A message based protocol would be ok. Is there already a free Java library somewhere? If not is there an abstract tutorial how to implement such a protocol? That (TCP over UDP) is what OpenVPN does. I do not know if there is any java implementation but the protocol should be easily available (do not know if it is difficult to implement). You Can Implement TCP hole Punching that will allow two peer to communicate behind fire wall. first go through this link http://www.brynosaurus.com/pub/net/p2pnat/ and then update the question if need further help on this.But the problem with TCP hole punching is that it can be successful in 60% of router available in the market whereas UDP hole punching is 80-90% successful. check out this link as well.http://stackoverflow.com/questions/917385/nat-traversal-with-java  was it helpful? http://www.syslog4j.org/ Not realy. Have looked into the UDPNetSyslog class. But it simply sends the message with UDP. But does not have any TCP. If the message gets lost on the way nobody would note this. Have I looked into the wrong class? While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.  If you search for a UDP-based message-oriented protocol I'd consider TeleHash. There are also Java libraries available. Maybe it's the wrong choice if you rely on high volumes of binary data to be transmitted but then a stream-based protocol might be the better choice anyway.
1103,A,"Why does a SOAP message have to be sent over HTTP? Below is a demo SOAP request message: HTTP/1.1 200 OK Content-Type: text/xml; charset=""utf-8"" Content-Length: nnnn <SOAP-ENV:Envelope xmlns:SOAP-ENV=""http://schemas.xmlsoap.org/soap/envelope/"" SOAP-ENV:encodingStyle=""http://schemas.xmlsoap.org/soap/encoding/""/> <SOAP-ENV:Header> <t:SessionOrder xmlns:t=""http://example.com"" xsi:type=""xsd:int"" mustUnderstand=""1""> 5 </t:SessionOrder> </SOAP-ENV:Header> <SOAP-ENV:Body> <GetStockQuote xmlns=""http://someexample.com""> <Price>MSFT</Price> </GetStockQuote> </SOAP-ENV:Body> </SOAP-ENV:Envelope> And we can see this SOAP message is encoded as if it is a web page. Why do we have to use the HTTP protocol? SOAP message is just some XML why not we just use XML as the information exchange protocol and get rid of the HTTP headers (thus leave HTTP alone). Many thanks. Update - 1 HTTP is not a transport level protocol. It is just a application-level protocol. It has nothing to do with transport. Actually my question is what's the motive of adding HTTP stuff to a SOAP message? XML is not a networking protocol it is set of data encoding rules. Who says SOAP works only over HTTP?? It seems like your actual questions are about what HTTP is what purpose it serves and how it functions. Once you've gained an understanding of that the reasons for the use of HTTP in SOAP implementations should be clear to you. @jball thanks for reminding. Maybe this question could be really solved if I fully understand what magic HTTP protocol has. And this magic qualifies HTTP as a protocol for distributed computing. All the browsers supports HTTP for compatibility and its the most widely used Internet Protocol. SOAP is a communication protocol that specifies the format for sending messages. RPC and CORBA has compatibility and security issues whereas HTTP is compatible with all the browsers. Now that HTTP communicates over TCP/IP. A SOAP method is an HTTP request/HTTP response that compiles with the SOAP encoding rules. using SOAP a protocol submitted to the W3C data can be enclosed in XML and transmitted using any number of Internet Protocols.  you can use TCP too and that was named .NET Remoting before and now its part of WCF...  The motive of using HTTP was to get through firewalls. You see most network IT people do not allow just any port to be open but for some reason they always allowed port 80 to be open for web pages. Because web servers have been tested over the years it is ""easier"" to secure them. By using HTTP you have an existing set of tools for dealing with a communications protocol. Thanks for your reply. If so what we should do is not to add HTTP headers but to make the SOAP message go through the port 80. Isn't it? That is a solution however there are smart switches now that can do packet inspection to see if it is a valid HTTP response/request. If this is in a intranet and you control the IT then yes you can just use port 80 as a regular port. If not you run the risk of a switch denying the communications because it isn't in proper format. Thanks. We are closing the target. So just adding HTTP headers will fool the firewall. If so I could just adding a dummy HTTP header to my SOAP message as long as it is a well-formated HTTP request/response. So the question comes down to: Does the HTTP part content has any actual meaning? Or it just exist to fool the firewall?  Overview SOAP is a messaging protocol and in a nutshell is just another XML language. Its purpose is the data exchange over networks. Its concern is the encapsulation of these data and the rules for transmitting and receiving them. HTTP is an application protocol and SOAP messages are placed as the HTTP payload. Although there is the overhead of HTTP it has the advantage that it is a protocol that is open to firewalls well-understood and widely-supported. Thus web services can be accessed and exposed via technology already in-place. SOAP messages are usually exchanged via HTTP. Although it is possible to use other (application) protocols e.g. SMTP or FTP the non-HTTP bindings are not specified by SOAP specs and are not supported by WS-BP (interoperability spec). You could exchange SOAP messages over raw TCP but then you would have web services that are not interoperable (not compliant to WS-BP). Nowadays the debate is why have the SOAP overhead at all and not send data over HTTP (RESTful WS). Why use HTTP for SOAP? I will try to address in more detail the question in the OP asking why use HTTP for SOAP: First of all SOAP defines a data encapsulation format and that's that. Now the majority of traffic in the web is via HTTP. HTTP is literary EVERYWHERE and supported by a well-established infrastructure of servers and clients(namely browsers). Additionally it is a very well understood protocol. The people who created SOAP wanted to use this ready infrastructure and SOAP messages were designed so that they can be tunneled over HTTP In the specs they do not refer to any other non-HTTP binding but specifically refer to HTTP as an example for transfer. The tunneling over HTTP would and did help in it's rapid adoption. Because the infrastructure of HTTP is already in-place companies would not have to spend extra money for another kind of implementation. Instead they can expose and access web services using technology already deployed. Specifically in Java a web service can be deployed either as a servlet endpoint or as an EJB endpoint. So all the underlying network sockets threads streams HTTP transactions etc. are handled by the container and the developer focuses only on the XML payload. So a company has Tomcat or JBoss running in port 80 and the web service is deployed and accessible as well. There is no effort to do programming at the transport layer and the robust container handles everything else. Finally the fact that firewalls are configured not to restrict HTTP traffic is a third reason to prefer HTTP. Since HTTP traffic is usually allowed the communication of clients/servers is much easier and web services can function without network security blockers issues as a result of the HTTP tunneling. SOAP is XML=plain text so firewalls could inspect the content of HTTP body and block accordingly. But in this case they could also be enhanced to reject or accept SOAP depending on the contents.This part which seems to trouble you is not related to web services or SOAP and perhaps you should start a new thread concerning how firewalls work. Having said that the fact that HTTP traffic is unrestricted often causes security issues since firewalls are essentially by-passed and that is why application-gateways come in. But this is not related to this post. Summary So to sum up the reasons for using HTTP: HTTP is popular and successful. HTTP infrastructure is in place so no extra cost to deploy web services. HTTP traffic is open to firewalls so there are no problems during web service functioning as a result of network security. Thanks for your reply. But what magic makes a SOAP message can pass a firewall once it is stuffed as a HTTP payload? @smwikipedia:It is not SOAP that does that.Firewalls do not restrict HTTP traffic in port 80 even in private corporations so HTTP is very convenient for communication among client/servers behind firewalls.Firewalls is a MAJOR issue in distributed computing and HTTP helps on this since its traffic is allowed. As a result web services (as a distributed mechanism) can function with no such headaches due to security blockings @smwikipedia:Please see my update in my answer.If you have an HTTP server deployed in port 80 you can have a rule to accept all inbound traffic to port 80 for the specific IP. For outbound you can have more complicated rules or use a proxy. HTTP is plain text so it can easily be inspected. On what criteria does a firewall decide it is HTTP traffic? The port number can be easily changed. Does the firewall make the decision based on the traffic format? If so how about we just add some dummy http headers to fool the firewall?  It is up to developer to choose the transfer layer for Simple Object Access Protocol. The XML is not a network protocol so the data cant be transfered using just XML. It has to be packed into something.  Another reason might be that (if I remember correctly) HTTP is also designated as a ""gold standard"" for how an internet protocol is supposed to look/work so if you were to develop an own protocol you'd basically (in an ideal world at least) end up with something very similar if you followed all the RFCs. Therefore why not use HTTP one of the worlds most common and well understood protocols.  SOAP can be sent over different transports. HTTP is just one of them. HTTP is not a transport level protocol. It is just a application-level protocol. It has nothing to do with transport. Actually my question is *what's the motive of adding HTTP stuff to a SOAP message?* In SOAP terms it is a transport. In the WSDL you bind the web service to a transport. I think I've read that the motivation behind the decision to allow HTTP as one of multiple transports is that most firewalls are already configured to allow HTTP (I know that's a rather silly explanation but I haven't seen any other explanation). Thanks I have read similar explanations related to firewall. But based on what could a firewall *believe* that it is a HTTP packet that is passing through? The existence of HTTP headers? So we add HTTP headers to SOAP messages? Not so persuasive. It's not really a silly explanation you have a pre defined set of protocols for dealing with HTTP. Using tried technology can save quite a bit of time. There are servers that have been tested and are secure hopefully preventing Bufferoverflow attacks etc. It is quite difficult to design a secure application layer protocol that goes over the internet. This is a simple way to use an existing protocol for messaging and is very portable across platforms. Thank you Andrew. Actually this answer does not relate to the reasoning behind using HTTP - Cratylus's does this much better."
1104,A,"programs hangs during socket interaction I have two programs sendfile.py and recvfile.py that are supposed to interact to send a file across the network. They communicate over TCP sockets. The communication is supposed to go something like this: sender =====filename=====> receiver sender <===== 'ok' ======= receiver or sender <===== 'no' ======= receiver if ok: sender ====== file ======> receiver I've got The sender and receiver code is here: Sender: import sys from jmm_sockets import * if len(sys.argv) != 4: print ""Usage:"" sys.argv[0] ""<host> <port> <filename>"" sys.exit(1) s = getClientSocket(sys.argv[1] int(sys.argv[2])) try: f = open(sys.argv[3]) except IOError msg: print ""couldn't open file"" sys.exit(1) # send filename s.send(sys.argv[3]) # receive 'ok' buffer = None response = str() while 1: buffer = s.recv(1) if buffer == '': break else: response = response + buffer if response == 'ok': print 'receiver acknowledged receipt of filename' # send file s.send(f.read()) elif response == 'no': print ""receiver doesn't want the file"" # cleanup f.close() s.close() Receiver: from jmm_sockets import * s = getServerSocket(None 16001) conn addr = s.accept() buffer = None filename = str() # receive filename while 1: buffer = conn.recv(1) if buffer == '': break else: filename = filename + buffer print ""sender wants to send"" filename ""is that ok?"" user_choice = raw_input(""ok/no: "") if user_choice == 'ok': # send ok conn.send('ok') #receive file data = str() while 1: buffer = conn.recv(1) if buffer=='': break else: data = data + buffer print data else: conn.send('no') conn.close() I'm sure I'm missing something here in the sorts of a deadlock but don't know what it is. TCP is a streaming protocol. It has no concept of message boundaries. For a blocking socket recv(n) will return a zero-length string only when the sender has closed the socket or explicitly called shutdown(SHUT_WR). Otherwise it can return a string from one to n bytes in length and will block until it has at least one byte to return. It is up to you to design a protocol to determine when you have a complete message. A few ways are: Use a fixed-length message. Send a fixed-length message indicating the total message length followed by the variable portion of the message. Send the message followed by a unique termination message that will never occur in the message. Another issue you may face is that send() is not guaranteed to send all the data. The return value indicates how many bytes were actually sent and it is the sender's responsibility to keep calling send with the remaining message bytes until they are all sent. You may rather use the sendall() method.  With blocking sockets which are the default and I assume are what you're using (can't be sure since you're using a mysterious module jmm_sockets) the recv method is blocking -- it will not return an empty string when it has ""nothing more to return for the moment"" as you seem to assume. You could work around this for example by sending an explicit terminator character (that must never occur within a filename) e.g. '\xff' after the actual string you want to send and waiting for it at the other end as the indication that all the string has now been received."
1105,A,"socket timeout and remove O_NONBLOCK option I implemented a socket timeout and retry but in order to do it I had to set the socket as a non-blocking socket. However I need the socket to block. This was my attempt at a solution to these two problems. This is not working. Subsequent send calls block but never send any data. When I connect without the select and the timeout subsequent send calls work normally. References: C: socket connection timeout How to reset a socket back to blocking mode (after I set it to nonblocking mode)? Code: fd_set fdset; struct timeval tv; fcntl(dsock F_SETFL O_NONBLOCK); tv.tv_sec = theDeviceTimeout; tv.tv_usec = 0; int retries=0; logi(theLogOutput LOG_INFO ""connecting to device socket num retrys: %i"" theDeviceRetry); for(retries=0;retries<theDeviceRetry;retries++) { connect(dsock (struct sockaddr *)&daddr sizeof daddr); FD_ZERO(&fdset); FD_SET(dsock &fdset); if (select(dsock + 1 NULL &fdset NULL &tv) == 1) { int so_error; socklen_t slen = sizeof so_error; getsockopt(dsock SOL_SOCKET SO_ERROR &so_error &slen); if (so_error == 0) { logi(theLogOutput LOG_INFO ""connected to socket on port %i on %s"" theDevicePort theDeviceIP); break; } else { logi(theLogOutput LOG_WARN ""connect to %i failed on ip %s because %s retries %i"" theDevicePort theDeviceIP strerror(errno) retries); logi(theLogOutput LOG_WARN ""failed to connect to device %s"" strerror(errno)); logi(theLogOutput LOG_WARN ""error: %i %s"" so_error strerror(so_error)); continue; } } } int opts; opts = fcntl(dsockF_GETFL); logi(theLogOutput LOG_DEBUG ""clearing nonblock option %i retries %i"" opts retries); opts ^= O_NONBLOCK; fcntl(dsock F_SETFL opts); We're gonna need you to post a complete test case that can be compiled and run. Is the last call to `fcntl` successful (return value 0)? If not what is it setting `errno` to? After you get the writeable event and no error you then need to call connect() again as documented. This tells you whether the connection succeeded or failed. Where is it ""documented""?  Why not use SO_RCVTIMEO or SO_SNDTIMEO socket options? Or did I miss something in your question?"
1106,A,"Is there a method to procedurally detecting if network router supports DHCP using C/C++? There is a scenario where an application tells a device on a network to get their IP address from the network router's DHCP server. If a DHCP server is not available the device's behavior becomes erratic. Is there a method to procedurally detect if the network router supports DHCP? Or is this something the device needs to do when attempting to get its IP address from the DHCP server? I was something along the lines of this Windows DHCP C API. That looks like bunch of hooks into their dynamic DNS implementation.  DHCP client on the device is doing exactly that. Well almost that. The DHCP DISCOVERY message is broadcast on the link. Then if there are any DHCP servers willing to serve this particular MAC address each one reserves an IP address from its pool and answers with the OFFER message. The client then picks what server it wants to ""bind"" to and issues the REQUEST message. The server confirms the granted lease of the IP address with an ACK message. The client discovers ""non-availability"" of DHCP servers by simply timing out waiting for offers. Take a look at the DHCPing project. Might give you some ideas. Links to the DHCP RFCs: RFC 2131 Dynamic Host Configuration Protocol RFC 2132 DHCP Options and BOOTP Vendor Extensions"
1107,A,How to make a program that sends data through the WiFi router I got a WiFi router connected to my PC. What I want is to send from another device some data to my PC through the Wi-Fi adapter. Program on the device is developed using EVC++. The one on PC - on VC++. EDIT 1: PC has an IP address. Another device s IP is set at program execution. I mean WiFi IP address. And then connection to WiFi router is proceeded. EDIT 2: What if it uses an ethernet and wifi together? How should I make a connection through WiFi? Is the other device capable of IP networking? Does it have an ip address? @kazanaki please refer to the EDIT. WiFI is no different to usual Ethernel network. So you just usually find out IP of other computer (in config/ask user/DNS) establish usual TCP/IP connection and send data on. What if I have both WiFi and ethernet? Now I am triing to use sockets. Hope I am on the right way. Yes that's the right way. If computer have several NIC it's up to the routing rules to see which interface would be used to pass packets through. Also you may choose interface to use (usually via specifying external IP of the interface). Thank you! You helped much!
1108,A,"while tesing I got erlang_error: ""{badmatch{errorsystem_limit}}{modulefunction3}""? I am doing load testing of my server with number of clients(200 300 500 ....3000). For testing I am using windows 64 bit systems and I am running server on one system and clients on another system. All the clients can do connections successfully but after one minute or two minutes later server through an erlang:error -> ""{badmatch{errorsystem_limit}}{moduleaccept_function3}"" for some server instances and dies those server instances. Can any on have a solution for this ? Thank you in advance.. sreenivas India. It appears that you have hit the system limit on the number of ports you are allowed to open on your system. Have a read of this doc here to see if you can determine what your limits are. In particular: Open ports The maximum number of simultaneously open Erlang ports is by default 1024. This limit can be raised up to at most 268435456 at startup (see environment variable ERL_MAX_PORTS in erlang(3)) The maximum limit of 268435456 open ports will at least on a 32-bit architecture be impossible to reach due to memory shortage. Open files and sockets The maximum number of simultaneously open files and sockets depend on the maximum number of Erlang ports available and operating system specific settings and limits. thanks OJ It is working and got results. You're welcome. Can you please accept the answer if it has resolved your problem?"
1109,A,"Are there any modern platforms with non-IEEE C/C++ float formats? I am writing a video game Humm and Strumm which requires a network component in its game engine. I can deal with differences in endianness easily but I have hit a wall in attempting to deal with possible float memory formats. I know that modern computers have all a standard integer format but I have heard that they may not all use the IEEE standard for floating-point integers. Is this true? While certainly I could just output it as a character string into each packet I would still have to convert to a ""well-known format"" of each client regardless of the platform. The standard printf() and atod() would be inadequate. Please note because this game is a Free/Open Source Software program that will run on GNU/Linux *BSD and Microsoft Windows I cannot use any proprietary solutions nor any single-platform solutions. Cheers Patrick Some embedded processors do not include any floating-point hardware at all. For desktop computers I do not see any reason to worry too much apart from details that only really annoy specialists (sqrt being incorrectly rounded on the Alpha this kind of thing. The differences that annoy them are in the implementation of the operations not of the format anyway). One variation between platforms is related to the handling of denormals. I asked a question about those a while back. Even that was not as bad as I expected. Alright thank you Pascal. It will definitely help to not have to worry as much about the binary format of floats in the game. Assuming IEEE-754 floats makes my life a whole lot easier. ^_^  If you properly abstract your network interface you can have functions/objects that serialize and deserialize the float datatypes. On every system I can think of these are the IEEE standard so you'd just have them pass the data through unchanged (the compiler will probably even optimize it out so you don't lose any performance). If you do encounter some system with a different format you can conditionally-compile in some code in these functions to do bit hacks to convert from the IEEE standard to the native format. You'll only need to change it in one place. You probably will never need to do so however unless you get into consoles/handhelds/etc. This is how my system is designed at the moment. I was not sure how common it is for a non IEEE float. I'll follow your advice and keep it. Thanks!  I think it is safe to assume that each platform has an implementation of the IEE-754 spec that you can rely on however even if they all implement the same spec there is no guarantee that each platform has the exact same implementation has the same FP control flags set does the same optimizations or implements the same non-standard extensions. This makes floating point determinism very hard to control and somewhat unreliable to use for this kind of thing (where you'll be communicating FP values over the network). For more information on that; read http://gafferongames.com/networking-for-game-programmers/floating-point-determinism/ Another problem to tackle is handling clients that don't have a floating point unit; most of the time these will be low-end CPUs consoles or embedded devices. Make sure to take this into account if you want to target them. FP emulation can be done but tends to be very slow on these devices so you'll have to get a hang of doing fixed point calculations. Be advised though writing elaborate classes to abstract floating point and fixed point calculations to the same code sounds like a plan; but on most devices isn't. It doesn't allow you to squeeze out the maximum precision and performance when dealing with fixed point values. Yet another problem is handling the endianness of the floating point values because you cannot just swap bytes and stack 'm in a floating point register again (the bytes might get a different meaning see http://www.dmh2000.com/cpp/dswap.shtml on that). My advice would be to convert the floats to fixed point intermediate values do an endian correction if needed and transmit that. Also don't assume that two floating point calculations on different machines will yield the same results; they don't. However floating point implementations other than IEEE-754 are rare. For example GPUs tended to use fixed point but are more likely to have a subset of IEEE-754 these days because they don't want to deal with division-by-zero exceptions but they will have extensions for half-floats that fit in 16 bits. Also realize that there are libraries out there that have already solved this problem (sending low-level data formats in a gaming context) for you. One such library is RakNet specifically it's BitStream class is designed to send these kinds of data reliably to different platforms while keeping the overhead to a minimum; for example RakNet goes through quite some trouble not to waste any bandwidth on sending strings or vectors. The determinism link you gave me is very interesting. I had been thinking of doing that with some bigger messages to make sure the clients were synchronised. As long as the positions are close enough (one unit is one metre) I think this will be fine. I don't intend to target console platforms and most embedded systems; I'm not even sure if I can do this as Free Software. As for low end processors that is more of a concern. The byte swap issue is very unfortunate...I briefly skimmed the article but I will have to read it later. Thank you very much for the information! There are some lesser known (handheld) consoles that allow you to publish open source software for them such as the GP2X and Open Pandora. The floating point determinism problem mainly comes up when you decided to decentralize the physics routines (eg. run them on the clients) or run replays on different machines because they tend to accumulate errors quite fast."
1110,A,Sending Information over a network I have computers connected in a wifi netwrok. One of them serves as a root(lets call it server) and is directly or indirectly connected to all other computers(lets call them clients). I want to send some information from root to all nodes(information is different for each node). Is there a way to do this without running any program on the client side(similar to PING) ? Or the only possible way is by using sockets over client and server? Ping does not actually send any data to the client. It just roundtrips a packet. To receive the information you need some kind of service running on the client. Sockets are needed. For minimal communication (not reliable) use UDP and for more reliable use TCP.  Is there a way to do this without running any program on the client side(similar to PING)? Yes provided that you don't care that the clients will never do anything with the information. Seriously without something on the client listening for and doing something with the data you send from the server what do you expect? i guess i will have to run program on client side too anyways thanx. @Vishal yes that's how such things work.
1111,A,"What do you use when you need reliable UDP? If you have a situation where a TCP connection is potentially too slow and a UDP 'connection' is potentially too unreliable what do you use? There are various standard reliable UDP protocols out there what experiences do you have with them? Please discuss one protocol per reply and if someone else has already mentioned the one you use then consider voting them up and using a comment to elaborate if required. I'm interested in the various options here of which TCP is at one end of the scale and UDP is at the other. Various reliable UDP options are available and each brings some elements of TCP to UDP. I know that often TCP is the correct choice but having a list of the alternatives is often useful in helping one come to that conclusion. Things like Enet RUDP etc that are built on UDP have various pros and cons have you used them what are your experiences? For the avoidance of doubt there is no more information this is a hypothetical question and one that I hoped would elicit a list of responses that detailed the various options and alternatives available to someone who needs to make a decision. As others have pointed out your question is very general and whether or not something is 'faster' than TCP depends a lot on the type of application. TCP is generally as fast as it gets for reliable streaming of data from one host to another. However if your application does a lot of small bursts of traffic and waiting for responses UDP may be more appropriate to minimize latency. There is an easy middle ground. Nagle's algorithm is the part of TCP that helps ensure that the sender doesn't overwhelm the receiver of a large stream of data resulting in congestion and packet loss. If you need the reliable in-order delivery of TCP and also the fast response of UDP and don't need to worry about congestion from sending large streams of data you can disable Nagle's algorithm: int opt = -1; if (setsockopt(sock_fd IPPROTO_TCP TCP_NODELAY (char *)&opt sizeof(opt))) printf(""Error disabling Nagle's algorithm.\n""); As I said assuming TCP is at one end of the scale and UDP at the other what else is there. If you want to be pedantic most of the discussed protocols are built on top of UDP. The assumption that TCP is at one end and UDP is at the other end is false. e.g. UDP has no flow control you can easily send packets too fast causing a router inbetween to drop all of them. Then what do you do ? Ignore the lost packets or resend them ? Resending them and you'll end up reimplementing TCP more or less. Another option for reliable communication is SCTP. A fast response doesn't necessarily equal a high throughput. I disagree. When nagle is used on TCP based protocols with lots of smaller packets it'll merge them together and create more larger packets. It causes some slight delay in sending so latecy may increase very slightly. However throughput can be lower with nagle off because more packets = more packet headers = greater overhead. Packets being dropped on a LAN is usually more to do with input buffers filling. If you have lots of clients sending data to the same host it may make zero difference. I don't believe turning off and on nagle is going to effect it in practice. One problem with Nagle's algorithm is that while it's fine in isolation some systems try to avoid acknowledging a packet if they think another packet is going to follow it; since acknowledging the second packet would implicitly acknowledge the first traffic could be reduced without affecting bandwidth in the case where the second packet does indeed handle the first. Unfortunately if the sender is using Nagle's algorithm the recipient is using delayed acks and after outputting some data the sender wants to receive some data in reply a semi-deadlock state can arise with... ...both sides of the connection waiting for the other to do something. In practice something will time out and things will get going again but the extra lag can often be very noticeable even on machines which have a fast connection between them.  What about SCTP. It's a standard protocol by the IETF (RFC 4960) It has chunking capability which could help for speed. Update: a comparison between TCP and SCTP shows that the performances are comparable unless two interfaces can be used. Update: a nice introductory article. Thanks for the update and comparison! SCTP has many great features (such as multihoming) and with the partial reliability extension (RFC 3758) it's an incredibly flexible option. It's included in the latest linux kernel versions but for windows you'll have to install your own SCTP stack. That's good I'm more interested in things that can be built on top of UDP rather than built on top of IP but it's certainly something that fits in the solution space. Thanks Miles that's a useful link! SCTP can be tunneled over UDP. http://tools.ietf.org/id/draft-ietf-sigtran-sctptunnel-00.txt Yes... But something that is built on top of UDP rather than at the same level as UDP is likely to be easier to implement in user space at least on Windows... @Len Holgate UDP is part of the Internet Protocol Suite so if it's built on top of UDP it is also built on top of IP :)  RUDP - Reliable User Datagram Protocol This provides: Acknowledgment of received packets Windowing and congestion control Retransmission of lost packets Overbuffering (Faster than real-time streaming) It seems slightly more configurable with regards to keep alives then ENet but it doesn't give you as many options (i.e. all data is reliable and sequenced not just the bits that you decide should be). It looks fairly straight forward to implement. I was looking at this but there does not appear to be many implementations. Got a recommendation ? No sorry. I didn't end up using it in the end and was always going to do an implementation from scratch.  Anyone who decides that the list above isn't enough and that they want to develop their OWN reliable UDP should definitely take a look at the Google QUIC spec as this covers lots of complicated corner cases and potential denial of service attacks. I haven't played with an implementation of this yet and you may not want or need everything that it provides but the document is well worth reading before embarking on a new ""reliable"" UDP design. A good jumping off point for QUIC is here over at the Chromium Blog. The current QUIC design document can be found here.  RUDP. Many socket servers for games implement something similar.  Protocol DCCP standardized in RFC 4340 ""Datagram Congestion Control Protocol"" may be what you are looking for. It seems implemented in Linux.  ENET - http://enet.bespin.org/ I've worked with ENET as a reliable UDP protocol and written an asynchronous sockets friendly version for a client of mine who is using it in their servers. It works quite nicely but I don't like the overhead that the peer to peer ping adds to otherwise idle connections; when you have lots of connections pinging all of them regularly is a lot of busy work. ENET gives you the option to send multiple 'channels' of data and for the data sent to be unreliable reliable or sequenced. It also includes the aforementioned peer to peer ping which acts as a keep alive.  May be RFC 5405 ""Unicast UDP Usage Guidelines for Application Designers"" will be useful for you.  We have some defense industry customers that use UDT (UDP-based Data Transfer) (see http://udt.sourceforge.net/) and are very happy with it. I see that is has a friendly BSD license as well. Can you elaborate on your customers and their use cases particularly in the defense sector? Probably not but it's worth a shot. I've actually tossed around the idea to my superiors about UDT in a file-transfer application but it hasn't really gone anywhere yet.  Did you consider compressing your data ? As stated above we lack information about the exact nature of your problem but compressing the data to transport them could help. Especially with modern compression libraries. Some are as fast as a memcpy. e.g. lz4.  It's difficult to answer this question without some additional information on the domain of the problem. For example what volume of data are you using? How often? What is the nature of the data? (eg. is it unique one off data? Or is it a stream of sample data? etc.) What platform are you developing for? (eg. desktop/server/embedded) To determine what you mean by ""too slow"" what network medium are you using? But in (very!) general terms I think you're going to have to try really hard to beat tcp for speed unless you can make some hard assumptions about the data that you're trying to send. For example if the data that you're trying to send is such that you can tolerate the loss of a single packet (eg. regularly sampled data where the sampling rate is many times higher than the bandwidth of the signal) then you can probably sacrifice some reliability of transmission by ensuring that you can detect data corruption (eg. through the use of a good crc) But if you cannot tolerate the loss of a single packet then you're going to have to start introducing the types of techniques for reliability that tcp already has. And without putting in a reasonable amount of work you may find that you're starting to build those elements into a user-space solution with all of the inherent speed issues to go with it. Ok I'll adjust the question. I'm more interested in the pros and cons of the various reliable UDP protocols out there rather than a 'use TCP' response ;) @Andrew - it's very EASY to beat TCP in two cases: (1) your application has lighter reliability requirements than ""all data always in order no duplicates no excessive queueing"". Or (2) you are using multicast. Reliable UDP is very common for multicast environments. Also TCP suffers horribly when used across a WAN connection (long haul issues). Why simple. TCP uses windows where the packets in the window have to be ack'd. ACK protocols suffer because of latency due to line distance. Google: WAN TCP ""speed of light"" @Ajaxx you are very correct around this however TCP/IP does this purposely because of the last internet meltdown. If you are doing high bit rate protocol without any congestion control well basically shame on you. If you own the network then go wild. @Steve Thanks - You're right. Answer modified for accuracy. ""where the sampling rate is significantly higher than the nyquist rate"" -- the sampling rate is always twice the nyquist rate by definition.  If you have a situation where a TCP connection is potentially too slow and a UDP 'connection' is potentially too unreliable what do you use? There are various standard reliable UDP protocols out there what experiences do you have with them? The key word in your sentence is 'potentially'. I think you really need to prove to yourself that TCP is in fact too slow for your needs if you need reliability in your protocol. If you want to get reliability out of UDP then you're basically going to be re-implementing some of TCP's features on top of UDP which will probably make things slower than just using TCP in the first place. Yes Andrew Edgecombe said as much but as I said I'm interested in the pros and cons of WHAT alternatives there are. Without that list of alternatives and their pros and cons then it's hard to decide what's best. Given a known reliablity function sometimes a UDP stream can be hand-tuned to outrace the TCP stream in hte OS. Rare though."
