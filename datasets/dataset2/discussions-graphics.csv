0,A,"Weird shape resizing behavior in WPF I am making a WPF application that will let the user draw and resize shapes. The resizing part is done using adorners and the shapes are my own classes derived from Shape. For example I have a Polyline and for each of it's points I am adorning a Thumb with a handler on it's DragDelta event:  void Thumb_DragDelta(object sender DragDeltaEventArgs e) { PolylineEx polyline = this.AdornedElement as PolylineEx; ResizingThumb thumb = sender as ResizingThumb; int index = (int)thumb.Tag; Point newPoint = new Point( polyline.Points[index].X + e.HorizontalChange polyline.Points[index].Y + e.VerticalChange); Trace.WriteLine(String.Format(""Arranging point {0}:{1}{2}"" index newPoint.X newPoint.Y)); polyline.Points[index] = newPoint; } Well the problem is that sometimes (often) dragging a thumb is not smooth at all. First I thought it was a performance issue because of a new Point being created every time (and other things) but then I noticed that sometimes the point is set up at weird coordinates that have nothing to do with the position of the mouse. I've also uploaded a sample project HERE could someone who knows his way around wpf take a look and enlighten me of what is happening? Also if anyone thinks of a better way for doing this any feedback would be highly appreciated. Thank you. Edit: HERE is a new link to the project files. Ur link don't werk. Try uploading it to SkyDrive or some universally accepted and reliable upload host. Thank you for your comment it's strange it's not working. Uploaded to SkyDrive now. I'm ending up answering my own question :) I think the problem was here: protected override System.Windows.Media.Geometry DefiningGeometry { get { StreamGeometry geometry = new StreamGeometry(); using (StreamGeometryContext context = geometry.Open()) { context.BeginFigure(Points[0] false true); foreach (Point pt in Points) { context.LineTo(pt true true); } geometry.Freeze(); return geometry; } } } I switched to using PathGeometry & LineSegments and it works fine now."
1,A,"Why won't rectangle draw? I am creating a rectangle as a custom control object that is placed within a Panel control. The control rectangle is created and evidenced by the Panel's Paint event: void myPanel_Paint(object sender PaintEventArgs e) { Graphics g = e.Graphics; foreach(ControlItem item in controls) g.DrawRectangle(new Pen(Brushes.Blue) new Rectangle(item.Location item.Size)); g.Dispose(); } In order to remove the foreach above and to improve rendering performance I am trying to give the custom rectangle control a similar Paint event (so it isn't confused with the standard Paint event I am calling my event Render). Custom base class constructor:  { controlBase = new Bitmap(50 25); /*Create a default bitmap*/ } Custom base class CreateGraphics:  { return Graphics.FromImage(controlBase); } Obviously I require an event handler for rendering/painting: public class RenderEventArgs : PaintEventArgs { public RenderEventArgs(Graphics g Rectangle rect) : base(g rect) { } } However the following is not providing the expected results: void item_Render(object sender RenderEventArgs e) { Graphics g = e.Graphics; g.DrawRectangle(new Pen(Brushes.Blue) new Rectangle(((myBaseClass)sender).Location ((myBaseClass)sender).Size)); g.Dispose(); } So I'm trying to figure out what I am missing. The Panel that contains the rectangle objects sets the size and location. I'm not sure if I have all the information here so if you have any questions please feel free to ask. Thanks... Edit: @Henk Holterman - I raise the OnRender in my derived Constructor: public ControlItem() : base() { Graphics g = CreateGraphics(); Pen p = new Pen(Color.Black 2.5f); BackColor = Color.Beige; g.DrawRectangle(p Bounds); OnRender(new RenderEventArgs(g Bounds)); g.Dispose(); } I am not sure that I am raising this in the right place(s). When the parent Panel control sets Location: public Point Location { get { return myRectangle.Location; } set { myRectangle.Location = value; DrawBase(); } } ...where... private void DrawBase() { Graphics g = CreateGraphics(); g.DrawImage(controlBase Location); g.Dispose(); } I don't understand why you aren't just using the OnPaint method in your ControlItem. Do you want it to paint differently in the panel than in other contexts? I am trying to keep from iterating through all the objects on a Paint of the Panel (parent control). Specifically if a property is changed on an object I want to re-render that specific rectangle. There is no need to do a foreach on the entire group of rectangles if only one of them changes. You can still handle the OnPaint() in your child control. Call the Invalidate() method when a property changes that requires a redraw of that control. This will only paint that control. Your parent panel doesn't need to be involved at all. One very big red flag: You should not Dispose() the e.Graphics object in a Paint eventhandler. The fact that you do (and in your own render event too) throws doubt on the rest of your design here. But as you don't show how/when the render event is raised (or: how it replaces the foreach loop) I can't be sure. As a rule only Dispose() of objects you created yourself and do so on the same level with a using() {} statement. Re the Edit: From what I can tell you (only) draw the YourControls when created or resized/repositiones and not when the system requests a (Re)Paint. And that won't work. Your Form's bitmap is not being saved or cached. I use the Dispose because everywhere you look on MSDN they use it and everyone always recommends using Dispose(). :/ If you __Create__ a graphics object then __Yes__ you should Dispose. But you're only borrowing this one. @dboarman if you are creating your own graphics object then it would be correct to Dispose() when you are done with the context. If you are referencing someone else's Graphics object you shouldn't Dispose() unless it is clear that you are responsible for cleaning up.  Abstracting inner items with Render states is nice for organizational purposes but doesn't necessarily improve performance. Also when using the Graphics object you can only draw on a Graphics object that has been initialized with a context whether a screen/display bitmap or other output media (like Metafiles). It is unusual to initialize a Graphics object in a constructor if you're not making use of off-screen contexts. To improve your drawing code you should start by minimizing the areas that are being drawn. A good heuristic to keep in mind is ""the lower the number of pixels drawn = the faster my display can be"". With that in mind you should look at using the ClipRectangle property in the PaintEventArgs object. This tells you the region of the Graphics object that needs to be updated. From here use whatever Graphics object is given to you to do your drawing update work. Also if you need to redraw one of your inner elements you should just invalidate the area that needs to be redrawn. This combined with the use of the ClipRectangle when redrawing will keep down the amount of actual display work being performed. PaintEventArgs.ClipRectangle Property (System.Windows.Forms) @ MSDN I've written a sample that you can pick apart to see all of the above in action. While not the best possible (for example some things can be recycled like Pens and Brushes) it may give you some better ideas on how to approach your drawing. Note that this sample doesn't use Events. I'm sure you can figure out how to alter it to use events if you like. I omitted them because foreach()/enumeration is not necessarily a drawback if you aren't redrawing everything every time you need an update. If your drawn figures are so complex that you want to use events you may be better off creating new user controls. Also you will need to check overlaps for each inner item whether you use events or foreach(). First I created a C# Forms application called DefinedRectangles. Then I added a DisplayRect class to represent rectangle outlines. Here is the source code for the Form. I added a MouseDoubleClick handler to the form to allow changes to the state of the rectangles. Rectangles can be solid/dashed depending on the last state and will flip back and forth as their inside area is double-clicked. using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Text; using System.Windows.Forms; namespace DefinedRectangles { public partial class Form1 : Form { public Form1() { InitializeComponent(); innerItems = new List<DisplayRect>(); innerItems.Add(new DisplayRect(new Rectangle(0 0 50 50) Color.Blue true)); innerItems.Add(new DisplayRect(new Rectangle(76 0 100 50) Color.Green false)); innerItems.Add(new DisplayRect(new Rectangle(0 76 50 100) Color.Pink false)); innerItems.Add(new DisplayRect(new Rectangle(101 101 75 75) Color.Orange true)); } List<DisplayRect> innerItems; private void Form1_Paint(object sender PaintEventArgs e) { foreach(DisplayRect dispItem in innerItems) { dispItem.OnPaint(this e); } } private void Form1_MouseDoubleClick(object sender MouseEventArgs e) { foreach (DisplayRect dispItem in innerItems) { dispItem.OnHitTest(this e); } } } } Here is the source code for the DisplayRect class. Note that when we need to invalidate we have to adjust our update rectangle to make up for quirks between rectangle borders and rectangle areas. using System; using System.Collections.Generic; using System.Text; using System.Windows.Forms; using System.Drawing; using System.Drawing.Drawing2D; namespace DefinedRectangles { public class DisplayRect { public DisplayRect(Rectangle dispArea Color dispColor bool dashed) { m_area = dispArea; m_areaColor = dispColor; m_solidLines = !dashed; } Rectangle m_area; Color m_areaColor; bool m_solidLines; public Rectangle Bounds { get { return m_area; } } public void OnPaint(object sender PaintEventArgs e) { if (!m_area.IntersectsWith(e.ClipRectangle)) { return; } Graphics g = e.Graphics; using (Pen p = new Pen(m_areaColor)) { if (m_solidLines) { p.DashStyle = DashStyle.Solid; } else { p.DashStyle = DashStyle.Dot; } // This could be improved to just the border lines that need to be redrawn g.DrawRectangle(p m_area); } } public void OnHitTest(object sender MouseEventArgs e) { // Invalidation Rectangles don't include the outside bounds while pen-drawn rectangles do. // We'll inflate the rectangle by 1 to make up for this issue so we can handle the hit region properly. Rectangle r = m_area; r.Inflate(1 1); if (r.Contains(e.X e.Y)) { m_solidLines = !m_solidLines; Control C = (Control)sender; C.Invalidate(r); } } } } WOW!!! Thank you very much for this insight. Much appreciated. No problem. Hope this helps. Also try modifying the example rectangles so that some of them overlap. You can see that all the rectangles in overlapping regions will change if a hit is detected in those areas. From there you may also need to add z-ordering if this is not a desired behavior."
2,A,"What would be a good way of creating graph on a website? I got a bunch of data in a database. The goal is to present them to a user in a readable way and since they're stock-data there needs to be a graph there. Now it brings one question: which approach would be better to create a graph on a server side dynamically or let the server just push raw data allowing the client to generate graph? I saw there are some jQuery libraries for doing this flot for example. I usually prefer to do as little as possible on the client side but this time I wonder: generating the graph on client side would produce lower server load. Also changing some parameter (like displaying data for bit diffrent timespan) would require only to fetch missing data from server with ajax and redraw graph instead of fetching completly diffrent image. This would give a more responsive UI. I saw that Google Finance uses flash for their graphs but I'd like to avoid it if it's possible... If you have some users get same data I think server side is better option so you can use cache and just draw graph once. If the users are customizing what data is displayed in the graph I might consider drawing the graph on the client side. If the graph is going to be relevant to more than one user I'd definitely generate it once on the server side and cache it for everyone.  What about using the Google Chart API? The Google Chart API lets you dynamically generate charts. To see the Chart API in action open up a browser window and copy the following URL into the address bar: http://chart.apis.google.com/chart?cht=p3&chd=t:6040&chs=250x100&chl=Hello|World Press the Enter or Return key and - presto! - you should see the following image: Pros of this solution: It won't generate more load on your servers. It doesn't involve the client. The charts look sexy (IMHO). Cons of this solution: It needs an internet connection. You rely on a third party (but it's Google and they have decent uptime). You pass informations to Google and this might be a problem.  Also depends on how important it is for your users to see the graphs and whether they are in a controlled environment. If you can not be sure they will have javascript enabled better generate the graphs server side.  The client-side offers more opportunities for the user here. Generating a bitmap on the server is pretty lame in that regard and as you mentioned it adds to your server load. I'd go for flot in that case. Browser support is really good it isn't wasteful with client resources and you can easily give your users a couple of extra features like zooming in or activating and deactivating part of the data. Of course flot graphs also look quite nice.  Having extremely CPU heavy JS libraries can actually hamper user experience (nothing like having my FireFox 3.5 freeze up for 1 minute while Yahoo Mail's latest and supposedly greatest code does something which I never wanted or needed for it to do). So if you can afford server side resources do it on a server. However make sure to cache the graph images with the same input data - it may make a difference between acceptable and DOA server utilization. At the very least allow user a ""lite"" option - ala Yahoo Finance which allows you to switch to simple non-flash graphs or Google Maps that has a lite HTML-only interface.  I'd probably just find a charting library that works with your chosen development language and go with it - unless you have a strong justification for writing your own charting system. If your using Asp.Net then the new charting control is probably a safe bet - it generates charts server (as images)  There are several issues with client side charting: For Javascript / Flash based charts your clients must have javascript / flash enabled. If your application is already js-dependent that's not a problem. What is a problem though is when sometime down the line you'll get a requirement to send reports to customers via emails (be they HTML emails or embedded PDFs or what have you). Suddenly flash / javascript are no help at all so you end up implementing server side charting and now either have to maintain both or refactor your UI. Using Google Charts avoids both of the above issues; however it introduces an external dependency. Something you may or may not want to live with. If all of the above don't scare you client side charting is great especially the interactivity of it."
3,A,"Decomposing a 3d mesh into a 2d net Suppose you have a 3 dimensional object represented as a 3d mesh in some common file format. How would you devise an algorithm to decompose the mesh into one or more 2d 'nets' - that is a 2-dimensional representation that can be cut out and folded to create the original 3d object. Amongst other things the algorithm would need to account for: Multiple possible decompositions for any given object Handling fitting a mesh into fixed size canvases (sheets of paper). Recognizing when two panels in the net would overlap (and are thus invalid). Breaking a mesh up into multiple nets if they can't be represented as a single one due to overlap or page size constraints. Generating tabs in the appropriate places for attaching adjacent faces. The obvious degenerate case is simply to create one net per face with tabs on half the edges. This isn't ideal obviously: The ideal case is a single continuous net. The reality for complex shapes is likely to be somewhere in the middle. I realize that finding the optimal net (fewest nets / least pages) is probably computationally expensive but a good heuristic for finding 'good enough' nets would suffice. Hi! Super interesting topic. Any advance on it after few years? The algorithms eed3si9n linked to will generate nice reasonable papercraft meshes from complicated geometry. If you'd like to unfold the mesh exactly as it is modeled such as for polyhedra models then here's a relatively simple technique for unfolding any mesh as it stand Construct a graph from your source mesh where each face is a vertex in the graph and two vertices are connected if they share a common edge in the mesh. One of these graphs represents an unfoldable mesh if and only if it has no loops i.e. it is a tree. A good tree represents the fewest fold lines to get to the farthest face from the starting point since each fold represents error that will accumulate in the finished model. Dijkstra's algorithm is good here but minimum spanning tree doesn't work. With each edge equally weighted all trees are minimum spanning trees even one that would unfold your mesh into one big spiral. As you glued the model together errors would build up until the last few faces didn't fit at all. Once you have the tree start by drawing your starting face at the origin. Then walk the tree and add the new faces by calculating the new vertex as the intersection of two circles with radii corresponding to the lengths of the edges in the original mesh. Locations for tabs correspond to edges that were in the original mesh but are not in the flattenable tree. User-specified cuts can be handled as edge deletions before the tree step. Some downsides of this technique are that it will happily create overlapping parts in the flat pattern and it is dependent on finding a good starting face. I tried Floyd-Warshal to find a minimum-diameter face to start from but its O(n^3) behavior made for excessively long coffee breaks. The overlapping parts could be dealt with by marking that branch of the tree as ""incomplete"" skipping it and re-running the algorithm on all incomplete faces again. Good answer thanks! It's not explicit from your answer that you intend that edges be removed until the graph is a tree but that seems to be the implication. I presume a minimum-spanning-tree algorithm is appropriate. Also your answer implies you've written this before - personal project or is it available somewhere? :) Actually minimum spanning tree doesn't work because it doesn't minimize the number of folds from the center. I've updated my answer to clarify this. As far as removing edges from the mesh goes this algorithm doesn't modify the original mesh. I found it easier to just generate the flattened mesh from scratch rather than maintain all the correct invariants while splitting edges. Lastly I have indeed written this before as a Blender plug-in with the intent of releasing it but got distracted before finishing the tabs part. I'll grab it off my old HD and post it. Ok I finally pulled it out and put it up at http://code.google.com/p/unfolder/ @Theran How did you generate those beautiful images/diagrams?  I know this is not an answer but it is related. Ex-SGI graphics guy Paul Haeberli's Lamina program is a solution for this problem. Wow. Neat program but $353 is a bit much for use in papercraft. ;)  When I read your question the words ""automatic papercraft algorithm"" came to me. So I googled it and found Papercraft Models using Generalized Cylinders (pdf) by Massarwi et al. We propose a new method for producing unfolded papercraft patterns of rounded toy animal figures from triangulated meshes by means of strip-based approximation. Although in principle a triangulated model can be unfolded simply by retaining as much as possible of its connectivity while checking for intersecting triangles in the unfolded plane creating a pattern with tens of thousands of triangles is unrealistic. Our approach is to approximate the mesh model by a set of continuous triangle strips with no internal vertices. Initially we subdivide our mesh into parts corresponding to the features of the model. We segment each part into zonal regions grouping triangles which are similar topological distances from the part boundary. We generate triangle strips by simplifying the mesh while retaining the borders of the zonal regions and additional cut-lines. The pattern is then created simply by unfolding the set of strips. The distinguishing feature of our method is that we approximate a mesh model by a set of continuous strips not by other ruled surfaces such as parts of cones or cylinders. Thus the approximated unfolded pattern can be generated using only mesh operations and a simple unfolding algorithm. Furthermore a set of strips can be crafted just by bending the paper (without breaking edges) and can represent smooth features of the original mesh models. There is also an earlier related paper called Paper craft models from meshes (9MB pdf) by Shatz et al. This paper introduces an algorithm for segmenting a mesh into developable approximations. The algorithm can be used in various applications in CAD and computer graphics. This paper focuses on paper crafting and demonstrates that the algorithm generates approximations that are developable easy to cut and can be glued together. It is also shown that the error between the given model and the paper model is small. Awesome! Thanks. I needed this to unwrap my meshes' net texture space into an atlas. You're a lifesaver."
4,A,"How can I underline some part of a multi-line text with GDI? I am using Graphics.DrawString to draw my usercontrol's text like this: protected override void OnPaint(PaintEventArgs e) { RectangleF bounds = DisplayRectangle; bounds.Inflate(-4 -4); // Padding StringFormat format = new StringFormat(); format.Alignment = StringAlignment.Near; format.LineAlignment = StringAlignment.Near; format.Trimming = StringTrimming.None; using (Brush bFore = new SolidBrush(ForeColor)) { g.DrawString(Text Font bFore bounds format); } } If control's Text is wider than the DisplayRectangle DrawString nicely breaks the Text into multiple lines at word boundaries. Now I want to underline some words from Text but I couldn't work it out. I tried splitting the Text then MeasureString the string just before an underlined part starts DrawString the normal part then DrawString the underlined part. But this works only if Text is single-line. I am sure using a child LinkLabel or RichTextBox to render my control's text will solve this but I don't like the idea of using a child control just to underline a few words. Is there another way? This is a crude example that will work using the string split into parts and two different font styles rather than drawing the underline separately ( though that would work too ). In actual practice I would recommend splitting the text by word not by phrase and dealing with each word individually in a loop. Otherwise like in this example the line-breaking doesn't work quite right. Dim fntNormal As New Font(myFontFamily myFontSize FontStyle.Regular GraphicsUnit.Pixel) Dim fntUnderline As New Font(myFontFamily myFontSize FontStyle.Underline GraphicsUnit.Pixel) g.DrawString(""This is "" fntNormal Brushes.Black rctTextArea) w1 = g.MeasureString(""This is "" fntNormal).Width w2 = g.MeasureString(""underlined"" fntUnderline).Width If w1 + w2 > rctTextArea.Width Then yPos = rctTextArea.Y + g.MeasureString(""This is "" fntNormal).Height + 5 xPos = rctTextArea.X Else yPos = rctTextArea.Y xPos = 0 End If g.DrawString(""underlined"" fntUnderline Brushes.Black xPos yPos) w1 = g.MeasureString(""underlined"" fntUnderline).Width w2 = g.MeasureString("" and this is not."" fntNormal).Width If w1 + w2 > rctTextArea.Width Then yPos += g.MeasureString(""underlined"" fntUnderline).Height + 5 xPos = rctTextArea.X Else xPos = 0 End If g.DrawString("" and this is not."" fntNormal Brushes.Black xPos yPos) This code can really be cleaned up and made more efficient to let you loop through each word in your text string. This example also does not include any code to check if you have exceeded the vertical limit of your bounds rectangle. Sorry for the VB code I just noticed your question was in C#. The two lines with xPos = 0 above. Shouldn't they be xPos = xPos + w1? Thanks I guess this is the best I can do. I also tried using MeasureCharacterRanges on word boundaries but it doesn't seem to work with multi-line text. Yes the xPos = 0 is wrong. I was in too much of a hurry with my cut and paste. :)"
5,A,"Graphics library in C I was wondering if there were any good free graphics libraries for C that are easy to use? It's for plotting 2d and 3d graphs and then saving to a file. It's on a Linux system and there's no gnuplot on the system right now. Or would it just be simpler to switch to another language and if so which one would be easy to learn? What type of graphics? 2D 3D both? How about platform? Windows Linux Mac portable? May you be a little bit more specific what such a library should do? Handle Images? Generate Charts? 3D-modeling? Is there something specific you plan to do (such as plotting and graphing image manipulation diagrams) or just general purpose graphics? Are you going to display these graphics to the screen or save to a file? Do you need 2D graphics or 3D graphics? I recommend the Qt GUI toolkit coupled with the open-source QwtPlot and QwtPlot3D. It's implemented in C++ easy-to-use extensible and free...  I like the Cairo library. It has a nice interface to C and it can output in many formats.  To plot 2D and 3D graphs in C I would recommand the library DISLIN You can see examples here or there. The code is pretty easy to use and gives nice results. The site says ""DISLIN is free for non-commercial use."" Just my two cents. Yes as long as you don't use it for commercial purposes DISLIN is completely free. This looks good. Thanks. You forgot to mention that dislin licenses are quite expensive. From the technical side I would add that the library is not namespaced in any way which makes it hard to use in complex scenarios e.g. when you mix with your own code and can't see the interfaces.  There's Clutter. Here are a few snippets from the about page: ""Clutter is an open source software library for creating fast visually rich portable and animated graphical user interfaces."" ""Clutter aims to be non specific — it implements no particular User Interface style but rather provides a rich generic foundation that facilitates rapid and easy creation of higher level tool kits tailored to specific needs."" ""Developed in C with language bindings for Perl Python C# C++ Vala and Ruby."" ""Scene-graph of layered 2D interface elements manipulated in 3D space via position grouping transparency scaling clipping and rotation."" I haven't tried it out myself but it seems pretty flexible if you are looking for something just to play around with.  Take a look at PGPLOT. It's old but works great and should be in the repos. PLPLOT is also an option it's similar and newer and should also be readily available in the repos. They're both extremely powerful and can do what you specified.  Most people use the gd library for rendering from C but you must implement the ""math plotting"" part.  I've used netpbm format a few time when I needed something simple. That's how I found out that qsort() (in my implementation and for the data provided) performs a merge sort! Source code: #include <stdio.h> #include <stdlib.h> #include <string.h> #include <time.h> #define ARRAY_SIZE 20 #define MAX_VALUE 10 unsigned char arr[ARRAY_SIZE]; void print_array(const void *left const void *right) { static int imgs = 0; int k j; FILE *img; char fname[100]; char rgb[100]; if (++imgs > 9999) return; sprintf(fname ""img/img%04d.ppm"" imgs); /* create image in ""img/"" directory */ img = fopen(fname ""w""); if (img) { fprintf(img ""P3\n%d %d\n255\n"" ARRAY_SIZE MAX_VALUE); for (j=0; j<MAX_VALUE; j++) { for (k=0; k<ARRAY_SIZE; k++) { int colour = 0; if (left && left == arr+k) colour = 2; if (right && right == arr+k) colour = 2; if (arr[k] == MAX_VALUE - j - 1) colour = 1; switch (colour) { default: sprintf(rgb ""%d %d %d"" 255 255 255); break; case 1: sprintf(rgb ""%d %d %d"" 0 0 0); break; case 2: sprintf(rgb ""%d %d %d"" 255 0 0); break; } } fprintf(img ""%s\n"" rgb); } } fclose(img); } else { perror(""img fopen""); } } int cmp(const void *left const void *right) { const unsigned char a = *(const unsigned char*)left; const unsigned char b = *(const unsigned char*)right; print_array(left right); if (a < b) return -1; if (a == b) return 0; return 1; } int main(void) { int k; unsigned int seed = 0; /* or time(0) */ srand(seed); for (k=0; k<ARRAY_SIZE; k++) { arr[k] = rand() % MAX_VALUE; } print_array(NULL NULL); qsort(arr (size_t)ARRAY_SIZE sizeof *arr cmp); print_array(NULL NULL); /* use imagemagick to convert group of files to .gif */ system(""convert -delay 0"" "" img/img*.ppm"" "" -loop 1 img/libc-qsort2.gif""); /* remove .ppm files */ system(""rm img/"" ""*ppm""); /* ... my editor does not like a slash and a star together even inside quotes */ return 0; }  This question is a bit vague ""graphics"" is a wide field. You can get pretty far using just plain SDL but it might also be considered ""too low-level"". You need to provide more requirements."
6,A,"What is the proper way to use GFX rotozoomSurface with SDL for drawing transparent sprites? I'm using the latest SDL/GFX libs on Fedora 10 and I'm trying to render a PNG or GIF image onto the screen surface. I could not get transparent PNG's to display at all so I replaced the transparent parts or my sprite with all white (and tried Magenta 2550255). The white-transparent PNG displays fine using this code: SDL_Surface *image = load_image(""sprite-white.png""); SDL_Surface *roto = SDL_DisplayFormat(image); SDL_SetColorKey(roto SDL_SRCCOLORKEY SDL_MapRGB( roto->format 255255255 )); SDL_BlitSurface( roto NULL surface &offset ); But when I try to rotate the sprite it does not drop all the white pixels. I replace the above code with this to rotate: SDL_Surface *image = load_image(""sprite-white.png""); SDL_Surface *roto = rotozoomSurface(image rotation_degrees 1 1); SDL_Surface *roto2 = SDL_DisplayFormat(roto); SDL_SetColorKey(roto2 SDL_SRCCOLORKEY SDL_MapRGB( roto2->format 255255255 )); SDL_BlitSurface( roto2 NULL surface &offset ); I end up with a white outline around some of the good pixels. GIF images give the same results. When trying with transparent PNG/GIF files the code was the same except I did not call SDL_SetColorKey. Then the PNG did not display properly at all. One strange thing I found was the transparent PNG looked the same in MS Paint as it did in SDL but GIMP/Photoshop programs opened it correctly. Is there something I'm missing setting up the destination surface? Google did not turn up much a working example would be great. Just turn off the anti-aliasing of the Rotozoomsurface and you should no more have trouble with the 'out line issue'. You used this : SDL_Surface *roto = rotozoomSurface(image rotation_degrees 1 1); Instead Use this: SDL_Surface *roto = rotozoomSurface(image rotation_degrees 1 0); 0 sets the anti-aliasing off.  I am assuming that when you say ""the latest SDL/GFX libs on Fedora 10"" you mean the SDL and SDL_gfx libraries. To properly display PNG images with transparency ensure that you also have SDL_image installed; you will need it unless you have a different library that can load a PNG image into a SDL_Surface. You should not need to use a color key for transparency (unless of course you want to use one). As for your problem with using a color key note that the SDL documentation recommends setting the color key before calling SDL_DisplayFormat(): ""If you want to take advantage of hardware colorkey or alpha blit acceleration you should set the colorkey and alpha value before calling this function."" You might also want to try using SDL_DisplayFormatAlpha() instead of SDL_DisplayFormat() like so: SDL_Rect imagePosition = { 50 50 0 0 }; SDL_Surface *image = IMG_Load(""example.png""); if (image != NULL) { image = SDL_DisplayFormatAlpha(image); } Later in your rendering loop you can then rotate the source image by some floating point angle on to some SDL_Surface screen: if (image != NULL) { SDL_Surface *rotation = rotozoomSurface(image angle 1 1); SDL_BlitSurface(rotation NULL screen &imagePosition); SDL_FreeSurface(rotation); } The above code will properly blit a PNG with transparency assuming your PNG has proper transparency. Make sure to link with -lSDL_image for the SDL_image library and -lSDL_gfx for the SDL_gfx library assuming you are using g++ on your Fedora 10 installation. You can also use sdl-config --libs for the required g++ arguments for SDL itself. The above code gave me the same results so I found some other png's and it worked fine along with my original code. The png that I was using for testing was somehow bad. Thanks!"
7,A,"How do you draw a string centered vertically in Java? I know it's a simple concept but I'm struggling with the font metrics. Centering horizontally isn't too hard but vertically seems a bit difficult. I've tried using the FontMetrics getAscent getLeading getXXXX methods in various combinations but no matter what I've tried the text is always off by a few pixels. Is there a way to measure the exact height of the text so that it is exactly centered. Another option is the getBounds method from the TextLayout class. Font f; // code to create f String TITLE = ""Text to center in a panel.""; FontRenderContext context = g2.getFontRenderContext(); TextLayout txt = new TextLayout(TITLE f context); Rectangle2D bounds = txt.getBounds(); int xString = (int) ((getWidth() - bounds.getWidth()) / 2.0 ); int yString = (int) ((getHeight() + bounds.getHeight()) / 2.0); // g2 is the graphics object g2.setFont(f); g2.drawString(TITLE xString yString);  Not sure this helps but drawString(s x y) sets the baseline of the text at y. I was working with doing some vertical centering and couldn't get the text to look right until I noticed that behavior mentioned in the docs. I was assuming the bottom of the font was at y. For me the fix was to subtract fm.getDescent() from the y-coordinate.  Note you do need to consider precisely what you mean by vertical centering. Fonts are rendered on a baseline running along the bottom of the text. The vertical space is allocated as follows: --- ^ | leading | -- ^ Y Y | Y Y | Y Y | ascent Y y y | Y y y | Y y y -- baseline ______Y________y_________ | y v descent yy -- The leading is simply the font's recommended space between lines. For the sake of centering vertically between two points you should ignore leading (it's leding BTW not leeding; in general typography it is/was the lead spacing inserted between lines in a printing plate). So for centering the text ascenders and descenders you want the baseline=(top+((bottom+1-top)/2) - ((ascent + descent)/2) + ascent; Without the final ""+ ascent"" you have the position for the top of the font; therefore adding the ascent goes from the top to the baseline. Also note that the font height should include leading but some fonts don't include it and the due to rounding differences the font height may not exactly equal (leading + ascent + descent). Any reason why you're adding 1? Oh wow That is a perfect explanation. Thank you very much! (also for the explanation of leading didn't know that) @Alex: Because (bottom+1-top) is the height; if bottom and top are the same the height is one not zero. The net effect is to round downward in the event that the mid-point is n.5 pixels.  I found a recipe here. The crucial methods seem to be getStringBounds() and getAscent() // Find the size of string s in font f in the current Graphics context g. FontMetrics fm = g.getFontMetrics(f); java.awt.geom.Rectangle2D rect = fm.getStringBounds(s g); int textHeight = (int)(rect.getHeight()); int textWidth = (int)(rect.getWidth()); int panelHeight= this.getHeight(); int panelWidth = this.getWidth(); // Center text horizontally and vertically int x = (panelWidth - textWidth) / 2; int y = (panelHeight - textHeight) / 2 + fm.getAscent(); g.drawString(s x y); // Draw the string. (note: above code is covered by the MIT License as noted on the page.) Yeah...I'm familiar with this concept...but it's wrong. The fm.getAscent() method is the problem. It doesn't report the real pixel ascent of the font and results in text that is closer to the bottom than the top."
8,A,"Confused in DDA algorithm  need some help I need help regarding DDA algorithm  i'm confused by the tutorial which i found online on DDA Algo  here is the link to that tutorial http://i.thiyagaraaj.com/tutorials/computer-graphics/basic-drawing-techniques/1-dda-line-algorithm Example: xaya=>(22) xbyb=>(810) dx=6 dy=8 xincrement=6/8=0.75 yincrement=8/8=1 1) for(k=0;k<8;k++) xincrement=0.75+0.75=1.50 yincrement=1+1=2 1=>(22) 2) for(k=1;k<8;k++) xincrement=1.50+0.75=2.25 yincrement=2+1=3 2=>(33) Now i want to ask that  how this line came xincrement=0.75+0.75=1.50  when it is written in theory that ""If the slope is greater than 1 the roles of x any y at the unit y intervals Dy=1 and compute each successive y values. Dy=1 m= Dy / Dx m= 1/ ( x2-x1 ) m = 1 / ( xk+1 – xk ) xk+1 = xk + ( 1 / m ) "" it should be xincrement=x1 (which is 2) + 0.75 = 2.75 or i am understanding it wrong  can any one please teach me the how it's done ? Thanks a lot) It doesn't look like a C++ question to me. yes  its C proggy  sorry for mistake There seems to be a bit of confusion here. To start with let's assume 0 <= slope <= 1. In this case you advance one pixel at a time in the X direction. At each X step you have a current Y value. You then figure out whether the ""ideal"" Y value is closer to your current Y value or to the next larger Y value. If it's closer to the larger Y value you increment your current Y value. Phrased slightly differently you figure out whether the error in using the current Y value is greater than half a pixel and if it is you increment your Y value. If slope > 1 then (as mentioned in your question) you swap the roles of X and Y. That is you advance one pixel at a time in the Y direction and at each step determine whether you should increment your current X value. Negative slopes work pretty much the same except you decrement instead of incrementing. you seems to be teaching me bresenham's algorithm  even more confused if you're telling about DDA  can please tell it more discriptively? thanks  Pixels locations are integer values. Ideal line equations are in real numbers. So line drawing algorithms convert the real numbers of a line equation into integer values. The hard and slow way to draw a line would be to evaluate the line equation at each x value on your array of pixels. Digital Differential Analyzers optimize that process in a number of ways. First DDAs take advantage of the fact that at least one pixel is known the start of the line. From that pixel the DDAs calculate the next pixel in the line until they reach the end point of the line. Second DDAs take advantage of the fact that along either the x or y axis the next pixel in the line is always the next integer value towards the end of the line. DDA's figure out which axis by evaluating the slope. Positive slopes between 0 and 1 will increment the x value by 1. Positive slopes greater than one will increment the y value by 1. Negative slopes between -1 and 0 will increment the x value by -1 and negative slopes less than -1 will increment the y value by -1. Thrid DDAs take advantage of the fact that if the change in one direction is 1 the change in the other direction is a function of the slope. Now it becomes much more difficult to explain in generalities. Therefore I'll just consider positive slopes between 0 and 1. In this case to find the next pixel to plot x is incremented by 1 and the change in y is calculated. One way to calculate the change in y is just add the slope to the previous y and round to the integer value. This doesn't work unless you maintain the y value as a real number. Slopes greater than one can just increment y by 1 and calculate the change in x. Fourth some DDAs further optimize the algorithm by avoiding floating point calculations. For example Bresenham's line algorithm is a DDA optimized to use integer arithmetic. In this example a line from (2 2) to (8 10) the slope is 8/6 which is greater than 1. The first pixel is at (2 2). The next pixel is calculated by incrementing the y value by 1 and adding the change in x (the inverse slope of dx/dy = 6/8 = .75) to x. The value of x would be 2.75 which is rounded to 3 and (3 3) is plotted. The third pixel would increment y again and then add the change in x to x (2.75 + .75 = 3.5). Rounding would plot the third pixel at (4 4). The fourth pixel would then plot (5 4) since y would be incremented by 1 but x would be incremented by .75 and equal 4.25. From this example can you see the problem with your code?"
9,A,"How do you represent a normal or texture coordinate using GLshorts? A lot of suggestions on improving the performance in iPhone games revolve around sending less data to the GPU. The obvious suggestion is to use GLshorts instead of GLfloat wherever possible such as for vertices normals or texture coordinates. What are the specifics when using a GLshort for a normal or a texture coordinate? Is it possible to represent a GLfloat texture coordinate of 0.5 when using a GLshort? If so how do you do that? Would it just be SHRT_MAX/2? That is does the range of 0 to 1 for a GLfloat map to 0 to SHRT_MAX when using a GLshort texture coordinate? What about normals? I've always created normals with GLfloats and normalized them to unit length. When using a GLshort for a normal are you sending a non-normalized vector to the GPU? If so when and how is it normalized? By dividing by all components by SHRT_MAX? I have had success using GLshorts as texture coords by multiplying them by 1000 when creating the VBO then dividing by 1000 before use in the shader. Of course you'll have to weigh the extra computations against the memory gain on your device. This comes up pretty high when Googling how to use GLshort to increase performance but apologies for posting on such an old thread.  The OpenGL ES 1.1 specification says that Normals are automatically brought back to the [-1:1] or [0:1] range when using integer types. (See Table 2.7 of specification for the full list of formulaes) (for shorts) n_x = (2c +1)/(2^16 − 1) So you don't need to rely on GL_NORMALIZE for normals (and can use whatever trick you want). Texture coordinates however do not get scaled (values outside the [0:1] range are perfectly valid...). If you want to apply such a scaling your best bet is to use a texture coordinate matrix at a somewhat significant cost. glMatrixMode(GL_TEXTURE); glLoadMatrix(matrix_that_does_conversion_based_on_type); Thanks for the answer. I may experiment with the texture matrix although it does sound like that is likely to be slower than just sending GLfloats to the GPU. @Jon-Eric: done. @Bahbar Can you add a link to the OpenGL ES specification you are using? Thanks!  Normals If you glEnable( GL_NORMALIZE ) then you can submit normals as GL_BYTE (or a short). A normal submitted as bytes 0xFF 0xCC 0x33 is normalized by the GPU to (0.77 0.62 0.15). Note that there is a small performance penalty from GL_NORMALIZE because the GPU has to normalize each normal. Another caveat with GL_NORMALIZE is that you can't do lighting trickery by using un-normalized normals. Edit: By 'trickery' I mean adjusting the length of a normal in the source data (to a value other than 1.0) to make a vert brighter or darker. Texture Coordinates As far as I can tell integers (bytes or shorts) are less useful for texture coordinates. There's no easy call instruct OpenGL to 'normalize' your texture coordinates. 0 means 0.0 1 means 1.0 and 255 means 255.0 (for tiling). There's no way to specify fractional values in between. However don't forget about the texture matrix. You might be able to use it to transform the integers into useful texture coordinates. (I haven't tried.) Maybe GLbyte or GLshort for textures is just for those cases where you are texturing a quad and you are only concerned with texture coordinates 0 and 1 and nothing in between. Just a guess. Could you elaborate on the ""lighting trickery"" for un-normalized normals? What would I be missing out on exactly? @AndrewGarrison The lighting trickery is I *think* you can tweak how bright individual verts are by making the normal either shorter or longer than 1.0. You likely won't tweak like that and you can always go back to floats if you must. I edited the answer to expand on the 'lighting trickery' comment and to add thoughts on texture coordinates. Awesome! What about texture coordinates? Are they also divided in the GPU when using GLshort or GLbyte? `GL_NORMALIZE` is only for normals. I'm just as curious as you regarding the texture coordinates."
10,A,"drawing text within a JPanel I'm looking for the most basic description of how to draw text within a JPanel. I know there are a billion tutorials out there but none of them are clicking with me and I have some specific questions which may help others who are confused. As a setup (a testing app) I have a single class which has a JLabel a JTextField a JButton and a JPanel. The application reads in ints from an external file and should display their average in the panel when the JButton is pressed. I have all the underlying programing sorted out (that is the button responds and prints the average to the command line) but I just cannot seem to sort out how to print the average to the panel. I guess my biggest question is how to incorporate the paint() or paintComponet() method in along with the rest of the code. Should it be it's own class? Should the JPanel be it's own class? It seems like that's what most of the tutorials are telling me I'm just not sure what the first step is exactly. The code looks like: import javax.swing.*; import java.awt.*; import java.awt.event.*; import java.io.*; public class Main extends JFrame implements ActionListener { private int[] intArray = new int[10000]; private int numOfInts = 0; private int avg = 0; protected JButton avgBtn; protected JTextField indexEntry; protected JLabel instructions; protected JPanel resultsPanel; public Main(){ //create main frame this.setTitle(""Section V question 2""); this.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); this.setSize(350 250); this.setLayout(new GridLayout(4 1)); //create instruction label and add to frame instructions = new JLabel(""Follow the instructions on the exam to use this program""); this.add(instructions); //create textfield for index entry and add to frame indexEntry = new JTextField(); this.add(indexEntry); //create button for average and add to frame avgBtn = new JButton(""Click for Average""); this.add(avgBtn); avgBtn.addActionListener(this); //create panel to display results and add to frame resultsPanel = new JPanel(); resultsPanel.setBackground(Color.BLUE); this.add(resultsPanel); //read in from file readFromFile(); //compute average computeAverage(); } private void readFromFile() { try { // Open the file FileInputStream fstream = new FileInputStream(""numbers.dat""); BufferedReader br = new BufferedReader(new InputStreamReader(fstream)); //create placeholder for read in ints String strLine; //Read File Line By Line int i = 0; while ((strLine = br.readLine()) != null) { //place ints in array and increament the count of ints System.out.println (strLine); intArray[i] = Integer.parseInt(strLine); numOfInts++; i++; } //Close the input stream in.close(); System.out.println (""numOfInts = "" + numOfInts); } catch (Exception e) { //Catch exception if any System.err.println(""Error: "" + e.getMessage()); } } //compute averaage private void computeAverage() { int sum = 0; for (int i = 0; i < numOfInts; i++) sum += intArray[i]; avg = sum/numOfInts; System.out.println(""avg = "" + avg); } //event handling public void actionPerformed(ActionEvent e) { if(e.getSource() == avgBtn) { computeAverage(); } } //""main"" function public static void main(String[] args) { Main m = new Main(); m.setVisible(true); } //paint public void paintComponent(Graphics g){ g.drawString(avg 75 75); } } Any and all help/direction is appreciated. I know I've used this code recently for other questions I just want to know it all! Ideally the panel would display the average of the read in ints when the button was clicked and display whatever was entered into the textfeild when the focus was on it and enter was pressed but I'm taking baby steps and like I said I'd like for this thread to be a general tutorial for others with similar questions who aren't finding answers from sun docs or other sites. Thanks so much in advance. Dan :) Add a JLabel to the JPanel. Call setText(String) on the JLabel and your text will be drawn within the JPanel. I love the simplicity of this. Not exactly what I was looking for. but thanks for looking at my question.  Create an inner class that extends JPanel inside your Main class: class MyPanel extends JPanel { @Override protected void paintComponent(Graphics g) { super.paintComponent(g); g.drawString(Integer.toString(avg) 75 75); } } Then you need to call repaint on that panel after calling computeAverage() in actionPerformed: //event handling public void actionPerformed(ActionEvent e) { if (e.getSource() == avgBtn) { computeAverage(); panel.repaint(); } } Thanks for the work you put into your answer. Accepted for that by itself... -1 the suggestion to change the paintComponent() method to paint() is wrong. Overriding paint() at the frame level is one of the worst things you can do when you don't invoke super.paint(). For one thing none of the components added to the frame will be painted since this is handled by the super class. You will also lose painting optimizations. you are right I fixed the solution  I think you should not be subclassing JFrame. Make an instance of JFrame an instance variable of Main class and add the JPanel etc. to it. this doesn't even attempt to answer the question would be better as a comment on the question. Agreed.Wanted to add quickly something which I thought would help. But looking back I agree my post doesn't provide much help. just post a comment still helpful but doesn't pose as an answer.  1) A JFrame doesn't have a paintComponent() method so the code you posted won't do anything. You need to create a custom JPanel and override its paintComponent() method to do your custom painting. 2) Even if you do the above the painting will still not display because a panel by default has a zero size. So you will then need to set the preferred size of the panel to make sure it is visible. 3) Why are you even doing this. All you need to do is use a JLabel and set the text of the JLabel. I find it hard to believe you looked at other tutorials. The Swing tutorial on Custom Painting has a 20 line program that shows the basics. Wow a little abrasive huh? Might want to change your screen name to camdickr. Thanks for the link. Except I provided the correct answer. The one you accepted was edited after mine was posted. I also suggested the JLabel before the other poster. I provided explanations why the code didn't work and gave suggestions to fix it. ""Give someone a fish they eat for a day. Teach someone to fish then eat for life. I gave you tools and advice to learn how to develop your own skill which will be better in the long run. That is why I don't post code. Expecting people to write code for you is the wrong attitude."
11,A,"Displaying GUI components in Java I have a GUI window that I've created using Netbeans. I then ported the code into my own program so that I can display .png's at my will. However the GUI components are not displaying and the window opens up with no size by default. I need the window to initially open up with the GUI components visible with the window of the correct size for everything to be visible. Can anyone help me out? thanks import java.awt.*; import java.awt.event.*; import javax.swing.*; import javax.swing.JFrame; public class AwtImage extends javax.swing.JFrame { private Image img; // Variables declaration - do not modify private javax.swing.JCheckBox jCheckBox2; private javax.swing.JLabel jLabel1; private javax.swing.JScrollPane jScrollPane1; private javax.swing.JScrollPane jScrollPane2; private javax.swing.JTextArea jTextArea1; private javax.swing.JTextField jTextField1; // End of variables declaration public static void main(String[] args){ AwtImage ai = new AwtImage(); } private void initComponents() { jScrollPane1 = new javax.swing.JScrollPane(); jTextField1 = new javax.swing.JTextField(); jScrollPane2 = new javax.swing.JScrollPane(); jTextArea1 = new javax.swing.JTextArea(); jCheckBox2 = new javax.swing.JCheckBox(); setDefaultCloseOperation(javax.swing.WindowConstants.EXIT_ON_CLOSE); setTitle(""Not Logged In""); getContentPane().setLayout(null); jTextField1.addKeyListener(new java.awt.event.KeyAdapter() { public void keyTyped(java.awt.event.KeyEvent evt) { jTextField1KeyTyped(evt); } }); jScrollPane1.setViewportView(jTextField1); getContentPane().add(jScrollPane1); jScrollPane1.setBounds(0 540 170 22); jTextArea1.setColumns(20); jTextArea1.setRows(5); jScrollPane2.setViewportView(jTextArea1); getContentPane().add(jScrollPane2); jScrollPane2.setBounds(0 440 166 96); jCheckBox2.setText(""Sit Out Next Hand""); jCheckBox2.addActionListener(new java.awt.event.ActionListener() { public void actionPerformed(java.awt.event.ActionEvent evt) { jCheckBox2ActionPerformed(evt); } }); getContentPane().add(jCheckBox2); jCheckBox2.setBounds(0 410 113 23); pack(); }// </editor-fold> private void jCheckBox2ActionPerformed(java.awt.event.ActionEvent evt) { // TODO add your handling code here: } private void jTextField1KeyTyped(java.awt.event.KeyEvent evt) { // TODO add your handling code here: } public AwtImage() { super(""Image Frame""); MediaTracker mt = new MediaTracker(this); img = Toolkit.getDefaultToolkit().getImage(""C:\\Documents and Settings\\Robert\\Desktop\\ClientServer\\Poker Table Art\\TableAndChairs.png""); mt.addImage(img0); setSize(600600); initComponents(); setVisible(true); addWindowListener(new WindowAdapter(){ public void windowClosing(WindowEvent we) { dispose(); } }); } public void update(Graphics g){ paint(g); } public void paint(Graphics g) { if(img != null) g.drawImage(img 0 10 this); // else // g.clearRect(0 0 getSize().width getSize().height); } } Sorry the code is now included code? and why are you 'porting' the code - let it stay in the class that has been generated and just update that class. I'd prefer not to getting the images into it is a pain why do you think it's pain? What's the difference except that you can't modify ""initComponents"" ? I have no experience with graphical programming in Java so i simply don't know what the common practices are. I suppose I could use netbeans but I have no idea where to put the code to display the images and now I have no idea how to display the images at my will. I'm just a bit overwhelmed with the amount of information out there I just need simple practical explanations of how to include images alongside the gui components. I persume you are calling ... .setSize(500500); on the main window? if you know the size of the image then I'd personally call... .setPreferedSize(xx); .setMinimenSize(xx); .setSize(xx); .pack(); on the main window/panel. can you post your code?  setSize() should be moved after initComponents() But again you wont need to port the code you can edit the source code in Netbeans without difficulties just keep initComponents folded.  without seeing your code it's near impossible to help you however: it sounds like you are missing a call to pack on your Window/JFrame which would cause the window to calculate it's size based on it's content and your UI components (checkbox + text boxes) are probably not showing up because you haven't added them correctly ie via the contents pane and a layout manager. just a guess!  There are several problems with the code. In addition to the other suggestions of using pack() or setting the frame size ""after"" adding the components to the frame and just before invoking the setVisible(true) method you need to look at the following. The code is based on old AWT painting techniques which should NOT be used with Swing. You should NEVER override the update() or paint() methods of the JFrame. The reason the child components are not painted is that the paint() method is responsible for painting them but you overrode this default behaviour. When using Swing custom painting is done by overriding the paintComponent() method of a JComponent or JPanel then you add this component to the content pane of the frame. Read the section from the Swing tutorial on Custom Painting for more information and working examples. One of the key points in the tutorial is to provide a preferred size for the component so it can be layed out properly by the layout manager. You should NOT be using a KeyListener. Again this is an old AWT technique. Swing now uses Key Bindings to map KeyStrokes to an Action. Read the section from the Swing tutorial on ""How to Use Key Bindings"" for more information. You should NOT be using a WindowListener to close the frame again this is an old AWT technique. Swing applications now use the frame.setDefaultCloseOperation(...) method to control this. The tutorial section on ""How to Use Icons"" shows you easier ways to load an image. So overall I suggested you start by downloading the Swing tutorial to learn the Swing programming techniques. The tutorial covers the basics and many simple examples to get you started."
12,A,Crop a child control to a region in .NET WinForms I am currently designing a user control which will have other controls contained within it. I would like to be able to control the region in which the they are rendered (similar to Graphics.SetClip). Ideally I would like to beable to have them render to an off screen image which I then present to the user as I choose. The last resort would be to place a panel in my control and then use that as their parent. Set the Control.Region property of the child controls. From the docs: The operating system does not display any portion of a window that lies outside of the window region. Note the region is relative to control on which it is set not your containing control (unlike Graphics.SetClip). Works a charm - can't believe I didn't see that there :)
13,A,"Simple Haskell graphics library? I'd like to experiment with Haskell a bit and I'm hoping to write a small 2D arcade game (Tetris or Breakout). Can you recommend a simple graphics library that will help me to get started quickly? Btw I've been experimenting with SDL and wxWidgets but haven't yet succeeded in running any samples because of dependency problems working on it... Cabal is great -- you should definitely download it. But it doesn't solve the issue of missing underlying native libraries. A lot of libraries (e.g. wxWidgets OpenGL probably GTK) are simply not included in the Cabal package. It's extremely important that one of the first thing you do is download Cabal Install. Without it dependency management is horrible. I just took a look at Haskell's GTK2HS and it does include the full GTK stack (which is nice). Unfortunately it doesn't have a Cabal package (not so nice) and its installer requires GHC 6.10.3. Not ""6.10.3 or higher"" -- just ""6.10.3."" Easy enough to work around but still... :P A better answer: http://stackoverflow.com/questions/5612201/haskell-library-for-2d-drawing @Don Stewart: Agreed -- the other answer is far better to the point where it supersedes everything said here. Have you perused the following lists: Haskell Graphics Libraries: There appears to be quite a few interfaces to OpenGL SDL and other graphic libs here. Haskell GUI: There's some wxWidget libs here as well.  It's not exactly a ""simple"" library but there is a lot of information online about OpenGL and GLUT as well as some very good tutorials and a ton of example code. The biggest issue you're up against is that the OpenGL and GLUT bindings in Haskell do not include the libraries that they bind to. (This is true for wxWidgets as well.) A lot of Linux distros come with OpenGL binaries bundled but not Windows. The Haskell Platform was supposed to fix this but it didn't seem to for me. So assuming you're installing on Windows here's what I'd recommend you try: Follow the directions in this blog to the letter. They're complicated -- involving installs of MinGW MSys and hand-compilation of a GLUT project from SourceForge but it's the only way I've gotten OpenGL to work. I've now successfully installed on three separate machines including XP and Vista so I can safely say that these are very good directions. Once it does work check out these two awesome tutorials. They really opened my eyes about just how powerful Haskell can be when it comes to graphics. You'll find the code involved a lot simpler than you may have anticipated. Check out the sample games on the Haskell OpenGL page. They're very experimental -- which is good as it means less code to wade through than you'll find in a production system -- but they're also surprisingly sophisticated. (And yes there's already more than one bare-bones Tetris implementation but don't let that stop you.) Another good source of sample code is Haskell's GLUT binding itself. Look for the examples directory and you'll find many ports of sample code from the OpenGL Red Book. OpenGL is very stateful so you may find the Haskell code a little daunting if you haven't fully grokked Monads yet. I'm using my OpenGL experiments as motivation to finally wrap my mind around the concept. Good luck! The configure step in OpenGL's configuration give me the error ""Couldn't reserve space for cygwin's heap"" apparently this may be caused by anti-virus software. Disabling Avast on my computer doesn't seem to help the problem. Patience and perseverance are getting well trained in this little project... Actually it's more likely a pathing problem especially if you have both Cygwin and MinGW installed. For some reason compiling the freeglut libraries doesn't work with Cygwin's gcc. Type ""gcc --version"" into the command line and make sure the resulting output looks something like this: ""gcc.exe (GCC) 3.4.5 (mingw-vista special r3)"" OpenGL state is all in the IO monad so practice may help with the basics of dealing with monads but doesn't help when trying to generalize to other monads which do far more useful things than simply keeping track of the state of the world. I don't mean to complain too much though; OpenGL/GLUT seems like a good start for OP though it's not very Haskell-ish. Personally I use Gtk2Hs with its GtkGlArea bindings as it's nicer than raw GLUT but it doesn't matter what's handling the windows once you get down to OpenGL itself. I haven't played around with GTK. I got wxWidgets working first so I did my UI experiments with it. But I will try it out now on your recommendation. I agree with you on the monad thing -- the IO monad is a pretty dull beast but it is a place to start. My eventual goal is to work my way up to arrows so I can try some reactive programming with Yampa. Right now all the concepts are flying over my head but I'm starting to put the pieces together. For easy setup with newer versions check http://openwires.blogspot.in/2012/10/opengl-glut-in-haskell-on-windows-okay.html  Cairo Is written in C but has haskell bindings Perhaps trying that might be a good idea. Ive only ever used its python bindings but those seemed to work well."
14,A,"Mixed Scaled and ordinary coordinates in Mathematica? Is it possible to specify a position in terms of Scaled coordinates in one direction and use the ordinary coordinates from my data points in the other direction on the plot? In other words I want to specify a position where the x coordinate is an ordinary coordinate and will change position in the plot if the plot range is changed but the y coordinate is Scaled coordinate and will remain at a fixed height relative to the plot. Is this for a list plot or are you using Graphics objects? I'm trying to use the function ""Scaled"" within a Graphics object. It is a bit late in coming but is this what you are looking for? data = {{1 0.5} {2 0.7} {3 0.4} {4 0.2}}; Graphics[ Line[data /. {x_ y_} :> Scaled[{0 y} {x 0}]] Axes -> True PlotRange -> {Automatic {0 100}} AspectRatio -> Full ] This is indeed what I'm looking for. Fine trick! Thank you."
15,A,"Graphics creative commons for video game i'm looking for ""graphics elements"" like sprite of cars human movement etc. using for a game open source made with pygame. Anyone know if exists a web site providing this service ? Thank you and regards You need to check out http://www.openclipart.org/ All their graphics are in SVG format so you can size them how you like. It's all public domain so no licensing issues. Not sure if they have the particular images you need but it is the site you're looking for. thank you very much!"
16,A,"What's the best way to add a GUI to a pygame application? Are there any good GUIs that support pygame surfaces as a widget within the application? If this isn't possible or practical what GUI toolkit has the best graphics component? I'm looking to keep the speedy rendering made possible by a SDL wrapper. Take a look at Albow PGU or Ocemp.  Best GUI toolkit in my opinions is wxPython (a binding for wxWidgets) which has GUI widgets for practically everything including an OpenGL widget and some work has been done with SDL as well - here. Sorry didn't read the question very clearly. wxPython/pygame integration is described here K  Albow and Ocemp are not being actively maintained. PGU is now being maintained by Peter Rogers and Victor Kam has converted most of it to Python 3. There are 5 pygame GUI toolkits which I've tried to get running under Python 3: Albow GooeyPy PGU pqGUI and sgc. (I didn't get GooeyPy to work with Python 3 but the others did.) I ran each of them through a simple Lines-Of-Code counter http://code.activestate.com/recipes/527746-line-of-code-counter/ to gauge their sizes: These are the results: Albow code min=2810 max=4551 (max = 162% of min) Albow\demo min= 453 max= 649 GooeyPy\gooeypy min=2034 max=3941 (max = 194% of min) GooeyPy\examples min= 178 max= 351 pgu\pgu min=2910 max=7047 (max = 242% of min) incl. pgu\pgu\gui min=1678 max=4638 (max = 276% of min) pgu\examples min= 822 max=2527 pqGUI.py min=1586 max=1834 (max = 116% of min) Example.py min= 178 max= 225 sgc (incomplete) min= 889 max=1243 (max = 140% of min) (I wanted to also try poutine by Shandy Brown but I couldn't find it.) In each case the ""min"" number is more representative of the ""size"" of the toolkit since it doesn't count whitespace and comments. When the ""max"" number is very close to the min number it means that there's not much whitespace and not many comments in the code. pqGUI exemplifies that because it has almost no comments at all which is too bad because (IMO) it makes the nicest-looking widgets. I liked pqGUI because I like the look of the widgets it makes but it is unsupported and undocumented and I can't find the author. Only two of the five are actively maintained/developed: PGU and sgc and sgc is new and incomplete. That makes PGU the clear leader. Dave P.S. (10/23/2012) -- I've created a GUI toolkit of my own for PyGame. It is widget-based and uses pygame events for communication so that it can easily be dropped into an existing pygame program without taking over the event loop. It supports forms buttons windows modal & non-modal message boxes & dialog boxes vertical menus text-entry boxes and sliders (scroll bars). It smoothly handles overlapping controls and forms-within forms. However it's not really complete: it lacks some controls you're likely to want like file-open dialogs and tables. The controls that do exist look nice but features like title-bars & scroll bars are fixed numbers of pixels in width rather than resizeable. The code is well-commented and there's a demo app with usage examples but there's no proper how-to-use documentation. And it's pre-beta so everything is subject to change. If in spite of those limitations someone wants to try it out then contact me by email. Ask about ""DavesGUI."" My email address is here: http://www.burtonsys.com/email/  Dont use wxPython its very hard to get to work well with Pygame as described over at the GUI section of the Pygame wiki. First of all pygame relies on the SDL which means that it can only have one window at a time. Thus trying to implement multiple Gtk Qt ... application instances that use pygame is an impossibility. The second problematic reason is that those toolkits use their own main loop which possibly forces you to pipe their events to your pygame instance and vice versa. And to mention some other points in short: Drawing the toolkit elements on the pygame window is impossible and the SDL/pygame fullscreen mode will be problematic. Instead opt for any of the libraries listed at the bottom of that page. I use pgu myself."
17,A,"Tessellating an arbitrary polygon by tiling triangles I need to fill an arbitrary polygon using a near-uniform tiling of triangles. How would I do this? You may provide either references to existing algorithms or even simply ideas or hints of your own. The following is presumed: The polygon may be convex (but bonus points if you come up with an algorithm that works for concave shapes) The polygon has an arbitrary number of edges (3 or more) The amount of tessellation (preferably the number of vertices added by the algorithm) should be parametrized The polygon's edges may be divided by the algorithm Triangles should be nearly uniform in size and shape (i.e. corners will tend towards 60 degrees) Preferably the number edges at a vertex should be few rather than many. This will likely follow from the previous point (I.e. the algorithm should produce a ""clean mesh""). This is not an easy problem to solve and I expect that a ""heuristic"" solution may be the most efficient... (right?) I added two. Happy? ;-) Hehe ok no worries guys :). It's not homework I'm basically wrapping a shape over a terrain heightmap in a 3d simulation I'm working on. I think it's also a generally useful problem to document and discuss though. I actually didn't want people to read too much into the context because they'd give different answers... It might help to provide a bit of background on what the actual problem you're trying to solve is. Is this a mesh used in simulation? If so what simulation method is being used? Well it sounded too much of a homework assignment posted verbatim here. It would help if you state that it is or isn't homework. If it's a question or some sort of challenge. No need to clarify it anymore from the comment you posted it is clear to me now but it wasn't when you first posted your err challenge/question/whatever. Doesn't actually sound that complicated algorithmically. Or am I missing anything? Some pseudocode: Place an exis-aligned enclosing square tightly around your polygon. Subdivide each square into four children (quad-tree like) where the number of iterations determines your ""amount of tessellation"". Each resulting square is either cleanly outside your polygon (=ignore) cleanly inside your polygon (=split into two triangles of resulting tesselation along the diagonal) or intersects your polygon. Exact cases for the intersection squares are left as an exercise to the reader. ;-) [At this point in time at least.] It will give you triangles with 45°/45°/90° angles though. If you want as close to 60° as possible you should start with tessellating the surface of your polygon with hexagons (with the size of the hexagon being your ""amount of tessellation"") and then checking each of the six 60°/60°/60° triangles that make up these hexagons. For these triangles you do the same as with the squares in the above pseudocode except for the fact that you don't need to split the ones that cleanly inside your polygon as they are already triangles themselves. Is this of any help whatsoever? Should work for any polygon (convex/concave/with holes in them) I think given that what exactly you do for intersecting squares/triangles handles such polygons. This is one of the ""easy"" ideas I had in mind as well. However it doesn't really tile the shape as nicely on the edges and the tesselation isn't all that easy to parameterize. (This is one of two ideas I already have...) (It's better thought out than the one I had though so thanks)  As Jason Orendorff pointed out you should try to use Triangle to generate a quality mesh. It has lots of options that you can play with to try to get an isotropic mesh. Then you could try to use an iterative algorithm to create a well-centered triangulation. More details are listed on this publications page. I have implemented the 2007 paper ""Well-Centered Planar Triangulation -- an Iterative Approach"" and it gives decent results on medium sized meshes. A well-centered triangulation is one in which all the circumcenters of triangles lie inside the respective triangle. Since you want something slightly different you could simply try to change the error metric involved. You could find a measure of ""non-congruence"" among the triangles and minimize that error. Most likely such an error function will be non-convex so the non-linear conjugate gradient optimization described is as well as you can do. You should refer to the referenced paper for an idea of how quickly it converges. In practice I rembember it takes many hundreds to a few thousand iterations. From what I can tell it looks better for simple meshes. More like 30 to 100 iterations. Thanks a lot this looks like what I was searching for! Do you have any indication of the performance of the algorithm? It's not too important I'm just curious.  Does Triangle do what you want? (The explanations of the algorithms on that site are better than whatever I could come up with.) From my initial gandering of the website it looks like they don't tile triangles in a uniform manner (triangles are not more or less equal in size) but I'll take some time to make sure. Thanks anyway! Actually this looks pretty close to what I was thinking... I might yet be able to use Delaunay triangulation by generating points equidistantly... +1 for now while I try to figure it out. ""The -a switch sets a maximum area constraint"" which appears to result in triangles of roughly the same size. And -q specifies a minimum angle. http://www.cs.cmu.edu/~quake/triangle.quality.html"
18,A,"Direct3D: Wireframe without Diagonals When using wireframe fill mode in Direct3D all rectangular faces display a diagonal running across due to the face being split in to two triangles. How do I eliminate this line? I also want to remove hidden surfaces. Wireframe mode doesn't do this. I need to display a Direct3D model in isometric wireframe view. The rendered scene must display the boundaries of the model's faces but must exclude the diagonals. Getting rid of the diagonals is tricky as the hardware is likely to only draw triangles and it would be difficult to determine which edge is the diagonal. Alternatively you could apply a wireframe texture (or a shader that generates a suitable texture). That would solve the hidden line issues but would look odd as the thickness of the lines would be dependant on z distance. Using line primitives is not trivial although surfaces facing away from the camera can be easily removed partially obscured surfaces would require manual clipping. As a final thought do a two pass approach - the first pass draws the filled polygons but only drawing to the z buffer then draw the lines over the top with suitable z bias. That would handle the partially obscured surface problem. Using the two pass approach I have another problem. I am not using textures. So I think the wireframe will be the same color as the fill and won't be visible! The first pass only draws to the z-buffer the image buffer is left unchanged (so you can see through the wire frame). The image buffer is only updated on the second pass. It's been a while since I did any D3D stuff so I'm not sure about the practicality of the first pass.  I think you'll need to draw those line manually as wireframe mode is a built in mode so I don't think you can modify that. You can get the list of vertex in your mesh and process them into a list of lines that you need to draw.  The built-in wireframe mode renders edges of the primitives. As in D3D the primitives are triangles (or lines or points - but not arbitrary polygons) that means the built-in way won't cut it. I guess you have to look up some sort of ""edge detection"" algorithms. These could operate in image space where you render the model into a texture assigning unique color for each logical polygon and then do a postprocessing pass using pixel shader and detect any changes in color (color changes = output black otherwise output something else). Alternatively you could construct a line list that only has the edges you need and just render them. Yet another alternative could be using geometry shaders in Direct3D 10. Somehow lots of different options here."
19,A,"Creating a composite Shape in Java 2D Using Java 2D I've patched several Bezier curves (CubicCurve2D) together to create a ""blob"". The problem I now face is how to: Efficiently fill the blob with a given colour. Efficiently determine whether a given point lies inside the blob. I noticed thst CubicCurve2D implements Shape which provides numerous contains methods for determining ""insideness"" and that Graphics2D is able to fill a Shape via the fill(Shape) (which I believe uses Shape's getPathIterator methods to do this). Given this I was hoping I could create a composite Shape whereby my getPathIterator(AffineTransform) method would simply link the underlying PathIterators together. However this is producing a NoSuchElementException once my shape contains more than one CubicCurve2D. Even if I do manage to achieve this I'm not convinced it will work as expected because a CubicCurve2D is always filled on the convex side and my ""blob"" is composed of concave and convex curves. The ""contains"" problem is even harder as a point can legitimately lie within the blob but not within any of the individual curves. Am I approaching this problem in the correct way (trying to implement Shape?) or is there an idiomatic way to do this that I'm unaware of? I would have thought that the problem of compositing geometric shapes would be fairly common. Does anyone have any suggestions regarding how to solve this problem? Thanks in advance. I'm not sure I understand your question but composite shapes can be created with the class java/awt/geom/Area. Cool - Thanks Pierre! I'll check it out. Pierre - Thanks a lot; this worked perfectly.  Looking to Shape for a solution is the right way to go about this. If you have a collection of curves that you are trying to assemble into a shape I would suggest that you use a GeneralPath. Simply add your curves or straight line segments as required. Look to the interface to see the various append methods. Also note that you can 'complete' the shape by joining the last point to the starting point. Once the path is closed there are a number of different versions of contains() that can be used please take the time to read each of their descriptions as there are trade-offs in terms of speed and accuracy depends on your application. Also it is easy to get a shape from the path and fill it transform it etc."
20,A,"Swing: How to read graphic information underneath a component? How could I ""read"" graphic information underneath a component (let's say as BufferedImage)? I want to make that component half-translucent (already done) and apply graphic effects on underlying stuff such as blur all elements under that component (but not the component itself). My approach is probable wrong: I try to get graphic information from Graphics2D instance given to me in the paint(...) method but it's empty right? Question: is your component top-level (i.e. do you want to know what's on the desktop under a JFrame) or do you just want to know about components that are layered underneath the component? If 1) then it's a simple matter to grab a screenshot with the java.awt.Robot method createScreenCapture(Rectangle); the Rectangle should be your window bounds in this case. If 2) then if you have a reference to the component underneath you can make use of the fact that paint(Graphics) doesn't care where the Graphics object came from. You can create a BufferedImage call createGraphics() (in case a Graphics2D is expected) and pass the result to the paint(Graphics) method of the component that you want to capture. Note that if the component is a container it will paint its children also; this may or may not be what you want. I need number 2). Thank you I'll try this!!!!"
21,A,What's the difference between D3DERR_OUTOFVIDEOMEMORY and E_OUTOFMEMORY guys I am developing a tool drawing primitves with DX9 in my XP-32. When create vertex buffer and index buffer there could be some error of creation failed. Return code could be D3DERR_OUTOFVIDEOMEMORY or E_OUTOFMEMORY. I am not sure what the difference of them. I use VideoMemory tool in DX sample to check the memory and it reports 1024MB. Does that mean if I create a bunch of managed resource more than 1024MB it will report D3DERR_OUTOFVIDEOMEMORY? If there is no more free virtual space memory in process and malloc fail DX9 will report E_OUTOFMEMORY? E_OUTOFMEMORY means that DirectX was unable to allocate (ie through malloc or new) the block of memory you requested. D3DERR_OUTOFVIDEOMEMORY means that DirectX was unable to allocate (ie out of the pool of memory either on the gfx card or reserved in main memory) the block of memory you requested. Caveat: Drivers might lie.  D3DERR_OUTOFVIDEOMEMORY is a directx memory error...not necessarily related to video memory it could memory occupied for holding a scene or drawing an image as you have found out if there is not enough memory for your process you will get E_OUTOFMEMORY. The latter refers to the memory that is assigned to your process being exhausted. You did not say what operating system/hardware spec you have best bet would be to look into getting a system memory upgrade if you're running low on resources.. Edit: Some laptops/netbooks have a graphics adapter that is 'fitted with system memory' ok these graphics cards are not serious for the likes of 'Beyond Call of Duty' and other top end games...the graphics card actually steal some memory from the main board thus inflating the amount of RAM that is available to the graphics controllers. They are fine if you are doing word processing/emailing and so on...but at the cost of the system ram which is gobbled by the controller a la 'Integrated graphics controller'... Hope this helps Best regards Tom. Thanks. My system is XP 32 bit with D3D9. From D3D Video Memory sample it shows hMonitor Device Name: \\.\DISPLAY1 GetVideoMemoryViaDirectDraw dwAvailableVidMem: 627 MB (658096128) GetVideoMemoryViaDxDiag n/a GetVideoMemoryViaWMI dwAdapterRAM: 640 MB (671088640) GetVideoMemoryViaDXGI n/a GetVideoMemoryViaD3D9 dwAvailableTextureMem: 1109 MB (1162870784) When I create vertex/index buffer near 1109 MB D3D will return E_OUTOFMEMORY error. But total process memory usage is only 1.3GB not high. And when I try to lock a existed d3d buffer object to read its content under a critical memory environment where process already used 1.7GB sometimes Lock will fail and report E_OUTOFMEMORY. It seems E_OUTOFMEMORY could happened in many situations. I am confused.
22,A,How to blit() in android? I'm used to handle graphics with old-school libraries (allegro GD pygame) where if I want to copy a part of a bitmap into another... I just use blit. I'm trying to figure out how to do that in android and I got very confused. So... we have these Canvas that are write-only and Bitmaps that are read-only? It seems too stupid to be real there must be something I'm missing but I really can't figure it out. edit: to be more precise... if bitmaps are read only and canvas are write only I can't blit A into B and then B into C? The code to copy one bitmap into another is like this: Rect src = new Rect(0 0 50 50); Rect dst = new Rect(50 50 200 200); canvas.drawBitmap(originalBitmap src dst null); That specifies that you want to copy the top left corner (50x50) of a bitmap and then stretch that into a 150x150 Bitmap and write it 50px offset from the top left corner of your canvas. You can trigger drawing via invalidate() but I recommend using a SurfaceView if you're doing animation. The problem with invalidate is that it only draws once the thread goes idle so you can't use it in a loop - it would only draw the last frame. Here are some links to other questions I've answered about graphics they might be of use to explain what I mean. How to draw a rectangle (empty or filled and a few other options) How to create a custom SurfaceView for animation Links to the code for an app with randomly bouncing balls on the screen also including touch control Some more info about SurfaceView versus Invalidate() Some difficulties with manually rotating things In response to the comments here is more information: If you get the Canvas from a SurfaceHolder.lockCanvas() then I don't think you can copy the residual data that was in it into a Bitmap. But that's not what that control is for - you only use than when you've sorted everything out and you're ready to draw. What you want to do is create a canvas that draws into a bitmap using Canvas canvas = new Canvas(yourBitmap) You can then do whatever transformations and drawing ops you want. yourBitmap will contain all the newest information. Then you use the surface holder like so: Canvas someOtherCanvas = surfaceHolder.lockCanvas() someOtherCanvas.drawBitmap(yourBitmap ....) That way you've always got yourBitmap which has whatever information in it you're trying to preserve. Mmmmh. While this approach works it is very slow on slow devices. My emulator dropped from ~25fps to barely 14. My Nx1 instead had apparently almost no problem and is still running at 55-60fps. so if I get the Canvas from a SurfaceHolder using lockCanvas I have no way to access that bitmap for reading? I'm sorry I still don't get one thing: you are drawing into a canvas not into a bitmap. Since canvas are as far as I understand read-only you can't then draw the destination to a NEW one. I've edited my answer to try to help this confusion. Yes you don't draw directly into Bitmaps but through the intermediary of a Canvas. Use something like this: `Bitmap someBitmap = Bitmap.createBitmap(width height Bitmap.Config.RGB_565);` to create a bitmap with specified dimensions and color space then `Canvas canvas = new Canvas(someBitmap);` to make drawing commands to that canvas modify the bitmap. ooh thanks man!  In android you draw to the canvas and when you want it to update you call invalidate which will the redraw this canvas to the screen. So I'm guessing you have overridden the onDraw method of your view so just add invalidate(); @Override public void onDraw(Canvas canvas) { // Draw a bitmap to the canvas at 00 canvas.drawBitmap(mBitmap 0 0 null); // Add in your drawing functions here super.onDraw(canvas); // Call invalidate to draw to screen invalidate(); } The above code simply redraws the bitmap constantly of course you want to add in extra thing to draw and consider using a timing function that calls invalidate so that it is not constantly running. I'd advice having a look at the lunarlander sources. yes but what if I want to draw a bitmap into another bitmap and post the second one in the canvas? Why not draw them both onto canvas? The canvas isn't update until the invalidate is called.
23,A,"correcting fisheye distortion programmatically BOUNTY STATUS UPDATE: I discovered how to map a linear lens from destination coordinates to source coordinates. I actually struggle to reverse it and to map source coordinates to destination coordinates. What is the inverse in code in the style of the converting functions I posted? I also see that my undistortion is imperfect on some lenses - presumably those that are not strictly linear. What is the equivalent to-and-from source-and-destination coordinates for those lenses? Again more code than just mathematical formulae please! Question as originally stated: I have some points that describe positions in a picture taken with a fisheye lens. I want to convert these points to rectilinear coordinates. I want to undistort the image. I've found this description of how to generate a fisheye effect but not how to reverse it. There's also a blog post that describes how to use tools to do it; these pictures are from that: Source image taken with a fisheye lens Corrected image (technically also with perspective correction but that's a separate step) How do you calculate the radial distance from the centre to go from fisheye to rectilinear? My function stub looks like this: Point correct_fisheye(const Point& pconst Size& img) { // to polar const Point centre = {img.width/2img.height/2}; const Point rel = {p.x-centre.xp.y-centre.y}; const double theta = atan2(rel.yrel.x); double R = sqrt((rel.x*rel.x)+(rel.y*rel.y)); // fisheye undistortion in here please //... change R ... // back to rectangular const Point ret = Point(centre.x+R*cos(theta)centre.y+R*sin(theta)); fprintf(stderr""(%d%d) in (%d%d) = %f%f = (%d%d)\n""p.xp.yimg.widthimg.heightthetaRret.xret.y); return ret; } Alternatively I could somehow convert the image from fisheye to rectilinear before finding the points but I'm completely befuddled by the OpenCV documentation. Is there a straightforward way to do it in OpenCV and does it perform well enough to do it to a live video feed? @mtrw My source image is fisheye distorted and I want to undistort it I don't quite get what you're looking for. The fisheye maps from a sphere to the picture plane. The reverse mapping would be from the picture back to a sphere right? What rectilinear coordinate are you looking for? So is the picture on http://photo.net/learn/fisheye/ what you're looking for? Yes the corrected picture e.g. via OpenCV or a formula for correcting any point in the picture. Do you need to do this for a range of cameras or just for one camera / one type of lens? Will did you ever get a conclusive answer to this? I'd be very interested to see any code you ended up with. @Will I know it's been a good while since this question was asked but I've got a similar problem -- did you ever get a piece of functional code for the transformation including perspective correction you mentioned? @jogal I ended up using the solution I posted myself below; good luck! I found this pdf file and I have proved that the maths are correct (except for the line vd = xd*fv+v0 which should say vd = yd+fv+v0). http://perception.inrialpes.fr/CAVA_Dataset/Site/files/Calibration_OpenCV.pdf It does not use all of the latest co-efficients that OpenCV has available but I am sure that it could be adapted fairly easily. double k1 = cameraIntrinsic.distortion[0]; double k2 = cameraIntrinsic.distortion[1]; double p1 = cameraIntrinsic.distortion[2]; double p2 = cameraIntrinsic.distortion[3]; double k3 = cameraIntrinsic.distortion[4]; double fu = cameraIntrinsic.focalLength[0]; double fv = cameraIntrinsic.focalLength[1]; double u0 = cameraIntrinsic.principalPoint[0]; double v0 = cameraIntrinsic.principalPoint[1]; double u v; u = thisPoint->x; // the undistorted point v = thisPoint->y; double x = ( u - u0 )/fu; double y = ( v - v0 )/fv; double r2 = (x*x) + (y*y); double r4 = r2*r2; double cDist = 1 + (k1*r2) + (k2*r4); double xr = x*cDist; double yr = y*cDist; double a1 = 2*x*y; double a2 = r2 + (2*(x*x)); double a3 = r2 + (2*(y*y)); double dx = (a1*p1) + (a2*p2); double dy = (a3*p1) + (a1*p2); double xd = xr + dx; double yd = yr + dy; double ud = (xd*fu) + u0; double vd = (yd*fv) + v0; thisPoint->x = ud; // the distorted point thisPoint->y = vd;  I blindly implemented the formulas from here so I cannot guarantee it would do what you need. Use auto_zoom to get the value for the zoom parameter.  def dist(xy): return sqrt(x*x+y*y) def fisheye_to_rectilinear(src_sizedest_sizesxsycrop_factorzoom): """""" returns a tuple of dest coordinates (dxdy) (note: values can be out of range) crop_factor is ratio of sphere diameter to diagonal of the source image"""""" # convert sxsy to relative coordinates rx ry = sx-(src_size[0]/2) sy-(src_size[1]/2) r = dist(rxry) # focal distance = radius of the sphere pi = 3.1415926535 f = dist(src_size[0]src_size[1])*factor/pi # calc theta 1) linear mapping (older Nikon) theta = r / f # calc theta 2) nonlinear mapping # theta = asin ( r / ( 2 * f ) ) * 2 # calc new radius nr = tan(theta) * zoom # back to absolute coordinates dx dy = (dest_size[0]/2)+rx/r*nr (dest_size[1]/2)+ry/r*nr # done return (int(round(dx))int(round(dy))) def fisheye_auto_zoom(src_sizedest_sizecrop_factor): """""" calculate zoom such that left edge of source image matches left edge of dest image """""" # Try to see what happens with zoom=1 dx dy = fisheye_to_rectilinear(src_size dest_size 0 src_size[1]/2 crop_factor 1) # Calculate zoom so the result is what we wanted obtained_r = dest_size[0]/2 - dx required_r = dest_size[0]/2 zoom = required_r / obtained_r return zoom  I took what JMBR did and basically reversed it. He took the radius of the distorted image (Rd that is the distance in pixels from the center of the image) and found a formula for Ru the radius of the undistorted image. You want to go the other way. For each pixel in the undistorted (processed image) you want to know what the corresponding pixel is in the distorted image. In other words given (xu yu) --> (xd yd). You then replace each pixel in the undistorted image with its corresponding pixel from the distorted image. Starting where JMBR did I do the reverse finding Rd as a function of Ru. I get: Rd = f * sqrt(2) * sqrt( 1 - 1/sqrt(r^2 +1)) where f is the focal length in pixels (I'll explain later) and r = Ru/f. The focal length for my camera was 2.5 mm. The size of each pixel on my CCD was 6 um square. f was therefore 2500/6 = 417 pixels. This can be found by trial and error. Finding Rd allows you to find the corresponding pixel in the distorted image using polar coordinates. The angle of each pixel from the center point is the same: theta = arctan( (yu-yc)/(xu-xc) ) where xc yc are the center points. Then xd = Rd * cos(theta) + xc yd = Rd * sin(theta) + yc Make sure you know which quadrant you are in. Here is the C# code I used  public class Analyzer { private ArrayList mFisheyeCorrect; private int mFELimit = 1500; private double mScaleFESize = 0.9; public Analyzer() { //A lookup table so we don't have to calculate Rdistorted over and over //The values will be multiplied by focal length in pixels to //get the Rdistorted mFisheyeCorrect = new ArrayList(mFELimit); //i corresponds to Rundist/focalLengthInPixels * 1000 (to get integers) for (int i = 0; i < mFELimit; i++) { double result = Math.Sqrt(1 - 1 / Math.Sqrt(1.0 + (double)i * i / 1000000.0)) * 1.4142136; mFisheyeCorrect.Add(result); } } public Bitmap RemoveFisheye(ref Bitmap aImage double aFocalLinPixels) { Bitmap correctedImage = new Bitmap(aImage.Width aImage.Height); //The center points of the image double xc = aImage.Width / 2.0; double yc = aImage.Height / 2.0; Boolean xpos ypos; //Move through the pixels in the corrected image; //set to corresponding pixels in distorted image for (int i = 0; i < correctedImage.Width; i++) { for (int j = 0; j < correctedImage.Height; j++) { //which quadrant are we in? xpos = i > xc; ypos = j > yc; //Find the distance from the center double xdif = i-xc; double ydif = j-yc; //The distance squared double Rusquare = xdif * xdif + ydif * ydif; //the angle from the center double theta = Math.Atan2(ydif xdif); //find index for lookup table int index = (int)(Math.Sqrt(Rusquare) / aFocalLinPixels * 1000); if (index >= mFELimit) index = mFELimit - 1; //calculated Rdistorted double Rd = aFocalLinPixels * (double)mFisheyeCorrect[index] /mScaleFESize; //calculate x and y distances double xdelta = Math.Abs(Rd*Math.Cos(theta)); double ydelta = Math.Abs(Rd * Math.Sin(theta)); //convert to pixel coordinates int xd = (int)(xc + (xpos ? xdelta : -xdelta)); int yd = (int)(yc + (ypos ? ydelta : -ydelta)); xd = Math.Max(0 Math.Min(xd aImage.Width-1)); yd = Math.Max(0 Math.Min(yd aImage.Height-1)); //set the corrected pixel value from the distorted image correctedImage.SetPixel(i j aImage.GetPixel(xd yd)); } } return correctedImage; } }  The description you mention states that the projection by a pin-hole camera (one that does not introduce lens distortion) is modeled by R_u = f*tan(theta) and the projection by common fisheye lens cameras (that is distorted) is modeled by R_d = 2*f*sin(theta/2) You already know R_d and theta and if you knew the camera's focal length (represented by f) then correcting the image would amount to computing R_u in terms of R_d and theta. In other words R_u = f*tan(2*asin(R_d/(2*f))) is the formula you're looking for. Estimating the focal length f can be solved by calibrating the camera or other means such as letting the user provide feedback on how well the image is corrected or using knowledge from the original scene. In order to solve the same problem using OpenCV you would have to obtain the camera's intrinsic parameters and lens distortion coefficients. See for example Chapter 11 of Learning OpenCV (don't forget to check the correction). Then you can use a program such as this one (written with the Python bindings for OpenCV) in order to reverse lens distortion: #!/usr/bin/python # ./undistort 0_0000.jpg 1367.451167 1367.451167 0 0 -0.246065 0.193617 -0.002004 -0.002056 import sys import cv def main(argv): if len(argv) < 10: print 'Usage: %s input-file fx fy cx cy k1 k2 p1 p2 output-file' % argv[0] sys.exit(-1) src = argv[1] fx fy cx cy k1 k2 p1 p2 output = argv[2:] intrinsics = cv.CreateMat(3 3 cv.CV_64FC1) cv.Zero(intrinsics) intrinsics[0 0] = float(fx) intrinsics[1 1] = float(fy) intrinsics[2 2] = 1.0 intrinsics[0 2] = float(cx) intrinsics[1 2] = float(cy) dist_coeffs = cv.CreateMat(1 4 cv.CV_64FC1) cv.Zero(dist_coeffs) dist_coeffs[0 0] = float(k1) dist_coeffs[0 1] = float(k2) dist_coeffs[0 2] = float(p1) dist_coeffs[0 3] = float(p2) src = cv.LoadImage(src) dst = cv.CreateImage(cv.GetSize(src) src.depth src.nChannels) mapx = cv.CreateImage(cv.GetSize(src) cv.IPL_DEPTH_32F 1) mapy = cv.CreateImage(cv.GetSize(src) cv.IPL_DEPTH_32F 1) cv.InitUndistortMap(intrinsics dist_coeffs mapx mapy) cv.Remap(src dst mapx mapy cv.CV_INTER_LINEAR + cv.CV_WARP_FILL_OUTLIERS cv.ScalarAll(0)) # cv.Undistort2(src dst intrinsics dist_coeffs) cv.SaveImage(output dst) if __name__ == '__main__': main(sys.argv) Also note that OpenCV uses a very different lens distortion model to the one in the web page you linked to. Thank you jmbr! The world is beginning to make some sense. I can't actually get R_u = f*tan(2*asin(R_d/(2*f))) to give me anything but NaN however. I have no knowledge of trigonometry and its that that is holding me back right now :( This relies on having access to the camera in question. I don't I'm just getting a video. Additionally it occurs to me that cameras are mass-produced and there can't be much variation individually surely? The tools linked in the original article do not require someone to stand in front of the camera with a checkerboard!? Camera parameters vary among the same camera just by tweaking the zoom. Also you could rely on auto-calibration techniques instead of using the checkerboard. Anyway I have edited my answer to address the first part of your question providing you with the formula you were looking for. This was auto-accepted - I was holding out for a more explicit answer to the question I posed. @Will: since the bounty system has been improved you can now accept another answer as well  If you think your formulas are exact you can comput an exact formula with trig like so: Rin = 2 f sin(w/2) -> sin(w/2)= Rin/2f Rout= f tan(w) -> tan(w)= Rout/f (Rin/2f)^2 = [sin(w/2)]^2 = (1 - cos(w))/2 -> cos(w) = 1 - 2(Rin/2f)^2 (Rout/f)^2 = [tan(w)]^2 = 1/[cos(w)]^2 - 1 -> (Rout/f)^2 = 1/(1-2[Rin/2f]^2)^2 - 1 However as @jmbr says the actual camera distortion will depend on the lens and the zoom. Rather than rely on a fixed formula you might want to try a polynomial expansion: Rout = Rin*(1 + A*Rin^2 + B*Rin^4 + ...) By tweaking first A then higher-order coefficients you can compute any reasonable local function (the form of the expansion takes advantage of the symmetry of the problem). In particular it should be possible to compute initial coefficients to approximate the theoretical function above. Also for good results you will need to use an interpolation filter to generate your corrected image. As long as the distortion is not too great you can use the kind of filter you would use to rescale the image linearly without much problem. Edit: as per your request the equivalent scaling factor for the above formula: (Rout/f)^2 = 1/(1-2[Rin/2f]^2)^2 - 1 -> Rout/f = [Rin/f] * sqrt(1-[Rin/f]^2/4)/(1-[Rin/f]^2/2) If you plot the above formula alongside tan(Rin/f) you can see that they are very similar in shape. Basically distortion from the tangent becomes severe before sin(w) becomes much different from w. The inverse formula should be something like: Rin/f = [Rout/f] / sqrt( sqrt(([Rout/f]^2+1) * (sqrt([Rout/f]^2+1) + 1) / 2 )  (Original poster providing an alternative) The following function maps destination (rectilinear) coordinates to source (fisheye-distorted) coordinates. (I'd appreciate help in reversing it) I got to this point through trial-and-error: I don't fundamentally grasp why this code is working explanations and improved accuracy appreciated! def dist(xy): return sqrt(x*x+y*y) def correct_fisheye(src_sizedest_sizedxdyfactor): """""" returns a tuple of source coordinates (sxsy) (note: values can be out of range)"""""" # convert dxdy to relative coordinates rx ry = dx-(dest_size[0]/2) dy-(dest_size[1]/2) # calc theta r = dist(rxry)/(dist(src_size[0]src_size[1])/factor) if 0==r: theta = 1.0 else: theta = atan(r)/r # back to absolute coordinates sx sy = (src_size[0]/2)+theta*rx (src_size[1]/2)+theta*ry # done return (int(round(sx))int(round(sy))) When used with a factor of 3.0 it successfully undistorts the images used as examples (I made no attempt at quality interpolation): (And this is from the blog post for comparison:) The reason your code works is that you are scaling (rxry) by a factor theta (which is now a ratio not an angle). If the original lens has (as the wiki article says) a ""linear mapping"" from view angle to image offset I believe the atan(r)/r is correct. The reverse of your mapping should be to scale by a factor of tan(r')/r' where r' is the radius from the center of the undistorted image. And the reason both of these work is that if you have vector v' = v*k(|v|) and you want |v'|=f(|v|) you take the absolute value of the first equation: |v'|=|v|*k(|v|)=f(|v|) -- so that k(|v|)=f(|v|)/|v| @comingstorm so what's the equiv for the nonlinnear mapping?"
24,A,Are there any good Javascript graphics libraries? After staring at this 3D cube and these triangles for a while I started wondering if there's any good reliable Javascript graphics library with basic 3D support. Any suggestion? +1 for those impressive links they really are! Check out Walter Zorn's library www.walterzorn.de/en There is good enough support now for canvas that it no longer makes sense to use Zorn's pseudo-canvas. Prestaul good enough that it can run on IEs with the same performance?  John Resig's port of the Processing library to Javascript: http://ejohn.org/blog/processingjs This library is just beyond cool. This kind of visualization will be the future of the web I think! Works fine for me. Must have been one of those days :-). the link appears to be dead... or it might be one of those days in cyberspace... Wow thanks I am wondering if I'll take the pain in learning Silverlight a friend of mine was laughing when I told him that I was working with Javascript (EXT-js mainly) but JavaScript is clearly not near the end... @Tom who knows maybe you'll be the last one to laugh ;) Khan Academy recently released their really cool Computer Science tutorials using the Processing libraries. http://www.khanacademy.org/cs/docs JavaScript has progressed tons since this answer was posted... and it's expecially nice with Hardware acceleration now...  The canvas html element may be the best backing and is used as such in many libraries (I know flot and processingjs mentionned by sibblings are using it) canvas element is the lower API abstraction level you can get which may be conceptually similar to Cairo or GDI http://en.wikipedia.org/wiki/Canvas_(HTML_element) https://developer.mozilla.org/En/HTML:Canvas  Here are a few physics engines written in javascript that have some graphics capabilities. http://blog.quantumstate.co.uk/javascript-physics-engine.html http://box2d-js.sourceforge.net/  Take a look at dojox.gfx: docs tests demos (last two links to the nightly snapshot on the test server optimized for debugging not for production). It uses native graphics: SVG VML Silverlight or Canvas — whatever is available on the client covering all major browsers (IE Firefox Safari/Webkit Opera). While it is 2D it can be used as a foundation for 3D stuff. In fact there is a library that takes advantage of it: dojox.gfx3d. Examples (can be found in tests): cylinders cube pilot test And for truly adventurous types there is a library done for Dojo as part of Google Summer of Code 2008: True 3D. Obviously it is much faster than 2D/3D hybrid and suitable for fluid animation. You can explore it on your own (the previous link is a publicly available Subversion repository) but be warned: it works only on Firefox and Opera with special 3D graphics add-ons from respective vendors. You'll find all gory details in the documentation. Have fun! thanks for references - great stuff  I'm very psyched about Raphaël. I've used it in one project and it works like a charm. just curious: I'm still left a little short on what a practical use of browser-based vector graphics would be. I heard Dmitry (the author of raphael) talk a couple of months ago and was pumped about VGs but I can't think of where I'd actually use it... I used it to create a on-screen keyboard within a touch screen based kiosk web app. But graphs / plots come to mind as well. :)  Flot is a pure Javascript plotting library for jQuery.  See question about SVG posted earlier: Is it sensible to dynamically generate SVG images on websites yet?
25,A,Is it better to use Bitmap or EncodedImage in BlackBerry? In BlackBerry is it better to use the Bitmap class or EncodedImage in terms of memory usage and performance? Are there any specific tips on using these classes? If your source image is actually a png gif jpeg or whatever you have to use EncodedImage. When you set a BitmapField you can either create it with a Bitmap or set an EncodedImage later. Curiously you can only set it focusable during creation where Bitmap is required. Bitmap will use more memory (fully decoded image) unless EncodedImage internally also keeps a decoded copy (no idea -- could depend on the JDE version too) but I have noticed getBitmap() is often very fast. You'd probably need to do your own profiling to see in your exact use case... Shouldn't that be an answer instead of a comment? :) My observation is that better: use Bitmap and drawBitmap for elements that require repaint often (ex background image in games) Maybe it's because Bitmap is a raw format so no performance hit for decoding EncodedImage before drawImage. On the other side GIF animation works perfectly with EncodedImage. use EncodedImage for animation or for a large amount of resources (ex photos or decore elements) When you load Bitmap from gif png jpg formats they will be opened as an EncodedImage anyway and if you do this many times it may beat performance (ex 50 seconds to load 14 png from resources to Bitmaps on bold avg size 80 kb tuned up to 2 seconds loading into EncodedImages) UPDATE stated by Fostah EncodedImage has a getBitmap() function that you can use to convert any EncodedImage to a Bitmap. So you can load in EncodedImage and then use as Bitmap
26,A,Graphic algorithm Unions intersect subtract I need a good source for reading up on how to create a algorithm to take two polylines (a path comprised of many lines) and performing a union subtraction or intersection between them. This is tied to a custom API so I need to understand the underlying algorithm. Plus any sources in a VB dialect would be doubly helpful. Several routines for you here. Hope you find them useful :-) // routine to calculate the square of either the shortest distance or largest distance // from the CPoint to the intersection point of a ray fired at an angle flAngle // radians at an array of line segments // this routine returns TRUE if an intersection has been found in which case flD // is valid and holds the square of the distance. // and returns FALSE if no valid intersection was found // If an intersection was found then intersectionPoint is set to the point found bool CalcIntersection(const CPoint &cPoint const float flAngle const int nVertexTotal const CPoint *pVertexList const BOOL bMin float &flD CPoint &intersectionPoint) { float d dsx dsy dx dy lambda mu px py; int p0x p0y p1x p1y; // get source position const float flSx = (float)cPoint.x; const float flSy = -(float)cPoint.y; // calc trig functions const float flTan = tanf(flAngle); const float flSin = sinf(flAngle); const float flCos = cosf(flAngle); const bool bUseSin = fabsf(flSin) > fabsf(flCos); // initialise distance flD = (bMin ? FLT_MAX : 0.0f); // for each line segment in protective feature for(int i = 0; i < nVertexTotal; i++) { // get coordinates of line (negate the y value so the y-axis is upwards) p0x = pVertexList[i].x; p0y = -pVertexList[i].y; p1x = pVertexList[i + 1].x; p1y = -pVertexList[i + 1].y; // calc. deltas dsx = (float)(cPoint.x - p0x); dsy = (float)(-cPoint.y - p0y); dx = (float)(p1x - p0x); dy = (float)(p1y - p0y); // calc. denominator d = dy * flTan - dx; // if line & ray are parallel if(fabsf(d) < 1.0e-7f) continue; // calc. intersection point parameter lambda = (dsy * flTan - dsx) / d; // if intersection is not valid if((lambda <= 0.0f) || (lambda > 1.0f)) continue; // if sine is bigger than cosine if(bUseSin){ mu = ((float)p0x + lambda * dx - flSx) / flSin; } else { mu = ((float)p0y + lambda * dy - flSy) / flCos; } // if intersection is valid if(mu >= 0.0f){ // calc. intersection point px = (float)p0x + lambda * dx; py = (float)p0y + lambda * dy; // calc. distance between intersection point & source point dx = px - flSx; dy = py - flSy; d = dx * dx + dy * dy; // compare with relevant value if(bMin){ if(d < flD) { flD = d; intersectionPoint.x = RoundValue(px); intersectionPoint.y = -RoundValue(py); } } else { if(d > flD) { flD = d; intersectionPoint.x = RoundValue(px); intersectionPoint.y = -RoundValue(py); } } } } // return return(bMin ? (flD != FLT_MAX) : (flD != 0.0f)); } // Routine to calculate the square of the distance from the CPoint to the // intersection point of a ray fired at an angle flAngle radians at a line. // This routine returns TRUE if an intersection has been found in which case flD // is valid and holds the square of the distance. // Returns FALSE if no valid intersection was found. // If an intersection was found then intersectionPoint is set to the point found. bool CalcIntersection(const CPoint &cPoint const float flAngle const CPoint &PointA const CPoint &PointB const bool bExtendLine float &flD CPoint &intersectionPoint) { // get source position const float flSx = (float)cPoint.x; const float flSy = -(float)cPoint.y; // calc trig functions float flTan = tanf(flAngle); float flSin = sinf(flAngle); float flCos = cosf(flAngle); const bool bUseSin = fabsf(flSin) > fabsf(flCos); // get coordinates of line (negate the y value so the y-axis is upwards) const int p0x = PointA.x; const int p0y = -PointA.y; const int p1x = PointB.x; const int p1y = -PointB.y; // calc. deltas const float dsx = (float)(cPoint.x - p0x); const float dsy = (float)(-cPoint.y - p0y); float dx = (float)(p1x - p0x); float dy = (float)(p1y - p0y); // Calc. denominator const float d = dy * flTan - dx; // If line & ray are parallel if(fabsf(d) < 1.0e-7f) return false; // calc. intersection point parameter const float lambda = (dsy * flTan - dsx) / d; // If extending line to meet point don't check for ray missing line if(!bExtendLine) { // If intersection is not valid if((lambda <= 0.0f) || (lambda > 1.0f)) return false; // Ray missed line } // If sine is bigger than cosine float mu; if(bUseSin){ mu = ((float)p0x + lambda * dx - flSx) / flSin; } else { mu = ((float)p0y + lambda * dy - flSy) / flCos; } // if intersection is valid if(mu >= 0.0f) { // calc. intersection point const float px = (float)p0x + lambda * dx; const float py = (float)p0y + lambda * dy; // calc. distance between intersection point & source point dx = px - flSx; dy = py - flSy; flD = (dx * dx) + (dy * dy); intersectionPoint.x = RoundValue(px); intersectionPoint.y = -RoundValue(py); return true; } return false; } // Fillet (with a radius of 0) two lines. From point source fired at angle (radians) to line Line1A Line1B. // Modifies line end point Line1B. If the ray does not intersect line then it is rotates every 90 degrees // and tried again until fillet is complete. void Fillet(const CPoint &source const float fThetaRadians const CPoint &Line1A CPoint &Line1B) { if(Line1A == Line1B) return; // No line float dist; if(CalcIntersection(source fThetaRadians Line1A Line1B true dist Line1B)) return; if(CalcIntersection(source CalcBaseFloat(TWO_PI fThetaRadians + PI * 0.5f) Line1A Line1B true dist Line1B)) return; if(CalcIntersection(source CalcBaseFloat(TWO_PI fThetaRadians + PI) Line1A Line1B true dist Line1B)) return; if(!CalcIntersection(source CalcBaseFloat(TWO_PI fThetaRadians + PI * 1.5f) Line1A Line1B true dist Line1B)) ASSERT(FALSE); // Could not find intersection? } // routine to determine if an array of line segments cross gridSquare // x and y give the float coordinates of the corners BOOL CrossGridSquare(int nV const CPoint *pV const CRect &extent const CRect &gridSquare) { // test extents if( (extent.right < gridSquare.left) || (extent.left > gridSquare.right) || (extent.top > gridSquare.bottom) || (extent.bottom < gridSquare.top)) { return FALSE; } float a b c dx dy s x[4] y[4]; int max_x max_y min_x min_y p0x p0y p1x p1y sign sign_old; // construct array of vertices for grid square x[0] = (float)gridSquare.left; y[0] = (float)gridSquare.top; x[1] = (float)(gridSquare.right); y[1] = y[0]; x[2] = x[1]; y[2] = (float)(gridSquare.bottom); x[3] = x[0]; y[3] = y[2]; // for each line segment for(int i = 0; i < nV; i++) { // get end-points p0x = pV[i].x; p0y = pV[i].y; p1x = pV[i + 1].x; p1y = pV[i + 1].y; // determine line extent if(p0x > p1x){ min_x = p1x; max_x = p0x; } else { min_x = p0x; max_x = p1x; } if(p0y > p1y){ min_y = p1y; max_y = p0y; } else { min_y = p0y; max_y = p1y; } // test to see if grid square is outside of line segment extent if( (max_x < gridSquare.left) || (min_x > gridSquare.right) || (max_y < gridSquare.top) || (min_y > gridSquare.bottom)) { continue; } // calc. line equation dx = (float)(p1x - p0x); dy = (float)(p1y - p0y); a = dy; b = -dx; c = -dy * (float)p0x + dx * (float)p0y; // evaluate line eqn. at first grid square vertex s = a * x[0] + b * y[0] + c; if(s < 0.0f){ sign_old = -1; } else if(s > 1.0f){ sign_old = 1; } else { sign_old = 0; } // evaluate line eqn. at other grid square vertices for (int j = 1; j < 4; j++) { s = a * x[j] + b * y[j] + c; if(s < 0.0f){ sign = -1; } else if(s > 1.0f){ sign = 1; } else { sign = 0; } // if there has been a chnage in sign if(sign != sign_old) return TRUE; } } return FALSE; } // calculate the square of the shortest distance from point s // and the line segment between p0 and p1 // t is the point on the line from which the minimum distance // is measured float CalcShortestDistanceSqr(const CPoint &s const CPoint &p0 const CPoint &p1 CPoint &t) { // if point is at a vertex if((s == p0) || (s == p1)) return(0.0F); // calc. deltas int dx = p1.x - p0.x; int dy = p1.y - p0.y; int dsx = s.x - p0.x; int dsy = s.y - p0.y; // if both deltas are zero if((dx == 0) && (dy == 0)) { // shortest distance is distance is to either vertex float l = (float)(dsx * dsx + dsy * dsy); t = p0; return(l); } // calc. point p on line that is closest to sourcePosition // p = p0 + l * (p1 - p0) float l = (float)(dsx * dx + dsy * dy) / (float)(dx * dx + dy * dy); // if intersection is beyond p0 if(l <= 0.0F){ // shortest distance is to p0 l = (float)(dsx * dsx + dsy * dsy); t = p0; // else if intersection is beyond p1 } else if(l >= 1.0F){ // shortest distance is to p1 dsx = s.x - p1.x; dsy = s.y - p1.y; l = (float)(dsx * dsx + dsy * dsy); t = p1; // if intersection is between line end points } else { // calc. perpendicular distance float ldx = (float)dsx - l * (float)dx; float ldy = (float)dsy - l * (float)dy; t.x = p0.x + RoundValue(l * (float)dx); t.y = p0.y + RoundValue(l * (float)dy); l = ldx * ldx + ldy * ldy; } return(l); } // Calculates the bounding rectangle around a set of points // Returns TRUE if the rectangle is not empty (has area) FALSE otherwise // Opposite of CreateRectPoints() BOOL CalcBoundingRectangle(const CPoint *pVertexList const int nVertexTotal CRect &rect) { rect.SetRectEmpty(); if(nVertexTotal < 2) { ASSERT(FALSE); // Must have at least 2 points return FALSE; } // First point set rectangle (no area at this point) rect.left = rect.right = pVertexList[0].x; rect.top = rect.bottom = pVertexList[0].y; // Increst rectangle by looking at other points for(int n = 1; n < nVertexTotal; n++) { if(rect.left > pVertexList[n].x) // Take minimum rect.left = pVertexList[n].x; if(rect.right < pVertexList[n].x) // Take maximum rect.right = pVertexList[n].x; if(rect.top > pVertexList[n].y) // Take minimum rect.top = pVertexList[n].y; if(rect.bottom < pVertexList[n].y) // Take maximum rect.bottom = pVertexList[n].y; } rect.NormalizeRect(); // Normalise rectangle return !(rect.IsRectEmpty()); }  This catalogue of implementations of intersection algorithms from the Stony Brook Algorithm Repository might be useful. The repository is managed by Steven Skiena author of a very well respected book on algorithms: The Algorithm Design Manual. That's his own Amazon exec link by the way :) That looks like it will do the trick and just a important I know what the algorithm is called and found more stuff on google thanks. Knowing the name of the algorithm is half the battle - maybe more.
27,A,"What is the preferred way to show large images in OpenGL I've had this problem a couple of times. Let's say I want to display a splash-screen or something in an OpenGL context (or DirectX for that matter it's more of a conceptual thing) now I could either just load a 2048x2048 texture and hope that the graphics card will cope with it (most will nowadays I suppose) but growing with old-school graphics card I have this bad conscience leaning over me and telling me I shouldn't use textures that large. What is the preferred way nowadays? Is it to just cram that thing into video memory tile it or let the CPU do the work and glDrawPixels? Or something more elaborate? `glDrawPixels` is really slow you certainly won't use it. I actually threw the glDrawPixels in there more for color... :) If this is a single frame splash image with no animations then there's no harm using glDrawPixels. If performance is crucial and you are required to use a texture then the correct way to do it is to determine at runtime the maximum supported texture size using a proxy texture. GLint width = 0; while ( 0 == width ) { /* use a better condition to prevent possible endless loop */ glTexImage2D(GL_PROXY_TEXTURE_2D 0 /* mip map level */ GL_RGBA /* internal format */ desiredWidth /* width of image */ desiredHeight /* height of image */ 0 /* texture border */ GL_RGBA /* pixel data format */ GL_UNSIGNED_BYTE /* pixel data type */ NULL /* null pointer because this a proxy texture */ ); /* the queried width will NOT be 0 if the texture format is supported */ glGetTexLevelParameteriv(GL_PROXY_TEXTURE_2D 0 GL_TEXTURE_WIDTH &width); desiredWidth /= 2; desiredHeight /= 2; } Once you know the maximum texture size supported by the system's OpenGL driver you have at least two options if your image doesn't fit: Image tiling: use multiple quads after splitting your image into smaller supported chunks. Image tiling for something like a splash screen should not be too tricky. You can use glPixelStorei's GL_PACK_ROW_LENGTH parameter to load sections of a larger image into a texture. Image resizing: resize your image to fit the maximum supported texture size. There's even a GLU helper function to do this for you gluScaleImage. oops ;-) (also just realized that my comment has a syntax error) GLint width = 0; // ;-) It is `GL_UNPACK_ROW_LENGTH` and it is used as follows: http://stackoverflow.com/a/205569/362515  I don't think there is a built-in opengl function but you might find a library (or write the function yourself) to break the image down into smaller chunks and then print to screen the smaller chunks (128x128 tiles or similar). I had problem with this using Slick2D for a Java game. You can check out their ""BigImage"" class for ideas here."
28,A,Opengl Selective glClipPlane I have a scene drawn in openGL (openGl 1.1 win32). I use glClipPlane to hide foreground objects to allow the user to see/edit distance parts. The selection is done natively without using openGL. But the glClipPlane applies to all openGL elements - coordinate icons gridlines etc and even elements drawn in gluOrtho2D on top - scale bars selection boxes etc. Is there anyway to selective override the clipplanes to allow these elements to be drawn while clipping the main scene? Isn't surrounding only the objects you want to hide with glEnable(GL_CLIP_PLANE); and glDisable(GL_CLIP_PLANE); enough? Yes it would have been even better if I disabled the plane with the same GL_CLIP_PLANEi that I enabled it with ! It's funny how after starring at it for hours you spot the error just after you explain it to someone else.
29,A,"C# graph drawing library? I'm looking for a (free) library which allows me to draw a CFG (control flow graph). Something like yFiles but free or preferably open source? Ideally this library would allow the user to navigate the graph (and modify it) i.e. the graph isn't just a static a priori rendered bitmap. Ideas? Update: Glee in combination with the mentioned QuickGraph library seems to work pretty nice. thx Update2: Graph# seems to be the most powerful library currently. There is also a nice tutorial on how to use it. GLEE is now called [Microsoft Automatic Graph Layout](http://research.microsoft.com/en-us/projects/msagl/) (MSAGL). MSAGL is distributed in a binary form only. A commercial license has to be bought. Try Microsoft Chart Controls for Microsoft .NET Framework 3.5 Not a graph library.  You might want to check out QuickGraph. NodeXL might also be of interest (visualization library). It's WPF but you can use a container to host it if you need WinForms. Looks interesting though this seems to be the ""algorithm part"" not the ""visualization part"" right? Just added some info on NodeXL which is a visualization API. NodeXL is a template for Excel is it really suitable for c#?  Try out this (Efficient Sugiyama algorithm is your friend): Graph#  I use GraphViz to generate this sort of graph. My app generates the .dot file that can then is then passed into GraphViz. It supports a load of file formats such as bmp jpg png pdf svg etc etc.  https://graphx.codeplex.com/ Inspired by Graph# actively developed and much more extensible."
30,A,Getting the active form caption color How can i get the color of the caption of the active form in winforms? (Without Api) The SystemColors class contains a number of properties corresponding to various system colors. The one you want is SystemColors.ActiveCaption.
31,A,"How big should an Internet Explorer toolbar icon be? I'm trying adding my own toolbar icon to Internet Explorer but am unsure what size it should be. The Designing Toolbar Icons for Internet Explorer articles on TechNet indicates 20x20 and 16x16 pixels. Messages (1 2) in the Internet Explorer Toolbar (Deskband) Tutorial at Code Project imply 22x22 and 16x16 pixels. Using Internet Explorer 7 icons that are 20x20 pixels seem to get stretched. Measuring shows they should be at least 24x24. Anyone have a definitive reference? Alternatively where does Internet Explorer get its existing toolbar icons from - I could measure it then! I presume its one of the system DLLs but which one? Also see: How big should a Firefox toolbar button be? I assume we're talking about favicons? Wikipedia defines them as 16x16 in the first sentence. The top four Google hits all indicate 16x16. http://www.animatedfavicon.com/question/what-favicon-size.php http://www.photoshopsupport.com/tutorials/jennifer/favicon.html http://www.clickfire.com/favicon-tutorial/ http://www.favicon.cc/ No I'm talking about toolbar icons. As added by a plugin.  Well by a process of trial and error it appears that icons sized 24x24 pixels are not rescaled. At least that is the case with Internet Explorer 7.0.6001.18000. Would still like find a definitive reference for IE7 and ideally other versions too. Another way to confirm it: int x y; HIMAGELIST hImageList = (HIMAGELIST) SendMessage(m_hWndToolbar TB_GETIMAGELIST 0 0); ImageList_GetIconSize(hImageList &x &y); ATLTRACE(""Image size %ux%u"" x y); Output: Image list 24x24 you should accept your own answer here"
32,A,Shapes-tool creating a vector mask every time cannot seem to fix in CS3? Every time I create a shape using the shape tool it places a vector mask on top of this. I don't know how I enabled this but it does not do it on my laptop version only my desktop. I can seem to disable this problem I am having. Even reinstalling and restoring defaults I cannot seem to stop this. Very frustrating anyone have a fix for this problem? Thanks in advance! This question would be better asked on Superuser.com. In fact check this out: http://superuser.com/questions/87343/photoshop-not-creating-vector-masksw-shape-tool
33,A,Anchor point of xy in JavaFX nodes Some sprite applications (namely pulpcore) allows you to define an anchor point for an object. For example if the anchor point of a rectangle is 00 and it's coordinates are x:0y:0 the top left point of the rectangle would be displayed on the top left point of the screen. If however the anchor point of the rectangle is 5050 and it's size is 100x100 the middle of the rectangle would be placed at 00 and only it's bottom-right part would be displayed. Is there a similar property in JavaFX? Is there an easy way to implement this (can you bind the x and the y coordinates of a JavaFX node to its own height and width?)? When I bind the xy coordinates to boundsInLocal/Parent.width I get a runtime exception. The origin for a Rectangle is upper left as you say. You have 2 options for positioning the node. You can use either layoutX and layoutY. var rect = Rectangle { layoutX: 50 layoutY: 50 widht: 100 height: 100 } or translateX and translateY var rect = Rectangle { translateX: 50 translateY: 50 widht: 100 height: 100 } The main purpose of using both layoutX/Y and translateX/Y is that translateX/Y is intended to be changeable whereas layoutX/Y is meant for initial positioning. For example you can change translateX/Y in an animation while the original position is remembered in layoutX/Y. The ultimate location will always be layoutX+ translateX and layoutY + translateY. To shift the origin to the center point (5050) use a negative translate. var rect = Rectangle { translateX: -50 translateY: -50 widht: 100 height: 100 } Some shapes are not based on upper left origin. For example Circle uses centerX and centerY. so you could do: var rect: Rectangle = Rectangle{ x: bind rect.width y: bind rect.height }; or preferably var rect: Rectangle = Rectangle{ layoutX: bind rect.width layoutY: bind rect.height }; Sorry I forgot x and y has been removed from Node. You have to use layoutX/Y or translateX/Y. So you could do Rectangle { layoutX: bind rect.layoutBounds.width }. Not good enough. I want to bind XY to the width of the current node.
34,A,How do you draw a line from one corner of the stage to the other? I am completely perplexed. I asked this question and it (any mentioned solution) doesn't seem to be working at all. All I want is to draw a line from one corner to the other. Here again is the link to the SWF file I have (it's embedded in an HTML document): test.html Here is the source: package { import flash.display.Sprite; import flash.events.Event; public class Main extends Sprite { public function Main():void { if (stage) init(); else addEventListener(Event.ADDED_TO_STAGE init); } private function init(e:Event = null):void { removeEventListener(Event.ADDED_TO_STAGE init); // entry point graphics.clear(); graphics.lineStyle(10 0x000000); graphics.moveTo(0 0); graphics.lineTo(stage.stageWidth stage.stageHeight); } } } It just doesn't work! The line goes from somewhere offscreen to about the middle of the stage. What on earth am I doing wrong? Did you try restarting Flash (or w/e you're creating the swf with)? I have had issues similar to this in the past and restarting Flash was all it took to fix them. I don't know why but every once in a while Flash just goofs up on me. @wallacoloo: Ya... I tried it with the ActiveX player and the plugin player. Same result. I'm using FlashDevelop btw. well i'm perplexed too. Copying the EXACT SAME CODE and running the swf WORKS. http://www.swfcabin.com/open/1271209077 so i swear it's something wrong with your embed tag or something. this is really really really weird. I'm using FlashDevelop. Is this a known issue or what? I'm using FlashDevelop too. I don't know why it's not working for you.... can someone else confirm whether it works for them or not? @jonathanasdf: What would be the command for manually compiling with `mxmlc`? http://livedocs.adobe.com/flex/3/html/help.html?content=compilers_13.html just plain mxmlc filename.as @jonathanasdf: Tried that - I just end up with the same thing... with a blue background now.  You compiled your SWF to be 800x600 while your embed is at 350x350. If you want your code to work anyway you should set the stage's scaleMode to StageScaleMode.NO_SCALE and the align to StageAlign.TOP_LEFT. By default they are StageScaleMode.NO_BORDER and StageAlign.TOP which makes your SWF display at about 466x350 (maintaining 4:3 ratio) thus having its upper left corner at about (-580) and it's lower right at about (408 350) (being horizontally centered (relatively to area of the embed)). ahh.. never thought of downloading the swf! :) @jonathanasdf: well 1. it's the FD default setting 2. if you download the SWF and open it in the standalone player it will open at its prefered size. of course there are plenty of other ways to find out. :) jeez... so it WAS a problem with his embed. But how do you know his SWF was set to be 800x600 not actually 350x350? Thank you everyone - I was beginning to think I had lost it.
35,A,"Collision-Detection methods in C++ I am new to c++ and I have been practicing collision in a small game program that does nothing and I just can't get the collision right So I use images loaded into variables background = oslLoadImageFile(""background.png"" OSL_IN_RAM OSL_PF_5551); sprite = oslLoadImageFile(""sprite.png"" OSL_IN_RAM OSL_PF_5551); bush = oslLoadImageFile(""bush.png"" OSL_IN_RAM OSL_PF_5551); While there are variables stored like sprite->x = 3; if ( (sprite->x + spritewidth > bush->x) && (sprite->x < bush->x + bushwidth) && (sprite->y + spriteheight > bush->y) && (sprite->y < bush->y + bushheight) ) { bushcol = 1; } else { bushcol = 0; } So when i press a button if (osl_keys->held.down) { if (bushcol == 1) { sprite->y = bush->y + 38; } else { sprite->y += 3; } } if (osl_keys->held.up) { if (bushcol == 1) { sprite->y = bush->y - 23; } else { sprite->y -= 3; } } if (osl_keys->held.right) { if (bushcol == 1) { sprite->x = bush->x - 28; } else { sprite->x += 3; } } if (osl_keys->held.left) { if (bushcol == 1) { sprite->x = bush->x + 28; } else { sprite->x -= 3; } } i was thinking of things like sprite->y = bushheight - 24; but it doesnt work Any suggestions? What exactly isn't working? Also did you mean to mix x and y in the expression sprite->y = busy->x - 3? First of all i suppose you mean sprite->y = bush->**y** - 3; second i dont know what platform you are using but often times the y-coordinates are inverted. i.e. y=0 corresponds to top of the screen. In that case your comparisons might not work. third collision checking can quickly become complicated when you add rotation and non-rectangular objects to it. You should consider using CGAL or some other computational geometry algorithms library which can handle polygon intersection.  I think you have the basic idea. Just check your work. Here is a simple version which compiles: #import <stdlib.h> typedef struct { // I'm going to say x y is in the center int x; int y; int width; int height; } Rect; Rect newRect(int x int y int w int h) { Rect r = {x y w h}; return r; } int rectsCollide(Rect *r1 Rect *r2) { if (r1->x + r1->width/2 < r2->x - r2->width/2) return 0; if (r1->x - r1->width/2 > r2->x + r2->width/2) return 0; if (r1->y + r1->height/2 < r2->y - r2->height/2) return 0; if (r1->y - r1->height/2 > r2->y + r2->height/2) return 0; return 1; } int main() { Rect r1 = newRect(1002004040); Rect r2 = newRect(1102104040); Rect r3 = newRect(1502504040); if (rectsCollide(&r1 &r2)) printf(""r1 collides with r2\n""); else printf(""r1 doesnt collide with r2\n""); if (rectsCollide(&r1 &r3)) printf(""r1 collides with r3\n""); else printf(""r1 doesnt collide with r3\n""); }  I'd suggest making a function solely for the purpose of bounding box colision detection. It could look like IsColiding(oslImage item1 oslImage item2) { /* Perform check */ } in which you perform the check if there is a collision between image 1 and image 2. As for the algorithm you're trying to use check out this wikipedia for example AABB bounding box Especially this part: In the case of an AABB this tests becomes a simple set of overlap tests in terms of the unit axes. For an AABB defined by MN against one defined by OP they do not intersect if (Mx>Px) or (Ox>Nx) or (My>Py) or (Oy>Ny) or (Mz>Pz) or (Oz>Nz)."
36,A,"Are there any good / interesting analogs to regular expressions in 2d? Are there any good (or at least interesting but flawed) analogs to regular expressions in two dimensions? In one dimension I can write something like /aaac?(bc)*b?aaa/ to quickly pull out a region of alternating bs and cs with a border of at least three as. Perhaps as importantly I can come back a month later and see at a glance what it's looking for. I'm finding myself writing custom code for analogous problems in 2d (some much more complicated / constrained) and it would be nice to have a more concise and standardized notation even if I have to write the engine behind it myself. A second example might be called ""find the +"". The goal is to locate a column of 3 or more as a b bracketed by 3 or more as with three or more as below. It should match: ..7...hkj.k f 7...a h o j ----a-------- j .ag- 8 9 .aaabaaaaa7 j k .ag- h j hh a----? j a hjg and might be written as [b^(a{3})v(a{3})>(a{3})<(a{3})] or... Suggestions? Can you give us an example? regular expressions end up being state machines doing such a state machine on a 2d space sounds exceptionaly complex (and a general solution without considerable cleverness would be very slow. Just detecting connected regions is quite complex which this would rely on... I would suggest building a library to enumerate regions (where many implementations exist for example manifolds internal patterns) and then make regions have several useful well defined operations on them like traversing the borders and at each point checking the values around it etc. I should have made this an answer :) Then use the objects/structures returned from these functions in a chained fashion (lambdas or anon classes will make this much easier) to make the code read more easily e.g.: bitmap.FindWithPattern('bc').Where(r => r.Border.All(p => p.Exterior[3] == 'aaa')) @ShuggyCoUk It's not too late to make it an answer. Nice problem. First ask yourself if you can constrain the pattern to a ""+"" pattern or if you would it need to test/match rectangles also. For instance a rectangle of [bc] with a border of a would match the center rectangle below but would also match a ""+"" shape of [c([bc]*a})v([bc]*a)>([bc]*a)<([bc]*a)] (in your syntax) xxxaaaaaxxx yzyabcba12d defabcbass3 oreabcba3s3 s33aaaaas33 k388x.egiee If you can constrain it to a ""+"" then your task is much easier. As ShuggyCoUk said parsing a RE is usually equivalent to a DFSM -- but for a single serial input which simplifies things greatly. In your ""RE+"" engine you'll have to debug not only the engine but also each place that the expressions are entered. I'd like the compiler to catch any errors it could. Given that you could also use something that was explicitly four RE's like: REPlus engine = new REPlus('b').North(""a{3}"") .East(""a{3}"").South(""a{3}"").West(""a{3}""); However depending on your implementation this may be cumbersome. And with regard to the traversal engine -- would the North/West patterns match RtL or LtR? It could matter if the patterns match differently with or w/o greedy sub-matches. Incidentally I think the '^' in your example is supposed to be one character to the left outside the parenthesis. Fixed the `^` typo thanks. The ""+"" example is just one--the border and pattern fill is another. Also I've got something that finds closed circuits with restricted path widths various ""bracketing"" structures and probably a few others (once you recognize a tool you often find unexpected uses).  Not being a regex expert but finding the problem interesting I looked around and found this interesting blog entry. Especially the syntax used there for defining the 2D regex looks appealing. The paper linked there might tell you more than me. Now why didn't google turn that up for me? Oh well no worries. This sounds like exactly what I was looking for thanks! After browsing through the paper I'm declaring this the solution. If anyone is interested the paper behind the blog entry is linked from the primary author's page here http://www.mat.uniroma2.it/~giammarr/Research/pubbl.html without needing to pay a publisher for the privilege of downloading it.  Regular expressions are designed to model patterns in one dimension. As I understand it you want to match patterns in a two dimensional array of characters. Can you use regular expressions? That depends on whether the property that you are searching for is decomposable into components which can be matched in one dimension or not. If so you can run your regular expressions on columns and rows and look for intersections in the solution sets from those. Depending on the particular problem you have this may either solve the problem or it may find areas in your 2d array which are likely to be solutions. Whether your problem is decomposable or not I think writing some custom code will be inevitable. But at least it sounds like an interesting problem so the work should be pleasant.  You're essentially talking about a spatial query language. There are plenty out there if you look up spatial query geographic query and graphic querying. The spatial part generally comes down to points lines and objects in a region that have other given attributes. Regions can be specified as polygons distance from a point (e.g. circles) distance from a linear feature such as a road all points on one side of a linear feature etc... You can then get into more complex queries like set of all nearest neighbours shortest path travelling salesman and tesselations like Delaunay TINs and Voronoi diagrams. While interesting that doesn't seem to be quite what I'm looking for. What I've found seems to be about geometric properties in a continuum while I'm more interested in structural features on a lattice."
37,A,"How would a 'fading touch trail' effect be implemented on the iPhone? I'm wondering about how you would implement an effect similar to this one implemented in flash. It would just be a black screen which when the user touches and drags over it produces an effect similar to the one seen in the flash movie. What is this effect called? Do you have a better example? One of my biggest problems is articulating exactly what it is I'm talking about. I need both a name for this type of effect as well as a video that can display what it is I'm talking about. This would help others understand the problem better. The closest term I could think of which describes something similar is a pointer trail. The only difference being that instead of showing the cursor it would show a solid trail that gently fades away. The website that made the flash example calls it Particle Ghosting. Another example of it is in this YouTube video for HP which shows the effect between 0:30 to 0:36. It starts out more like paint but eventually turns into the fading trail effect. If you have a better name for this effect or a better example please mention it and I will update the post. Which iPhone technology would be required? OpenGL or Quartz 2D? I'm guessing core graphics is out of the question due to speed. Any other technology I'm missing? It needs to be fast enough without any lag. Assuming a trail that lasts about a second at any given point there may be a considerable amount of animated pixels at the same time when the user moves their finger fast. Possibly even filling up most of the screen with pixels that need to be faded out. The effect should look very elegant nothing like the choppiness of the pointer trail in XP. How would it be implemented? Ideally I don't want the fadeout effect to be as rigid as it is in the flash example (where it's a perfect circle) and more like in the HP video where the fade out is more organic (where there are different fadeouts at every point in the trail). Would I manipulate individual pixels directly? I.e. would I need to keep track of every pixel that the finger has moved over and call a function repeatedly such as PaintPixel(x y brightness) which modifies the RGB value of the pixel? This seems like an excessive amount of pixel modification and sounds like it would slow the whole system down. However I have never done this kind of thing before so I wouldn't know if this is how it's usually done. For example let's assume that a single touch takes up 32x32 pixels. We have 32 x 32 = 1024 pixels for each touch. As the finger moves an additional 1024 pixels will need to be animated most of which are already overlapping with the previous touch. Assuming the finger passed over the height of the iPhone within a second we're talking about 480 x 32 pixels = 15360 pixels being updated many times a second. Would I use a small video clip that fades from white to black and just generate a lot of instances of the video clip as the finger moves? I don't have a version of Flash that can open up the flash example so I can't examine the source code of it but I'm guessing that it's just creating new instances of video clips as the dot moves around. This gets a bit tricky for an organic fade out which would probably involve making several fade out video clips and randomly choosing one for each point. Would I use a set of images (e.g. 100 or so circles each representing a touch which fades from solid white to black) and then create and replace instances of them as time progresses? For example as the finger moves I would display touch100.jpg at that point which represents pure white and in the next iteration the same point would display touch99.jpg then touch98.jpg continuing until it reaches touch0.jpg which represents pure black. Any other ideas I'm missing? I'm sorry if these ideas sound outlandish. As you can probably tell I have never done anything similar to this before so I don't know what the usual way of implementing this kind of effect is and I'm just throwing out any idea I can think of. Which resources would you recommend to learn how to implement this? Is there any learning material you would recommend for someone trying to learn how to implement this kind of thing? I don't want to spend a bunch of time learning one technology (e.g. Quartz) when I would need to know an entirely different technology to implement it (e.g. OpenGL). Is there any other information I forgot to provide? Are there any additional questions you would recommend I ask? I would do the following: Create a bitmap onto which you will draw call it your canvas Create a image representing the trail in your example a solid circle Every frame draw this circle where the user currently has their finger Also on every frame apply a transform to the canvas which multiplies the alpha of each pixel by a value smaller than 1 ie 0.99 This will make everything that is on screen slowly fade with the circles drawn most recently being most visible. I've done similar things in Flash (dust behind a car) and it works pretty well As for a technology you could probably do this with Quartz only if the performance was too bad would I switch to OpenGL  If you'd like to use OpenGL you can: Draw the trail out as an image you'll need to experiment with sizes but the trail image should have an alpha channel. Next add code to calculate a movement vector for your pointer. Now calculate a line perpendicular to the movement vector for your pointer. Create points that fall along the line that you calculated these points should be offset from the position of your pointer (the distance depends on the size of your pointer). Store a buffer of the two points calculated above over time. Now you'll use OpenGL to create a quad strip using the points in your buffer as vertices. Finally apply the trail image that you drew as a texture to the quad strip that you created.  If you download Cocos2D and launch the ParticlesTest you will see many examples that are similar to this effect.  pheelicks has a good design simple to implement and it may work well. It has the disadvantage of having to re-calculate the entire screen every frame which may be less efficient than would be optimal. I would propose the following design that may have better performance (or it may have worse performance; most performance things have to be tested to see). For every circle generate a non-opaque CALayer and attach it to your view. Assign its contents to be a pre-rendered circle CGImageRef. Attach a fade animation to it. Attach it to the view. Using an animation delegate remove the layer when it completes its animation. I would probably then re-use the layer rather than destroying it and creating a new one. It already has your circle in it after-all so reusing it should be extremely cheap. (EDIT: I was rethinking this; you don't even have to remove and re-add the layer just move it to its new location or hide it if you don't need it. You could make all these circles sub-layers of a ""trail"" layer if you wanted an easy way to remove the whole trail when it's not needed.) The advantage of my scheme is that it only calculates pixels that matter and relies on the iPhone's optimized drawing system to do the calculations. It can generate a large number of layers but the system is designed to handle that (and the number of layers is fixed by your trail-length and frame-rate). The code should also be very simple."
38,A,Equivalent of ClientRectange in ASP.NET page.control I am using a 3rd party library for some GDI+ drawing capabilities where the method to actually implement the drawing takes a Graphics object and a Rectangle object as parameters. In the Paint event of my WinForms application I can then execute: externalLibrary.Draw(e.Graphics ClientRectangle); When implementing the same thing in ASP.NET I can create a new Graphics object but is there an equivalent of ClientRectangle for a Page or WebUserControl? Drawing in ASP.NET is different than drawing in a Windows Forms application.The ClientRectangle is a property used in Windows Forms applications and not in ASP.NET applications. Drawing in ASP.NET is a two step procedure: you have to use the GDI+ or any library that uses GDI+ and dynamically generate an image (through for example an .aspx file). Then you can link that image using a HTML < img > tag in an .aspx file (the place that will render your drawing). The dimensions of the generated image play the role of the ClientRectangle property like when drawing in a Windows Forms application.
39,A,Fast Pixel Precision 2D Drawing API for Graphics App? I woud like to create a cross-platform drawing program. The one requirement for writing my app is that I have pixel level precision over the canvas. For instance I want to write my own line drawing algorithm rather than rely on someone elses. I do not want any form of anti-aliasing (again pixel level control is required.) I would like the users interactions on the screen to be quick and responsive (pending my ability to write fast algorithms.) Ideally I would like to write this in Python or perhaps Java as a second choice. The ability to easily make the final app cross-platform is a must. I will submit to different API's on different OS'es if necessary as long as I can write an abstraction layer around them. Any ideas? addendum: I need the ability to draw on-screen. Drawing out to a file I've got figured out. The Pyglet library for Python might suit your needs. It lets you use OpenGL a cross-platform graphics API. You can disable anti-aliasing and capture regions of the screen to a buffer or a file. In addition you can use its event handling resource loading and image manipulation systems. You can probably also tie it into PIL (Python Image Library) and definitely Cairo a popular cross-platform vector graphics library. I mention Pyglet instead of pure PyOpenGL because Pyglet handles a lot of ugly OpenGL stuff transparently with no effort on your part. A friend and I are currently working on a drawing program using Pyglet. There are a few quirks - for example OpenGL is always double buffered on OS X so we have to draw everything twice once for the current frame and again for the other frame since they are flipped whenever the display refreshes. You can look at our current progress in this subversion repository. (Splatterboard.py in trunk is the file you'll want to run.) If you're not up on using svn I would be happy to email you a .zip of the latest source. Feel free to steal code if you look into it.  If language choice is open a Flash file created with Haxe might have a place. Haxe is free and a full dynamic programming language. Then there's the related Neko a virtual machine (like Java's Ruby's Parrot...) to run on Mac Windows and Linux. Being in some ways a new improved form of Flash naturally it can draw stuff. http://haxe.org/  I just this week put together some slides and demo code for doing 2d graphics using OpenGL from python using the library pyglet. You can see my stuff here: http://tartley.com/?p=378 It is very fast (relatively speaking for python) I have managed to get around 1000 independently positioned and oriented objects moving around the screen each with about 50 vertices. It is very portable all the code I have written in this environment works on windows and Linux and mac (and even obscure environments like Pypy) without me ever having to think about it. Update: There are a bunch of newer posts on the same topic too: http://tartley.com/?cat=27 Thanks. Fixed now. I realize this is an old post but the link is broken.  I would recommend wxPython It's beautifully cross platform and you can get per pixel control and if you change your mind about that you can use it with libraries such as pyglet or agg. You can find some useful examples for just what you are trying to do in the docs and demos download.  QT's Canvas an QPainter are very good for this job if you'd like to use C++. and it is cross platform. There is a python binding for QT but I've never used it. As for Java using SWT pixel level manipulation of a canvas is somewhat difficult and slow so I would not recommend it. On the other hand Swing's Canvas is pretty good and responsive. I've never used the AWT option but you probably don't want to go there.
40,A,Alternatives to Love2D for a Lua 2D Graphics Lib I'm looking for alternatives to the Love2D graphics/game library which ideally would support the following: Easy primitive rendering (e.g. points lines 2d polygons) Ability to load and draw images basic text rendering (though something more full-featured would be nice) Do any others exist? Try also Canvas Draw. I'll take a look at that too it seems to come with a standard Lua install. CD works quite well used in combination with IUP to provide the UI framework needed to be a well-behaved Windows GUI application.  There is also the wxWidgets framework which has a Lua binding named wxLua. It is included in the Lua for Windows batteries-included distribution and should be as platform-portable as either Lua or wxWidgets itself.  Try Cairo and LuaCairo. This is great for image making but if i wanted something that drew to a window what would i use? Or would cairo do that too? Cairo's web page says it support X11 and Win32 among several others. That just appears to mean that it works on windows...
41,A,"Java howto paint on a graphic in an order I want In C# I saw already the SpriteBatch class (In the XNA plugin for XBOX games). In that class is it posible to paint in layers. Now I want to do the same in Java because I'm making braid (See my other questions) and I have an ArrayList from all the GameObjects and that list is not made in the correct paintoreder. For exemple: I have a door but the door is painted on the player. Martijn Can you be a little more specific? What API are you using to do the drawing? Why is this question tagged ""C++""? sorry this was a mistake How about sorting the items before rendering? The background items have to be painted first and the foreground ones last. If you have a List and items are Comparable you can use Collections.sort(list); If you need a special order you can implement your own Comparator. All that of course requires the items to hold some info on their z-position. And you shouldn't do this in the paint method. sort items when they're changed ie. added. thanks I will try it and let you know something. It looks a great solution thanks for the idea. It works.  Sorting the list of items should solve your issue but if you do find you need to paint layers you can do it like this: Create a BufferedImage for each layer  BufferedImage[] bi = new BufferedImage[3]; bi[0] = new BufferedImage(w h BufferedImage.TYPE_INT_ARGB); Paint into the buffered images  Graphics2D bg2 = bi[0].createGraphics(); bg2.drawXXX(...); That should all be outside the actual paint method. In the paint or paintComponent method use alpha compositing to assemble the layers  AlphaComposite ac = AlphaComposite.getInstance(AlphaComposite.SRC_OVER 1.0f); g2.setComposite(ac); for (int i = 0; i < bi.length; i++) { g2.drawImage(b[i] 0 0 this); }"
42,A,"Best thing for 3D and raytracing I want to play around with some graphics stuff. Simple animations and things. I want to fool around with raytracing too. I need help finding a library that will help me do these things. I have a few requirements: Must be able to do raytracing Must be for a high level language (python .NET etc.). Please no C/C++ Must have good documentation preferably with examples. Does anyone know of a good library i can use to fool around with? I'm not aware of any libraries that satisfy your request (at least not unless I decide to publish the code for my own tracer...). Writing a tracer isn't actually that hard anyway. I'd strongly recommend getting hold of a copy of ""An Introduction to Ray Tracing"" by Glassner. It goes through the actual math in relatively easy to understand terms and also has a whole section on ""how to write a ray tracer"". In any event a ""library"" isn't all that much use on its own - pretty much every ray tracer has its own internal libraries but they're specific to the tracer. They typically include: a base class to represent 3D objects subclasses of that for each geometric primitive vector and matrix classes (3D and 4D) texturing functions and/or classes light classes of various types (point light spot light etc) For my own tracer I actually used the javax.vecmath packages for #3 above but had to write my own code for #1 and #2 based on the Glassner book. The whole thing is well under 2k lines of code and most of the individual classes are about 40 lines long.  First thing that come to my mind is the popular open source P.O.V Raytracer (www.povray.org). POV scenes are defined entirely with script files and some people made Python code to generate them easily. http://code.activestate.com/recipes/205451/ http://jabas-unblog.blogspot.com/2007/04/easy-procedural-graphics-python-and-pov.html Be aware that POV-RAY is NOT open source http://www.povray.org/povlegal-3.5.html  Have a look at blender.org - it's an open-source 3d project with python scripting capabilities.  I believe there are few people putting together ray-tracers using XNA Game Studio. One example of this with code can be seen over at: Bespoke Software » Ray Tracing - Materials  The well developed raytracers that are open source are Yafray Povray For realtime 3D (it will be language dependant of course) there is JMonkeyEngine (Java) not sure whether that meets your ""high level language"" requirement. You could consider a 3D game scripting language too like GameCore or BlitzBasic"
43,A,"WPF 3-D performance for head-tracking app I’m working on creating a full-screen 3-D app (based on Johnny Lee's Wii head tracking app) with some augmented reality features and it seems that WPF is too slow to render even the simple models I’m using at a reasonable frame rate. I think the problem is that I need to change both the view and projection of the camera on just about every frame because of the nature of the app (it uses a web cam to track your face and uses that data to move the camera around and change its perspective). I've spent a lot of time trying to narrow down the problem and it's definitely related to the graphics and not the speed of the head-tracking API that I'm using. Also I recreated the app in XNA and it seems to work fine there (28 FPS versus 9 in WPF). Finally when I remove the ""walls"" or make the window much smaller (say 800 x 600) WPF's performance greatly improves which makes me think that the bottleneck is the graphics calculations. As a result I need to either find a new graphics back-end to work with or find a way to make WPF much faster for this app. I’m mostly looking at DirectX and XNA and possibly OpenGL. Any recommendations on which of these APIs would be best to use for this app in .NET? Or alternatively any idea what I'm doing wrong in WPF that's slowing things down? I would check out SlimDX a much thinner DirectX wrapper than XNA or WPF The problem with changing the projection on every frame is that API's like XNA/WPF werent designed with this in mind so were optimized to have projection set once in the initialization phase then not again. Why? I can set my view/projection matrices on each frame in XNA without problems. They are just input to the shaders that does that calulations in hardware. Calculating the matrices themselves and updating those shaders is not a bottleneck.  I've not done enough with 3D in WPF to be able to say what could be causing the slowness but I did notice that it's model datastructure isn't very efficient. While this might not be the cause it could be symptomatic of general slowness to the whole pipeline. One thing that does spring to mind is that WPF is rendering the scene in software rather than using the hardware acceleration on the graphics card. The fact that you get better performance (though you don't say how much better) with a smaller window. If you remove any textures from your model do you get better performance too? I don't think it really matters whether you go for Direct3D or OpenGL - virtually all modern cards support them equally well. XNA is the obvious choice if you're sticking with .NET as it's integrated into Visual Studio - even the Express edition. @Jonathan - the texture idea was only a suggestion - it used to be one of the things that could trigger software rendering. There is no ""texture"" per se. I'm just using the SolidColorBrush on all of my models. XNA is probably my first choice at the moment assuming WPF isn't going to cut it. My ownly problem so far is that the way graphics work in XNA seems very odd to me but maybe that's just a learning curve I need to get over.  I would suggest a hybrid choice here: use WPF for what its good at (Windows UI composition etc) and use XNA to render the 3D. There are samples out there that demonstrates combining XNA with WinForms. It should be possible to do the same ""trick"" to render XNA onto surfaces in WPF. Update: there are supposedly issues with using XNA directly with WPF. This thread indicates that using XNA with WinForms and then hosting the WinForms control in WPF is a workaround to these issues. I've not tested this myself though (yet). See my update. This could be a way around but yeah going with SlimDX is now probably the easiest bet. There are currently some issues embedding XNA in WPF it has something to do with conflicting device properties from what I understand. The XNA and WPF folks are aware of the issue but I don't think it's been resolved yet. Using something like SlimDX though shouldn't have the same issues."
44,A,Problem in Drag And Move Forms in .Net I Write a Windows Application By C Sharp. I Use a Picture in Background of my Form (MainForm) And I Use Many Picture in Buttons in This FormAnd also I Use Some Panel And Label with Transparent Background Color. My FormsPanels And Buttons has flicker. I solve this problem by a method in this thread. But Still when other Forms Start over this Formmy Forms hangs when I Drag and Move my Forms over this Form.How can I Solve this Problem to Move And Drags my Forms easily And Speed? Edit:: My Forms Load Data From Access 2007 DataBase file.I Use DatasetsDataGridViews And Other Components to Load And show Data in My Forms. Which method from the other question are you using? @MusiGenesis :----> Hans Passant As I understand flickering is not a problem; but Form *hang*-ing is right? Sounds like you have something inside your form thats eating up the form probably in `OnPaint` on `OnSize` just thinking! Can you share some code? @KMan : No ProblemI Can....But Please See Edit Section of My Question... You just made it less obvious that your form paints very slowly by using the techniques shown in my answer. The tricks don't speed it up they merely make the ugliness less visible. But they fall flat when you have to paint your form from scratch which happens when you move another window across it. The painting cannot keep up with the barrage of paint requests that are generated each time the overlapping form moves by one or more pixels. An instant fix is to upgrade your operating system to Vista or Windows 7 windows don't overlap anymore with Aero enabled. Thank you.But I want Run this Program in Windows Xp.Not Solution Exist for this Problem? Does it really matter what you want to run? What your customer runs ought to count. Surely they are not using a 9 year old operating system?
45,A,"Direct3D Line thickness How do I change the thickness of lines when drawing line lists using Direct3D? This post says that line width is not supported and goes on to provide a workaround. Other options? While we are on this topic do shaders allow one to draw lines with dash patterns? Line thickness is not only not supported by Direct3D but neither is it supported by any currently existing GPU. There is no GPU that I am aware of that can even draw proper lines at all (they all fake lines by using degenerate polygons - that is the second and third vertices are at the same position so the triangle essentially collapses to a single line). While it is possible to set a line width in OpenGL the OpenGL driver creates extruded quads (which are not supported by and current GPU either and emulated using two triangles for each quad) to draw the ""lines"" on the screen. So there's probably no way around creating extruded quads for that purpose. There are several ways to achieve that as Stringer Bell explained in his answer. The easiest way that works for me is to create a vertex buffer that contains each vertex twice with normals pointing ""right"" or ""left"" depending on whether they are the right or left edge of the line. Then a very simple vertex shader can perform the extrusion (by changing the vertex position by a multiple of its normal vector). This way you can quickly change the line width without having to recalculate your geometry on the CPU. This can be useful if you want to adjust the line width depending on the object's size or distance. This is an interesting solution. Did you achieved to get same results than with a glLineWidth call (on a per pixel basis)? I didn't check it per-pixel but for untextured antialiased lines the results should be sufficiently similar. Using a texture (to emulate OpenGL's stipple patterns for example) is quite sure to get you into a world of pain with nonlinear interpolation of texture coordinates (which NVidia Quadros handle suprisingly bad even compared to the mostly identical Geforce part). The shader needs to perform a lot of additional work to get the texcoords right. You can try that yourself just use some sort of ""line length in screen pixels"" for texture coordinates and draw the line(s) with a texture that has a stipple pattern on it. You'll be surprised how uneven the pattern is. At least I was. ;-)  Regarding your dash pattern (stippling) question your best bet is probably to render the line as a thin quad and apply a texture to it in the pixel shader where the texture contains your dash pattern. You will need to set the sampler address mode to wrap something like: SamplerState wrapTriLinear { AddressU = WRAP; AddressV = WRAP; }  You can use a geometry shader that will take as an input a segment of your line and output a quad (a triangle strip made of two triangles) so that the width of the quad is constant in screen space and matches the desire line thickness. It works perfectly well (for having implemented it in a CAD 3D engine). If geometry shader is not an option a workaround could be to use a vertex shader but it will require some re-work of your VB. Keep in mind that the VS must then have knowledge of the line segment in its whole so you will end up storing p and p+1 for each of your VB elements plus the cost of duplication of indices/vertices (depending the topology used and if you render your line as an indexed primitive or not). If performance is not an issue doing the expand on the CPU is maybe the way to go. EDIT: About dash patterns: you can use a geometry shader too in order to emulate glLineStipple behavior. If you have a GL_LINES topology that is to say isolated lines the pattern restarts at each new line segment. So you just have to compute in the geometry shader the screen-space horizontal start (or vertical start depending the orientation) of your line segment and pass this extra infos to the pixel shader. The pixel shader will then be responsible of discarding fragments according to the factor and pattern values (DirectX 10/11 Integer and Bitwise instructions make it easy). Again this works well and you can combine it with emulated width lines (with the first technique above mentioned). Now if you have GL_LINE_STRIP topology the pattern restarts at each new primitive (so for each new draw call). The situation becomes a bit more complicated since you need to have the knowledge of the number of pixels that have been rendered before and this for each line segment. You can achieve that with rendering your line strip in a temporary VB using DirectX 10 stream-out functionality (each element of this VB corresponds to the screen-space length of each segment). Then you have to do a Parallel Prefix Sum (aka Scan) of this VB (for accumulating each line segment length values). Lastly you render your line strip like for GL_LINES but use this extra Scan VB informations in the PS.  ""The ID3DXLine interface implements line drawing using textured triangles."" this is not supported in directx11..."
46,A,Rotation matrix for a bicycle Assume you have a flat plane with a bicycle resting on it. As the bicycle enters a turn it leans into the turn with angle theta. At the same time the bike frame points in the same direction as the bike velocity. Thus given the bike velocity vector v (assumed to be in the XZ plane) and the lean angle theta how can you find the rotation matrix for the bike? http://mathoverflow.net/ @Ignacio: mathoverflow is for graduate-level mathematics not things like this The bike will fall over. You need a handlebar angle relative to the bird's eye view as well as a lean angle relative to the driver behind bicyclist's view. Use a quaternion. Axis is bike velocity and 'lean into turn' is axis angle. Then convert to rotation matrix
47,A,What are some of the better open source raytracers? I'm just wondering what options are out there as far as open source ray tracing software. eg. Yafaray POV-Ray what else? Any opinions on their relative merits would also be appreciated. Are you asking about software or about their source? I personally like this one: http://www.ioccc.org/2004/gavare.c I'm more interested in actually using the software. Would like to see some more in-depth answers. POV-Ray is in my opinion the best open source general purpose raytracer you will find. Unless you're looking for a specific feature unavailable in POV-Ray I can't see any reason why you wouldn't use that.  http://mantawiki.sci.utah.edu/manta/Main_Page: It's interactive! http://www.pbrt.org/: It has a book!
48,A,Painted Border Runs In Certain Situations In my tool I use a a panel to change pages. Each page has it's own panel and when I change a page I send the panel with the controls. On the panel I use as the canvas I have the following paint event:  private void panelContent_Paint(object sender PaintEventArgs e) { e.Graphics.CompositingQuality = CompositingQuality.HighQuality; e.Graphics.SmoothingMode = SmoothingMode.HighQuality; // Paints a border around the panel to match the treeview control e.Graphics.DrawRectangle(Pens.CornflowerBlue e.ClipRectangle.Left e.ClipRectangle.Top e.ClipRectangle.Width - 1 e.ClipRectangle.Height - 1); e.Graphics.Flush(); base.OnPaint(e); } This method basically draws a nice border around the panel so it look better. For some reason when I move a another form above this panel the lines that make up the border start to run a little. Occasionally small lines will be drawn from the border too. The problem only happens for a few seconds before the entire panel redraws again. Is there anything I can do to prevent this from happening? ClipRectangle tells you what part of the control needs to be repainted. If you're moving something over it this is probably going to be the intersection of your object and the one being moved. You can use this information to more efficiently repaint your control. You probably want to draw the rectangle from (0 0) to (panelContent.Width-1 panelContent.Height-1). Works like a charm: private void panelContent_Paint(object sender PaintEventArgs e) { e.Graphics.CompositingQuality = CompositingQuality.HighQuality; e.Graphics.SmoothingMode = SmoothingMode.HighQuality; e.Graphics.DrawRectangle(Pens.CornflowerBlue 0 0 panelContent.Width - 1 panelContent.Height - 1); e.Graphics.Flush(); base.OnPaint(e); } Thank you!
49,A,"d3dxcompileshaderfromfile versus d3dxcreatetexturefromfile I can use D3DXCreateTextureFromFile fine but was wondering what D3DXCompileTextureFromFile is and its purpouses it seems to be basically the same thing? However I""m unsure how to use it properly... You say D3DXCompile ""Texture"" FromFile in your post body I will assume you mean D3DXCompileShaderFromFile. That function compiles shader code which is a variation of C that is compiled specifically to run on the GPU per vertex or per pixel. D3DXCreateTextureFromFile creates a texture which can be used by a shader for any number of purposes but is usually used for texturing which is described quite well in this article."
50,A,Displacement map in .Net I have built a Flex application that composes image resources color layers blurs etc to generate images (99% of the drink images in www.absolutdrinks.com are generated by this app). One of the effects used by the app is Flash's ability to apply displacement map filter in which x- and y- offset for an image is defined by a 2D image (sy x offset in the red channel and y offset in the blue channel). This is used to get the bulge of garnishes placed in the liquid of the drink. I am now looking for a way to do this image generation server side (and possibly in a Silverlight app). I can see ways to reproduce all features of the Image generation app except for the displacement map filter. Is there any way to do this via the controls in the Windows.Media namespace? If not: are there any other ways? Unfortunately there is no DisplacementMap filters neither in Silverlight nor in WPF. There are two ways you could follow. Create custom pixel shader effect and implement displacement algorithm in it. Pixel shaders are supported both by Silverligth 3.0+ and WPF. Use WriteableBitmap to get access to the pixels and again implement the algorithm.  You can use a Pixel Shader with normal maps to accomplish this. Bump mapping in Silverlight 3
51,A,Direct3D Mesh with combination of lines and triangles I need to create a Direct3D mesh consisting of some vertices (generated at run-time) which I need rendered as a combination of LineList and TriangleList. i.e. some vertices are rendered as a LineList and some of them as a TriangleList. How can I create this Direct3D mesh? Well create a vertex buffer and put all the verts in it. Next create an index buffer. Put the line list indices in there. Next add the triangle list indices to the index buffer. Finally .. render something like as follows: pDevice->DrawIndexedPrimitive( D3DPT_LINELIST 0 0 numLineIndices 0 numLineIndices / 2 ); pDevice->DrawIndexedPrimitive( D3DPT_TRIANGLELIST 0 0 numTriangleIndices 0 numTriangleIndices / 3 ); Is there a way to combine all of the above into one single mesh object? If you mean a D3DXMesh .. no .. D3DXMeshes handle only triangle lists. If you are talking any sort of mesh then sure. It depends how you define your mesh structure.
52,A,"graphics don't draw when loop condition is dates. c# winForms so i got this piece of code. (currPosX is defined earlier) while (earliestDate < DateTime.Today) { currPosX = currPosX + 5; e.Graphics.DrawLine(Pens.Black currPosX 0 currPosX 10); earliestDate = earliestDate.AddDays(1); } the graphics don't draw. it's really weird since this only happens when the condition statement is a date comparison. I debugged and it does go in the loop and the values are messed with (currPosX for example). But no display. one more weirdness if I add a MessageBox.Show(""blabla"") in the loop the message box pops up and graphics are drawn. what's going on here? EDIT: just to remind you guys when it's a non-datetime condition it works. meaning that this code works. it does display a series of lines int i = 0; while(i < 10) { currPosX = currPosX + 5; e.Graphics.DrawLine(Pens.Black currPosX 0 currPosX 10); i++; } DateTime is not an accurate measurement the message box's delay helps though. Use .NET's PreformanceCounter instead (measure time passed from before entering loop).  As your tests indicate the issue has nothing to do with comparing DateTime's. Since your code is entering the loop and you know the painting is being done something else must be going on. We'll probably need to see more code to identify the problem Trying to step through painting code is useless. The fact that the debugger and application window trade focus will completely screw things up. You are better off using tracepoints not breakpoints. But here's some possibilities: Are you sure your coordinates are in the control's visible client area? Are you doing the above in the control's Paint event? Are you remembering to invalidate the control using the Invalidate or Refresh method? Are you painting on the UI thread? Do you have any non-standard control styles set? UPDATE In response to your edit: Your problem is that earliestDate will keep creeping forward because you are modifying it in your Paint event and the value will persist between Paint events. Paint events occur repeatedly every time the control is invalidated. You have two options. Copy earliestDate to a local variable in the Paint event and use that Reset earliestDate back to its starting value at the end of the event. I suggest option 1. I used your #1 solution and it works. thx yes visible area. yes in the Paint event. invalidate or refresh doesn't work nothing happens. I don't know what UI thread is. I don't know what control style is. check my edit I updated my answer in response to your edits.  I did a simple test project that just has a form with no controls on it (the code is below). As you can see I added a little code in the constructor to initialize the earliest data so that the while loop in the Paint event will be executed once. Also hard coded the currPos value. If you run this it will draw a vertical line as expected. But if you do anything that invalidates the Graphics (for example minimize and restore the form) it will not redraw he graphics. So it draws it once but will not draw again for 24 hours! public partial class Form1 : Form { DateTime earliestDate; public Form1() { earliestDate = DateTime.Now; earliestDate = earliestDate.AddDays(-1); InitializeComponent(); } private void Form1_Paint(object sender PaintEventArgs e) { while (earliestDate < DateTime.Today) { float currPosX = 0; currPosX = currPosX + 5; e.Graphics.DrawLine(Pens.Black currPosX 0 currPosX + 5 10); earliestDate = earliestDate.AddDays(1); } } } thx but your code is not good for me because i have 2 panels. the top panel has a scroll bar on it and when you scroll it the bottom panel(which contains the graphics) must follow. so unless I'm wrong the control needs to redraw every time I scroll the top panel You are right that you need to redraw every time. This code wasn't intended to solve the problem - it was just showed a test that confirms that you can't skip the redrawing. +1 Brilliant! That sounds like it would be the problem."
53,A,"Point to point linear gradient? I want to make an application that can generate point to point gradient (like Photoshop does). I'm familiar with how to generate an up to down gradient but not point to point. How is this conceptually done. Thanks Perhaps take a look in the GIMP source code. Although you probably won't find a description of the concept it might help. This routine calls your functor for each point in the image with the appropriate value. Use the value as an index into a colour lookup table and you'll have your blend. Functor has this prototype: void (int x int y int value). /** Produce a linear gradient. This implementation uses float values. */ struct Linearf { template <class Functor> void operator() ( int rows int cols int x0 int y0 int x1 int y1 int const scale Functor f) { if (x0 != x1) { // 1/m' float const m = float (y1 - y0) / (x0 - x1); for (int y = 0; y < rows; ++y) { float const p0 = m * (y - y0) + x0; float const p1 = m * (y - y1) + x1; float const d = (scale + 0.9999f) / (p1 - p0); for (int x = 0; x < cols; ++x) { if (x < p0) { f (x y 0); } else if (x >= p1) { f (x y scale); } else { f (x y d * (x - p0)); } } } } else { // Special case for horizontal lines. } }  I can't say if this is exactly how Photoshop does it or that it's the most optimal way of doing it but this should be the basic principle. Think of the two points as defining a vector. You can find a normal vector for this which will be perpendicular to the original vector (since that's the definition of a normal vector). For each discrete point (pixel) on the line calculate the gradient color as you would for an up-down (or left-right) gradient of the same length as your vector. Then draw a line of the selected color such that it passes through the currently chosen point and is parallel with the normal vector. It's actually very similar to the approach you'd use for an up-down gradient except it's rotated. I think you are right that this is not exactly how Photoshop does it and it is not the most optimal way but you are close. What you really want to do is to scan the filled image left to right top to bottom. Since a cut through a linear gradient is linear for each scan line you can compute the start and end color and then interpolate between them quickly (preferably using a Bresenham algorithm). You are doing same amount of calculation as if you were drawing slanted lines but you are accessing the memory in cache-friendly way.  Building on Michael Madsen's answer- For every point in the region you're filling compute the closest point on the line segment. (You'll have to google that but there are plenty of examples.) At some point in the computation of that closest point there's a value computed that ranges from 0 at the start point to 1 at the end point. Plug that into whatever your gradient function is. The overview of the algorithm is basically... pc = # the point you are coloring now p0 = # start point p1 = # end point v = p1 - p0 d = Length(v) v = Normalize(v) # or Scale(v 1/d) v0 = pc - p0 t = Dot(v0 v) t = Clamp(t/d 0 1) color = (start_color * t) + (end_color * (1 - t)) You can actually lose the make t/d just t if you scale v by 1/d^2 instead of 1/d. But anyway... I think this'll get you there. Perhaps obviously the first half is ""static"" so you only need to loop over the last four lines. I'm sure that drawing the perpendicular lines will be way faster fwiw. :) Drawing perpendicular lines *will* most likely be faster but is also likely to introduce a lot of aliasing if the gradient is not axis-aligned. Working point by point will most likely (in theory) give a better looking result in such cases. Too slow you are not taking advantage of the ""linear"" in the linear gradient. Compute start and end color for each scan line interpolate. Hi I'm not sure if you (dash-tom-bang) will see this but I was wondering if you knew a similar algorithm to this but for gradients running around a radius (radial gradient) Thanks! Sure- just use distance to point. At d=0 use the ""start"" color (set t=0) at d>=endDist use the ""end"" (t=1) color and in between use d/endDist for your t value in the last line above. 1) Drawing perpendicular lines will be much slower and the results will look terrible 2) Calculating the starting and ending colours for each line and then interpolating will not produce the correct looking results since the fractional slope information is discarded 3) Using a distance function will be slow."
54,A,"How can I get the dimensions of a drawn string without padding? I'm trying to create an image with some text on it and I want the image's size to match the size of the rendered text. When I use System.Windows.Forms.TextRenderer.MeasureText(...) to measure the text I get dimensions that include font padding. When the text is rendered it seems to use the same padding. Is there any way to determine the size of a string of text and then render it without any padding? This is the code I've tried: // Measure the text dimensions var text = ""Hello World""; var fontFamily = new Font(""Arial"" 30 FontStyle.Regular); var textColor = new SolidBrush(Color.Black); Size textSize = TextRenderer.MeasureText(text fontFamily Size.Empty TextFormatFlags.NoPadding); // Render the text with the given dimensions Bitmap bmp = new Bitmap(textSize.Width textSize.Height); Graphics g = Graphics.FromImage(bmp); g.TextRenderingHint = TextRenderingHint.AntiAliasGridFit; g.DrawString(text fontFamily textColor new PointF(0 0)); bmp.Save(""output.png"" ImageFormat.Png); This is what currently gets rendered out: This is what I want to render: I also looked into Graphics.MeasureString(...) but that can only be used on an existing Graphics object. I want to know the size before creating the image. Also calling Graphics.MeasureString(...) returns the same dimensions so it doesn't help me any. I think it is because the font height is 46 for 30point font. This is so you can have special uppercase characters (ÉÖÄÅ) and lover case characters(gqp). The leading and trailing blank spaces probably also have some thing to do with this. That was it! http://i.imgur.com/Py4zK.png"
55,A,"Draw text on a .net Control with negative color of the background - blend I'd like to draw a progress bar with percentage in its center but I'd like the text to have negative (contrasted) color of the background. So the part of the text that is over the filled portion of ProgressBar would be white and the part over unfilled portion would be white. I could do this simply by ""cheating"" painting the black part of the text first painting the progress rectangle (it would cover the part to be hidden) painting the white text over the progress rectangle only (clipping it) The performance impact of painting the text twice is negligible in this application but I'm interested if there is some simple way to do it in two steps only (like having the progress bar somehow invert the already painted text) with blending. You can use XOR to calculate the negative (inverted) color; Public Function InvertColor(color As Long) As Long Return (color Xor &HFFFFFF) End Function That is no problem I know the base and inverted color but I'm asking about how to do ""XOR"" blending in graphics  What you're doing is fine. Old tricks like SetROP2() don't work anymore with text getting anti-aliased especially with ClearType rendering. Getting the aliasing pixels with the wrong color is very noticeable. Graphics.CompositingMode accordingly doesn't support the effects anymore."
56,A,"Button text disappearing with 4Gb Ram and IBM Java 1.5 We have a very strange error occurring at a developer site which we are unable to replicate ourselves. A developer in Poland has recently upgraded his Windows XP Service Pack 3 machine to 4Gb of Ram When he did so he started experiencing graphical errors in java programs using IBM JDK 1.5 This errors only occur in IBM JDK 1.5 and not in any other version. The problem manifests itself when you create a button or control on a form and move the mouse over it. We have a test program import java.awt.FlowLayout; import javax.swing.JButton; import javax.swing.JFrame; public class GraphicTest { public static void main(String args[]) { JFrame frame = new JFrame(""GraphicTest""); frame.getContentPane().setLayout(new FlowLayout()); frame.setSize(200 200); JButton button = new JButton(""Test button""); button.setVisible(true); frame.getContentPane().add(button); frame.setVisible(true); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); } } which shows the problem straight away. However the problem doesn't arise on my own machine when I upgrade the same windows version to 4Gb of Ram. Has anyone else ever seen an issue like this? Just to clarify this a bit this issue only happens with IBM JDK 1.5 and only happens when we have 4Gb of Ram. It doesn't happen on any other version of the JDKs and if we reduce the amount of memory to 3 Gb the problem disappears. I can say with 99.99% confidence that it's not the RAM. Have you really narrowed it down to make sure that this *only* happens in Java and *only* with the IBM JDK and *only* with 1.5? An easy way to test the assumption that it's the RAM (which seems unlikely) would be to just remove the RAM and try it again. My guess is that it's something more subtle that changed around the same time. I had exactly the same thing on an IBM box with 3.24 G of memory: All swing apps would display - but the text (on menus forms buttons - everywhere it seems) were blank. The same swing program running on Sun JDK worked no problem. I reduced the Hardware Acceleration from 'Full' to 'None' ( on the Plug and Play Monitor settings off 'advanced' in display) - using an Intel(r) 82865G graphics card. Now swing apps work just fine it seems. Good spot...  Try reducing the hardware optimization in Windows' graphics drivers (accessible through the extended display control panel). If the machine in question has an onboard graphics adapter that uses a part of the main memory then upgrading RAM might expose problems in the driver (or the RAM may even be faulty). Turns out this was exactly the problem both developers had the same graphics chipset Intel G31/G33 Chipset and with hardware acceleration turned down the graphic problems disappeared. It may be possible to fix the problem by updating the graphics drivers. We tried that with the latest graphics drivers and still the error occurs. I guess they just need to either keep the foot off the accelerator or get a new card. Thanks for all the help.  The first obvious thing to always say is: Confine usage of Swing components to the AWT Event Dispatch Thread (EDT). public class GraphicTest { public static void main(final String args[]) { java.awt.EventQueue.invokeLater(new Runnable() { public void run() { runEDT(); } }); } private static void runEDT() { assert java.awt.EventQueue.isDispatchThread(); JFrame frame = new JFrame(""GraphicTest""); ... I don't know why memory size would be important. Perhaps it affects the timings in someway. Perhaps the JVM decides it is running on a server-class machine and runs with more aggressive optimisation. In other words you think this might be fixed by extracting a createAndShowGui() method and doing an invokeLater()?  Just to rule out the hardware failure hypothesis: have the developer test his RAM. Memtest86 will do this. We have had the developer use 2 different machines both machines using the memory from the other one and on both occassions they see the same problem but we cant reproduce it on machines in our own office."
57,A,"OpenGL glDrawPixels on dynamic 3D arrays How do you draw the following dynamic 3D array with OpenGL glDrawPixels()? You can find the documentation here: http://opengl.org/documentation/specs/man_pages/hardcopy/GL/html/gl/drawpixels.html float ***array3d; void InitScreenArray() { int i j; int screenX = scene.camera.vres; int screenY = scene.camera.hres; array3d = (float ***)malloc(sizeof(float **) * screenX); for (i = 0 ; i < screenX; i++) { array3d[i] = (float **)malloc(sizeof(float *) * screenY); for (j = 0; j < screenY; j++) array3d[i][j] = (float *)malloc(sizeof(float) * /*Z_SIZE*/ 3); } } I can use only the following header files: #include <math.h> #include <stdlib.h> #include <windows.h> #include <GL/gl.h> #include <GL/glu.h> #include <GL/glut.h> Is this for homework? Yes but it's just a mall part I can't solve. The whole homework will be a ray tracer and this structure stores the screen pixels. Uh ... Since you're allocating each single pixel with a separate malloc() you will have to draw each pixel with a separate call to glDrawPixels() too. This is (obviously) insane; the idea of bitmapped graphics is that the pixels are stored in an adjacent compact format so that it is quick and fast (O(1)) to move from one pixel to another. This looks very confused to me. A more sensible approach would be to allocate the ""3D array"" (which is often referred to as a 2D array of pixels where each pixel happens to consist of a red green and blue component) with a single call to malloc() like so (in C): float *array3d; array3d = malloc(scene.camera.hres * scene.camera.vres * 3 * sizeof *array3d);  wouldn't you want to use glTexImage2D() instead: see here  Thanks unwind. I got the same advice on gamedev.net so I have implemented the following algorithm: typedef struct { GLfloat R G B; } color_t; color_t *array1d; void InitScreenArray() { long screenX = scene.camera.vres; long screenY = scene.camera.hres; array1d = (color_t *)malloc(screenX * screenY * sizeof(color_t)); } void SetScreenColor(int x int y float red float green float blue) { int screenX = scene.camera.vres; int screenY = scene.camera.hres; array1d[x + y*screenY].R = red; array1d[x + y*screenY].G = green; array1d[x + y*screenY].B = blue; } void onDisplay( ) { glClearColor(0.1f 0.2f 0.3f 1.0f); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glRasterPos2i(00); glDrawPixels(scene.camera.hres scene.camera.vres GL_RGB GL_FLOAT array1d); glFinish(); glutSwapBuffers(); } My application doesn't work yet (nothing appears on screen) but I think it's my fault and this code will work."
58,A,"What is the best way to scale images in Java? I have a web application written in Java (Spring Hibernate/JPA Struts2) where users can upload images and store them in the file system. I would like to scale those images so that they are of a consistent size for display on the site. What libraries or built in functions will offer the best results? I will consider the following criteria in making my decision (in this order): Free/Open Source (essential) Easy to implement Quality of results Performance Size of executable Look into also to java-image-scaling library. It created better quality images that ImageIO.  Have a look at the Java Image I/O API to read/write the image. Then use AffineTransform to resize. Also here's a complete example using java.awt.Image.  The best tool for image editing is ImageMagick and it is open source. There are two interfaces for the Java Language: JMagick which uses JNI interface to ImageMagick and im4java what is a command line interface for ImageMagick  I would really recommend giving imgscalr a look. It is released under an Apache 2 license hosted on GitHub been deployed in a handful of web applications already has a very simple but pedantically documented API has code that works around 2 major image bugs in the JDK for you transparently that you'll only ever notice if you suddenly start getting ""black"" images after a scale operation or horrible-looking results gives you the best possible looking results available in Java is available via Maven as well as a ZIP and is just a single class. Basic use looks like this: BufferedImage img = ImageIO.read(...); // load image BufferedImage scaledImg = Scalr.resize(img 320); This is the simplest call where the library will make a best-guess at the quality honor your image proportions and fit the result within a 320x320 bounding box. NOTE the bounding box is just the maximum W/H used since your image proportions are honored the resulting image would still honor that say 320x200. If you want to override the automatic mode and force it to give you the best-looking result and even apply a very mild anti-alias filter to the result so it looks even better (especially good for thumbnails) that call would look like: BufferedImage img = ImageIO.read(...); // load image BufferedImage scaledImg = Scalr.resize(img Method.QUALITY 150 100 Scalr.OP_ANTIALIAS); These are all just examples the API is broad and covers everything from super-simple use cases to very specialized. You can even pass in your own BufferedImageOps to be applied to the image (and the library automatically fixes the 6-year BufferedImageOp JDK bug for you!) There is a lot more to scaling images in Java successfully that the library does for you for example always keeping the image in one of the best supported RGB or ARGB image types while operating on it. Under the covers the Java2D image processing pipeline falls back to an inferior software pipeline if the image type used for any image operations is poorly supported. If all that sounded like a lot of headache it sort of is... that's why I wrote the library and open sourced it so folks could just resize their images and move on with their lives without needing to worry about it. Hope that helps.  I tried imgscalr comparing to standard Java 1.6 and I cannot say it is better. What I've tried is BufferedImage bufferedScaled = Scalr.resize(sourceImage Method.QUALITY 8000 height); and  Image scaled = sourceImage.getScaledInstance(-1 height Image.SCALE_SMOOTH); BufferedImage bufferedScaled = new BufferedImage(scaled.getWidth(null) scaled.getHeight(null) BufferedImage.TYPE_INT_RGB); bufferedScaled.getGraphics().drawImage(scaled 0 0 null); some 5 minute testing by my eye got impression that second thing (pure Java 1.6) produces better results."
59,A,"BlackBerry - Exception when creating a graphics object from a bitmap I am making the following call in my blackberry application (API ver 4.5)... public void annotate(String msg EncodedImage ei) { Bitmap bitmap = ei.getBitmap(); Graphics g = new Graphics(bitmap); g.drawText(msg00); } And I keep getting an IllegalArgumentException when I instantiate the Graphics object. Looking at the documentation for Graphics is confusing as it leaves many things unstated. What does it mean by 'default type of the device'? How do you know if the type of 'bitmap' is not supported? Does this mean that there are different types of bitmaps? Can different encodedImages generate different types of bitmaps? Is there another way to add my string to the associated encoded image? public Graphics(Bitmap bitmap) Constructs a Graphics object for drawing to a bitmap. Parameters: bitmap - Bitmap to draw into. Must be Bitmap.COLUMNWISE_MONOCHROME or the default type of the device. Throws: IllegalArgumentException - If the type of 'bitmap' is not supported or the bitmap is readonly. I'd imagine that the default type depends on the graphics chip and hardware. (If you have a monochrome screen the default would probably be different than if you had a color one.) Bitmap has a static method getDefaultType() which ""Queries the default Bitmap type for the device"". There's also a non-static method getType(). It seems to be telling you the rule is that for the code above to work then either: bitmap.getType() == Bitmap.getDefaultType() ...or... bitmap.getType() == COLUMNWISE_MONOCHROME And presumably neither of these conditions are true. You can do a sanity check on that and maybe print out the result of getDefaultType() so you know what your target is. Looks like you'll have to convert the bitmap or get it from somewhere else.  The Graphics object isn't normally constructed explicitly. Rather you are given an instance of it in the paint() method if you've overridden it. I suspect what you want to do is create a subclass of BitmapField and override the paint() method to include your code for drawing text on the bitmap.  Are you sure that your Bitmap is mutable? You can't create Graphics objects from immutable Bitmaps. That is one cause of an IllegalArgumentException. You can set the decode mode for your EncodedImage (EncodeImage.setDecodeMode). There are different modes that allow you to specify whether the file is native or readonly...along with other modes that can be combined. The size of the bitmap might be another IllegalArgumentException. Of course this is relevant to the target device. The size seems to be my issue. once I cut the image in size the error goes away. Anyone know of any documentation on image size and blackberry device?"
60,A,"java/swing: converting a text string to a Shape I want to convert some arbitrary text to a Shape (java.awt.Shape) and then stroke/fill the Shape to draw it. How can I do this? Not sure what you mean here. Can you elaborate? Do you want the text to become the shapes of the characters in the text? Use the TextLayout class (see the getOutline() method). Theres an example here +1: see my comment on the answer from @Sawas both our answers are correct apparently... :)  If I understood you correctly this is not to address your exact answer but it's a start... //Rough pseudo code import java.awt.Color; import java.awt.Font; import java.awt.FontMetrics; import java.awt.Graphics2D; import java.awt.TexturePaint; import java.awt.geom.AffineTransform; import java.awt.geom.Rectangle2D; import java.awt.image.BufferedImage; BufferedImage image = new BufferedImage(width height BufferedImage.TYPE_INT_RGB); Graphics2D graphics = (Graphics2D)image.getGraphics(); //Paint with texturing brush Rectangle2D rect2D = new Rectangle2D.Double(0 0 width height); graphics.setPaint(new TexturePaint(image rect2D)); graphics.fill(rect2D); //Draw text graphics.drawString(""my text goes here"" xPos yPos); In Summary Create a BufferedImage object of width and height and ImageType. Get the image's Graphics object. Paint that graphics like you please (i.e. create a rectangle circle text etc.) Write that image to a stream (file ServletRequest etc.) ?? I appreciate the help but my question specifically asked how to convert text to a Shape as in `java.awt.Shape`. ok I just saw the edited question now. It wasn't clear earlier. Look also @Holograham's comment below your question. Initially we were unclear about your question. I suggest that you become more discreet with your question then @Jason S. We are trying to help but we cannot read what's in your mind. Thanks but I want to stay in vector graphics  Hm I did not know the answer to this but after a bit tweaking and poking around in with Eclipse content assist i found this which seems to be what you need: EDIT: i changed to code to change the way the string is displayed which is the reason you asked what you asked :) Try it. It renders the string with red color and a dashed outline import java.awt.BasicStroke; import java.awt.Color; import java.awt.Component; import java.awt.Dimension; import java.awt.Font; import java.awt.Graphics; import java.awt.Graphics2D; import java.awt.RenderingHints; import java.awt.Shape; import java.awt.font.GlyphVector; import javax.swing.JFrame; import javax.swing.JPanel; public class Test extends JPanel{ private Shape s; public Test() { Font f = getFont().deriveFont(Font.BOLD 70); GlyphVector v = f.createGlyphVector(getFontMetrics(f).getFontRenderContext() ""Hello""); s = v.getOutline(); setPreferredSize(new Dimension(300300)); } @Override protected void paintComponent(Graphics g) { super.paintComponent(g); Graphics2D g2 = (Graphics2D)g.create(); g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON); g2.translate(100 150); g2.rotate(0.4); g2.setPaint(Color.red); g2.fill(s); g2.setPaint(Color.black); g2.setStroke(new BasicStroke(3 BasicStroke.CAP_BUTT BasicStroke.JOIN_ROUND 1 new float[]{10.4f1.5f} 0)); g2.draw(s); } public static void main(String[] args) { JFrame f = new JFrame(""Test""); Component c = new Test(); f.getContentPane().add(c); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); f.pack(); f.setVisible(true); } } Also note that you can get the individual characters from the string by calling: getGlyphOutline(glyphIndex) Great -- I'm accepting this because it's a complete & understandable example. I'm puzzled because it looks like there are two ways to do this. The GlyphVector approach that you used and the TextLayout approach that @objects answered with. Not sure what the advantages/disadvantages are of each.... in addition to a font and a text string GlyphVector() seems to just take a FontRenderContext but TextLayout takes a FontRenderContext and an AffineTransform. indeed... It would be interesting to hear from a Sun employee (or from someone experienced in the matter) on the differences. It appears that TextLayout is used for many text glyph related operations such as caret shapes switching characters... I initially thought that one of the two must be a new addition to the API but from what i can see both classes existed in the 1.4 API (for me and many that's the first version worthy enough to be called Java :P )."
61,A,.NET programmable graphics matching? Does anyone know of a .NET programmable/usable API for reading an image file and comparing it to an existing set of images? e.g. I have three pictures of the letters A B and C. I then copy the picture of A and modify it so that it is flipped 180 degrees. I'd like to be able to have a piece of software that detects that it is a match to the existing letter A. I appreciate any help! EDIT: Replace the letters A B and C with an image of an apple and orange and a banana. This isn't so much about recognizing alphanumeric characters as it is comparing shapes/images. EDIT #2: Imagine it as a way to detect the result of rolled dice. Imagine a standard pipped six-sided die. When you roll a six there are six dots. It could land any way but what I'd like to try to do is have a camera take a picture of the die and compare it with control images to detect the value. What you seem to be looking into (no pun intended) is part of the computer vision research programme. What exactly characterizes your target images? Are you only looking for stencil matches of exactly the same size but different orientations? Slight displacements cause significantly different pixel patterns for fine and slanted features. Should rescalings be recognized? What about slanted perspectives? Your edit suggests you are actually into much more difficult areas: full image recognition requires full AI. From what perspectives would you expect to recognize a banana under what lighting conditions and what kinds of bananas -- green ripe slightly squished from having sat on it... I hope you get my point! Don't get me wrong: this is fun stuff but requires heavy artillery. What libraries you may find will help you with the heavy linear algebra and statistics lifting but you need to know a lot to apply those. For more light-hearted reading (comparatively!) my introduction to the area came with Hofstadter's Gödel Escher Bach and his Metamagical themas on recognizing letter shapes. That got me interested in typography too: I never knew there were so many ways of drawing a lower-case 'a'! Imagine it as a way to detect the result of rolled dice. Imagine a standard pipped six-sided die. When you roll a six there are six dots. It could land any way but what I'd like to try to do is have a camera take a picture of the die and compare it with control images to detect the value. You really should have put this in your question. It is very difficult to answer your question when the exact background to it is unknown.  If I understand your question (esp. Edit 2) correctly you want to search for circular patterns in a digial image from a camera or scanner. As RobS already said hough transform and template matching in the fourier spectrum are good ways to do this. You will probably find lots of libraries for hough transformation or FFT but I'm not sure you'll be able to use one of those without actually understanding the algorithms. For example: Standard hough transformation only works for lines it has to be adapted for circles. Also it needs some kind of preprocessing to find the edges of the circle. It has a few parameters (the size of the internal parameter space) that are hard to adjust if you don't know what they mean. If you can binarize your image i.e. if the circular patterns you're looking for are significantly brighter or darker than the background it might be simpler to Binarize the image Group connected areas of pixels aka Blobs (e.g. using Flood-Fill) Decide if a blob is one of your patterns by comparing some of it's characteristics (e.g. total area number of boundary pixels average brightness average contrast) to the kind of pattern you'd expect Neither of these subproblems (Binarization Segmentation Pattern matching) is simple or even solvable in general but if your problem is simple enough you might just get away with a few very simple algorithms. Thanks I'd forgotten that it is the generalized hough transform that is required. But I think that the highly constrained situation of the pattern matching that is required here should make the problem relatively easily solvable. Binarizing the image is a great start so +1. I'd probably go with hough transform too as it doesn't require that the circles are (much) brighter than the background. But it's pretty advanced for a beginner especially GHT with 3-5 degrees of freedom. Finding/measuring blobs is much easier conceptually.  If you have Office installed you can use its OCR component I've used Office OCR and that was bug- and painfull. Will this work with non-alphanumeric images? The letters were just an example. You could replace A B and C with a picture of a banana an orange and an apple. 80Bower : no it won't  In one of your comments above you mention that you want to detect die rolls using a camera system. There are several approaches to this problem here are two: 1) Very simple approach. Do circle detection using the hough transform on the pictures of your die faces and count the number of circles. You'll know approximately the size of the pips on the dice to that should help set up the hough algorithm. 2) Complex approach. Get images of each face of your die and compute a Fourier Transform and extract the power spectrum (2D then collapse across orientation). The power spectrum will give you a signature for each of the die faces independent of the orientation of the die relative to the camera. You can compare these signature power spectra with those from the die rolls. The closest match should be your pip count.... Hope this helps a little. Thanks for the ideas. I'm gonna look around for a library that can help with these.  Here is an open source Image Recognition program. It is in beta. It might be a start for you. From the description: Search for images based on the characteristics of the image itself very fast searching once the images are loaded. Shows a list of results sorted by likeness. You can also search for duplicates within a library of images.
62,A,How can you draw 32bppargb images with the Win32 AlphaBlend function? I've been fussing with this for the better part of the night so maybe one of you can give me a hand. I have found GDI+ DrawImage in C# to be far too slow for what I'm trying to render and from the looks of forums it's the same for other people as well. I decided I would try using AlphaBlend or BitBlt from the Win32 API to get better performance. Anyway I've gotten my images to display just fine except for one small detail—no matter what image format I use I can't get the white background to disappear from my (transparent) graphics. I've tried BMP and PNG formats so far and verified that they get loaded as 32bppargb images in C#. Here's the call I'm making: // Draw the tile to the screen. Win32GraphicsInterop.AlphaBlend(graphicsCanvas destination.X destination.Y this.TileSize this.TileSize this.imageGraphicsPointer tile.UpperLeftCorner.X tile.UpperLeftCorner.Y this.TileSize this.TileSize new Win32GraphicsInterop.BLENDFUNCTION(Win32GraphicsInterop.AC_SRC_OVER 0 Convert.ToByte(opacity * 255) Win32GraphicsInterop.AC_SRC_ALPHA)); For the record AC_SRC_OVER is 0x00 and AC_SRC_ALPHA is 0x01 which is consistent with what MSDN says they ought to be. Do any of you guys have a good solution to this problem or know a better (but still fast) way I can do this? Graphics.DrawImage() speed is critically dependent on the pixel format. Format32bppPArgb is 10 times faster than any other one on any recent machine I've tried. Also make sure you the image doesn't get resized be sure to use a DrawImage() overload that sets the destination size equal to the bitmap size. Very important if the video adapter's DPI setting doesn't match the resolution of the bitmap. I'm not sure how to get my image into Format32bppPArgb...it loads as Format32bppArgb. Any thoughts? Use the Bitmap constructor that lets you specify the PixelFormat. Draw the original bitmap into it with Graphics.FromImage(). How do I convert back from `Graphics` to an `Image`? I figured it out. Thanks for the help...  Have you tried an opacity of just 255 rather than a calculated one? This blog post describes what you're trying to do:- http://blogs.msdn.com/andreww/archive/2007/10/10/preserving-the-alpha-channel-when-converting-images.aspx Critical thing is that he carries out a conversion of the image to make the alpha channel compatible..  Ok. From a pure Win32 perspective: In order for AlphaBlend to actually alpha blend... it needs the source device context to contain a selected HBITMAP representing an image with 32bpp bitmap with a pre-multiplied alpha channel. To get a device bitmap with 32bpp you can either call one of the many functions that will create a screen compatible device bitmap and hope like hell the user has selected 32bpp as the desktop bitdepth. OR ensure that the source bitmap is a DIBSection. Well the library or framework that is creating it from the loaded image for you. So C# is loading your images with 32bpp argb BUT how are you converting that C# representation of the bitmap into a HBITMAP? You need to ensure that a DIB Section is being created not a DDB (or device dependent bitmap) and that the DIB Section is 32bpp.
63,A,"JavaFX Designer At the moment I'm using JavaFX only for fun and learning it. What I find annoying is designing the graphics by coding them and watching the result in a preview window - you know what I mean? Is there something for JavaFX like what Expression Blend is for Silverlight. Hey you can use Scene builder for design a FXML file without coding. here on YouTube i found the tutorial of the beginning with Scene Builder. http://www.youtube.com/watch?gl=IN&v=a-zdDXPjKgI  Yes there is support for exporting from Adobe tools. It is called JavaFX Production Suite.  Update Tor Norbye talk about JavaFX designer at JavaOne 2009 conference and he published screenshot on his blog. http://blogs.oracle.com/tor/entry/finally_over I use NetBeans but where's the designer? I can drag&drop components in the source so the source is ""automagically"" created and I can use the preview but I don't find a way to ""draw"" like i.e. I can do for SVG in Inkscape.  You might take a look at the JavaFX SceneBuilder. http://docs.oracle.com/javafx/scenebuilder/1/user_guide/jsbpub-user_guide.htm"
64,A,"Bouncing Ball in Java This is probably a really basic problem but I can't seem to find any other articles on it. Anyway I have written a small bouncing ball program in Java to try and expand my basic skills. The program is just a simple bouncing ball that will drop and hopefully bounce for a while. The original program worked fine but now I have tried to add gravity into the program. The gravity actually works fine for a while but then once the bounces get really small the animation becomes erratic for a very short time then the position of the ball just constantly decreases. I've tried to figure out the problem but I just can't see it. Any help would be most welcome. public final class Ball extends Rectangle { float xspeed = 1.0f; float yspeed = 1.0f; float gravity = 0.4f; public Ball(float x float y float width float height) { super(x y width height); } public void update(){ yspeed += gravity; move(xspeed yspeed); if(getX() < 0){ xspeed = 1; } if(getX() + getWidth() > 320){ xspeed = -1; } if(getY() < 0){ yspeed = 1; } if(getY() + getHeight() > 200 && yspeed > 0){ yspeed *= -0.98f; } if(getY() + getHeight() > 200 && yspeed < 0){ yspeed *= 0.98f; } } public void move(float x float y){ this.setX(getX()+x); this.setY(getY()+y); } } EDIT: Thanks that seems to have sorted the erratic movement. I'm still struggling to see how I can stop my ball moving down when it has stopped bouncing. Right now it will move stop bouncing then continue moving down passed the ""floor"". I think it's to do with my yspeed += gravity line. I just can't see how I'd go about stopping the movement down. I suspect it's because when the ball bounces it will actually be slightly below the ""ground"" and at low speeds it won't move back above the ground in one tick - so the next update() will see it still below the ground and bounce again - but downwards this time so the cycle continues. You need to move the ball back up to ground level when it bounces something like this:  if(getY() + getHeight() > 200){ yspeed *= -0.981; setY(200 - getHeight()); } move seems to take deltas not absolute values your ball is likely to fly off the edge of the universe :-) heh oops. Fixed.  When you do yspeed += gravity; you are assuming that the ball has space move through a distance dx = v_i * t + 1/2 (-g) t^2. When you are very near the floor this may not be true. It fail if: You are near enough the the floor and moving down You are very near the floor and have low velocity (like when the ball has lost most of it's energy) This bug causes your simulation to stop conserving energy resulting in the erratic behavior you see at low amplitude. You can reduce the problem by using smaller time steps and you can get rid of it outright if you do test computation to notice when you're out of room and to select a safe time step for that iteration (i.e. always use your default unless there is a problem then calculate the best time step). However the basic approximation you're using here has other problems as well. Look in any numeric analysis text for a discussion of solving differential equations numerically.  First things first: setting y-speed to 1 when you bounce on the top of the window is not correct you should set yspeed to -yspeed (but if you start within the borders it should never bounce up to the top anyway). Secondly your multiply by -0.981 when bouncing on the bottom is okay but I'm concerned with the constant 0.4 gravity being added to yspeed every iteration. I think that's what is causing you wiggles at the bottom since you do the move before checking which can result in the ball dropping below ground level. I would try ensuring the the y value can never go below ground level by replacing the move with: if (getY() + getHeight() + yspeed > 200) { move(xspeed 200 - getY() - getHeight()); } else { move(xspeed yspeed); }  [EDIT: I think I misunderstood so this probably isn't much use :) ] if(getY() + getHeight() > 200){ yspeed *= -0.981; } You're negating the vertical velocity on every update. I'd probably try handling gravity in update-sized slices. Assuming you're doing 30 updates per second (for 30fps) maybe something like // Define some constants SecondsPerUpdate = (1.0f / 30); AccelDueToGravity = 0.981; if(getY() + getHeight() > 200){ yspeed -= (AccelDueToGravity * SecondsPerUpdate); }  The problem is that when the bounces get really small the yspeed *= -0.981; line will get called in short succession. The ball will go below the bottom start coming back up but still be below the bottom (because 0.981 < 1.0) eventually and it will behave eradically. Here's how you fix it: if(getY() + getHeight() > 200){ yspeed *= -0.981; setY(400 - getY() - getHeight()); // I believe this is right. } By fixing the position you won't alternate between decreasing and increasing as quickly and won't get stuck in the situation where it is always decreasing because it is always below the bounds. 200 - (getY() + getHeight() - 200) = 200 - getY() - getHeight() + 200 = 400 - getY() - getHeight()"
65,A,"Calculating pixel size on an iPhone Is there a way in the iPhone SDK to calculate the size (in millimeters) of a single pixel? Well the size of a pixel is a constant. The screen size of a current iPhone or iPod touch 2"" x 3"" (50.8 mm x 76.2 mm) and the resolution is 320 x 480 pixel. 50.8 / 320 (or 76.2 / 480) => the size of 1 pixel is 0.15875 mm x 0.15875 mm Agreed little point to calculate the value of a fixed known value. Thanks for that. I did originally think of doing that but then it occurred to me that if they changed the screen size and resolution in a future model then I would have more changes to make to make my application display nicely on the new device. Well I think if future versions change the pixel density (currently 160 ppi) you would check the device model and use different constants. I guess if Apple changes resolution and/or screen size on future models they will provide that API in an updated SDK. indeed the pixel density has been doubled for iphone 4 but no need to test for it in your applications since the coordinates you specify are in points not pixels. where pixels:points used to have a 1:1 ratio they now have a 2:1 ratio.  Answering the question as asked about the size of pixels: Pixel size on an iPhone and iPod Touch The earlier iPhones (pre-iPhone 4) Apple iPhone Technical Specifications said : 480-by-320-pixel resolution at 163 pixels per inch(ppi). About 0.006135 inches per pixel or 0.1558282 mm per pixel. The first three iPod touch generations stated the same 163 ppi. The iPhone 4 specs said 960-by-640-pixel resolution at 326 ppi . So pixel width is 1 inch / 326 pixels per inch or about 0.003067 inches per pixel or 0.0779 mm per pixel. You use points not pixels. Edit: As noted in Olaf's comment below pixels are actually addressable using half-points. The fourth generation iPod touch (Sept 2010) has specs the same as the iPhone 4 960-by-640-pixel resolution at 326 ppi The iPhone 4S (Oct 2011) is unchanged from the iPhone 4 in terms of resolution. The iPhone 5 (Sept 2012) specs said 1136-by-640 pixel resolution at 326 ppi. Pixel size is unchanged. Screen diagonal is 4 inches. The iPhone 5C and iPhone 5S (Sept 2013) have the same resolution pixel size and diagonal as the iPhone 5. The iPhone 6 4.7 inch (Sept 2014) specs are 1334-by-750-pixel resolution at 326 pixels per inch (ppi). pixel size is unchanged from 4 4S 5 5s. The iPhone 6 Plus 5.5 inch (Sept 2014) specs are 1920-by-1080-pixel resolution at 401 pixels per inch (ppi). pixel size is about 20% smaller. The pixel width is 1 inch / 401 pixels per inch or about 0.002494 inches per pixel or 0.06334 mm per pixel. Pixel size on an iPad The iPad 1 and 2 are 9.7 inch (diagonal) display with 1024-by-768-pixel resolution at 132 ppi per the iPad specs. That is about .0075758 inches per pixel or 0.1924 mm per pixel. The new iPad (March 2012) is a 9.7 inch (diagonal) display with 2048-by-1536-pixel resolution at 264 ppi per the current iPad specs. That is about .0037879 inches per pixel or 0.09621 mm per pixel. The iPad Mini (1st generation - October 2012) is a 7.9 inch (diagonal) display with a 1024-by-768-pixel resolution at 163 ppi per the original iPad mini specs. That is about .006135 inches per pixel or 0.156 mm per pixel. The iPad Mini (2nd generation - October 2013) is a 7.9 inch (diagonal) display with a 2048-by-1536-pixel resolution at 326 ppi per the current iPad mini specs. That is about .0030675 inches per pixel or 0.0779 mm per pixel. Comparison of the iPad Air and iPad 2 iPad Mini with Retina and iPad Mini display specs. You don't need to have the iPhone SDK calculate the size of a single pixel. One option is to determine what you are running on and then select the needed mm size. The iPhone (up to and including the 4S) / iPod Touch screen sizes with a 3.5 inch (diagonal) display are NOT exactly 2"" x 3"". They are a tiny bit smaller than that. The iPhone 5 has a 4 inch (diagonal) display. What the questioner may actually need: points. See Removers comment to the previous answer. Coordinates are specified in points not pixels. of course they are individually addressable. You can specify positions on half-points which would make your graphics look fuzzy on old iphones but just as crisp on iphone 4. (or you could use opengl or an offscreen buffer and copy it to the screen for low-level access) When people are saying 960 by 640 or 640 x 960 here why are they not saying whether this is portrait or landscape mode? Please specify. The mode depends on how you are holding the iPhone or iPad. See this http://stackoverflow.com/questions/4482893/iphone-what-is-mean-landscape The iPad Mini resolution is 163 ppi not 162 ppi. Per spec linked in the answer. + 1 for the quick update on the iPhone 6 and 6+"
66,A,How to do directed graph drawing in PHP? I'm looking for a way to draw directed graphs in PHP. (as in http://upload.wikimedia.org/wikipedia/commons/0/08/Directed_acyclic_graph.png). I want it to create an image of the graph just like GD can output an image. I've googled a lot on this but I only can find a lot of libraries for drawing graphs in general (with bars etc) not directed graphs. P.S. I've tried using dot (the linux program) via system() but unfortunately I have no permission to do that on the server. Also I have no rights to install PHP extensions and stuff like that on the server so it should work with normal PHP (ideally just by including a file). Thanks in advance. Why can't you do this using gd? It would be relatively trivial you just need to keep track of where each node is. Do you want to just give the list of nodes and what they connect to and it automatically generates the directed graph? I've tried this but it is quite complex to make that work without (to much) bugs it results in really ugly formatted graphs. The biggest problem is the structure of the network avoid to much crossing lines etc. Haven't tried it yet but this looks very promising. http://www.kylescholz.com/blog/2006/06/using%5Fforce%5Fdirected%5Fgraphs.html  I'm not aware of any graph visualization implementation in php. However I suggest you to consider drawing the graph with javascript for instance with the canviz JS library which works on most browsers (yes including IE 6 & 7 but not 8 currently).  After a quick Google search I found graph.php which in the comments states that it connects nodes through arcs vice straight lines in the example provided but may be a good step in the right direction. Maybe I'm missing something but as far I can see this is only the data structure (which I already have). It doesn't seem to draw the graph? Aye I noticed this as I was getting ready to put it to use. I thought the code on that page was clipped. Sorry 'bout that.  I use php to generate json that is consumed by the d3 force-directed graph system. So the display is all handled client side all I need to do is make the right json data structures... -FT  I found a PEAR interface to GraphViz; I have not used it before so can't give you any personal recommendation whether it's good or bad. (but perhaps that doesn't solve your problem since you say you can't install applications) It would indeed be a good solution but PEAR is unfortunately not installed on my account and I have no rights to do that myself. You can also manually download the package and it's dependencies. You don't need the PEAR installer in order to use PEAR packages.
67,A,"Vector drawing tool for iPhone development This isn't strictly a programming question but I'm asking it here because it's certainly a software development question if you take ""software development"" to include all aspects of creating a software system. I am an independent iPhone developer. Except for translations I handle all aspects of my apps myself—graphics included. I have to create icons buttons and UI elements of all sorts on a regular basis. I've learned a few tricks along these lines and while they're certainly not works of art I can effectively use gradients shadows border strokes transparency and textures to create minimalistic attractive effects. So far I've used a vector drawing tool called VectorDesigner for all of my development with occasional raster postprocessing by Pixelmator. It's worked mostly okay so far but VectorDesigner has a host of issues: It uses a package format for its files which interferes with the use of Subversion. It is very much a print tool and I have to be very careful not to end up with objects on fractional pixel values which cause antialiasing. While you can take the union or intersection of shapes or add and subtract them curves tend to deform with repeated boolean operations sometimes quite dramatically. And it offers very little control over strokes to the point where I barely use them. So I'm looking for a better tool for this specific purpose: shape-based drawing of simple icons buttons and UI elements on a Mac by someone without graphic design training. Good functions for exporting would be a plus—ideally it should be almost as easy to export a PNG to the place it goes in my project as it is to save (not save as) the file. The perfect tool for me would be one that would allow you to define an object's shape by stacking up areas and masks defined by primitive shapes (which would remain separately editable) then define properties on those objects like transforms and strokes. I have no idea if something like this exists though. Adobe's tools generally strike me as very heavyweight and are usually expensive but I suppose they're a possibility. (Fireworks with its emphasis on screen design seems like it might be particularly suitable but I don't know that much about it.) But what else is out there? If you're in a position like me what do you use? What do you recommend? Edited to add: Of course a graphic designer could get better results from an ancient copy of MacPaint than I could from Illustrator CS5. No tool can replace skill and taste and many programmers have little of either. I'm aware of that. But I'm fortunate enough to have at least some taste—enough that my users compliment my apps' appearance in their reviews. I'm not hugely talented but I do know my limitations and I don't let myself produce anything ugly. Given my budget that will have to do for now. Try Opacity. A little rough on the edges but one of the coolest and most unique features they have is export as source code (in Quartz Cocoa Cocoa Touch or Canvas)  I think the only acceptable answer here should be ""hire a designer"". But it sounds like Pixelmator/Inkscape are your best bets. Though if you do find something better that'd be really cool. Like a jQueryUI but for native.  I'd suggest OmniGraffle OmniGraffle is easy to use can save as a PNG can create binary non-package files (it's an option in the interface). You can also set the units to pixels to ensure exact alignment. (Canvas Size -> Ruler Units) Finally the Graffletopia website has some nice iPhone stencils for getting it right: http://graffletopia.com/search/iphone I actually have OmniGraffle already but it looks like I'd need the Pro version to really be able to make custom shapes. $100 more for something that still won't fill all of my needs is kind of steep. Still not a bad idea. Commenting on my own answer... not everything is possible in OmniGraffle but you mentioned post-processing in Pixelmator already. I just think you solve some of your issues with VectorDesigner if you used OmniGraffle."
68,A,"opengl: question about glutMainLoop() can somebody explain how does glutMainLoop work? and second question why glClearColor(0.0f 0.0f 1.0f 1.0f); defined after glutDisplayFunc(RenderScene); cause firstly we call glClear(GL_COLOR_BUFFER_BIT); and only then define glClearColor(0.0f 0.0f 1.0f 1.0f); int main(int argc char* argv[]) { glutInit(&argc argv); glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); glutInitWindowSize(800 00); glutInitWindowPosition(30050); glutCreateWindow(""GLRect""); glutDisplayFunc(RenderScene); glutReshapeFunc(ChangeSize); glClearColor(0.0f 0.0f 1.0f 1.0f); <-- glutMainLoop(); return 0; } void RenderScene(void) { // Clear the window with current clearing color glClear(GL_COLOR_BUFFER_BIT); // Set current drawing color to red // R G B glColor3f(1.0f 0.0f 1.0f); // Draw a filled rectangle with current color glRectf(0.0f 0.0f 50.0f -50.0f); // Flush drawing commands glFlush(); } glutMainLoop() just runs a platform-specific event loop and calls any registered glut*Func() callbacks as needed. RenderScene() won't be called by GLUT until you call glutMainLoop(). So in reality glClearColor() gets called first not glClear().  glutDisplayFunc(RenderScene); This only sets the callback function it doesn't actually call it until it enters the main app loop in the call to glutMainLoop. So glClearColor comes before glClear."
69,A,"Resizing an image with alpha channel I am writing some code to generate images - essentially I have a source image that is large and includes transparent regions. I use GDI+ to open that image and add additional objects. What I want to do next is to save this new image much smaller so I used the Bitmap constructor that takes a source Image object and a height and width then saved that. I was expecting the alpha channel to be smoothed like the color channels but this did not happen -- it did result in a couple of semitransparent pixels but overall it is very blocky. What gives? Using img As New Bitmap(""source100x100.png"") ''// Drawing stuff Using simg As New Bitmap(img 20 20) simg.Save(""target20x20.png"") End Using End Using Edit: I think what I want is SuperSampling like what Paint.NET does when set to ""Best Quality"" Draw the image on the new bitmap instead of creating the new bitmap from the image. Create an empty bitmap by just specifying the size then use the Graphics.FromImage method to create a Graphics object for the bitmap. Use the DrawImage method to draw the image onto the new bitmap. There are properties like Interpolationmode and PixelOffsetMode in the Graphics object that you can use to control how the image is scaled and drawn. SmoothingMode helped a bunch too. Thx."
70,A,"How does Content-Aware fill work? In the upcoming version of Photoshop there is a feature called Content-Aware fill. This feature will fill a selection of an image based on the surrounding image - to the point it can generate bushes and clouds while being seamless with the surrounding image. See http://www.youtube.com/watch?v=NH0aEp1oDOI for a preview of the Photoshop feature I'm talking about. My question is: How does this feature work algorithmically? My theory? It ties in with Google Earth/Maps to determine what you took a picture of then just pulls surrounding image data down and inserts it into your original image :) It's magic. Prove me wrong. It's not an 'upcoming' version - it's already out. Solving this: http://cs.stackexchange.com/questions/23794/interpolation-optimization-problem could be used easily for ""Inpainting"". Well they are not going to tell for the obvious reasons. The general name for the technique is ""inpainting"" you can look this up. Specifically if you look at what Criminisi did while in Microsoft http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.67.9407 and what Todor Georgiev does now at Adobe http://www.tgeorgiev.net/Inpainting.html you'll be able to make a very good guess. A 90% guess I'd say which should be good enough.  There is very similar algorithm for GIMP for a quite long time. It is called resynthesizer and probably you should be able to find a source for it (maybe at the project site) EDIT There is also source available at the ubuntu repository And here you can see processing the same images with GIMP: http://www.youtube.com/watch?v=0AoobQQBeVc&feature=related When I last looked at resynthesizer (version 0.13 I think) it was behind state of the art (see e.g. Criminisi's work cited in my answer) and thus pretty slow. Of course it's still the best from what's available for free.  I am a co-author of the PatchMatch paper previously mentioned here and I led the development of the original Content-Aware Fill feature in Photoshop along with Ivan Cavero Belaunde and Eli Shechtman in the Creative Technologies Lab and Jeff Chien on the Photoshop team. Photoshop's Content-Aware Fill uses a highly optimized multithreaded variation of the algorithm described in the PatchMatch paper and an older method called ""SpaceTime Video Completion."" Both papers are cited on the following technology page for this feature: http://www.adobe.com/technology/projects/content-aware-fill.html You can find out more about us on the Adobe Research web pages. Thank you for sharing some very valuable information. This should be the correct answer.  The general approach is called seam-carving. Ariel Shamir's group is responsible for the seminal work here which was presented in SIGGRAPH 2007. See: http://www.faculty.idc.ac.il/arik/site/subject-seam-carve.asp For the record seam-carving is NOT content-aware fill. It is a simpler algorithm that allows you to resize the image vertically and horizontally without distorting important features.  As a guess (and that's all that it would be) I'd expect that it does some frequency analysis (some like a Fourier transform) of the image. By looking only at the image at the edge of the selection and ignoring the middle it could then extrapolate back into the middle. If the designers choose the correct color plains and what not they should be able to generate a texture that seamlessly blends into the image at the edges. edit: looking at the last example in the video; if you look at the top of the original image on either edge you see that the selection line runs right down a ""gap"" in the clouds and that right in the middle there is a ""bump"". These are the kind of artifacts I'd expect to see if my guess is correct. (OTOH I'd also expect to see them is it was using some kind of sudo-mirroring across the selection boundary.) People used Fourier-like transforms to do these things in the 80s. Things happened since although in some conditions this will give you a good initial guess. AB: If I had to guess I'd still say it's doing the same basic thing just a more advanced version of it.  I'm guessing that for the smaller holes they are grabbing similarly textured patches surrounding the area to fill it in. This is described in a paper entitled ""PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing"" by Connelly Barnes and others in SIGGRAPH 2009. For larger holes they can exploit a large database of pictures with similar global statistics or texture as describe in ""Scene Completion Using Millions of Photographs"". If they somehow could fused the two together I think it should work like in the video. Hi Could you assist with that: http://cs.stackexchange.com/questions/23794/interpolation-optimization-problem Thank You.  I work on a similar problem. From what i read they use ""PatchMatch"" or ""non-parametric patch sampling"" in general. PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing Why do i can't post link properly? Hi Could you assist with that: http://cs.stackexchange.com/questions/23794/interpolation-optimization-problem Thank You."
71,A,"Qt plotting application Currently I'm trying to develop some simple plot prototype and I'm struggling with some kind of white/empty sheet syndrome. I'm back to Qt after 2 years so I feel quite retarded. My application should: plot and manage custom layers of data plot on custom canvas background manage markers on plot My plan is to use following design: QGraphicsScene /View/Item as a sprite like management widgets for background markers pointers and other ""bitmap"" objects etc. QPainter/ Qpixmap or QPicture for actual data layers - and if possible set them as QGraphicItem to simplify management of dynamic graphics I don't want to use Qwt or similar library unless I can plot with it on custom background (I don't like the look of the qwt's graphic style). Is my plan proper in scope of qt class usage and composition? I'd like to have at least clear overview of the classes which should be involved for this kind of prototype. Thanks in advance. P. I don't know what you mean by ""custom background"" but the appearance of the qwt widgets can be totally configured (and that's not a bad thing given the default one). By custom background I mean my own bitmap with real time changing grid (my own graphic grid) and on top of that there would be data layers with plots. May be MathGL is appropriate for you. It have Qt widget or you can use RGBA image directly to combine it with any background in your widget.  You may want to take a look at the Core Plot framework. Core Plot is OS X specific but it is built on the the OS X Core Animation system which has a lot of conceptual similarity to the Qt Graphics View Framework. You'll have to learn to visually parse the Objective-C (a less-than-two day process for any competent C++ developer) but you should be able to see the general architecture relatively easily. The Core Plot wiki has some nice high-level documentation that might set you on your way without even needing to look at the code.  You don't say much about your project for me to propose a more helpful answer but have a look at the Qt demos involving the graphics view especially diagram scene and 40000 chips. I think you will find them inspiring for what you want to do.  I think you have the basic idea with QGraphicsView. Here are a few resources which might help: Graphics View Diagram Scene If you want to use the new animation and state set classes: Stickman Also take a look at gunnar's labs blog. He recently did a series on graphics performance. All of these are strictly Qt (animation and state set are in 4.6). They are in C++ but hopefully you can translate what you need to python. Thanks for the links I'm actualy familiar with them but the blog is new to me and it seems to look very nice.  I recommend you to use QCustomPlot which is a Qt C++ library. It focuses on making good looking publication quality 2D plots graphs and charts and also has high performance for realtime visualization applications. You get it here: http://www.qcustomplot.com/"
72,A,"Is PyOpenGL a good place to start learning opengl programming? I want to start learning OpenGL but I don't really want to have to learn another language to do it. I already am pretty proficient in python and enjoy the language. I just want to know how close it is to the regular api? Will I be able to pretty easily follow tutorials and books without too much trouble? I know C++ gives better performance but for just learning can I go wrong with PyOpenGL? Thanks alot With the caveat that I have done very little OpenGL programming myself I believe that for the purposes of learning PyOpenGL is a good choice. The main reason is that PyOpenGL like most other OpenGL wrappers is just that: a thin wrapper around the OpenGL API. One large benefit of PyOpenGL is that while in C you have to worry about calling the proper glVertex3{dfiX} command Python allows you to just write glVertex3(xyz) without worrying about telling Python what type of argument you passed in. That might not sound like a big deal but it's often much simpler to use Python's duck-typing instead of being overly concerned with static typing. The OpenGL methods are almost completely all wrapped into Python methods so while you'll be writing in Python the algorithms and method calls you'll use are identical to writing OpenGL in any other language. But since you're writing in Python you'll have many fewer opportunities to make ""silly"" mistakes with proper pointer usage memory management etc. that would just eat up your time if you were to study the API in C or C++ for instance. Same can be said about JOGL for Java fans.  PyOpenGL I don't think it is a good choice. In my opinon in C/C++ it is easier to play around around with your OpenGL code - start with simple app then add shader then add some geometry functions make a texture/geometry generator build scene via CSG etc. You know - to have fun play around with code experiment and learn something in process. I honestly just don't see myself doing this in python. Surely it is possible to do OpenGL programming in Python but I see no reason to actually do it. Plus several OpenGL functions take memory pointers as arguments and although there probably a class (or dozen of alternatives) for that case I don't see a reason to use them when a traditional way of doing things is available in C/C++ especially when I think about amount of wrappers python code uses to pass vector or array of those into OpenGL function. It just looks like making things more complicated without a real reason to do that. Plus there is a noticeable performance drop especially when you use ""RAW"" OpenGL. Besides if you're going to make games it is very likely that you'll have to use C++ or some other ""non-python"" language. P.S. I've done enough OpenGL programming a lot of DirectX programming but I specialize on C++ and use python only for certain algorithmic tests tools and scripts. Hey. I marked you down because in my opinion this isn't good advice. I thought I'd leave a comment explaining why instead of doing it anonymously and leaving no feedback. You say you think it's easier in C++ but don't say why. The Python code is shorter and simpler requiring fewer parameters and less thinking. ...I spent 7 years writing C and C++ but the Python equivalent would be just the same as the C/C++ version but slightly easier to create because as Mark Rushakoff says there are no pointer or memory management issues to deal with. PyOpenGL will automatically wrap all OpenGL function calls with parameter checking and error checking by default making diagnosis easier. Using PyOpenGL you can pass a vertex array as an ordinary Python list - there are no wrappers. There are performance implications but not complexity issues. Plus I know many people who make games using Python. Your assumption that this requires C/C++ is based on your assumption that OpenGL is bad in Python which is based on your assumption that making games must require C/C++... :-) Actually the wrappers talk here has some point +1 :)"
73,A,"Lattice problems: lattice objects coming from JAGS but device can't be set I ran JAGS with runjags in R and I got a giant list back (named results for this example). Whenever I access results$density two lattice plots (one for each parameter) pop up in the default quartz device. I need to combine these with par(mfrow=c(2 1)) or with a similar approach and send them to the pdf device. Nothing I tried is working. Any ideas? I've tried dev.print pdf() with dev.off() etc. with no luck. Vince For instance for the included example from run.jags check the structure of the list using sink(""results_str.txt"") str(results$density) sink() Then you will see components named layout. The layout for the two plots of each variable can be set using results$density$m$layout <- c(12) print(results$density$m) The plots for different parameters can be combined using the c.trellis method from the latticeExtra package. class(results$density$m) <- ""trellis"" # overwrite class ""plotindpages"" class(results$density$c) <- ""trellis"" # overwrite class ""plotindpages"" library(""latticeExtra"") update(c(results$density$m results$density$c) layout=c(22)) Another approach is to use grid viewports: library(""grid"") results$density$m$layout <- c(21) results$density$c$layout <- c(21) class(results$density$m) <- ""trellis"" class(results$density$c) <- ""trellis"" layout <- grid.layout(2 1 heights=unit(c(1 1) c(""null"" ""null""))) grid.newpage() pushViewport(viewport(layout=layout)) pushViewport(viewport(layout.pos.row=1)) print(results$density$m newpage=FALSE) popViewport() pushViewport(viewport(layout.pos.row=2)) print(results$density$c newpage=FALSE) popViewport() popViewport() This works - but I don't want the V1 and V2 plots - they are redundant. How can I get rid of these?  Here's a way to ditch the ""V1"" panels by manipulation of the Trellis structure: p1 <- results$density$c p2 <- results$density$m p1$layout <- c(11) p1$index.cond[[1]] <- 1 # remove second index p1$condlevels[[1]] <- ""c"" # remove ""V1"" class(p1) <- ""trellis"" # overwrite class ""plotindpages"" p2$layout <- c(11) p2$index.cond[[1]] <- 1 # remove second index p2$condlevels[[1]] <- ""m"" # remove ""V1"" class(p2) <- ""trellis"" # overwrite class ""plotindpages"" library(grid) layout <- grid.layout(2 1 heights=unit(c(1 1) c(""null"" ""null""))) grid.newpage() pushViewport(viewport(layout=layout)) pushViewport(viewport(layout.pos.row=1)) print(p1 newpage=FALSE) popViewport() pushViewport(viewport(layout.pos.row=2)) print(p2 newpage=FALSE) popViewport() popViewport()  The easiest way to combine the plots is to use the results stored in results$mcmc: # prepare data see source code of ""run.jags"" thinned.mcmc <- combine.mcmc(list(results$mcmc) collapse.chains=FALSE return.samples=1000) print(densityplot(thinned.mcmc[c(12)] layout=c(12) ylab=""Density"" xlab=""Value"")) Clean and effective! Thanks. I'd still be curious (for general lattice knowledge) how to ditch those V1 and V2 plots in your other example."
74,A,"Read the RGB value of a given pixel in Python Programatically If I open an image with open(""image.jpg"") how can I get the RGB values of a pixel if I have the coordinates of the pixel? Then how can I do the reverse of this? Starting with a blank graphic 'write' a pixel with a certain RGB value? It would be so much better if I didn't have to download any additional libraries. install PIL using the command ""sudo apt-get install python-imaging"" and run the following program. It will print RGB values of the image. If the image is large redirect the output to a file using '>' later open the file to see RGB values import PIL import Image FILENAME='fn.gif' #image can be in gif jpeg or png format im=Image.open(FILENAME).convert('RGB') pix=im.load() w=im.size[0] h=im.size[1] for i in range(w): for j in range(h): print pix[ij]  Image manipulation is a complex topic and it's best if you do use a library. I can recommend gdmodule which provides easy access to many different image formats from within Python. Anyone know why this was downvoted? Is there a known problem with libgd or something? (I had never looked at it but it's always nice to know there's an alternative to PiL)  It's probably best to use the Python Image Library to do this which I'm afraid is a separate download. The easiest way to do what you want is via the load() method on the Image object which returns a pixel access object which you can manipulate like an array: from PIL import Image im = Image.open(""dead_parrot.jpg"") #Can be many different formats. pix = im.load() print im.size #Get the width and hight of the image for iterating over print pix[xy] #Get the RGBA Value of the a pixel of an image pix[xy] = value # Set the RGBA Value of the image (tuple) Alternatively look at ImageDraw which gives a much richer API for creating images. Fortunately installing PIL is very straightforward in Linux and Windows (don't know about Mac) Installing PIL on Mac took me way too damn long. @ArturSapek I installed PIL by `pip` which was fairly easy. I used this on my Mac (Pypi): `easy_install --find-links http://www.pythonware.com/products/pil/ Imaging`  You can use pygame's surfarray module. This module has a 3d pixel array returning method called pixels3d(surface). I've shown usage below: from pygame import surfarray image display import pygame import numpy #important to import pygame.init() image = image.load(""myimagefile.jpg"") #surface to render resolution = (image.get_width()image.get_height()) screen = display.set_mode(resolution) #create space for display screen.blit(image (00)) #superpose image on screen display.flip() surfarray.use_arraytype(""numpy"") #important! screenpix = surfarray.pixels3d(image) #pixels in 3d array: #[x][y][rgb] for y in range(resolution[1]): for x in range(resolution[0]): for color in range(3): screenpix[x][y][color] += 128 #reverting colors screen.blit(surfarray.make_surface(screenpix) (00)) #superpose on screen display.flip() #update display while 1: print finished I hope been helpful. Last word: screen is locked for lifetime of screenpix.  PyPNG - lightweight PNG decoder/encoder Although the question hints at JPG I hope my answer will be useful to some people. Here's how to read and write PNG pixels using PyPNG module: import png array point = (2 10) # coordinates of pixel to be painted red reader = png.Reader(filename='image.png') w h pixels metadata = reader.read_flat() pixel_byte_width = 4 if metadata['alpha'] else 3 pixel_position = point[0] + point[1] * w new_pixel_value = (255 0 0 0) if metadata['alpha'] else (255 0 0) pixels[ pixel_position * pixel_byte_width : (pixel_position + 1) * pixel_byte_width] = array.array('B' new_pixel_value) output = open('image-with-red-dot.png' 'wb') writer = png.Writer(w h **metadata) writer.write_array(output pixels) output.close() PyPNG is a single pure Python module less than 4000 lines long including tests and comments. PIL is a more comprehensive imaging library but it's also significantly heavier. When running this code I had to change `'has_alpha'` to `'alpha'`. The pixel list was formatted slightly differently as well. I suppose there's a different PyPNG version out by now breaking these things? @Joost thanks I updated answer as PyPNG indeed has changed since 2008.  I think the Python Image Library would help here PIL  There's a really good article on wiki.wxpython.org entitled Working With Images. The article mentions the possiblity of using wxWidgets (wxImage) PIL or PythonMagick. Personally I've used PIL and wxWidgets and both make image manipulation fairly easy. Note that your link doesn't show up properly. Thanks; for some reason it worked fine on the preview but not on the fully-posted version! +1 for more than one way of doing what OP asked.  As Dave Webb said. Here is my working code snippet printing the pixel colours from an image: import os sys import Image im = Image.open(""image.jpg"") x = 3 y = 4 pix = im.load() print pix[xy]"
75,A,How to create a paint app for iPhone? I would like to develop a MS paint like app for the iPhone. Could you guys point me to some sample or tutorials on this topic? I'm new to Objective C and Xcode. Thanks Break down your project and focus in your question if want any answers I'm really new to iPhone development. Just need something to get me started I was going to vote to close because this is so open ended but really it's not on second thought.. this is not google ... No need for Open GL indeed. You can check out this blog post . It has a great example code for a simple paint app in iPhone.  Try playing with this sample project. It's a very simple paint app using Open GL. http://developer.apple.com/iphone/library/samplecode/GLPaint/Introduction/Intro.html
76,A,"How do I get a common-lisp GUI in Windows? I'm using Emacs with CLISP and Slime and want to be able to draw pictures on the screen. I'm specifically thinking about drawing graphs but anything that would let me draw basic shapes and manipulate them would be able to get me started. There's cl-cairo2 - a binding to Cairo vector drawing library. It can be used to draw various pictures on various surfaces. There's a cl-2d library that uses cl-cairo2 to draw charts. And there's cl-gtk2 - a binding to Gtk+ library. You can create widgets that are drawn with cl-cairo2 (or cl-2d) that draw what you want.  I think I've found my own answer. Clojure seems to have everything I was looking for just because I can now use all of the Java GUI items natively in LISP. It is a different dialect of LISP than the Common-Lisp I was using but seems to have a lot of community support and integrates with my Windows installation of Emacs either through SLIME or through the Inferior-Lisp interpreter. So far I've been very impressed. Oh a code sample: (. javax.swing.JOptionPane (showMessageDialog nil ""Hello World"")) Any guesses what this does? :) Bill Clementson's blog has quite a bit on Clojure including a lot of helpful posts on installing it. See here: his posts on Clojure  You could switch from CLISP to the free LispWorks Personal Edition and use the CAPI Graphics Ports drawing API. Or you could use Lisp's Foreign Function Interface and use one of the graphics toolkits available for your OS.  Doug is right; CAPI will work fine. Other things you can try: cltk: http://www.cliki.net/Lisp-Tk I know that Allegro has something for Windows programming also but I've never tried it. What may also work is cells-gtk: http://common-lisp.net/project/cells-gtk/ Again I can only tell you that it exists but not how bad it is or if it even really works... I can not comment also on the quality of http://www.cliki.net/GTK%20binding But that's mostly what is available. Corman Lisp probably has something to offer for Windows programming also. Anyway the choices on Windows are relatively slim. The you can probably have the most confidence in CAPI which is used for the LispWorks IDE on Windows Linux MacOS X and on quite few big unices also... Regards  Clojure is an excellent Lisp and Swing is a solid (if not particularly visually exciting) windowing toolkit. If you want do do more advanced graphics and/or dabble with game programming you might want to check out Slick which is a general purpose graphics/game library that sits on top of Swing and gives you access to OpenGL and lots of other stuff. I've found the Clojure/Slick combination an excellent way to do exploratory graphics programming as you can interact with the graphics window directly from the REPL.  I know this is an old post but so the information is here for others like me who find this thread looking for the same thing. This library for tk bindings in common lisp seems to work fairly well. http://www.peter-herth.de/ltk/  For rolling your own (like you said basic shapes) try Lispbuilder-SDL or one of the cl-cairo FFIs (it's just my guess that the latter work with MS Windows though).  CLISP users might find The following useful for their graphics applications: cl-vectors is a pure Common Lisp library to create transform and render anti-aliased vectorial paths. It can be installed using ASDF-Install. http://projects.tuxee.net/cl-vectors/ Vecto is a simplified interface to the powerful CL-VECTORS vector rasterization library....the results can be saved to a PNG ... Since Vecto and all supporting libraries are written completely in Common Lisp without depending on external non-Lisp libraries it should work in any Common Lisp environment. Vecto is available under a BSD-like license. The current version is 1.4.3 released on August 26 2009. http://www.xach.com/lisp/vecto/"
77,A,Drawing over a JPanel and adding the JPanel to JFrame I need to draw a graph over a JPanel by overriding the JPanel's paintComponent() method. While designing gui using netbeans when i drag/drop a JPanel over JFrame it generates code by creating a private variable JPanel object. In such a case how can i override its method to draw over it... or else if i write code for a class by extending the JPanel and override the method to paint it I have to create a new JFrame and add the JPanel to it.. JFrame fr=new JFrame(); fr.add(pane); //pane is the object of class that extends JPanel where i draw fr.setVisible(true); In this case it works.. But if i get a reference of the auto-created class which extends JFrame by netbeans and use that to add the JPanel using the add method of the reference got it doesn't work... class x extends JPanel { paintComponent(Graphics g){ //overridden method //my code for drawing say lines goes here.. } } class y extends Thread { z obj; y(z obj){ this.obj=obj; } public void run(){ x pane=new x(); pane.setVisible(true); obj.add(pane); obj.setVisible(true); //im not getting the pane visible here.. if i created a new JFrame class here as i said earlier and added the pane to it i can see it.. } } class z extends JFrame { z(){//code generated by netbeans} public static void main(String args[]) { new y(new z()).start(); } } It shows no error but when i run the program only the Jframe is visible.. JPanel is not shown... Pardon me if the question is silly.. im a beginner.. Thanks in advance... Your code is pretty much obfuscated. Anyway instead of obj.add(pane); you need obj.getContentPane().add(pane);  Behavior of your code is unpredictable because you are violating main rule of Swing development: all UI work should be done on Event Dispatch Thread (EDT). Your code should look something like: public static void main(String args[]) { SwingUtilities.invokeLater( new Runnable() { void run() { JFrame z = new JFrame(); z.add(new X()); // works only in java 6 //z.getContentPane().add(new X()); // works in any version of java z.pack(); // assuming your pane has preferred size z.setVisible(true); } }); } More about the subject is here: http://java.sun.com/products/jfc/tsc/articles/threads/threads1.html  It sounds like you are a beginner using Swing. However using the library JXLayer makes painting over components extremely easy and intuitive Check out their demos and sample code. Otherwise the excellent JFreeChart is a great free Java graphing (and visualization) library
78,A,Draw a parallel line I have x1y1 and x2y2 which forms a line segment. How can I get another line x3y3 - x4y4 which is parallel to the first line as in the picture. I can simply add n to x1 and x2 to get a parallel line but it is not what i wanted. I want the lines to be as parallel in the picture. Is it a homework?? no it isn't. is that easy for you to think as a homework? then please answer me. :) What you want to do is to offset the coordinates in the direction orthogonal. If you know vector math multiply the vector created by the distance between the endpoints of the line by the following matrix: [ 0 -1 ] [ 1 0 ] Say that the first line has the points (x1y1) (x2y2) with x=x2-x1 y=y2-y1. We also have L = sqrt(x*x+y*y) the length of the line (pardon the notation). Then the next line should be offset by [ 0 -1 ] [x] [ 1 0 ] [y] => dx = -y / L dy = x / L which is the normalized offset for the new line. In C#-like pseudocode: var x1 = ... x2 = ... y1 = ... y2 = ... // The original line var L = Math.Sqrt((x1-x2)*(x1-x2)+(y1-y2)*(y1-y2)) var offsetPixels = 10.0 // This is the second line var x1p = x1 + offsetPixels * (y2-y1) / L var x2p = x2 + offsetPixels * (y2-y1) / L var y1p = y1 + offsetPixels * (x1-x2) / L var y2p = y2 + offsetPixels * (x1-x2) / L g.MoveTo(x1py1p) // I don't remember if this is the way g.LineTo(x2py2p) // to draw a line in GDI+ but you get the idea actually i don't know vector math. Could you kindly write a C# or pseudo function to achieve what you're saying? Got it working. But got one more question if I want to offset 10 pixels away from the original line should I multiply dx * 10 ? Thanks @Krumelur. See my edit. Sorry about terrible syntax though :) You sir just saved me from having to re-learn geometry! Keep up the great work!  Did you try subtracting n to y1 and y2 along with adding n to x1 and x2? I guess that may work Probably you can put the edge conditions i.e. check if y1y2 or zero or not. subtracting y1 and y2 is not working when the lines are 90 degree or 180 degree. @Krumelur's method don't need to check on edges. ;)
79,A,"Problem with PNG images in C# Working in Visual Studio 2008. I'm trying to draw on a PNG image and save that image again. I do the following: private Image img = Image.FromFile(""file.png""); private Graphics newGraphics; And in the constructor: newGraphics = Graphics.FromImage(img); Building the solution gives no errors. When I try to run it I get this: A Graphics object cannot be created from an image that has an indexed pixel format. I don't have much experience with using images in C#. What does this mean and how can I remedy this? EDIT: through debugging Visual Studio tells me that the image has a format8bppindexed Pixel Format. So if I can't use the Graphics class what do I use? EDIT2: After reading this I think it's safe to assume that I better stick to JPG files when working with GDI+ no? EDIT3: my using-statements: using System; using System.Collections.Generic; using System.Drawing; using System.Drawing.Imaging; using System.Windows.Forms; Any luck with this method? http://www.c-sharpcorner.com/UploadFile/rrraman/graphicsObject08232007102733AM/graphicsObject.aspx I use PNG files with the Graphics object all the time. Post a link to the PNG file you're using and we'll see what's wrong with it. Internally GDI works with Bitmaps JPG is compressed and not really great having a compressed image for working with the raw data. Your image is 8bppIndexed this is a Bitmap format where the colours are stored in the palette not the pixel data. The Graphics objects can't modify the pixel values directly as that won't change it. You need to convert it to 24bppRGB You cannot create a graphics from an indexed image format (PNG GIF...). You should use a Bitmap (file or convert your image to a bitmap). Image img = Image.FromFile(""file.png""); img = new Bitmap(img); newGraphics = Graphics.FromImage(img); Indeed dead link there. Perhaps I don't have a the needed ""using"" statement but Visual Studio doesn't recognise that function. Which function ? As per the docs this won't work with an Indexed image http://msdn.microsoft.com/en-us/library/system.drawing.graphics.fromimage.aspx badbod99 > You missed the img = new Bitmap(img); You're 100% right!!! It works perfectly tried with PNG and Indexed bitmaps. It's within the System.Drawing namespace. http://msdn.microsoft.com/en-us/library/system.drawing.image.fromfile.aspx  Without a better PNG library that supports indexed PNGs you're out of luck trying to draw to that image since evidently the GDI+ graphics object doesn't support indexed images. If you don't need to use indexed PNGs you could trap that error and convert your input to regular RGB PNGs using a 3rd party utility. edit: I did find this link http://fci-h.blogspot.com/2008/02/c-indexed-pixel-problem.html that gives a method to draw on your image however it won't affect the original just a copy you can Save() if you require. In case the link goes down: Bitmap bm = (Bitmap) System.Drawing.Image.FromFile(""Fci-h.jpg""true); Bitmap tmp=new Bitmap (bm.Width bm.Height ); Graphics grPhoto = Graphics.FromImage(tmp); grPhoto.DrawImage(bm new Rectangle(0 0 tmp.Width  tmp.Height ) 0 0 tmp.Width  tmp.Height  GraphicsUnit.Pixel); GDI+ doesn't support creating a graphics context from them which has the same effect for the OP. Anyways here's a decent link for a workaround solution: http://fci-h.blogspot.com/2008/02/c-indexed-pixel-problem.html That blogpost did it. Thanks."
80,A,"Resizing Image Quality Reduction I have written a simple PhotoEditor helper class to downscale and crop images uploaded to my website. Everything is working but I am seeing unsatisfactory quality when the image is saved to file. I have read up on the different settings you can tweak below is my setup for resizing cropping is identical.  public Image ResizeImage(Image imgToResize Size size) { int sourceWidth = imgToResize.Width; int sourceHeight = imgToResize.Height; float nPercentW = (size.Width/(float) sourceWidth); float nPercentH = (size.Height/(float) sourceHeight); float nPercent = nPercentH < nPercentW ? nPercentH : nPercentW; var destWidth = (int) (sourceWidth*nPercent); var destHeight = (int) (sourceHeight*nPercent); var src = imgToResize; using (var dst = new Bitmap(destWidth destHeight imgToResize.PixelFormat)) { dst.SetResolution(imgToResize.HorizontalResolution imgToResize.VerticalResolution); using (var g = Graphics.FromImage(dst)) { var mime = GetMimeType(imgToResize); ImageFormat format; if (mime == ""image/gif"" || mime == ""image/png"") { //convert all gif to png better resize quality g.SmoothingMode = SmoothingMode.AntiAlias; g.InterpolationMode = InterpolationMode.HighQualityBicubic; g.DrawImage(src 0 0 dst.Width dst.Height); format = ImageFormat.Png; } else { //jpeg g.CompositingQuality = CompositingQuality.HighQuality; g.InterpolationMode = InterpolationMode.HighQualityBicubic; g.SmoothingMode = SmoothingMode.HighQuality; g.PixelOffsetMode = PixelOffsetMode.HighQuality; format = ImageFormat.Jpeg; } g.DrawImage(src 0 0 dst.Width dst.Height); // At this point the new bitmap has no MimeType // Need to output to memory stream var m = new MemoryStream(); dst.Save(m format); var img = Image.FromStream(m); return img; } } } As you can see I am using the suggested settings for Interpolation Smoothing etc. I am also saving the jpeg with quality 100. The resultant image has noticeable blurring and artefacts even when resizing down to a modest 75% of its original size. I looked around and this is the recommended way. I found a simplistic way to resize and decided to give that a go. Bitmap NewImg = new Bitmap(original new Size(387257)); editor.SaveImage(@""C:\simpleResize.jpg"" NewImg ImageFormat.Jpeg); Surprisingly this produces a much nicer image although it is quite a bit bigger ~30% larger in memory footprint. My question is what's the difference and what setting am I missing on my resize routine that could account for the uglier result. Id like to get my resize routine to produce the exact same result as the simple resize. Your help is much appreciated. This is my first foray into image processing. EDIT Simple Resize (82KB) My Resize (55KB) The dst.Save(m format); looks like your problem. You're encoding it as jpeg there with default quality (not 100%) then immediately decoding it back to an image. dst is already an Image (Bitmap class inherits from Image) so you can just return it as-is. Hi David. You were right. My reasoning behind that step was so that the mimetype was part of the resultant image. I store the image in the session temporarily and wanted to be able to retrieve it later. If you update your answer ill mark it as answered. I found this example snippet of code (sorry I've forgotten the source) that might help you: // Create parameters EncoderParameters params = new EncoderParameters (1); // Set quality (100) params.Param[0] = new EncoderParameter(Encoder.Quality 100); // Create encoder info ImageCodecInfo codec = null; foreach (ImageCodecInfo codectemp in ImageCodecInfo.GetImageDecoders()) { if (codectemp.MimeType == ""image/jpeg"") { codec = codectemp; break; } } // Check if (codec == null) throw new Exception(""Codec not found for image/jpeg""); // Save image.Save(filename codec params); Nice. It preserves the quality of the image and reduces the image size.  My first thought was that .NET's jpeg encoder uses chroma subsampling even at the highest quality setting so color information is stored at half resolution. The setting is hard-coded as far as I can tell. But that wouldn't explain why you would get better quality in the second example. Unless maybe in the 2nd it didn't use antialiasing giving a sharper (but lower quality) image and the artifacts went unnoticed. Edit: The dst.Save(m format); looks like your problem. You're encoding it as jpeg there with default quality (not 100%) then immediately decoding it back to an image. dst is already an Image (Bitmap class inherits from Image) so you can just return it as-is Thanks for a quick response. I have tried changing the AA settings to see if it helps but it doesn't seem to be the cause of the artefacts. I'm flipping back and fourth between the simple resize and the complex and the artefacts are only present on the complex. Anyone know what the default quality is?"
81,A,"Does computer graphics have a practic use besides games/movies/pictures? I am learning Computer Graphics at the university and I'm trying to figure out the use of it ... and I fail. Only games and movies stick in to my mind I'm sure there are other uses for let's say graphic algorithms openGL 2D and 3D 3ds Max ... Is it useful later on ? Where do I need it ? Thank you Well ""only games and movies"" accounts for the largest value in exports from the USA so even if there were absolutely no other applications for graphics (which of course there are) it would still be a big area of opportunity for employment. First of all there is of course 3D: CAD (did you know that today all cars planes boats or actually anything is designed via a computer and a CAD software?) Scientific visualization terrain visualization (think of google-earth) scientific simulations (any kinds of simulations actually from fluids to sounds or deformations molecules and so on...) medical visualization and assistance during surgery etc... User interfaces (not only 3D on the screen but also new input devices need to be designed) ... And of course 2D is part of Computer Graphics. Think of digital photography image filter algorithms real-time encoding and decoding of highly compressed video digital cameras chips etc... List is endless. Scientific visualization see kitware.com .. However I think gaming / movie is the biggest business. In terms of revenue i think the CAD business is much more. But we'd need figures :)  Medical and scientific visualization simulations preservation of cultural heritage come to mind.  In addition to what the others have said think about simulators. Flights probably have to be simulated these days in order to avoid crashes. While it can be done without any representaton people will need to see the output in the form of some visual representation and thus you need graphics. Even this website has graphics on it. Depending on the browser it might not be hardware accelerated but it is a graphcs context. Even console based applications provide you with text that is represented visually and that's because human eyes are the easiest way of offering output.  How about the 3d CAD software that engineers and architects use?"
82,A,Direct video card access Guys I am trying to write a class in C# that can be used as a direct replacement for the C# Bitmap class. What I want to do instead though is perform all graphic functions done on the bitmap using the power of the video card. From what I understand functions such as DrawLine or DrawArc or DrawText are primitive functions that use simple cpu math algorithms to perform the job. I instead want to use the graphics card cpu and memory to do these and other advanced functions such as skinning a bitmap (applying a texture) and true transparancy. My problem is in C# how do I access direct video functions? Is there a library or something I need? DirectX or OpenGL. I prefer OpenGL personally especially for the level of manipulation that you're talking about. Do you want do do this behind the scenes or in real time (say for a game or photo-editing program?) That may make a difference in what library you should use. OpenGL and DirectX are pretty much the simplest ways to go about this. They wrap they slightly more complex video driver interface which if you're asking this question you don't want to get into. OpenGL isn't complex at all and their are many many tutorials available online. I prefer to do this behind the scenes. Is DirectX and OpenGL the only ways to go? I was hoping for something without the complexity of these libraries. @AndreasReiff CUDA and C++ are really only intended for general computation though with CUDA/OpenGL interop you can effectively write shaders in either CL or CUDA. But both DX and GL allow you to read anything back from the GPU that you've generated. Guys used to do general computation in OpenGL and GLSL before the other languages were available. Can you do that with Open GL and DirectX?? I thought you would need something like C++ Amp or Cuda or similar to get the result back to the CPU?  You mean like DirectX? I would prefer not to deal with the complexity of DirectX. I want to be able to call MyNewBitmapClass.DrawLine(5101520) and have it hide all those complex details @icemanind that's exactly what you could do with DirectX put it as what happens inside your DrawLine method. Regardless you basically need to choose either DirectX or OpenGL there's no way a single person could ever hope to achieve the capabilities you need within any reasonable amount of time without those frameworks. @Chris Thanks I guess I will brush up on my DirectX skills. Such a scary world to enter! lol
83,A,"Custom Play Pause Button Iphone I'd like to ask what is the best way to achieve the following coding with cocoa for IPhone. As a learning exercise I'd like to make a very simple mp3 player. The part I'm most interested in at the moment is the custom visual components for this player. In particular how would I make a custom play pause button with mouseover and mousedown states? In addition I would like to have a ""drawer"" that opens and closes depending on the play/pause button is showing its play or pause state. When play is hit the buttons icon changes to a pause icon and the drawer slides open. When pause is hit the pause icon changes to a play icon and the drawer closes. SHould I be looking at quartz for this? I've taken on your suggestions and have visited the dev reference center. Changing the state of the play pause pause button has been a breeze. However adding my panel subviews to my UIVIEWController and animating is proving very difficult for me (objective -c novice). First of all when I try to make my panel views I get warnings saying that the panel UIView may not respond to initWithFrame. (initwithframe is a function of UIView ?) Secondly I can't add an image to my panel UIView. I get warning saying warning: passing argument 1 of 'addSubview:' from distinct Objective-C type . Finally when trying to apply animation to my panel I get errors saying UIView' may not respond to '-commitAnimations' and any other animation related method I try to add. Can you help? I think mainly I don't understand why I'm getting these ""may not respond to method"" warnings seeing as I'm calling the method on the correct type (or so I think) // Implement viewDidLoad to do additional setup after loading the view typically from a nib. - (void)viewDidLoad { [super viewDidLoad]; // SET UP THE PLAY PAUSE BUTTON playPause = [UIButton buttonWithType:UIButtonTypeCustom]; playPause.frame = CGRectMake(-20280 100100); [playPause setImage:[UIImage imageNamed:@""play.png""] forState:UIControlStateNormal]; [playPause addTarget:self action:@selector(buttonPushed:) forControlEvents:UIControlEventTouchUpInside]; playing=NO; //SET UP THE AUDIO PLAYER NSLog(@""Set up theAudio""); NSURL *url = [NSURL fileURLWithPath:[NSString stringWithFormat:@""%@/DumpTheChump.mp3"" [[NSBundle mainBundle] resourcePath]]]; NSError *error; audioPlayer = [[AVAudioPlayer alloc] initWithContentsOfURL:url error:&error]; audioPlayer.numberOfLoops = -1; //SETUP THE BOTTOM part of the PANEL UIImage *panelBottomImage=[UIImage imageNamed:@""panel_bottom.png""]; panelBottom=[UIView initWithFrame:(CGRectMake(-20 280 285 128))]; [panelBottom addSubview:panelBottomImage]; //SETUP THE TOP PART OF THE PANEL. (THE ONE THAT SHOULD SLIDE UP AND DOWN) UIImage *panelTopImage=[UIImage imageNamed:@""panel_top.png""]; panelTop=[UIView initWithFrame:(CGRectMake(-20 280 285 128))]; [panelTop addSubview:panelTopImage]; [self.view addSubview:panelBottom]; [self.view addSubview:panelTop]; [self.view addSubview:playPause]; // 285*128 } - (void) buttonPushed:(id)sender { NSLog(@""It works!""); switch (playing) { case NO: playing=YES; [audioPlayer play]; [playPause setImage:[UIImage imageNamed:@""pause.png""] forState:UIControlStateNormal]; [panelTop beginAnimations:nil context:nil]; [panelTop setAnimationDuration:2]; [panelTop setAnimationDelegate:self]; //UIView setAnimationDidStopSelector:@selector(onAnimationC omplete:finished:context; // set the position where button will move to panelTop.frame = CGRectMake(-20 100 285 128); [panelTop commitAnimations]; break; case YES: playing=NO; [audioPlayer pause]; [playPause setImage:[UIImage imageNamed:@""play.png""] forState:UIControlStateNormal]; [playPause setImage:[UIImage imageNamed:@""pause.png""] forState:UIControlStateNormal]; [panelTop beginAnimations:nil context:nil]; [panelTop setAnimationDuration:2]; [panelTop setAnimationDelegate:self]; //UIView setAnimationDidStopSelector:@selector(onAnimationC omplete:finished:context; // set the position where button will move to panelTop.frame = CGRectMake(-20 280 285 128); [panelTop commitAnimations]; break; default: break; } }  No need to use Quartz. Start with UIButton. You can assign just one image to a button (and it will handle highlighting it on touch) or assign a different image for each button state if you want to customize how it looks when highlighted (setImage:forState:). To achieve the toggle effect between play and pause you should replace the button's images whenever it is tapped (in your action for UIControlEventTouchUpInside). For the drawer animation create a UIView that will serve as the drawer and add it to your main view. Adjust its frame so that it is placed offscreen. Then when you want to animate it onto the screen use a simple animation block: [UIView beginAnimations:nil context:nil]; drawerView.center = CGPointMake(...); // set coordinate to the end position [UIView commitAnimations]; Try to read up on all this in the documentation. Then ask more specific questions if you still have problems. Thats a great starting point. You've saved me a lot of barking up the wrong tree time"
84,A,How to prevent InvalidOperationException when using System.Drawing.Graphics? I am using the System.Drawing.Graphics.DrawLines(Pen pen PointF[] points) method in a multithreaded application but the System.Drawing.Graphics isn't shared between threads. Why it keeps throwing the System.InvalidOperationException: The object is currently in use elsewhere ? Simple answer: don't do that. Only access the GUI on the GUI thread. It can happen in a GUI project. Please keep your answer so other users can benefit for it.  The problem was: I was using the same System.Drawing.Pen instance for all threads. I had to clone it for every thread in order to solve the problem. var pens = new Pen[0]; lock (this._pens) { pens = (Pen[])this._pens.Select(a => (Pen) a.Clone()).ToArray(); } Even the lock is essential in order to solve this problem
85,A,"Ruby Programmer collaboration with Graphics Designer: Best practices? I've been programming for many years and have just recently got the RoR ""itch"". I'm thinking about how I could utilize a graphic designer and wondered how best to collaborate with them: 1) Do scaffolding and then show them where to place the graphics in the views (seems ugly) 2) Have them do screens and make them keep a certain dir structure for all media links i.e. /public/images public/stylesheets etc. Will this confuse the graphics designer LOL? 3) Better to just have them build the images (like a header or footer image etc.) and place them in yourself. But this approach means you have to start thinking about design and layout which seems better left to the graphics specialist. All of the above seem flawed to me and I was wondering if there are any ""best practices"" for this problem? Searched for articles etc. but didn't find much. Suggestions? Thanks all. i work in a company that works 100% with ruby on rails development. which approach we take to integrate views. First thing that we develop are views. The design team develop a view tell us which data will be filled in this view and our development team develop the necessary ERB code. So normally a development process are like: We develop some views with the client Our dev team do the spec for functions The design give to us the application layout We implement it and start to work in the first models and controllers spec'ed New whishes and views will be asked by the client that will be sent by our design team and it will be spec'ed by our dev team.  Normally give them their own rails server teach them the basics of where they need to add controllers and views (no need for any model stuff) to work with the default route. I integrate the HTML they stick in the CSS. Teaching them how to render partials helps them with consistency. Think that's about it. Sounds like you have a pretty technical set of designers! I can't imagine having them even touching the controllers! Our designers can understand Ruby albeit they don't like HAML and SASS but I don't blame them for that. The key to understand here is that developers shouldn't lead the direction for the website they tend to have an overall better grip over web standards more so than developers as well as having the creative/artistic flair which a lot of developers lack. Sure it's good to help designers sometimes with some technical decisions but you kinda want the UX/UI people doing the work with regards to assisting the client during paper prototyping and site story collection."
86,A,"Need a function to limit a line (known by its coordinates) in its length I need a function which takes a line (known by its coordinates) and return a line with same angle but limited to certain length. My code gives correct values only when the line is turned 'right' (proven only empirically sorry). Am I missing something? public static double getAngleOfLine(int x1 int y1 int x2 int y2) { double opposite = y2 - y1; double adjacent = x2 - x1; if (adjacent == Double.NaN) { return 0; } return Math.atan(opposite / adjacent); } // returns newly calculated destX and destY values as int array public static int[] getLengthLimitedLine(int startX int startY int destX int destY int lengthLimit) { double angle = getAngleOfLine(startX startY destX destY); return new int[]{ (int) (Math.cos(angle) * lengthLimit) + startX (int) (Math.sin(angle) * lengthLimit) + startY }; } BTW: I know that returning arrays in Java is stupid but it's just for the example. Oh you're right! it's from some older code. Your check for NaN will always return false as nothing is ever equal to NaN. Also returning 0 for error is bad as 0 is a valid angle. It would be easier to just treat it as a vector. Normalize it by dividing my its magnitude then multiply by a factor of the desired length. In your example however try Math.atan2. Exactly atan2 is the method to use here. It has more information to work with since it gets x and y as separate arguments and can correctly return angles from -π to +π. atan only has enough information to cover the a range of -π/2 to π/2.  It's an easy problem if you understand something about vectors. Given two points (x1 y1) and (x2 y2) you can calculate the vector from point 1 to 2: v12 = (x2-x1)i + (y2-y2)j where i and j are unit vectors in the x and y directions. You can calculate the magnitude of v by taking the square root of the sum of squares of the components: v = sqrt((x2-x2)^2 + (y2-y1)^2) The unit vector from point 1 to point 2 equals v12 divided by its magnitude. Given that you can calculate the point along the unit vector that's the desired distance away by multiply the unit vector times the length and adding that to point 1.  In Python because I don't have a Java compiler handy: import math def getLengthLimitedLine(x1 y1 x2 y2 lengthLimit): length = math.sqrt((x2-x1)**2 + (y2-y1)**2) if length > lengthLimit: shrink_factor = lengthLimit / length x2 = x1 + (x2-x1) * shrink_factor y2 = y1 + (y2-y1) * shrink_factor return x2 y2 print getLengthLimitedLine(10 20 25 -5 12) # Prints (16.17 9.71) which looks right to me 8-) @duffymo: Good point about abs() - I've removed it. But I think the 'if' is necessary otherwise it'll *grow* the line when it's shorter than the limit. +1 - I'm starting to greatly appreciate the brevity and elegance of Python. Nice. No need for the ""if"" check on length - it's valid for all situations except length == 0. That's the check you need to make. No need for abs() either since squares are always positive. Yes it will but that's not necessarily an incorrect outcome. It only fails to meet the narrow requirements laid down by the poster. It's equally valid to extend the line in another use case.  Just use the Pythagorean theorem like so: public static int[] getLengthLimitedLine(int start[] int dest[] int lengthLimit) { int xlen = dest[0] - start[0] int ylen = dest[1] - start[1] double length = Math.sqrt(xlen * xlen + ylen * ylen) if (length > lengthLimit) { return new int[] {start[0] start[1] start[0] + xlen / lengthLimit start[1] + ylen / lengthLimit} } else { return new int[] {start[0] start[1] dest[0] dest[1];} } }  No need to use trig which can have some nasty edge cases. Just use similar triangles: public static int[] getLengthLimitedLine(int startX int startY int destX int destY int lengthLimit) { int deltaX = destX - startX; int deltaY = destY - startY; int lengthSquared = deltaX * deltaX + deltaY * deltaY; // already short enough if(lengthSquared <= lengthLimit * lengthLimit) return new int[]{destX destY}; double length = Math.sqrt(lengthSquared); double newDeltaX = deltaX * lengthLimit / length; double newDeltaY = deltaY * lengthLimit / length; return new int[]{(int)(startX + newDeltaX) (int)(startY + newDeltaY)}; }  Encapsulate Line in a class add a unit method and a scale method. public class Line { private float x; private float y; public Line(float x1 float x2 float y1 float y2) { this(x2 - x1 y2 - y1); } public Line(float x float y) { this.x = x; this.y = y; } public float getLength() { return (float) Math.sqrt((x * x) + (y * y)); } public Line unit() { return scale(1 / getLength()); } public Line scale(float scale) { return new Line(x * scale y * scale); } } Now you can get a line of arbitrary length l by calling Line result = new Line(x1 x2 y1 y2).unit().scale(l);"
87,A,"If window spans multiple monitors I can't draw to it If I have a window that spans both monitors on a multimonitor system I can't seem to erase (paint black) the entire window. Instead only the primary window is drawn black. The secondary remains the original white color. Has anyone seen this behavior? wxwidgets: wxClientDC dc(this); Erase(dc); void SpriteWindowFrame::Erase(wxDC& dc) { dc.SetBackground(*wxBLACK_BRUSH); dc.SetBrush(*wxBLACK_BRUSH); dc.Clear(); //wxLogDebug(""Erase called. Rect is %i %i w:%i h:%i"" GetPosition().x GetPosition().y GetSize().GetWidth() GetSize().GetHeight()); } Inside dc.Clear() function there is this code wxwidgets: void wxDC::Clear() { WXMICROWIN_CHECK_HDC RECT rect; if ( m_canvas ) { GetClientRect((HWND) m_canvas->GetHWND() &rect); } else { // No I think we should simply ignore this if printing on e.g. // a printer DC. // wxCHECK_RET( m_selectedBitmap.Ok() wxT(""this DC can't be cleared"") ); if (!m_selectedBitmap.Ok()) return; rect.left = -m_deviceOriginX; rect.top = -m_deviceOriginY; rect.right = m_selectedBitmap.GetWidth()-m_deviceOriginX; rect.bottom = m_selectedBitmap.GetHeight()-m_deviceOriginY; } #ifndef __WXWINCE__ (void) ::SetMapMode(GetHdc() MM_TEXT); #endif DWORD colour = ::GetBkColor(GetHdc()); HBRUSH brush = ::CreateSolidBrush(colour); ::FillRect(GetHdc() &rect brush); ::DeleteObject(brush); #ifndef __WXWINCE__ int width = DeviceToLogicalXRel(VIEWPORT_EXTENT)*m_signX height = DeviceToLogicalYRel(VIEWPORT_EXTENT)*m_signY; ::SetMapMode(GetHdc() MM_ANISOTROPIC); ::SetViewportExtEx(GetHdc() VIEWPORT_EXTENT VIEWPORT_EXTENT NULL); ::SetWindowExtEx(GetHdc() width height NULL); ::SetViewportOrgEx(GetHdc() (int)m_deviceOriginX (int)m_deviceOriginY NULL); ::SetWindowOrgEx(GetHdc() (int)m_logicalOriginX (int)m_logicalOriginY NULL); #endif } Using the debugger I checked what GetClientRect returned and sure enough it returns a rectange with location 0 and width/height of the combined two monitors so it's right. Maybe fillrect function is not capable of drawing to two displays? Can you trace into the constructor of the wxClientDC? wxClientDC dc(this); A lot depends on what type of DC wx has given you. The windows API to retrieve a window DC is hdc = GetDC(hwnd) and on multimonitor systems it retrieves a handle to a 'mirror driver' DC thats meant to reflect calls to all the underlying display device DCs that the monitor spans. The only possible reason I can think of for this behaviour is wx is somehow retrieving a display DC rather than a window DC.  I'm sure Chris is correct that the ""overlapping window"" case is handled somewhere for you. But where? Rendering with windows GDI and ""display contexts"" such as you mention is very primitive and prone to all sorts of problems. GDI is one of poorest interfaces ever seen poor even for Microsoft. Since most ""window"" programs work OK on multiple monitors think of animating things in a ""window"" - and how that ""window"" makes its way to the ""display"" is best left a mystery. Maybe DC is fundamentally not multi-monitor capable. Look for anything that allows multiple DCs to be treated uniformly. Rending graphics onto a grid of paper sheets would be like a tiled ""printer DC"". A video wall would be a tiled ""display DC"" and you would be happy with a 2-monitor hack i.e. ""multimon dc"" echoes to ""owning"" display and ""another one"" if a window spans both. If you want to do ""real"" animation on windows you will need to move to DirectX. It is also a lot to learn but much more capable: scene graphs textures video alpha channels ..."
88,A,"Create Editable plots from R I'm creating a series of plots in R (I'm using ggplot2 but that's not essential) and I want to be able to save my output so I can then edit it for furthur use For instance I might want to move legends about or adjust colours etc. I have seen that ggplot2 has a save command but that seems to produce pdf's or bitmaps neither of which are particularly editable How do other people do this ? Any good ideas ? Here's some sample code to produce a sample plot; library(ggplot2) dataframe<-data.frame(fac=factor(c(1:4))data1=rnorm(400100sd=15)) dataframe$data2<-dataframe$data1*c(0.250.50.751) dataframe testplot<-qplot(x=fac y=data2data=dataframe colour=fac geom=c(""boxplot"" ""jitter"")) testplot Thanks Paul. With ggplot and lattice you can use save to save the plot object to disk and then load it later and modify it. For example: save(testplot file = ""test-plot.rdata"") # Time passes and you start a new R session load(""test-plot.rdata"") testplot + opts(legend.position = ""none"") testplot + geom_point()  Other editable formats: Take a look at help(devices) for other formats which are available: these include svg pictex and xfig all of which are editable to greater or lesser extents. Note that PDFs can be edited for instance using the Omnigraffle tool available for Apple's OSX. Other ways to record plot data: In addition you can record R's commands to the graphics subsystem for repeating it later - take a look at dev.copy:  Most devices (including all screen devices) have a display list which records all of the graphics operations that occur in the device. 'dev.copy' copies graphics contents by copying the display list from one device to another device. Also automatic redrawing of graphics contents following the resizing of a device depends on the contents of the display list. Using Rscript to create a repeatable editable plot: I typically take a third strategy which is to copy my R session into an Rscript file which I can run repeatedly and tweak the plotting commands until it does what I want: #!/usr/bin/Rscript x = 1:10 pdf(""myplot.pdf"" height=0 width=0 paper=""a4"") plot(x) dev.off();  Thanks for the answers I've played around with this and after some help from my friend Google I found the Cairo package which allows creation of svg files I can then edit these in Inkscape. library(Cairo) Cairo(600600file=""testplot.svg""type=""svg""bg=""transparent""pointsize=8 units=""px""dpi=400) testplot dev.off() Cairo(12001200file=""testplot12200.png""type=""png""bg=""transparent""pointsize=12 units=""px""dpi=200) testplot dev.off() Now I just have to play around with the various settings to get my plot as good as it can be before writing the file. As far as I know Inkscape can also read .pdf now."
89,A,"Procedural skydome Does anybody know how to or where I can find info related on how to do a procedural skydome? Any help is welcome. See this thread from GameDev. There's some example code in C++ in there too. I don't think they're discussing what I've asked. I want to generate a skydome mesh procedurally not to change the texture that's applied nor calculate it automatically (though I may do something like that in the future so it's an interesting read anyway thanks for that.) Why do you want to change the mesh? the whole point of a skydome is to be arbitrarily far away. So almost by definition the mesh isn't really important: you choose a simple fixed set of polys (e.g. a giant box) and use textures to draw the skydome. If that's not what you mean by a ""procedural skydome"" what *do* you mean? Er let me correct myself: the mesh can be in the shape of a sphere (which is what makes it a dome) but generally you will use textures in the shape of a giant box. These textures are what determines the color of the sky at any particular point -- so when you asked for info on a ""procedural skydome"" we assumed you wanted to replace these textures with a procedural shader of some sort. I wasn't sure how to call it I'm sorry if the question is confusing. What I want is to create a sphere procedurally; I know what a skydome is. I'm also going to do some sort of procedural shader to apply to the skydome but I'm going to do it after and I have it more or less figured out. I'm still kind of a newbie on computer graphics with much to read and learn sorry if my question was confusing.  A skydome is simply a sphere drawn around the entire level. Just draw a sphere make sure back face culling is off and front face culling is on (since you're inside the sphere). To procedurally generate a sphere is trivial my usual approach is to start with a hardcoded Icosahedron and subdivide the faces until the required detail is reached. There is a thread on gamedev about generating a sphere: http://www.gamedev.net/community/forums/topic.asp?topic%5Fid=537269 I'm not sure that really answers your question seeing your response to the other answer makes me think there is some confusion about what a skydome is. To reiterate it's just a sphere the important bit is the texture you draw on it."
90,A,"Draw transparent arcs not images in J2ME? I've been happily coding an opensource game for fun but I just realized that Graphics.setColor() doesn't understand 0xAARRGGBB. I want to get a more and more transparent colour in which I'd draw some arcs (discs). My goal was to make them shine though each other like in this image: http://kenai.com/projects/xplode (the logo there) I know that transparency manipulation is possible with images (getRGB etc) yet this proves very slow for a game with lots of size and transparency changes. Is there any way i could draw transparent scalable discs in J2ME without using JSR 184 nor JSR-226? I fear that there is now ""good"" way to deal with it and I'll just go on without this effects but why not ask when in need ? I am not sure if you can do such a thing in a reasonably efficient manner using the Graphics class alone. Why don't you try OpenGL ES Specification for MIDP JSR 239?  Instead of creating images each frame with createRGBImage() you can try using getRGB() and drawRGB()  Transparency stuff doesn't need to be slow as long as you keep it to a minimum and stay away from very old handsets. You should also note that alpha is sometimes ignored on some implementations and sometimes quantized to ugly levels (Some motorola phones snap alpha values to the nearest 2-bit value. Bleh). I'd be tempted to approach this as a single transparent layer superimposed onto the screen. Keep an int array the size of your screen into which you will plot your transparent pixels (int[width * height]). On each frame make a call to Image.createRGBImage and draw that image to the screen. As an experiment you might first want to make sure that the alpha processing isn't going to slow you down by filling this array with a constant value and plotting that to the screen. E.g. 0x80FF0000 should blend a red tint onto the screen and you can see easily if this alpha business is going to be workable on your chosen handset i.e you can measure how it impacts your frame rate. Next you need to plot circles into this array. Look for Bresenham's circle drawing algorithm and rework it to plot into an int array of pixels. You might want to maintain a lookup table that has a pre-calculated radius/sqrt(2) for each radius you want to be able to plot. Next is how to blend colours to get them to overlap. The simplest would be just to plot a colour value with alpha into the pixel. You wouldn't get colour blending with already plotted circles but it would be cheap and the circles would blend with the background. Another method would be to ignore the colour blend but to do a simple merge on the alpha. E.g. newColour = ((destColour & 0xFF000000) + 0x20000000) | srcColour; /* Where source colour is of the form 0x00RRGGBB. Note: Adding 0x20 to alpha each * time will only work for 7 overlapping circles. */ The colour of the overlapping circle would dominate the underlying one but the alpha would become more opaque where they overlap which would at least allow the player to see all the circles properly. If you wanted to do an accurate colour blend then the calculations are a lot more complicated especially if you're blending one value with alpha onto another with alpha since the colour contributions from each pixel vary with the proportion of alpha in each colour. It's not impossible but making it efficient is a pain. I'd be tempted to go for a cheaper option. Great idea thanks! I'll write some sample code right away to test if the devices can handle such an approach."
91,A,"setOpaque(true/false); Java In Java2D when you use setOpaque I am a little confused on what the true and false does. For example I know that in Swing Opaque means that when painting Swing wont paint what is behind the component. Or is this backwards? Which one is it? Thanks The short answer to your question is that ""opaque"" is defined in English as completely non-transparent. Therefore an opaque component is one which paints it's entire rectangle and every pixel is not at all translucent to any degree. However the Swing component opacity API is one of those mis-designed and therefore often mis-used APIs. What's important to understand is that isOpaque is a contract between the Swing system and a particular component. If it returns true the component guarantees to non-translucently paint every pixel of it's rectangular area. This API should have been abstract to force all component writers to consider it. The isOpaque API is used by Swing's painting system to determine whether the area covered by a given component must be painted for components which overlap it and which are behind it including the component's container and ancestors. If a component returns true to this API the Swing system may optimize painting to not paint anything in that area until invoking the specific component's paint method. Because of contractual implication of isOpaque the API setOpaque should not exist since it is actually incorrect for anything external to call setOpaque since in turn the external thing can't know whether the component in question will (or even can) honor it. Instead isOpaque should have been overridden by each concrete component to return whether it actually is in fact opaque given it's current properties. Because the setOpaque API does exist many components have mis-implemented it (quite understandably) to drive whether or not they will paint their ""background"" (for example JLabel and JPanel filling with their background color). The effect of this is to create an impression with users of the API to think that setOpaque drives whether or not that background should paint but it doesn't. Furthermore if say you wish to paint a JLabel with a translucent background you need to set a background color with an alpha value and do setOpaque(true) but it's not actually opaque - it's translucent; the components behind it still need to paint in order for the component to render properly. This problem was exposed in a significant way with the Java 6's new Nimbus Look & Feel. There are numerous bug reports regarding transparent components filed against Nimbus (see stack overflow question Java Nimbus LAF with transparent text fields). The response of the Nimbus development team is this : This is a problem [in] the orginal design of Swing and how it has been confusing for years. The issue is setOpaque(false) has had a side effect in [existing] LAFs which is that of hiding the background which is not really what it is [meant] for. It is [meant] to say that the component may have transparent parts and [Swing] should paint the parent component behind it. So in summary you should not use setOpaque. If you do use it bear in mind that the combination of some Look & Feels and some components may do ""surprising"" things. And in the end there is actually no right answer. Powerful answer ! Late to the party here but thanks for explaining the relationship between isOpaque and alpha channel. I've been stumped because they seem to cover the same area -- it's helpful to know that's because isOpaque is goofy. :)  javadoc says : If true the component paints every pixel within its bounds. Otherwise the component may not paint some or all of its pixels allowing the underlying pixels to show through. try this example program too... http://www.java2s.com/Code/JavaAPI/javax.swing/JPanelsetOpaquebooleanisOpaque.htm When true that means content behind the object correct? when true the components behind are not shown.. when false its shown.. *we needn't see javadoc for this an english dictionary is enough* :) :) Yes that is what I thought but after reading many things I was getting confused. Thanks :)"
92,A,Rotation and Scaling -- How to do both and get the right result? I've got a set of Java2D calls that draw vectors on a graphics context. I'd like for the image to be doubled in size and then rotated 90 degrees. I'm using the following code to do this: Graphics2D g2 = // ... get graphics 2d somehow ... AffineTransform oldTransform = g2.getTransform(); AffineTransform newTransform = (AffineTransform)oldTransform.clone(); newTransform.concatenate(AffineTransform.getTranslateInstance(x1 x2)); newTransform.concatenate(AffineTransform.getScaleInstance((double)newW/(double)iconW (double)newH/(double)iconH)); newTransform.concatenate(AffineTransform.getRotateInstance(Math.toRadians(rotationAngle) (double)iconW/2.0d (double)iconH/2.0d)); // ... do my drawing ... This rotates and scales however the scale isn't applied the way I would like. It is as if it is rotated before scaling thus making the image wider on the wrong axis. Is there a better way to do this? Change the order in which you concatenate the transforms to control the order in which they are applied in the composite.  I believe those transforms are implemented like a stack - so the last transform is performed first. Try reversing the order of the rotate and scale transformations and you should get what you are looking for. newTransform.concatenate(AffineTransform.getTranslateInstance(x1 x2)); newTransform.concatenate(AffineTransform.getRotateInstance(Math.toRadians(rotationAngle) (double)iconW/2.0d (double)iconH/2.0d)); newTransform.concatenate(AffineTransform.getScaleInstance((double)newW/(double)iconW (double)newH/(double)iconH)); Ok this works but I don't understand it. Why is it ok to apply the translate first? Ie if translate is first it happens first. But rotate happens after scale even when rotation is applied first? There's something there I'm not understanding. Hard to explain exactly why the translate will still work correctly but if you look at the underlying matrix math you'll see why it makes sense.  Rotations are always performed about the origin. In order to rotate about a certain point you must translate the points. This page explains the maths behind what you're trying to do and show why transformations need to be applied in a certain order. The second and third arguments to getRotateInstance manage the transformation. This wasn't the same problem.
93,A,iPhone OpenGL ES 2d background texture i have a 1024 x 1024 image I use for a texture in my game for the background. Im wondering if their is a proper way to handle drawing a large background texture. How I am doing it currently: texCoord { 00100111 } vertice { 000heightwidth0widthheight } texCoordPointer(texCoord) vertexPointer(vertice) bind the texture enable client (texCoordArr vertexCoordArr) drawArray disable client (texCoordArr vertexCoordArr) That's fine... I don't know if the GL|ES on the iPhone supports the glDrawTexOES extension but if it does you may safe some lines of code. It won't make drawing any faster though. Also some additional hints: try to make the texture exactly as large as the screen. There is no need to store the image in 1024*1024 if the real resolution is more around 480*320. If you zoom or pan the image it's another thing of course. You may save quite a bit of memory if you don't upload mipmaps for the backdrop. Textures are required to be the power of two (64x64 256x256 etc) up until 2.2.1 not sure about 3.0 (it does show variable-sized textures in the 3.0 simulator can't test it on the device yet as I'm still on 2.2.1). Yep. It's mandatory to either pad the images to a power of two or split them into smaller chunks.
94,A,"what is the idea behind scaling an image using lanczos? I'm interested in image scaling algorithms and have implemented the bilinear and bicubic methods. However I have heard of the lanczos and other more sophisticated methods for even higher quality image scaling and I am very curious how they work. Could someone here explain the basic idea behind scaling an image using lanczos (both upscaling and downscaling) and why it results in higher quality? I do have a background in fourier analysis and have done some signal processing stuff in the past but not with relation to image processing so don't be afraid to use terms like ""frequency response"" and such in your answer :) EDIT: I guess what i really want to know is the concept and theory behind using a convolution filter for interpolation. (Note: i have already read the wikipedia article on lanczos resampling but it didn't have nearly enough detail for me) thanks alot! have you looked at any example code? I don't know how readable e.g. mplayer's swscale implementation is. But you're probably looking for the theory behind it which I don't know either. Interested readers will find an illustrated extended examination of this question on our sister site at http://gis.stackexchange.com/a/14361 There's an excellent article [comparing resampling algorithms](http://www.panotools.org/dersch/interpolator/interpolator.html) although its test is rotation rather than scaling. The selection of a particular filter for image processing is something of a black art because the main criterion for judging the result is subjective: in computer graphics the ultimate question is almost always: ""does it look good?"". There are a lot of good filters out there and the choice between the best frequently comes down to a judgement call. That said I will go ahead with some theory... Since you are familiar with Fourier analysis for signal processing you don't really need to know much more to apply it to image processing -- all the filters of immediate interest are ""separable"" which basically means you can apply them independently in the x and y directions. This reduces the problem of resampling a (2-D) image to the problem of resampling a (1-D) signal. Instead of a function of time (t) your signal is a function of one of the coordinate axes (say x); everything else is exactly the same. Ultimately the reason you need to use a filter at all is to avoid aliasing: if you are reducing the resolution you need to filter out high-frequency original data that the new lower resolution doesn't support or it will be added to unrelated frequencies instead. So. While you're filtering out unwanted frequencies from the original you want to preserve as much of the original signal as you can. Also you don't want to distort the signal you do preserve. Finally you want to extinguish the unwanted frequencies as completely as possible. This means -- in theory -- that a good filter should be a ""box"" function in frequency space: with zero response for frequencies above the cutoff unity response for frequencies below the cutoff and a step function in between. And in theory this response is achievable: as you may know a straight sinc filter will give you exactly that. There are two problems with this. First a straight sinc filter is unbounded and doesn't drop off very fast; this means that doing a straightforward convolution will be very slow. Rather than direct convolution it is faster to use an FFT and do the filtering in frequency space... However if you actually do use a straight sinc filter the problem is that it doesn't actually look very good! As the related question says perceptually there are ringing artifacts and practically there is no completely satisfactory way to deal with the negative values that result from ""undershoot"". Finally then: one way to deal with the problem is to start out with a sinc filter (for its good theoretical properties) and tweak it until you have something that also solves your other problems. Specifically this will get you something like the Lanczos filter: Lanczos filter: L(x) = sinc(pi x) sinc(pi x/a) box(|x|<a) frequency response: F[L(x)](f) = box(|f|<1/2) * box(|f|<1/2a) * sinc(2 pi f a) [note that ""*"" here is convolution not multiplication] [also I am ignoring normalization completely...] the sinc(pi x) determines the overall shape of the frequency response (for larger a the frequency response looks more and more like a box function) the box(|x|<a) gives it finite support so you can use direct convolution the sinc(pi x/a) smooths out the edges of the box and (consequently? equivalently?) greatly improves the rejection of undesirable high frequencies the last two factors (""the window"") also tone down the ringing; they make a vast improvement in both the perceptual artifact and the practical incidence of ""undershoot"" -- though without completely eliminating them Please note that there is no magic about any of this. There are a wide variety of windows available which work just about as well. Also for a=1 and 2 the frequency response does not look much like a step function. However I hope this answers your question ""why sinc"" and gives you some idea about frequency responses and so forth. a great and detailed answer :) thankyou very much :)) I know this answer is very old by now but I do wonder: Is it really desirable to downscale images with a frequency box filter? Wouldn't that intrinsically mean that such things as one-pixel outlines are eliminated (by the ideal filter)? Is that ever desirable or are sinc-based filters only really good for images that can be considered ""continuous"" in nature (such as raster-sampled photographs) and never for images that are more pixel-arty in nature? One-pixel outlines would not disappear but they do yield a particularly clear version of the visually obnoxious ""ringing"" effect described above when put through a pure sinc filter. Lanczos-style filters are better in this respect but you are correct that sinc-based filters are most appropriate for ""continuous"" images -- pixel-art-style images don't do well with *any* kind of linear resizing filter due to their reliance on sharp horizontal and vertical edges.  Did this other question help: http://stackoverflow.com/questions/943781/where-can-i-find-a-good-read-about-bicubic-interpolation-and-lanczos-resampling HI thanks :) that is a good answer that does answer about some of what i wanted to know but im still curious about why the sinc function was chosen and in general about the use of convolution filters for interpolation and image scaling. I would like an explanation the explains their use in terms of their frequency response / fourier transform"
95,A,Resizing an image in c# without losing lower an right pixel-parts I have a c#-class that looks roughly like this:  class ImageContainer { Image image; internal ImageContainer getResized(int width int height) { Bitmap bmp = new Bitmap(width height); //Create a System.Drawing.Graphics object from the Bitmap which we will use to draw the high quality scaled image System.Drawing.Graphics gr = System.Drawing.Graphics.FromImage(bmp); //Set the System.Drawing.Graphics object property SmoothingMode to HighQuality gr.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.HighQuality; //Set the System.Drawing.Graphics object property CompositingQuality to HighQuality gr.CompositingQuality = System.Drawing.Drawing2D.CompositingQuality.HighQuality; //Set the System.Drawing.Graphics object property InterpolationMode to High gr.InterpolationMode = System.Drawing.Drawing2D.InterpolationMode.HighQualityBicubic; //Draw the original image into the target Graphics object scaling to the desired width and height System.Drawing.Rectangle rectDestination = new System.Drawing.Rectangle(0 0 width height); gr.DrawImage(image rectDestination 0 0 image.Width image.Height GraphicsUnit.Pixel); //dispose / release resources ImageContainer ic = new ImageContainer(); ic.image = bmp; return ic; } } The resizing works fine but DrawImage doesn't draw the most right an lower pixel-fragments when scaling down an image. the problem is solved by gr.PixelOffsetMode = System.Drawing.Drawing2D.PixelOffsetMode.HighQuality; :) PixelOffsetMode.Half also works.
96,A,"Flex 3 and scaling DropShadowFilter I have an application that requires resizing of a component that will be scaled up and down quite frequently. I noticed that when I scale the component any filter I use on it will not scale with it. I realize this makes sense but I was wondering if Flex had a tool built in that would allow me to scale the filter with the component. I know I can write some actionscript on a custom component and change the scale of the filter properties based off of the component's current scale... Any suggestions? if you define the filters in mxml they should scale automatically. <mx:Canvas width=""300"" height=""300""> <mx:filters> <mx:DropShadowFilter /> </mx:filters> </mx:Canvas> That should produce a canvas that has a basic drop shadow regardless of its size. heres a link to the docs on the various filters: http://livedocs.adobe.com/flex/3/langref/flash/filters/package-detail.html I tried that before and it didn't work. However you're link did help a little bit. It made me realize that if I wanted to implement a scaling function based off the scale of my canvas that I'll need to do it the way I was trying to avoid (data-binding in a custom dropShadow class). I grabbed a quote from the livedocs: ""This filter supports Stage scaling. However it does not support general scaling rotation and skewing. If the object itself is scaled (if scaleX and scaleY are set to a value other than 1.0) the filter is not scaled. It is scaled only when the user zooms in on the Stage.""  My suggestion would be to work with a resizing (allowScale=true maintaintAspectRatio=false) <mx:Image> PNG 24 with the shadow in it. You will save CPU and have much better control on it (you're only adding 5-10KB tops). The dropshadowfilter class was definitely never designed to be animated. Hope this helps."
97,A,How to calculate the Rect of a row of N boxes filling up a length L with S space between them This problem must have been solved a million times but Google has not been my friend. I need to programmatically space a set of boxes to fill a certain length and be separated by a certain distance. This is what I want: Here is what I'm getting: Since I'm working in Objective-C using Core Graphics I need a series of Rects that I can draw or drop an image into. My naive attempt draws a set of boxes with a certain spacing but leaves a space at the end. Here is my code which is in a drawRect: method CGContextRef context = UIGraphicsGetCurrentContext(); CGFloat barStartX = 96; CGFloat barStartY = 64.0; CGFloat barWidth = 16; CGFloat barHeight = 64; CGFloat barGutter = 8; int barSegments = 8; for (int segmentNumber = 0; segmentNumber <= (barSegments - 1); ++segmentNumber) { // get the box rect CGRect segment = CGRectMake(barStartX + (barWidth * segmentNumber) barStartY  barWidth - barGutter barHeight); // plot box CGContextFillRect(context segment); } Before I create an impenetrable monstrosity of one-off code that even I won't understand 6 months from now I'm wondering if there is a general solution to this spacing problem. The answer doesn't have to be in Objective-C as long as it's somewhat C-like. Readability has priority over performance considerations. I would do something like this: CGFloat barStartX = 96; CGFloat barStartY = 64.0; CGFloat barWidth = 128; CGFloat barHeight = 64; ... int numSegments = 8; // number of segments CGFloat spacing = 8; // spacing between segments CGFloat segmentWidth = (barWidth - spacing*(numSegments - 1)) / numSegments; CGRect segmentRect = CGRectMake(barStartX barStartY segmentWidth barHeight); for (int segmentNumber = 0; segmentNumber < numSegments; segmentNumber++) { segmentRect.origin.x = segmentNumber*(segmentWidth + spacing); CGContextFillRect(context segmentRect); } I like to define the rectangle outside of the loop and then in the loop update only the properties of the rectangle that are actually changing. In this case it is only the x coordinate that changes every time so the loop becomes pretty simple. I got your code and increased barWidth to 128 because it's now used as total width instead of single box width. 'segment' should be 'segmentRect'. segmentRect.x should be segmentRect.origin.x. Then it works. @willc2: Glad to hear you got it working and thank you for pointing out my errors. I have changed the code to fix them. :)  Given n boxes of width w and spacing s the total length will be: l = n × w + (n-1) × s You know all the variables. The same formula can be used to place an arbitrary box. From your diagram we can see that the length is the same as the right-edge coordinate of the final box. So you can just use n from 1 through whatever to find the right-edge of all your boxes. Finding left edge from that is trivial: subtract w add 1 (as a box from 80 to 80 is 1px wide). Note that n counts from 1 not 0. You can change the formula of course to count from 0.  Draw blue and pink rectangles separately and draw one last blue rectangle :) (or don't draw one last pink one)  I think that despite your effort this question is a bit unclear. Here's my attempt though. The equation that you describes in the title is: N*x + (N-1)*S = L Solving that for x gives us the width necessarry for each box: x = (L - (N-1)*S) / N This would result in code similar to something like this: CGContextRef context = UIGraphicsGetCurrentContext(); int barSegments = 8; CGFloat barStartX = 96; CGFloat barStartY = 64.0; CGFloat barTotalWidth = 196.0; CGFloat barHeight = 64; CGFloat barGutter = 8; CGFloat barWidth = (barTotalWidth - (barSegments-1)*barGutter) / barSegments; for (int segmentNumber = 0; segmentNumber < barSegments; ++segmentNumber) { // get the box rect CGRect segment = CGRectMake(barStartX + ((barWidth + barGutter) * segmentNumber) barStartY  barWidth barHeight); // plot box CGContextFillRect(context segment); } This is exactly the kind of elegant solution that I knew was out there thank you.
98,A,Fastest possible way to render 480 x 320 background as iPhone OpenGL ES textures I need to display 480 x 320 background image in OpenGL ES. The thing is I experienced a bit of a slow down in iPhone when I use 512 x 512 texture size. So I am finding an optimum case for rendering iPhone resolution size background in OpenGL ES. How should I slice the background in this case to obtain the best possible performance? My main concern is speed. Should I go for 256 x 256 or other texture sizes here? First of all are you testing performance on the Simulator? If so stop immediately and try it on actual hardware. Performance testing on the simulator is worthless. With that out of the way I'm not sure how many of these you've already implemented so I apologize if I'm giving you what you already know: If you're drawing a fullscreen opaque background every frame you may not need a GL_CLEAR call gmaclachlan points out you should clear the buffers every run for performance reasons however this is contrary to some other user experience. This may differ between hardware and iOS revisions; however performance can be tested just by commenting out the line. Check before and after. If you're using the Z-axis at all (ie: 3D objects over a 2D background think New Super Mario Bros) you will want to clear the depth buffer Use texture compression (PVRTC) if the compression artifacts aren't an issue it's hardware accelerated Use a 16-bit texture for the background if you can't use PVRTC Make sure you're not trying to depth test/alpha blend the background if there's nothing under it Use the OES_Draw_Texture extension if you're using fixed-function int rect[4] = {0 0 480 320}; glBindTexture(GL_TEXTURE_2D texBackground); glTexParameteriv(GL_TEXTURE_2D GL_TEXTURE_CROP_RECT_OES rect); glDrawTexiOES(0 0 z 480 320); Beyond that try grabbing just that segment of your code and seeing just how many background per second it's capable of spitting out. Just a note: it's always a good idea to use glClear() at the start of the frame (even if you're going to overwrite the frame entirely) on this hardware - it means that the driver knows no information needs to be kept from the previous frame and saves all the work involved in doing that. Why would it save anything unless you specify kEAGLDrawablePropertyRetainedBacking = YES? I tried testing (on HW) with Instruments and found more utilisation using glClear than without. @gmaclachlan @v01d: what iOS versions and hardware were you using? I reached my GL_CLEAR conclusions with an original iPhone (lowest common denominator) and a target platform of 3.0.0. Since a glClear line is simple enough to comment/uncomment it's worth testing both ways to see if it works. Also added a bit about the 3D-over-2D and the requirement to clear depth if that's in play.
99,A,Is there an automated tool for making photo-mosaics (using images as pixels)? I occasionally see portraits and other images which have been redrawn into an abstract form where each pixel in the redrawn image is actually another much smaller picture. I am looking for a tool (or library) which can perform this type of transformation automatically. Does something like that exist? You got a preferred language? :) Is it something like a picture mosaic like this you are referring to ? http://www.bz-berlin.de/aktuell/berlinmosaik/ Nope. Any language will do as long as the library is stable and works properly. I don't know about any existing tool; but the general algorithm would be to look at each pixel in the source image and from a pool of pictures select one to represent the picture. You would probably want select the image that has the most content of the pixel color. (So when you encounter a blue pixel you select an image to represent it that is mostly blue and so on).  Maybe this can help. Its phpgd and mysql based. Good luck.  You are thinking of a Photomosaic. You can use AndreaMosaic. There is a HowTo here.
100,A,"Deferred Shading DirectX demos? I've been reading a lot about deferred shading and want to try and get into it. Problem is I can't find a sample which demonstrates how deferred shading can support so many lights simultaneously - I found one demo which was very simple with a single light in Code Sampler and an nVidia HDR sample butnothing beyond that. Would anyone know where I should go for a good introductory tutorial (with code) on how to have deffered shading with lighting? I can make it work with one light but one light is a bit too simple (rather obviously :P). Also I only know how to make directional lights in deferred shading code and it's nice an dall but somewhat different to regular ways of rendering lights so I was wondering if there wree tutorials or anything I could find or just reading material that would help me figure out how writing shaders and special fx in deferred rendering works? Thanks fo rany help! NVIDIA stuff is usually good: http://developer.nvidia.com/object/6800_leagues_deferred_shading.html Here's a reasonable XNA tutorial as well: http://www.ziggyware.com/readarticle.php?article_id=155 In terms of blogs: Wolfgang Engel's is a good start and Christer Ericson recently posted a bunch of links (in the Graphics section of his ""Catching Up Part 2"" post). Oh and the G-Buffer paper is required reading too. Less practical but a good review of the process and rationale."
101,A,"How does Photoshop (Or drawing programs) blit? I'm getting ready to make a drawing application in Windows. I'm just wondering do drawing programs have a memory bitmap which they lock then set each pixel then blit? I don't understand how Photoshop can move entire layers without lag or flicker without using hardware acceleration. Also in a program like Expression Design I could have 200 shapes and move them around all at once with no lag. I'm really wondering how this can be done without GPU help. I don't think super efficient algorithms could justify that? Thanks Certainly modern Photoshop uses GPU acceleration if available. Another possible tool is DMA. You may also find it helpful to read the source code of existing programs like GIMP. @Kirill that's not true. DMA can be used without a GPU and the CPU can initiate it. I think there's a bit of misunderstanding here. What you're really talking about is bus mastering not DMA (DMA has been obsolete for years). @Jerry my understanding is that bus mastering is a form of DMA. @Matthew: The reverse would be more accurate: DMA was a primitive form of bus-mastering. DMA was a fairly specific term referring to a specific technique that worked with ISA (and similar buses). On PCs it was pervasive in 8088/8086 machines but used only for floppy disk transfers starting with the 286 (because the DMA controllers were then slower than CPU transfers and even setting up the DMA controller to do a transfer usually took more time than doing the transfer without it). Huh all of the platforms that I work on still call it 'DMA.' I will have to look up what ""bus mastering"" means but in context we're setting up a buffer in memory with commands that the memory manager (hardware) is pointed at and told ""GO!"". Then copies of that memory appears as if by magic wherever we told it to go. But my question really was how it can do it without because it didn't used to have it. DMA is not an another tool. It helps GPU to transfer data without using CPU.  Look at this question: http://stackoverflow.com/questions/197948/reduce-flicker-with-gdi-and-c All you can do about DC drawing without GPU is to reduce flickering. Anything else depends on the speed of filling your memory bitmap. And here you can use efficient algorithms multithreading and whatever you need. Ok so at the end of the day it's the speed that I can fill the memory bitmap I think what I'll do then to get easy opacity is use OpenGL and just draw textures to quads. thanks  Double (or more) buffering is the way it's done in games where we're drawing a ton of crap into a ""back"" buffer while the ""front"" buffer is being displayed. Then when the draw is done the buffers are swapped (a pointer swap not copies!) and the process continues in the new front and back buffers. Triple buffering offers another bonus in that you can start drawing two-frames-from-now when next-frame is done but without forcing a buffer swap in the middle of the screen refresh. Many games do the buffer swap in the middle of the refresh but you can sometimes see it as visible artifacts (tearing) on the screen. Anyway- for an app drawing bitmaps into a window if you've got some ""slow"" operation do it into a not-displayed buffer while presenting the displayed version to the rendering API e.g. GDI. Let the system software handle all of the fancy updating. Thanks i'm already double buffering it is a very useful technique!"
102,A,".NET Graphics.ScaleTransform converts print job to bitmap. Any other way to scale text? I'm using Graphics.ScaleTransform to stretch lines of text so they fit the width of the page and then printing that page. However this converts the print job to a bitmap - for a print with many pages this causes the size of the print job to rise to obscene proportions and slows down printing immensely. If I don't scale like this the print job remains very small as it is just sending text print commands to the printer. My question is is there any way other than using Graphics.ScaleTransform to stretch the width of the text? Sample code to demonstrate this is below (would be called with Print.Test(True) and Print.Test(False) to show the effects of scaling on print job): Imports System.Drawing Imports System.Drawing.Printing Imports System.Drawing.Imaging Public Class Print Dim FixedFont As Font Dim Area As RectangleF Dim CharHeight As Double Dim CharWidth As Double Dim Scale As Boolean Const CharsAcross = 80 Const CharsDown = 66 Const TestString = ""!""""#$%&'()*+-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~"" Private Sub PagePrinter(ByVal sender As Object ByVal e As PrintPageEventArgs) Dim G As Graphics = e.Graphics If Scale Then Dim ws = Area.Width / G.MeasureString(Space(CharsAcross).Replace("" "" ""X"") FixedFont).Width G.ScaleTransform(ws 1) End If For CurrentLine = 1 To CharsDown G.DrawString(Mid(TestString & TestString & TestString CurrentLine CharsAcross) FixedFont Brushes.Black 0 Convert.ToSingle(CharHeight * (CurrentLine - 1))) Next e.HasMorePages = False End Sub Public Shared Sub Test(ByVal Scale As Boolean) Dim OutputDocument As New PrintDocument With OutputDocument Dim DP As New Print .PrintController = New StandardPrintController .DefaultPageSettings.Landscape = False DP.Area = .DefaultPageSettings.PrintableArea DP.CharHeight = DP.Area.Height / CharsDown DP.CharWidth = DP.Area.Width / CharsAcross DP.Scale = Scale DP.FixedFont = New Font(""Courier New"" DP.CharHeight / 100 FontStyle.Regular GraphicsUnit.Inch) .DocumentName = ""Test print (with"" & IIf(Scale """" ""out"") & "" scaling)"" AddHandler .PrintPage AddressOf DP.PagePrinter .Print() End With End Sub End Class UPDATE: I used interop with GDI calls instead. Here's the relevant code; the GDI class is just full of definitions I copied from the wiki at http://pinvoke.net/ for the relevant functions and constants.  ' convert from Graphics units (100 dpi) to device units Dim GDIMappedCharHeight As Double = CharHeight * G.DpiY / 100 Dim GDIMappedCharWidth As Double = CharWidth * G.DpiX / 100 Dim FixedFontGDI As IntPtr = GDI.CreateFont(GDIMappedCharHeight GDIMappedCharWidth 0 0 0 0 0 0 GDI.DEFAULT_CHARSET GDI.OUT_DEFAULT_PRECIS GDI.CLIP_DEFAULT_PRECIS GDI.DEFAULT_QUALITY GDI.FIXED_PITCH ""Courier New"") Dim CharRect As New GDI.STRUCT_RECT Dim hdc As IntPtr = G.GetHdc() GDI.SelectObject(hdc FixedFontGDI) ' I used SetBkMode transparent as my text needed to overlay a background GDI.SetBkMode(hdc GDI.TRANSPARENT) ' draw it character by character to get precise grid For CurrentLine = 1 To CharsDown For CurrentColumn = 1 To CharsAcross With CharRect .left = GDIMappedCharWidth * (CurrentColumn - 1) .right = GDIMappedCharWidth * CurrentColumn .top = GDIMappedCharHeight * (CurrentLine - 1) .bottom = GDIMappedCharHeight * CurrentLine End With ' 2341 == DT_NOPREFIX|DT_NOCLIP|DT_VCENTER|DT_CENTER|DT_SINGLELINE GDI.DrawText(hdc Mid(TestString & TestString & TestString CurrentLine+CurrentColumn 1) 1 CharRect 2341) Next Next GDI.DeleteObject(FixedFontGDI) G.ReleaseHdc(hdc) Yes the Graphics class supports scaling text. But it needs to do so by rendering the text to a bitmap first rescale the bitmap and pass that resized bitmap to the printer driver. All those bitmaps make for a large spooler file. You will need to justify the text yourself. There's no support for this in the framework. One way to do it is to hijack a rich edit control and let it take care of the justification and printing. Version 5 msftedit.dll supports full justification. The best way to find the code you need is to find one of the many projects that implement a text editor with RTB similar to Wordpad on Windows. Thanks for your advice! I took a look at using rich edit controls but it seemed to overwhelming so I ended up using interop with the old GDI calls straight to the Graphics hDC - fortunately the CreateFont for that supports setting width as well as height and passes it on to the printer without bitmap rendering.  Im guessing here but you should increase the size of the Font by a percentage of the proportion you want to scale by. I do set the height of the font [shown in the code New Font(""Courier New"" DP.CharHeight / 100 FontStyle.Regular GraphicsUnit.Inch)] relative to the height of the line but need to set width independently of that. Unfortunately scaling just size stretches both width and height in the same proportion."
103,A,"Java2d: JPanel set background color not working I have the code shown below: public VizCanvas(){ { this.setBackground(Color.black); this.setSize(400400); } } It worked fine and displays the panel in black background. But when I implement the paint method which does nothing the color changes to default color i.e gray. I tried to set graphics.setColor() but it didn't help. Custom painting should be done by overriding the paintComponent() method NOT the paint() method. Then all you do is invoke super.paintComponent() to get the background painted. Setting the size of the component does nothing. The layout manager will override the size. You should be setting the preferred size or override the getPreferredSize() method. Read the Swing tutorial for Swing basics. There are sections on ""custom painting"" and ""using layout managers"". http://java.sun.com/j2se/1.4.2/docs/api/java/awt/Canvas.html#paint%28java.awt.Graphics%29 ""Most applications that subclass Canvas should override this method in order to perform some useful operation (typically custom painting of the canvas). The default operation is simply to clear the canvas. Applications that override this method need not call super.paint(g)."" I assume that their VizCanvas subclasses Canvas but their constructor seems a bit bare. Hmm.  You need to do a fill of the canvas to your background colour in the painting method. Something along the lines of: g.setColor(Color.BLACK); g.fillRect(0 0 getWidth() getHeight()); After that draw whatever you need to. You could also try calling super.paint(g) in the paint method instead before doing anything."
104,A,"How do I keep from running out of memory on graphics for an Android app? I've been working on an Android app in Eclipse and so far my program hasn't really grown past midget size. However I've already run into an issue with an Out of Memory error. You see I've been using graphics comprised solely of bitmaps and PNGs in this program and recently when I tried to add a little bit more functionality to the program (mainly including a few more bitmaps and causing an extra sprite to be created) it started crashing in the graphics thread's constructor -> sprite's constructor. When I tracked the problem down it turned out to be an Out of Memory error that is seemingly caused by adding too many picture files to the program and creating Drawables out of them. This would be a problem as I really don't have that many picture resources worked into that program...maybe 20 or so. I haven't even started to include sound yet. These images aren't all that fancy. My questions are this: 1) Are programs for the Android phone really that limited on how much memory they can employ or is it probably something other than the 20-30 resource pictures causing that error? 2) If the memory for Android apps is so awful it can't even handle 20-30 picture resources being loaded into Drawables that exist at the same time then how in the world are you supposed to make decent graphics and sound for that thing? Thanks. What size are the 20 images combined? Android gives an app 16MB if I remember correctly. Also is this after orientation changes and such or just during the app initialization? Hmm.... I've written Android apps with a number of static images (namely for frame animation) so reading your question has raised a few questions: What run environment are you using? Is this using an SDK emulator or are you running on a real phone? If you're in an emulator you might want to ensure that it's been created with specs that match the phone (even then it will probably run slower than it would on a phone due to specialized hardware the phone has). Do you need to have all the Drawables loaded at the same time? For example if you're drawing an image over and over again do you create a Drawable for each time you draw it or can your re-use the same Drawable and simply resize an instance of Rect for the actual call? Similarly can you load them on-demand? E.g. if you have a photo-album app can you only display photos 1-9 and load photos 10-19 after a user clicks a Next button? Are your images large? High-density? I realize these are high-level and hard to apply; I don't think I have a great idea of where you're problem might be from the description. I've hit a few snafus myself and was able to get the most savings by taking a hard look at how often I really needed to use that new keyword and what was critical to display and what wasn't. I also found the page on optimizing for performance to be very helpful (I'm mostly making games some examples here). Good luck!  Memory leaks and creating Drawables without any reason are not an issue except for something I'll mention in a second. However other things y'all have mentioned are valuable pieces of information. It is a video game I've been trying to make and so it's kind of impractical from what I can discern to be constantly creating and deleting Drawables depending just on whether they're being shown on the screen right that second; for sprites I believe I need to keep stuff loaded until those sprites themselves are eliminated and not simply off the screen. As for the error I basically just loaded too many Drawables into memory during the iniation of the program evidently. Now one thing that I've just now thought about - and this is NOT what caused that particular error but it would've caused similar ones down the road - is that I was creating an array of drawables for each and every sprite when I just now noticed that I should only do that for every kind of sprite. I'll have to look into the precise method of reducing the graphics to 16-bit and stuff because much of this application won't really need fancy stuff. I think I'll monitor this thread for another couple of days in case anybody else has any other ideas and then I'll mark it answered. Thanks guys.  The size of the graphics you're trying to load is very important. You can easily load several dozen bitmaps if they're small (e.g. 50x50) but you can't do more than a few if each one is big enough to fill the screen. It also depends on what you're trying to do with those images. For example if you're trying to smoothly rotate an image 90 degrees even something about 300x300px is enough to almost kill the app. There are a few things you need to do. Try not to load images until you need them. Try to reduce their graphics detail if you can. For example you can decode bitmaps into RGB 565 ARGB_4444 or ARGB_8888 which each take different amounts of memory. Set this using a BitmapFactory.Options object as a parameter when decoding images. See if you can 'cheat' - for example it's easier on memory to take one small image and fill the screen with the image repeated over and over (like tiles) than to use one large image. It's also easier to process an image that's a quarter the size you want it to be but drawn with a 2x scale factor. Read this article from the developer of Light Racer and his follow-ups to get some insight into how tweaking performance is one of the most difficult but crucial things to do when developing a game.  The easiest way is to add these 2 lines to the manifest in to the application tag android:hardwareAccelerated=""false"" android:largeHeap=""true"" But these are not recommended if you are building a memory efficient App But this really works.  As already mentioned by Steve and Paul the crucial factor is how big your images are. Android applications are limited to 16Mb on standard phones and 24Mb on hidef phones. That is a hard limit. This severely limits the amount of graphics you have. It's easy to calculate your graphic size: width x height x size of pixel (16 or 32 bit). Some tips: You need to check your code carefully for memory leaks. It is extremely easy to leak memory in Android and if you leak a couple of bitmaps a few times you'll run out of memory fast. This article may help. Always load your graphics using ARGB_4444 (16bit) if at all possible. That alone can reduce memory consumption for graphics by a factor 2. Always null and recycle resources if you can to assist the gc. Try to ensure that graphics are only used once. Having a resource handling class that deals with this can be very useful (at least if you're doing games)."
105,A,perl: tk: a way/widget that allows pixel level control over the output I want something like a canvas but where i'd be able to manipulate pixels easily in addition to all the provided geometries that can be drawn on canvas. Is it possible to embed something like GD::Image into a canvas? So then I maybe could make the image transparent and set some pixels in it (GD::Image->setPixel()) positioning it over the canvas? ps: well that doesn't necessarily have to be perl as there seem to be bindings for all the libs for most scripting (and not only) languages. Tk's canvas isn't designed to provide pixel-level control but you can do it by putting an image item in and manipulating the pixels in that. I'm not sure about the GD::Image but I know you can do this with the Tk photo image which supports transparency and has been transparent by default for quite a long time now. The other advantage of doing it with images is that they can be restacked within the list of items hidden removed added back in etc. This gives you a lot of scope for things which would be moderately awkward if you just drew raw. thanks this still looks like a workaround not a real solution but seems like i'll have to accept the answer as the question is not too popular ;)
106,A,"A good 2d engine for Java? Does anyone knows a good 2D engine for Java with sprites animations and collisions handling? What kind of games? Any in particular? For a tactical rpg I'm working on: http://sourceforge.net/projects/fatal-assault/ jGame Arianne Tangent: You'd be better off branching away from Java. The game development industry is C++/Python heavy with C# in third. Why the down vote? Probably for denigrating Java; it actually makes a good game platform. That's what I figured: fanboyism. If one reads carefully you'll see I actually didn't denigrate Java. I just made true statement on what kind of languages are prevalent in the broader game industry. I didn't say you *can't* make a game in Java. @SoftwareMonkey: I did not ""denigrate"" Java. That's preposterous. I informed the OP that the industry is heavily biased to C++. Yeah but your entire answer had nothing to do with the OPs question. He just wanted to find a good Java engine he didn't ask anything about the game industry as a whole. @vodnik: My entire answer did not. But half was directly in answer to his question. The rest is something your fanboism couldn't swallow: a helpful additional comment. Very sad. @Perce: ""Specifically"". Wow I guess I can't help people who try too see things beyond their intended purpose. I guess downvotes from ""perfect minds"" are unavoidable. So much for trying to be helpful... To be honest it appears your ""answer"" was put there specifically to hand out unsolicited advice. If you remove that it's relatively a pretty weak effort. And the advice may be terribly wrong. If the OP has no interest in going into commercialized gamedev and is only an ""indy"" or hobby dev they're going to be much better off (more productive) staying in Java for probably most types of games. I don't mean it was put there specifically to steer someone away from Java (I expect you're also a Java dev yourself). I'm saying it _appeared_ that way (there's no way for a 3rd party to know for sure...)  JGame is probably what you're looking for. You might also want to check out this question ( http://stackoverflow.com/questions/293079/java-2d-game-frameworks ) that has a list of Engines out there and a bit of feedback on some of them. Hope it's helpful. `Page Not Found` for your link. Baffling why that question was removed it was a good discussion albeit 3 years old since thus answer was done. JGame Slick2D and LibGDX are still valid choices. Another URL to look at for choices: http://www.java-gaming.org/index.php?topic=26897.0  Slick2D seems to be a pretty solid choice. It's widely used and it is based on OpenGL (via LWJGL) so you can get some pretty good performance if you need it. FYI in October the Slick2D site moved to http://www.slick2d.org/  Greenfoot from the makers of BlueJ would be a good choice if it is your first time with game-development in Java. It is not even an easy-to-learn API but also comes with a development-environment with fully integrated Greenfoot surface. The game-environment is the greenfoot.World while every element in the game is a greenfoot.Actor instance. The Actor class provides a method for true bitmap-intersection (greeenfoot.Actor.intersects())."
107,A,"How to draw semi-transparent text on a graphics object? I want to draw a text with 32 bit transparency on a graphics object. When I try I only get black color in the result. If I try to draw a line with the same semitransparent color it works perfectly. I have this code: lBitmap As New Bitmap(32 32 PixelFormat.Format32bppArgb) lGraphic As Graphics = Graphics.FromImage(lBitmap) lGraphic.SmoothingMode = SmoothingMode.HighQuality lGraphic.InterpolationMode = InterpolationMode.HighQualityBicubic lGraphic.Clear(Color.Transparent) Dim lTestTransparencyColor As Color = Color.FromArgb(100 140 0 230) lGraphic.DrawLine(New Pen(lTestTransparencyColor) 0 0 32 32) lBrush As New SolidBrush(lTestTransparencyColor) lGraphic.DrawString(""A"" New Font(""Arial"" 12) lBrush 0 0) Dim lImage As Image = lBitmap lImage.Save(""D:\Test.png"" Imaging.ImageFormat.Png) The line is drawn with the transparency applied correctly but the text is black with no transparency. Edit: If I set a solid color as background on the Graphics object then the text transparency works but I need it to be truly transparent in the result png file not just transparent against a solid background color in the image. This problem occurs also if I set a partial transparent color as background like this: lGraphic.Clear(Color.FromArgb(100 0 255 0)) I was thinking it may be that SolidBrush does not support transparency but I found a predefined brush named Transparent (Brushes.Transparent) that was a SolidBrush when I looked at it in debug. I tried to use Brushes.Transparent as the brush when drawing the text and it was successfully not showing at all. That means I get full transparency to work but not partial transparency. Set the TextRenderingHint to either SingBitPerPixel or SingleBitPerPixelGridFit: lGraphic.TextRenderingHint = Drawing.Text.TextRenderingHint.SingleBitPerPixel Thanks a lot for making me aware of this setting! I used `TextRenderingHint.AntiAlias` instead. Then I get perfect result with semi-transparent antialiasing!  I have followed this tutorial many times always with success: http://www.codeproject.com/KB/GDI-plus/watermark.aspx Hope it helps you I'm not sure what you mean really with ""32 bit transparancy"" but I believe the above link tells you how it is possible to adjust the transparancy level with the alpha filter when you create the brush: SolidBrush semiTransBrush2 = new SolidBrush(Color.FromArgb(153 0 00)); It is only a problem when the background of the graphics object is transparent. If the background has a non-transparent color the text transparency works. I will update my post to clarify this."
108,A,Estimate pixels of proportional font in SVG or Java Is there an way to estimate the number of pixels used by a proportional font? I am writing software that creates an image in SVG and transform that to PNG in Java. In this image I am using a text with a proportional font (size 16). I can sometimes fit 26 characters in the picture and sometimes only 19. This is because 'WWW' takes much more space as 'li1'. How can I estimate the number of pixels a String needs? Have a look at the FontMetrics class - the stringWidth() method in particular. If you have a graphics object g you are using for painting g.getFontMetrics(f).stringWidth(message) Thanks. This worked well.
109,A,Simulating 3D 'cards' with just orthographic rendering I am rendering textured quads from an orthographic perspective and would like to simulate 'depth' by modifying UVs and the vertex positions of the quads four points (top left top right bottom left bottom right). I've found if I make the top left and bottom right corners y position be the same I don't get a linear 'skew' but rather a warped one where the texture covering the top triangle (which makes up the quad) seems to get squashed while the bottom triangles texture looks normal. I can change UVs any of the four points on the quad (but only in 2D space it's orthographic projection anyway so 3D space won't matter much). So basically I'm trying to simulate perspective on a two dimensional quad in orthographic projection any ideas? Is it even mathematically possible/feasible? ideally what I'd like is a situation where I can set an x/y rotation as well as a virtual z 'position' (which simulates z depth) through a function and see it internally calclate the positions/uvs to create the 3D effect. It seems like this should all be mathematical where a set of 2D transforms can be applied to each corner of the quad to simulate depth I just don't know how to make it happen. I'd guess it requires trigonometry or something I'm trying to crunch the math but not making much progress. here's what I mean: Top left is just the card center is the card with a y rotation of X degrees and right most is a card with an x and y rotation of different degrees. Some images of what you're trying to achieve and what you actually get would go a long way to make this question clearer. use imgur.com/ for free and quick image storage. I just added a pic Your pictures show a perspective projection not an orthographic projection. Which one do you want? I want to simulate perspective rendering in orthographic projection by modifying UVs and the quads positions when I show the card in perspective I'm only doing it to highlight what I'm trying to achieve in orthographic view. In other words I want to do what you see in the perspective shots in orthographic projection instead. To compute the 2D coordinates of the corners just choose the coordinates in 3D and apply the 3D perspective equations : Original card corner (xyz) Apply a rotation ( by matrix multiplication ) you get ( x'y'z') Apply a perspective projection ( choose some camera origin direction and field of view ) For the most simple case it's : x'' = x' / z y'' = y' / z The bigger problem now is the texturing used to get the texture coordinates from pixel coordinates : The correct way for you is to use an homographic transformation of the form : U(xy) = ( ax + cy + e ) / (gx + hy + 1) V(xy) = ( bx + dy + f ) / (gx + hy + 1) Which is fact is the result of the perpective equations applied to a plane. abcdefgh are computed so that ( with UV in [0..1] ) : U(top''left'') = (00) U(top''right'') = (01) U(bottom''left'') = (10) U(bottom''right'') = (11) But your 2D rendering framework probably uses instead a bilinear interpolation : U( x  y ) = a + b * x + c * y + d * ( x * y ) V( x  y ) = e + f * x + g * y + h * ( x * y ) In that case you get a bad looking result. And it is even worse if the renderer splits the quad in two triangles ! So I see only two options : use a 3D renderer compute the texturing yourself if you only need a few images and not a realtime animation. In simple terms the problem is that you can't just calculate the positions of the corners you have to calculate the position of each pixel in the texture separately. That's because farther textures look smaller but if you just compute the corner positions and assume the textures are stretched equally between them the far away parts of the triangle will have textures too large and the close parts will have textures too small.
110,A,Looking for GD tutorial in C/C++ I see a lot of GD tutorials for PHP even though GD was written in C not PHP. Could you please suggest any good tutorial for GD in C? Did you look around at http://www.libgd.org/Documentation or ask on the mailing list gd-devel@lists.php.net? Site temporarily unavailable as of 2012-01 use http://wayback.archive.org/web/20110615000000*/http://www.libgd.org/Documentation if you need Direct URL http://web.archive.org/web/20100207231223/http://www.libgd.org/Documentation A bit of googling turned up nothing for me either - this is one of those things for now anyway that you must study the API reference and make up your own toy test programs. Use the source Luke! IMO reading libgd sources would be more useful than guessing how it works through toy test programs. You can also find sources to other programs using libgd for example http://www.google.com/codesearch?q=gdImageCreate+lang:c+-file:gd +1 ephemient. That's what I was thinking of after days of fruitless searching. @Viet -- also http://www.boutell.com/gd/manual2.0.33.html  I'm making this which might reasonably be a comment an answer so that it doesn't get lost in a cloud of comments. It's a very high quality user guide: clear complete and correct with full code examples. The outstanding doc is here: http://www.boutell.com/gd/manual2.0.33.html .
111,A,Moving Rectangle in VB? It is my understanding and experience that VB.NET does not perform well with moving graphics from point A to point B in a form. How do I draw a rectangle or a line and move it from point A to point B? Is there a reliable way to do this without seeing a black rectangle around the moving object on every frame? I've tried this with bitmaps before but it doesn't work. I see the frame rendering and it's way to slow. Perhaps there is an animation Control or library? Thanks. You could use WPF. Even if you using WinForms you can interop with a WPF user control using the ElementHost control How would that help?  I'm not going to provide a full example here as I'm more used to C# but here is pseudo code for how I'd do it. function paint() draw line (x y x + xEnd y + yEnd) // Use the graphics object here. end function update() update x update y end Something must be called either every frame or every time you have an event (key press etc...). This updates the x and y cooridinates of the line respectively. GDI+ would be used for drawing the line in other words the built in graphics library is more than enough for simple drawings. This could be improved by using vectors (2D) to represent the line coordinates rather than standard data types for individual x and y coordinates. If you post it in c# I'd appreciate it. I have enough general programming experience to work it out from there.  First I would not blame VB.Net. Both C# and VB.Net use the same graphics api (GDI+). Here is an example I found and I believe it will help you to understand what needs to be done. VB Helper +1. I guess you are right. What alternatives are there to GDI+? Is there a way to implement Dirrect X or is that even going in the right direction? You can use DirectX from VB.Net but I have never done it myself. WPF is another option - http://windowsclient.net/WPF/.
112,A,What is the best way to draw objects and deal with them? Hi I want to draw on my PictureBox and then deal with those drawings that i created. is it a good way to store their points in an array and deal with this array??. Or the better way is to make them objects and deal with these objects?.And if so could you please put me on the head of way to make that??. Thank you my really teachers. Thank you very much. If elements of the drawing might change then an object (Shape) is best. But a collection of points is an object as well. The main alternative would be a layered approach with a separate bitmap with a transparent background holding the points. First of all thank u for your speedy reply Then: the shapes will not very changed but I'll move them and resize them. So what is the best way?. Thanx An object which might contain a list of points if that convenient.
113,A,What is faster with PictureBox? Many small redraws or complete redraw I have a PictureBox (WinMobile 6 WinForm) on which I draw some images. There is a background image that goes in the background and it does not change. However objects that are drawn on the picturebox are moving during the application so I need to refresh the background. Since items that are redrawn fill from 50% to 80% of the surface the question is which of the two is faster: 1) Redraw only parts of the background image that have been changed (previous+next location of the moving object). 2) Redraw complete background and then draw all the objects in their current position. Now the reason for asking is because I am not sure how much of processor power is needed for a single drawImage operation and what are the time consuming factors. I am aware if there is almost complete coverage of the background it would be stupid to redraw portions of it because by drawing portions I will have drawn the complete picture. But since sometimes only half of the image had changed (some objects remained in their old position) it may (perhaps) be benefitial to redraw only those regions. But I need your insight on this... Thanks. Less number of DrawImage is better since it uses GDI+ to render and it goes very slow. I would recommend to use BitBlt of the old GDI instead of DrawImage. With this you can put all necessary portions to a temporary canvas (Graphics obj) and then put everything into a PictureBox at once. If you ignore in your loop all portions that are outside of the visible screen this should work fast.
114,A,"How do I create a ""jumping"" circle in Java GUI? I would like to have a red filled circle at some place in my window. After the click on the circle the circle should jump from the original place to a new one (disappear from the old place and appear on the new one). What would be the best way to do it? I also would like to know how to draw a filled circle in Java. Are there any simple tools to do it? Or may be the easiest way is to use an image created by some other software? ADDED: For the beginning I would like to have just a redraw (a circle disappear and at ""the same moment"" it appears on the new position). I think it will be simple that some visual effects and I would like to start from the simples possible choice. Do you want an animated action for the jump or just a redraw for the circle? This can be done using Swing Graphics2D and a custom JPanel. The tutorial contains a similar example: http://java.sun.com/docs/books/tutorial/uiswing/painting/step3.html"
115,A,Looking for image distortion algorithm Recently I saw demonstration of image distortion algorithm that preserves some objects in image. For example changing weight/height ratio but preserving people faces in image to look realistically. The problem is i can't find the reference to that demonstration neither I know the name of such transformations. Can anybody point me to a references to such algorithms? Seam carving is the current favourite. Liquid Rescale is an implementation of it.  Perhaps you are referring to liquid rescale? Exactly. Thanks.
116,A,Can XCode draw the call graph of a program? I am new to Mac OSX and I wonder if Xcode can generate  for a given C++ source code the call graph of the program in a visual way. I also wonder if for each function and after a run whether it can also print the %time spent on the function If so I would thank really some links with tutorials or info after googling I did not find anything relevant Thanks I think there is a difference between a callgraph and profiling: As far as I understand a callgraph helps you understanding what the program does (eg which method calls which other methods and so on). A callgraph can be drawn from static code analysis (doxygen can do this) A profiler profiles the program at runtime and tells you how much time the cpu spent on a certain function. This is something different. I agree. This should be two separate questions. Hi can anyone explain this step: A callgraph can be drawn from static code analysis (doxygen can do this). I am installing Doxygen and try to get a call graph for an Objective-c project within Xcode. I'm not sure about drawing a call graph but for profiling you should look for Shark which is part of the XCode developer bundle. Check out developer.apple.com/mac for documentation.  Use Run->Run With Performance Tool->Time Profiler to launch the app in Instruments.app. Instruments uses Dtrace under the hood and provides a huge wealth of profiling and measurement tools. The Time Profiler template will get you time profile and call stacks but I don't think there is any way to get a call graph for the entire execution (though there's so much in Instruments I may easily be wrong). Shark is an older profiling tool from Apple and provides some very nice optimization hints. In general you should start with Instruments and use Shark only if needed for optimizing. thanks it looks very promising i will have a look at it
117,A,Convex hull of 4 points I would like an algorithm to calculate the convex hull of 4 2D points. I have looked at the algorithms for the generalized problem but I wonder if there is a simple solution for 4 points. I've been scratching around on paper trying to figure out an algorithm but am having no luck at the moment. Thanks. You might want to look at Melkman's Algorithm. Melkman's Convex Hull Algorithm. Melkman's algorithm is considered to be the best convex hull algorithm for simple polygons. Melkman's is a great algorithm but not for finding the convex hull of points. For using Melkman you need to order your points as vertices of a simple polygon. Once you've done that for 4 points you've mostly solved the convex hull problem for them.  Take three of the points and determine whether their triangle is clockwise or counterclockwise:: triangle_ABC= (A.y-B.y)*C.x + (B.x-A.x)*C.y + (A.x*B.y-B.x*A.y) For a right-handed coordinate system this value will be positive if ABC is counterclockwise negative for clockwise and zero if they are collinear. But the following will work just as well for a left-handed coordinate system as the orientation is relative. Compute comparable values for three triangles containing the fourth point: triangle_ABD= (A.y-B.y)*D.x + (B.x-A.x)*D.y + (A.x*B.y-B.x*A.y) triangle_BCD= (B.y-C.y)*D.x + (C.x-B.x)*D.y + (B.x*C.y-C.x*B.y) triangle_CAD= (C.y-A.y)*D.x + (A.x-C.x)*D.y + (C.x*A.y-A.x*C.y) If all three of {ABDBCDCAD} have the same sign as ABC then D is inside ABC and the hull is triangle ABC. If two of {ABDBCDCAD} have the same sign as ABC and one has the opposite sign then all four points are extremal and the hull is quadrilateral ABCD. If one of {ABDBCDCAD} has the same sign as ABC and two have the opposite sign then the convex hull is the triangle with the same sign; the remaining point is inside it. If any of the triangle values are zero the three points are collinear and the middle point is not extremal. If all four points are collinear all four values should be zero and the hull will be either a line or a point. Beware of numerical robustness problems in these cases! For those cases where ABC is positive: ABC ABD BCD CAD hull ------------------------ + + + + ABC + + + - ABCD + + - + ABCD + + - - ABD + - + + ABCD + - + - BCD + - - + CAD + - - - [should not happen] +1: Elegant and efficient. Actually on looking this over it should be a bit more efficient *and* accurate if you do all the differences first: ABC=(A.y-B.y)*(C.x-A.x)+(B.x-A.x)*(C.y-A.y) [and so forth for ABD etc.]  I did a proof of concept fiddle based on a crude version of the gift wrapping algorithm. Not efficient in the general case but enough for only 4 points. function Point (x y) { this.x = x; this.y = y; } Point.prototype.equals = function (p) { return this.x == p.x && this.y == p.y; }; Point.prototype.distance = function (p) { return Math.sqrt (Math.pow (this.x-p.x 2) + Math.pow (this.y-p.y 2)); }; function convex_hull (points) { function left_oriented (p1 p2 candidate) { var det = (p2.x - p1.x) * (candidate.y - p1.y) - (candidate.x - p1.x) * (p2.y - p1.y); if (det > 0) return true; // left-oriented if (det < 0) return false; // right oriented // select the farthest point in case of colinearity return p1.distance (candidate) > p1.distance (p2); } var N = points.length; var hull = []; // get leftmost point var min = 0; for (var i = 1; i != N; i++) { if (points[i].y < points[min].y) min = i; } hull_point = points[min]; // walk the hull do { hull.push(hull_point); var end_point = points[0]; for (var i = 1; i != N; i++) { if ( hull_point.equals (end_point) || left_oriented (hull_point end_point points[i])) { end_point = points[i]; } } hull_point = end_point; } /* * must compare coordinates values (and not simply objects) * for the case of 4 co-incident points */ while (!end_point.equals (hull[0])); return hull; } It was fun :)  Here's a more ad-hoc algorithm specific to 4 points: Find the indices of the points with minimum-X maximum-X minimum-Y and maximum-Y and get the unique values from this. For example the indices may be 0212 and the unique values will be 021. If there are 4 unique values then the convex hull is made up of all the 4 points. If there are 3 unique values then these 3 points are definitely in the convex hull. Check if the 4th point lies within this triangle; if not it is also part of the convex hull. If there are 2 unique values then these 2 points are on the hull. Of the other 2 points the point that is further away from this line joining these 2 points is definitely on the hull. Do a triangle containment test to check if the other point is also in the hull. If there is 1 unique value then all the 4 points are co-incident. Some computation is required if there are 4 points to order them correctly so as to avoid getting a bow-tie shape. Hmmm.... Looks like there are enough special cases to justify using a generalized algorithm. However you could possibly tune this to run faster than a generalized algorithm.  Or just use Jarvis march. yep. nice and simple. here's a good implementation-- http://tixxit.net/2009/12/jarvis-march/
118,A,Cloning ID3DXMesh with declration that has 12 floats breaks? I have the following vertex declration: struct MESHVERTInstanced { float x y z; // Position float nx ny nz; // Normal float tu tv; // Texcoord float idx; // index of the vertex! float tanx tany tanz; // The tangent const static D3DVERTEXELEMENT9 Decl[6]; static IDirect3DVertexDeclaration9* meshvertinstdecl; }; And I declare it as such: const D3DVERTEXELEMENT9 MESHVERTInstanced::Decl[] = { { 0 0 D3DDECLTYPE_FLOAT3 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_POSITION 0 } { 0 12 D3DDECLTYPE_FLOAT3 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_NORMAL 0 } { 0 24 D3DDECLTYPE_FLOAT2 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_TEXCOORD 0 } { 0 32 D3DDECLTYPE_FLOAT1 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_TEXCOORD 1 } { 0 36 D3DDECLTYPE_FLOAT3 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_TANGENT 0 } D3DDECL_END() }; What I try to do next is copy an ID3DXMesh into another one with the new vertex declaration as such: model->CloneMesh( model->GetOptions() MESHVERTInstanced::Decl gd3dDevice &pTempMesh ); When I try to get the FVF size of pTempMesh (D3DXGetFVFVertexSize(pTempMesh->GetFVF())) I get '0' though the size should be 48. The whole thing is fine if I don't have the last declaration '{ 0 36 D3DDECLTYPE_FLOAT3 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_TANGENT 0 }' in it and the CloneMesh function does not return a FAIL. I've also tried using different declarations such as D3DDECLUSAGE_TEXCOORD and that has worked fine returning a size of 48. Is there something specific about D3DDECLUSAGE_TANGENT I don't know? I'm at a complete loss as to why this isn't working... Not every D3DVERTEXDECL can be converted to FVF. You should triple-check that you have an FVF-compatible declaration before trying to get the FVF size. Although it's beyond me why you're using FVF it's fairly useless and you won't be gaining future skills as D3D10 doesn't even allow FVF. Shaders and the programmable pipeline. I suggest that you get DirectX 9.0c- A Shader Approach by Frank Luna which is excellent and up-to-date. Also available in a D3D10 flavour although I believe that D3D11 is not out. Edit: Not sure what you're doing with parsing the vertex buffer.. the size of a vertex buffer is sizeof(the struct you posted in the OP) * the length you input to create it. You shouldn't be dealing with vertex buffers manually at all - that's what ID3DXMesh exists for. Well is there another way to get the size of a vertex buffer so I can parse it? Also what's the alternative to FVF?  TBH AS DeadMG points out that vertex decl is NOT FVF compatible. You ask if there is another way to get the size of the vertex buffer. You evidently know how many vertices are in the buffer and you OBVIOUSLY know how large the vertex structure is (48 bytes) as you DEFINE it as such in your vertex declaration. const D3DVERTEXELEMENT9 MESHVERTInstanced::Decl[] = { { 0 0 D3DDECLTYPE_FLOAT3 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_POSITION 0 } //Starts at offset 0 and is 3 floats or 12 bytes in size. Total = 12 bytes. { 0 12 D3DDECLTYPE_FLOAT3 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_NORMAL 0 } //Starts at offset 12 and is 3 floats or 12 bytes in size. Total = 24 bytes. { 0 24 D3DDECLTYPE_FLOAT2 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_TEXCOORD 0 } //Starts at offset 24 and is 2 floats or 8 bytes in size. Total = 32 bytes. { 0 32 D3DDECLTYPE_FLOAT1 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_TEXCOORD 1 } //Starts at offset 32 and is 1 float or 4 bytes in size. Total = 36 bytes. { 0 36 D3DDECLTYPE_FLOAT3 D3DDECLMETHOD_DEFAULT D3DDECLUSAGE_TANGENT 0 } //Starts at offset 36 and is 3 floats or 12 bytes in size. Total = 48 bytes. D3DDECL_END() }; Failing that just use D3DXGetDeclLength. Ah yep that helped also ID3DXMesh::GetNumBytesPerVertex() seems to work as well. It wasn't so much that I knew how large the vertex declaration was but I wanted to know how I can get the size automatically in cases I did not know the size of the vertex declaration.
119,A,"What would be the best way to simulate Radar in C#? I have a Picture box inside a Groupbox in my form with the Picture of a radar set as the background picture. My intention is to dynamically load tiny Jpeg images within the radar area (overlaid) at runtime but I am unsure as to the best way to achieve this. All crazy ideas welcomed (but I would prefer sane easy to do ones). Thank you all. As for actual example:  // Among others using System.Collections.Generic; using System.Drawing; using System.IO; class TinyPic { public readonly Image Picture; public readonly Rectangle Bounds; public TinyPic(Image picture int x int y) { Picture = picture; Bounds = new Rectangle(x y picture.Width picture.Height); } } class MyForm : Form { Dictionary<String TinyPic> tinyPics = new Dictionary<String TinyPic>(); public MyForm(){ InitializeComponent(); // assuming Panel myRadarBox // with your background is there somewhere; myRadarBox.Paint += new PaintEventHandler(OnPaintRadar); } void OnPaintRadar(Object sender PaintEventArgs e){ foreach(var item in tinyPics){ TinyPic tp = item.Value; e.Graphics.DrawImageUnscaled(tp.Picture tp.Bounds.Location); } } void AddPic(String path int x int y){ if ( File.Exists(path) ){ var tp = new TinyPic(Image.FromFile(path) x y); tinyPics[path] = tp; myRadarBox.Invalidate(tp.Bounds); } } void RemovePic(String path){ TinyPic tp; if ( tinyPics.TryGetValue(path out tp) ){ tinyPics.Remove(path); tp.Picture.Dispose(); myRadarBox.Invalidate(tp.Bounds); } } } This of course is very basic assumes image source is path and doesn't take care of many intricate things but that's the quick and dirty jist of it which you can certainly build on. Cheers..... Having not done GDI+ or any graphics pprogrammingin C# and a Deadline looming (This was sprung on me <48 before deadline) This could help..  It depends a lot on what your ""radar"" needs to look like but almost certainly you'll need to implement the Paint event handler and draw the contents of the radar display yourself. A picture box will only get you so far (""not very""). GDI+ is very easy to use to draw circles lines text and images and will give you complete control over how your display looks.  Click here to run a sample application that demonstrates the basics of how to do radar (or one way at least). Note: this application does not do double-buffering or transparency of the tiny image. Source code for the project is here. Update code: public partial class Form1 : Form { private Bitmap _canvas; private float _sweepStartAngle = -90; private float _sweepAngle = 15; private SolidBrush _sweepBrush = new SolidBrush(Color.Red); private Rectangle _sweepRect; private Timer _sweepTimer = new Timer(); private Bitmap _submarine; private Point _submarinePosition = new Point(0 0); private Random rnd = new Random(); public Form1() { InitializeComponent(); _canvas = new Bitmap(pbScope.Width pbScope.Height); pbScope.Image = _canvas; _sweepRect = new Rectangle(0 0 pbScope.Width pbScope.Height); _submarine = (Bitmap)pbSubmarine.Image; RedrawScope(); _sweepTimer.Interval = 100; _sweepTimer.Tick += new EventHandler(_sweepTimer_Tick); _sweepTimer.Start(); } void _sweepTimer_Tick(object sender EventArgs e) { _sweepStartAngle += _sweepAngle; RedrawScope(); } private void RedrawScope() { using (Graphics g = Graphics.FromImage(_canvas)) { // draw the background g.DrawImage(pbBackground.Image 0 0); // draw the ""sweep"" GraphicsPath piepath = new GraphicsPath(); piepath.AddPie(_sweepRect _sweepStartAngle _sweepAngle); g.FillPath(_sweepBrush piepath); //g.FillPie(_sweepBrush _sweepRect _sweepStartAngle _sweepAngle); // move the submarine and draw it _submarinePosition.X += rnd.Next(3); _submarinePosition.Y += rnd.Next(3); // check if submarine intersects with piepath Rectangle rect = new Rectangle(_submarinePosition _submarine.Size); Region region = new Region(piepath); region.Intersect(rect); if (!region.IsEmpty(g)) { g.DrawImage(_submarine _submarinePosition); } } pbScope.Image = _canvas; } private void Form1_FormClosing(object sender FormClosingEventArgs e) { _sweepTimer.Stop(); _sweepTimer.Dispose(); } private void Form1_Load(object sender EventArgs e) { //GraphicsPath piepath = new GraphicsPath(); //piepath.AddPie( } } private void InitializeComponent() { System.ComponentModel.ComponentResourceManager resources = new System.ComponentModel.ComponentResourceManager(typeof(Form1)); this.pbScope = new System.Windows.Forms.PictureBox(); this.pbBackground = new System.Windows.Forms.PictureBox(); this.pbSubmarine = new System.Windows.Forms.PictureBox(); ((System.ComponentModel.ISupportInitialize)(this.pbScope)).BeginInit(); ((System.ComponentModel.ISupportInitialize)(this.pbBackground)).BeginInit(); ((System.ComponentModel.ISupportInitialize)(this.pbSubmarine)).BeginInit(); this.SuspendLayout(); // // pbScope // this.pbScope.Location = new System.Drawing.Point(12 12); this.pbScope.Name = ""pbScope""; this.pbScope.Size = new System.Drawing.Size(300 300); this.pbScope.TabIndex = 0; this.pbScope.TabStop = false; // // pbBackground // this.pbBackground.Image = ((System.Drawing.Image)(resources.GetObject(""pbBackground.Image""))); this.pbBackground.Location = new System.Drawing.Point(341 12); this.pbBackground.Name = ""pbBackground""; this.pbBackground.Size = new System.Drawing.Size(300 300); this.pbBackground.TabIndex = 1; this.pbBackground.TabStop = false; this.pbBackground.Visible = false; // // pbSubmarine // this.pbSubmarine.Image = ((System.Drawing.Image)(resources.GetObject(""pbSubmarine.Image""))); this.pbSubmarine.Location = new System.Drawing.Point(658 45); this.pbSubmarine.Name = ""pbSubmarine""; this.pbSubmarine.Size = new System.Drawing.Size(48 48); this.pbSubmarine.TabIndex = 2; this.pbSubmarine.TabStop = false; this.pbSubmarine.Visible = false; // // Form1 // this.AutoScaleDimensions = new System.Drawing.SizeF(6F 13F); this.AutoScaleMode = System.Windows.Forms.AutoScaleMode.Font; this.ClientSize = new System.Drawing.Size(326 328); this.Controls.Add(this.pbSubmarine); this.Controls.Add(this.pbBackground); this.Controls.Add(this.pbScope); this.Name = ""Form1""; this.Text = ""Radar""; this.Load += new System.EventHandler(this.Form1_Load); this.FormClosing += new System.Windows.Forms.FormClosingEventHandler(this.Form1_FormClosing); ((System.ComponentModel.ISupportInitialize)(this.pbScope)).EndInit(); ((System.ComponentModel.ISupportInitialize)(this.pbBackground)).EndInit(); ((System.ComponentModel.ISupportInitialize)(this.pbSubmarine)).EndInit(); this.ResumeLayout(false); } Sure although it's not terribly practical to post code that depends on control initialization and layout etc. Could you edit in the code? Today's standards require more than just an external link.  The simplest way is to load your tiny JPEGs into tiny PictureBoxes and add them to the main PictureBox's Controls collection (i.e. place them on the PictureBox) at runtime. Since this will probably produce flicker the slightly more complex way is to keep the main picture and the tiny pictures in class-level Bitmap objects and in the main PictureBox's Paint event you copy the main picture followed by each tiny picture onto a second class-level Bitmap (named _doubleBuffer or something like that) using the DrawImage method and then copy _doubleBuffer onto your PictureBox (also using DrawImage). Whenever you need to update your display and redraw everything you just call the PictureBox's Invalidate method. There are loads of examples here on SO that show how to use these methods. Good luck it sounds fun (if you're rewriting the classic arcade game Submarine let me know - I loved that game). Damn!! That sounds Crazy complicated... Think I'll go read up on GDI+ for beginners as suggested by JW above. thanks though. Didn't play Submarine; I'm more going for The NES version of ""The Hunt for The Red October"". It's not crazy complicated and it's actually just an elaboration of Jason's answer. I'm also talking about GDI+ and handling the Paint event. The double-buffering part is essential for avoiding flicker. Why do I need to double buffer when the Radar Image is set as my main picturebox's background image. I was thinking of drawing a container (a panel in this case) whithin the draw area making it invisible then create a rectf and set its boundaries to the container and draw within the rectf object boundaries... What'd you think of that scenario? You'll get flicker - trust me on this one. It's complicated but flicker basically comes from GDI+ drawing operations getting ""caught"" in the middle of your monitor's refresh (along with other problems). The only way to avoid this is to do all your complicated multi-step drawing operations on a canvas that is not visible and then rendering this canvas to the visible surface all at once. Try out your approach - this will help you learn the basics of GDI+ (which are the same no matter what surface you're drawing on). It's relatively easy to make your code double-buffered after you've written the essential drawing stuff. Ok I'm having some major problems with the way you told me. I have managed to define a Pie region within my main Picturebox (the picture pox has my radar image set as it's Background) but I have no way to add the little tiny images without distorting them. @dark-star1: not sure what you mean by distorting but I'm guessing you mean they're being stretched and/or shortened. Are you rendering them onto your picture box using the DrawImage method? There are 30 overloads of this method and depending on which one you use and how you use it the tiny images may not be drawn where and how you want them drawn. Your best bet is to come up with a small code sample that demonstrates the problem and post it here as another question. I would guess that I wouldn't even be able to answer it before someone else does. :) Yes the image is being blown out of proportion and I have used the DrawImage and DrawImageUnscaled methods. a link to the code: http://stackoverflow.com/questions/1319649/how-do-i-display-an-image-within-a-region-defined-in-a-picturebox @dark-star1: I'm not sure what your Pie region is exactly but it's possible to set up Transforms in GDI that take the normal x/y coordinate system and distort it in some way and (based on another question of yours I saw) that may be what's happening here. Normally if you use a DrawImage overload that uses DrawMode.Pixel the tiny image should be rendered exactly as it is. Again without a code sample I'm flying blind here. My pie region is just a semi circle overlay on top of my radar background which is this image: http://www.aefreemart.com/2006/06/animating-radar-screen.html cut in half (horizontally of course) :)"
120,A,"Fastest approach for rotating an image on the iPhone I need to rotate a UIImage in response to user input. I'd like this to be as slick as possible and therefore would like to know which is the quickest way to perform the transformation and render. Ideally I'd like to stay within the UIKit or Quartz frameworks. The properties of the image are as follows: Size approximately 250x300 pixels Not opaque - needs to render with an alpha channel What are the best approaches and practices for implementing this functionality? Note: There is an approach described in this stackoverflow answer but I am uncertain if this is optimal. I will of course give this a try but I'd be keen to know if this is considered to be the 'best practice'. If you are displaying your UIImage in a UIImageView why not just set the transform property on the view ? myimageView.transform = CGAffineTransformMakeRotation(<angle in radians>); It's as simple as that and for a small image like your talking about this is highly performant - all UIKit views are layer backed so it's hardware accelerated. This property is also animatable.  This example of applying a Core Animation layer to a UIImage might provide some inspiration: UIImage* rotatingImage = [UIImage imageNamed:@""someImage.png""]; CATransform3D rotationTransform = CATransform3DMakeRotation(1.0f * M_PI 0 0 1.0); CABasicAnimation* rotationAnimation = [CABasicAnimation animationWithKeyPath:@""transform""]; rotationAnimation.toValue = [NSValue valueWithCATransform3D:rotationTransform]; rotationAnimation.duration = 0.25f; rotationAnimation.cumulative = YES; rotationAnimation.repeatCount = 10; [rotatingImage.layer addAnimation:rotationAnimation forKey:@""rotationAnimation""]; Thanks for sharing this Core Animation variant. I'll give this a try also. I agree with the Core Animation suggestion. If you do animated rotation via Quartz you'll see terrible performance because of all the redraws you'll need. You may also wish to check out http://stackoverflow.com/questions/486609/how-can-i-use-animation-in-cocos2d and http://stackoverflow.com/questions/839296/how-can-i-rotate-a-uiimageview-with-respect-to-any-point-other-than-its-center Shouldnt rotatingImage be a UIImageView? And set its image with someImage.png because a UIImage doesnt have a layer property."
121,A,"How to produce Photoshop stroke effect? I'm looking for a way to programmatically recreate the following effect: Give an input image: I want to iteratively apply the ""stroke"" effect. The first step looks like this: The second step like this: And so on. I assume this will involves some kind of edge detection and then tracing the edge somehow. Is there a known algorithm to do this in an efficient and robust way? Basically a custom algorithm would be according to this thread: Take the 3x3 neighborhood around a pixel threshold the alpha channel and then see if any of the 8 pixels around the pixel has a different alpha value from it. If so paint a circle of a given radius with center at the pixel. To do inside/outside modulate by the thresholded alpha channel (negate to do the other side). You'll have to threshold a larger neighborhood if the circle radius is larger than a pixel (which it probably is). This is implemented using gray-scale morphological operations. This is also the same technique used to expand/contract selections. Basically to stroke the center of a selection (or an alpha channel) what one would do is to first make two separate copies of the selection. The first selection would be expanded by the radius of the stroke whereas the second would be contracted. The opacity of the stroke would then be obtained by subtracting the second selection from the first. In order to do inside and outside strokes you would contract/expand by twice the radius and subtract the parts that intersect with the original selection. It should be noted that the most general morphological algorithm requires O(m*n) operations where m is the number of pixels of the image and n is the number of elements in the ""structuring element"". However for certain special cases this can be optimized to O(n) operations (e.g. if the structuring element is a rectangle or a diamond)."
122,A,"Java and GraphicsMagick -- Will it work? I am considering using GraphicsMagick (http://www.graphicsmagick.org/) in a Java project. Does anyone have any experience with this? Suggestions on how to get started? It seems like there isn't a native Java library so it may be a little more difficult. Thanks! Currently the only reasonable way to achieve this is by using the command line from Java (runtime.exec). You should use im4java to do this as suggested above. im4java will enable you to build up your ""gm command"" string using java method calls it also provides a number of other useful features. The big advantage of using this technique over actual language bindings is simplicity and reliability. Reliability is important especially if your Java app is running on a Java based server or servlet engine like tomcat. The reason being that a memory fault or other error while using language bindings could bring down the whole Java virtual machine.  It's definitely possible. Take a look at IM4Java a Java abstraction around the commandline interfaces of various ImageMagick like tools (including GM) that feels like a language binding. Very little documentation but sufficiently simple. Obviously your images have to be accessible from the OS (e.g. not inside ResourceBundles).  We did our project with GraphicsMagick and Java Q&A here obvious influence our decision. It's a long way but we eventually got it done. We tweaked both GraphicsMagick and im4java very hard to get the performance and reliability we want. Thought I should contribute back: http://kennethxu.blogspot.com/2013/04/integrate-java-and-graphicsmagick.html"
123,A,"Gwt Graphics and ClickHandler I am using gwt graphics and gwt dnd for a program. I have a button which when clicked creates a circle at specified position on the panel and the circle is draggable. For dragging i have used gwt-dnd. I have also added a click handler to the circle which when click should print ""ERD Circle"". Here is the code layout:  Button b = new Button(""Circle""); b.addClickHandler(new ClickHandler(){ @Override public void onClick(ClickEvent event) { // TODO Auto-generated method stub Circle c = new Circle(20 15 10); d.add(c); c.addClickHandler(new ClickHandler(){ @Override public void onClick(ClickEvent event) { // TODO Auto-generated method stub System.out.println(""ERD Circle""); } }); dragController.makeDraggable(d); boundaryPanel.add(d 200 200 ); } }); This button is then added to an absolutepanel which in turn is added to the rootpanel. Problem that i am facing is click handler does not work when it is made draggable. If i remove dragController.makeDraggable(d); click handler works perfectly fine. Question: Am i doing anything wrong here? If the code is correct then is there anything else that i need to add to get both click handler and draggable working? Any suggestions will be of great help. Thank you. I got it working. I just added dragController.setBehaviorDragStartSensitivity(1); to my code and its fine now."
124,A,"Implemeting ""drawing modes"" in a graphics library? i would like to implement 'drawing modes' (in my own graphics library). That is drawing with AND OR etc However i am storing colors using floats each channel between 0 and 1.0 Do i have to first convert each color channel to 0-255 before i can use the AND OR etc drawing modes? and then convert back to float (0.0-1.0) ? Or is there another way of doing it? thanks I believe the question is not clear enough. AND OR etc. are boolean operators. Many languages support bitwise versions as well. So you first need to define what exactly is the meaning of AND-ing or OR-ing two color values. What is Red AND Green? Is it Black? If the answer to the above question is positive then you probably want to apply these operators in the bitwise sense on the (integer) RGB representation of your colors. In this case you do need to: 1. Convert the floats to (8-bit or other resolution) integers 2. Pack the 3 channels (or 4 with Alpha) into one word (probably 32-bit integer) 3. Apply the bitwise operator 4. Unpack the channels and convert back to floats. Note that in conversion of floats to int you first need to multiply your float value by MAX_COLOR (255 in your example) and then cast. Otherwise you end up with all channels being 0. Opposite when you convert back to floats first cast then divide by MAX_COLOR to normalize your values."
125,A,"Basic art for the programmer? This perhaps isn't really a programming question but it's something I'm sure lots of programmers other than me have faced! I've almost finished an iphone application I've been working on the only thing I'm missing is a spot of art - a title screen small and large icons that kind of thing. I have no artistic talent but I've been finding it hard to get anyone to do this for me despite offering a share of future profits. Anyone got any experience of either where to get artists or the best way to do a tiny spot of artwork yourself? Please don't just answer 'GIMP' or 'Photoshop' unless you can provide a reasonably good tutorial to getting started with those apps as well! It would be even better if your solution works well for 1-button mouse Macbook owners! Another idea is the program Paint Shop Pro. I've only used version 7 but it has a feature called tubes. This includes pre-drawn items that you can ""paint"" with. For instance you can use a tree tube to paint a forest. It won't make you an artist but it could help if you don't have any art skills (like me). There are also thousands of Photoshop tutorials out there. If you search for ""[what you want to draw] photoshop tutorial"" you will probably find one that will point you in the right direction.  Other people have some good suggestions on where to find designers. I tried to create my icon using stock libraries but the big lesson that I learned is to pay very close attention to the licence. I made the mistake of using clip-art that was really designed for PowerPoint and not distribution in software. The licence agreement was not clear but it looked as though they wanted to claim copyright on the whole ""derived item"" and claim an absurd royalty rate. (Full story on my applications website.) The other thing to bear in mind is that you're effectively using their artwork as your trademark. (From the other side your trademark would be freely available for other people to use too.) In short: probably best to get someone to design the artwork for you even if it is a little more expensive up front.  Coming up with art isn't an easy task. If you can't find someone to do it for you put something you feel is good together find someone who can recognize good design and have them go over your rough drafts so you can refine them. We easily become blind to problems in our own work so having someone else critiquing it can help take you ok design and make it much better. As for resources go Smashing Magazine has lots and lots of free icons in addition to other resources. Have a look around http://www.smashingmagazine.com for ideas. I particularly like the Silk icon set by famfamfam - http://www.famfamfam.com/lab/icons/silk/ If you find yourself needing stock photos stock.xchng is a free site with many good photos you can use. http://www.sxc.hu/ Something to keep in mind while you may be able to come up with something work able by assembling these professionally designed parts you will end up with a much better look and result if you hire a professional graphics designer. Just because you have access to the same tools does not mean that you will achieve the same results. Graphics design is a talent that takes time to practice nurture and develop - much the same way that you developed your programming skills. Just because someone has access to a compiler does not mean they can write a program - just because you have Photoshop and does not mean that you have the skill and talent developed to use it.  few ideas for a fast start: http://www.icon-king.com/projects/nuvola/ http://tango.freedesktop.org/Tango_Icon_Gallery  I've found photos and artwork at iStockphoto. You could also try Wikimedia Commons. IStockphoto is some kind of paid service right? Yes it is a pay site. You pay a one-time royalty-free fee for any work you use.  In my experience it's best to give in and pay someone. If you don't know any designers offhand start with some of the free icon sites like interfacelift.com and look for icons that fit the style of your application. You can't use the icons in your software but it's a good place to find links for freelancers and design firms. There are some free icon sets floating around out there but the ones that are well designed match the style you're aiming for and don't have restrictive licenses are pretty rare. It's hard to pay $500 or more out of pocket but if you pick a good designer I think in most cases it will add a level of polish that you wouldn't be able to accomplish by yourself or with free graphics. If your application is solid otherwise that extra polish should pay for itself in the long run. The link below has some links you might find useful if you do decide to hire someone: http://delicious.com/mbcharbonneau/Icons  Maybe something a little less web and a little more in person is something you're looking for? You've probably got friends that doodle. Have a look at their old doodles. If you find something that fits and you like it ask them if you can use it and if they say yes copy it/scan it/digitize it do whatever you need to do to get it into a format you can use.  See also http://stackoverflow.com/questions/58833/hiring-a-freelance-graphics-person  A good source of information is Smashing Magazine. Here's a link to their icons tag. http://www.smashingmagazine.com/tag/icons/ They have a ton of free graphics and inspiration.  Go to deviantart.com they have a place where you can advertise artistic jobs. This does not provide an answer to the question. To critique or request clarification from an author leave a comment below their post. @wolvvorin he asked where to find artists to do work and I told him That is not a comment it is an answer.  StackOverflow used a graphic artist's equivalent of ""rentacoder"" called 99 Designs (thanks Wolfie). It did some sort of contest where Joel and Jeff put up a fixed amount of prize money and their requirements and graphic artists submitted designs and they choose which one won the prize. I have no idea if StackOverflow chose this site but http://www.crowdspring.com/ is a site that offers such a service.  I'll second the ""in person"" answer but go further: network. Do you know someone who does graphic design design art or animation? Start there. Or ask someone who might know people like that. The long-term answer is to meet more people like that. I realize that it doesn't help you specifically solve the right now problem but the truth is that any programmer is going to do better work if they can find talented people to fill out any potential team. Specialized roles like DBAs and Information Architects might seem like more obvious teammates for a programmer but I find designers & artists to be worth their weight in gold if they are the type of person who I can communicate with and get great work from. When I ran a contest on 99designs awhile back I picked a winner (one of the people who was also able to communicate very well) & worked with the winning designer but I also got contact information from the best 5 designers and keep it on file. Getting off topic: the same goes for other specializations: besides the usual roles I have in my list of contacts: sound guys (producers etc.) musicians marketing & ad folk voice artists video editors 3d animation IT support technicians network engineers and hardware purchasing people.  Tons of Photoshop tutorials in http://www.photoshop-pack.com/"
126,A,How can I multiply an image against the alpha of another image in Cocoa? Having loaded two different images I want to do the following operation and obtain image3. image1.red * image2.alpha = image3.red image1.green * image2.alpha = image3.green image1.blue * image2.alpha = image3.blue I wrote the code in the link below for Android and was basically looking for the same functionality in Objective-C/Cocoa. http://www.ruibm.com/?p=184 Thanks Rui The bitmap with the rounded rectangle in the code sample would be equivalent to image2. I'm not sure I understand how the above code relates to creating a rounded rectangle...unless the rounded rectangle is one either image1 or image2. If it is then you have to clamp the results to a max and/or a min. Draw a rounded rectangle path and clip your image with it. http://developer.apple.com/mac/library/documentation/GraphicsImaging/Conceptual/drawingwithquartz2d/dq%5Fimages/dq%5Fimages.html#//apple%5Fref/doc/uid/TP30001066-CH212-CJBHBCGB The point about the sample code isn't the rounded rectangle it's the fact that I apply image2's (which happens to be a bitmap with a rounded rectangle) alpha to image1 colours and get a nice anti-aliased bitmap with rounded corners. I need to generalize this because image2 isn't easy to draw via code. Try actually clicking the link and reading the documentation before you dismiss it. See all those pretty anti-aliased masks they show you how to use? Neat huh? Chill out dude! I did click the link and wanted to change my vote but stackoverflow wouldn't allow me anymore.
127,A,"How do I create a bitmap with an alpha channel on the fly using GDI? I am using layered windows and drawing a rounded rectangle on the screen. However I'd like to smooth out the jagged edges. I think that I'll need alpha blending for this. Is there a way I can do this with GDI? CreateDIBSection. Fill in the BITMAPINFOHEADER with 32bpp. Fill in the alpha channel with pre-multiplied alpha and youre good to go. AlphaBlend is the API to actually blit 32 bpp bitmaps with an aplha channel.  You can do this in C# using the LockBits method of the BitMap class (see this question for an explanation). You definitely don't want to use GetPixel and SetPixel for this as they are hideously slow (even though you'd just be manipulating the edges and not the entire bitmap). Sadly I'm not using a managed language. I'll need to use vanilla Windows APIs. That's doable just not by me. =)  Any chance of using GDI+ instead of GDI? It supports antialiasing and transparency right out of the box. If I did use GDI+ I'd be stuck with the flat APIs which seem less friendly to use. Though if GDI+ made my job easier I'd be tempted to switch to something more 'class friendly'. I'll check this out; thanks!  There isn't an easy way to do such drawing with just GDI calls. What you want isn't just alpha blending: you want anti-aliasing. That usually involves drawing what you want at a larger resolution and then scaling down. What I've done in the past for similar problems is to use an art program to draw whatever shape I want (e.g. a rounded corner) much larger than I needed it in black and white. When I wanted to draw it I would scale the black and white bitmap to whatever size I wanted (using a variant of a scaling class from Code Project). This gives me a grayscale image that I can use as an alpha channel which I'd then use for alpha blending either by calling the Win32 function AlphaBlend or by using a DIBSection and manually changing the appropriate pixels. Another variation of this approach would be to allocate a DIBSection about four times larger than you wanted the final result draw into that and then use the above scaling class to scale it down: the scaling from a larger image will give the appropriate smoothing effect. If all this sounds like quite a lot of work: well it is. EDIT: To answer the title of this question: you can create a bitmap with an alpha channel by calling CreateDIBSection. That won't on its own do what you want though I think. I stumbled across CreateDIBSection while scouring MSDN this afternoon and it seems to be piece I was missing. I use Wu's line/circle algorithms for drawing anti-aliased lines and circles; as long as I have access to the actual pixel data I can set it to whatever I want. And you're right; it is turning out to be a lot of work! ;p It sounds like you're basically there. DIB sections are great for graphics work: you get a HBITMAP handle but you can access the raw pixel data too. Anti-aliasing is not ""scaling down"" it's adding extra shading pixels around diagonal or curved edges to give it a smoother appearance. This works MUCH better if your added pixels have an alpha component (i.e. they use alpha blending)."
128,A,"Digital or LCD-like display class? I remember seeing a tutorial on Sun's site that had a class for a digital-like display but I can no longer find it. How would you make numbers appear like a digital clock using Swing? Where do you want to display those numbers ? console/swing app/web app/sth else ? You mean something like this? Or possibly more old school? Have a look at the Harmonic Code Blog the guy writes has provides a library called the Steel Series which does these components and many more. This entry shows of some of his clock / round gauges components. However its best to see his components in action - he provides 2 webstart demos of his components Standard Steel Series (direct link to webstart app) Extra Steel Series (direct link to webstart app)  I found this tutorial for constructing a 7 segment display component in Java. Unfortunately it's in German. Here's http://www.addison-wesley.de/Service/Krueger/kap23002.htm&ei=A_MOS7KuIpLesgah8qCnAw&sa=X&oi=translate&ct=result&resnum=9&ved=0CDUQ7gEwCA&prev=/search%3Fq%3Djava%2B%25227%2Bsegment%2522%2Bdisplay%2Bcomponent%26hl%3Den%26safe%3Doff%26sa%3DG"">Google's translation. Bah that link doesn't show up. Here it is again: http://translate.google.de/translate?hl=en&sl=de&u=http%3A//www.addison-wesley.de/Service/Krueger/kap23002.htm&ei=A%5FMOS7KuIpLesgah8qCnAw&sa=X&oi=translate&ct=result&resnum=9&ved=0CDUQ7gEwCA&prev=/search%3Fq%3Djava%2B%25227%2Bsegment%2522%2Bdisplay%2Bcomponent%26hl%3Den%26safe%3Doff%26sa%3DG Again? translation . ('m gonna bug this in meta)  There is no such class in Java. That being said if you're in Swing you can adjust the Font settings for the text to a more digital look. On a website you'd do it with CSS etc."
129,A,"wxPython - PaintDC not refreshing import wx class TestDraw(wx.Panel): def __init__(selfparent=Noneid=-1): wx.Panel.__init__(selfparentid) self.SetBackgroundColour(""#FFFFFF"") self.Bind(wx.EVT_PAINTself.onPaint) def onPaint(self event): event.Skip() dc=wx.PaintDC(self) dc.BeginDrawing() width=dc.GetSize()[0] height=dc.GetSize()[1] if height<width: self.drawTestRects(dc) else: dc.Clear() dc.EndDrawing() def drawTestRects(selfdc): dc.SetBrush(wx.Brush(""#000000""style=wx.SOLID)) dc.DrawRectangle(50505050) dc.DrawRectangle(100100100100) class TestFrame(wx.Frame): def __init__(self parent title): wx.Frame.__init__(self parent title=title size=(640480)) self.mainPanel=TestDraw(self-1) self.Show(True) app = wx.App(False) frame = TestFrame(None""Test App"") app.MainLoop() This code should draw the test rectangles only when the height is less than the width and otherwise the window should remain clear. However if you mess with resizing the window the panel isn't actually redrawn unless it is moved off the window. What am I doing wrong? The documentation for a SizeEvent claims that there may be some complications when drawing depends on the dimensions of the window. I do not know exactly what is going on behind the scenes. I followed the suggestion on the link and added the call self.Refresh() to the top of onPaint() and this seems to give the desired behavior. See mghie's answer for a more efficient example of working code. That way every paint action causes a new one. Do you get 100% processor load? No I don't but I shouldn't need to rethrow a paint event. :P (just read the doc for wx.Window.Refresh() and saw that it throws a paint event too). I was more trying to point in the direction as to why it's happening.  You can bind a method to handle wx.EVT_SIZE or the panel and invalidate it there. Alternatively simply use the wx.FULL_REPAINT_ON_RESIZE for the panel."
130,A,"create images clickable on jpanel hi How i can add icon (car earth or other) image that can be clickabel by user? i want to add them on an jpanel with overrided paint method. Do you mean click and drag? Just use a JLabel with an icon. Then add a MouseListener to listen for clicks. JLabel label = new JLabel(yourIcon); // probably an ImageIcon label.addMouseListener(new MouseAdapter(){ public void mouseClicked(MouseEvent e) { System.out.println(""Click at: "" + e.getPoint(); } }); You should not handle mouseClicked. The standard is to handle mousePressed (or released depending on your requirement). That is another reason why a button is preferred. It collapses the pressed/released into an ActionEvent. Whereas a mouseClicked will not fire if the mouse is moved by even a pixel between the pressed and released events.  The easiest way is to add an Icon to a JButton then you can use an ActionLlistener to handle the mouse click. You can also use: button.setBorderPainted( false ); to get rid of the border so it looks like a label. Using a button can sometimes have issues in various look and feels especially if the image has transparent images - i.e. in rollover or armed state painting. How would this be different when using an Icon and text on a button the way it is normally used?. Would you not have the same problems? This sounds like a problem with the LAF although I've never seen it with Metal or Windows."
131,A,"What Are High-Pass and Low-Pass Filters? Graphics and audio editing and processing software often contain functions called ""High-Pass Filter"" and ""Low-Pass Filter"". Exactly what do these do and what are the algorithms for implementing them? Here is a super simple example of a low pass filter in C++ that processes the signal one sample at a time: float lopass(float input float cutoff) { lo_pass_output= outputs[0]+ (cutoff*(input-outputs[0])); outputs[0]= lo_pass_output; return(lo_pass_output); } Here is pretty much the same thing except it's high pass: float hipass(float input float cutoff) { hi_pass_output=input-(outputs[0] + cutoff*(input-outputs[0])); outputs[0]=hi_pass_output; return(hi_pass_output); }  I have a page describing a very simple implementation of a frame rate-independent low-pass filter that I commonly use for smoothing out user input or graphing frame rates. http://phrogz.net/js/framerate-independent-low-pass-filter.html  Here is a good resource to get you started: The Scientist and Engineer's Guide to Digital Signal Processing +1 Thanks for the link  They are generally Electrical circuits that tend to pass parts of analog signals. High pass tends to transmit more of the high frequency parts and low pass tends to pass more of the low frequency parts. They can be simulated in software. A walking average can act as a low pass filter for instance and the difference between a walking average and it's input can work as a high pass filter.  Filtering describes the act of processing data in a way that applies different levels of attenuation to different frequencies within the data. A high pass filter will apply minimal attentuation (ie. leave levels unchanged) for high frequencies but applies maximum attenuation to low frequencies. A low pass filter is the reverse - it will apply no attenuation to low frequencies by applies attenuation to high frequencies. There are a number of different filtering algorithms that are used. The two simplest are probably the Finite Impulse Response filter (aka. FIR filter) and the Infinite Impulse Response filter (aka. IIR filter). The FIR filter works by keeping a series of samples and multiplying each of those samples by a fixed coefficient (which is based on the position in the series). The results of each of these multiplications is accumulated and is the output for that sample. This is referred to as a Multiply-Accumulate - and in dedicated DSP hardware there is a specific MAC instruction for doing just this. When the next sample is taken it's added to the start of the series and the oldest sample in the series is removed and the process repeated. The behavior of the filter is fixed by the selection of the filter coefficients. One of the simplest filters that is often provided by image processing software is the averaging filter. This can be implemented by an FIR filter by setting all of the filter coefficients to the same value.  High-pass filter lets high-frequency (detailed/local information) pass. Low-pass filter lets low-frequency (coarse/rough/global information) pass.  Wikipedia: High-pass filter Low-pass filter Band-pass filter These ""high"" ""low"" and ""band"" terms refer to frequencies. In high-pass you try to remove low frequencies. In low-pass you try to remove high. In band pass you only allow a continuous frequency range to remain. Choosing the cut-off frequency depends upon your application. Coding these filters can either be done by simulating RC circuits or by playing around with Fourier transforms of your time-based data. See the wikipedia articles for code examples.  Here is how you implement a low-pass filter using convolution: double[] signal = (some 1d signal); double[] filter = [0.25 0.25 0.25 0.25]; // box-car filter double[] result = new double[signal.Length + filter.Length + 1]; // Set result to zero: for (int i=0; i < result.Length; i++) result[i] = 0; // Do convolution: for (int i=0; i < signal.Length; i++) for (int j=0; j < filter.Length; j++) result[i+j] = result[i+j] + signal[i] * filter[j]; Note that the example is extremely simplified. It does not do range checks and does not handle the edges properly. The filter used (box-car) is a particularly bad lowpass filter because it will cause a lot of artifacts (ringing). Read up on filter design. You can also implement the filters in the frequency domain. Here is how you implement a high-pass filter using FFT: double[] signal = (some 1d signal); // Do FFT: double[] real; double[] imag; [real imag] = fft(signal) // Set the first quarter of the real part to zero to attenuate the low frequencies for (int i=0; i < real.Length / 4; i++) real[i] = 0; // Do inverse FFT: double[] highfrequencysignal = inversefft(real imag); Again this is simplified but you get the idea. The code does not look as complicated as the math. Very cool to have code samples. Why convolution in one case and FFT in the other? @dfrankow No particular reason. Just to show how it looks in the different domains. Updated the text to reflect this. Thanks. The boxcar filter (rectangle function) is indeed a very bad low pass filter. A sinc filter is better. Are you sure the first part of your answer is correct where you apply convolution in the time domain using a rectangle function? I thought a low-pass filter in the time domain required the convolution of a sinc function?"
132,A,"How do I do a iPhone button-like fill I'd like to spruce up some of the user controls I use and thought some attractive fills should do the job (part of it perhaps). Like the fill in the background of iPhone buttons or the Office 2007 ribbon bar (perhaps you know a few more). Edit: To clear things up I don't need a simple gradient fill - there is a Windows API call for that. I am more interested in how the entire effect is achieved. Here is a link to an article that shows how to do it in PhotoShop but I want to accomplish that in code. Specifically the glossy gradient at the top ends in a curve roughly in the middle of the button. I half expect that this will need to be rendered separately and blended somehow. Any ideas? If you're talking about the 3D effect of rounding or depth to a flat surface it's called a gradient. Are you talking about gradients? They are basically an interpolation (often linear) between two or more colors. These are more complex than just a linear gradient. If you look at the IPhone for example there is a gradient on the top half of the button but it is somewhat rounded at the bottom of the gradient and has lots of contrast with what is below. Here is a tutorial for doing it in PhotoShop which is of course useless in my case: http://www.keepthewebweird.com/iphone-style-icon-tutorial/ Now how do I do that in code? FWIW: ""fill algorithm"" sounds like ""flood fill algorithm"" so the question as worded is a bit confusing. What you're actually asking for is just an arbitrary composition of some gradients. Maybe if you explain which part of it you're having trouble with you'll get a more helpful answer. OK I did say I didn't know exactly what to call it (still don't). And I'm asking for help precisely because the implementation is a total unkown. I'll edit the question to better reflect what I'm looking for. The easiest way to do this is probably by assembling images based on the button size. Create a base image and resize some layers over top of it rather than trying to procedurally generating a complex gradient. +1 I've found some examples and this seems to be the common way of doing it. I'm going to go with skamradt's answer though.  You could try http://www.tmssoftware.com/site/advsmoothbutton.asp. As far as I know this control is included in a free Smooth Controls pack with D2009. For other Delphi versions you could buy the TAdvSmoothButton control itself or as part of a control pack. The smooth Controls are really great components : just try them.  You're talking about gradient fills. You can get some code samples here to start with - search that page for ""gradient"" to find the relevant sections.  The ""glass"" effect is made simply by creating a white ellipse around 4x larger than tall placing it over the image to be ""glassed"" so that the bottom arc is centered on the image then alpha blending. I believe this sample delphi code will be some help in performing the final portion."
133,A,"Draw offscreen with JOGL As part of a larger project I'm trying to implement a facility using JOGL that will export 3D renderings to bitmap formats. We do this by creating a GLJPanel and drawing the scene we want to it then extracting the bitmap. This all works fine as long as the system has at least one visible window on the screen - not necessarily the window containing the panel we are drawing to. But if we try to do this without making any window visible the GLJPanel won't draw. Stepping through the JOGL source I find that it won't draw unless it has a valid peer - essentially unless addNotify() has been called on it. The documentation says that addNotify() is only called when the panel is made part of a visible window heirarchy. Changing to a GLCanvas doesn't make much difference - the failure mode is different. WindowsOnscreenGLDrawable.realized is not set and this means lockSurface returns LOCK_SURFACE_NOT_READY causing makeCurrent() to fail. Any help would be welcome on how to create a Java app that can create and export 3D scenes without having to make it's window visible. You should look into method: glReadPixels() more info here. Basically it works more or less like this: Init(); //doing some initializations in your JOGL app glDrawBuffer(GL_BACK); DrawGLScene(); //doing drawing here glReadBuffer(GL_BACK); //Copy the image to the array imageData glReadPixels(0 0 WIDTH HEIGHT GL_RGBA GL_UNSIGNED_BYTE imageData); Thanks for the help but it's the ""DrawGLScene"" part of the code that won't work. However I seem to have fixed this. can you share your solution?  Not sure if I should be answering my own question but here goes with what I found seems to work now. The key is GLPbuffer which is an offscreen GLAutoDrawable and can be created without a visible component heirarchy. This article was helpful in getting it to work. I'll leave off accepting this answer until I've confirmed it's fully functional. I should also say that the answer came from this forum not my own meagre brain."
134,A,Simulate 3D space in 2D I'm starting to code my first game and I want to make simple 2D sprite game. However I want to simulate 3D space & physics and am searching for some tutorial/guide/algorithms that would teach me the basics... but so far without luck. Do you have any recommendations? Books? I don't care about programming language any language will do as I can read algorithms in most languages and for start I just want to understand exiting solutions for 3D -> 2D problem. Thanks! Edit: I am not so much looking into physics for now as for projecting 3D space onto 2D This is the best article I've found on subject: http://www.create-games.com/article.asp?id=2138 Another great article: http://pixwiki.bafsoft.com/mags/5/articles/circle/sincos.htm Hi. So your game will actually be 3D but the user can only walk in 2-Dimensions? If so any 3D physics book will do as the physics will be working in 3D space. Your world would be 3D but the camera will be flat on (with a slight tilt) to give a 3D view. (This isn't an answer as I just wanted to clarify what you wanted) Start with 1D or else ... Well you are right. I guess I am looking for a way to project a 3D world on 2D than take users input from that 2D and calculate how that affects my 3D world... and show it of course... lol on 1D :) [adding 15 characters] You don't say what language you're using but OpenGL and variants of it exist I believe from internet search for several common programming environments. It provides some very powerful tools for creating 3D objects setting viewports into the virtual 3D space placing lights defining textures. It might take a couple weeks of spare time to master but it certainly spares you doing much of the perspective math you would need to roll your own 3D tools. There are good tutorials on the intenet. Good luck  1980's games systems used parallax techniques to give a feeling of depth with 2D implementations. Yeah... these are the things I'm looking for... that page you gave me led me to more nice pages: http://www.gorenfeld.net/lou/pseudo/ http://en.wikipedia.org/wiki/Pole_Position_%28video_game%29  If you're talking about the process of rendering a 3D scene as a 2D image (i.e. on a screen) then you'll want to look at perspective projections. It's quite heavy on maths though and involves a lot of work with transformation matrices and linear algebra. You should make sure you're up to scratch on both linear algebra and calculus if you're planning on creating a 3D physics-based game.  If you're doing 2D you might like to start with simple 2D physics. I'd especially recommend Box2D for that purpose! It's easy to learn and integrate and it's tutorials will let you on the basics of physics in games.
135,A,"Mouse coordinates and rotation How would you go about mapping mouse coordinates to world coordinates in a 2d context? For example you have an image which you can drag. Next you apply a rotation to that image and redraw it. Now when you drag the image it does not get translated correctly. For example after rotating 90 degrees dragging up would cause the image to translate to the right. Any ideas? Currently attempting to rotate the mouse coordinates like so: mouseX = ((mouseX*Math.cos(rotation*180/Math.PI))-(mouseY*Math.sin(rotation*180/Math.PI))) mouseY = ((mouseX*Math.sin(rotation*180/Math.PI))+(mouseY*Math.cos(rotation*180/Math.PI))) But this doesn't seem to be working... You've got some image that lives in world coordinates and you're applying an affine transformation (composition of rotations translations and scaling) to it to get what it looks like on the screen: World -------[affine transformation]-------> Screen Now if you want to map something that's in screen coordinates (for instance mouse cursor position) to world coordinates you need to use the inverse of that affine transformation. If your transformation is just a rotation as in your code snippet negative rotation will do the trick. In general you'll want to represent your transformation and its inverse as 3x3 matrices A and A^-1. If W and S are world and screen affine coordinates you have S = AW and W = A^1 S. To add on another transformation T to A multiply A on the left by T and A^-1 on the right by T^-1: A = TA A^-1 = A^-1 T^-1  The conversion of degree to radian should be double angle = (double) angleOfRotation / (double)180 * 3.14;  This is because the translation is done before the rotation. Let me explain this a little bit: O=====> (translate X) | (translate Y) | x (location) When you rotate this object you'll rotate the translations with it:  O | | (translate X) | | x==< (translate Y & location) To solve this; you should first rotate the object when it's in it's origin and then translate. Rotating the object probably will also rotate the translation of that object (built-in) so you'll have to reverse the ""translation"" so it'll get at the correct point but with correct rotation. translate ---> rotate ---> inverse-translate To do this; x = x * cos(-rot) - y * sin(-rot) y = x * sin(-rot) + y * cos(-rot) Where rot is in radians."
136,A,"Bump Mapping on the iPhone Using OpenGL ES on the iPhone is it possible to do bump mapping (using normal perturbation maps)? From my google searching it seems the OpenGL ES extension that supports it doesn't allow bump mapping. According to this guy that writes gaming middleware for the iphone one can see the potential of the hardware by watching demos on th imgtec site (the marker of the iPhones graphics chip). One such demo is a bump mapping demo. But there's no source to be found. Kevin Doolan also mentions the GL extensions are not enabled for developers. @CVertex The DOT3 blending mode is supported on the iPhone which is the very least you need in order to do it. The Vertex Program extension isn't exposed on the iPhone but that it is not required. The purpose of the extension in this case is to allow you to transform the light vector into Tangent space - but as I mentioned on my blog this is something you can do on the CPU and just feed the results (as vertex colors encoded as DOT3 format normals) to GL. It will obviously be much slower than if you were able to use a Vertex Program. If you are doing Object Space bump mapping then you don't need to transform the light vector or encode it at the vertex level - you can just feed it in as a constant color. The full source for the bump mapping demo you referred to IS available on the imgtec site. Download the 1.x SDK available here. It is a demo of Object Space bump mapping (search for PolyBump). KevinD.  It seems to be possible (at least when using OpenGL ES 1.1+) but I also couldn't find any tutorials on how to do it. There's a GDC 2006 PDF that mentions it and there will be a Khronos Group course about techniques like this one. It's also mentioned in the ""OpenGL ES 1.1 in more detail"" chapter on this page."
137,A,Connect 4 C# (How to draw the grid) I've worked out most of the code and have several game classes. The one bit I'm stuck on at the moment it how to draw the actual Connect 4 grid. Can anyone tell me what's wrong with this for loop? I get no errors but the grid doesn't appear. I'm using C#. private void Drawgrid() { Brush b = Brushes.Black; Pen p = Pens.Black; for (int xCoor = XStart col = 0; xCoor < XStart + ColMax * DiscSpace; xCoor += DiscSpace col++) { // x coordinate beginning; while the x coordinate is smaller than the max column size times it by // the space between each disc and then add the x coord to the disc space in order to create a new circle. for (int yCoor = YStart row = RowMax - 1; yCoor < YStart + RowMax * DiscScale; yCoor += DiscScale row--) { switch (Grid.State[row col]) { case GameGrid.Gridvalues.Red: b = Brushes.Red; break; case GameGrid.Gridvalues.Yellow: b = Brushes.Yellow; break; case GameGrid.Gridvalues.None: b = Brushes.Aqua; break; } } MainDisplay.DrawEllipse(p xCoor yCoor 50 50); MainDisplay.FillEllipse(b xCoor yCoor 50 50); } Invalidate(); } @John Rasch - sorry we both seem to be editing. I'll stop. The code in Drawgrid() needs to be executed when the window is redrawing itself. The Invalidate() call tells the application that it needs to redraw the window contents (it triggers a redraw of your window). This code (with the exception of the Invalidate() call) should be in your overridden OnPaint() method otherwise whatever gets drawn by this code will be immediately overwritten by the default drawing code in OnPaint() (which by default will probably draw a white background) when you make the Invalidate() call. protected override void OnPaint(PaintEventArgs e) { Graphics g = e.Graphics; // (your painting code here...) } I'm sorry I'm trying to work out what you mean (I'm very new to C# and have just started a module in 'Software Development' at University). I call Drawgrid() at the end of the Form1_Load(object sender EventArgs e) is this not enough? @Matt - In a windowing system a window needs to be able to redraw its contents at various times. Rather than having a one-time method that draws on the window the drawing code is located in a callback (typically called OnDraw or OnPaint). The Operating system will call this method whenever the window's appearance needs to be refreshed. Your code must be within such a callback - this is how you do drawing in a window. Look up **OnPaint** in C# literature. Thanks for all your help. Problem solved. Phew. Sorry I don't have an onDraw() method. Before this function I have this code to store the image as a bitmap: private void Form1_Load(object sender EventArgs e) { MainBitmap = new Bitmap(this.ClientRectangle.Width this.ClientRectangle.Height System.Drawing.Imaging.PixelFormat.Format24bppRgb); MainDisplay = Graphics.FromImage(MainBitmap); MainDisplay.Clear(Color.Aqua); Drawgrid(); } Thank you for the quick reply. Sorry that it isn't nicely spaced out in the above comment. @Matt Wilde: That is not how you do custom drawing in Winforms. You need to do what Scott says inherit your board control from `System.Windows.Forms.Control` and override its `OnPaint` method. If you want to use a bitmap as some kind of buffer that is OK (not great) but then you actually need to draw the bitmap to the control surface in the `OnPaint` method using `Graphics.DrawImage`. Simply drawing on an in-memory bitmap does not make its content magically appear on screen.
138,A,How can I fill a GraphicsPath with overlapping curves? Using .NET's System.Drawing.Graphics GDI stuff I have a shape consting of two arrays of points. They are the red and green pixels in the image below. Now I am trying to fill the interior of this shape with a color. Drawing it as simple lines works just fine. Like this:  g.DrawCurve(Pens.Red points1); g.DrawCurve(Pens.Green points2); That gives the left image (1). To fill this thing I tried using a GraphicsPath like this:  GraphicsPath gp = new GraphicsPath(); gp.AddCurve(points1); gp.AddCurve(points2); g.FillPath(Brushes.Blue gp); It works... sorta. Problem is when the shape overlap itself as you can see in the middle image (2) and wont fill the overlapping part. I tried using gp.widen() to get the outline and then fill after that:  gp.Widen(new Pen(Color.Blue 3)); g.FillPath(Brushes.Blue gp); That ought to work but it only seems to fill the 3-pixel slice outside the shape not the whole thing as seen in the image (3). Any ideas how to solve this? Can you add points1 and points2 initialization? By default the GraphicsPath uses the FillMode.Alternate enumeration. You need FillMode.Winding GraphicsPath gp = new GraphicsPath(FillMode.Winding); The Winding mode considers the direction of the path segments at each intersection. It adds one for every clockwise intersection and subtracts one for every counterclockwise intersection. If the result is nonzero the point is considered inside the fill or clip area. A zero count means that the point lies outside the fill or clip area. More info here I actually tried Winding mode earlier but it completely messed up the fill. It eventually turned out to be a problem with last point != first point in the the arrays. So I reversed the points2 array and lo and behold FillMode.Winding works! I'll check in again tomorrow and mark this as answered (unless something else turns up :-)
139,A,NSBezierPath / Line Intersection / flatten I'm currently porting my jruby/java2d Graph Drawing/Layouting application to macruby/cocoa. Therefore I need to get the intersection point of an open NSBezierPath with an closed NSBezierPath. In java2d I used the following trick. I flattened both paths and did a simple line intersection test for each segment. So is there a simple way to convert a NSBezierPath to a bunch of straight lines? My current algorithm simply walks the line (in a binary search way) until I find a NSPoint for which containsPoint is true. But it only works for straight lines. The one I implemented in java2d worked for curved paths too. def getIntersection edge path out = edge.source ins = edge.target until (out.dist(ins) < 1.0) mid = out + ((ins - out) * 0.5) if (path.containsPoint (NSMakePoint(mid.x mid.y))) ins = mid else out = mid end end return out end So is there a simple way to convert a NSBezierPath to a bunch of straight lines? Send the path a bezierPathByFlatteningPath message. This will return a new path so converted. Thanks for not giving me a RTFM which I clearly deserved.
140,A,"How to create an ""inkblot"" chart with R? How can I create a chart like http://junkcharts.typepad.com/junk_charts/2010/01/leaving-ink-traces.html where several time series (one per country) are displayed horizontally as symmetric areas? I think if I could display one time series in this way it is easy to generalize to several using mfrow. Sample data: #Solar energy production in Europe by country (EC)(1 000 toe) Country199619971998199920002001200220032004200520062007 Belgium111111223335 Bulgaria------------ Czech Republic000000002234 Denmark677888999101011 Germany (including ex-GDR from 1991)5770837896150184216262353472580 Estonia------------ Ireland000000000011 Greece86899397991009999101102109160 Spain2623262933384348586583137 France151617182619191819222937 Italy8911111214161821303856 Cyprus323334353534353640414354 Latvia------------ Lithuania------------ Luxembourg (Grand-Duché)000000001222 Hungary000001222223 Netherlands678101214161920222223 Austria42485558646974808692101108 Poland000000000000 Portugal161617181819202121232428 Romania000000000000 Slovenia------------ Slovakia000000000000 Finland000011111111 Sweden445556455669 United Kingdom66771113162025303746 Croatia000000000001 Turkey159179210236262287318350375385402420 Iceland------------ Norway000000000000 Switzerland181921232426232425262830 #-='Not applicable' or 'Real zero' or 'Zero by default' :=Not available "" #Source of Data:Eurostat http://spreadsheets.google.com/ccc?key=0Agol553XfuDZdFpCQU1CUVdPZ3M0djJBSE1za1NGV0E&hl=en_GB #Last Update:30.04.2009 #Date of extraction:17 Aug 2009 07:41:12 GMT http://epp.eurostat.ec.europa.eu/tgm/table.do?tab=table&init=1&plugin=1&language=en&pcode=ten00082 I noted today there is a cool plotrix::kiteChart function With a little polishing this ggplot solution will look like what you want: Here's how to make it from your data: require(ggplot2) First let's take your input data and import and restructure it into a form ggplot likes: rdata = read.csv(""data.csv"" # options: load '-' as na ignore first comment line #Solar # strip whitespace that ends line accept numbers as col headings na.strings=""-"" skip=1 strip.white=T check.names=F) # Convert to long format and check years are numeric data = melt(rdata) data = transform(datayear=as.numeric(as.character(variable))) # geom_ribbon hates NAs. data = data[!is.na(data$value)] > summary(data) Country variable value year Austria : 12 1996 : 25 Min. : 0.00 Min. :1996 Belgium : 12 1997 : 25 1st Qu.: 0.00 1st Qu.:1999 Croatia : 12 1998 : 25 Median : 7.00 Median :2002 Cyprus : 12 1999 : 25 Mean : 36.73 Mean :2002 Czech Republic: 12 2000 : 25 3rd Qu.: 30.00 3rd Qu.:2004 Denmark : 12 2001 : 25 Max. :580.00 Max. :2007 (Other) :228 (Other):150 Now let's plot it: ggplot(data=data aes(fill=Country)) + facet_grid(Country~.space=""free"" scales=""free_y"") + opts(legend.position=""none"") + geom_ribbon(aes(x=yearymin=-valueymax=+value)) Great solution! This shows how powerful ggplot2 is when it is used by skillfull hands.  Late to this game but I created a stacked ""blot"" chart using ggplot2 and another set of data. This uses geom_polygon after the data has been smoothed out. # data: Masaaki Ishida (luna@pos.to) # http://luna.pos.to/whale/sta.html head(blue 2) ## Season Norway U.K. Japan Panama Denmark Germany U.S.A. Netherlands ## ## [1] 1931 0 6050 0 0 0 0 0 0 ## ## [2] 1932 10128 8496 0 0 0 0 0 0 ## ## U.S.S.R. South.Africa TOTAL ## ## [1] 0 0 6050 ## ## [2] 0 0 18624 hourglass.plot <- function(df) { stack.df <- df[-1] stack.df <- stack.df[sort(colnames(stack.df))] stack.df <- apply(stack.df 1 cumsum) stack.df <- apply(stack.df 1 function(x) sapply(x cumsum)) stack.df <- t(apply(stack.df 1 function(x) x - mean(x))) # use this for actual data ## coords.df <- data.frame(x = rep(c(df[1] rev(df[1])) times = dim(stack.df)[2]) y = c(apply(stack.df 1 min) as.numeric(apply(stack.df 2 function(x) c(rev(x)x)))[1:(length(df[1])*length(colnames(stack.df))*2-length(df[1]))]) id = rep(colnames(stack.df) each = 2*length(df[1]))) ## qplot(x = x y = y data = coords.df geom = ""polygon"" color = I(""white"") fill = id) # use this for smoothed data density.df <- apply(stack.df 2 function(x) spline(x = df[1] y = x)) id.df <- sort(rep(colnames(stack.df) each = as.numeric(lapply(density.df function(x) length(x$x))))) density.df <- do.call(""rbind"" lapply(density.df as.data.frame)) density.df <- data.frame(density.df id = id.df) smooth.df <- data.frame(x = unlist(tapply(density.df$x density.df$id function(x) c(x rev(x)))) y = c(apply(unstack(density.df[2:3]) 1 min) unlist(tapply(density.df$y density.df$id function(x) c(rev(x) x)))[1:(table(density.df$id)[1]+2*max(cumsum(table(density.df$id))[-dim(stack.df)[2]]))]) id = rep(names(table(density.df$id)) each = 2*table(density.df$id))) qplot(x = x y = y data = smooth.df geom = ""polygon"" color = I(""white"") fill = id) } hourglass.plot(blue[-12]) + opts(title = c(""Blue Whale Catch""))  You can use polygon in base graphics for instance x <- seq(as.POSIXct(""1949-01-01"" tz=""GMT"") length=36 by=""months"") y <- rnorm(length(x)) plot(x y type=""n"" ylim=c(-11)*max(abs(y))) polygon(c(x rev(x)) c(y -rev(y)) col=""cornflowerblue"" border=NA) Update: Using panel.polygon from lattice: library(""lattice"") library(""RColorBrewer"") df <- data.frame(x=rep(x3) y=rnorm(3*length(x)) variable=gl(3 length(x))) p <- xyplot(y~x|variable data=df ylim=c(-11)*max(abs(y)) layout=c(13) fill=brewer.pal(3 ""Pastel2"") panel=function(...) { args <- list(...) print(args) panel.polygon(c(args$x rev(args$x)) c(args$y -rev(args$y)) fill=args$fill[panel.number()] border=NA) }) print(p) This is really nice. Could you elaborate on howto stack several time series together? See my updated answer ... Looks very nice!  Using rcs' first approach here a solution for the sample data with base graphics: rawData <- read.csv(""solar.csv"" na.strings=""-"") data <- ts(t(as.matrix(rawData[2:13])) names=rawData[1] start=1996) inkblot <- function(series col=NULL min.height=40 col.value=24 col.category=17 ...) { # assumes non-negative values # assumes that series is multivariate series # assumes that series names are set i.e. colnames(series) != NULL x <- as.vector(time(series)) if(length(col)==0){ col <- rainbow(dim(series)[2]) } ytotal <- 0 for(category in colnames(series)) { y <- series[ category] y <- y[!is.na(y)] ytotal <- ytotal + max(y min.height) } oldpar = par(no.readonly = TRUE) par(mar=c(23010)+0.1 cex=0.7) plot(x 1:length(x) type=""n"" ylim=c(01)*ytotal yaxt=""n"" xaxt=""n"" bty=""n"" ylab="""" xlab="""" ...) axis(side=1 at=x) catNumber <- 1 offset <- 0 for(category in rev(colnames(series))) { print(paste(""working on: "" category)) y <- 0.5 * as.vector(series[category]) offset <- offset + max(max(abs(y[!is.na(y)])) 0.5*min.height) print(paste(""offset= "" str(offset))) polygon(c(x rev(x)) c(offset+y offset-rev(y)) col=col[catNumber] border=NA) mtext(text=y[1] side=2 at=offset las=2 cex=0.7 col=col.value) mtext(text=y[length(y)] side=4 line=-1 at=offset las=2 cex=0.7 col=col.value) mtext(text=category side=4 line=2 at=offset las=2 cex=0.7 col=col.category) offset <- offset + max(max(abs(y[!is.na(y)])) 0.5*min.height) catNumber <- catNumber + 1 } } inkblot(data) I still need to figure out the vertical grid lines and the transparent coloring. In your function plot need `y` value which doesn't exists. And `max` in `ytotal` computation fail on `NA` values. Thank you for pointing this out. I corrected the code accordingly."
141,A,"2D motion blur solutions I'm thinking of chucking motion blur into my 2D program but I doubt the results of my current algorithm. My approach looks like this at the moment: Draw to backbuffer. When time to update front buffer blend the backbuffer onto the front buffer. Repeat What would cause the ""motion blur"" effect is obviously the blending as objects in motion will leave a fading trail. This is clearly not very demanding on the hardware the double buffering will be done anyway and the only extra step is the alpha blending which is a simple calculation. However the trails will be very sharp and not blurry at all which may looks a bit strange. I could do a box blur on the back buffer before the blending step but it feels like it could be very taxing on low-end systems like the Nintendo DS. Are there any solutions that let me do it more efficiently or yield a better looking result? It might not be very sharp at all. It all depends on the blending function used. For instance 0.1/0.9 for previous/current image respectively will provide some faint trail. I might add that this is essentially how motion blurring in OpenGL is done (through accumulation buffer) Thanks for the input. Good to know that my idea wasn't too amateurish :)  There is no real way to do it more efficiently to my knowledge. Thats about as efficient as it gets. It looks pretty good if frame rate is good and movement isn't too sharp as well. The best looking plan would be to for each pixel draw a semi-transparent line from the new position to the old position. This is not a trivial calculation however. I should add that even this better looking plan will look wrong if between frames the object has not move linearly. Thanks. That's an interesting thought Goz hadnät considered that. It could look odd if an object went from one end to the screen to the other in a single frame if the opacity of the backbuffer is low.  Really you should render many intermediate frames and blend them into one result. Say for example that your output frame rate is 50 fps. You'd probably get a reasonable result if you rendered internally at 500 fps and blended groups of ten frames together before showing that to the user. The approach you are using at the moment simulates persistence as if you were rendering onto an old CRT with slow phosphor. This isn't really the same thing as motion blur. If your frame duration is 20ms (1000/50) then a motion blurred frame consists of renderings spanning the 20ms frame duration all with the same alpha weighting. The persistence solution consists of renderings from 20 40 60 80 100ms ago gradually fading out. Indeed. Depending on the kind of primitives you are rendering there are probably some optimizations you could apply. Or you could try a hybrid of my suggestion and yours. The algorithm you proposed was a standard demo coding trick back in the 90s. It can produce nice results. However if you are looking for a general purpose simple fast implementation of motion blur you are out of luck. That's why you hardly ever see it implemented. This is a good suggestion however it sounds like it could take a lot of processing power."
142,A,"Drawing two-dimensional point-graphs I've got a list of objects (probably not more than 100) where each object has a distance to all the other objects. This distance is merely the added absolute difference between all the fields these objects share. There might be few (one) or many (dozens) of fields thus the dimensionality of the distance is not important. I'd like to display these points in a 2D graph such that objects which have a small distance appear close together. I'm hoping this will convey clearly how many sub-groups there are in the entire list. Obviously the axes of this graph are meaningless (I'm not even sure ""graph"" is the correct word to use). What would be a good algorithm to convert a network of distances into a 2D point distribution? Ideally I'd like a small change to the distance network to result in a small change in the graphic so that incremental progress can be viewed as a smooth change over time. I've made a small example of the sort of result I'm looking for: Any ideas greatly appreciated David Edit: It actually seems to have worked. I treat the entire set of values as a 2D particle cloud construct inverse square repulsion forces between all particles and linear attraction forces based on inverse distance. It's not a stable algorithm the result tends to spin violently whenever an additional iteration is performed but it does always seem to generate a good separation into visual clusters: I can post the C# code if anyone is interested (there's quite a lot of it sadly) Graphviz contains implementations of several different approaches to solving this problem; consider using its spring model graph layout tools as a basis for your solution. Alternatively its site contains a good collection of source material on the related theory. Thanks moonshadow it *is* a good starting point. I'm not marking this as the answer yet as I'm hopeful for some more links.  You might want to Google around for terms such as: automatic graph layout; and force-based algorithms. GraphViz does implement some of these algorithms not sure if it includes any that are useful to you. One cautionary note -- for some algorithms small changes to your graph content can result in very large changes to the graph.  The previous answers are probably helpful but unfortunately given your description of the problem it isn't guaranteed to have a solution and in fact most of the time it won't. I think you need to read in to cluster analysis quite a bit because there are algorithms to sort your points into clusters based on a relatedness metric and then you can use graphviz or something like that to draw the results. http://en.wikipedia.org/wiki/Cluster_analysis One I quite like is a 'minimum-cut partitioning algorithm' see here: http://en.wikipedia.org/wiki/Cut_(graph_theory) @Andrew oh my that looks awfully complicated. I foresee some further forays into clustering algorithms in my near future so thank you for these links however I think the problem has been solved by a simple spring-charge system. It works fine on small and large data-sets and I can run about 500 iterations and still have the result be real-time. Ok if that gives satisfactory results it's certainly simple."
143,A,"graphics: best performance with floating point accumulation images I need to speed up some particle system eye candy I'm working on. The eye candy involves additive blending accumulation and trails and glow on the particles. At the moment I'm rendering by hand into a floating point image buffer converting to unsigned chars at the last minute then uploading to an OpenGL texture. To simulate glow I'm rendering the same texture multiple times at different resolutions and different offsets. This is proving to be too slow so I'm looking at changing something. The problem is my dev hardware is an Intel GMA950 but the target machine has an Nvidia GeForce 8800 so it is difficult to profile OpenGL stuff at this stage. I did some very unsientific profiling and found that most of the slow down is coming from dealing with the float image: scaling all the pixels by a constant to fade them out and converting the float image to unsigned chars and uploading to the graphics hardware. So I'm looking at the following options for optimization: Replace floats with uint32's in a fixed point 16.16 configuration Optimize float operations using SSE2 assembly (image buffer is a 1024*768*3 array of floats) Use OpenGL Accumulation Buffer instead of float array Use OpenGL floating-point FBO's instead of float array Use OpenGL pixel/vertex shaders Have you any experience with any of these possibilities? Any thoughts advice? Something else I haven't thought of? Thanks in advance D This sounds advansed is there any chanse we could se a screenshot? Try to replace the manual code with sprites: An OpenGL texture with an alpha of say 10%. Then draw lots of them on the screen (ten of them in the same place to get the full glow). Have you tried to reduce the alpha value over time? That should give you a fade effect too. thanks but the performance issue isn't happening in the sprite render (since i'm just drawing single pixels anyway). As I said: Stop drawing this yourself and replace each particle with several semi-transparent sprites. The more sprites you draw at one place the more ""glow"" you should get. yep tried that. 256 steps isn't enough for a nice fade... i didn't make this clear in the original question but i'm really interested in leaving nice trails and the glow is secondary. nice trails need float images so you can fade them out smoothly over a long time.  If you by ""manual"" mean that you are using the CPU to poke pixels I think pretty much anything you can do where you draw textured polygons using OpenGL instead will represent a huge speedup. hanks; i'm not poking pixels on the graphics hardware rather using additive blending on a floating point array then drawing that as a texture. the biggest problem with using textured polygons directly is subsequently getting access to/modifying the FBO so i can do trails that fade out over time  The problem is simply the sheer amount of data you have to process. Your float buffer is 9 megabytes in size and you touch the data more than once. Most likely your rendering loop looks somewhat like this: Clear the buffer Render something on it (uses reads and writes) Convert to unsigned bytes Upload to OpenGL That's a lot of data that you move around and the cache can't help you much because the image is much larger than your cache. Let's assume you touch every pixel five times. If so you move 45mb of data in and out of the slow main memory. 45mb does not sound like much data but consider that almost each memory access will be a cache miss. The CPU will spend most of the time waiting for the data to arrive. If you want to stay on the CPU to do the rendering there's not much you can do. Some ideas: Using SSE for non temporary loads and stores may help but they will complicate your task quite a bit (you have to align your reads and writes). Try break up your rendering into tiles. E.g. do everything on smaller rectangles (256*256 or so). The idea behind this is that you actually get a benefit from the cache. After you've cleared your rectangle for example the entire bitmap will be in the cache. Rendering and converting to bytes will be a lot faster now because there is no need to get the data from the relative slow main memory anymore. Last resort: Reduce the resolution of your particle effect. This will give you a good bang for the buck at the cost of visual quality. The best solution is to move the rendering onto the graphic card. Render to texture functionality is standard these days. It's a bit tricky to get it working with OpenGL because you have to decide which extension to use but once you have it working the performance is not an issue anymore. Btw - do you really need floating point render-targets? If you get away with 3 bytes per pixel you will see a nice performance improvement. thanks for your answer! i didn't make this clear in the original question but i'm really interested in leaving nice trails which need float images so you can fade them out smoothly over a long time... You could use half-floats or 16 bit per channel integers then...  It's best to move the rendering calculation for massive particle systems like this over to the GPU which has hardware optimized to do exactly this job as fast as possible. Aaron is right: represent each individual particle with a sprite. You can calculate the movement of the sprites in space (eg accumulate their position per frame) on the CPU using SSE2 but do all the additive blending and accumulation on the GPU via OpenGL. (Drawing sprites additively is easy enough.) You can handle your trails and blur either by doing it in shaders (the ""pro"" way) rendering to an accumulation buffer and back or simply generate a bunch of additional sprites on the CPU representing the trail and throw them at the rasterizer."
144,A,Ruby library/gem for game graphics? I am creating a simple Blackjack game in Ruby and I have finished all the game logic stuff (finally!) and currently have it running through the command line. It is all working so now I need a library or gem that will make the whole graphics side of things easier. Any ideas? Thanks. Maybe you could try to use Gosu which is a 2D game development library for the Ruby and C++ programming languages available for Mac OS X Windows and Linux. Available as a gem. More information here You can also watch a fun presentation I saw in Barcelona during Euruko (ruby conference). Available Here  As well as Gosu mentioned elsewhere there's Rubygame which also appears to be regularly updated. No opinion on either suggested as an alternative for comparison. UPDATE: New(-ish?) kid on the block: Ray. Fairly graphically-oriented it would appear. I have been using Rubygame for a while. It looks it is enough for a hobby project in 2D. I haven't try to do something bigger. I've just checked out both Gosu and Rubygame; both are awesome but Rubygame looks to be on the backburner - it is not being updated. Gosu has a lot more recent activity and active forums. The [Ruby tutorial for Gosu](https://github.com/jlnr/gosu/wiki/Ruby-Tutorial) is great for a beginner.  Shoes would be a good candidate it's very lightweight cross platform & fun to use. It has a nice ruby API which you can use to draw shapes and use native GUI widgets.
145,A,Blanket Alpha Component in GDI+ Is there a way I can set an Alpha value that affects all subsequent GDI/GDI+ calls? in other words I am looking to set the transparency for all calls on a Graphics object. (I'm looking for something similar to how the *Transform functions affect the Graphics object they are called on) Answer edited...I would suggest taking a float that supplies your transparency value. Unless there's something I'm not aware of there's no facility in GDI for accomplishing this. Are you looking for some way to change the relative alpha values of all of the colors used in drawing in a GDI context? I don't think anything like that exists. A better idea might be to cache your colors in your own custom repository then when you want to set a global (or scope-wide) alpha value you can use that to manipulate those colors. Obviously if I had more information I could probably come up with a solution that's more appropriate to your environment but that's all I can offer just based on what's provided in the question. Edit After reading your comment my suggestion would be to turn your painting code into a function that takes a Graphics object a location/size (and whatever else is appropriate if it isn't already this way) and a transparency float (ranging from 0 - 1.0). You can then create your colors in your function based upon the supplied transparency value. I have code which draws some shapes on being given a Graphics object. I would like to reuse the same code to paint a semi-transparent overlay during a drag-and-drop. I was hoping to do something like g->Transparency = 0.5f... :)
146,A,Anybody know of a collection of country outline maps? I've been in doubt whether to ask this here but I don't know where else to go and I think any answers can be useful to other developers... What I want to do in the community site that I am building is to have a map outline graphic on the header of the user profile. So if the user indicates he is from Brazil there will an small outline graphic of Brazil next to his avatar. All I want is the graphic I do not need an interactive map. For this to work I figure I could link up the country code to a collection of graphics each graphic outlining one country. Problem is is there a resource anywhere with such a repository I can use or buy? I have found some collections that cover part of the world but I need it to be complete and up-to-date. You might also like the GADM-Website: http://gadm.org/  Have a look at these .. http://geography.about.com/library/blank/blxindex.htm http://www.visguy.com/2007/04/23/map-of-world/ http://www.dafont.com/geobats.font http://www.webresourcesdepot.com/free-vector-world-maps-collection/ and don't forget to check the CIA factbook maps https://www.cia.gov/library/publications/the-world-factbook/docs/refmaps.html  There's also the CIA World DataBank II which probably has more detail than you need.
147,A,"Merging and splitting overlapping rectangles to produce non-overlapping ones I am looking for an algorithm as follows: Given a set of possibly overlapping rectangles (All of which are ""not rotated"" can be uniformly represented as (lefttoprightbottom) tuplets etc...) it returns a minimal set of (non-rotated) non-overlapping rectangles that occupy the same area. It seems simple enough at first glance but prooves to be tricky (at least to be done efficiently). Are there some known methods for this/ideas/pointers? Methods for not necessarily minimal but heuristicly small sets are interesting as well so are methods that produce any valid output set at all. What programming language are you using? Do you mean take the union of the rectangles and split it out into rectangular regions? Or must the union of the original lines also be expressed in the output? In other words if I have two 10x20 rectangles that abut could I output a single 20x20 square? Or do I need to always have at least two outputs if I have two inputs? The only requeirements from the (union of the) output set is to occupy the same area as the (union of the) input set and to be composed of non-overlapping rectangles. It's desirable that it'll also be minimal: include as few rects as possible. So yes you'd better spit out a single rect in that case. So if I were trying to do this the first thing I'd do is come up with a unified grid space. Find all unique x and y coordinates and create a mapping to an index space. So if you have x values { -1 1.5 3.1 } then map those to { 0 1 2 } and likewise for y. Then every rectangle can be exactly represented with these packed integer coordinates. Then I'd allocate a bitvector or something that covers the entire grid and rasterize your rectangles in the grid. The nice thing about having a grid is that it's really easy to work with and by limiting it to unique x and y coordinates it's minimal and exact. One way to come up with a pretty quick solution is just dump every 'pixel' of your grid.. run them back through your mapping and you're done. If you're looking for a more optimal number of rectangles then you've got some sort of search problem on your hands. Let's look at 4 neighboring pixels a little 2x2 square. When I write algorithms like these typically I think in terms of verts edges and faces. So these are the faces around a vert. If 3 of them are on and 1 is off then you've got a concave corner. This is the only invalid case. For example if I don't have any concave corners I just grab the extents and dump the whole thing as a single rectangle. For each concave corner you need to decide whether to split horizontally vertically or both. I think of the splitting as marking edges not to cross when finding extents. You could also do it as coloring into sets whatever is easier for you. The concave corners and their split directions are your search space.. you can use whatever optimization algorithm you'd like. Branch/bound might work well. I bet you could find a simple heuristic that performs well (for example if there's another concave corner directly across from the one you're considering always split in that direction. Otherwise split in the shorter direction). You could just go greedy. Or you could just split every concave vert in both directions which would generally give you fewer rectangles than outputting every 'pixel' as a rect and would be pretty simple. Reading over this I realize that there may be areas that are unclear. Let me know if you want me to clarify anything.  Something based on a line-sweep algorithm would work I think: Sort all of your rectangles' min and max x coordinates into an array as ""start-rectangle"" and ""end-rectangle"" events Step through the array adding each new rectangle encountered (start-event) into a current set Simultaneously maintain a set of ""non-overlapping rectangles"" that will be your output set Any time you encounter a new rectangle you can check whether it's completely contained already in the current / output set (simple comparisons of y-coordinates will suffice) If it isn't add a new rectangle to your output set with y-coordinates set to the part of the new rectangle that isn't already covered. Any time you hit a rectangle end-event stop any rectangles in your output set that aren't covering anything anymore. I'm not completely sure this covers everything but I think with some tweaking it should get the job done. Or at least give you some ideas... :) @schellsan - imagine a vertical line sweeping ""left to right"" over the input set (-x to +x). When you hit the start of any input rectangle you need to create a new output rectangle to cover it unless it's already fully-covered by one of your existing output rectangles. When you reach the end of any input rectangle if there are no other input rectangles in that space (of y-coords) you need to end some output rectangle (and perhaps create some new ones depending on the input configuration). Yes this is what I've figured. Thanks for you help! This comment has spurred some good ideas for me I've gotten most of the way through an implementation but am having trouble with some edge cases. Can you elaborate on the end-event case? What does it mean for a rectangle to 'not be covering anything anymore'?"
148,A,"When does the CPU wait on the GPU? In an application which is GPU bound I am wondering at what point the CPU will wait on the GPU to complete rendering. Is it different between DirectX and OpenGL? Running an example similar to below obviously the CPU doesn't run away - and looking in task manager CPU usage (If it were a single core machine) would be below 100%. While (running){ Clear (); SetBuffers (); // Vertex / Index SetTexture (); DrawPrimitives (); Present (); } Can't answer this without knowing what OS you are asking about That would be windows - XP upwards On Windows the waits are in the Video driver. They depend on somewhat on driver implentation though in a lot of cases the need for a wait is dictated by the requirements of the API you are using (whether calls are defined to be syncronous or not). So yes it would most likely be different between DirectX and OpenGL.  The quick summary is that you will probably see the wait in Present() but it really depends on what it is the Present() call. Generally unless you specifically say you want notice of when the GPU is finished you might end up waiting at the (random to you) point the driver's input buffer fills up. Think of the GPU driver & card as a very long pipeline. You can put in work at one end and eventually after a while it comes out to the display. You might be able to put in several frames worth of commands into the pipeline before it fills up. The card could be taking a lot of time drawing primitives but you might see the CPU waiting at a point several frames later. If your Present() call contains the equivalent of glFinish() that entire pipeline must drain before that call can return. So the CPU will wait there. I hope the following can be helpful: Clear (); Causes all the pixels in the current buffer to change color so the GPU is doing work. Lookup your GPU's clear pix/sec rate to see what time this should be taking. SetBuffers (); SetTexture (); The driver may do some work here but generally it wants to wait until you actually do drawing to use this new data. In any event the GPU doesn't do much here. DrawPrimitives (); Now here is where the GPU should be doing most of the work. Depending on the primitive size you'll be limited by vertices/sec or pixels/sec. Perhaps you have an expensive shader you'll be limited by shader instructions/sec. However you may not see this as the place the CPU is waiting. The driver may buffer the commands for you and the CPU may be able to continue on. Present (); At this point the GPU work is minimal. It just changes a pointer to start displaying from a different buffer. However this is probably the point that appears to the CPU to be where it is waiting on the GPU. Depending on your API ""Present()"" may include something like glFlush() or glFinish(). If it does then you'll likely wait here."
149,A,window handlers for opengl I've been programming opengl using glut as my window handler lately i've been thinking if there are any advantages to switching to an alternate window handler such as wxWidgets or qt. Are there any major differences at all or is it just a matter of taste? Since glut provides some additional functions for opengl-programming beyond the window handling features would there be a point in combining an additional toolkit with glut? do you want game-like windows inside your openGL window or OS level windows around your openGL window? I can only speak from experiential of using QT: Once you have the basic structure set up then it is a simple case of doing what you have always done: for example the project I am working on at the moment has an open gl widget embedded in the window. This widget has functions such as initializeGL resize...paintGL etc. Advantages include the ability to pass variables to an from the other windows / widgets etc. QT also has additional functions for handelling mouse clicks and stuff (great for 2d stuff 32d stuff requires some more complex maths)  SDL while not just a window handler will make it easier to use OpenGL than raw Win32 code. However my experience with Qt GTK and wxWidgets has not been too bad... certainly not that much better than Win32 its probably a matter of taste in those cases. I'd recommend avoiding widgets and wrappers like GLUT if you want a fine level of control over the window and resources but if you are just looking for speed of development then these tools are ideal.  Well - glut is okay to get a prototype going. It depends on the OS but later on you may want to prevent the screen-saver from starting you may want to disable the task switch (only the bad guys do this though) You may want to react to power down events. You may find out that glut can't deal with dead-keys on one or another operation system or or or... There are thousand reasons why you may want to get rid of it. It's a framework designed to make the start easy and do 90% of the usual stuff you need but it can never do 100%. You can always hack yourself into glut or lift the init-code but one day you will find out that it's easier to redesign the init-code from scratch and adjust it to your task. Opening a window and initializing OpenGL is not rocket-science . Use glut as long as it works for you but as soon as it makes problems get rid of it. It'll take you just a hand full of hours so you won't loose much. For a toy project glut is the way to go though.  You need to move from glut as soon as you want more complex controls and dialogs etc. QT has an excellent openGL widget there is also an interesting article in the newsletter about drawing controls ontop of GL to give cool WPF style effects. wxWidgets also comes with an opengl example but I don't have much experience of it.
150,A,How many FPS should I have for updating a custom progress bar? I just wrote a custom progress bar it's single buffered and will remain so. How many frames per second are desirable for something like this? I don't want to waste too much CPU updating the screen unnecessarily. I'd be happy with an update per second or two for functional purposes. 10-20 fps if you want it to look good.  You might want to go the other way around and update the progress bar whenever there is a pixel of bar to update. If you have a 200 pixel bar then update it when each 0.5% of the processing is completed. That’s every 300 ms for a 1 minute process but every 4.5 s for a 15 min process. As the examples indicate the fps will generally be slower than you’d need for smooth large-motion animation; otherwise you wouldn’t need a progress bar. Depending on your design it may be easier to have the process announce to the progress bar each time it completes x% than to have the progress bar keep checking process every n ms.  As a rule of thumb 10 fps is a reasonable minimum for very small simple animations with motion. 30 fps is a minimum for more complex motion and/or larger scenes. However generally progress bars have very little change from frame to frame. If you are using a very simple animation you may find that less than 10fps works. I suggest starting at 10fps and checking the result. Tune from there. This was the best answer due to the fact that I omitted. I wasn't really writing a progress bar more of an in-progress bar. Just goes back and forth until an unmeasurable http upload completes.
151,A,"Any code/library to scale down an UIImage? Is there any code or library out there that can help me scale down an image? If you take a picture with the iPhone it is something like 2000x1000 pixels which is not very network friendly. I want to scale it down to say 480x320. Any hints? Why do you want to scale it? Is it just for display or upload? See http://vocaro.com/trevor/blog/2009/10/12/resize-a-uiimage-the-right-way/ - this has a set of code you can download as well as some descriptions. If speed is a worry you can experiment with using CGContextSetInterpolationQuality to set a lower interpolation quality than the default.  Please note this is NOT my code. I did a little digging and found it here. I figured you'd have to drop into the CoreGraphics layer but wasn't quite sure of the specifics. This should work. Just be careful about managing your memory. // ============================================================== // resizedImage // ============================================================== // Return a scaled down copy of the image. UIImage* resizedImage(UIImage *inImage CGRect thumbRect) { CGImageRef imageRef = [inImage CGImage]; CGImageAlphaInfo alphaInfo = CGImageGetAlphaInfo(imageRef); // There's a wierdness with kCGImageAlphaNone and CGBitmapContextCreate // see Supported Pixel Formats in the Quartz 2D Programming Guide // Creating a Bitmap Graphics Context section // only RGB 8 bit images with alpha of kCGImageAlphaNoneSkipFirst kCGImageAlphaNoneSkipLast kCGImageAlphaPremultipliedFirst // and kCGImageAlphaPremultipliedLast with a few other oddball image kinds are supported // The images on input here are likely to be png or jpeg files if (alphaInfo == kCGImageAlphaNone) alphaInfo = kCGImageAlphaNoneSkipLast; // Build a bitmap context that's the size of the thumbRect CGContextRef bitmap = CGBitmapContextCreate( NULL thumbRect.size.width // width thumbRect.size.height // height CGImageGetBitsPerComponent(imageRef) // really needs to always be 8 4 * thumbRect.size.width // rowbytes CGImageGetColorSpace(imageRef) alphaInfo ); // Draw into the context this scales the image CGContextDrawImage(bitmap thumbRect imageRef); // Get an image from the context and a UIImage CGImageRef ref = CGBitmapContextCreateImage(bitmap); UIImage* result = [UIImage imageWithCGImage:ref]; CGContextRelease(bitmap); // ok if NULL CGImageRelease(ref); return result; } Is this an answer? Does it work? Should I upvote it?  Please see the solution I posted to this question. The question involves rotating an image 90 degrees instead of scaling it but the premise is the same (it's just the matrix transformation that is different).  This is what I am using. Works well. I'll definitely be watching this question to see if anyone has anything better/faster. I just added the below to a category on UIimage. + (UIImage*)imageWithImage:(UIImage*)image scaledToSize:(CGSize)newSize { UIGraphicsBeginImageContext( newSize ); [image drawInRect:CGRectMake(00newSize.widthnewSize.height)]; UIImage* newImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return newImage; } so what happens if the ratio is different? what will drawInRect do? Say original is 2000x1000 and I passed in 480x480 This method takes a LONG time to run sometimes more than a minute. I'm scaling down the picture taken by the iPhone 3GS camera to 500x500. Why is that I wonder? From the developer docs on UIImage - ""Draws the entire image in the specified rectangle scaling it as needed to fit."" I have noticed that it takes a while (but for me it's 4-8 seconds rather than over minute) which was why I was watching the other answers... I intend to benchmark them. Why is is that it takes < 1 second to scale a picture into a UIImageView but so many seconds to scale it into another UIImage?"
152,A,"Image in memory compression exception when getting byte array from the Image I have a bitmap object which is taking a huge part of the memory at runtime I want to compress it (JPEG format) in memory then later on use it. I am using this for the compression: MemoryStream ms = new MemoryStream(); oBmp.Save(ms System.Drawing.Imaging.ImageFormat.Jpeg); oBmp.Dispose(); oBmp = null; Image ResultImg = Image.FromStream(ms); ms.Dispose(); ms = null; I dont know if this is really saving some memory or that everything is back to normal memory consumption when I load back the Image from the stream. anyway I am later on trying to get a byte array from this Image i am using: MemoryStream ms = new MemoryStream(); imageIn.Save(ms System.Drawing.Imaging.ImageFormat.Jpeg); return ms.ToArray(); Where ImageIn is the same Image saved in the previous code. but I am getting a GDI+ exception : [A generic error occurred in GDI+.] The same code works fine if I didnt do this ""in memory compression"" but I really need it to save memory. Thanks My Mistake I should never have called: ms.Dispose();"
153,A,"Linear algebra for graphics in C I'm developing software that writes to a tiny LCD screen (less than 1"" x 1""). I've got all the usual suspects - lines filled polygons fonts etc. I remember however learning how to do fun vector manipulation in linear algebra many moons ago and creating rotating wireframe objects. I'd like to do that again but figured there must be a quick and dirty tutorial and/or simple vector library in C that does all the heavy lifting so I can skip the implementation and go right to the eye candy. Any pointers? @ska Aw no eye-candy for me... ;-P I used to use LAPACK++ (http://math.nist.gov/lapack++/) but I see it is now being replaced with TNT (http://math.nist.gov/tnt/) For simple rotations you might just rather coding a matrix type and implementing matrix multiplication LAPACK & friends are for matrix decompositions big linear systems 1000x1000 matrices and up i.e. number crunching. I would opt for the second choice."
154,A,"Graphics.CopyFromScreen from Web = The handle is invalid In an asp.net app I try to get screenshot of server. Code works OK in VS.NET's web server but on IIS (even on local) I get the error. I've tried checking WWW Service's ""Service Interacts with Desktop"" property but didnt work. Any ideas? It is not clear what you are trying to do but it sounds like you want to do something from a web application. In that case copying something from the screen is meaningless as the server does not have a screen (in the sense that any screen connected to the server does not exist for the web application). I am trying to send ""server""'s screenshot; another application will be kept running there I think the IIS server runs as a service so there's no Desktop to get a handle to im not sure how that would work."
155,A,In OpenGL how do I make a simple background quad? Let's say I want to draw a simple quad like this: glBegin(GL_QUADS); glVertex2f(-1.0-1.0); glVertex2f(1.0-1.0); glVertex2f(1.0 1.0); glVertex2f(-1.0 1.0); glEnd(); Is there a way to draw this such that it appears behind all 3D objects and fills up the whole screen? My camera moves with the mouse so this quad also has to appear stationary with camera movement. The purpose is to make a simple background. Do I have to do some crazy transformations based on eye position/rotation or is there a simpler way using glMatrixMode()? Cant you just place it in the background box yourself in if you want it all around you. You need to draw it first to make it behind all the objects.  Here are the steps that I'd use for each frame: Clear the depth buffer with glClear(GL_DEPTH_BUFFER_BIT); Turn off depth testing with glDisable(GL_DEPTH_TEST); Load in a fixed orthographic projection with gluOrtho2D(-1.0 1.0 -1.0 1.0); Draw your quad as you described it. Turn depth testing back on. Load your your regular perspective projections and draw the rest of the view. Exactly what I was just trying to formulate into english :)  It sounds like what you want to do is similar to what you would when drawing a 2D HUD in a simple game or just a background that is persistent. I'm no expert but I did just that relatively recently. What you want to do is change the projection matrix to an orthographic projection render your quad and then switch back to whatever projection you had before. You can do this with the matrix stack and it is completely independent of any camera. So first: glMatrixMode(GL_PROJECTION); glPushMatrix(); glLoadIdentity(); int w = glutGet(GLUT_WINDOW_WIDTH); int h = glutGet(GLUT_WINDOW_HEIGHT); gluOrtho2D(0 w h 0); This pushes a new projection matrix onto the projection stack (I was using glut for this project but it should translate to just normal OpenGL). Then I get the window width and height and use gluOrtho2D to set up my orthographic projection. Next: glMatrixMode(GL_MODELVIEW); glPushMatrix(); glLoadIdentity(); // Draw your quad here in screen coordinates Here I just push a new clean matrix onto the modelview. You may not need to do this. Lastly: glPopMatrix() // Pops the matrix that we used to draw the quad glMatrixMode(GL_PROJECTION); glPopMatrix(); // Pops our orthographic projection matrix which restores the old one glMatrixMode(GL_MODELVIEW); // Puts us back into GL_MODELVIEW since this is probably what you want I hope this helps. As you can tell it requires no use of the eye position. This is mainly because when we use a new orthographic projection that perfectly fits our window this is irrelevant. This may not put your quad in the back however if this is executed after the other draw calls. EDIT: Turning off depth testing and clearing the depth buffer could help as well as Boojum suggests. An important final step is before the last line add this: glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(/*your perspective vars*/); If not the 3D models go weird.
156,A,"Imaging Question: How to determine image quality? I'm looking for ways to determine the quality of a photography (jpg). The first thing that came into my mind was to compare the file-size to the amount of pixel stored within. Are there any other ways for example to check the amount of noise in a jpg? Does anyone have a good reading link on this topic or any experience? By the way the project I'm working on is written in C# (.net 3.5) and I use the Aurigma Graphics Mill for image processing. Thanks in advance! I think you're asking about how to determine the quality of the compression process itself. This can be done by converting the JPEG to a BMP and comparing that BMP to the original bitmap from with the JPEG was created. You can iterate through the bitmaps pixel-by-pixel and calculate a pixel-to-pixel ""distance"" by summing the differences between the R G and B values of each pair of pixels (i.e. the pixel in the original and the pixel in the JPEG) and dividing by the total number of pixels. This will give you a measure of the average difference between the original and the JPEG.  Reading the number of pixels in the image can tell you the ""megapixel"" size(#pixels/1000000) which can be a crude form of programatic quality check but that wont tell you if the photo is properly focused assuming it is supposed to be focused (think fast-motion objects like trains) nor weather or not there is something in the pic worth looking at that will require a human or pigeon if you prefer. sure you can't tell whether you're dealing with a nice image or not ;) I just need to roughly check whether the image might be of poor quality technically. If you have a 3000*2000px image with a filesize of 500kb the probability is quite high that it is not of too good quality. I just created 20271 byte (19.7 KB) image that is 3000*2000 and I would say that the quality is Perfect it exactly reflects what I intended it to. http://unkwndesign.com/HQPic.png All the measurement of 3000*2000 = 1.5MB tells you is how much data is stored in the file which may or may not be indicative of it's quality. And more colors == bigger filesize what if the pic is of a starry sky? It could easily have a small file size high resolution and excelent Quality right the number of colors really is a drawback on this form of comparison. but once again it doesn't have to be too exact just a rough guideline. I would stick with something like megapixel count as resolution is usually a good enough (highly) rough determination. Some of these suggestions obviously come from a complete misunderstanding of JPEG compression (which is basically an amalgam of everything from DCTs to Huffman coding). You cannot determine the quality of the image from its filesize... its just not possible. @James Burgess - That is correct which is more or less what I stated.  I'm not entirely clear what you mean by ""quality"" if you mean the quality setting in the JPG compression algorithm then you may be able to extract it from the EXIF tags of the image (relies on the capture device putting them in and no-one else overwriting them) for your library see here: http://www.aurigma.com/Support/DocViewer/30/JPEGFileFormat.htm.aspx If you mean any other sort of ""quality"" then you need to come up with a better definition of quality. For example over-exposure may be a problem in which case hunting for saturated pixels would help determine that specific sort of quality. Or more generally you could look at statistics (mean standard deviation) of the image histogram in the 3 colour channels. The image may be out of focus in which case you could look for a cutoff in the spatial frequencies of the image Fourier transform. If you're worried about speckle noise then you could try applying a median filter to the image and comparing back to the original image (more speckle noise would give a larger change) - I'm guessing a bit here. If by ""quality"" you mean aesthetic properties of composition etc then - good luck! basically I only need to know the amount of noise and compression in a given image. Okay - so with any luck the EXIF tags will give you the compression and something based on a median filter and comparison back to the original image with give you a measure of noise: http://en.wikipedia.org/wiki/Median_filter  I wanted to do something similar but wanted the ""Soylent Green"" option and used people to rank images by performing comparisons. See the question responses here.  The 'quality' of an image is not measurable because it doesn't correspond to any particular value. If u take it as number of pixels in the image of specific size its not accurate. You might talk about a photograph taken in bad light conditions as being of 'bad quality' even though it has exactly the same number of pixels as another image taken in good light conditions. This term is often used to talk about the overall effect of an image rather than its technical specifications."
157,A,"Recommend some Bresenham's-like algorithm of sphere mapping in 2D? I need the fastest sphere mapping algorithm. Something like Bresenham's line drawing one. Something like the implementation that I saw in Star Control 2 (rotating planets). Are there any already invented and/or implemented techniques for this? I really don't want to reinvent the bicycle. Please help... Description of the problem. I have a place on the 2D surface where the sphere has to appear. Sphere (let it be an Earth) has to be textured with fine map and has to have an ability to scale and rotate freely. I want to implement it with a map or some simple transformation function of coordinates: each pixel on the 2D image of the sphere is defined as a number of pixels from the cylindrical map of the sphere. This gives me an ability to implement the antialiasing of the resulting image. Also I think about using mipmaps to implement mapping if one pixel on resulting picture is corresponding to more than one pixel on the original map (for example close to poles of the sphere). Deeply inside I feel that this can be implemented with some trivial math. But all these thoughts are just my thoughts. This question is a little bit related to this one: Textured spheres without strong distortion but there were no answers available on my question. UPD: I suppose that I have no hardware support. I want to have an cross-platform solution. Did you take a look at Jim Blinn's articles ""How to draw a sphere"" ? I do not have access to the full articles but it looks like what you need.  I'm a big fan of StarconII but unfortunately I don't remember the details of what the planet drawing looked like... The first option is triangulating the sphere and drawing it with standard 3D polygons. This has definite weaknesses as far as versimilitude is concerned but it uses the available hardware acceleration and can be made to look reasonably good. If you want to roll your own you can rasterize it yourself. Foley van Dam et al's Computer Graphics -- Principles and Practice has a chapter on Bresenham-style algorithms; you want the section on ""Scan Converting Ellipses"". For the point cloud idea I suggested in earlier comments: you could avoid runtime parameterization questions by preselecting and storing the (xyz) coordinates of surface points instead of a 2D map. I was thinking of partially randomizing the point locations on the sphere so that they wouldn't cause structured aliasing when transformed (forwards backwards whatever 8^) onto the screen. On the downside you'd have to deal with the ""fill"" factor -- summing up the colors as you draw them and dividing by the number of points. Er also you'd have the problem of what to do if there are no points; e.g. if you want to zoom in with extreme magnification you'll need to do something like look for the nearest point in that case. About Starcon2 here you can recall it: http://sc2.sourceforge.net/screenshots/slaveshield.php http://sc2.sourceforge.net/screenshots/gasgiant.php http://sc2.sourceforge.net/screenshots/titan_pcmenu.php And about the magnification we can use mipmaps when shrinking and lerp for zooming.  The standard way to do this kind of mapping is a cube map: the sphere is projected onto the 6 sides of a cube. Modern graphics cards support this kind of texture at the hardware level including full texture filtering; I believe mipmapping is also supported. An alternative method (which is not explicitly supported by hardware but which can be implemented with reasonable performance by procedural shaders) is parabolic mapping which projects the sphere onto two opposing parabolas (each of which is mapped to a circle in the middle of a square texture). The parabolic projection is not a projective transformation so you'll need to handle the math ""by hand"". In both cases the distortion is strictly limited. Due to the hardware support I recommend the cube map. re your update about cross-platform: OpenGL has cube maps as a native texture type which you can optionally mipmap. Yes thanks. But I want to have this on mobile platforms also... OIC -- you want to to do it *all* by hand! In that case either mapping will do. In my experience it's the filtering that's the hardest to implement and you can make the mapping largely independent of that. Well... yes and no. I can do algorithm significantly faster if I mix it all together. I can simplify math and so on. The sphere should work out nicely that way -- you can get (XYZ) coordinates easily rotate them with a 3x3 matrix then test and normalize the result to select and address your texture maps. The parabolic mapping should be simpler in that respect. The filtering is still hard though... ...for antialiasing don't think of the map from texture to sphere -- think of the map from texture to screen. You can't keep the edge of the sphere from being distorted so mipmaps are not enough by themselves to antialias well. Hardware uses anisotropic filtering... ...which is a pain to implement by hand not to mention slow. Actually you might consider a radical representation shift like representing your map as a point cloud and transforming the points *forward* instead of mapping pixels backwards onto a set of textures. Good luck! Hehe it's really funny idea with point cloud :) But it makes it really not *forward* but backward isn't it? I mean just because we will then be able to run through this ""cloud"" and so we will be able to have a back traced coordinates of these points.  There is a nice new way to do this: HEALPix. Advantages over any other mapping: The bitmap can be divided into equal parts (very little distortion) Very simple recursive geometry of the sphere with arbitrary precision. Example image. Thanks. I was very surprised and pleased by this simple trick too."
158,A,Application Screen capture and rendering I'm trying to write a simple app that will do a screen capture of an application and then rendering that capture in the 'main' application (ie the application that took the screen capture). I've figured out how to get the window handle and get the application's screen capture but I'm having trouble rendering the captured screen in the 'main' application. Using GDI I have the following code to render: Bitmap bit(hSrcbmphpal); graphics.DrawImage(&bitGdiplus::PointF(00)); where hSrcbmp is a bitmap of the captured screen and graphics is a GDI+ 'Graphics' object. I get the following error after the constructor call to Bitmap: Gdiplus::Image = {nativeImage=0x00000000 lastResult=Win32Error loadStatus=-858993460 } *Using Visual Studio 2005 *Windows XP *Visual C++ (non-managed) Any ideas? Another question: Any better approach? C# or DirectX or openGL? Thanks Screen capture is a Win32 FAQ for 18 years. See on win32 group for standard code (MS and other) C and C++ yes screen capture but what about re-rendering that same capture in a different window
159,A,Detecting Singularities in a Graph I am creating a graphing calculator in Java as a project for my programming class. There are two main components to this calculator: the graph itself which draws the line(s) and the equation evaluator which takes in an equation as a String and... well evaluates it. To create the line I create a Path2D.Double instance and loop through the points on the line. To do this I calculate as many points as the graph is wide (e.g. if the graph itself is 500px wide I calculate 500 points) and then scale it to the window of the graph. Now this works perfectly for most any line. However it does not when dealing with singularities. If when calculating points the graph encounters a domain error (such as 1/0) the graph closes the shape in the Path2D.Double instance and starts a new line so that the line looks mathematically correct. Example: However because of the way it scales sometimes it is rendered correctly sometimes it isn't. When it isn't the actual asymptotic line is shown because within those 500 points it skipped over x = 2.0 in the equation 1 / (x-2) and only did x = 1.98 and x = 2.04 which are perfectly valid in that equation. Example: In that case I increased the window on the left and right one unit each. My question is: Is there a way to deal with singularities using this method of scaling so that the resulting line looks mathematically correct? I myself have thought of implementing a binary search-esque method where if it finds that it calculates one point and then the next point is wildly far away from the last point it searches in between those points for a domain error. I had trouble figuring out how to make it work in practice however. Thank you for any help you may give! Why not try to derive your function to find out if there are asymptotes and where they are? I imagine doing this the right way is pretty difficult but I don't know for sure. One way to avoid this is to decrease your step size (say at most 0.01 preferably 0.001 or even less so definitely calculate more than 500 points) and not draw lines at all. If the points themselves are close enough the rendering will look just fine for most functions. However it might not for exponential functions and in general functions that grow really fast. It's also going to be slower. Just an idea until and if I think of something better. I'm also not sure if this is doable in Java but it should. @IVlad That method of calculation was how I did it before I created my current system. The reason I moved away from static steps is because of the speed. It got to the point where it would be calculating thousands of points for a 500px graph if the window was 40 units across and there would be a noticeable lag. I also tried your only-plot-points idea before I asked this question but it ended looking weird for exponential functions so I dropped the idea. I'm not sure why you mention that the graph is 40 units across. The distance should be calculated in terms of pixels not the domain of the function you are graphing. Also if plotting points you should have the number of points depend on the current slope of the function. Horizontal lines should take one point per pixel while nearly vertical lines will take more. nasufara you may want to edit this post and replace asymptotes with singularities since that is what you are really asking. The function y = 1 - exp(-x) has an asymptote as x->+\infty and y->1 but you won't have any problem graphing it. I think you are mostly on the right track. I don't think figure 2 is mathematically incorrect. For bonus points you should have a routine which checks the diff between two consecutive values y1 & y2 and if it is greater than a threshold inserts more points between y1 and y2 until no diff is greater than the threshold. If this iterative rountine is unable to get out of the while loop after 10 iterations or so then that indicates presence of a singularity and you can remove the plot between y1 and y2. That will give you figure 1. This is pretty easy to fool though. Consider the function 0.1/abs(x-1). All consecutive points are very close together (and can be made even closer by using 0.01 for example) so thresholds won't always work. I think we can also find functions where you would actually check the wrong consecutive points and not the right ones maybe by using exponential functions. Consider (e^x - 1)/abs(x) @IVlad I don't understand your problem. I think that morpheus offers a pretty good subroutine ('intervalnesting') which is also easy to implement. the only minor enhancement I would do is to define the y-threshold-value from the previous interval ... I think what are referring to is the case when there is a singularity between y1 & y2 but |y1-y2| is less than threshold. That is certainly possible. I meant what @morpheus explained. My functions are examples in which there is a singularity between y1 & y2 but |y1 - y2| might not even enter the threshold and if it does chances are all the other intervals will too so it's going to be pretty slow depending on how many times we make it iterate each interval. It's a good idea just that it won't always work. I tend to agree with @morpheus that figure 2 is not necessarily incorrect. There has been plenty of graphing software on the market which would produce graphs just like that even the high-end stuff. I think that OP's problem is writing a function to discriminate between near-vertical lines which are asymptotes and near-vertical lines which are not. Consider the plot of sin(1/x) around the origin.  You could use interval arithmetic ( http://en.wikipedia.org/wiki/Interval_arithmetic ) and calculate the interval of the function on each interval [x(i) x(i+1)]. If the resulting interval is infinite skip that line segment. Speed-wise this should only be a couple times slower than just evaluating the function.  I finally figured out a way to have singularities graphed properly. Essentially what I do is for every point on the graph I check to see if it is inside the visible graphing clip. If I hit a point on the graph that is outside the visible clip I graph that first point outside the clip and then stop graphing any invisible points after that. I keep calculating points and checking if they are inside the visible clip not graphing ones that are outside the clip. Once I hit a point that is inside the clip again I graph the point before that point and then graph the current point. I keep doing this until I've graphed the entire line. This creates the illusion that the entire line is begin drawn when only the visible parts are. This won't work if the window is large and the actual graph size in pixels is small but it does suffice for me.  If morpehus's solution is too slow for you you can consider all the absolute values of jumps between two consecutive function values and try to identifies large outliers -- these will be the infinite jumps. If you decide to try this and need help leave a comment here.
160,A,C# GDI+ X-Y Axis questions How does one find the coordinates halfway on the X and Y axis in a user control or form? How can I identify the range of the X-axis and the Range of the Y axis on a user control or form? You are probably looking for the ClientRectangle property. To find the range: do you mean the range on screen? if so use the RectangleToScreen function. To find the middle point you can use Rectangle r = this.ClientRectangle; Point p = new Point( (int)((r.X + r.Width) / 2) (int)((r.Y + r.Height) / 2)); Could this be used to find the range of the X-Y Axis or halfway point on either? It's not very clear what you mean by 'axis' maybe you should clarify that a little.  What does axis mean in your context? Given the Height and Width properties you should be able to work out the halfway position (remembering that Y is positive downwards!) If however you are implementing axes in your own units you will want to create some helper functions to convert from your units to pixels (and back potentially)
161,A,Catmull-Rom splines - how do they work? From this site which seems to have the most detailed information about catmull-rom splines: http://www.mvps.org/directx/articles/catmull/ it makes mention of needing four points to create the spline. However it does not mention how the points p0 and p3 affect the values between p1 and p2. Another question I have is how would you create continuous splines? Would it be as easy as defining the points p1 p2 to be continuous with p4 p5 by making p4 = p2 (that is assuming we have p0 p1 p2 p3 p4 p5 p6... pN). A more general question is how would one calculate tangents on catmull rom splines? Would it have to involve taking two points on the spline (say at 0.01 0.011) and getting the tangent based on pythagoras given the position coordinates those input values give? Normal Catmull-Rom is also prone to loops and self-intersection which can be a problem. I strongly recommend using the centripetal parameterization shown here: http://stackoverflow.com/questions/9489736/catmull-rom-curve-with-no-cusps-and-no-self-intersections/19283471#19283471 The Wikipedia article goes into a little bit more depth. The general form of the spline takes as input 2 control points with associated tangent vectors. Additional spline segments can then be added provided that the tangent vectors at the common control points are equal which preserves the C1 continuity. In the specific Catmull-Rom form the tangent vector at intermediate points is determined by the locations of neighboring control points. Thus to create a C1 continuous spline through multiple points it is sufficient to supply the set of control points and the tangent vectors at the first and last control point. I think the standard behavior is to use P1 - P0 for the tangent vector at P0 and PN - PN-1 at PN. According to the Wikipedia article to calculate the tangent at control point Pn you use this equation: T(n) = (P(n - 1) + P(n + 1)) / 2 This also answers your first question. For a set of 4 control points P1 P2 P3 P4 interpolating values between P2 and P3 requires information form all 4 control points. P2 and P3 themselves define the endpoints through which the interpolating segment must pass. P1 and P3 determine the tangent vector the interpolating segment will have at point P2. P4 and P2 determine the tangent vector the segment will have at point P3. The tangent vectors at control points P2 and P3 influence the shape of the interpolating segment between them. I'm sorry if I'm missing something but could you point out where in the article it gives that formula? I see a different one at http://en.wikipedia.org/wiki/Cubic_Hermite_spline#Catmull.E2.80.93Rom_spline. (I'm not all that good at math. I just wanted to know how you got the formula.)  Take a look at equation 2 -- it describes how the control points affect the line. You can see points p0 and p3 go into the equation for plotting points along the curve from P1 to P2. You'll also see that the equation gives P1 when t == 0 and P2 when t == 1. This example equation can be generalized. If you have points R0 R1 ... RN then you can plot the points between RK and RK+1 by using equation 2 with P0 = RK-1 P1 = RK P2 = RK+1 P3 = RK+2. You can't plot from R0 to R1 or from RN-1 to RN unless you add extra control points to stand in for R-1 and RN+1. The general idea is that you can pick whatever points you want to add to the head and tail of a sequence to give youself all the parameters to calculate the spline. You can join two splines together by dropping one of the control points between them. Say you have R0 R1 ... RN and S0 S1 ... SM they can be joined into R0 R1 ... RN-1 S1 S2 ... SM. To compute the tangent at any point just take the derivative of equation 2.
162,A,CGAffineTransform from two lines - each represented by a pair of CGPoints You can probably see where I am going with this - but is there any easy way to generate a CGAffineTransform from two lines - each represented by a pair of CGPoints: A[(a1xa1y) (a2xa2y)] B[(b1xb1y) (b2xb2y)] The resultant CGAffineTransform when applied to line A would of course produce line B. It potentially could involve a translation scale and rotation. Certainly I would hope to be able to write this myself after brushing up on some trig but I was wondering if anything is already available to do this? Note: I am not asking you to write this for me - I just don't want to miss a Core Graphics trick! Yes but there are at least four solutions: two translations and two scales (one per operation and direction). That's without counting transformations that both translate and scale. “The Math Behind the Matrices” from the Quartz 2D Programming Guide is a good overview of how each operation works; from that it should be simple enough to invert it and come up with a matrix for the desired operation and direction. Certainly I would hope to be able to write this myself after brushing up on some trig… You won't need that except for rotation. Thanks for the references. However I think my question wasn't clear enough so I have revised it. The pairs of points each represent a line so a transformation between the two could involve a rotation. Oh I see. Yeah that seems possible. Scale is easy: divide lengthB by lengthA. Rotation: Take the angle of each line then subtract one angle from the other. Translation: Take the lines' center points and subtract one from the other. The trickiest part will be putting them all together in the matrix in the correct order. Scale first I *think*. Thanks - that's what I'll do.
163,A,"Best direction for displaying game graphics in C# App I am making a small game as sort of a test project nothing major. I just started and am working on the graphics piece but I'm not sure the best way to draw the graphics to the screen. It is going to be sort of like the old Zelda so pretty simple using bitmaps and such. I started thinking that I could just paint to a Picture Box control using Drawing.Graphics with the Handle from the control but this seems cumbersome. I'm also not sure if I can use double buffering with this method either. I looked at XNA but for now I wanted to use a simple method to display everything. So my question. Using the current C# windows controls and framework what is the best approach to displaying game graphics (i.e. Picture Box build a custom control etc.) EDIT: I will add how I am currently drawing to the picture box. I have a Tile object that just contains the pixels for the tile ( List< List< Color>> texture; ) nothing more for simplicity. I then draw that to the pic box by iterating through the pixels and using the FillRectangle method using a brush with the current pixel color and the size specified by a scale variable: int scale = 5; for (int i = 0; i < texture.Width;) { for (int j = 0; j < texture.Height; ++j) { int x = i * scale; int y = j * scale; picBox.FillRectangle(new SolidBrush(currentPixelColor) new Rectangle(x y scale scale)); } } Yah pretty cumbersome. Any suggestions or comments are appreciated. Everyone thanks for your suggestions. Give me a few days to dink around with XNA again and look through/try out your suggestions and I will accept one of your answers. @Mike: GDI+ *can* scale small images so they're drawn larger but this isn't usually advisable in a game application. For this to look decent you have to call `DrawImage` on a `Graphics` object with its `InterpolationMode` set to `HighQualityBicubic` (or one of the other high-quality modes) which slows down the operation considerably. Your most performant approach would be to start with an image of whatever size you want and draw that without any scaling. Or draw the smaller image scaled once to a larger image and then draw the larger image unscaled. I really think you should reconsider using XNA. A simple override of `Draw()` suffice to draw an object on screen. Plus it has a handful of utility methods extensibility and so on for creating games. Can you expand on why you think it is cumbersome? If you think GDI is cimbersome I dont think XNA/DirectX/OpenGL/etc will be of any help in the simplicity category. XNA is certainly simpler and easier than drawing everything yourself but as a result it's less of a learning experience not to mention that it locks you in to the XNA way of doing things. I believe XNA is one of the simplest API as simple as GDI+ but faster. Managed Direct-X & .net 3'd party engines (such as TrueVision) are more complex. WPF is direct-x based also but more user-interface-centric :) MusiGenesis I totally agree and being locked into that framework is the main reason I wanted to look at other venues. Plus I don't think XNA is cross platform is it? Mike Webb the sample you posted is totally unnecessary. GDI will size rects for you given source and destination rects. Not to mention you have two ""new"" calls in your loop that should be hoisted out to improve performance. Neil N thanks for the comments. The code above is exactly why I am posting here. I used the scale variable because a 16x16 pixel image is tiny obviously and I wanted to scale it to the size I wanted. Will GDI+ scale small images to the size that I want them easily? Also would it be better to store the tile textures as bitmaps instead of using only their pixels (i.e. am I trying to simplify too much)? See [this answer](http://stackoverflow.com/questions/1284694/what-would-be-the-best-way-to-simulate-radar-in-c/1321945#1321945) for a sample application (source code included) that will get you started. I honestly believe XNA is by far one of the simplest game design frameworks I've used. Aside from that you could utilize WPF to draw objects on screen.  First I strongly recommend using XNA or DirectX for game development... However if you do not want XNA nor DirectX help.... then you will be forced to use .Net GDI+ painting tools..... By this way you can draw points lines circles rectangles arcs bitmaps texts and many more by GDI+ (that all is -of course- available in C# and .Net...) You may make a new simple control (or use an existing one such as image box or the form itself !) then you will have to override its OnPaint(PaintEventArgs e) function... Then you will use e.Graphics to draw whatever you want... Example: Simple Tetris Game Tutorial in C# EDIT: Using a GDI+ to draw pixel by pixel picture is quite a performance drop!! Better is to draw shapes yourself (circles.. etc)  I would recommend that you take another look at XNA and try a few samples. It is really simple to make simple games with XNA as long as you stick with 2D. The framework does an excellent job at wrapping all the details in a easy to understand flow where you basically fill in the blanks so to speak. I did a complete (but very simple) Xbox game for my son in just 8 hours without little previous experience. If you move to 3D things become more complex as you have to understand various view models shaders and so forth but for 2D it is really simple to get started. Another advantage of 2D is that the required tools are easier to get. I did all the graphics using Photoshop the sounds were MP3s and voices I recorded using the Windows recorder. For 3D you need complex tools for building and exporting models and so forth. @Brian: thanks for the link. I got all excited when I saw that xbox supports a subset of the compact framework but then I saw that it doesn't even support `System.Drawing`. Oh well. I did think about the fact that I could pop the game on the XBox which would be awesome but I wanted to go cross platform if I wanted too. Can XNA be cross platform or would I have to recode my project for another platform? Not unless you count Windows Xbox Zune and the upcoming Windows Phone as cross platform. However because of the somewhat restricted setup the developer experience is really good. You use Visual Studio and to develop and debug games running on the console. It is really smooth imo. Only drawback is the $100 license to be able to deploy to the Xbox. If you just want to do Windows games there's no license. Is there a pretty good comprehensive tutorial or help site for learning XNA? Slightly off-topic but is it possible to deploy a C# WinForms app to the Xbox? There are lots of useful tutorials on the XNA Creators Club site http://creators.xna.com/en-US/. I also picked up a couple of books but none of them were really good imo. @MusiGenesis: WinForms is not supported on the Xbox. See http://msdn.microsoft.com/en-us/library/bb198211(XNAGameStudio.10).aspx Wound up looking into XNA again. Pretty simple stuff. Looks like I'll be using it. Thanks for the suggestions and comments all of you."
164,A,How to set Anti Aliasing in Blackberry Storm? I m drawing in a bitmap like.. bitmap[i] = new Bitmap(60 60); Graphics g = new Graphics(bitmap[i]); g.setColor(Color.BLACK); g.drawLine(....); Now how to set Anti-Aliasing on before g.drawLine()? For antialiasing mode use public void setDrawingStyle(int drawStyle boolean on) For lines use graphics.setDrawingStyle(Graphics.DRAWSTYLE_AALINES true); graphics.drawLine(x1 y1 x2 y2); graphics.setDrawingStyle(Graphics.DRAWSTYLE_AALINES false); For poligons use graphics.setDrawingStyle(Graphics.DRAWSTYLE_AAPOLYGONS true); graphics.drawRect(x y width height); graphics.setDrawingStyle(Graphics.DRAWSTYLE_AAPOLYGONS false); Thanks.. I cant see its effect in Simulator. I will test it on Storm. The flag `DRAWSTYLE_AAPOLYGONS` also applies to circles and ellipses. Its effect is perceptible in simulators but you'll probably need to increase zoom to 4x.
165,A,"Per-component alpha-channels with OpenGL? I'm a complete beginner with openGL and I'm trying to see if what I want to do is feasible before I get started. I'd like to know if it's possible to perform blending with one alpha channel per component (one for red one for green and one for blue) with openGL? If not what are some possible workarounds? Thanks! You could do this with a pixel shader but you'd need to find a way to store/access per component alpha values. You could store them in a texture and treat the normal color components as the alpha values for the corresponding color. What effect/output are you trying to achieve?  This isn't something that's directly supported. It's fairly easy to achieve on your own though: Render your triangle (or whatever) using the 3-channel ""alpha"" texture and glBlendFunc(GL_ZERO GL_ONE_MINUS_SRC_COLOR) Enable multitexture and set the ""alpha"" texture to modulate your rgb texture Render your triangle with glBlendFunc(GL_ONE GL_ONE) hum that does not work with any overdraw I think ? It's the usual problem with multi-pass algorithms... @Bahbar Ordinary alpha blend doesn't work with any overdraw either. You'll have to sort the fragments on depth to get the correct result. right you do have to sort with single pass. Your solution still works if you do the multi-pass at the same sorting granularity though. I had not realized it at first. Thanks a lot! That's very helpful.  What you're describing is not possible with OpenGL. That said considering that you're new to OpenGL maybe what you're describing is not exactly what you're after ? Blending is the step that happens once you decided what color your current fragment (usually coming from a triangle) is supposed to have. That color needs to be merged with the previous color in the frame-buffer. Are you sure that's what you're after ? There are perfectly valid reasons to want to do this in the real world most objects doesn't have equal transparency over the full color spectrum (consider a stained glass window). @Andreas Brinck: Yes there are. But the OP clearly states he's completely new to OpenGL. Maybe he's not aware of exactly what blending means (I've seen it confused with fragment color computation many times before)."
166,A,any good class for generating graphs and pie charts I'm working on a project which includes generating pie and bar graph charts on the fly with user inputs. Can anyone suggest me some good libraries to do that? It can be any like jquery css php class or anything free.. And i'm going to use it mainly offline. Thanks :) possible duplicate of [Graphs/Charts in PHP](http://stackoverflow.com/questions/395541/graphs-charts-in-php) http://stackoverflow.com/questions/110839/best-graph-and-diagram-toolset-for-php http://stackoverflow.com/questions/11147/pie-chart-drawing-in-php http://stackoverflow.com/questions/1935330/php-chart-library-solution http://stackoverflow.com/questions/528263/what-is-the-best-open-source-php-charting-solution http://stackoverflow.com/questions/1156252/free-chart-libraries-for-php http://stackoverflow.com/questions/2038405/charts-using-php search SO with: [php] graph or chart i have used flot: http://code.google.com/p/flot/  this one. best ever -> http://jpgraph.net/ this is good.thanks  Just saw: http://code.google.com/apis/charttools/ Looks pretty cool although I haven't actually done any development with it yet. can i use it offline?
167,A,"graph (node and edges) in access We've got a pretty complex graph in an Access DB stored as nodes (node1 node2 etc.) and edges between the nodes (n1->n2 etc.). We'd like to visualize this in an MS Access application. What ""graphics packages"" would you suggest? How can these be integrated into Access? We're currently using Access 2002. NOTE: A tree control is not sufficient as we have a more general graph (if you displayed our graph as a tree one node would occur multiple times). Look at GLEE I think you can stil get hold of the free version (http://research.microsoft.com/en-us/downloads/f1303e46-965f-401a-87c3-34e1331d32c5/default.aspx) I've always looked at Excel as being an excellent data visualization and graphing tool in conjunction with Access. (Of course the Excel folks soemtimes don't appreciate my attitude.) Can you easily do the graph in Excel? Modules: Sample Excel Automation http://www.mvps.org/access/modules/mdl0006.htm Modules: Transferring Records to Excel with Automation http://www.mvps.org/access/modules/mdl0035.htm Sorry not a clue when it comes to node/edge graphing. Excel would be well suited if my data was a ""chart"" (like a line or bar chart) but not for a complex node&edge graph. Just one of many problems I see: How would I lay out the nodes in order to minimize the number of edges crossing?  It depends what you want your Graph visualisation to do if it is just a picture then you could try using a command line based package like GraphViz. You'd just need to write a macro to invoke GraphViz output your Graph in the DOT syntax to the GraphViz command line and then display the generated image in Access using a Form. If you want an interactive visualisation of your Graph I've never used any such package myself so can't recommend one but I'm sure there are some out there. A static display of the graph would be fine so I think I'll try your suggestion ..."
168,A,"Intersection of a line - game development I am creating a game where I want to determine the intersection of a single line. For example if I create a circle on the screen I want to determine when I have closed the circle and figure out the points that exist within the area. Edit: Ok to clarify I am attempting to create a lasso in a game and I am attempting to figure out how I can tell if the lasso's loop is closed. Is there any nice algorithm for doing this? I heard that there is one but I have not found any references searching on my own. Edit: Adding more detail I am working with an array of points. These points happen to wrap around and close. I am trying to figure out a good way of testing for this. Thanks for the help. Thoughts? Sorry but your question is not very clear for me. Do you mean that you want to compute the intersection of a single line with several objects already drawn ? In that case one solution is to compare objects not to compare each point of each object (it seems that you were talking about that but I am not sure). For example not determine if a circle cross a line you compute the distance between the line and the center of the circle and you compare with the radius of your circle. Can you detail your question ? I agree. Try to be more specific about the nature of your problem. Your question has been addressed many times in the game development literature. It falls under the broad category of ""collision detection."" If you are interested in understanding the underlying algorithms the field of computational geometry is what you want. Bounding rectangle collision detection in Java Collision detection on Stack Overflow Circle collision detection in C# Collision detection algorithms Detailed explanation of collision detection algorithms Game development books will also describe collision detection algorithms. One book of this sort is Game Physics by Eberly."
169,A,Java VolatileImage - validate() vs contentsLost() When using VolatileImage to do accerlerated Java2D operations how do you properly use the methods validate(GraphicsConfiguration) and contentsLost() to ensure that nothing goes wrong. When do you call which method? The Javadocs for VolatileImage show you exactly how to use the class. You're right. Me stupid.
170,A,X (close) shape in SVG I'm trying to make this SVG shape (ignore the color background just the X shape) Close SVG Path but don't have Illustrator and wouldn't know how to use it. Can someone help out with the general idea or point me to alternatives. I'm using it in flex. A cheap and easy way to do this is to get a piece of graph paper and mark off a 16x16 (or other dimension) block of squares. Write the numbers 0-15 along the top and left sides of the block each number corresponding to one of the small squares. Then draw your figure (in this case an X) starting with one of the squares and without lifting your pen from the paper continue the outline of it until you are back to the starting point. (This does not have to be perfect or even a very good drawing.) Next make a list of the points where the line changes direction. These are the nodes you are going to use for your drawing. Let's draw a simple downward-pointing triangle in the top left of the big block using this method. Start at 00 and continue until 07 at which point you turn and continue the line to 33 and then you pivot again and continue the line back to 00 and stop. Now you have an array of points: [new Point(00) new Point(03) new Point(33) new Point(00)]; Using the methods of the graphics object you can do the following: private function drawInvertedTriangle():void { var aryPoints:Array = [ new Point(00)  new Point(03)  new Point(33)  new Point(00)]; var bgColor:uint = 0x0000CC; // darkish blue var child:Shape = new Shape(); child.graphics.beginFill(bgColor); child.graphics.moveTo(aryPoints[0].x aryPoints[0].y); for (var i:int=1; i<aryPoints.length; i++) { var pt:Point = aryPoints[i]; // for clarity in this sample child.graphics.lineTo(pt.x pt.y) } child.graphics.endFill(); addChild(child); } I've hard-coded your points into this method but you could easily make aryPoints an argument to the function in which case it would draw any shape for which it has an array of at least 3 elements. Now for drawing the X you want you would use the graphics.drawRoundRect() method first and then draw your X on top of that. Investigate the gradientFill and other tools that are available to you using the Graphics class. Looks like this is more or less the way I'll have to do it. Thanks for the effort.
171,A,Haskell: OpenGL how to prevent immediate window closing Whenever I close OpenGL window it makes ghci console from which the app was started to immediately disappear. Is there a way to prevent this behaviour? A sample app. Thanks. before calling mainLoop you can change the actionOnWindowClose mode : actionOnWindowClose $= MainLoopReturns mainLoop Don't know if it works for non-Linux users. @saul: It fails to run properly with message `*** Exception: user error (unknown GLUT call glutSetOption check for freeglut)` in Windows though. People described the problem in [comments here](http://netsuperbrain.com/blog/posts/freeglut-windows-hopengl-hglut). Thanks for pointing to a solution anyway and there are backticks for code. ;)
172,A,"How to paint an area of a bitmap with 'transparent white'? I want to replace all pixels in a rectangular region of a Bitmap with 'transparent white' - i.e. a=0 b=255 r=255 g=255. FillRectangle doesnt do this - given a transparent brush the existing pixels are unchanged. Do I need to use SetPixel individually for each pixel in the rectangle? If you use the composite painting methods then the alpha will be used to blend the colour so nothing will happen. If you want to set the bitmap either create it from data with the background you want or set the background using LockBits to manipulate the data en-masse. You also might be able to do in using a bitblt method with the appropriate flags but I don't know how to translate that to managed code.  FillRectangle doesnt do this - given a transparent brush the existing pixels are unchanged. Makes sense since you are drawing with 0% opacity. :) A fast solution for a Control would be a color key. You can set the key color of the control to a specific color (i.e. magenta); all pixels that are of this color would then be rendered transparent. (If the control supports it of course.) That or bitmap.MakeTransparent(color) for general purposes. The problem is again that the color specified would be transparent so you'd have to choose a color that doesn't exist in the image. (There's another example here: http://msdn.microsoft.com/en-us/library/ms172507%28v=VS.80%29.aspx) Edit: In the end the LockBits() approach mentioned in the other comments could be what you seek. Once you understand how the width and stride values interact it's easy to manipulate the image at the lowest possible level. Here's a general example: http://www.bobpowell.net/lockingbits.htm Just make sure the image supports transparency (see PixelFormat) then lock the image with an appropriate mode (i.e. PixelFormat.Format32bppArgb) loop over every pixel read for bytes write your new color etc. Yes it totally makes sense :) I was just sayin. Other than that I dont think your answer is relevant - I am working with a Bitmap not a control. And although I could paint the rectangle a particular colour (and hope that that colour isnt present anywhere else in the image) and then make it transparent can I specify 'transparent white'? I dont think so. yourBitmap.MakeTransparent(Color.White) should do that. The downside is that everything white is transparent then that's true. A problem is that not every image format supports transparency so it probably also depends on how your image object is created. Have a look at the PixelFormat enumeration and member. And then of course the approach to lock the bitmap to work in the ""unsafe"" pointer dimension. What Pete said. :)  This little hack should do the trick: Graphics g = Graphics.FromImage(myBitmap); g.SetClip(new RectangleF(10 10 20 20)); g.Clear(Color.Transparent); g.RestoreClip(); g.Dispose(); Nope. That sets the pixels to ""transparent black"": 0000 not ""transparent white"": 0255255255. You could change it to `g.Clear(Color.FromArgb(0 255 255 255));` Although Hans Passant's answer is probably better  Important tip: Make sure when you create the image you do this mainImage = new Bitmap(totalWidth maxHeight PixelFormat.Format32bppARgb); and not mainImage = new Bitmap(totalWidth maxHeight PixelFormat.Format32bppRgb); It'll save you some trouble. Don't assume 32 bit means alpha channel ;-)  I believe that you need to use SetPixel (or equivalent method of setting the color values directly) to get the pixels to be ""transparent white"". You can use the Graphics.Clear method to set the color or pixels but you can't use it to set them to both transparent and a color. I tried this to set the pixels in a part of a bitmap: using (Graphics g = Graphics.FromImage(theBitmap)) { g.Clip = new Region(new Rectangle(10 10 80 80)); g.Clear(Color.FromArgb(0 Color.White)); } The pixels in the region end up as ""transparent black"": 0000. Even drawing a solid white rectangle before clearing doesn't help. When the alpha is zero in a color the other color components are also zero. Using an almost transparent alpha like 1 works fine the pixels end up as ""almost transparent white"": 1255255255.  You'll have to set the Graphics.CompositingMode property. For example: protected override void OnPaint(PaintEventArgs e) { var img = Properties.Resources.Chrysanthemum; e.Graphics.DrawImage(img 0 0); e.Graphics.CompositingMode = System.Drawing.Drawing2D.CompositingMode.SourceCopy; using (var br = new SolidBrush(Color.FromArgb(0 255 255 255))) { e.Graphics.FillRectangle(br new Rectangle(50 50 100 100)); } } The actual color you use doesn't matter you'll get a black rectangle with an alpha of 0. It's not *that* uncommon to use colour channels for data say for a map of terrain in a game where water roads rocks snow are ARGB. So you do sometimes want to be able to create bitmaps with zero alpha and non-zero RGB values. BTW the color in my case is relevant because of later processing that 'mixes' transparent areas colors with other areas colors."
173,A,"Browser graphics: Java Applet vs Flash vs anything else? We sell photoalbums which our customers create theirselves using a client album editor program (for Windows). Now we are going to develop an online program so customers could create their albums in the browser: upload photos and edit them. This is going to be a rich browser application with full graphics support. The problem is what technology to use? Our server application is build in Java and we think about Java Applets so that we could reuse some Java-code. We are also not very familiar with Flash. But some people say that Flash is preferred. Maybe there're some modern technologies now? SVG or some Google technologies (like GWT but with graphics support) or something? What do you think? Thanks in advance! UPDATE The photobook editor was created using GWT + SVG. Flash (Flash/Flex Builder) has some nice tool for integrating with back end services so probably it's good choice. No the client is in .Net. But all the server stuff - order shipment printing to PDF etc. is in Java. Is the client album editor program written in java? I would drop Java Applet I've seen so many failures there. Version problems problems on different platforms and so on and so on. Not to mention that you have to download and install JRE. For some reason flash is easier in this respect. But of course you need someone with experience on Flash if you chose that one. If you have a working Java-Editor for the albums maybe you can package it as Webstart-Application? If this way of deployment is ok you may save yourself a lot of work. http://java.sun.com/javase/technologies/desktop/javawebstart/index.jsp  Other possible options include: Javascript raw or using a Javascript toolkit like Ext-js GWT JavaFX Silverlight ... but that won't work with lots of browsers.  It depends on what tasks the tool should do regarding to editing images. If it helps here are some similar questions: Online Image Editor - Ajax or Flex / Flash?? Photoshop-like embeddable web based image editor? Free Open Source In-browser image editors HTH George  Switching from windows to another propritary solution (like Flash and Java) guides you ""out of the frying pan into the fire"". Try standards! What about HTML 5 with drag and drop GMail allows to add attachments this way.  There are four big choices you can go for: HTML5 + CSS 3 + AJAX Silverlight Flash Java HTML 5 + CSS 3 + AJAX (For the surrounding website.) As there are a lot of graphics involved and these upcoming standards are fairly new it would be hard to get everything you want accomplished in this language you could build up a large part of the site in this but the image editor itself will not be fairly easy to develop compared to other languages. Silverlight (Unless you have a lot of time... A bad choice for now...) A lot of users don't have this installed by default yet although the support for installing it is reasonable (Moonlight on other OSes different browsers can do Silverlight) if you want to reach a wide public you might want to skip this for now. This one would involve you to learn C# .NET WPF and other Microsoft technologies to get you going which will require a lot of learning time from your company. Seesmic the second popular Twitter Client however seems to use Silverlight to install and update it's desktop application in a quick way in the browser so it might not be too hard to require your users to have Silverlight. Another bonus to learning C# .NET and WPF is that you can use the learned techonologies to write desktop applications. (Which again could run on other OSes Mono's support is getting good) Java VS Flash (Stay unless you have reasons to change!) As seen on Google Trends: Java Flash Silverlight you will notice that Java and Flash are on the same level so they are both fairly supported by your users. You could best stay with Java as you have the experience in that language not to forget that you could reuse code you have written. Another bonus on staying with Java is that you don't to switch between software for developing different parts of your software infastructure. You could look up Java VS Flash on Google and only when you have the reason to change you should do so there is no point at learning something with the same (or maybe worse) capabilities when the thing you use at the moment is fine to do the job. There is nothing wrong with learning something new when you have the time for it though but you'll have the drawback of being new to it...  Java applets are a feature-rich well-established technology for embedding applications in web pages and you are a Java shop with an established code base in Java. Unless there's a particular platform that you want to target where you know that Flash support is better than Java support then in your position I would really just stick to Java.  It's all about your target client. If your target is the general public running desktop applications and your web application is more like a frontend you should stick with flash wich is already present on something like 90% of home computers. If you need something more powerfull with a lot of code already done and available and your users don't mind having the java jre installed java is the way. But it's all about the user. I suggest you to make some kind of analysis on your user base ( see if who access your download page have java or flash installed) and make your decision based ont that.  You could try Adobe Flex. It's specifically designed for RIA's and offers a lot of Flash based functionality. It uses a specific form of XML (MXML) for layout (and supports CSS) and Actionscript for scripting and has a lot of prebuild components. Also checkout Tour de Flex which is itself a Flex application containing code examples and advanced components. If the default components seem too restrictive you can use your own Flash based ones. The code is then compiled to a swf file or an air file (for desktop applications). You can also use a framework like GraniteDS to transmit data between a Java based server and the Flex client allowing you to reuse some of your code. I believe it takes advantage of things like Spring as well if your projects uses it. And there's a version of Eclipse designed for working with it: Flash Builder (previously Flex Builder). It comes as a standalone IDE or an Eclipse plug-in (it does require a licence though - 60 day trial is also available I think). I think it will be better looking and faster than Java applets (not sure about JavaFX though since I've never used it). But on the downside it will require some overhead learning how it works (more so if you plan on using GraniteDS as well) but once you get the hang of it I think it's a great tool. Edit: ActionScript is very similar to Javascript (they're both based on ECMA script) so if you know that it won't be too hard to learn AS. Also the MXML part is somewhat similar to using HTML but a lot more powerful since you can define your own classes and use them directly as a tag in the MXML part. Edit2: You could create an animated album using components like this one (demo here)"
174,A,"Java create anaglyph (red/blue image) I'm writing a Java game engine (http://victoryengine.org) and I've been experimenting with generating ""3d"" images with depth that you can see with those red/blue glasses. I'm using Java2D for graphics. I've created something that works but is very slow (by manually copying pixel values and stuff like that). What I need to is take two BufferedImages (one for the left eye one for the right) and combine them into one (either another buffer or directly to screen). For one I just want the red channel and for the other one the green and blue ones. What's the fastest way to do this? Look ath the JAI BandMerge operation: http://download.java.net/media/jai/javadoc/1.1.3/jai-apidocs/javax/media/jai/operator/BandMergeDescriptor.html Create your stereoscopic pairs as greyscale images and use band merge to combine them as red and green channels in the final image. Then where is the blue? You can add the same image twice to get green and blue if that is your requirement"
175,A,"Ray Generation Inconsistency I have written code that generates a ray from the ""eye"" of the camera to the viewing plane some distance away from the camera's eye: R3Ray ConstructRayThroughPixel(...) { R3Point p; double increments_x = (lr.X() - ul.X())/(double)width; double increments_y = (ul.Y() - lr.Y())/(double)height; p.SetX( ul.X() + ((double)i_pos+0.5)*increments_x ); p.SetY( lr.Y() + ((double)j_pos+0.5)*increments_y ); p.SetZ( lr.Z() ); R3Vector v = p-camera_pos; R3Ray new_ray(camera_posv); return new_ray; } ul is the upper left corner of the viewing plane and lr is the lower left corner of the viewing plane. They are defined as follows:  R3Point org = scene->camera.eye + scene->camera.towards * radius; R3Vector dx = scene->camera.right * radius * tan(scene->camera.xfov); R3Vector dy = scene->camera.up * radius * tan(scene->camera.yfov); R3Point lr = org + dx - dy; R3Point ul = org - dx + dy; Here org is the center of the viewing plane with radius being the distance between the viewing plane and the camera eye dx and dy are the displacements in the x and y directions from the center of the viewing plane. The ConstructRayThroughPixel(...) function works perfectly for a camera whose eye is at (000). However when the camera is at some different position not all needed rays are produced for the image. Any suggestions what could be going wrong? Maybe something wrong with my equations? Thanks for the help. I don't think you want the vector graphics tag. Vector graphics usual refers to a file format that stores data in a form that means ""draw a line from (x1y1) to (x2y2) with thickness t and color c."" whereas you will be storing data pixel by pixel no? well I AM using a scene file format to display my graphics that stores data in a similar form that you described but my question has nothing to do with that. I misunderstood what ""vector-graphics means"" -.- The reason why my code wasn't working was because I was treating xyz values separately. This is wrong since the camera can be facing in any direction and thus if it was facing down the x-axis the x coordinates would be the same producing increments of 0 (which is incorrect). Instead what should be done is an interpolation of corner points (where points have xyz coordinates). Please see answer in related post: http://stackoverflow.com/questions/2539088/3d-coordinate-of-2d-point-given-camera-and-view-plane  Here's a quibble that may have nothing to do with you problem: When you do this: R3Vector dx = scene->camera.right * radius * tan(scene->camera.xfov); R3Vector dy = scene->camera.up * radius * tan(scene->camera.yfov); I assume that the right and up vectors are normalized right? In that case you want sin not tan. Of course if the fov angles are small it won't make much difference. is there a way to directly interpolate between the upper left corner of the viewing plane and the lower left corner of the viewing plane given the pixel's (ij) coordinate without separately computing the X component and Y component? I made the change and it didn't make much of a difference. What happens is that the rays are being cast but not the entire range of the rays. So I'm assuming the mapping is not done in the correct way because multiple rays are being cast to the same pixel or the resulting value of the ""cast"" pixel is very close to the one next to it so the entire range of pixels is not being fully reconstructed (i.e. there is no exactly one ray for pixel at viewing plane position (ij)) That *sounds* like the kind of problem you get when popping back and forth from integer to floating point math but I don't see any sign of that in the code you've posted. Have you tried stepping through the code for a simple case where you get the error (in the debugger or by hand). Sometime that is the easiest way to go tedious or not. I was hoping that that would be my last resort. From the fact that it works on scenes whose camera is at (000) but doesn't work for scenes whose camera is at some other position I thought that there was some error in my logic when computing the X and Y coordinates of the generated point on the viewing plane."
176,A,Generating graphics at runtime with Cocos2D - How to display? I'm trying to create dynamic graphics for my game which I'm building with Cocos2D. The graphics generation will occur at predictable finite points such as level loading. I'm having a hard time figuring out how to actually draw this at runtime. From what I can tell the easiest way would be to draw into a PNG file at runtime and then load an AtlasSprite based on the PNG file but I can't seem to figure out if this is indeed the best way or how to go about doing it. Any suggestions? Any luck figuring it out? I think I'm starting to get there. I've cobbled together a method that compiles based on your answer now I just have to figure how why nothing is drawn. I'm not sure how Cocos2D loads Sprites or Atlases so this is a more general answer. It might be worth taking a look at the Texture2D class that comes with the old CrashLanding example app. It uses a bitmap graphics context to generate a texture of a string for drawing with OpenGL. The code uses the CGBitmapContextCreate function to create a context. You can draw whatever you want onto it. Then once you've finished drawing you can either save the file as a PNG or you can call glTexImage2D on the data to use it with OpenGL. There's more information about it in the Graphics and Drawing documentation specifically the section: Creating and Drawing Images. Edit: It looks like Cocos2D comes with Texture2D so you should be in good shape. Check out the initWithString method here.
177,A,How to install the Allegro library on Visual Studio 2008 and Windows Vista for the C programming language? How do you set up to use the full Allegro library on Windows Vista and Visual Studio 2008? Do you have to compile it or is it only a matter of setting searchpaths? There appears to be a .Net library download that and add it as a reference in your development project. That should be it.  It is possible to download a pre-compiled MSVC binary. Then it is just a matter of telling Visual Studio where to find Allegro's *.h headers and *.lib library files. Also the Allegro DLL's need to be somewhere in the system path. The Allegro wiki has instructions for configuring a Visual Studio Express 2008 project... with Allegro 4 (stable version) with Allegro 5 (development version) Check out the related question for some other useful Allegro development resources.
178,A,"What is the best way to read represent and render map data? I am interested in writing a simplistic navigation application as a pet project. After searching around for free map-data I have settled on the US Census Bureau TIGER 2007 Line/Shapefile map data. The data is split up into zip files for individual counties and I've downloaded a single counties map-data for my area. What would be the best way to read in this map-data into a useable format? How should I: Read in these files Parse them - Regular expression or some library that can already parse these Shapefiles? Load the data into my application - Should I load the points directly into some datastructure in memory? Use a small database? I have no need for persistence once you close the application of the map data. The user can load the Shapefile again. What would be the best way to render the map once I have read the in the Shapefile data? Ideally I'd like to be able to read in a counties map data shapefile and render all the poly-lines onto the screen and allow rotating and scaling. How should I: Convert lat/lon points to screen coordinates? - As far as I know the Shapefile uses longitude and latitude for its points. So obviously I'm going to have to convert these somehow to screen coordinates to display the map features. Render the map data (A series of polylines for roads boundaries etc) in a way that I can easily rotate and scale the entire map? Render my whole map as a series of ""tiles"" so only the features/lines within the viewing area are rendered? Ex. of TIGER data rendered as a display map: Anyone with some experience and insight into what the best way for me to read in these files how I should represent them (database in memory datastructure) in my program and how I should render (with rotating/scaling) the map-data on screen would be appreciated. EDIT: To clarify I do not want to use any Google or Yahoo maps API. Similarly I don't want to use OpenStreetMap. I'm looking for a more from-scratch approach than utilizing those apis/programs. This will be a desktop application. Alright it's been two years. Where's the app? If you don't mind paying for a solution Safe Software produces a product called FME. This tool will help you translate data from any format to just about any other. Including KML the Google Earth Format or render it as a JPEG (or series of JPEGs). After converting the data you can embed google earth into your application using their API or just display the tiled images. As a side not FME is a very powerful platform so while doing your translations you can add or remove parts of data that you don't necessarily need. Merge sources if you have more than one. Convert coordinates (I don't remember what exactly Google Earth uses). Store backups in a database. But seriously if your willing to shell out a few bucks you should look into this. You can also create flags (much like in your sample map) which contain a location (where to put it) and other data/comments about the location. These flags come in many shapes and sizes.  Funny question. Here's how I do it. I gather whatever geometry I need in whatever formats they come in. I've been pulling data from USGS so that amounts to a bunch of: SHP Files (ESRI Shapefile Technical Description) DBF Files (Xbase Data file (*.dbf)) I then wrote a program that ""compiles"" those shape definitions into a form that is efficient to render. This means doing any projections and data format conversions that are necessary to efficiently display the data. Some details: For a 2D application you can use whatever projection you want: Map Projections. For 3D you want to convert those latitude/longitudes into 3D coordinates. Here is some math on how to do that: transformation from spherical coordinates to normal rectangular coordinates. Break up all the primitives into a quadtree/octree (2D/3D). Leaf nodes in this tree contain references to all geometry that intersects that leaf node's (axis-aligned) bounding-box. (This means that a piece of geometry can be referenced more than once.) The geometry is then split into a table of vertices and a table of drawing commands. This is an ideal format for OpenGL. Commands can be issued via glDrawArrays using vertex buffers (Vertex Buffer Objects). A general visitor pattern is used to walk the quadtree/octree. Walking involves testing whether the visitor intersects the given nodes of the tree until a leaf node is encountered. Visitors include: drawing collision detection and selection. (Because the tree leaves can contain duplicate references to geometry the walker marks nodes as being visited and ignores them thereafter. These marks have to be reset or otherwise updated before doing the next walk.) Using a spatial partitioning system (one of the trees) and a drawing-efficient representation is crucial to achieving high framerates. I have found that in these types of applications you want your frame rate as high as possible 20 fps at a minimum. Not to mention the fact that lots of performance will give you lots of opportunities to create a better looking map. (Mine's far from good looking but will get there some day.) The spatial partitioning helps rendering performance by reducing the number of draw commands sent to the processor. However there could come a time when the user actually wants to view the entire dataset (perhaps an arial view). In this case you need a level of detail control system. Since my application deals with streets I give priority to highways and larger roads. My drawing code knows about how many primitives I can draw before my framerate goes down. The primitives are also sorted by this priority. I draw only the first x items where x is the number of primitives I can draw at my desired framerate. The rest is camera control and animation of whatever data you want to display. Here are some examples of my existing implementation: @Simucal The question is funny because the best way to deal with the data depends completely upon your expectations for the final app. Is it a 3D display or 2D? Are you displaying geographic data with height maps? Does the viewer need to zoom out to include the entire data set? Lots of questions... +1 good answer. How is the question funny? lol @Jonke The spatial partitioning is used to limit the amount that you need to process - for drawing this is view volume clipping. Now if the user attempts to use/view the whole dataset then some form of LOD control must be exerted. ""Using a spatial partitioning system (one of the trees) and a drawing-efficient representation"" can you elborate? I tried once to use openglto render a very large dataset (all the roads in eu) and I got stuck somewhere because I seemed to me that I needed to simplify all the lines to simpler ones. @Frank Krueger 2d no heightmaps. Just think of the most rudimentary drawing of the road/boundary data (including scaling/zooming and rotating). I'll be loading at most a few counties at a time so not much data at any given time. @Simucal Well then follow my advise. ;-) It may seem like overkill but I find that your map drawing code is deserving of every optimization you can throw at it. Consult the game programming literature for more.  SharpMap is an open-source .NET 2.0 mapping engine for WinForms and ASP.NET. This may provide all the functionality that you need. It deals with most common GIS vector and raster data formats including ESRI shapefiles. SharpMap is great but if you use unmodfied dataset from ex a shp/dbf you will probably get performance problems if you have a very large dataset.  for storing tiger data locally I would chose Postgresql with the postgis tools. they have an impressive collection of tools for you especially the Tiger Geocoder offers good way of importing and using the tiger data. you will need to take a look at the tools that interact with postgis most likely some sort of mapserver from http://postgis.refractions.net/documentation/: There are now several open source tools which work with PostGIS. The uDig project is working on a full read/write desktop environment that can work with PostGIS directly. For internet mapping the University of Minnesota Mapserver can use PostGIS as a data source. The GeoTools Java GIS toolkit has PostGIS support as does the GeoServer Web Feature Server. GRASS supports PostGIS as a data source. The JUMP Java desktop GIS viewer has a simple plugin for reading PostGIS data and the QGIS desktop has good PostGIS support. PostGIS data can be exported to several output GIS formats using the OGR C++ library and commandline tools (and of cource with the bundled Shape file dumper). And of course any language which can work with PostgreSQL can work with PostGIS -- the list includes Perl PHP Python TCL C C++ Java C# and more. edit: depite mapserver having the word SERVER in its name this will be usable in a desktop environment.  When I gave this answer the question was labeled ""What would be the best way to render a Shapefile (map data) with polylines in .Net?"" Now it is a different question but I leave my answer to the original question. I wrote a .net version that could draw vector-data (such as the geometry from a shp file) using plain GDI+ in c#. It was quite fun. The reason was that we needed to handle different versions of geometries and attributes with a lot of additional information so we could not use a commercial map component or an open source one. The main thing when doing this is establish a viewport and translate/transform WGIS84 coordinates to a downscale and GDI+ xy coordinates and wait with projection if you even need to reproject at all.  First I recommend that you use the 2008 TIGER files. Second as others point out there are a lot of projects out there now that already read in interpret convert and use the data. Building your own parser for this data is almost trivial though so there's no reason to go through another project's code and try to extract what you need unless you plan on using their project as a whole. If you want to start from the lower level Parsing Building your own TIGER parser (reasonably easy - just a DB of line segments) and building a simple render on top of that (lines polygons letters/names) is also going to be fairly easy. You'll want to look at various map projection types for the render phase. The most frequently used (and therefore most familiar to users) is the Mercator projection - it's fairly simple and fast. You might want to play with supporting other projections. This will provide a bit of 'fun' in terms of seeing how to project a map and how to reverse that projection (say a user clicks on the map you want to see the lat/lon they clicked - requires reversing the current projection equation). Rendering When I developed my renderer I decided to base my window on a fixed size (embedded device) and a fixed magnification. This meant that I could center the map at a lat/lon and with the center pixel=center lat/lon at a given magnification and given the mercator projection I could calculate which pixel represented each lat/lon and vice-versa. Some programs instead allow the window to vary and instead of using magnification and a fixed point they use two fixed points (often the upper left and lower right corners of a rectangle defining the window). In this case it becomes trivial to determine the pixel to lat/lon transfer - it's just a few interpolation calculations. Rotating and scaling make this transfer function a little more complex but shouldn't be considerably so - it's still a rectangular window with interpolation but the window corners don't need to be in any particular orientation with respect to north. This adds a few corner cases (you can turn the map inside out and view it as if from inside the earth for instance) but these aren't onerous and can be dealt with as you work on it. Once you've got the lat/lon to pixel transfer done rendering lines and polygons is fairly simple except for normal graphics issues (such as edges of lines or polygons overlapping inappropriately anti-aliasing etc). But rendering a basic ugly map such as it done by many open source renderers is fairly straightforward. You'll also be able to play with distance and great circle calculations - for instance a nice rule of thumb is that every degree of lat or lon at the equator is approximately 111.1KM - but one changes as you get closer to either pole while the other continues to remain at 111.1kM. Storage and Structures How you store and refer to the data however depends greatly on what you plan on doing with it. A lot of difficult problems arise if you want to use the same database structure for demographics vs routing - a given data base structure and indexing will be fast for one and slow for the other. Using zipcodes and loading only the nearby zipcodes works for small map rendering projects but if you need a route across the country you need a different structure. Some implementations have 'overlay' databases which only contain major roads and snaps routes to the overlay (or through multiple overlays - local metro county state country). This results in fast but sometimes inefficient routing. Tiling Tiling your map is actually not easy. At lower magnifications you can render a whole map and cut it up. At higher magnifications you can't render the whole thing at once (due to memory/space constraints) so you have to slice it up. Cutting lines at boundaries of tiles so you can render individual tiles results in less than perfect results - often what is done is lines are rendered beyond the tile boundary (or at least the data of the line end is kept though rendering stops once it finds it's fallen off the edge) - this reduces error that occurs with lines looking like they don't quite match as they travel across tiles. You'll see what I'm talking about as you work on this problem. It isn't trivial to find the data that goes into a given tile as well - a line may have both ends outside a given tile but travel across the tile. You'll need to consult graphics books about this (Michael Abrash's book is the seminal reference freely available now at the preceding link). While it talks mostly about gaming the windowing clipping polygon edges collision etc all apply here. However you might want to play at a higher level. Once you have the above done (either by adapting an existing project or doing the above yourself) you may want to play with other scenarios and algorithms. Reverse geocoding is reasonably easy. Input lat/lon (or click on map) and get the nearest address. This teaches you how to interpret addresses along line segments in TIGER data. Basic geocoding is a hard problem. Writing an address parser is a useful and interesting project and then converting that into lat/lon using the TIGER data is non-trivial but a lot of fun. Start out simple and small by requiring exact name and format matching and then start to look into 'like' matching and phonetic matching. There's a lot of research in this area - look at search engine projects for some help here. Finding the shortest path between two points is a non-trivial problem. There are many many algorithms for doing that most of which are patented. I recommend that if you try this go with an easy algorithm of your own design and then do some research and compare your design to the state of the art. It's a lot of fun if you're into graph theory. Following a path and pre-emptively giving instructions is not as easy as it looks on first blush. Given a set of instructions with an associated array of lat/lon pairs 'follow' the route using external input (GPS or simulated GPS) and develop an algorithm that gives the user instructions as they approach each real intersection. Notice that there are more lat/lon pairs than instructions due to curving roads etc and you'll need to detect direction of travel and so forth. Lots of corner cases you won't see until you try to implement it. Point of interest search. This one is interesting - you need to find the current location and all the points of interest (not part of TIGER make your own or get another source) within a certain distance (as the crow flies or harder - driving distance) of the origin. This one is interesting in that you have to convert the POI database into a format that is easy to search in this circumstance. You can't take the time to go through millions of entries do the distance calculation (sqrt(x^2 + y^2)) and return the results. You need to have some method or algorithm to cut the amount of data down first. Traveling salesman. Routing with multiple destinations. Just a harder version of regular routing. You can find a number of links to many projects and sources of information on this subject here. Good luck and please publish whatever you do no matter how rudimentary or ugly so others can benefit! WOW that is an excellent answer. Thanks for making the effort. Thanks it's a neat subject.  One simplification over a Mercator or other projection is to assume a constant conversion factor for the latitude and longitude. Multiply the degrees of latitude by 69.172 miles; for the longitude pick the middle latitude of your map area and multiply (180-longitude) by cosine(middle_latitude)*69.172. Once you've converted to miles you can use another set of conversions to get to screen coordinates. This is what worked for me back in 1979. My source for the number of miles per degree.  the solution is : a geospatial server like mapserver geoserver degree (opensource). They can read and serve shapefiles (and many other things). For example geoserver (when installed) serve data from US Census Bureau TIGER shapefiles as demo a javascript cartographic library like openlayers (see the examples at link text There are plenty of examples on the web using this solution Most of these 'servers' can be run locally and indeed can be run with a few parameters to spit out the images you need then exit. Still it's more work trying to get several pieces of software working in concert like this. I should point out this will be for a desktop application. How does that factor into your reply?  You could also work with Microsoft's visual earth mapping application and api or use Google's api. I have always programmed commercially with ESRI products and have not played with the open api's that much. Also you might want to look at Maker! and Finder! They are relatively new programs but I think they are free. Might be limited on embedding the data.Maker can be found here. The problem is that spatial processing is fairly new in the non commercial scale.  One solution is to use MapXtreme. They have API's for Java and C#. The API is able to load these files and render them. For Java: http://www.mapinfo.com/products/developer-tools/desktop%2c-mobile-%26-internet-offering/mapxtreme-java For .NET: http://www.mapinfo.com/products/developer-tools/desktop%2c-mobile-%26-internet-offering/mapxtreme-2008 I used this solution in a Desktop application and it worked well. It offers a lot more that only rendering information. Now doing this from scratch could take quite a while. They do have an evaluation version that you can download. I think it just prints ""MAPXTREME"" over the map as a watermark but it is completely usable otherwise  Though you already decided to use the TIGER data you might be interested in OSM (Open Street Map) beacuse OSM has a complete import of the TIGER data in it enriched with user contributed data. If you stick to the TIGER format your app will be useless to international users with OSM you get TIGER and everything else at once. OSM is an open project featuring a collaboratively edited free world map. You can get all this data as well structured XML either query for a region or download the whole world in a large file. There are some map renderers for OSM available in various programming languages most of them open source but still there is much to be done. There also is an OSM routing service avaliable. It has a web-interface and might also be queriable via a web service API. Again it's not all finished. Users could definitely use a desktop or mobile routing application built on top of this. Even if you don't decide to go with that project you can get lots of inspiration from it. Just have a look at the project wiki and at the sources of the various software projects which are involved (you will find links to them inside the wiki)."
179,A,How to create plots in multiple windows and keep them seperate in R I'm sure this is an easy problem but my google / help foo has failed me so it's up to you. I have an R script that generates several plots and I want to view all the plots on screen at once (in seperate windows) but I can't work out how to open multiple graphics windows. I'm using ggplot2 but I feel this is a more basic problem so I'm just using base grapics for this simple example x<-c(1:10) y<-sin(x) z<-cos(x) dev.new() plot(y=yx=x) dev.off() dev.new() plot(x=xy=z) But this doesn't work. I'm on Windows if this matters (Windows + Eclipse + StatEt) See this related question as well: http://stackoverflow.com/questions/1801064/how-to-separate-two-plots-in-r If you are working in Rstudio this might not work as they dont support multiple graphical devices (as of now). To have plots open in separate windows use x11() after every plot command x<-c(1:10) y<-sin(x) z<-cos(x) plot(y=yx=x) x11() plot(x=xy=z)  This works fine if you remove the line with dev.off(). Perfect I also just found the help for dev.list and dev.set which should do everything I want. Ta.
180,A,"Matlab: Adding symbols to figure Below is the user interface I have created to simulate LDPC coding and decoding The code sequence is decoded iteratively by passing values between the left and right nodes through the connections. The first thing it would be good to add in order to improve visualization is to add arrows to the connections in the direction of passing values. The alternative is to draw a bigger arrow at the top of the connection showing the direction. Another thing I would like to do is displaying the current mathematical operation below the connection (in this example c * H'). What I don't know how to do is displaying special characters and mathematical symbols and other kinds of text such as subscript and superscript in the figure (for example sum sign and subscript ""T"" instead of sign =""'"" to indicate transposed matrix). I would be very thankful if anyone could point to any useful resources for the questions above or show the solution. Thank you. In regards to the 1st question annotation (http://www.mathworks.com/access/helpdesk/help/techdoc/ref/annotation.html) might be an alternative solution. In regards to the 2nd question try text property in Matlab Help. Search ""Character Sequence"" for the special characters; search ""Specifying Subscript and Superscript Characters"" for the subscript and superscript.  For drawing the arrow I would go Jonas' suggestion arrow.m by Erik Johnson on the MathWorks File Exchange. It's the easiest way I've found to create arrows in figures. For creating text with symbols you can use the function TEXT. It lets you place text at a given point in an axes and you can use the 'tex' (default) or 'latex' options for the 'Interpreter' property to get access to different symbols. For example this places the text you want at the point (00) using 'latex' as the interpreter: hText = text(00'$\sum c*H_T$''Interpreter''latex'); The variable hText is a handle to the text object created which you can then use with the SET command to change the properties of the object (string position etc.).  To add arrows you can either use the built-in QUIVER or for more options ARROW from the file exchange. Both of these have to be plotted into axes so if you want a big arrow on the top you have to create an additional set of axes above the main axes. As far as I know you cannot use TeX or LaTeX symbols in text uicontrols. However you can use them in axes labels. Thus I suggest that you add an XLabel to the axes for example xlabel('\sigma c*H_T') or (note the $-signs required for LaTeX) xlabel('$\sum c*H_T$''interpreter''latex') EDIT I hadn't mentioned the use of text (as suggested by @gnovice and @YYC) because I thought it wasn't possible to place text outside of the axes. It turns out that I was wrong. text(0.5-0.2'\Sigma etc.') should work fine as well. I guess the only advantage of using 'xlabel' would be that you can easily add and position the axes label during GUI creation."
181,A,"Recommended method to create a ""table"" using core-animation? What would be the recommended method to create a ""table"" like display (columns rows header and footer) using core-animation which I can ""build"" using animations? I've got my data to hand in an NSDictionary and I've started laying things out but I feel I'm probably not doing it optimally; I'm creating nested sets of CALayers and working out its position of the previously-created layer's position and size. At the moment it all feels very hacky is taking much longer than I expected and seems like its going to be a nightmare to animate using CABasicAnimation. Any advice? If it helps this isn't for the iPhone but rather for the OS X desktop. It seems as though the best way to accomplish this is to create a tree of CALayers in a similar way as mimicking tables in HTML using DIVs; - ""container"" CALayer - ""row"" CALayer - ""cell"" CALayer Using CAConstraint will help with ensuring these layers don't overlay their siblings."
182,A,"Drawing C# graphics without using Windows Forms Could someone provide an example for drawing graphics without using Windows Forms? I have an app that doesn't have a console window or Windows form but i need to draw some basic graphics (lines and rectangles etc.) Hope that makes sense. Where do you need to draw some graphics ? in a file ? Tell us what your app have that the graphics can be drawn on. Im doing a mouse gestures app so i want to draw to the screen without having a visible window. Are you saying you want to draw over someone's view of their desktop and applications? Yes effectivly and then clear it once they release a key after theyve finished drawing the gesture.. Something like this?  using System.Drawing; Bitmap bmp = new Bitmap(200 100); Graphics g = Graphics.FromImage(bmp); g.DrawLine(Pens.Black 10 10 180 80);  What is point in drawing without display ? Are you looking for a control library project ? Im doing a mouse gestures app so i want to draw to the screen without having a visible window.  Right the way i've done it is with a windows form but make the background transparent and then get rid of all the borders... Thanks for the replys anyway.. J  This should give you a good start:  [TestFixture] public class DesktopDrawingTests { private const int DCX_WINDOW = 0x00000001; private const int DCX_CACHE = 0x00000002; private const int DCX_LOCKWINDOWUPDATE = 0x00000400; [DllImport(""user32.dll"")] private static extern IntPtr GetDesktopWindow(); [DllImport(""user32.dll"")] private static extern IntPtr GetDCEx(IntPtr hwnd IntPtr hrgn uint flags); [Test] public void TestDrawingOnDesktop() { IntPtr hdc = GetDCEx(GetDesktopWindow() IntPtr.Zero DCX_WINDOW | DCX_CACHE | DCX_LOCKWINDOWUPDATE); using (Graphics g = Graphics.FromHdc(hdc)) { g.FillEllipse(Brushes.Red 0 0 400 400); } } } This will be a flicker-hell  The question is a little unfocused. Specifically - where do you want to draw the lines and rectangles? Generally speaking you need a drawing surface usually provided by a windows form. Where does the need to avoid windows forms come from? Are you using another kind of window? For a windows form you could use code similar to this: namespace WindowsFormsApplication1 { public partial class Form1 : Form { public Form1() { InitializeComponent(); } protected override void OnPaint(PaintEventArgs e) { base.OnPaint(e); e.Graphics.DrawLine(new Pen(Color.DarkGreen) 11 3 20 ); e.Graphics.DrawRectangle(new Pen(Color.Black) 10 10 20 32 ); } } } You can generally do this with any object that lets you get a handle for a ""Graphics"" object (like a printer)."
183,A,How do I draw curved lines on an HTML page using JavaScript? Is it possible to draw curved lines in an HTML page (normally across cells in a table) using JavaScript (for web graph control)? Too bad IE doesn't natively support SVG/CANVAS... these would be your best choice. Raphael.js is a javascript library that lets you draw vector graphics on any webpage any browser. this library is really coll...  You can achieve this using canvas element. But isn't supported in some browsers. Drawing graphics with canvas Drawing shapes Take a look at this also Edit: The link provided by @Joeri Sebrechts http://excanvas.sourceforge.net/ yes its in HTML5. IE doesnt support this... is there any other better way... `canvas` tag is a part of a HTML5. I wouldn't recommend using it since IE doesn't support it (and IE is still browser user by most people worldwide) and other browsers that support it (Safari Firefox and Opera) also have some issues with that:http://media.liquidx.net/js/plotkit-doc/SVGCanvasCompat.html In IE I've had success using canvas by implementing it with ExplorerCanvas. It doesn't do high-performance stuff but it is adequate for many purposes. http://excanvas.sourceforge.net/ I have added your comment as an edit to my answer. Thanks @Joeri Sebrechts i just came to see the below site its claiminng that IE supports canvas. below the links for demos using canvas.. i am using IE 7.. it just works http://media.liquidx.net/js/plotkit-tests/sweet.html http://www.liquidx.net/plotkit/ am totally confused.. did they really use HTML 5 canvas element...??
184,A,"How do I force a UIImageView and its subviews to redraw in a new context? kSBCanvas is a SBCanvas which is a subclass of UIImageView. It has a few UIImageView subviews. It all renders great to the iPhone screen. I need composite the kSBCanvas and its subviews to an imageview that I want to write to disk. I do the following:  UIGraphicsBeginImageContext(kSBCanvas.bounds.size); [kSBCanvas drawRect: [kSBCanvas bounds]]; UIImage *theImage=UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); then get a PNG representation and write it to disk. The kSBCanvas renders but not the subview images. I checked and kBCanvas has subviews. Do I have to call drawRect on the subviews explicitly? Easy enough but it does not seem right. Try this instead: UIGraphicsBeginImageContext(kSBCanvas.bounds.size); { [kSBCanvas.layer renderInContext: UIGraphicsGetCurrentContext()]; UIImage *theImage = UIGraphicsGetImageFromCurrentImageContext(); } UIGraphicsEndImageContext(); Rendering the layer should also render all the subviews. Seems right but the compiler says ""no 'renderInContext:' method found? kSBCanvas is absolutely an object that's a subclass of UIImageView. I know this is simple. What's going on? Also I can't use theImage if it's bracked as shown above. No big deal. You will need to `#include ` Just remove the brackets. It is my own weird style. Brilliant. Thank you so much. It's confusing that the header has to be added but the solution is exactly what I needed."
185,A,"Simplest way to draw primitives in Java (not OpenGL) Just started to getting familiar with graphics in Java. What is the simplest way to draw primitives (lines rectangles) in Java without OpenGL (JOGL)? Looking for methods like putPixel lineTo etc. UPD. Where to paint? Canvas? Panel? Bitmap? The original Java user interface classes are called AWT. These were ""heavyweight"" components that sometimes acted differently on different systems (Windows Mac Unix). These components were difficult to use to make a GUI. Sun developed Swing which is a set of ""lightweight"" components that to the maximum degree possible work the same on different systems. These components made GUI development somewhat easier. In order to have a canvas for graphics you start with a javax.swing.JFrame. You add a child javax.swing.JPanel to the JFrame. You draw on the JPanel by overriding the paint method. The JPanel paint method takes a java.awt.Graphics as input. You can cast Graphics to java.awt.Graphics2D. The methods of Graphics2D allow you to draw rectangles images text lines and arbitrary polygons. You can find out more about Swing by reading Sun's Creating a GUI with JFC/Swing tutorial. You can find out more about 2D Graphics by reading Sun's 2D Graphics tutorial. More details on the Java classes I've mentioned can be found in the Javadoc. ""overriding the print method"" - I think you mean ""paint method"". You're correct. I've corrected my answer. Thanks for pointing out the mistake.  My recent question about horizontal scrolling in Java includes a tiny little graphics example source code that you could use as a base to work from. There are both AWT and Swing implementations. The AWT doesn't support horizontal scrolling so I'll be using swing. Not recommending these as best practice or anything they were a quick demonstration of my particular issue but it might be enough to get you started. Link is http://stackoverflow.com/questions/2843314/how-to-use-my-trackpad-for-horizontal-mousewheel-scrolling-in-a-java-awt-scrollpa  You can get a Graphics/Graphics2D instance from any AWT/Swing component via the paint method. JPanel is probably best since it fits well with swing and is lightweight meaning that only one native window is created - for the top level window. Swing components can also be double-buffered meaning that the painting is done to an offscreen buffer first before being transferred to the screen. This gives a smoother appearance and avoids flickering that can happen when painting directly to the screen and is particularly important for smooth animation. You can specifically draw to an offscreen buffer (""bitmap"") that you can use afterwards e.g. to draw an image for saving as a file later:  BufferedImage offImg = new BufferedImage(width height BufferedImage.TYPE_INT_RGB); Grapics2D g2 = offImg.createGraphics(); // .. optionally set attributes .. g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON);  The built in interface is called ""Graphics2D"". Here's a link to a Java Graphics2D Tutorial: http://java.sun.com/docs/books/tutorial/2d/index.html Thanks for adding the link Andy."
186,A,Drawing pies using PHP ImageMagick Is there a way to get GD's function imagefilledarc()'s result in using PHP ImageMagick? I've looked around but haven't found a satisfying solution so far. I want it to support transparency and use it to draw pies. As far as I know ImagickDraw::arc is the only way to do it (unless you want to create a function that draws it pixel by pixel). But to get it working the same way as gd you just have to make minor changes. For instance this: imagearc($image $cx $cy $width $height $start $end $color); Should be equivalent to this (I haven't tested it): ImagickDraw::setStrokeColor($imageMagickColor); //I don't remember how to allocate the color ImagickDraw::arc($cx-$width/2 $cy-$height/2 $cx+$width/2 $cy+$height/2 $start $end);  Since this post gets hit by googlers every now and then and doesn't have a valid answer I figured I should give some hints. There is an excellent library called Imagine (https://github.com/avalanche123/Imagine) which supports the most popular PHP image libraries (ImageMagick GraphicsMagick and GD) using the same calls. It's API includes a pieSlice function (http://imagine.readthedocs.org/en/latest/api/Draw/drawer_interface.html#Imagine\Draw\DrawerInterface::pieSlice). The source for that function is https://github.com/avalanche123/Imagine/blob/develop/lib/Imagine/Imagick/Drawer.php#L219  Not exactly what you were asking but it's easy to draw pie charts with Google Chart API http://code.google.com/apis/chart/ That's not what I want. But thank you anyway.  Your best bet would be to use ImageMagick::Draw. It has an arc command; instead of specifying the center you specify the bounding rectangle. You just need to add a command before it to set the fill and perhaps after it to close the fill. http://us.php.net/manual/en/function.imagickdraw-arc.php It doesn't work the same way. That command basically does an ellipse within the bounding box cuts at a line between the given degrees and deletes one of the pieces.
187,A,Data display with 3d graphs in Java? Can you please recommend a library that can draw a simple (see the image) 3D graph in Java? The library should comply with the following requests: it has to be freely available for non-commercial use it should be simple to use (given a set of data it should draw a 3D graph) when drawn users should be able to do basic control over the graph (rotate change perspective) Please provide links to the library its API and examples. Thank you for your answers. There are some libs which could fit your requirements (taken from here): IDV freehep e.g. used from JAS scavis compatible with freehep jgnuplot jbeam this is free for universities etc  Just a simple google search lead me to this: http://java.freehep.org/demo/LegoPlot/. It has that lego look but I think that's more accurate for some data not sure what you're into.
188,A,"How to change xy origin of canvas to bottom left and flip the y coordinates? I have a bunch of data points that I would like to two-way bind to points on a canvas. The points assume larger y values are reflected in an upwards direction like most math graphs. How do I change the xy origin of the canvas to the bottom left corner and reverse it's interpretation of the y coordinate? (I would like to stay in XAML) <Canvas> <Canvas.LayoutTransform> <ScaleTransform ScaleX=""1"" ScaleY=""-1"" CenterX="".5"" CenterY="".5"" /> </Canvas.LayoutTransform> </Canvas> Thanks for this but aren't ScaleX CentreX and CenterY redundant? Just using ScaleY=""-1"" seems to do the trick. Also others beware that this will also flip any text upside down. yes the other properties are redundant but it's convention (from what I've seen) to include those properties. And yes it will flip the text but if you're just drawing points should be ok. if you need to draw text you could draw it on a 2nd transparent Canvas that is on top of the graph Canvas @qntmfred any sample implementation of text implementation. because the point system will affect text placement as well  I tried the ScaleTransform method extensively: It does not work. It only shifts one of the 2 coordinates never both. This however works as advertised: <Canvas Name=""myCanvas"" Width=""0"" Height=""0"" RenderTransform=""1 0 0 -1 0 0"" HorizontalAlignment=""Center"" VerticalAlignment=""Center"" >  If you use databinding you can use a TypeConvertor but for that you have to go outside the XAML and you need to know the size of the canvas beforehand.  I'd probably create a custom panel instead of using Canvas and give it the attached properties that make sense for your needs. Here is an example of implementing a custom panel: http://blog.boschin.it/articles/silverlight-radialpanel.aspx Something like Canvas is very simple since you don't have to do much in the measure and arrange overrides. You may also be able to inherit from Canvas and override ArrangeOverride I haven't tried that but it may work. Bill you don't say what is wrong with using Canvas? Well I don't see how you can do this with a straight Canvas if you scale it -1 in the Y everything will be flipped. I guess you could scale all of the elements -1 as well."
189,A,"How can I get rid of jerkiness in WinForms scrolling animation? I'm writing a simple control in C# that works like a picture box except the image is constantly scrolling upwards (and re-appearing from the bottom). The animation effect is driven by a timer (System.Threading.Timer) which copies from the cached image (in two parts) to a hidden buffer which is then drawn to the control's surface in its Paint event. The problem is that this scrolling animation effect is slightly jerky when run at a high frame rate of 20+ frames per second (at lower frame rates the effect is too small to be perceived). I suspect that this jerkiness is because the animation is not synchronized in any way with my monitor's refresh rate which means that each frame stays on the screen for a variable length of time instead of for exactly 25 milliseconds. Is there any way I can get this animation to scroll smoothly? You can download a sample application here (run it and click ""start"") and the source code is here. It doesn't look horribly jerky but if you look at it closely you can see the hiccups. WARNING: this animation produces a pretty weird optical illusion effect which might make you a little sick. If you watch it for awhile and then turn it off it will look as if your screen is stretching itself vertically. UPDATE: as an experiment I tried creating an AVI file with my scrolling bitmaps. The result was less jerky than my WinForms animation but still unacceptable (and it still made me sick to my stomach to watch it for too long). I think I'm running into a fundamental problem of not being synced with the refresh rate so I may have to stick to making people sick with my looks and personality. Use double buffering. Here are two articles: 1 2. Another factor to ponder is that using a timer doesn't guarantee you to be called at exactly the right time. The correct way to do this is to look at the time passed since the last draw and calculate the correct distance to move smoothly. then consider the second part of my answer: your timer event is not called at the exact time you'd expect it to be. It is double buffered already. The problem is not flickering. Yeah thanks to windows multitasking my times are quantized to 15ms and I'm trying to animate frames that last 20ms. I think my problem is unsolvable. That just means that you have to skip a frame once in a while or move the content for non-integral distances. Look into basic gaming texts to see how that is calculated. No can do on frame skipping or non-integral distance moves. The smoothness of the scrolling is exactly what I was looking for. From what I've read I would have to synchronize the animation with the monitor's refresh rate which I fundamentally can't do because I also have to synchronize the animation with music.  I had the same problem before and found it to be a video card issue. Are you sure your video card can handle it? Which model notebook are you using? Acer Veriton L460 and my desktop -- it runs very smooth. The Acer uses a LCD monitor and the desktop uses a flat panel (both Dell's). It's a Satellite A135 relatively underpowered (1.5G ram 1.60GHz processor). If you have a better video card does the animation in the sample app look smooth to you? Probably not. I use a Toshiba Satellite notebook that can only do 60 Hz.  I modified your example to use a multimedia timer that has a precision down to 1 ms and most of the jerkiness went away. However there is still some little tearing left depending on where exactly you drag the window vertically. If you want a complete and perfect solution GDI/GDI+ is probably not your way because (AFAIK) it gives you no control over vertical sync. No I'm running at 60Hz (on a TFT monitor). It's hard to describe jerkiness in approachable terms but the improvement was that the ""frequency"" of ""jerks"" went down significantly ie the occasional ""jumps"" upwards occurred every 10-12 seconds rather than every 1-2. The tearing was still distracting though so I'm not saying the MM timer is a 100% solution but it sure delivers on timing accuracy. Are you running a refresh rate higher than 60 Hz? I tried using a multimedia timer after posting this question but I didn't see any noticeable improvement.  (http://www.vcskicks.com/animated-windows-form.html) This link has an animation and they explain the way they accomplish it. There is also a sample project that you can download to see it in action.  I had a similar problem a couple of months ago and solved them by switching to WPF. The animated control ran a lot smoother than with a standard timer-based solution and I didn't have to take care of synchronization any more. You might want to give it a try. you're right. i didn't notice the importance of using WinForms. i'll take another look at your code then. Pleaaaase anonymous downvoters express yourselves! It wasn't me. I voted you up even though I can't use WPF. Someone might have thought your comment wasn't really an answer to my question since WinForms isn't technically WPF. I should have just said ""windows app"" instead of WinForms.  Some ideas (not all good!): When using a threading timer check that your rendering time is considerably less than one frame interval (from the sound of your program you should be fine). If rendering takes longer than 1 frame you will get re-entrant calls and will start rendering a new frame before you've finished the last. One solution to this is to register for only a single callback at startup. Then in your callback set up a new callback (rather than just asking to be called repeatedly every n milliseconds). That way you can guarantee that you only schedule a new frame when you've finished rendering the current one. Instead of using a thread timer which will call you back after an indeterminate amount of time (the only guarantee is that it is greater than or equal to the interval you specified) run your animation on a separate thread and simply wait (busy wait loop or spinwait) until it is time for the next frame. You can use Thread.Sleep to sleep for shorter periods to avoid using 100% CPU or Thread.Sleep(0) simply to yield and get another timeslice as soon as possible. This will help you to get much more consistent frame intervals. As mentioned above use the time between frames to calculate the distance to scroll so that the scroll speed is independent of the frame rate. But note that you will get temporal sampling/aliasing effects if you try to scroll by a non-pixel rate (e.g. if you need to scroll by 1.4 pixels in a frame the best you can do is 1 pixel which will give a 40% speed error). A workaround for this would be to use a larger offscreen bitmap for scrolling and then scale it down when blitting to screen so you can effectively scroll by sub-pixel amounts. Use a higher thread priority. (really nasty but may help!) Use something a bit more controllable (DirectX) rather than GDI for rendering. This can be set up to swap the offscreen buffer on a vsync. (I'm not sure if Forms' double buffering bothers with syncing) I'm actually driving this animation off of callbacks from an audio output engine so theoretically my timing is accurate to 1/44 of a millisecond. I've experimented with non-whole-pixel movements but with these images in particular the interpolation artifacts are severe and it still seems jerky anyway. I've tried the higher thread priority but it had no effect I could detect. I think I may have to go the DirectX route although I was hoping to avoid this (I don't think it works in Windows Mobile is one problem).  You need to stop relying on the timer event firing exactly when you ask it to and work out the time difference instead and then work out the distance to move. This is what games and WPF do which is why they can achieve smooth scrolling. Let's say you know you need to move 100 pixels in 1 second (to sync with the music) then you calculate the time since your last timer event was fired (let's say it was 20ms) and work out the distance to move as a fraction of the total (20 ms / 1000 ms * 100 pixels = 2 pixels). Rough sample code (not tested): Image image = Image.LoadFromFile(...); DateTime lastEvent = DateTime.Now; float x = 0 y = 0; float dy = -100f; // distance to move per second void Update(TimeSpan elapsed) { y += (elapsed.TotalMilliseconds * dy / 1000f); if (y <= -image.Height) y += image.Height; } void OnTimer(object sender EventArgs e) { TimeSpan elapsed = DateTime.Now.Subtract(lastEvent); lastEvent = DateTime.Now; Update(elapsed); this.Refresh(); } void OnPaint(object sender PaintEventArgs e) { e.Graphics.DrawImage(image x y); e.Graphics.DrawImage(image x y + image.Height); }  You would need to wait for a VSYNC before you draw the buffered image. There is a CodeProject article that suggests using a multimedia timer and DirectX' method IDirectDraw::GetScanLine(). I'm quite sure you can use that method via Managed DirectX from C#. EDIT: After some more research and googling I come to the conclusion that drawing via GDI doesn't happen in realtime and even if you're drawing in the exact right moment it might actually happen too late and you will have tearing. So with GDI this seems not to be possible. What led you to conclude GDI doesn't happen in realtime but DirectX does? I stumbled across this article: http://www.virtualdub.org/blog/pivot/entry.php?id=74 - since smooth scrolling without tearing is actually possible with DirectX or DirectDraw I concluded it's ""more realtime"" ;) .. and since drawing in GDI happens via messages it's always possible that the message gets queued up. Thanks for the link. I was already convinced that this was an essentially impossible problem (and I hate to ever admit that about anything) and that article confirms it. You're the top candidate for the bounty here so far.  Well if you wanted to run the timer at a lower speed you can always change the ammount the image is scrolled in the view. This gives better preformance but makes the effect look kinda jerky. Just change the _T += 1; line to add the new step... Actually you could even add a property to the control to adjust the step ammount. The effect is even worse when I slow down the timer but move more than 1 pixel at a time."
190,A,WPF 2D image binding performance I have a high speed camera that I am displaying in a WPF control. The camera's SDK provides me with an array of bytes representing the image data. I'm looking for the most efficient way to bind to these images on a Canvas control. These images are displayed side-by-side from a collection capped at 20 images. My current method does work exactly the way I want it to but I feel like there are too many unnecessary conversions. I first convert my byte array to a Bitmap like so: //note dataAdress is an IntPtr I get from the 3rd party SDK //i also have width and height ints bmp = new Bitmap(width weight PixelFormat.Format32bppRgb); BitmapData bData = bmp.LockBits(new Rectangle(0 0 width height) ImageLockMode.WriteOnly PixelFormat.Format32bppRgb); int datasize = width * height * 4; byte[] rgbValues = new byte[datasize]; Marshal.Copy(dataAdress rgbValues 0 datasize); Marshal.Copy(rgbValues 0 bData.Scan0 datasize); bmp.UnlockBits(bData); Then I call this function to convert the Bitmap to a BitmapSource: BitmapSource source = System.Windows.Interop.Imaging.CreateBitmapSourceFromHBitmap(bmp.GetHbitmap() IntPtr.Zero System.Windows.Int32Rect.Empty BitmapSizeOptions.FromWidthAndHeight(bmp.Width bmp.Height)); Note that I'm skipping some Disposing/cleanup for readability. I have to use BitmapSource so I can bind an ImageSource to display the images. If this were XNA I would use Texture2Ds and this would fly. I figure since the underpinnings of WPF use DirectX there has got to be a better way than using gross Bitmaps. Any ideas? BitmapSource has static methods to create it from an array of bytes (see http://msdn.microsoft.com/en-us/library/ms616045%28v=VS.100%29.aspx). If for some reason this isn't good enough Bitmap at least has a GetHBitmap function to retrieve the HBitmap instead of you creating it yourself via LockBits etc (though the underlying implementation is probably similar).
191,A,"getting window screenshot windows API I am trying to make a program to work on top of an existing GUI to annotate it and provide extra calculations and statistical information. I want to do this using image recognition as I have learned a fair amount about this in University using Matlab and similar things. I can get a handle to the window I want to perform image recognition on but I don't know how to turn that handle into an image of that window and all its visible child windows. I suppose I am looking for something like the screenshot function but restricted to a single window. How would I go about doing this? I suppose I'd need something like a .bmp to mess about with. Also it would have to be efficient enough that I could call it several times a second without my PC grinding to a halt. Hopefully this isn't an obvious question I typed some things into google but didn't get anything related. One simple way is using the PrintWindow API (which is an automated Alt + Print basically). The following example takes a screenshot of the calculator but you would just have to replace the handles. void CScreenShotDlg::OnPaint() { // device context for painting CPaintDC dc(this); // Get the window handle of calculator application. HWND hWnd = ::FindWindow( 0 _T( ""Calculator"" )); // Take screenshot. PrintWindow( hWnd dc.GetSafeHdc() 0 ); } (see http://weseetips.com/2008/07/14/how-to-capture-the-screenshot-of-window/ ) i can't find the description about the function PrintWindow on MSDN please describe how to save as bitmap object for reuse  I think CImage class will be helpful. void CreateImage(HWND hwnd) { CImage img; img.m_hDC = ::GetWindowDC(hwnd); img.Save(strFileName); } That looks handy thanks. I'll try it out."
192,A,"How do I get all the shader constants (uniforms) from a ID3DXEffect? I'm creating an effect using hr = D3DXCreateEffectFromFile( g_D3D_Device shaderPath.c_str() macros NULL 0 NULL &pEffect &pBufferErrors ); I would like to get all the uniforms that this shader is using. In OpenGL I used glGetActiveUniform and glGetUniformLocation to get constant's size type name etc. Is there a D3DX9 equivalent function? Thanks! D3DXHANDLE handle = m_pEffect->GetParameterByName( NULL ""Uniform Name"" ); if ( handle != NULL ) { D3DXPARAMETER_DESC desc; if ( SUCCEEDED( m_pEffect->GetParameterDesc( handle &desc ) ) ) { // You now have pretty much all the details about the parameter there are in ""desc"". } } You can also iterate through each parameter by doing the following: UINT index = 0; while( 1 ) { D3DXHANDLE handle = m_pEffect->GetParameter( NULL index ); if ( handle == NULL ) break; // Get parameter desc as above. index++; }"
193,A,"How Do I Find Good Graphic Designers for my Web Application Projects? While this isn't strictly programming related it's something I've run into as a web developer on several occasions and I imagine others have run into as well. For that reason I hope this question can remain open. As web developers we know how to make our applications work. But clients/customers/visitors aren't necessarily happy with a system that just works they want a site with some graphic personality. Something that ""looks pretty"". Whether it's ""trendy"" ""professional"" ""cool"" or ""grungy"" there's always a feeling that a web site needs to give it's audience in conjunction with the functionality of the site itself. Unfortunately a Google search for ""web designer"" returns a list of everybody in the world who owns a copy of MS FrontPage and/or PhotoShop. So the questions are: Where do you turn when you need a (talented) designer on your project? What criteria do you use to determine if a designer candidate has skills or is qualified? Please DO NOT CLOSE THIS. I'm aware it's subjective but I've run across the exact same problem on countless occasions. Designer freelance sites. These are sites where you post a project and the designers each post their proposals. You choose the one you are happy with. Believe it or you will get lots of quality offerings - almost too many. Nothing else even comes close. 99 Designs Crowdspring (The latter is my favorite) Also some good points here: http://blog.siteroller.net/tips-when-looking-for-a-web-developer  Freelancing/marketplace websites. You can make a post for a job you need done sometimes however you really need to go out and look at the design offers that graphic designers post it might take a bit of work to find a really good one they won't always come to you. Sites such as: http://www.getafreelancer.com/ http://marketplace.sitepoint.com/ http://www.talkfreelance.com/ http://www.elance.com/ I usually look at their portfolio and make sure they have nice graphical skills as well as decent communication skills as indicated by the content they have on their website. Don't forget http://www.rentacoder.com it's not only for software.  Not sure about the quality of designers there but there should be some overlap on Rent a Coder  I hate to advertise for myself on here but I can't help but suggest you visit my website and look under the portfolio tab the url is in my profile. If you hate to advertise yourself on here then don't!  37Signals recently launched Haystack to address exactly this problem. Thanks for the link. I've been looking through the samples on there and it's odd- this is one industry where you DONT get what you pay for. There's ""under $3000"" designers with good looking stuff and ""over $50000"" designers with awful looking sites. A lot of times the ""awful looking"" sites are awful because if the inisitance of the client... just keep that in mind. Of course i personally like to leave the ""awful"" sites out of my port but i guess some people dont. This is the reason that I prefer not to use sites like this. You can find very good and ridiculously cheap designers by word of mouth.  I think the best approach is to find a website design that looks good to you and maybe a few of your end users. Find out who designed it and then ask them if they would be available for a side-project or full time employment. If he doesn't have the time I am sure he would have friends to suggest to you. As always be sure to see the portfolio of any graphic design before hiring him and perhaps ask him some questions about his design for your specific project. I wouldn't suggest templates because they often force you to build your website function around the design instead of the other way around.  You could also look on Odesk.com. They seem to have some nice professionals with lots of feedback.  You can search for templates instead of designers (to pick one example Get Template): a template is like a designer distilled (and much cheaper). Having done that you could also contact the designers of some templates that you like."
194,A,"Java AWT/SWT/Swing: How to plan a GUI? I've already realized some applications with a small graphical user interface. Nothing complex but I've encountered several problems that components aren't displayed or just not behaving as expected. Now my question: How do you plan those user interfaces? What do you do when you need to make changes? How do you debug strange behaviours?! This applies for nearly every type of gui-design. Sure with Microsofts Visual Studio you have a big advantage because you nearly get what you see in the designer. Does a good and open-source (or freeware) designer for AWT exist? Already looked around and didn't find anything really intelligent. EDIT: Until now I've also created all of my GUIs by hand. Sure it is cleaner code but sometimes it's very hard to find the layouting bugs. If Visual Studio by MS is able to create approximately clean code why aren't the others? I've heard about some Eclipse Visual designer. Is that one already production-ready? --- regards Thanks for the good responses all of you provided useful information. I've building around with Visual Editor for Eclipse. For a prototype it's a good tool but not really for production use. Do it by hand. GUI builders aren't good unless you have the 'partial class' concept in C# and even then they often cause more problems than they solve. Use the GUI builder tools to make a prototype - sure but not for production code. Also another little trick I've used over the years to good effect when trying to debug layout or ""which panel am I really seeing here"" problems is to give each 'container' panel a really garish background color (yellow blue etc). Something obvious enough that you'll see it even if it's only one pixel wide. And my favorite layout for simple dialogs is BoxLayout. It's not great you have to write a lot of boilerplate but at least it generally works the way you would expect it to in your head. Don't overthink layouts until you have to.  I'm not a big fan of GUI builders: They typically autogenerate bucket-loads of code that then locks in your whole development team to using one IDE. Also this code is often unreadable (check the code generated when using Matisse under Netbeans). My recommendations for GUI design / debugging would be: Add a main method to each panel (or ""top-level"" component) implementation allowing other developers to easily determine what a component looks like. Favour the use of Actions over ActionListeners and register these actions with each JComponent's ActionMap. This allows them to be ""extracted"" and added to other parts of the UI (e.g. JToolBar) whilst still having their state controlled by the ""owning"" JComponent (i.e. loose coupling). Use assert to ensure that all UI component modifications are occurring on the Event Dispatch thread; e.g. assert SwingUtilities.isEventDispatchThread(). To debug strange layout behaviour consider painting a component's background in red! Centralise the capturing and reporting of workflow events and exceptions. For example I typically implement a TaskManager class that is registered with my UI's status bar. Any background processing (performed within SwingWorkers) is passed a handle to a Task created by the TaskManager. Interracting with the Task (by calling setDescription(String) setThrowable(Throwable) cancel()) causes the status bar to be updated. It also causes the glass pane to be displayed for ""global"" tasks ... but this is all decoupled / hidden from the individual SwingWorkers. Do not use the Observer / Observable classes but instead favour ChangeListener PropertyChangeListener or your own custom listener implementation for propagating events. Observer passes an Object as it's event forcing client code to check the type using instanceof and to perform downcasts making code unreadable and making relationships between classes less clear. Favour the use of JTable over JList even in situations where your table only has one column. JList has some nasty features in its API including the fact that you need to provide a prototype value for it to calculate its size correctly. Never use DefaultTableModel as it typically results in you storing your ""model"" data in two places: In your actual business objects and also within the 2D array that DefaultTableModel sits on. Instead simply subclass AbstractTableModel - It's very easy to do this and means your implementation can simply delegate through to the data structure (e.g. List) storing your data. Everything is this answer is great I'd just like to add one bit. Consider drawing your GUI on paper. Use a different color pen/pencil to indicate hidden panels layout managers actions etc. Works WONDERS! You could do the same on a white board but I prefer the precision of paper and pen (plus we've got all sorts of scrap at work) +1 Nice tip with the assertion. The bugs that happen when you access the UI from outside the EDT are ofttimes subtle and weird. And very hard to find. Nice set of best practices here. Well done. You do not need to specify a prototype value for JList. The proper size is calculated automatically. This is done by looping through all the items in the model and invoking the renderer to determine the preferred size. If you have a large model this may be inefficent and you may wish to provide a prototype value to bypass this behavour. JTable on the other hand provides no support for this at all an you must guess at the row height and column width and update the table and TableColumn manually. @camickr: I was referring to the case where the JList starts off empty and has items added to it during the application life-time. I still prefer JTable as it also offers sorting / filtering out of the box. To size the table columns I typically use a utility method similar to: http://www.exampledepot.com/egs/javax.swing.table/PackCol.html  I would suggest you to use netbeans for GUI development in AWT/SWING. Regards Sameer Please have a look at the following link. It shall help you. It's the just the matter of drag and drop. http://java.sun.com/docs/books/tutorial/javabeans/nb/index.html  I use JFormDesigner for gui generation. It generates nice clean java code and I've learned a few things from reading through the generated code. Makes localization a snap. It's a really quick way to throw together an involved layout especially complex menubars and grid layouts.  I'm one of those archaic dudes who do GUI layout by hand. I'm also not afraid of the infamous GridBagLayout! I keep things simple for myself by emulating the coding standard used by Visual Age years ago: I use a lot of JPanels to organize parts of the GUI and each one gets its own makeXXX() method to create it lay it out and return it to a a parent panel or the constructor. That way each makeXXX only has to concentrate on a small part of the whole works. Some components need to be accessible by various instance methods; I declare those as instance fields. The other stuff that's just decoration or layout need not be exposed outside the makeXXX methods. That's mostly it. Works for me. GridBagLayout rocks. i absolutely totally agree! I'd replace GridBagLayout (shudder...) with MigLayout - http://www.miglayout.com/ GridBagLayout killed Jesus. Any time someone uses GridBagLayout an angel loses his wings. Better: identify your layouting needs and write your customed layout managers (e.g. for key/value layouts). If you like `GridBagLayout` you surely love [MiGLayout](http://www.miglayout.com/). ;)  There has been a temporarily-dead (but now apparently at least half-alive) plugin for Eclipse for visual GUI design and Netbeans still has support for it. The resulting code was less than stellar though. At least for people having to work with that codebase afterwards it's quite a pain. As for me I tend to plan on paper beforehand and try to get all nestings of panels with their layouts right on the first try. Java GUI code is inherently write-only in my experience. Last time I did such a thing I first created every control I needed and then pieced it together in multiple panels and layouts etc. That way was at least manageable and worked without too much pain when changes had to be made. I tend not to think too much about the particular layout in Winforms and WPF due to the as you noted also strong designer support. Also WPF is very easy to handle even in XAML. Ans partial classes make working with partly designer-generated and partly handwritten code very pleasant. Alas no such thing in the Java world. Oh thanks for the notice. Didn't notice they raised it from the dead as for the most part of me using Java it was dead or near-so. correct me if I'm wrong but I thought the eclipse VE has become active again (after some years). Read something about a version 1.4 !? I like the code that generates VE. It is much cleaner comparing with Netbeans GUI builder it is easy to understand and to modify ""by hands"". @bancer: I have to admit the comment on code quality was more about Netbeans. I had the joy of modifying such code only once but it was painful :-)  While NetBeans' Matisse editor is admittedly handy the code it produces is fairly esoteric and the layouts are fragile. So I've been taking best of both worlds using NetBeans for WYSIWYG prototyping and then later recoding the whole thing by hand. I was close to actually trying to write a converter from XAML to Swing code because VS and WPF were *very* handy in prototyping UIs. But then I lacked time and still coded it by hand *sigh*  Apart from tools discussion just some ideas and thoughts Before you touch the keyboard draw the GUI elements on paper. Like the classic storyboard used for video production. If you have customers use the hand-drawn (!) designs to communicate the ideas (.. just read that you already plan on paper) Plan to implement a model-view-controller (MVC) or model-view-presenter pattern Databinding is a great technique to consider. It guarantees synchronisation between your model (data) and the view (GUI) and offers input validation on-the-fly conversion and many more useful things (Provided the link for JFace databinding but I'm sure there are other frameworks for Swing/AWT as well)  NetBeans might the best option for building GUI in WYSIWYG manner but many java developers write their GUI by hands as it is not that difficult. Care about the thickness of your borders and gaps between your controls and you're ok :) and yes about the alignment. There are many things in UI which are quite complex from technical point of view and there are tricks for debugging (e.g. breakpoints may not help if your control is repainted often => use logging) but really challenging stuff happens on a sheet of paper and in designer's brain.  I for myself use Pencil for some prototyping first then start coding ""by hand"" (i.e. not using any GUI editor). The provided link gives a HTTP 500 error. This is an updated link: http://pencil.evolus.vn/"
195,A,"What language is good to write simple nice GUI apps? I never did to much GUI programming (besides a bit of QT and Delphi). I need to write simple GUI app which would interactively visualize graphs. Very similar tool to ""GraphViz GUI for Mac"". What would you suggest? Thanks. Consider Python with PyGTK: cross-platform documented proven.  I think it really depends on the platform that you're targeting. If you are writing native apps for the Mac Objective-C is probably the best choice as the APIs are built around that. If you're writing for Windows you probably want C/C++ or C# for the same reason. Cross-platform apps would make you choose a platform-neutral language such as Java or Python (as jldupont suggested). er... C/C++ for Windows is simple? C# yes VB/VB.NET yes but C/C++ are a bit up there. I'm not suggesting that C/C++ are simple as languages nor is Objective-C just that they are logical choices for the platform.  JRuby is a good choice - it's cross platform and you get the double benefit of a large number of components available for java (for instance someone probably has done an interactive graph visualisation panel already) and the ability to connect and use them from ruby (which is a great language to program in).  I use quite a few and I'd say C# in Visual Studio on Windows (which is great and free!) and something like C#+MonoDevelop (or equally a scripting language like Perl) and Glade/GTK on Linux/BSD/other UNIX's while on Mac OS then XCode and Interface Builder are the obvious choice. I would say C# with Visual Studio Express is the easiest and XCode and Interface Builder have the steepest learning curve. I've not used QT though so can't compare but imagine it's similar to GTK. Thinking outside the box a little REALBasic is also excellent at getting the job done (if you can get past the fact that it's OO Basic). It's not free but they recently greatly reduce the price and it's really great to use for simple GUI app development."
196,A,How can you draw separate parts a control in different places with WPF? I'm trying to split the drawing of a WPF UserControl onto two separate pages of a custom Paged Panel that I'm writing. I thought of using a VisualBrush to draw bits of the control. Is this the way to go? I presume I'll also have to put the controls that need to be visually split in an invisible container. Has anyone come across an effective way of doing this? I haven't done anything like what you are trying to do but you can try using a VisualBrush to draw to some container (say Rectangle if your pages are rectangular). You can then use a clipping geometry to clip the area of the UserControl you are trying to display in a page. Good luck!
197,A,"What is the formula for alpha blending for a number of pixels? I have a number of RGBA pixels each of them has an alpha component. So I have a list of pixels: (p0 p1 p2 p3 p4 ... pn) where p_0_ is the front pixel and p_n_ is the farthest (at the back). The last (or any) pixel is not necessary opaque so the resulting blended pixel can be somehow transparent also. I'm blending from the beginning of the list to the end not vice-versa (yes it is raytracing). So if the result at any moment becomes opaque enough I can stop with correct enough result. I'll apply the blending algorithm in this way: ((((p0 @ p1) @ p2) @ p3) ... ) Can anyone suggest me a correct blending formula not only for R G and B but for A component also? UPD: I wonder how is it possible that for determined process of blending colors we can have many formulas? Is it some kind of aproximation? This looks crazy as for me: formulas are not so different that we really gain efficiency or optimization. Can anyone clarify this? There are various possible ways of doing this depending on how the RGBA values actually represent the properties of the materials. Here's a possible algorithm. Start with final pixel colours lightr=lightg=lightb=0 lightleft=1; For each rgba pixel encountered evaluate: lightr += lightleft*r*(1-a) lightg += lightleft*g*(1-a) lightb += lightleft*b*(1-a) lightleft *= 1-a; (The RGBA values are normalised between 0 and 1 and I'm assuming that a=1 means opaque a=0 means wholly transparent) If the first pixel encountered is blue with opacity 50% then 50% of the available colour is set to blue and the rest unknown. If a red pixel with opacity 50% is next then 25% of the remaining light is set to red so the pixel has 50% blue 25% red. If a green pixel with opacity 60% is next then the pixel is 50% blue 25% red 15% green with 10% of the light remaining. The physical materials that correspond to this function are light-emitting but partially opaque materials: thus a pixel in the middle of the stack can never darken the final colour: it can only prevent light behind it from increasing the final colour (by being black and fully opaque).  Alpha-blending is one of those topics that has more depth than you might think. It depends on what the alpha value means in your system and if you guess wrong then you'll end up with results that look kind of okay but that display weird artifacts. Check out Porter and Duff's classic paper ""Compositing Digital Images"" for a great readable discussion and all the formulas. You probably want the ""over"" operator. It sounds like you're doing something closer to volume rendering. For a formula and references see the Graphics FAQ question 5.16 ""How do I perform volume rendering?"". Great links man! Thank you very much!"
198,A,"How do I use glutBitmapString() in C++ to draw text to the screen? I'm attempting to draw text to the screen using GLUT in 2d. I want to use glutBitmapString() can someone show me a simple example of what you have to do to setup and properly use this method in C++ so I can draw an arbitrary string at an (XY) position? glutBitmapString(void *font const unsigned char *string); I'm using linux and I know I need to create a Font object although I'm not sure exactly how and I can supply it with the string as the second arguement. However how do I also specify the x/y position? A quick example of this would help me greatly. If you can show me from creating the font to calling the method that would be best. You have to use glRasterPos to set the raster position before calling glutBitmapString(). Note that each call to glutBitmapString() advances the raster position so several consecutive calls will print out the strings one after another. You can also set the text color by using glColor(). The set of available fonts are listed here. // Draw blue text at screen coordinates (100 120) where (0 0) is the top-left of the // screen in an 18-point Helvetica font glRasterPos2i(100 120); glColor4f(0.0f 0.0f 1.0f 1.0f); glutBitmapString(GLUT_BITMAP_HELVETICA_18 ""text to render""); Thanks for the help adam. Also for a long time it kept telling me glutBitmapString was not defined and I eventually found it named as ""_glutBitmapString"" in GL/glui.h. Any idea why?"
199,A,"Drawing line graphics leads Flash to spiral out of control! I'm having problems with some AS3 code that simply draws on a Sprite's Graphics object. The drawing happens as part of a larger procedure called on every ENTER_FRAME event of the stage. Flash neither crashes nor returns an error. Instead it starts running at 100% CPU and grabs all the memory that it can until I kill the process manually or my computer buckles under the pressure when it gets up to around 2-3 GB. This will happen at a random time and without any noticiple slowdown beforehand. WTF? Has anyone seen anything like this? PS: I used to do the drawing within a MOUSE_MOVE event handler which brought this problem on even faster. PPS: I'm developing on Linux but reproduced the same problem on Windows. UPDATE: You asked for some code so here we are. The drawing function looks like this: public static function drawDashedLine(i_graphics : Graphics i_from : Point i_to : Point i_on : Number i_off : Number) : void { const vecLength : Number = Point.distance(i_from i_to); i_graphics.moveTo(i_from.x i_from.y); var dist : Number = 0; var lineIsOn : Boolean = true; while(dist < vecLength) { dist = Math.min(vecLength dist + (lineIsOn ? i_on : i_off)); const p : Point = Point.interpolate(i_from i_to 1 - dist / vecLength); if(lineIsOn) i_graphics.lineTo(p.x p.y); else i_graphics.moveTo(p.x p.y); lineIsOn = !lineIsOn; } } and is called like this (m_graphicsLayer is a Sprite): m_graphicsLayer.graphics.clear(); if (m_destinationPoint) { m_graphicsLayer.graphics.lineStyle(2 m_fixedAim ? 0xff0000 : 0x333333 1); drawDashedLine(m_graphicsLayer.graphics m_initialPos m_destinationPoint 10 10); } Yes I do. There can be multiple events active at one time. Say you set an EnterFrame event. Then you set your framerate to be 30fps. If your application doesn't have time to finish one EnterFrame call it will still go on to the next call and this will fire another separate event. It is not linear... one event will not wait for another to finish and they can and will overlap and compound. A very heavy function executed on an event basis can easily crash a machine without causing a compilation error. The timeline simply fires off it's event... more or less broadcast ""I hit a frame do something"" ""I hit a frame again do something again""... etc. you could start by showing us some code :) yeah show us your codesounds like you have an infinite loop in there somewhere. Yep if you start firing off drawing commands on enterframe or on a mouse event each iteration can start colliding with the next firing if you don't handle things efficiently. @liquidleaf- what do you mean by ""colliding with the next firing""? is it possible for an event handler to be called again before it has finished? or do you just mean that it takes so much time that the next frame will be skipped? Well CookieOfFortune was right- there was an infinite loop in the case that one of the end points was at infinity (which wasn't supposed to happen ;) Thanks for your help.  while(dist < vecLength) This should always jump out as a red flag during debugging and code review especially if the arguments come from a calculation which you can't reliably predict when writing code or the values don't have a strict upper limit (like Point.distance() or Math.min()). Been bitten by those a number of times. :-) Probably because the method is stuck in a infinite loop which allocates more resources with each iteration and never releases them back. It just keeps going until the process breaks down. You're right. Still though why would Flash the way it does grabbing all this memory and never releasing it?"
200,A,"RGB for color composition rather than primary hues Why do computers use RGB (red green and blue) values for color composition rather than the primary hues red yellow and blue? Computers use the additive colour model which involves adding together RGB to form white and is the usual way of forming colours when using a light source. Printers use subtractive color normally using Cyan(C) Magenta(M) and Yellow(Y) and often Black(K). Abbreviated CMYK Cyan is opposite to Red Magenta is opposite to Green and Yellow is opposite to Blue. This is a really simple explanation of a complex issue the guy that came up with additive colour was James Maxwell (yes that one) so if you dig into the many articles about him that may explain much better.  For efficiency: the RGB model is additive. For example superimpose pure red and pure blue light and you get magenta. It's also easy to build into monitors. If you take a magnifying glass and look at your monitor you'll be able to see individual red green and blue dots that vary in intensity to compose the colors needed. As ffpf mentioned check out Wikipedia. Here's a link to the article on the RGB color model.  Because combining light sources (which computer monitors do) does not work the same way as combining printed ink. It's just a guess.  Computer screens emit light to display pixels. Mixing different colours of light is called Additive colour. Additive colour uses red green and blue as primary colours. Subtractive colour is how different colours of materials mix such as paints. Subtractive colour uses red yellow and blue as primary colours. How I think of it is that when light reflects off an object into your eyes the object absorbs some of the colour and reflects the rest to your eyes. So if an object's green it means it's absorbing the red and the blue out of the white light. This is why mixing red green and blue light creates white light but mixing red yellow and blue paint creates black (the mixed paint now absorbs all primary colours.) That is the reason for the difference between additive and subtractive light. And white light being a mix of red/green/blue is a convenience to do with the ranges of sensitivity of the light receptors in our eyes. An RGB mix looks white to humans and so does blackbody radiation at certain temperatures but the spectra are different as you could prove with a prism.  The hues of magenta yellow and cyan are primary for subtractive combination (e.g. paints or inks) rather than additive combination such as light where red green and blue are primary. Wikipedia has more detail on the whys and wherefores. To be more precise the subtractive primaries are magenta yellow and cyan - not red yellow and blue. @Hugh: Good point.  Just for clarification the primary colours you learn at school are incorrectly given as red yellow & blue. In fact they are Cyan Yellow & Magenta just like your inkjet printer. As the previous posts state Cyan Yellow & Magenta are the subtractive prime colours; you see what the pigments reflect. Red Green & Blue are the additive primary colours that CRTs Plasmas & LCDs use. My high-school art teacher told me that when crayons were starting to be marketed for children there were no non-staining versions of Magenta and Cyan pigments. Permanent stains would have hurt the marketing effort to parents so they went with ""close enough"" colors for the small 8-color sets. Possibly apocryphal or at least wildly inaccurate."
201,A,"Moving Objects in Center-of-Mass Viewport I'm modeling a force-based physics simulation where several particles are interacting with each other. The particles can move in such a way that a static viewport can easily lose track of them (imagine the whole group of particles moving off the screen to the right and the viewport displaying a blank background). My current solution is to simply track the ""center"" of the particles by finding the min/max of the x's and y's and make the viewport's middle follow that. It works fine (no need to find the true center of mass). Here's a visual: My problem is I'm attempting to add the ability to manually drag particles around with the mouse. I haven't found an elegant way to do this. The problem is the translation between viewport coordinates and simulation coordinates is constantly changing because the viewport follows the particles' center. When I mouse down on a particle I want it to have a fixed location in the simulation (it ignores all forces on it). When I move the mouse with the button depressed I want the particle to move in the simulation. I have achieved this. However it produces unpleasant side-effects because of the viewport and the other particles that are still in motion. The particle isn't moving in the simulation when the mouse is depressed and stationary but it is moving on screen because the center of all the particles changes and the viewport adjusts accordingly. The cursor is stationary so the particle ""moves"" out from under it. When the mouse is moved ever-so-slightly the particle teleports back under the cursor because the cursor's new position represents radically different simulation coordinates. Any suggestions for a nice coherent user experience without the above side effects? I've already thought of: Pausing the simulation while the mouse is down Suspending viewport changes while the mouse is down Moving the mouse cursor as the viewport changes (really bad) But they all have other bad side-effects or take away value from the simulation. Also I think this question is language and platform agnostic so don't attach literal and implementation-specific meanings to ""viewport"" ""viewport coordinates"" or ""simulation coordinates"". Treat them as generic graphics concepts. I don't think there's a need to pause the simulation. As you are describing it it seems that your mouse input (view input) directly changes the model data and the view is updated accordingly when next update/flush/whatever is issued. Perhaps you could insert a new single particle that's only used during ""animation"" and that's not coupled to any of the other particles. While introducing this particle you could hide the real particle (and it's edges). So while moving the mouse you are only updating the data of this animated particle. When the animation ends (you let the mouse go) the animated object is deleted the real particle is updated with its data and shown again. In one way this is a form of pause since you are cheating the updating system. The data of the particle system is not updated during animation it just looks like you are moving the particle of the real system when you're in fact just cheating the user graphically. Hope this makes some sense 2 am here :) This would be an option except as it stands now the selected particle does affect OTHER particles it just doesn't get affected itself by them. I do need to keep the particle ""in"" the simulation. So unfortunately I can't play graphics tricks  Change the algorithm that adjusts the viewport to use the cursor's position as the origin of the transformation. That way the viewport and simulation coordinates at the cursor would remain fixed. You could just do this when the mouse is pressed but it may also have pleasing results if it were done at all times. It complicates the maths somewhat but it doesn't seem like you're left with too many options. This is very intriguing I never thought of that. There would be a quick jump on mouse down but it would be pleasing while dragging the particle around. I'll get back to you :-)"
202,A,Find Coordinates for point on screen? The thing is I have some graphics shown in a form rectangle for example and I want to capture when the point gets over thees fields. So I Thoght I try to find the corrrdinates for thees rectangles but as thay are the coords in the form it dose not match the ones with the mouse location. So I wonder is there a way to find what coord on the screen a Point has on the screen and not in the form or controller? I cant understand what do you want to do can you explain more graphically ? Posting a bit of code showing what you're currently using would help us answer your question :-) Try and re-write the post as Ahmed i don't quite understand. If you are using the graphics object for the form by calling this.CreateGraphics() within the form then the coordinates used when you draw the rectangle should be exactly the same as those returned by the click event on the form.  Do you know what coordinates your pointer is in? You can get the coordinates for your window with a call to GetWindowRect() and subtract the top/left from your mouse cursor to get client coordinates. I seem to remember there being a function to do that for you in fact but it's been some time since I dabbled in custom GUI controls.  Each control hs PointToFoo methods for conversion. Note that you should call this from the parent of the object who's location you want: Point scrPos = this.PointToScreen(panel1.Location); Alternatively you can get the panel's screen coordinates with: Point scrPos = panel1.PointToScreen(new Point(00)); Note that the above two examples could gve different result due to the border-size of the panel. Excellent! now I can use Point scrPos = menuItemForm.PointToScreen(mSize.Location); to locate the point where the rectangle mSize is located and there by detect mouse-over for this rectangle. might not work all the time. Check this duplicate post : http://stackoverflow.com/questions/4768677/c-sharp-how-to-get-the-coordinates-of-a-specific-point-on-screen-not-mouseloca/20688514#20688514
203,A,How to model rules for generating geometric patterns? For my problem it would be best to find a numeric representation of kazakh national ornaments for generating new ones. But other approaches are also fine. The ornaments essentially consist of combinations of relatively basic ornaments. Usually the ornaments are symmetrical. Here are few examples of basic elements: (The images are a bit distorted) And this is an example of a more complex ornament: How could I encode an ornament's representation in as few numbers as possible? So that I could write a program that would generate an ornament given some sequence of numbers Any ideas are appreciated. As I write this I have thought that generating images of snowflakes may be somewhat relevant although it's possibly just a fractal. You have to realize that your question is actually not how to represent them but how to generate them. Still you might get some ideas. But don't hold your breath because it can get complicated EDIT: In researching problems such as this you could start with L-systems this paper seems to convey the idea. Actually here's an attempt at an answer: Represent it as a set of grammar rules. @Daniyar if you find the answer useful... http://stackoverflow.com/faq Yes but it would be best to generate them by changing values in the sequence of numbers used in representation. @Unreason thanks for tip I have changed the question. Could you specify what will be the output of grammer rules? Positions and sizes of the basic ornaments? @Daniyar the output of grammar can be anything take a look at the paper above it is quite interesting. That anything for a given grammar can be for example set of rules that transform starting shapes into a certain pattern (for analogy look at an L-system that creates one type of tree branch) It might be but it is obfuscating the real question if you are looking for 'the' way to generate them if you are looking for 'a' way to generate them or just exploring your problem space then it makes sense. In short I was thinking of finding 'the' way similar to nine block pattern(http://www.levitated.net/daily/lev9block.html). Given the above examples we can not say that that will be 'the' way. Nine block pattern is definitively just a way: using some set of rules it produces patterns. Such task is relatively easy and essentially(!) different from what you are asking: the exact set of rules that will generate geometric shapes (patterns) from your problem space. @Daniyar you might feel this is just small semantical difference but if you were to phrase your question as - 'how to model rules for generating a geometric patterns' you might receive much more attention and quality answers.  I found this dissertation (read it as book) on image texture generation. Image Texture Tools
204,A,"Blackberry - how to resize image? I wanted to know if we can resize an image. Suppose if we want to draw an image of 200x200 actual size with a size of 100 x 100 size on our blackberry screen. Thanks I'm not a Blackberry programmer but I believe some of these links will help you out: Image Resizing Article Resizing a Bitmap on the Blackberry Blackberry Image Scaling Question Thanks. these articles are great!! It works.  I am posting this answers for newbie in Blackberry Application development. Below code is for processing Bitmap images from URL and Resizing them without loass of Aspect Ratio :  public static Bitmap imageFromServer(String url) { Bitmap bitmp = null; try{ HttpConnection fcon = (HttpConnection)Connector.open(url); int rc = fcon.getResponseCode(); if(rc!=HttpConnection.HTTP_OK) { throw new IOException(""Http Response Code : "" + rc); } InputStream httpInput = fcon.openDataInputStream(); InputStream inp = httpInput; byte[] b = IOUtilities.streamToBytes(inp); EncodedImage img = EncodedImage.createEncodedImage(b 0 b.length); bitmp = resizeImage(img.getBitmap() 100 100); } catch(Exception e) { Dialog.alert(""Exception : "" + e.getMessage()); } return bitmp; } public static Bitmap resizeImage(Bitmap originalImg int newWidth int newHeight) { Bitmap scaledImage = new Bitmap(newWidth newHeight); originalImg.scaleInto(scaledImage Bitmap.FILTER_BILINEAR Bitmap.SCALE_TO_FIT); return scaledImage; } The Method resizeImage is called inside the method imageFromServer(String url). 1) the image from server is processed using EncodedImage img. 2) Bitmap bitmp = resizeImage(img.getBitmap() 100 100); parameters are passed to resizeImage() and the return value from resizeImage() is set to Bitmap bitmp.  in this there is two bitmap.temp is holding the old bitmap.In this method you just pass bitmap widthheight.it return new bitmap of your choice. Bitmap ImgResizer(Bitmap bitmap  int width  int height){ Bitmap temp=new Bitmap(widthheight); Bitmap resized_Bitmap = bitmap; temp.createAlpha(Bitmap.HOURGLASS); resized_Bitmap.scaleInto(temp  Bitmap.FILTER_LANCZOS); return temp; } just call this method and hold this in to a bitmap..enjoy scaleInto() for Bitmap is available since OS 5.0. @Lucas Yes it has been since BlackBerry API 5.0.0  Keep in mind that the default image scaling done by BlackBerry is quite primitive and generally doesn't look very good. If you are building for 5.0 there is a new API to do much better image scaling using filters such as bilinear or Lanczos.  You can do this pretty simply using the EncodedImage.scaleImage32() method. You'll need to provide it with the factors by which you want to scale the width and height (as a Fixed32). Here's some sample code which determines the scale factor for the width and height by dividing the original image size by the desired size using RIM's Fixed32 class. public static EncodedImage resizeImage(EncodedImage image int newWidth int newHeight) { int scaleFactorX = Fixed32.div(Fixed32.toFP(image.getWidth()) Fixed32.toFP(newWidth)); int scaleFactorY = Fixed32.div(Fixed32.toFP(image.getHeight()) Fixed32.toFP(newHeight)); return image.scaleImage32(scaleFactorX scaleFactorY); } If you're lucky enough to be developer for OS 5.0 Marc posted a link to the new APIs that are a lot clearer and more versatile than the one I described above. For example: public static Bitmap resizeImage(Bitmap originalImage int newWidth int newHeight) { Bitmap newImage = new Bitmap(newWidth newHeight); originalImage.scaleInto(newImage Bitmap.FILTER_BILINEAR Bitmap.SCALE_TO_FILL); return newImage; } (Naturally you can substitute the filter/scaling options based on your needs.) Thanks was using it incorrectly. Not integer related but still wrong. @TreeUK: I have that exact function in working code right now. Are you converting the `int`s to `Fixed32` and using `Fixed32.div()` to figure out the scale factors? Normal integer division won't cut it.  Just an alternative: BlackBerry - draw image on the screen BlackBerry - image 3D transform  Here is the function or you can say method to resize image use it as you want : int olddWidth; int olddHeight; int dispplayWidth; int dispplayHeight; EncodedImage ei2 = EncodedImage.getEncodedImageResource(""add2.png""); olddWidth = ei2.getWidth(); olddHeight = ei2.getHeight(); dispplayWidth = 40;\\here pass the width u want in pixels dispplayHeight = 80;\\here pass the height u want in pixels again int numeerator = net.rim.device.api.math.Fixed32.toFP(olddWidth); int denoominator = net.rim.device.api.math.Fixed32.toFP(dispplayWidth); int widtthScale = net.rim.device.api.math.Fixed32.div(numeerator denoominator); numeerator = net.rim.device.api.math.Fixed32.toFP(olddHeight); denoominator = net.rim.device.api.math.Fixed32.toFP(dispplayHeight); int heighhtScale = net.rim.device.api.math.Fixed32.div(numeerator denoominator); EncodedImage newEi2 = ei2.scaleImage32(widtthScale heighhtScale); Bitmap _add =newEi2.getBitmap();  For BlackBerry JDE 5.0 or later you can use the scaleInto API."
205,A,Non axis-aligned rectangle intersection I'm trying to find an algorithm that will compute the intersection between 2 rectangles which are not necessarily axis-aligned and return the resulting intersection. This question describes finding whether an intersection exists. I'd like to have the resulting shape of the intersection if it exists. My application of the algorithm will use one axis-aligned rectangle and one that is not necessarily axis-aligned but a general algorithm would be preferable. Thanks! The answer is fully given here ... http://stackoverflow.com/questions/8011267/area-of-rectangle-rectangle-intersection Here's a polygon-polygon intersection algorithm with sources in C and Java that returns the area of intersection. From what I can tell from your link they're looking at the area of the shape where I need the definition of the shape itself. Thanks for responding though!  One algorithm for finding the intersection of any two convex polygons involves first finding the convex hull around all the vertices. The convex hull follows the outline of whichever polygon is outermost; the shape you're seeking follows the outline of whichever polygon is innermost so follow whichever one the convex hull isn't following. Here is a prettier picture of the same algorithm. This extremely brief wiki page mentions two other algorithms. One has you breaking the polygons into trapezoids. The other is a very clever way of walking around both polygons counterclockwise in lock step. I think I like this one best but as the wiki says it's rather hard to describe. This is what I was looking for thanks!
206,A,Flash lineStyle rounds thickness? The sprite which contains all element in my game world uses the real-world meter as distance unit it doesn't know what pixels are. I then apply a scale to this sprite to make it appear correct on screen. Currently I use 1 meter = 100 pixels so scale = 100. If I try to draw a line inside this sprite it appears lineStyle(thinkness) rounds the thickness parameter. If I specify 0.5 (50 cm) it always gets drawn with 1 pixel (1 cm). If I specify 0.6 the line becomes 100 pixels or 1 meter thick. So basically I can only draw lines of 1 100 200 etc pixels thinkness. Anything I can do about this? Otherwise I'll have to use a smaller unit like millimeters for my world. I'm not sure what you would like the Flash player to do split your pixels up into sub-pixels and render the line at 0.6pixels thickness? It should round at the very end. After all a line of 0.06 is perfectly possible if you scale the whole image by 100 because it should result in a line of 6 pixels The thickness parameter for lineStyle should be an integer in the range 0 to 255. This integer is the thickness of the line in points. So sending a real value just confuses Flash. You'll have to do the math yourself then pass lineStyle the appropriate integer value. With your 1 meter = 100 pixels and a point reasonably approximated by a pixel you'd want to multiply your thickness values in meters by 100 then convert to an integer. I guess there's nothing else to do but that or something like it. Odd decision by Adobe though they have no problem specifying boxes or bitmaps smaller than 1.
207,A,"Handling graphics in OOP style In an object-oriented programming style does how does one tend to handle graphics? Should each object contain its own graphics information? How does that information get displayed? My experience with making graphical programs is limited but I have tended to have the objects and the graphics be only loosely related. For instance if I implemented a chess game I would tend to have a graphics object responsible for making the board getting images for the chess pieces loading them into a hash attached to names such as White_Queen and then updating the screen whenever things changed. The changes themselves would be passed to the main game loop from the objects and then to the graphics object. The piece objects would contain the name of the piece graphics they were attached to in a string but wouldn't have any graphics information in them. Is this OOP? Is it good OOP? What I am thinking about now is that since the project I am working on involves procedurally generated graphics (very very simple ones) I could have the procedures stored in the objects and get the graphics object to read the procedures from objects passed to it. Would this be OOP? Would it be good? In the real world as I have come to know it things do not contain graphics. Things contain properties and my brain interprets them. Blind people's brains and the brains of hawks all interpret different objects differently. Is this OOP? Should I try to do it that way? Hey thanks for your help as usual! z. PS I am writing code in C for the nintendo DS. I'm drawing graphics by coloring pixels individually! Model-View-Controller (MVC) is a popular arrangement of graphics (and other presentation information) in OOP. Objects that are models carry information but don't deal with presentation; objects that are views display that information (e.g. graphically). The wikipedia entry I'm pointing to and many other pages you can search for will give you tons more info!  You have several questions in one. Hopefully this will help. Should each object contain its own graphics information? How does that information get displayed? Programmers tend to differentiate between the terms ""graphics"" and ""graphical user interface"" (GUI). Generally graphics mean static images (like photographs) or dynamic images (like video games) whereas GUI refers to the normal things you see in an application such as windows buttons or menus. Are you referring to graphics or GUIs? There are several layers of abstraction between your program and getting an image on your monitor. On a Windows PC it usually comes down to calling Win32 or DirectX functions which handle all the lowest layer stuff. I could have the procedures stored in the objects and get the graphics object to read the procedures from objects passed to it. You should not put your main functionality in the graphics code or vice versa. Keep them distinct. This makes it much easier to say change the way your pieces look in a chess game or maybe modify the rules without changing the user interface. As mentioned by Alex Martelli the MVC architecture is a good way to separate the business logic (the way it works) from the presentation of your application (the way it looks). In MVC the view is generally thought of holding the GUI which may or may not have ""graphics"". Is this OOP? Is it good OOP? It sounds to me like you need to get a better grasp on what object oriented design means. Since there are so many external resources on this topic I suggest that you Google it. In the mean time maybe you should just experiment and try different ways of programming your game rather than worrying if it's good design. You can figure out what works well and what doesn't--you don't need to get it perfect the first time! In the real world as I have come to know it things do not contain graphics. On a metaphysical level I suppose you're correct... what we see is simply a representation of what is around us electrical signals generated by photons hitting our retinas. When you write your program you are simply trying to generate images which are meaningful to people. I'm not really sure where you could go with this. I'm drawing graphics by coloring pixels individually! Wouldn't it be easier to use some graphics library to handle this stuff for you? That way you could just do things like DrawCircle() rather that doing all the pixels yourself. That said you can learn a lot by implementing your own low level graphics library... just remember that you will spend a lot of time on the library rather than on the game itself."
208,A,"How can I set RenderingHints globally? For a Java application can I set the RenderingHints on a global basis? Currently I've defined these in the paintComponent method as shown below. I would prefer however to set them once when the application starts and have them persist throughout the session. @Override protected void paintComponent(Graphics g) { super.paintComponent(g); Graphics2D g2d = (Graphics2D) g; g2d.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON); g2d.setRenderingHint(RenderingHints.KEY_TEXT_ANTIALIASING RenderingHints.VALUE_TEXT_ANTIALIAS_ON); } Thanks. There's a system property for that. That handles KEY_TEXT_ANTIALIASING but what about KEY_ANTIALIASING? Looks like this will handle it for the text. However as mmyers pointed out I've not seen an option for KEY_ANTIALIASING. System.setProperty(""swing.aatext""""true"");"
209,A,How to darken a custom UIButton I've subclassed the UIButton class and now a custom view will be drawn using quartz 2d. It all looks fine but how can I darken the button at clicking it. How can I set the views for the different states if I am using quartz in the same class to draw the button? Thanks for your help When a button is tapped the setHighlighted: method gets called. If you overwrite it you can do whatever you want upon tap (and release). Redraw or switch views.
210,A,Connecting Catmull-Rom splines together and calculating its length? I'm trying to create a class which takes in any number of points (position and control) and creates a catmull-rom spline based on the information given. What I'm doing - and I'm really unsure if this is the right way to do it - is storing each individual point in a class like so: class Point { public: Vector3 position; Vector3 control; } Where obviously position is the position of the point and control is the control point. My issue is connecting up the splines - obviously given the above class holding a point in the spline array indicates that any given position can only have one control point. So when having three or more points in a catmull rom spline the various individual catmull-rom splines which are being connected share one position and one control with another such spline. Now with position being the same is required - since I want to create splines which are continuous between themselves. However I really wonder should the control points also be the same between the two splines? With a bit of fiddling of the control points I can make it appear to be transitioning smoothly from one spline to another however I must emphasize that I'm not sure if the way they are transitioning is consistent with how catmull-rom splines form their shape. I'd much rather do it correctly than sit on my hands and rationalize that it's good enough. Obviously the second part of my question is self explanatory: Given two control and position points how do I calculate the length of a catmull-rom spline? To define the spline between two control points the Catmull-Rom spline needs the control points and the tangent vector at each control point. However the tangent vector at internal (i.e. non-endpoint) control points is defined by the control points on either side of it: T(Pn) = (Pn+1 - Pn-1) / 2. For closed curves the spline is completely defined by the set of control points. For non-closed curves you need to also supply the tangent vector at the first and last control point. This is commonly done: T(P0) = P1 - P0 and T(Pn) = Pn - Pn-1. Thus for closed curves your data structure is just a list of control points. For general splines it's a list of points plus the first and last normal vector. If you want to have a cardinal spline then you can add a weighting factor to the tangent vector calculation as in the Wikipedia article. To calculate the length of such a spline one approach would be to approximate it by evaluating the spline at many points and then calculating the linear distance between each neighboring pair of points.  With regards to measuring the lengths you can do this with calculus since its a polynomial spline. You need to do integrate the distance function across the line. Its described quite well on Wikipedia...
211,A,"How does a convolution matrix work? I know this isn't very relevant to programming but I need to know how a convolution matrix works for a PHP GD function. I've searched a lot on Google but can't find anything that explains it well. See this: http://www.adobetutorialz.com/articles/1988/1/Convolution-Matrix The operation replaces each pixel with the weighted average of the pixels around it where the weights are given by the matrix. Here's an example convolution matrix: 1 1 1 1 1 1 1 1 1 What this does is replace each pixel with the average value of the 3x3 block centered on that pixel. Here's another: 0 0 0 0 1 0 0 0 0 This matrix doesn't do anything it gives you the original back. The weights can be negative too. This matrix subtracts the average value of the pixels next to a pixel:  0 -1 0 -1 4 -1 0 -1 0 Convolution matrices allow you to do fine-tuned blurring and sharpening effects. You can tune the directionality and the frequency response of filters using a convolution matrix if it's large enough. However it's usually used for quick-n-dirty blurring and sharpening. What is ment by the ""value"" of the pixel? The x/y coords or the colour values? Hussain the value is always the colour representation in some colour space such as Grayscale RGB etc. In the latter each pixel has three ""values"" one per channel.  I don't know specifically for PHP but in general a convolution matrix is used to implement certain kinds of image processing effects. A simple example taken from the PHP manual on GD http://www.php.net/manual/en/function.imageconvolution.php: Let's say you have a matrix like this: $M = array(array( 2 0 0) array( 0 -1 0) array( 0 0 -1)); When you apply that convolution matrix to an image then for each pixel located at (xy) in the image the corresponding pixel in the output becomes: $I = $in_image; $out_image[xy] = $I[x-1y-1]*$M[0][0] + $I[xy-1]*$M[0][1] + $I[x+1y-1]*$M[0][2] + $I[x-1y] *$M[1][0] + $I[xy] *$M[1][1] + $I[x+1y] *$M[1][2] + $I[x-1y+1]*$M[2][0] + $I[xy+1]*$M[2][1] + $I[x+1y+1]*$M[2][2]; In other words the convolution matrix is used to compute each result pixel as a linear combination of the source pixel and the pixels surrounding it. The divisor parameter is used to divide the whole result by something (this is usually the sum of all the values in the matrix) and the offset is used to add a constant term to the final output value. I saw them being used for transport planning once... it freaked me out.  I went into a load of the theory behind convolution filters here: http://stackoverflow.com/questions/1696113/how-do-i-gaussian-blur-an-image-without-using-any-in-built-gaussian-functions/1696200#1696200  Convolution matrix works on each channel of the pixel of the image independently. So you have to learn about channels first. Then you can ""play"" with this matrix interactively if you have a Photoshop: go to the menu Filters->Other->Custom. The central item of the matrix represents current pixel all other represent surrounding pixels. And don't forget about the ""Scale"" value at the bottom. I think you can easily understand the way this matrix works. You may have a look at this thing here. It describes the convolution matrix as well."
212,A,"Check if a point is in a rotated rectangle (C#) I have a program in C# (Windows Forms) which draws some rectangles on a picturebox. They can be drawn at an angle too (rotated). I know each of the rectangles' start point (upper-left corner) their size(width+height) and their angle. Because of the rotation the start point is not necessarely the upper-left corner but that does not matter here. Then when I click the picturebox I need to check in which rectangle (if any) I have clicked. So I need some way of checking if a point is in a rectangle but I also need to take into account the rotation of each rectangle. Does anybody know of a way to do this in C#? Are the rectangles rotated about the origin the upper left corner or another arbitrary point? Would the rectangles be allowed to overlap? If so would you want all the rectangles in a point or just the one in the top layer?  There's a general (language-neutral) discussion of your issue here Yes but I referenced C# because I thought maybe it knows of an easier way to solve this problem.  I know this was already answered but I had to do something similar a while ago. I created an extension method for the System.Windows.Point class that helped do exactly what Neil suggested:  public static double GetAngle(this Point pt) { return Math.Atan2(pt.X -pt.Y) * 180 / Math.PI; } public static Point SetAngle(this Point pt double angle) { var rads = angle * (Math.PI / 180); var dist = Math.Sqrt(pt.X * pt.X + pt.Y * pt.Y); pt.X = Math.Sin(rads) * dist; pt.Y = -(Math.Cos(rads) * dist); return pt; } This would allow me to work with the angles of points around 0 0. So if you know the center of the rect that you are testing you would offset the point by the negative of this value (for example: pt.X -= 32; pt.Y -= 32) And then you would apply the negative rotation of the rectangle (as suggested by Neil: pt.SetAngle(-45);)... Now if the point is within 64 64 you know you hit the rectangle. More specifically I was checking a pixel of a rotated image to make sure I hit a pixel of a specific color.  You could keep a second undisplayed image where you draw duplicates of the rectangles each uniquely colored. When the user clicks on the picturebox find the color of the corresponding pixel in the 2nd image which will identify which rectangle was clicked. That would be a waste of memory and it would be slow. Outis' solution does work. i am using it in CF project right now to do some particularly complex hit testing. But also check out: http://msdn.microsoft.com/en-us/library/system.drawing.rectangle.contains(VS.80).aspx The Rectangle class has a Contains method that can be used to test point containment. +1 for non-math solution not a perfect solution as it wouldnt scale too well but for your average sized image this wouldnt be too bad. If you can keep the bit depth of this ""buffer"" low it wouldnt take too much memory at all. @psasik - post yours as an answer. It's the most straightforward assuming the OP is using .NET and generic C#. It would use a chunk of memory but needn't to be that much. There probably would be less than 256 rectangles; an 800x600 grayscale image would take up 468k which is well within a modern computer's capacity and memory usage for programs. And how would it be slow? It would be O(1) to look up a pixel O(1) or O(log(n)) to map the pixel to a rectangle depending on how you stored the list of rectangles. Faster than checking each rectangle. It's a standard time-space tradeoff. Yes this is a good solution but it's not the one I'm taking. I'll be going with Neil N's. Thank you :) Outis I like your solution the only thing I'm not sure of is how you would handle overlapping rectangles? This is where the bit depth of the memory block comes into play and could get hairy. But depending on the size of the image the total number of rectangles expected this would likely be easier to implement. And less code = better code almost always. @Neil: It's not (yet) defined in my suggestion. If the OP wants all rectangles under a point my suggestion isn't workable. If the OP wants just one that will define the order that rectangles are drawn. Your suggestion IS workable for all rects under a point you just have to use your buffer accordingly. Say he has 8 rects. Keep a byte for each pixel and assign a bit to each rect. Ah interpret pixel colors as bit-vectors where a rectangle's index in a bit-vector is the rectangle's z-index.  Is it possible to apply the same rotation applied to the rectangle to the point in reverse? For example Rectangle A is rotated 45 degrees clockwise from its origin (upper left corner) you would then just rotate point B around the same origin 45 degrees COUNTER clockwise then check to see if it falls within Rectangle A pre-rotation To be fair you brought the ""z-"" to the buffering idea. I think the ideal solution depends on the framework you have set up in your application. In some cases I think Outis's idea of a z-buffer is more ideal. Keep in mind his solution is the same way GPU's render objcts in 3D. This is certainly the way to go but I can't seem to get my math right this moment. Thank you. Hey thanks so much! I literally spent hours messing with trigonometry then to my suprise this actually worked! It's a shame all I can give you is an upvote! :) Neil I had a hard time understanding this at first. It seems simple but I had to write it down on paper to visually understand what was happening. For anyone struggling with it for whatever reason see http://tomsfreelance.com/stackoverflow/CollisionDetectionRotatedRectangle.png for a visual aid."
213,A,"Is this image segmentation? I've been writing a Java program that aides in cutting and working with sprites such as CSS sprites. The main idea is that the image is segmented into subregions so the software understands the ""spritesheet"" is composed of multiple sprites. The algorithm I developed works by scanning horizontal (x) and vertical (y) axes for breaks based on transparency/color mask. This doesn't detect each subpart but applying it recursively may. The 1D axes are transformed into rectangles of where parts are in the image. http://en.wikipedia.org/wiki/Image%5Fsegmentation The Wikipedia article (above) has information on many different techniques. One important thing is that I'm not doing recognition such as detecting foreground objects. Technically using the color mask is separation of a background and foreground though. Have you seen this project? http://csssprites.org/ It is image segmentation but not a general purpose one. Using a mask limits its purpose depending on the kind of mask you're applying but if you're interested only in a specific type of image structure it's quite ok I'd call it domain-specific image segmentation.  You're right. You are technically doing foreground extraction despite having very little background to extract from. I would suggest taking a look at Connected Component Regions this IMO will assist you in extracting nonuniform regions that are well connected. I looked at the Wikipedia page most of those techniques are based on attempting to segment the image on either: ""Interesting regions"" i.e. mahalobis distance or color distance (these are not the same distance measures) By models Pattern recognition"
214,A,Can Graphics.DrawImage unintentionally trim an image? I'm using code that takes a bitmap and converts it to 24 BPP so that I can use it in a program that specifically requires that file format. Here is the code:  using (Bitmap tempImage = new Bitmap(pageToScan.FullPath)) { if (tempImage.PixelFormat != System.Drawing.Imaging.PixelFormat.Format24bppRgb) { using (Bitmap tempImage2 = new Bitmap(tempImage.Size.Width tempImage.Size.Height System.Drawing.Imaging.PixelFormat.Format24bppRgb)) { using (Graphics g = Graphics.FromImage(tempImage2)) { g.DrawImage(tempImage new Point(0 0)); } RecognizeBitmap(pageToScan tempImage2); //Thanks to Tim on this refactoring. } } else RecognizeBitmap(pageToScan tempImage); } I have two questions about the code above: With a particular image I think that this clipped the rightmost 200 pixels right off of tempImage2. Is this possible? How might this happen and how can I stop it? A friend of mine suggested that it might have to do with the stride of the TIFF file being used. Is there a faster way to convert an image to 24 BPP in memory? A better way is to use the Bitmap.Clone method. This takes a PixelFormat as a parameter: using (Bitmap tempImage = new Bitmap(pageToScan.FullPath)) { if (tempImage.PixelFormat != System.Drawing.Imaging.PixelFormat.Format24bppRgb) { Rectangle r = new Rectangle(0 0 tempImage.Width tempImage.Height); RecognizeBitmap(pageToScan tempImage.Clone(r PixelFormat.Format24bppRgb); } else { RecognizeBitmap(pageToScan tempImage); } } Cool. Should I put a using around tempImage.Clone? Yes. You should probably do that and refactor the whole method at the same time to get rid of any leftover duplicated code.
215,A,Java ImageIO: How can I read a BufferedImage from file so that it uses DataBufferFloat? I need to read a BufferedImage from file which doesn't use DataBufferInt (as normally) but DataBufferFloat. Please note: I don't just need some standalone DataBufferFloat but really a BufferedImage with underlying DataBufferFloat. The API around these things is very complex I just can't find how to do this. Please help. EDIT Found out what is not working: DataBufferDouble dbd = new DataBufferDouble(destWidth * destHeight * 4); // Exception here: // java.lang.IllegalArgumentException: Unsupported data type 5 WritableRaster wr = WritableRaster.createPackedRaster( dbd destWidth destHeight 32 new Point(0 0)); BufferedImage bi = new BufferedImage(ColorModel.getRGBdefault() wr false (Hashtable<? ?>) null); createPackedRaster is not appropriate for this. It creates a Raster with a SinglePixelPackedSampleModel which stores r/g/b/a values in bit fields within an int so its transferType can only be an integral type. You probably want a generic raster with a PixelInterleavedSampleModel e.g. DataBufferDouble dbd = new DataBufferDouble(destWidth * destHeight * 4); SampleModel sm = new PixelInterleavedSampleModel(DataBuffer.TYPE_DOUBLE destWidth destHeight 4 destWidth * 4 new int[] {2 1 0 3}); WritableRaster wr = WritableRaster.createWritableRaster(sm dbd null); ColorModel cm = new ComponentColorModel(ColorSpace.getInstance(ColorSpace.CS_LINEAR_RGB) true true ColorModel.TRANSLUCENT DataBuffer.TYPE_FLOAT); BufferedImage bi = new BufferedImage(cm wr true new Hashtable<Object Object>());
216,A,"WPF : using a designed vector image as a control template in WPF I am exploring the idea of using professionally designed vector images as the ControlTemplates in my WPF application. The idea is to make several types of controls each with a different visual design which can then be dragged and dropped. This is exactly the same use-case as a visual designer (a'la visio) I have the following XAML. It defines a control template with a target type of Button and I have a Button which uses this template. What I want to know is how can I modify this template so that it will use only the height and width of the Button. It seems to me as if the Template renders itself relative to the top-left corner of the button (If I move the button the image moves) but it takes no account of the dimensions of the button so it keeps its ""designed"" size no matter what I do. <Window x:Class=""Euphrosyne.MainWindow"" xmlns=""http://schemas.microsoft.com/winfx/2006/xaml/presentation"" xmlns:x=""http://schemas.microsoft.com/winfx/2006/xaml"" Title=""Window1"" Height=""300"" Width=""300""> <Window.Resources> <ControlTemplate TargetType=""{x:Type Button}"" x:Key=""PresentBox""> <Canvas Name=""svg2"" > <Canvas.Resources/> <Canvas Name=""layer1""> <Path Name=""path2385"" Fill=""#D45500"" Stroke=""#FF000000"" StrokeThickness=""1px"" StrokeLineJoin=""Miter"" StrokeEndLineCap=""Flat"" Data=""M 110 82.362183 A 40 30 0 1 1 39.041709 63.365048 L 70 82.362183 z""/> <Path Name=""path2387"" Fill=""#FF0000"" Data=""M 100 100 L 79.96762 89.419767 L 59.895213 99.923868 L 63.767267 77.602471 L 47.574559 61.758424 L 69.999999 58.543274 L 80.064762 38.247014 L 90.052393 58.581339 L 112.46547 61.881608 L 96.212721 77.664061 L 100 100 z""/> <Path Name=""path2389"" Fill=""#C83737"" Data=""M 100 40.000002 L 97.601178 65.149257 L 116.9586 81.382849 L 92.298963 86.872981 L 82.841672 110.29944 L 70 88.543275 L 44.797648 86.788031 L 61.520697 67.85185 L 55.402078 43.340588 L 78.579163 53.393551 L 100 40.000002 z""> <!--path--> </Path> </Canvas> </Canvas> </ControlTemplate> </Window.Resources> <Grid> <Button Template=""{StaticResource PresentBox}"" >Hi there</Button> </Grid> </Window> You could set the Path.RenderTransform to a custom ScaleTransform to scale the sub elements to parent width/height.  Generally Canvas doesn't respect the Size of the parent control so if you replace Canvas with Grid you can get the Button size on to the Paths inside the controltemplate.  <Grid x:Name=""layer1"" Width=""Auto"" Height=""Auto""> <Path x:Name=""path2385"" Fill=""#D45500"" Stroke=""#FF000000"" StrokeThickness=""1px"" StrokeLineJoin=""Miter"" StrokeEndLineCap=""Flat"" Data=""M 110 82.362183 A 40 30 0 1 1 39.041709 63.365048 L 70 82.362183 z"" Margin=""006.45949.147"" d:LayoutOverrides=""Width Height""/> <Path x:Name=""path2387"" Fill=""#FF0000"" Data=""M 100 100 L 79.96762 89.419767 L 59.895213 99.923868 L 63.767267 77.602471 L 47.574559 61.758424 L 69.999999 58.543274 L 80.064762 38.247014 L 90.052393 58.581339 L 112.46547 61.881608 L 96.212721 77.664061 L 100 100 z"" Margin=""004.49362"" d:LayoutOverrides=""Width Height""/> <Path x:Name=""path2389"" Fill=""#C83737"" Data=""M 100 40.000002 L 97.601178 65.149257 L 116.9586 81.382849 L 92.298963 86.872981 L 82.841672 110.29944 L 70 88.543275 L 44.797648 86.788031 L 61.520697 67.85185 L 55.402078 43.340588 L 78.579163 53.393551 L 100 40.000002 z"" Margin=""00051.7"" d:LayoutOverrides=""Width Height""> <!--path--> </Path> </Grid> Or else you can use TemplateBinding which can give Button's properties to the controltemplate. for example.  Height=""{TemplateBinding Height}"" Width=""{TemplateBinding Width}""> And if you cant remove Canvas at all then use a ViewBox around it as Oren suggests above where is the d xml namespace in the d:LayoutOverrides=""Width Height"" coming from? Sorry that was inserted by my Expression blend edit :)  Wrap the Canvas in a Viewbox. The Canvas does not correlate its own dimensions with those of its content. Hence you can see the content move with the Canvas but not resize. A Viewbox augments the Canvas's behavior by stretching its content when its own dimensions change. You can control the Viewbox's behavior e.g. stretch horizontally vertically or both. Unfortunately there's no limo stretching which would have been nice for UI controls. Here's how it might look. Note that I am guessing as for the width and height. They should match those of the actual path. You should also make sure the path is flushed against the X and Y axes to avoid leaving unintended margins. <Viewbox Name=""viewbox1"" Height=""200"" Width=""200"" HorizontalAlignment=""Left"" VerticalAlignment=""Top"" Stretch=""Fill""> <Canvas Name=""layer1""> <Path Name=""path2385"" Fill=""#D45500"" Stroke=""#FF000000"" StrokeThickness=""1px"" StrokeLineJoin=""Miter"" StrokeEndLineCap=""Flat"" Data=""M 110 82.362183 A 40 30 0 1 1 39.041709 63.365048 L 70 82.362183 z""/> <Path Name=""path2387"" Fill=""#FF0000"" Data=""M 100 100 L 79.96762 89.419767 L 59.895213 99.923868 L 63.767267 77.602471 L 47.574559 61.758424 L 69.999999 58.543274 L 80.064762 38.247014 L 90.052393 58.581339 L 112.46547 61.881608 L 96.212721 77.664061 L 100 100 z""/> <Path Name=""path2389"" Fill=""#C83737"" Data=""M 100 40.000002 L 97.601178 65.149257 L 116.9586 81.382849 L 92.298963 86.872981 L 82.841672 110.29944 L 70 88.543275 L 44.797648 86.788031 L 61.520697 67.85185 L 55.402078 43.340588 L 78.579163 53.393551 L 100 40.000002 z""/> </Canvas> </Viewbox>"
217,A,"Create static graphics files (png gif jpg) using Ruby or Python I'd like to create a graphic image on the fly based on user input and then present that image as a PNG file (or jpg or gif if necessary but PNG is preferred). This is actually for an astrology application; what I'd like to do is generate the chart in PNG for display. Python or Ruby is fine; in fact the library available may determine the language I use. Update Here's an example image: Similar to - http://stackoverflow.com/questions/1704670/ruby-rails-image-processing-libraries I found Gruff easy to use when I was in your shoes. Shameless blog plug.  For Python I the most common choice for image formats is PIL and then Pycairo for vector formats. The two can work together for exadmple in this cookbook entry to use PIL images for Pycairo surfaces.  when I was doing python chalange (http://www.pythonchallenge.com/) I've used Python Image Library (http://www.pythonware.com/library/pil/)  In Python you'd typically use PIL the Python Image Library. I've never used PIL for anything beyond the simplest tasks so I can't say how well it performs in practice. I'd start digging into PIL with a look at its documentation particularly the documentation for the draw module. +1: did it last week. Worked nicely -- creates on-the-fly charts.  Look at Ruby-GD2 or an ImageMagick or GraphicsMagick binding.  Maybe a vectorial format is better suited for your needs but is hard to tell without having a concrete example of what you'd like to get. For example if the images are all alike you could create a SVG base image with Inkscape then edit it programmaticaly from Python or Ruby (either by editing the text or using a XML library) and finally export it to PNG. Update: After seeing the example image I think SVG would be the most convenient choice. A SVG image is an XML file that basically says ""draw a circle from here to here write the string '13º52' there"" etc. You could draw a unique base chart in Inkscape and have your program just adding the lines and symbols for each case. Finally you export to PNG. The advantages are: easier for you to draw the image is fully scalable you can change the styling just by editing a property (""make all lines wider"" ""change all text to Arial"" ""paint the background blue"") you can export to any format without losing quality and I think it's more mantainable. Graphic link is above. The thing about vector formats is I'm not sure how portable they are; I don't expect my target audience to be very technical. Vector formats make it easier to draw and you could deliver the final image exported as PNG to your users."
218,A,"Which floating-point image format should I use? In the past I've saved RGB images (generated from physical simulations) as 8-bits/channel PPM or PNG or JPEG. Now I want to preserve the dynamic range of the simulation output which means saving a floating point image and then treating conversion to 8-bits/channel as a post-processing step (so I can tweak the conversion to 8-bit without running the lengthy simulation again). Has a ""standard"" floating point image format emerged ? Good free supporting libraries/viewers/manipulation tools preferably available in Debian would be a bonus. [OpenEXR](http://www.openexr.com/). Have you looked into Radiance RGBE (.hdr) and OpenEXR (.exr). RGBE has some source code here. NVIDIA and ATI both support EXR data in their graphics cards. There are source code and binaries from the OpenEXR download page. ILM created OpenEXR and it has wide support. OpenEXR has support for 16 and 32 bit floating point per channel and is what most people use these days unless they've written their own format. The Pixel Image Editor for linux has EXR support for editing too. pfstools is also necessary if you're going to work with HDR on linux. Its a set of command line programs for reading writing and manipulating HDR and has Qt and OpenGL viewers. Theres also jpeg2exr for linux Heres some other debian packages for OpenEXR viewers. Based on this it looks like theres also a Gimp plugin somewhere. OpenEXR looks perfect. Excellent Debian support too.  It looks like the modern incarnation of FITS would fit your stated needs but I would also suggest you consider using a 2D histogram structure from one of the good analysis packages in wide use by the physics community: ROOT or AIDA are the modern ones that I am familiar with. NB: It's been more than a decade since I used FITS for anything but I recall it begin a nice and flexible way to store fairly raw data."
219,A,Resources for image distortion algorithms Where can I find algorithms for image distortions? There are so much info of Blur and other classic algorithms but so little of more complex ones. In particular I am interested in swirl effect image distortion algorithm. I can't find any references but I can give a basic idea of how distortion effects work. The key to the distortion is a function which takes two coordinates (xy) in the distorted image and transforms them to coordinates (uv) in the original image. This specifies the inverse function of the distortion since it takes the distorted image back to the original image To generate the distorted image one loops over x and y calculates the point (uv) from (xy) using the inverse distortion function and sets the colour components at (xy) to be the same as those at (uv) in the original image. One ususally uses interpolation (e.g. http://en.wikipedia.org/wiki/Bilinear_interpolation ) to determine the colour at (uv) since (uv) usually does not lie exactly on the centre of a pixel but rather at some fractional point between pixels. A swirl is essentially a rotation where the angle of rotation is dependent on the distance from the centre of the image. An example would be: a = amount of rotation b = size of effect angle = a*exp(-(x*x+y*y)/(b*b)) u = cos(angle)*x + sin(angle)*y v = -sin(angle)*x + cos(angle)*y Here I assume for simplicity that the centre of the swirl is at (00). The swirl can be put anywhere by subtracting the swirl position coordinates from x and y before the distortion function and adding them to u and v after it. There are various swirl effects around: some (like the above) swirl only a localised area and have the amount of swirl decreasing towards the edge of the image. Others increase the swirling towards the edge of the image. This sort of thing can be done by playing about with the angle= line e.g. angle = a*(x*x+y*y)  There is a Java implementation of lot of image filters/effects at Jerry's Java Image Filters. Maybe you can take inspiration from there. I wish I could upvote this 10 times. Thank you so much :) but this take BufferedImage as input of filets which is not supported in Android yes bufferedImages are not supported in android..then have u any other idea for these effect.  Take a look at ImageMagick. It's a image conversion and editing toolkit and has interfaces for all popular languages. The -displace operator can create swirls with the correct displacement map. If you are for some reason not satisfied with the ImageMagick interface you can always take a look at the source code of the filters and go from there.  The swirl and others like it are a matrix transformation on the pixel locations. You make a new image and get the color from a position on the image that you get from multiplying the current position by a matrix. The matrix is dependent on the current position. here is a good CodeProject showing how to do it http://www.codeproject.com/KB/GDI-plus/displacementfilters.aspx  there has a new graphic library have many feature http://code.google.com/p/picasso-graphic/
220,A,"Is it possible to deploy a Common Lisp (or other dialect) desktop application for several platforms? I would like to develop a graphical application in Common Lisp or other Lisp dialect that could be deployed in Mac Windows and Linux as a way of improving my knowledge of this language. Ideally: would compile the code would use a common graphical library wouldn't need installation of the runtime environment. I would like to make a little game or graphical app and to be able to show it with a simple installation in a computer with any of these operating systems. Someone has experience with similar situations or could point me to best choices of graphical libraries and compilers runtime environments etc... Thanks! By ""compile the code"" do you mean generate machine code or is VM bytecode OK? Also what if the bytecode is JIT-compiled to machine code by the VM? Yes that's all fine! I'm most interested in the language features. OK. I've updated my answer. I still think PLT Scheme will be the best system for what you want to do as you have outlined it here. There is ECL a Common Lisp implementation that seems to do what you want (I have not used it yet though).  I'm one of the developers of lispbuilder-sdl which is currently hosted on google code. http://code.google.com/p/lispbuilder/ This gives you a way of running common lisp programs on linux windows and mac machines without modification. We use the SDL library and various extensions of it. A minimal program looks like this: (sdl:with-init () (sdl:window 320 240) (sdl:draw-surface (load-image ""lisp.bmp"")) (sdl:with-events () (:quit-event () t) (:video-expose-event (sdl:update-display)))) The wiki also explains how to build self contained exe's for various of the common lisp implementations. Wow very nice. That might very well be the thing that makes me give LISP another try.  PLT Scheme can do all that you ask. The library it uses out of the box is WxWindows but you can get bindings for other GUI systems if you really want to. PLT Scheme is probably the Lisp with the most work done on making it easy to write and distribute ""a little game or graphical app."" They include many examples of games in the base distribution and their flagship IDE DrScheme is written in their own graphics framework. It is free to create and distribute compiled PLT Scheme code on any platform. Edit: You asked for any Lisp dialect. Would Clojure also be an option? Edit 2: You said that Clojure is an option. If you are already very familiar with Java and Swing I think that Clojure would be a good way to go -- with the caveat that it's got some pretty big syntactic differences with other Lisps. A book is forthcoming. If you aren't already a Java expert I think PLT Scheme would still be the best choice. I am coming at this from the ""just beginning to learn Lisp"" and ""small demo application"" angles. Other people have noted that commercial Common Lisp implementations support what you want but those will be harder (and more expensive) to use as introductory systems. All of these implementations have macro systems which I think is what you mean by ""code is first-class."" OK downvoted because it's not Common Lisp I guess. The question asks for any dialect. whups my mistaken reading. PLT is a good system. Some common lisps are far beyond it in terms of delivering commercial software etc. though. CL is a well thought out industrial programming environment after all. So I guess it depends what OP needs. Yes Clojure and PLT Scheme are options too. I don't know much about the differences but one important feature for me is that code is First class like in Lisp. It's downvoted because you wrote that 'PLT Scheme is probably the Lisp with the most work done on making it easy to write and distribute a little game or graphical app'. Is that the case? I have experience programming java so clojure might be a good options. Is it suitable for mid-size applications? For example if the demo grows in size is it still suitable? @Rainer: I think the presence of the games package in the distribution the included DSLs for animation and the one-click package creation reflect the PLT group's commitment to that particular use case. @Álvaro: probably the biggest downfall of Clojure in that respect is that it is a new language and thus the definition is still in flux. If you want to take advantage of features added later you may need to update old code. Steven LispWorks and Allegro CL also make it dead easy to develop graphical applications and deliver them. A lot of work has been invested in these features and to make them easy to use. For example into the GUI designer of Allegro CL. MCL was the easiest of all of them. Sure but their documentation and examples are targeted for a different use case. Hmm I have a LispWorks here. There is an example directory capi/applications/examples/. What do you think files like chat-client.lisp hangman.lisp maze.lisp othello.lisp pong.lisp might be about? Fair enough. Maybe my real prejudice is that it would cost the original asker quite a bit of money. Although I didn't state it in the question I would prefer if it is an open-source solution.  Most Common Lisp implementations can dump executable images. Sometimes these are two files (kernel + image) but often this can be just one executable. But the executable usually runs only on the platform it was compiled for. Commercial implementations like LispWorks or Allegro CL have extended capabilities - for example one can remove unused parts of the Lisp system. This is called 'delivery'. There is some information about LispWorks applications. LispWorks is commercial and covering platforms can be expensive. You would have to buy the development environment - delivery is free on popular platforms. LispWorks has a graphical library for Windows Mac and Unix/Linux. The latter is based on the oldish Motif. The advantage is that the code can be very portable over platforms. The LispWorks development environment itself is a LispWorks application. On the Mac for example the 32bit version and the 64bit version runs on both PowerPC and x86 from a single application. Information about 'free' versions of Common Lisp and libraries is collected on CLIKI. Writing MS Windows applications is not the strongest part of 'free' Common Lisp though.  What I'm doing presently for a graphical application that uses OpenGL and GLFW is developing primarily with SBCL and giving my testers deliveries via cl-launch. However my plan is to use CCL to build an application bundle on OS X and ECL to build a stand-alone executable on Linux and Windows. The bundles I'm building at the moment with cl-launch are fairly large (typically 30M and up) while the tests I've done with ECL have been much smaller (libecl weighs in at about 1.3M on my system). However I would expect SBCL to perform better (though I'd profile to make sure first!) so your choice will depend on your application. However if I were doing this commercially I would invest in one of the commercial implementations. Rainer Joswig mentions LispWorks and Allegro above. For Windows app delivery you can also consider Corman Lisp. My impression is that the fastest but most expensive route to doing application delivery across those three OSes is to buy Allegro but an alternative (more work but cheaper) would be to use CCL on OS X Corman on Win32 and ECL or SBCL on Linux. LispWorks seems to be a choice in between although many people swear by it so I wouldn't discount it as inferior to Allegro just because it's more affordable. The graphics library issue is something separate; my impression is that the situation is constantly improving (callbacks in CFFI seem to work on most platforms now which is a big help in interfacing to most C toolkits) but I've been working more with GL GLFW GLUT and SDL (though not yet with lispbuilder mentioned by justinhj above which looks cool). I did experiment a little with wxCL a year or two ago and it seemed promising. The nice thing about CL is that with so many good implementations you can develop in your implementation of choice and most of your code should be easily ported to whichever implementation you choose for application delivery on a given platform.  Rainer has a good comment on the fundamental issues: There are good commercial solutions doing exactly what you ask (but they are not free development environments and may involve recurring fees) with well supported cross-platform libraries. There are also free software approaches but it is more difficult to deliver on all three platforms as you require (not impossible but more fiddling about). I will add though that ""not installing the runtime environment"" is a bit problematic. You certainly don't need to have the end-user separately install a lisp but your program may well need to essentially install the entire runtime in order to work depending on what you do. The flexibility at this level means it can be difficult to programatically determine what bits are and are not needed which is why the free software solutions usually don't bother with the somewhat tedious and tricky work of writing a tree shaker to do this. Delivering the environment with my app is ok I just wouldn't like to force the user to make a full install on his own  Ecl lisp can compile very small executable (a few kbs) but on Ubuntu in order to execute this executable file there must be libecl.so.11.1 file in your /usr/local/lib directory. This file is 5.7 mb."
221,A,Java: create graphics without awt? Is there any library out there to create graphics without using AWT? What I need is simple drawing functions (like to draw a line) and text drawing functions to create graphics in memory for a Google app engine application. App engine does not support AWT. Thanks! In the same spirit as rleir comment could batik be an option ? http://xmlgraphics.apache.org/batik/ Does the client support SVG? If so you could look at http://java.sun.com/javame/technology/msa/jsr226.jsp It's website that anyone can use so the client may not support SVG. AlBlue: that's not true. I agree with you BobMcGee. Just to clarify: I voted some answers down because they were *TOTALLY* wrong and some of them had been voted up before my vote thing I just can't understand. But I also voted up other answers that -although not giving a solution to my question- were good answers. I think that's what the voting system is for. @AlBlue: That's true (and problematic) but a lot of the answers are unhelpful or actually require AWT and are such that a quick google would confirm that. I think it's fair to rate down answers that a quick fact check indicates are incorrect. @David No I've implemented a simple class to do some drawings and then I used a java library to compress it as png. This project of mine is dead now so I can't tell you the name of the png lib. As for drawing text: I've found no solution. Did you ever find a good answer to this? I've researched everything in the list appengine-awt is dead and doesn't work SenseLan doesn't do what's needed Batik for GAE project is dead TinyLine hasn't been updated in a year is buggy and unusable and the author takes a week to respond to a simple email and python has the same problems. You might also try the appengine-awt project though it's a bit experimental.  Google Web Toolkit contains a nice graphics library designed for interfacing with the Google app engine. edit to clarify: Google App Engine is designed for hosting applications on the web. You need to design graphics that can run in the browser. To do this you need to write code in a web language Javascript for example. Google Web Toolkit contains a Java graphics library which compiles down to Javascript saving you the effort of writing the Javascript yourself. What? Google Web Toolkit is not a graphics library... But I need to create images on the server.  I hesitate to mention PJA which appears to work if the AWT classes are present but the security manager prevents you from using them.  There's a blog here about deploying Javafx applications to Google App Engine: http://110j.wordpress.com/2008/12/31/javafx-on-google-app-engine/ you can use that to create graphics but you need to do some research first into 2D graphics and such: http://www.dieajax.com/2007/10/15/10-minute-tutorial-javafx-basic-2d-graphics-and-animation/ JavaFX would be running on the client side not the server side. Yes I need to create images on the server. On second thought it's not an elegant solution but would it work for you to generate the image on the client side (JavaFX or Java Applet) and upload the image to the server? No way... what if I need to create a captcha? Besides that I can't think of it as a solution to create images on the server.  Use Batik for GAE which is available as a dependency of FOP on GAE. You can also track the issue further on the Google app engine bug tracker where others have shared other ideas in the comments.  If you can use Python on GAE instead of Java then there's pybmp.  Not unless you want to implement your own image class (say a bitmap) and rendering algorithms for lines shapes images. If you have experience with computer graphics and rasterization this may not be very hard but otherwise it will be more than you want to bite off. That's what I've done and it works great but the problem is rendering text. That's something I think I cant solve writing my own code. At least not as easily as drawing lines or circles. Well I found this: http://fonteditor.org/ ... I will try it out as soon as I can. @Damian: Text rendering is complex and I'm not sure you'll find an easy solution. You *might* try using the non-native parts of AWT as a standard package -- I recall seeing a fully software (no GPU) implementation of a lot of the drawing functionality somewhere. They were using it for benchmarking and demonstrating a parallel approach.  TinyLine provides vector graphics support on the Google App Engine server side and also provides SVG rendering support. See the SVG Thumbnail images demo. Didn't work for us and the developer seems to have pretty much dropped it.  You might try using SenseLan. In the requirements section it says they don't use awt or ImageIO. Of course there is the Images api but it seems fairly limited in what it offers. Edit: It looks like there are a couple of Python possibilities that could offer you some limited drawing capabilities. You could probably write appropriate image functionality as python web services and keep the rest of the app in Java: http://stackoverflow.com/questions/1131992/replacing-functionality-of-pil-imagedraw-in-google-app-engine-gae http://denislaprise.com/2008/08/21/drawing-images-on-google-app-engine/ Well senselan is great but it only converts images from format to format. What I need is simple graphic functions (like to draw lines) and to draw text. As the title says it must be a Java library. My app is already developed in java. @Damian: Jhython -- run Python code in Java and would let you bridge the gap.  'The Java 2D API is a set of classes for advanced 2D graphics and imaging encompassing line art text and images' http://java.sun.com/products/java-media/2D/index.jsp Here's another possibility: org.eclipse.draw2d It probably relies on eclipse SWT. Look at the javadoc: http://java.sun.com/j2se/1.4.2/docs/guide/2d/spec.html I can't use AWT.
222,A,"Where is the best place to re-learn graphics programming Thinking in regards to Sliverlight I would like to know where would be good places to go to get a refresher on 3d space transforms matrix manipulation and all that good stuff. Think I may have found it myself. Was looking at: http://msdn.microsoft.com/en-us/library/cc189037(VS.95).aspx and http://www.c-sharpcorner.com/UploadFile/mgold/TransformswithGDIplus09142005064919AM/TransformswithGDIplus.aspx Your question specified 3D this is all 2D. You found your answer in 2 minutes. Next time you might want to search before asking lol 3D is just a part of it. But also want the 2D stuff  It's not a place but I've found 3D Programming for Windows by Charles Petzold excellent. It covers everything you ask about and is focused specifically on WPF/silverlight. Of course Petzold (as usual) is able to communicate the important concepts beautifully.  Personally I think that although the bible (by Foley & Van Damn that is) was the greatest book for its time but it is somewhat outdated. I would suggest 'Advanced animation & Rendering techniques' by Alan and Mark Watt. The only problem with this book is that it gives you a good understanding almost about every broad aspect in CG but it assumes you have some familiarity with it and does not explain it all the way. You can always have a look in the Bib and find enhanced articles and books about each subject you are interested in depth. If you want further on once you have more understanding or if you want to dive into the world of computer graphics and the use of GPU I suggest to have a look at the three 'GPU Gems'.  There's always The Bible It is expensive and very heavy on the theory so there's also the cheaper Bible Lite As pointed out in some comments and additional answers it is definitely worth noting that this book is now quite dated. However in the context of the original question there's not really been any change in the low-level principles of linear algebra in a seriously long time. If you are looking to learn about high-level graphics programming this may well not be the first book for you. But if you like to know about ""the guts-of-the-machine"" and the underlying maths -- perhaps you are the kind of person that thinks folk should learn C :-) -- then go nuts. Absolutely can't beat the Foley and van Dam and some other guys that gave a few opinions... although it is quite dated by now.  As previously mentioned you should really learn linear algebra here are some great video lectures about it MIT Linear Alebgra Video Lectures.  Nehe is good if you want practical tutorials rather than theory. I like these tutorials but they haven't been kept up to date. If you can get them running using current compiler and OS they are still really good. It takes a little work to get them running but it's worth it.  Any linear algebra textbook should provide the math refresher; there's a fairly good one available online at Linear Algebra textbook home page.  Free graphics algorithms can be found in the comp.graphics.algorithms faq"
223,A,Open source (under the MIT licence) library for loading and saving image files? I am writing an imaging library (using the MIT licence) and would like to hook it up to a library that supports loading and saving of images in various file formats (bmp png jpg etc) I am aware of FreeImage but it appears to use a GPL licence. Is there anything out there similar to FreeImage that uses MIT rather than GPL ? Well just so you know there's also DevIL and it is under the LGPL.  The ImageMagick license is not MIT but it appears to be compatible (though I'm no expert on these license issues). The Adobe Generic Image Library has an MIT license although I am not sure if it has what you are looking for. ImageMagick is way overkill for what i want i think. ImageMagick not only loads/saves but offers all range of image manipulation tools. I solely need the loading/saving.  Have you looked at the PIL (python image library). It should do what you want and it looks like an MIT license (it has a free permissive one) . not using python unfortunately
224,A,"Generating Professional Graphs In Windows In Windows I am looking at generating professional graphs using any mainstream programming language (C# VB.Net Java PERL etc). The best free looking graphs I have found so far is Microsoft Chart Controls for .NET. What other graph controls/modules do you suggest? Note: Added free to the requirements. There are dozens and dozens of commercial charting components out there. Personally I like the DevExpress tools. XtraCharts is really good well documented and relatively affordable.  Have you seen Zedgraph?. It is LGPL but rather extensive.  jFreeChart is [one of] the best solution[s] out there for Java. I totally recommend it.  GLE is a scripting language designed for generating professional quality graphs and diagrams for publication. It uses LaTeX for typesetting so including equations in your graphs is a snap and they look great. Christopher Bishop used GLE for all the diagrams in his textbook ""Pattern Recognition and Machine Learning"" and let me tell you they looked great. Open-source to boot! First you write the GLE script and tell it to accept a text file as the graph input data. Then you can generate the data set in whatever language you want export as a text file and call the GLE script to generate the graph. More info from the website: GLE (Graphics Layout Engine) is a graphics scripting language designed for creating publication quality graphs plots diagrams figures and slides. GLE supports various graph types (function plots histograms bar graphs scatter plots contour lines color maps surface plots ...) ... GLE's output formats include EPS PS PDF JPEG and PNG."
225,A,How do I use a shape to define a clipping region? I am still simulating a radar (or attempting to) and through trial and error managed to draw a pie on top of my picturebox's background the more or less covers the target area I wish to draw to. Now I'm trying to make that area my clipping region. How do I achieve this? I haven't come across anything that explains this clearly. I have the following code: void OnPaintRadar(Object sender PaintEventArgs e) { Graphics g = e.Graphics; Rectangle radar_rect = new Rectangle(myRadarBox.Left + 90 myRadarBox.Left + 18 myRadarBox.Width - 200 myRadarBox.Height + 200); using (Pen drw_pen = new Pen(Color.White 1) ) { g.DrawPie(drw_penradar_rect 180 180); } } what I want to do now is make th pie I've just drawn my clipping area. You can't use the pie you've drawn on the Graphics you need to define it separately for the region : GraphicsPath gpath new GraphicsPath(); gpath.AddPie(rect startAngle sweepAngle); gpath.CloseFigure(); this.Region = new Region(gpath); Thanks very much I still have a problem adding my image to the region. I have a tiny pic with I use to depict an object; using g.DrawImage fills the entire region with the pic which isn'tthe result I wanted and g.DrawImageUnscaled shows nothing.
226,A,What is the best 2D graphics library for Windows Mobile? The title speaks for itself... I've tried to find another question like it but must have missed it if it exists. I'm looking for anything from a core library replacing GAPI to a .net wrapper... It's for building a nice UI so no 3D needed... What graphics functionality do you need that isn't in System.Drawing? The System.Drawing classes in .NET are a lot more powerful than people realize. The only thing missing (which is actually really important in 2D graphics) is the ability to read and write individual pixels. This is technically possible using the GetPixel and SetPixel methods on the Bitmap object but these methods are unimaginably slow. Fortunately there is a super-fast alternative that you can implement by calling LockBits on a bitmap and then accessing the bitmap's byte data directly. I've used this to good effect. In fact I've spent the last week creating a fisheye lens effect entirely in .NET (no libraries or PInvoke calls at all). This JPEG is kind of crappy but it shows what's possible using just .NET:  IMO I would suggest getting hold of the frame buffer somehow and drawing to it directly if possible or using D3DM or use the drawing services provided by the Native Platform (GDI) through APIs like DrawGradient() etc from coredll.dll and since you are on WM you might be able to leverage aygshell.dll to draw widgets. Here are some links... 1)Using gradient fill from dot net 2)Using gradient fill directly 3)Other native GDI functions  Take a look at WMGL http://www.mobilityflow.com/products/wmgl/
227,A,"pyopengl: Could it replace c++? I'm starting a computer graphics course and I have to choose a language. Choices are between C++ and Python. I have no problem with C++ python is a work in progress. So i was thinking to go down the python road using pyopengl for graphics part. I have heard though that performance is an issue. Is python / pyopengl mature enough to challenge C++ on performance? I realize its a long shot but I'd like to hear your thoughts experiences on uses of pyopengl. Thanks in advance. its not whether Python is mature enough its just an issue of computer performance computers aren't to the performance level yet to where they can afford to waste the extra cycles that python uses compared to c++ this will change however at some point as it inevitably does (with computer speeds increasing year after year).. a language like c++ would always be used I think for cutting edge science etc where they are attempting to use every last bit of computing resource but for most applications higher level languages would eventually take over as they have throughout history so far Python is a dynamic language that get interpreted and compiled on runtime and as such cannot have better performance then C++ - take a look at this post for comparison between several programming languages. Another good reason to prefer C++ is parallel execution. Many tasks in CG can be optimized by splitting them to multiple threads tgat run in parallel - ever tried to start a new thread using Python? Could you make precise what you imply with ""ever tried to start a new thread using Python""? Threads are very easy to start in Python. :) Did you mean that CPython's threads suffer from the Global Interpreter Lock? Actually I never did multi-threaded development in Python but I saw a lot of posts against it  Python is an awesome language but it's not the right tool for graphics. And if you want to do anything remotely advanced you'll have to use unpythonic libraries and will end up with ugly C code written in Python. Thats exactly what a friend told me same words different language. That's why you write your own libraries when it comes to doubt.  Python is the way to go. Since all opengl programming is uploading data to the video card RAM then using opengl to operate on it the speed limitations in python are moot. Also it makes the hard things in C++ easy ie opening files images sounds etc. As for the person above implementing octrees there is nothing stopping you from using numpy which is written in C from implementing it. (also make sure you are using linear memory like a binary tree and not pointers to objects in a link like structure) Blog post on this subject  It depends a LOT on the contents of your computer graphics course. If you are doing anything like the introductory course I've taught in the past it's basically spinning cubes and spheres some texture mapping and some vertex animation and that's about it. In this case Python would be perfectly adequate assuming you can get around the Unpythonic (and lets be honest un-C++) OpenGL state-machine paradigm. For things like doing your matrix maths you can use Numpy the core of which is written in C and is really quite quick. You'll be up and running faster iterate faster and most likely have more fun. If however you are doing some hardcore cutting edge millions-of-triangles-per-scene-skinned-animated-everything computer graphics course stick with C++. If your class has given you the choice it's probably a safe bet that Python will be ok. If you want to leverage your knowledge into a real job in computer graphics though pretty much every game and graphics engine is written in C or C++ while Python (or Lua) is left as a scripting language. Thanks i appreciate the real world computer graphics paragraph Can't explain how precisely correct I think this answer is. @phkahler - what do you mean? I think phkahler means you're so very very correct that he couldn't explain it in words. ;] & Xavier. Yes it's a great answer and clearly I was unable to articulate that thought :-)  Here's my personal experience: When I first heard about PyOpenGL I was absolutely thrilled. OpenGL in my favourite language? Deal! So I started learning 3D graphics programming by myself. I went through several tutorials and books like NeHe and the OpenGL SuperBible. Because PyOpenGL's functions are identical to that of OpenGL itself's (with very minor differences) it wasn't hard to replicate most of the examples. Besides NeHe has many source code in Python that others made. It wasn't too long after (about 2 weeks) I read up on Quaternions and implemented in Python myself. Now I have a GLSL-enabled environment with full 3D camera interaction options. I made a simple Phong shader and used Quaternions to drive my camera rotations. I haven't got a single performance hit yet. Months later I came back to this code. I attempted a Python Octree implementation and when I went to 8 levels (256x256x256 voxels) it took more than 2G of RAM to compute and minutes after it still isn't done. I realised when you store many objects in Python it's not just a simple struct like in C++. That's where I realised I need to factor this out write this in C++ and then glue it back with a Python call. Once I'm done with this if I remember I will update you. ;] (To answer your question no Python will never replace C++. Those two lanaguages have different purposes and different strengths.) As computer performance increases a dynamic language can potentially become useful eventually even for high performance games computer power should always be viewed as less valuable than programmers when computers advance enough it would make sense to use Python for things that now are only able to be done in c++ of course it won't be faster in terms of running on the machine but the development time would be.. there would be a paradigm shift so only things that not even possible now would be done in c++ and everything done now could be done in a language like Python >> of course it won't be faster in terms of running on the machine I've done a large octtree in Python as a series of Numpy operations. Numpy is crazy insane I've gotten used to the idea that stuff that makes me cringe at the mere thought of attempting it in C can be done in a couple of lines of Python. Numpy does the same thing for big data crunching."
228,A,j2me 2d graphics engine For J2ME I found a number of GUI frameworks like LWUIT J2ME Polish Twuik etc however I am looking out for a 2d Graphics Engine for the Java ME platform preferably lightweight < 50K I came across TinyLine this supports reasonable features for a mobile device. On similar lines do we have an open source or free 2d graphics engine library or framework available. Perhaps you are looking for this? http://stackoverflow.com/questions/1284093/what-libraries-are-available-to-help-create-2d-java-games-for-phones/1284139 These are mostly tile based animations however I am looking for animations with lines shapes curves. Thanks.
229,A,"Icons for an IPhone application user interface This has been asked before but not satisfying for me. If you develop an IPhone application you need some basic icons. For me they fall into two categories: a) Icons that represent your application and b) Icons in your applications user interface. For the first category you are basically stuck to create your own icon or to hire someone because it will represent your application and is therefor somewhat an trademark / servicemark. (e.g. iStockphoto license 4.4.) Icons in the user interface can be bought or downloaded from the usual places. The problem is that non of the sites I've found so far have special Iphone icons. E.g. great outline icons that fit into the navbar. Home-icon anyone? Where do I get icons that are made for the IPhone touch user interface? possible duplicate of [Basic art for the programmer?](http://stackoverflow.com/questions/334812/basic-art-for-the-programmer) I have an article up on my site that shows how to use OmniGraffle with a template I provide to create great iPhone toolbar icons in minutes: http://steveweller.com/articles/toolbar-icons/ You still have to draw your own but that's all you have to do and the toolbar icons have to be simple anyway.  If you're using Photoshop here's an excellent starter template: http://blog.cocoia.com/2010/iphone-4-icon-psd-file/  I think that these: glyFX or these Glyphish might be more like what you were looking for.  Here is a very nice set of TabBar icons for the iPhone - 60 bones though but they look like they're worth it: Alas the website is gone. ""The resource you are looking for has been removed had its name changed or is temporarily unavailable."""
230,A,Image Transformation on iPhone how to? Since I cannot pre-render all the images in PNGs and real-time image transformation functions are required namely: skew perspective (like the transform action found in Photoshop) Which API (CoreAnimation? OpenGL ES?) should I look into? Even better is there any sample code around? Thanks! For skew/shear you can use: CGAffineTransform CGAffineTransformMakeShear( CGFloat inX  CGFloat inY ) { return CGAffineTransformMake( 1  inY  inX  1  0  0 ); } For perspective a 2D affine transform is insufficient. You can apply a 3D affine transform to a layer and get the results that perspective is trying to emulate. myView.layer.transform = CATransform3DMakeRotation( ... );
231,A,"How to draw a (bezier) path with a fill color using GDI+? I am making a SVG renderer for Windows using the Windows API and GDI+. SVG allows setting the 'fill' and 'stroke' style attributes on a Path. I am having some difficulty with the implementation of the 'fill' attribute. The following path represents a spiral:  <svg:path style=""fill:yellow;stroke:blue;stroke-width:2"" d=""M153 334 C153 334 151 334 151 334 C151 339 153 344 156 344 C164 344 171 339 171 334 C171 322 164 314 156 314 C142 314 131 322 131 334 C131 350 142 364 156 364 C175 364 191 350 191 334 C191 311 175 294 156 294 C131 294 111 311 111 334 C111 361 131 384 156 384 C186 384 211 361 211 334 C211 300 186 274 156 274"" /> The fill color is yellow and it should fill the entire shape this is however what I get: My GDI+ calls look like this: Gdiplus::GraphicsPath bezierPath; bezierPath.AddBeziers(&gdiplusPoints[0] gdiplusPoints.size()); g.FillPath(&solidBrush &bezierPath); g.DrawPath(&pen &bezierPath); Apparently the code is correct for drawing the shape but not for filling it. Can anyone help me in figuring out what's going wrong? Try to set the FillMode property of your GraphicsPath to FillMode::Winding an alternate filling method that should suits your needs. I just found it out myself typical :S But thanks!"
232,A,"Antialiased composition by coverage? Does anyone know of a graphics system which handles composition of multiple anti-aliased lines well? I'm showing a dependency diagram and have a bunch of curves emanating from a point. These are drawn anti-aliased in the usual way of blending partially covered pixels. So if two lines would occupy the same half of a pixel the antialiasing blends it to 75% filled rather than 50% filled. With enough lines drawn on top of each other the pixel blend clamps and you end up with aliased lines. I know anti-grain geometry has algorithms for calculating blends which cater for lines which abut and that oversampling might work but are there any other approaches? It sounds like you want a premade drawing library which I do not know of. However to answer your question of knowing any approach that would work you can consider a pixel to be a square. You can then approximate any shape that you draw as a polygon that intersects the pixel box. By clipping these polygons against the box of the pixel and against each other you can get a very good estimate of the areas associated with each color that intersects the pixel for accurate antialiasing. This is of course very slow to calculate and is not suitable for interactive drawing.  Handling this form of line composition well is going to be slow (you have to consider all the lines that impinge upon each pixel using a deferred rendering approach). I doubt that there are many (if any) libraries out there that will do it for you. The quickest and easiest method (and possibly the only realistic and cost effective solution for your case) which will work with virtually any drawing library would be to supersample it - draw to an offscreen bitmap at much higher resolution (e.g. 4 times wider and higher with lines of 4 pixels width. Disable antialiasing when drawing this as it'll only slow it down) and then scale the result down with bilinear filtering. The main down-side is that it uses a lot of memory for the offscreen bitmap. I think that's about the only practical method without spending a lot of time working on it.  If you need an existing system that gets antialiased lines ""visually correct"" you might try using one of several existing RenderMan-compliant 3D renderers. The REYES algorithm which many of these renderers use works by breaking up primitives into micropolygons then sampling them at several random point locations within each pixel. So even if you have a million lines collectively obscuring 50% of a pixel the resulting image value will show roughly 50% coverage. (This is for example how the millions of antialiased hairs are drawn on characters in many animated movies.) Of course using a full-blown 3D renderer to draw 2D lines is like driving nails with a sledgehammer. You'd need a fairly pathological scenario for the 3D renderer to be any more efficient than simply supersampling with a traditional 2D renderer."
233,A,"How to Draw Two Detached Rectangles in DirectX using the D3DPT_TRIANGLESTRIP Primitive Type I am new to DirectX and I am trying to draw two rectangles in one scene using D3DPT_TRIANGLESTRIP. One Rectangle is no problem but two Rectangles is a whole different ball game. Yes i could draw them using four triangles drawn with the D3DPT_TRIANGLELIST primitive type. My curiosity is on the technic involved using D3DPT_TRIANGLESTRIP. Parts of the code i am using for one Rectangle using D3DPT_TRIANGLESTRIP is as follows: CUSTOMVERTEX recVertex[] = { { 10.0f 10.0f 0.10f 1.0f 0xffffffff } // x y z rhw color { 220.0f 10.0f 0.10f 1.0f 0xffffffff } { 10.0f 440.0f 0.10f 1.0f 0xffffffff } { 220.0f 440.0f 0.10f 1.0f 0xffffffff } }; if( FAILED( g_pd3dDevice->CreateVertexBuffer( 4 * sizeof( CUSTOMVERTEX ) 0 D3DFVF_CUSTOMVERTEX D3DPOOL_DEFAULT &g_pVB NULL ) ) ) { return E_FAIL; } More vital code... VOID* precVertex; if( FAILED( g_pVB->Lock( 0 sizeof( recVertex ) ( void** )&pGameField 0 ) ) ) { return E_FAIL; } memcpy( precVertex recVertex sizeof( recVertex ) ); then Render like so... g_pd3dDevice->SetStreamSource( 0 g_pVB 0 sizeof( CUSTOMVERTEX ) ); g_pd3dDevice->SetFVF( D3DFVF_CUSTOMVERTEX ); g_pd3dDevice->DrawPrimitive( D3DPT_TRIANGLESTRIP 0 2 ); Based on this model I could easily duplicate the code change the x and y values on the Custom Vertex and create another vertex buffer and it would work. Personally i feel this is not the best way to go especially considering a situation when i have to draw say 100 rectangles or something like that. The thing is I don't have other ideas. So my question is what is the most efficient way to draw two Rectangles with D3DPT_TRIANGLESTRIP? Also is there a possible way of duplicating and transforming the current rectangle? The most efficient way is to use an indexed draw and then to repeat indices in between the 2 rectangles. ie indices 0 1 2 3 3 4 4 5 6 7 produces the following triangles from triangle stripping 0 1 2 - 1 2 3 - 2 3 3 - 3 3 4 - 3 4 4 - 4 4 5 - 4 5 6 - 5 6 7 the triangles 2 3 3 - 3 3 4 - 3 4 4 and 4 4 5 all have zero area and will be discarded very early on the graphics card. This way you get 4 triangles rendered in one call to DrawIndexedPrimitive. Of course that means you are sending in 10 indices. You may as well just send in 12 indices and draw both rectangles as 4 triangles in a tri list. When using indexed drawing you'll find that the hardware keeps track of what vertices it has already transformed so that it doesn't have to bother again. ie if you define your index buffer as 0 1 2 3 0 2 4 5 6 7 4 6 and you have a post transform cache of at least 4 vertices (Pretty much every graphics card has a significantly larger post transform cache The GeForce 3 certainly did!) then you can see that both triangles will be rendered and yet each vertex will only be transformed once. Counter-intuitively on any post DX8 hardware (ie any DX9 or 10 hardware) you'll find that the use of this post transform cache and extra simplicity of triangle lists means that you'll actrually get BETTER performance from a triangle list than a triangle strip. So in summary in answer to the question ""So my question is what is the most efficient way to draw two Rectangles with D3DPT_TRIANGLESTRIP?"" the answer is to draw is a D3DPT_TRIANGLELIST and stop worrying about it :)  You could extend recVertex to 8 points. This is a common technique. 3D hardware designed to work exactly in such mode. Check out this tutorial for instance. If you want to draw 100 (I assume that means ""a huge number"" in context of question) rectangles you should consider using meshes.  It's generally best to batch up geometry into larger indexed vertex buffers and call the appropriate draw functions with the correct offsets into the buffer for each 3D object. Remember hardware doesn't view your objects as separate entities it only cares about vertex/index buffers textures shader programs and assorted state. Your job is to feed it all to the hardware in an optimum manner and batch up a reasonable amount of drawing tasks per frame. For tiny little geometry and fixed function like this you should definitely be slapping the whole lot into a single buffer.  Add the four vertices for the second rectangle to the vertex buffer then call DrawPrimitive twice. g_pd3dDevice->SetStreamSource( 0 g_pVB 0 sizeof( CUSTOMVERTEX ) ); g_pd3dDevice->SetFVF( D3DFVF_CUSTOMVERTEX ); g_pd3dDevice->DrawPrimitive( D3DPT_TRIANGLESTRIP 0 2 ); g_pd3dDevice->DrawPrimitive( D3DPT_TRIANGLESTRIP 4 2 ); Ok this is what i was actually looking for. Its given me great insight. only one thing remember to change the 4 to 8 since we now have 8 vertices in the CreateVertexBuffer function."
234,A,"Understanding colors Kindly point towards theory/material to read for understanding colors and what makes a good color combinations. Mind it that I am not interested in say ""Color combinations for web application"" etc. More of the lines of say ""Colors and humans"". Material free to read is what i am looking for. Thanks Great question but I fear this is one of the topics where no short answer exists as there are simply too many things to consider and too many nuances. I'd be interested in an answer anyway. Actually you're a lot better off asking that question on art forums. Colors is a very very wide and complex subject in itself. @Loki: agreed but there's some minimum that can be useful for developers and therefore is a good question for this forum. Color and humans is a very complex topic. Scientists do not completely understand how we humans perceive color. (See also http://en.wikipedia.org/wiki/Psychophysics) There are a lot good books out there but some free resources I use: http://en.wikipedia.org/wiki/Color http://en.wikipedia.org/wiki/Color_vision http://handprint.com/HP/WCL/wcolor.html http://www.efg2.com/Lab/Library/Color/ http://www.cis.rit.edu/fairchild/  Some theory here: http://www.worqx.com/color/index.htm  In addition to the links I'd like to post my way of selecting pleasant colors: NEVER ever use pure colors. Even if you want a pure color don't!. If you want a strong bright green for example don't use 00ff00. Use something like 10e013 instead. If you have one color that you like and you want another one that fits to the first open a graphic program. Go to the color picker type in your color and then switch to HSV mode. Then adjust either one of Hue Saturation or Lightness. Don't modify two or all parameters just one. That makes sure the color you choose is perceptually related to the color you've started with. If you have no idea what color to start with get a classic masterpiece of painting from the net. Blur it a bit and then pick some nice colors from it. If you use some common sense it's hard not to end with pleasant colors this way. Just to give you an example: I've just picked these colors: From this painting: http://www.cs.nthu.edu.tw/~sheu/Images/Monet.jpg I know - it's not exaclty what you've asked for but I learned these tricks the hard way. Great rules of thumb for those of us who aren't designers at all and still want to avoid causing eye cancer. Avoiding eye cancer was the reason why I came up with these rules (with a lot of help from graphical artists btw... ) Thanks for the ans nils :)  Information Visualization by Colin Ware: Contains much more than just color theory. But it's a great academic resource for understanding some basics of human perception and how to make sure your designs work well.  Robin Williams' ""The Non-designer's Design Book"" is a must-read for anyone who needs to prepare material for publication (including software UI) and includes a chapter on colour: http://www.amazon.com/Non-Designers-Design-Book-3rd-Designers/dp/0321534042/ref=sr_1_1?ie=UTF8&s=books&qid=1233828208&sr=1-1  See the Color Scheme Generator Color Wizard Color Combinations. They all have some theory or rationale."
235,A,"How to fill a Path in Android with a linear gradient? Given a closed Path object result is like this: Although that is a rectangle I'm looking for something which works with any closed Path. this may help protected void onDraw(Canvas canvas) { super.onDraw(canvas); int w = getWidth(); int h = getHeight(); Paint p = new Paint(Paint.ANTI_ALIAS_FLAG|Paint.FILTER_BITMAP_FLAG); Path pth = new Path(); pth.moveTo(w*0.27f0); pth.lineTo(w*0.73f0); pth.lineTo(w*0.92fh); pth.lineTo(w*0.08fh); pth.lineTo(w*0.27f0); p.setColor(0xff800000); p.setShader(new LinearGradient(000h0xff0000000xffffffffShader.TileMode.CLAMP)); canvas.drawPath(pthp); }  While steelbytes' answer will probably give you more control over the individual sections of the gradient you can do it without the path: protected void onDraw(Canvas canvas) { super.onDraw(canvas); Paint p = new Paint(); // start at 00 and go to 0max to use a vertical // gradient the full height of the screen. p.setShader(new LinearGradient(0 0 0 getHeight() Color.BLACK Color.WHITE Shader.TileMode.MIRROR)); canvas.drawPaint(p); } ok but NEVER construct the Paint instance in the onDraw(...) methods @ebtokyo: For performance reasons? Would you cache it on creation and and update its size on each onDraw (or maybe in resize)? Yes for performance reasons. Lint comments allocations in onDraw() like this: ""Avoid object allocations during draw/layout operations (preallocate and reuse instead)"" @ebtokyo: also LinearGradient instance should be pr-instansiated in another stage on the view lifycle"
236,A,Method for building lightweight cross-platform text editor I'm planning to build a simple lightweight text editor that combines a great look with keyboard focused input. I want to have a lot of control over things like antialiasing and all the graphics in general but I don't care about having a whole library of widgets. Almost the entire UI will be text-based and in the main canvas/window of the app. Toolkits like GTK and Qt seem like overkill - tons of widgets I don't need and a complex codebase. Titanium AIR and XULRunner are even bigger in some ways - dev would be quick but that's not exactly the lightweight approach. Shoes seemed like a nearly perfect fit but it's a little too small and doesn't support enough events (e.g. no window resize). What do you think should I just build it on Cairo/Pango or another graphics library and roll the platform specific stuff myself? I'd rather use a framework of some kind. Basically all I want is: good event handling windowing menus drawing with really great type rendering choices I would love to build this cross-platform from the start. Even if you use just Cairo/Pango you still need windows on the screen. The simplest combination would be cairo + pango + GDK (the windowing part of GTK+) however even in that case you have no menus and constructing menus with pure Cairo would be a tough (but not impossible) accomplishment. If you insist on lightweight and crossplatform then check the following Fast Light Toolkit Fox Toolkit wxWidgets (goes great with Python) Thanks. wxWidgets looks like a great choice. wxRuby feels like a more complicated more mature version of Shoes which is exactly what I wanted.  Scintilla is a cross-platfrom source code editing component (based on GTK+) with excellent support for syntax highlighting code folding text zooming and of course all the usual text editing functions; it should be an excellent foundation for a text editor.
237,A,"beamer includegraphics with screenshots I'm using the LaTeX-Beamer class for making presentations. Every once in a while I need to include screenshots. Those graphics are pixel-based of course. I use includegraphics like this: \begin{figure} \includegraphics[width= \paperwidth]{img/analyzer.png} \end{figure} or usually something like this: \begin{figure} \includegraphics[width= 0.8\linewidth]{img/analyzer.png} \end{figure} This leads to pretty bad readibility of the contained text so I'm asking for your best practices: How would you include screenshots containing text considering that I will do the output PDF with pdflatex? EDIT: I suppose I'm looking for something like an 1:1 presetation of the image within beamer. However [scale = 1.0] doesn't achieve what I'm looking for. Thank you for coming back and sharing. `keepaspectratio` seems to be the key here. Just for reference it has been answered at [tex stackexchange][1] [1]: http://tex.stackexchange.com/questions/11954/automatically-scale-big-and-small-graphics-for-beamer-presentations Have you tried to convert the image to .eps or .pdf file and use this file in LaTeX? Maybe try also latex dvips and ps2pdf. Problem might be in used viewer in Linux I use Document viewer or ePDFViewer and output is much worse than in Adobe Reader or Acrobat which I use in Windows... But the viewer seems to be an issue looks a lot better in Acrobat Reader on Windows. I tried to convert it however I think it's a scaling issue...  How about scaling it as follows: \includegraphics[scale=0.5]{images/myimage.jpg} This works for me. Thanks for contributing. `scale` didn't work for my issue but see the proposed answer in the comment section of the question there's been a solution over at the TeX Stack Exchange site.  I have done exactly what you do and e.g defined \newcommand{\screenshot}[1]{\centerline{% \includegraphics[height=7.8cmtransparent]{#1}}} % 7.8in which worked with whatever style I was using at the time. The files included with this macro were all PNGs created with one the usual Linux screen capture tools. Edit: You may have to play with the size (height and width) of your input files. It came out rather nice for me (and this was from a presentation in 2006). What does `transparent` parameter do?  Your best bet is to scale the image outside of Latex for inclusion and include it in 1:1 ratio. The scaling done by graphics packages in Latex isn't going to be anywhere near as good as possible from other tools. Latex (Tex) has limited floating-point arithmetic capabilities whereas an external tool can use sophisticated algorithms to get the scaling better. Another option is to use only a part of the screenshot the one you want to concentrate on. Edit: If you can change the font size before taking the screenshot that's another option—just increase the font size for the screenshots. Of course you can combine the two methods. makes sense but how do I determine what the size has to be for the 1:1 ratio? If I try images around 800x600 and just pass them in I only get to see a scaled version which only shows the upper left corner of the image.. The only solution I can see is to put something like: `\message{width = \the\textwidth}` in your document and see what the current text width `w` is (it should be in points which is `1/72.27` inches). Then if your PDF is `n` dpi in resolution you need `w*n/72.27` pixels wide image. I can't seem how to do this as I need to decrease the resolution of the image which means that the text would be ultimately unreadable. The unscaled image takes more than the entire slide of the beamer presentation (I see only the upper left corner of the image) and the only option is to resize my 800x600 screenshot to something like 200x300. What does the answer mean by increasing the ""font size"" and which method of increasing font size should be used? It's been a while if I'm not mistaken the font size was the one of a potential text editor the screenshot was taken of. The idea was simply to increase the font size of the editor to achieve more readibility in the resulting screenshot (even if it was scaled). Also notice that I was using `LaTeX Beamer` on Linux back then and the choice of the pdf viewer had an impact on the image quality shown."
238,A,"Drop shadows on iPhone Clock App ""Alarm"" tab In the clock app that comes with the iPhone there is a tab view for setting alarms. In that view each of the UITableViewCell instances have a drop shadow around them. Does anyone know how to achieve the same effect? Also it looks like the tab bar at the very bottom has a drop shadow above it as well. Ideas on how to achieve the same look would be greatly appreciated. I would guess that there is an extra cell which contains just a background image which is the transparent dropshadow. If it's not a cell (because that might create scrolling weirdness) it is probably an extra view positioned below the bottom cell of the UITableview which - again - simply contains an image of the drop shadow. Hmmm... I don't think the extra cell theory would work but the extra view below the cell would definitely be a possibility. I will try it out tomorrow  I was wondering how to do that and it just occurred to me to use the UITableView's footer view: myTableView.tableFooterView = myCustomViewWithDropShadowImage;  Like Thomas said create a 100% width image (say 320 x 40px on non-retina devices) and create 4 instances of UIImageView with it. The first at the top of your main view. The second at the bottom and in addition do this: UIImageView* bottomShadow = [[UIImageView alloc] initWithImage:[UIImage imageNamed:@""BottmShadow.png""]] bottomShadow.transform = CGAffineTransformMakeScale(1 -1); (Flip it vertically) Then do the same with the other two but place them as subviews of the Table View. One of them just outside the first row: CGRect tableTopShadowFrame = tableTopShadow.frame; tableTopShadowFrame.origin.y = -(tableTopShadowFrame.size.height); [tableTopShadow setFrame:tableTopShadowFrame]; and the other just below the last row (you need to know the height of all the rows together. If your rows are all the same height it is row height times number of rows). Finally you need to set the table's backgroundColor property to transparent tableView.backgroundColor = [UIColor clearColor]; and possibly set some darkish gray for your main view's background color."
239,A,"How do I get the length of a VBO to render all vertices when using glDrawArrays()? I create a VBO in a function and I only want to return the VBO id. I use glDrawArrays in another function and I want it to draw all the vertices in the VBO without needing to also pass the number of vertices. The VBO also contains texture coordinate data. Thank you. You need to return it sorry. Data about the VBO might live somewhere far away from your CPU and be slow to access so you need to keep locally whatever data you need. Okay thank you. The reason it is a problem is because I'm using C++ and python together with ctypes. Ctypes doesn't like structures when I've tried which I would use to return the two things needed. I could use global variables but that just seems like a ridiculous solution so I will try structures again. Thank you for the answer. Can you pass by reference from Python to C? That's another way you can ""return"" data from a function; it might help. Ctypes doesn't mind me using pointers so I allocated some memory for an array and stored the data I needed in this array. The data is all GLUint so an array is perfect. I used the new and delete operator to do this. It's almost completely working now. At least the screen is not blank. :D Oh dear. I have a memory leak. I suppose my deletion function isn't being called...  Maybe it could be not useful for you application you can use glGetBufferParameteriv with argument GL_BUFFER_SIZE: it returns the number of bytes of the buffer object. It's difficoult to say that this is the solution since you should know the internal format of the buffer element (and indeed its size in bytes) in order to have the number of elements composing the buffer object. For sure the best solution is to keep most information in a class representing the buffer object but as I can understand from your question this is hard to implement. Thank you for your answer. ;) I did managed to get it working by storing the information myself."
240,A,Free or Open Source Diagramming Component for Winforms I need to be able to generate dependency diagrams programmatically. I'd like it to be able to generate a bunch of boxes with labels and connectors linking them and ideally the component would position them automatically onto a design surface which could then be manually rearranged. I'm using Winforms and C# 2.0 (VS2005). EDIT: However since it'll be an internal tool I can probably use 3.5 SP1 and WPF if there any suggestions in that arena. Must be free or open source. Any recommendations? Here is some references which might be interesting for you: Diagrams.NET Diagram.NET is a free open-source diagramming tools written entirely in C#. Put Diagram.NET WinForm Control into your form and like Microsoft Visio® the user can draw shapes and links. With some code you can control change add and delete these elements. Microsoft Chart Controls The samples environment for Microsoft Chart Controls for .NET Framework contains over 200 samples for both ASP.NET and Windows Forms. The samples cover every major feature in Chart Controls for .NET Framework. They enable you to see the Chart controls in action as well as use the code as templates for your own web and windows applications.  Did you try the CodeProject article series by Sukram: WPF Diagram Designer  Take a look at my question: http://stackoverflow.com/questions/1710642/generate-dynamic-flow-chart It had some decent answers.  Too late for the questioner but perhaps interesting for researchers: NShape NShape is an Open Source diagram designing framework for .NET WinForms. Software developers use NShape to integrate diagramming capabilities into their applications. Using NShape applications let users view annotate modify and create diagrams like flow charts wiring schemes or project charts. NShape is open source and has a dual license which allows it to be employed for free in open source projects and for a license fee in commercial projects.  You might want to take a look at GraphViz which is being distributed under CPL (i.e. free of charge). It is not exactly for .NET/WinForms but can be useful anyway. The library's main purpose is to visualize graphs (and dependency diagram is effectively a [directional] graph). You can use it to either get an image (graph layout) or an array of points (coordinates for you items). Interesting. I guess I could write some code which transforms my own input to whatever format GraphViz takes and run it against that. Worth investigating - might be better than reinventing the wheel. Unfortunately I have not tried any of this (was not in need) so I cannot help you here. Thanks for this it's working well although the diagrams are large and are going to be hard to format for printing. It would be good if I could do a little manual adjustment - do you know of a way? I wonder if I render as SVG can I import into Visio?
241,A,ImageIcon + ImageIcon = ImageIcon I have two ImageIcons and I want to create a third ImageIcon which has nr 2 drawn upon nr 1. How would I best do that? The following code takes an Image from two ImageIcons and creates a new ImageIcon. The image from the second ImageIcon is drawn on top of the image from the first then the resulting image is used to make a new ImageIcon: Image img1 = imageIcon1.getImage(); Image img2 = imageIcon2.getImage(); BufferedImage resultImage = new BufferedImage( img1.getWidth(null) img1.getHeight(null) BufferedImage.TYPE_INT_ARGB); Graphics2D g = resultImage.createGraphics(); g.drawImage(img1 0 0 null); g.drawImage(img2 0 0 null); g.dispose(); ImageIcon resultImageIcon = new ImageIcon(resultImage); Edit (Fixed some errors added transparency support.) For allowing transparency the BufferedImage.TYPE_INT_ARGB can be used for the image type in the constructor rather than BufferedImage.TYPE_INT_RGB which does not have an alpha channel. That is pretty close the new image seems to have a black background instead of a transparent one Changed the type to TYPE_4BYTE_ABGR and that took care of the alpha. Many thanks
242,A,"Rendering hundreds of ""block"" images -vs- Partially rendering a ""map"" of blocks It seems that none of these solutions eliminate the lag associated with rendering images onto the screen (whether from .png's or using CG). There is a 28x16 grid of blocks that are each 16x16 pixels and at some points in the game about half of them need to change their image because of a state change. Having each block as a subview of the main view causes a lag when half of them need to individually change their image (either from .png's or using CG). I tried having one ""map"" view with whose drawRect: method is: CGContextRef context = UIGraphicsGetCurrentContext(); CGContextClearRect(context rect); // width and height are defined as the width and height of the grid (28x16 for iPhone) for (int x = 0; x < width; x++) { for (int y = 0; y < height; y++) { // states[] is an enum instance variable that holds the states of each block in the map (state determines image) if (states[(x * height) + y] == PXBlockStateEmpty) { CGContextSetFillColorWithColor(context [UIColor colorWithRed:153.0/255.0 green:153.0/255.0 blue:153.0/255.0 alpha:0.5].CGColor); CGContextSetStrokeColorWithColor(context [UIColor colorWithRed:0.0 green:0.0 blue:0.0 alpha:0.5].CGColor); CGContextAddRect(context CGRectMake(x * 16 y * 16 16 16)); CGContextDrawPath(context kCGPathFillStroke); } else if (states[(x * height) + y] == PXBlockStateSolid || states[(x * height) + y] == PXBlockStateSolidEdge) { CGContextSetFillColorWithColor(context [UIColor colorWithRed:0.0 green:0.0 blue:102.0/255.0 alpha:0.9].CGColor); CGContextSetStrokeColorWithColor(context [UIColor colorWithRed:0.0 green:0.0 blue:0.0 alpha:0.5].CGColor); CGContextAddRect(context CGRectMake(x * 16 y * 16 16 16)); CGContextDrawPath(context kCGPathFillStroke); } else if (states[(x * height) + y] == PXBlockStateBuilding) { CGContextSetFillColorWithColor(context [UIColor colorWithRed:51.0/255.0 green:51.0/255.0 blue:51.0/255.0 alpha:0.5].CGColor); CGContextFillRect(context CGRectMake(x * 16 y * 16 16 16)); } } } So my solution was to call setNeedsDisplayInRect: on the map passing the frame (a 16x16 rect) of the block whose state changed. Am I using setNeedsDisplayInRect: incorrectly or is this just an inefficient way to do it? Both options (one subview vs hundreds of subviews) lag the game for a bit when a lot of the board is filled with a different image and the second solution in particular lags whenever ANY block's image needs to be updated. Any ideas? Thank you for your help! Calling setNeedsDisplayInRect: affects the argument to drawRect: but is otherwise the same as setneedsDisplay. The rectangle drawRect: receives is the union of all dirty rectangles. If you only draw content that intersects that rectangle leaving other content untouched you may see a speed improvement. Judging from the code snippet above you draw every tile every time. Since it is a union you might also track dirty tiles separately and only draw dirty tiles. In that case call CGContextClearRect for individual tiles instead of clearing the whole context unless they are all dirty. CGContextAddRect adds a rectangle to a path that will be drawn. Since you draw the path at every loop without beginning a new path you redraw areas as the loop progresses. It would probably be faster to use CGContextFillRect and CGContextStrokeRect instead of a path. Making each tile a separate UIView incurs more overhead than drawing as a whole. You should be able to get better speed from this method once the kinks are worked out. What is a ""dirty rectangle""? I don't understand the first paragraph of your answer :( http://developer.apple.com/mac/library/documentation/cocoa/conceptual/CocoaViewsGuide/Optimizing/Optimizing.html figured it out! thanks for pointing me in the right direction drawonward!"
243,A,How do I set a surf to one color (no gradient) in my matlab-plot? My dataset consists of three vectors (xy and z). I plot these values as dots in a 3d-plot with plot3(xyz) which is fine. I also want to show a plane in the same plot. To get the data of this plot I use linear regression on x and y to get a new z. This is how it looks: I want the surf to be filled with only one color (say light blue or gray) and set the opacity to make it see-through. How can I do this? The easiest way to create a surface that has just 1 color and a given transparency value is to set the 'FaceColor' and 'FaceAlpha' properties of the surface object: hSurface = surf(...your arguments to create the surface object...); set(hSurface'FaceColor'[1 0 0]'FaceAlpha'0.5); This example sets the surface color to be red and the transparency to 0.5. You can also set the edge properties too (with 'EdgeColor' and 'EdgeAlpha').  @matlabDoug has what you need I think. The property cdata holds color data that gets a color map applied to it. Setting it to an array the same size as your surface data with each element in that array having the same value will make your surface one color. With the default color map setting everything in cdata to zero will make your surface blue and setting everything to 1 will make the surface red. Then you can play with the alpha to make it transparent.  It is not clear to me what you want to do. When you say one color for the surf do you mean exactly one color or do you mean you want shades of gray? Here is some code that will do a variety of things you can choose which lines to use: x = rand(120); y = rand(120); z = rand(120); [XY] = meshgrid(linspace(0110)linspace(0110)); Z = rand(10)*0.1; clf plot3(xyz'.'); hold on h = surf(XYZ) hold off %% This will change the color colormap(copper) %% This will remove colordata set(h 'cdata'zeros(10)) %% This will make transparent alpha(0.5) I tried this. Setting cdata to zero everywhere moved the plane so that it lied at z=0. So that doesn't seem like a good solution.  Completing the answer from gnovice an extra ingredient in set(hsurface...) may be required (Matlab R2010b 64): hSurface = surf(...your arguments to create the surface object...); set(hSurface 'FaceColor'[1 0 0] 'FaceAlpha'0.5 'EdgeAlpha' 0); to make invisible the point-to-point edges of the plotted surface
244,A,"Image/""most resembling pixel"" search optimization? The situation: Let's say I have an image A say 512x512 pixels and image B 5x5 or 7x7 pixels. Both images are 24bit rgb and B have 1bit alpha mask (so each pixel is either completely transparent or completely solid). I need to find within image A a pixel which (with its' neighbors) most closely resembles image B OR the pixel that probably most closely resembles image B. Resemblance is calculated as ""distance"" which is sum of ""distances"" between non-transparent B's pixels and A's pixels divided by number of non-transparent B's pixels. Here is a sample SDL code for explanation: struct Pixel{ unsigned char b g r a; }; void fillPixel(int x int y SDL_Surface* dst SDL_Surface* src int dstMaskX int dstMaskY){ Pixel& dstPix = *((Pixel*)((char*)(dst->pixels) + sizeof(Pixel)*x + dst->pitch*y)); int xMin = x + texWidth - searchWidth; int xMax = xMin + searchWidth*2; int yMin = y + texHeight - searchHeight; int yMax = yMin + searchHeight*2; int numFilled = 0; for (int curY = yMin; curY < yMax; curY++) for (int curX = xMin; curX < xMax; curX++){ Pixel& cur = *((Pixel*)((char*)(dst->pixels) + sizeof(Pixel)*(curX & texMaskX) + dst->pitch*(curY & texMaskY))); if (cur.a != 0) numFilled++; } if (numFilled == 0){ int srcX = rand() % src->w; int srcY = rand() % src->h; dstPix = *((Pixel*)((char*)(src->pixels) + sizeof(Pixel)*srcX + src->pitch*srcY)); dstPix.a = 0xFF; return; } int storedSrcX = rand() % src->w; int storedSrcY = rand() % src->h; float lastDifference = 3.40282347e+37F; //unsigned char mask = for (int srcY = searchHeight; srcY < (src->h - searchHeight); srcY++) for (int srcX = searchWidth; srcX < (src->w - searchWidth); srcX++){ float curDifference = 0; int numPixels = 0; for (int tmpY = -searchHeight; tmpY < searchHeight; tmpY++) for(int tmpX = -searchWidth; tmpX < searchWidth; tmpX++){ Pixel& tmpSrc = *((Pixel*)((char*)(src->pixels) + sizeof(Pixel)*(srcX+tmpX) + src->pitch*(srcY+tmpY))); Pixel& tmpDst = *((Pixel*)((char*)(dst->pixels) + sizeof(Pixel)*((x + dst->w + tmpX) & dstMaskX) + dst->pitch*((y + dst->h + tmpY) & dstMaskY))); if (tmpDst.a){ numPixels++; int dr = tmpSrc.r - tmpDst.r; int dg = tmpSrc.g - tmpDst.g; int db = tmpSrc.g - tmpDst.g; curDifference += dr*dr + dg*dg + db*db; } } if (numPixels) curDifference /= (float)numPixels; if (curDifference < lastDifference){ lastDifference = curDifference; storedSrcX = srcX; storedSrcY = srcY; } } dstPix = *((Pixel*)((char*)(src->pixels) + sizeof(Pixel)*storedSrcX + src->pitch*storedSrcY)); dstPix.a = 0xFF; } This thing is supposed to be used for texture generation. Now the question: The easiest way to do this is brute force search (which is used in example routine). But it is slow - even using GPU acceleration and dual core cpu won't make it much faster. It looks like I can't use modified binary search because of B's mask. So how can I find desired pixel faster? Additional Info: It is allowed to use 2 cores GPU acceleration CUDA and 1.5..2 gigabytes of RAM for the task. I would prefer to avoid some kind of lengthy preprocessing phase that will take 30 minutes to finish. Ideas? Now this is a fun problem :) Decent code no sqrts or heavy math. Well done already. Just fighting a high order O to get what you need. Not a great solution but reducing the bit depth of both A and B would speed things up a little. PDiff is an open source perceptual image difference tool that maybe has some helpful techniques for you.  You may try to find approximate solution: Patch Match This paper presents interactive image editing tools using a new randomized algorithm for quickly finding approximate nearest neighbor matches between image patches. Previous research in graphics and vision has leveraged such nearest-neighbor searches to provide a variety of high-level digital image editing tools. However the cost of computing a field of such matches for an entire image has eluded previous efforts to provide interactive performance. Our algorithm offers substantial performance improvements over the previous state of the art (20-100x) enabling its use in interactive editing tools. Interesting but I've found another solution.  I'd consider moving your early out on cur Difference into your inner loop so that it can short circuit well before the inner loop is done if the error is already too great. Your trading ifs for some heavy math. Also the pixel scale value on the error could be a multiply instead of a divide (minor on new machines.) Any chance of reading multiple pixels at a time or paralleling their processing? For threading you could start threads per each external for loop iteration (breaking up into how ever many threads you want to use) to allow your CPUs to be more efficicent. Syncing max error would be the only issue - which could be accomplished by storing the errors into an external table and comparing at the end to prevent memory contention. Caching your structures to get rid of -> 's can help but the compiler usually does this for you. Just some thoughts to begin with. Still looking...  Answering to my own question. Short answer: I was able to drop alpha channel so I've decided to use image pyramids (see pyramid and gaussian pyramid on the net). It gave huge speed improvement. Long answer: My initial goal was texture synthesis. Alpha was used to generating pixels that weren't filled yet and B represented a portion of already generated image. (I.e. A was sample pattern and B was generated image) After a bit of researching I've found that either there is no quick way to do a search in N-dimensional space (for example 3x3 pixel area is basically an 24 component vector (center pixel excluded) while 7x7 wlil be 144-component searching for such area will be 24-dimensional or 144-dimensional search). Well there are ways (for example paper named ""I-COLLIDE: an interactive and exact collision detection system for large-scale environments"" uses 3 sorted arrays (each sorted on different dimension) to do 3 dimensional search) but they obviously will work better for floats and lower number of dimensions. Suggestion to use motion detection wasn't useful because (it seems) motion detection assumes that pixels represent moving objects (not true in my case) and at least some optimization relies on that. In the end I've found paper named ""Fast Texture Synthesis using Tree-structured Vector Quantization"" (Li-Yi Wei Marc Levoy Stanford University) which uses technique based on algorithm similar to the one I used. Image to be searched is downsampled several times (similar to mip-map generation) search performed on the lowest level first and then on the next. It may not be the best way to do actual image search for other application but it perfect for my purposes. The paper is relatively old but it works for me. The same paper mentions a few techniques for accelerating search even further. One of them is ""Tree-structured vector quantization (TSVQ)"" although I can't give more info about it (haven't checked it - current texture generator works with acceptable speed on my hardware so I probably won't look into further optimizations).  One potential speedup might be to use binary operators. For example you could march through A XOR B for subsequent overlapping regions of A. The resulting region whose values are closest to 0 would be the portion of A that most closely resembles B. If you had to take the alpha mask into account assume A's alpha mask is all 1s and include it in the XOR- 32 bits per pixel instead of 24. I was talking about GPU(videocard) accelerated implementation that uses shaders for bruteforce search. From within shader code you ""see"" any color as 4component vectors of floats (rgba) and you can't performa XOR operations on them. You may get better performance because GPU is optimized for certain types of calculations and faster texture memory access. Unfortunately brute force search doesn't benefit from them - because random texture memory access very quickly brings performance down. Also it looks like I was wrong and GPU/OpenGL implementation works slower on my machine than single-threaded CPU version. This probably happens because it isn't top-notch gaming videocard and because implementation wasn't efficient (using shaders instead of CUDA wasn't a good idea). Hmm. I'll think about it but frankly I was looking for different/better search algorithm. Plus the fastest implementation of that thing I have uses OpenGL GLSL shaders and they operate on floats internally (no XOR). @SigTerm: You said both images are 24 bit RGB - I may be missing something but how does having those values stored in floating point make for better performace?  You'll want to look at motion estimation which is used in video coding to find the location of a block in a previously coded picture that most closely resembles the block to be coded. (NOTE: I don't have enough reputation to post 2 links so you'll have to look up motion estimation in wikipedia). Some simple block matching algorithms can be found here. These work by only analyzing a subset of points in the search area. If you want to find the specific point that minimizes your matching function you'll have to do a full search. Full-search speedup is usually achieved by early termination - ending the evaluation of a point if it is already impossible for it to improve upon the previous best result. min_sad = INT_MAX // minimum sum of absolute difference min_point = {0 0} foreach (point p : all_search_points ) { sad = 0 for( y = 0; y < block_height; ++y ) for( x = 0; x < block_width && sad < min_sad; ++x ): sad += abs( block_b[yx] - block_a[p.y+y p.x+x] ) if( sad < min_sad ) min_sad = sad min_point = p } Early termination is also useful when examining only a subset of search points though the speedup isn't as great as with full search. Sounds promising I'll check it out. Terminating search is also a good idea."
245,A,"Generate PDF from structured data I want to be able to generate a highly graphical (with lots of text content as well) PDF file from data that I might have in a database or xml or any other structured form. Currently our graphic designer creates these PDF files in Photoshop manually after getting the content as a MS Word Document. But usually there are more than 20 revisions of the content; small changes here and there spelling corrections etc. The 2 disadvantages are: 1) The graphic designer's time is unnecessarily occupied. The first version is the only one he/she should have to work on. 2) The PDF file becomes the document which now has the final revised content and the initial content is out of sync with it. So if the initial content needs to be somewhere else (like on a website) we need to recreate it from the PDF file. Generating the PDF file will help me solve both these problems. Perhaps some way in which the graphic designer creates a ""Template"" and then puts in tags/holders and maps these tags/holders to the relevant data. Thanks :-) In the past I have written scripts that spit out LaTeX then used texi2pdf to solve this kind of problem.  Sounds like a job that SQL Server Reporting Services can handle quite easily. Reporting Services allows you to query the data define the layout and export to PDF without any intervention. The PDF output can be distributed via email stored on a file share and accessed via a page on the report server. It can handle XML data sources too.  A possible way is to use a template engine like FreeMarker or StringTemplate: these are often used to generate HTML but they are flexible enough to output any format actually. The problem is to make a PDF template I suppose. Perhaps you can take a sample output and edit it to replace data with placeholders to be filled by the template engine. Might not be trivial!  Take a look at iReport and JasperReports at http://jasperforge.org. iReport lets you design reports and then you can either programatically fill it with the JasperReports library (Java) or just use iReport to manually create the report. I have only used it for tabular data but I don't think there would be any problem for other types of documents.  I use the ReportLab python library for this. It could perhaps solve your problem but you will need to do some work...  You could create a form and populate the entries programmatically using a pdf library like iText (Java).  Another approach to generating a PDF file from data is to use prawn which is based on ruby. I was very pleasantly surprised by how much functionality is included in prawn. It may take some investment up front but this approach will give you a lot of flexibility.  You could look at doing the workflow in PostScript which is plain text that you can easily compose from fragments. Then you can use any free tool to convert to PDF.  Take a look at Prince XML. This tool allows to generate PDF based on XML or HTML and CSS.  There are some tools out there for doing this. XSL-FO is useful. Here is a tutorial for creating a pdf from xml (or xhtml) with cocoon. Also see Apache FOP. You could format your SQL data as XML and still use the same templates this way."
246,A,What is wrong with my snap to grid code? First of all I'm fairly sure snapping to grid is fairly easy however I've run into some odd trouble in this situation and my maths are too weak to work out specifically what is wrong. Here's the situation I have an abstract concept of a grid with Y steps exactly Y_STEP apart (the x steps are working fine so ignore them for now) The grid is in an abstract coordinate space and to get things to line up I've got a magic offset in there let's call it Y_OFFSET to snap to the grid I've got the following code (python) def snapToGrid(originalPos offset step): index = int((originalPos - offset) / step) #truncates the remainder away return index * gap + offset so I pass the cursor position Y_OFFSET and Y_STEP into that function and it returns me the nearest floored y position on the grid That appears to work fine in the original scenario however when I take into account the fact that the view is scrollable things get a little weird. Scrolling is made as basic as I can get it I've got a viewPort that keeps count of the distance scrolled along the Y Axis and just offsets everything that goes through it. Here's a snippet of the cursor's mouseMotion code: def mouseMotion(self event): pixelPos = event.pos[Y] odePos = Scroll.pixelPosToOdePos(pixelPos) self.tool.positionChanged(odePos) So there's two things to look at there first the Scroll module's translation from pixel position to the abstract coordinate space then the tool's positionChanged function which takes the abstract coordinate space value and snaps to the nearest Y step. Here's the relevant Scroll code def pixelPosToOdePos(pixelPos): offsetPixelPos = pixelPos - self.viewPortOffset return pixelsToOde(offsetPixelPos) def pixelsToOde(pixels): return float(pixels) / float(pixels_in_an_ode_unit) And the tools update code def positionChanged(self newPos): self.snappedPos = snapToGrid(originalPos Y_OFFSET Y_STEP) The last relevant chunk is when the tool goes to render itself. It goes through the Scroll object which transforms the tool's snapped coordinate space position into an onscreen pixel position here's the code: #in Tool def render(self screen): Scroll.render(screen self.image self.snappedPos) #in Scroll def render(self screen image odePos): pixelPos = self.odePosToPixelPos(odePos) screen.blit(image pixelPos) # screen is a surface from pygame for the curious def odePosToPixelPos(self.odePos): offsetPos = odePos + self.viewPortOffset return odeToPixels(offsetPos) def odeToPixels(odeUnits): return int(odeUnits * pixels_in_an_ode_unit) Whew that was a long explanation. Hope you're still with me... The problem I'm now getting is that when I scroll up the drawn image loses alignment with the cursor. It starts snapping to the Y step exactly 1 step below the cursor. Additionally it appears to phase in and out of allignment. At some scrolls it is out by 1 and other scrolls it is spot on. It's never out by more than 1 and it's always snapping to a valid grid location. Best guess I can come up with is that somewhere I'm truncating some data in the wrong spot but no idea where or how it ends up with this behavior. Anyone familiar with coordinate spaces scrolling and snapping? Thanks for the answer there may be a typo but I can't see it... Unfortunately the change to snapToGrid didn't make a difference so I don't think that's the issue. It's not off by one pixel but rather it's off by Y_STEP. Playing around with it some more I've found that I can't get it to be exact at any point that the screen is scrolled up and also that it happens towards the top of the screen which I suspect is ODE position zero so I'm guessing my problem is around small or negative values. Regarding the typo you are passing `originalPos` instead of `newPos` to `snapToGrid()`. As for the negative values could it be related to the fact that int(-1.5) == -1 ?  Do you have a typo in positionChanged() ? def positionChanged(self newPos): self.snappedPos = snapToGrid(newPos Y_OFFSET Y_STEP) I guess you are off by one pixel because of the accuracy problems during float division. Try changing your snapToGrid() to this: def snapToGrid(originalPos offset step): EPS = 1e-6 index = int((originalPos - offset) / step + EPS) #truncates the remainder away return index * gap + offset  Ok I'm answering my own question here as alexk mentioned using int to truncate was my mistake. The behaviour I'm after is best modeled by math.floor(). Apologies the original question does not contain enough information to really work out what the problem is. I didn't have the extra bit of information at that point. With regards to the typo note I think I may be using the context in a confusing manner... From the perspective of the positionChanged() function the parameter is a new position coming in. From the perspective of the snapToGrid() function the parameter is an original position which is being changed to a snapped position. The language is like that because part of it is in my event handling code and the other part is in my general services code. I should have changed it for the example thank mark your question answered Glad you fixed it. Just FYI you don't have to create a new post each time you have an update you can just edit your original question.
247,A,Fast work with Bitmaps in C# I need get all pixels from Bitmap work this they and save them to Bitmap. If I use Bitmap.GetPixel() and Bitmap.SetPixel() then I have very slow program. How I can fast convert Bitmap to byte[] and back? I need byte[] with size (4 * width * height) and contains RGBA of each pixel Following link contains bitmap pixel access methods comparatively. http://csharpexamples.com/fast-image-processing-c/ There is another way that is way faster and much more convenient. If you have a look at the Bitmap constructors you will find one that takes and IntPtr as the last parameter. That IntPtr is for holding pixel data. So how do you use it? Dim imageWidth As Integer = 1920 Dim imageHeight As Integer = 1080 Dim fmt As PixelFormat = PixelFormat.Format32bppRgb Dim pixelFormatSize As Integer = Image.GetPixelFormatSize(fmt) Dim stride As Integer = imageWidth * pixelFormatSize Dim padding = 32 - (stride Mod 32) If padding < 32 Then stride += padding Dim pixels((stride \ 32) * imageHeight) As Integer Dim handle As GCHandle = GCHandle.Alloc(pixels GCHandleType.Pinned) Dim addr As IntPtr = Marshal.UnsafeAddrOfPinnedArrayElement(pixels 0) Dim bitmap As New Bitmap(imageWidth imageHeight stride \ 8 fmt addr) What you have now is a simple Integer array and a Bitmap referencing the same memory. Any changes you make to the Integer array will be directly affecting the Bitmap. Let us try this with a simple brightness transform. Public Sub Brightness(ByRef pixels() As Integer ByVal scale As Single) Dim r g b As Integer Dim mult As Integer = CInt(1024.0f * scale) Dim pixel As Integer For i As Integer = 0 To pixels.Length - 1 pixel = pixels(i) r = pixel And 255 g = (pixel >> 8) And 255 b = (pixel >> 16) And 255 'brightness calculation 'shift right by 10 <=> divide by 1024 r = (r * mult) >> 10 g = (g * mult) >> 10 b = (b * mult) >> 10 'clamp to between 0 and 255 If r < 0 Then r = 0 If g < 0 Then g = 0 If b < 0 Then b = 0 r = (r And 255) g = (g And 255) b = (b And 255) pixels(i) = r Or (g << 8) Or (b << 16) Or &HFF000000 Next End Sub You may notice that I have used a little trick to avoid doing floating point math within the loop. This improves performance quite a bit. And when you are done you need to clean up a little of course... addr = IntPtr.Zero If handle.IsAllocated Then handle.Free() handle = Nothing End If bitmap.Dispose() bitmap = Nothing pixels = Nothing I have ignored the alpha component here but you are free to use that as well. I have thrown together a lot of bitmap editing tools this way. It is much faster and more reliable than Bitmap.LockBits() and best of all it requires zero memory copying to start editing your bitmap.  You can use Bitmap.LockBits method. Also if you want to use parallel task execution you can use the Parallel class in System.Threading.Tasks namespace. Following links have some samples and explanations. http://csharpexamples.com/fast-image-processing-c/ http://msdn.microsoft.com/en-us/library/dd460713%28v=vs.110%29.aspx http://msdn.microsoft.com/tr-tr/library/system.drawing.imaging.bitmapdata%28v=vs.110%29.aspx  You can do it a couple of different ways. You can use unsafe to get direct access to the data or you can use marshaling to copy the data back and forth. The unsafe code is faster but marshaling doesn't require unsafe code. Here's a performance comparison I did a while back. Here's a complete sample using lockbits: /*Note unsafe keyword*/ public unsafe Image ThresholdUA(float thresh) { Bitmap b = new Bitmap(_image);//note this has several overloads including a path to an image BitmapData bData = b.LockBits(new Rectangle(0 0 _image.Width _image.Height) ImageLockMode.ReadWrite b.PixelFormat); byte bitsPerPixel = GetBitsPerPixel(bData.PixelFormat); /*This time we convert the IntPtr to a ptr*/ byte* scan0 = (byte*)bData.Scan0.ToPointer(); for (int i = 0; i < bData.Height; ++i) { for (int j = 0; j < bData.Width; ++j) { byte* data = scan0 + i * bData.Stride + j * bitsPerPixel / 8; //data is a pointer to the first byte of the 3-byte color data } } b.UnlockBits(bData); return b; } Here's the same thing but with marshaling: /*No unsafe keyword!*/ public Image ThresholdMA(float thresh) { Bitmap b = new Bitmap(_image); BitmapData bData = b.LockBits(new Rectangle(0 0 _image.Width _image.Height) ImageLockMode.ReadWrite b.PixelFormat); /* GetBitsPerPixel just does a switch on the PixelFormat and returns the number */ byte bitsPerPixel = GetBitsPerPixel(bData.PixelFormat); /*the size of the image in bytes */ int size = bData.Stride * bData.Height; /*Allocate buffer for image*/ byte[] data = new byte[size]; /*This overload copies data of /size/ into /data/ from location specified (/Scan0/)*/ System.Runtime.InteropServices.Marshal.Copy(bData.Scan0 data 0 size); for (int i = 0; i < size; i += bitsPerPixel / 8 ) { double magnitude = 1/3d*(data[i] +data[i + 1] +data[i + 2]); //data[i] is the first of 3 bytes of color } /* This override copies the data back into the location specified */ System.Runtime.InteropServices.Marshal.Copy(data 0 bData.Scan0 data.Length); b.UnlockBits(bData); return b; } Thanks and I didn't even have to goad anybody. :) You should be aware when doing this that the order of the color channels may be different depending on what PixelFormat you're using. For example with 24bit Bitmaps the first byte of the pixel is the blue channel followed by green then red as opposed to the commonly expected Red-Green-Blue order. Thanks for you effort!  Building on @notJim answer (and with help from http://www.bobpowell.net/lockingbits.htm) I developed the following that makes my life a lot easier in that I end up with an array of arrays that allows me to jump to a pixel by it's x and y coordinates. Of course the x coordinate needs to be corrected for by the number of bytes per pixel but that is an easy extension. Dim bitmapData As Imaging.BitmapData = myBitmap.LockBits(New Rectangle(0 0 myBitmap.Width myBitmap.Height) Imaging.ImageLockMode.ReadOnly myBitmap.PixelFormat) Dim size As Integer = Math.Abs(bitmapData.Stride) * bitmapData.Height Dim data(size - 1) As Byte Marshal.Copy(bitmapData.Scan0 data 0 size) Dim pixelArray(myBitmap.Height)() As Byte 'we have to load all the opacity pixels into an array for later scanning by column 'the data comes in rows For y = myBitmap.Height - 1 To 0 Step -1 Dim rowArray(bitmapData.Stride) As Byte Array.Copy(data y * bitmapData.Stride rowArray 0 bitmapData.Stride) 'For x = myBitmap.Width - 1 To 0 Step -1 ' Dim i = (y * bitmapData.Stride) + (x * 4) ' Dim B = data(i) ' Dim G = data(i + 1) ' Dim R = data(i + 2) ' Dim A = data(i + 3) 'Next pixelArray(y) = rowArray Next  You want LockBits. You can then extract the bytes you want from the BitmapData object it gives you. I think it's faster if you use the `BitmapData` object returned from LockBits inside an `unsafe` block with a pointer cast (NOTE: I actually have no idea if this is faster or not but I'm trying to goad someone else into benchmarking it). It'd save you the copies into and out of the BitmapData which is nice. Not sure how much savings that would grant in practice though; memcpy() is really fast these days.
248,A,Drawing a hierarchical tree: treemapping I'm trying to develop a view of a hierarchical tree in which the weight of each node is the actual number of children it has. A leaf node has weight 1. I want to arrange these items in a way they can be browsed going deeper into the tree by showing the root categories (with no parent) at the beginning. Clicking on a node makes the view repaint iself to show just children of that node. The tricky part is that the size in pixel of a node should be proportional to its weight compared to adjacents nodes. According to wikipedia this is called treemapping and what I need is a tiling algorithm I was trying to figure out by myself but it seems more complex that I expected.. To give you an example there is a program for Mac Os X called GrandPerspective that shows folder sizes of your HD: I want to arrange nodes in a way like this! (of course size is proportional to folder size) Any suggestions? Thanks That's a squarified treemap. You can read the paper explaining this technique.  The data structure used in the file system example you show is most likely a KD tree. I'm not exactly sure how well the problem you want to solve maps to the file system example but this is how I would go along solving the file system case myself: You start with a rectangle representing the root of the hard disk. You take all files and directories in the directory and give them a size. For files the size is the size of the file For directories the size is the the complete size of all files it contains (including all its subfolders and their subfolders and so on). Now you try to cut this list into two as equally sized lists as possible. Now you cut the input rectangle into two rectangles that has the same size proportions as the two lists you cut the input files into. You should make the cut along the axis that is shorter of the input rectangles size to make sure you always have as quadratic as possible rectangles. Now you run the algorithm recursively on the two lists with their corresponding rectangle. The base cases would be: There is only a single file in the list. You then fill the rectangle with the color of the file type. There is a single directory in the list. For this case you run the algorithm recursively on the contents in the directory inside the rectangle. Choosing how to split the lists into two as equally sized parts as possible may not be trivial (is it a knapsack?). A decent heuristic approach would probably be to sort the list in descending order and take the elements out of the list and put it in the currently smallest of the two resulting lists. EDIT: The splitting problem is called partition and is a special case of knapsack. It's covered in this thread here on SO. Ok maybe the screenshot I put it's misleading because I don't want to use it for files but for other hierarchical data structured approximately in the same way. Yes I think it's a sort of knapsack problem that's why I was wondering which kind of algorithm I should choose this divide et impera approach can work.. I'll try and let you know (and accept it if it seems reasonable :) thanks! Added some info about the list splitting part. Turns out that it's NP complete but there are good ways of solving it using heuristics for your instance. Just tried the greedy approach and it seems to work quite well thanks!
249,A,Drawing text on a framebuffer in Linux from C How can a program draw text on a frame buffer mapped in as an array? What is needed is both a means of representing the individual characters and of drawing the characters pixel by pixel in a manner that is not too inefficient. The representation of the characters should ideally be defined solely in code and no third party libraries would be required. Does anyone know of code to do this available under a liberal license? Or a tool to generate data definitions for the font for use in program code e.g. the array of bitmap glyph/character values? Is the intent to display the result on a screen or write it to a file? To display on screen ideally up to 60fps I don't have any information specific to frame buffers but I do have an interesting way of encoding a font. If you have an application that can write to the XBM format you can encode a font just by creating an image containing all the characters. The XBM file can be included as a C or C++ file and by using the proper offsets you can easily access a single character. Make sure each character starts at an X-coordinate divisible by 8 because the image is coded as one bit per pixel; anything that doesn't line up on an 8-bit boundary will need masking and shifting.  I think the best way to do it is using bitmap fonts: http://www.iua.upf.es/~ggeiger/redbookhtml/ch09.html. This tutorial is for OpenGL but you probably find a lot of useful info.  To draw a line on a 2D array use the Besengam's algorithm. To draw the characters using straight lines  build your alphabet using a series of moveTo lineTo. E.g. for a simple 'L': image.moveTo(0-fontHeight); image.lineTo(0 0); image.lineTo(fontWidth0);
250,A,Writing Name Using Bezier Curves In C# I have to make a program that uses C# Generated Graphics to make a replica of my name that I wrote in cursive. Twist is I have to use Bezier Curves. I've already called a function to make Bezier Curves using 4 points and a gravity concept. My question to you is What would be the easiest way to make around 10 curves. Here is my function for a Bezier Curve. public static void bezierCurve( Graphics g double p1x double p1y double p2x double p2y double p3x double p3y double p4x double p4y) { double t r1x r4x r1y r4y; float x y; Pen black = new Pen(Color.Black); r1x = 3 * (p2x - p1x); r4x = 3 * (p4x - p3x); r1y = 3 * (p2y - p1y); r4y = 3 * (p4y - p3y); t = 0; while (t <= 1) { x = (float) ((2 * Math.Pow(t 3) - 3 * Math.Pow(t 2) + 1) * p1x + (-2 * Math.Pow(t 3) + 3 * Math.Pow(t 2)) * p4x + (Math.Pow(t 3) - 2 * Math.Pow(t 2) + t) * r1x + (Math.Pow(t 3) - Math.Pow(t 2)) * r4x); y = (float) ((2 * Math.Pow(t 3) - 3 * Math.Pow(t 2) + 1) * p1y + (-2 * Math.Pow(t 3) + 3 * Math.Pow(t 2)) * p1y + (Math.Pow(t 3) - 2 * Math.Pow(t 2) + t) * r1y + (Math.Pow(t 3) - Math.Pow(t 2)) * r4y); g.DrawRectangle(black x y 1 1); t = t + 0.01; } } Thanks for fixing that for me Drew. It wasn't coding properly for some reason. I would suggest taking some vector editing software e.g. InkScape or Corel draw your name with beziers using that software then save as .SVG. The SVG format is easy to understand here is an example of encoding a bezier path. Copy the coordinates from the path into your program. Alternatively use a piece of graph paper to get the coordinates by hand. C# already has a function for drawing Beziers see Graphics.DrawBezier that is going to be much more efficient (and producing better-looking results) than your implementation.
251,A,"Writing BMP data getting garbage I'm working on understanding and drawing my own DLL for PDF417 (2d barcodes). Anyhow the actual drawing of the file is perfect and in correct boundaries of 32 bits (as monochrome result). At the time of writing the data the following is a memory dump as copied from C++ Visual Studio memory dump of the pointer to the bmp buffer. Each row is properly allocated to 36 wide before the next row. Sorry about the wordwrap in the post but my output was intended to be the same 36 bytes wide as the memory dump so you could better see the distortion. The current drawing is 273 pixels wide by 12 pixels high monochrome... 00 ab a8 61 d7 18 ed 18 f7 a3 89 1c dd 70 86 f5 f7 1a 20 91 3b c9 27 e7 67 12 1c 68 ae 3c b7 3e 02 eb 00 00 00 ab a8 61 d7 18 ed 18 f7 a3 89 1c dd 70 86 f5 f7 1a 20 91 3b c9 27 e7 67 12 1c 68 ae 3c b7 3e 02 eb 00 00 00 ab a8 61 d7 18 ed 18 f7 a3 89 1c dd 70 86 f5 f7 1a 20 91 3b c9 27 e7 67 12 1c 68 ae 3c b7 3e 02 eb 00 00 00 ab 81 4b ca 07 6b 9c 11 40 9a e6 0c 76 0a fc a3 33 70 bb 30 55 87 e9 c4 10 58 d9 ea 0d 48 3e 02 eb 00 00 00 ab 81 4b ca 07 6b 9c 11 40 9a e6 0c 76 0a fc a3 33 70 bb 30 55 87 e9 c4 10 58 d9 ea 0d 48 3e 02 eb 00 00 00 ab 81 4b ca 07 6b 9c 11 40 9a e6 0c 76 0a fc a3 33 70 bb 30 55 87 e9 c4 10 58 d9 ea 0d 48 3e 02 eb 00 00 00 ab 85 7e d0 29 e8 14 f4 0a 7a 05 3c 37 ba 86 87 04 db b6 09 dc a0 62 fc d1 31 79 bc 5c 0a 8e 02 eb 00 00 00 ab 85 7e d0 29 e8 14 f4 0a 7a 05 3c 37 ba 86 87 04 db b6 09 dc a0 62 fc d1 31 79 bc 5c 0a 8e 02 eb 00 00 00 ab 85 7e d0 29 e8 14 f4 0a 7a 05 3c 37 ba 86 87 04 db b6 09 dc a0 62 fc d1 31 79 bc 5c 0a 8e 02 eb 00 00 00 ab 85 43 c5 30 e2 26 70 4a 1a f3 e4 4d ce 2a 3f 79 cd bc e6 de 73 6f 39 b7 9c db ce 6d 5f be 02 eb 00 00 00 ab 85 43 c5 30 e2 26 70 4a 1a f3 e4 4d ce 2a 3f 79 cd bc e6 de 73 6f 39 b7 9c db ce 6d 5f be 02 eb 00 00 00 ab 85 43 c5 30 e2 26 70 4a 1a f3 e4 4d ce 2a 3f 79 cd bc e6 de 73 6f 39 b7 9c db ce 6d 5f be 02 eb 00 00 Here is the code to WRITE the file out -- verbatim immediately at the time of the memory dump from above FILE *stream; if( fopen_s( &stream cSaveToFile ""w+"" ) == 0 ) { fwrite( &bmfh 1 (UINT)sizeof(BITMAPFILEHEADER) stream ); fwrite( &bmi 1 (UINT)sizeof(BITMAPINFO) stream ); fwrite( &RGBWhite 1 (UINT)sizeof(RGBQUAD) stream ); fwrite( ppvBits 1 (UINT)bmi.bmiHeader.biSizeImage stream ); fclose( stream ); } Here's what ACTUALLY Gets written to the file. 00 ab a8 61 d7 18 ed 18 f7 a3 89 1c dd 70 86 f5 f7 1a 20 91 3b c9 27 e7 67 12 1c 68 ae 3c b7 3e 02 eb 00 00 00 ab a8 61 d7 18 ed 18 f7 a3 89 1c dd 70 86 f5 f7 1a 20 91 3b c9 27 e7 67 12 1c 68 ae 3c b7 3e 02 eb 00 00 00 ab a8 61 d7 18 ed 18 f7 a3 89 1c dd 70 86 f5 f7 1a 20 91 3b c9 27 e7 67 12 1c 68 ae 3c b7 3e 02 eb 00 00 00 ab 81 4b ca 07 6b 9c 11 40 9a e6 0c 76 0d 0a fc a3 33 70 bb 30 55 87 e9 c4 10 58 d9 ea 0d 48 3e 02 eb 00 00 00 ab 81 4b ca 07 6b 9c 11 40 9a e6 0c 76 0d 0a fc a3 33 70 bb 30 55 87 e9 c4 10 58 d9 ea 0d 48 3e 02 eb 00 00 00 ab 81 4b ca 07 6b 9c 11 40 9a e6 0c 76 0d 0a fc a3 33 70 bb 30 55 87 e9 c4 10 58 d9 ea 0d 48 3e 02 eb 00 00 00 ab 85 7e d0 29 e8 14 f4 0d 0a 7a 05 3c 37 ba 86 87 04 db b6 09 dc a0 62 fc d1 31 79 bc 5c 0d 0a 8e 02 eb 00 00 00 ab 85 7e d0 29 e8 14 f4 0d 0a 7a 05 3c 37 ba 86 87 04 db b6 09 dc a0 62 fc d1 31 79 bc 5c 0d 0a 8e 02 eb 00 00 00 ab 85 7e d0 29 e8 14 f4 0d 0a 7a 05 3c 37 ba 86 87 04 db b6 09 dc a0 62 fc d1 31 79 bc 5c 0d 0a 8e 02 eb 00 00 00 ab 85 43 c5 30 e2 26 70 4a 1a f3 e4 4d ce 2a 3f 79 cd bc e6 de 73 6f 39 b7 9c db ce 6d 5f be 02 eb 00 00 00 ab 85 43 c5 30 e2 26 70 4a 1a f3 e4 4d ce 2a 3f 79 cd bc e6 de 73 6f 39 b7 9c db ce 6d 5f be 02 eb 00 00 00 ab 85 43 c5 30 e2 26 70 4a 1a f3 e4 4d ce 2a 3f 79 cd bc e6 de 73 6f 39 b7 9c db ce 6d 5f be 02 eb 00 00 Notice the start of the distortion with the ""0d"" in the result from reading the file back in the 4th line about the 15th byte over... Then there are a few more staggered around which in total skew the image off by 9 bytes worth... Obviously the drawing portion is working ok as everything remains properly aligned in memory for the 12 lines. Your 0x0A (\n) gets converted to DOS format 0x0D0A (\r\n) becouse you're write the file in text mode. Switch to binary mode.  Shouldn't you open the file in a compound mode i.e. writable & binary as in wb+? Notice the start of the distortion with the ""0d"" That's ASCII code for Carriage Return (CR) -- added on some OSes with newline (where a newline is actually a sequence of CR/LF). This should go away once you start writing the output in binary mode. Your code looks neat otherwise. Cheers! THANK YOU SO MUCH!!! The rest makes it ALL perfect...  I actually just did a similar thing in java (printing bmp data to a thermal receipt printer). There are a couple of things i want to share with you: bmp image data != an image format from microsoft. the MS bitmap has about 54 bytes of header information before any image data. (i spent a day or two working on this before I realized the difference) bmp image data reads left to right top to bottom with the most significant bit on the left. make sure the barcode image has a bitdepth of 1. this means 1 bit = 1 pixel. hexidecimal ""ab"" is 10101011 in binary those 8 pixels will be filled in accordingly. if you have a barcode 36 bytes wide the barcode resolution is 288 x 12 not 273 x 12. (36 * 8 = 288). the image data should be 432 bytes in size (12 rows of 36 bytes). i dont know what this means: Anyhow the actual drawing of the file is perfect and in correct boundaries of 32 bits (as monochrome result). monochrome means its either 1 color or another. the pixel (think bit) is either filled in or it isnt. Hope this helps"
252,A,"why doesn't this simple C# tryout work this creates streaks instead of dots. why ? I am trying to paint individual pixels. another method was also tried (using fillrectangle)  which also didn't give the desired result got bars instead of dots. protected override void OnPaint(PaintEventArgs pea ) { Graphics g = pea.Graphics ; for ( int y = 1 ; y <= Width ; y++ ) { for ( int x = 1 ; x <= Height ; x++ ) { System.Random rand = new System.Random() ; Color c = (Color.FromArgb(rand.Next(256) rand.Next(256) rand.Next(256))); // SolidBrush myBrush = new SolidBrush(c); // g.FillRectangle(myBrush x y 1 1); Bitmap pt = new Bitmap(1 1); pt.SetPixel(0 0 c); g.DrawImageUnscaled(pt x y); } } } what is happening here ? Not that it answers your question. But confusing that you would use X to iterate over the vertical position and y over the horizontal To continue on that thought: since you are using x and y correctly in the DrawImageUnscaled call but are using them incorrectly in the loop I suspect not the whole of your picture will be filled nitpick3: I can not imagine that DarImageUnscaled would use it's coordinate system starting with 1. So your loops should really start with 0 i was just trying something different. You need to create the System.Random object outside the loops. When initialized inside the loop it keep getting the same seed (since it's initialized according to the system clock which would still have the same value) and therefore the same color.  Yeah what they said. and you might get better mileage bitblitting something like that. e.g. draw it offscreen and then paint it all at once. protected override void OnPaint(PaintEventArgs pea) { using (var b = new Bitmap(this.Width this.Height)) { using (var g = Graphics.FromImage(b)) { var rand = new Random(); for (int x = 0; x <= Width; x++) { for (int y = 0; y <= Height; y++) { Color c = (Color.FromArgb(rand.Next(256) rand.Next(256) rand.Next(256))); using (var newPen = new Pen(c 1)) { g.DrawRectangle(newPen x y 0.5f 0.5f); } } } } pea.Graphics.DrawImage(b 0 0); } } protected override void OnPaint(PaintEventArgs pea) { Graphics g = pea.Graphics; var rand = new Random(); for (int x = 0; x <= Width; x++) { for (int y = 0; y <= Height; y++) { Color c = (Color.FromArgb(rand.Next(256) rand.Next(256) rand.Next(256))); using (var newPen = new Pen(c 1)) { g.DrawRectangle(newPen x y 0.5f 0.5f); } } } }y look at my comments under the question. There are still incorrect things here. just to make you happy and annoy anyone looking at the code.... just change it in the draw. dyslexia anyone? ;^) Shouldn't the loops start at 0 though? Hey it's not my mess I just fix the really broken stuff and don't sweat the small stuff. ok... it will become CW very shortly.  You need to move the System.Radom outside of the loop. What is happening here is that you are creating the same random point every time the loop iterates. Also if you add using System; to the start of the program than you can simply say Random (but that is not important just a side note). Once the Random is created you can then get the next number by simply calling Next on the object. Try this out and I believe it will rid you of your problems!  You are drawing a different coloured dot on every pixel in the rectangle 00WidthHeight. But as the previous answers state because random is being intialised to the system clock within each loop the loop is drawing the same colour giving you a streak of the same colour (Horizontally?) until the system clock ticks on a tiny bit and gives you a different seed. If you move the System.Random outside the loop so you get a different colour each Next then you will see dots.  You should not create a new Random object each time. Since that will give you the exact same ""random"" numbers repeated over and over. Instead use the same Random object and just call Next on in. thanks a lot! . but is it me or is getting a the same 'random' everytime a little illogical Random numbers are generated from a starting value called the ""seed"". If you use the same seed every time you will also get the exact sequence of random numbers each time. Quite possibly you are using the same seed here (or it's using the system clock which doesn't have time to change much inside the quick loop). http://msdn.microsoft.com/en-us/library/system.random.aspx hmmm thanks for explaining!"
253,A,"Which algorithm does Neatimage use to denoise images? Which kind of algorithm does Neatimage use to remove noise and grain from photos? I understand that this is proprietary software but probably someone has an idea. References to publications or to similar algorithms are welcome. At its most basic noise reduction normally uses pixel averaging. The problem of course is that simple averaging loses detail. Averaging more pixels reduces noise more but loses more detail. Averaging fewer pixels loses less detail but reduces the noise less. Something like NeatImage or Noise Ninja will do its pixel averaging adaptively -- for example it'll start with a scan for changes that occur over enough pixels that they're unlikely to be noise and where it sees those do the averaging over fewer pixels. They will also take the channels of the picture into account. A normal digital camera has a filter in front of each sensel. The normal arrangement is something like g-r-g-b (aka a Bayer pattern). In a typical case the green filter transmits more light than the red or (especially) the blue. To maintain the color balance in the final picture the brightness of the blues in the picture has to be ""boosted"" to compensate. This however tends to increase the noise in the blue channel. To compensate for that the noise reducer will normally do rather minimal averaging in the green channel somewhat more in the red channel and more still in the blue channel. An advanced noise reducer will normally start with a model of the noise for an individual sensor and apply the noise reduction based on that model. IIRC NeatImage also allows you to take ""dark frames"" (e.g. a 30 second exposure with the lens cap on) to get a better map of the exact noise characteristics of your exact sensor and take that into account (I know Noise Ninja allows that and if memory serves NeatImage does as well). Normally for this to work its best you want to start with something like five dark frames. You statistically analyze those to find 1) which pixels are consistently bright or dark (""stuck pixels"") and 2) any consistent patterns you can find in the noise so you can eliminate those directly (e.g. the part of the sensor near the processing may get warmer and therefore noisier than other parts) and 3) the type and degree of variation to expect from noise even where there isn't really a pattern (e.g. some sensors show luminance noise others mostly chrominance noise). Thank you Jerry. I supposed it works in Lab or similar colorspace internally (it has separate settings for chroma and luminance noise). I've also seen that Neatimage uses a patch of the image to evaluate noise params. It is still unclear how it uses this info but the result is pretty good.  I think all the three major (Noiseware Neat Image Noise Ninja) apply some kind of Wavelets Denoising. The reason is simple all non local methods are too slow to implement. Though DXO's Raw Converter uses Non Local Means. You could easily find some articles on that (And come back to point us for the best you found)... Actually they pretend they use something else (http://www.neatimage.com/overview.html). But don't specify. I hoped someone more knowledgeable in NR may guess. Yeah. That's marketting speak for ""We've tuned the wavelets in a way that works well for our use case and maybe thrown a few other filters on top."" They say better than ""Classic"" so they might use ""Advanced"" wavelets techniques. Noise Ninja declares it uses ""Second Generation"" Wavelets. I would still bet on Wavelets. It's still pretty vague. I'd like to know more technical details.  This paper looks promising: http://research.microsoft.com/~larryz/04359321.pdf It discusses NeatImage's algorithm briefly and would be a good place to start. Thank you Leonard. This paper seems indeed very useful. Thanks!"
254,A,"How do I calculate a four colour gradient? If I have four colours (A B C & D) on four corners of a square and I want to fill that square with a gradient that blends nicely between the four colours how would I calculate the colour of the point E? The closer E is to any of the other points the strong that colour should affect the result. Any idea how to do that? Speed and simplicity is preferred to accuracy. Check out this site which gives a visual demo of @ThibThib's comment that ""gradients in HSV will be more satifying"": http://www.perbang.dk/rgbgradient/ It is a gradient creator that will create and show BOTH an RGB gradient and an HSV gradient. If you try 9 steps from FFAAAA to AAFFAA (light red to green) you’ll get a nice transition through light yellow and the HSV and RGB ones look similar. But try 9 steps from FF0000 to 00FF00 (bold red to green) and you’ll see the RGB one transition through a yucky greenish brown. The HSV gradient however transitions through bold yellow.  Determine the distance of point E to each point ABCD The color for point E will be the combination of Red / Green / Blue. Calculate each color axis as the average of the same color axis for ABCD ponderating by distance. distance_a = sqrt((xa-xe)^2+(ya-ye)^2) distance_b = .... sum_distances = distance_a + distance_b ... red = (red_a*distance_a + red_b*distance_b ... ) / sum_distances color_E = ColorFromARgb(redgreenblue) Something is wrong with this approach I fear. The ""weight"" for how much ""A"" contributes to the color for ""E"" should be proportional to the *inverse* of the distance. As written above if E is right at A then the distance to A is 0 and therefore A will contribute *nothing* to the value of E. But that is the very case where ONLY A should contribute to E.  The best solution when a gradient is required between two colors is to use the HSV representation (Hue Saturation Value). If you have the HSV values for your two colors you just make linear interpolation for H S and V and you have nice colors (interpolation in RGB space always lead to ""bad"" results). You also find here the formulae to go from RGB to HSV and from HSV to RGB respectively. Now for your problem with the four corner you can make a linear combination of the four H/S/V values weighted by the distance from E to that four points ABC and D. EDIT: same method than tekBlues but in HSV space (it is quite easy to test it in RGB and in HSV spaces. And you will see the differences. In HSV you just turn around the chromatic cylinder and this is why it gives nice result) EDIT2: if you prefer ""speed and simplicity"" you may use a L1-norm instead of a L2-norm (euclidian norm) So if a is the size of your square and the coordinate of your points are A(00) B(0a) C(a0) D(aa) then the Hue of a point E(xy) can be computed with: Hue(E) = ( Hue(B)*y/a + Hue(A)*(1-y/a) ) * (x/a) + ( Hue(D)*y/a + Hue(C)*(1-y/a) ) * (1-x/a) where Hue(A) is the Hue of point A Hue(B) the Hue of B etc... You apply the same formulae for the Saturation and Value. Once you have the Hue/Saturation/Value for your point E you can transform it in RGB space. I got it to work thank you very much. Looks pretty good too! The combination method above is also known as ""bi-linear interpolation"". You are performing this separately on H S and V values."
255,A,"Is a closed polygonal mesh flipped? I have a 3d modeling application. Right now I'm drawing the meshes double-sided but I'd like to switch to single sided when the object is closed. If the polygonal mesh is closed (no boundary edges/completely periodic) it seems like I should always be able to determine if the object is currently flipped and automatically correct. Being flipped means that my normals point into the object instead of out of the object. Being flipped is a result of a mismatch between my winding rules and the current frontface setting but I compute the normals directly from the geometry so looking at the normals is a simple way to detect it. One thing I was thinking was to take the bounding box find the highest point and see if its normal points up or down - if it's down then the object is flipped. But it seems like this solution might be prone to errors with degenerate geometry or floating point error as I'd only be looking at a single point. I guess I could get all 6 axis-aligned extents but that seems like a slightly better kludge and not a proper solution. Is there a robust simple way to do this? Robust and hard would also work.. :) I think you should explain better what you mean with 'flipped' i.e. is that flipped over x y or z? I think he means that if the normals are pointing inward (since they should be pointing outward). You're writing the app so can you introduce the convention that all triangles obey the ""right hand rule""? i.e.: if you were to curl your right hand around the triangle from point 0 to point 2 the normal would point in the direction of your thumb. If you create your objects with this convention in place perhaps you can avoid the test for inversion altogether. As a general modeling app it's tricky to prohibit users from doing the wrong thing. With an open surface I don't think there's a correct answer to whether it's flipped or not both are equally correct. And users can go from an open to a closed surface easily so I think the best I can do is fix it then. ""I'm drawing the meshes double-sided"" Why are you doing that? If you're using OpenGL then there is a much better way to go and save yourself all the work. use: glLightModeli(GL_LIGHT_MODEL_TWO_SIDE 1); With this all the polygons are always two sided. The only reason why you would want to use one-sided lighting is if you have an open or partially inverted mesh and you want to somehow indicate what part belong to the inside by having them unlighted. Generally the problem you're posing is an open problem in geometry processing and AFAIK there is no sure-fire general way that can always determine the orientation. As you suggest there are heuristics that work almost always. Another approach is reminiscent of a famous point-in-polygon algorithm: choose a vertex on the mesh and shoot a ray from it in the direction of the normal. If the ray hits an even number of faces then the normal is pointed to the outside if it is odd then the normal is towards to inside. notice not to count the point of origin as an intersection. This approach will work only if the mesh is a closed manifold and it can reach some edge cases if the ray happens to pass exactly between two polygons so you might want to do it a number of times and take the leading vote. Thanks for this answer. To clarify I'm writing a new geometry type for an existing graphics pipeline. I just provide tessellation meshes with a few drawing options. In this case I'm trying to match the behavior of other geometry types which draw single-sided for closed objects to allow you to see out of them when the camera is inside. It still draws isoparameter lines so you can tell where you are but getting my winding backwards looks really bad. Turning on two sided lighting has performance implications in OpenGL though. If there's a way to avoid the need it can be beneficial. There's also implications if you're going to be using shaders vs. the FF pipeline - handling 2 sided lighting in shaders is much less fun.  This is a robust but slow way to get there: Take a corner of a bounding box offset from the centroid (force it to be guaranteed outside your closed polygonal mesh) then create a line segment from that to the center point of any triangle on your mesh. Measure the angle between that line segment and the normal of the triangle. Intersect that line segment with each triangle face of your mesh (including the tri you used to generate the segment). If there are an odd number of intersections the angle between the normal and the line segment should be <180. If there are an even number it should be >180. If there are an even number of intersections the numbers should be reversed. This should work for very complex surfaces but they must be closed or it breaks down."
256,A,"Qt jpg image display I want to display .jpg image in an Qt UI. I checked it online and found http://qt-project.org/doc/qt-4.8/widgets-imageviewer.html. I thought Graphics View will do the same and also it has codec to display video. How to display images using Graphics View? I went through the libraries but because I am a totally newbie in Qt I can't find a clue to start with. Can you direct me to some resources/examples on how to load and display images in Qt? Thanks. If the only thing you want to do is drop in an image onto a widget withouth the complexity of the graphics API you can also just create a new QWidget and set the background with StyleSheets. Something like this:  MainWindow::MainWindow(QWidget *parent) : QMainWindow(parent) { ... QWidget *pic = new QWidget(this); pic->setStyleSheet(""background-image: url(test.png)""); pic->setGeometry(QRect(5050128128)); ... }  #include ... int main(int argc char *argv[]) { QApplication a(argc argv); QGraphicsScene scene; QGraphicsView view(&scene); QGraphicsPixmapItem item(QPixmap(""c:\\test.png"")); scene.addItem(&item); view.show(); return a.exec(); } This should work. :) List of supported formats can be found here how can we use this with UI created in QT Creator? Please note that for Qt 4.6 this code has an error. Try this one: int main(int argc char *argv[]) { QString fileName(""C:/aaa..gif""); QApplication a(argc argv); QGraphicsScene scene; scene.addPixmap(QPixmap(fileName)); QGraphicsView view(&scene); view.show(); return a.exec(); } The code from this post worked perfectly with Qt 4.7.0 How can you add a GraphicsView to a layout object?  I want to display .jpg image in an Qt UI The simpliest way is to use QLabel for this: int main(int argc char *argv[]) { QApplication a(argc argv); QLabel label(""<img src='image.jpg' />""); label.show(); return a.exec(); }  You could attach the image (as a pixmap) to a label then add that to your layout... ... QPixmap image = new QPixmap(""blah.jpg""); QLabel imageLabel = new QLabel(); imageLabel.setPixmap(image); mainLayout.addWidget(imageLabel); ... Apologies this is using Jambi (Qt for Java) so the syntax is different but the theory is the same.  I understand your frustration the "" Graphics view widget"" is not the best way to do this yes it can be done but it's almost exactly the same as using a label ( for what you want any way) now all the ways listed do work but... For you and any one else that may come across this question he easiest way to do it ( what you're asking any way ) is this. QPixmap pix(""Path\\path\\entername.jpeg""); ui->label->setPixmap(pix); } This is not complete it can be very confusing for someone who doesn't know to play around with images."
257,A,"What events can I use in order to notify a control that part of it has left the viewable area of the screen? I have a userControl I'm doing some Painting to and when the control is moved to the edge of the screen or moved so that the Vista Taskbar overlaps it the screen edge/taskbar edge is being drawn to the control leaving ugly lines over the paintable area of the control. What is the best way to detect this and call Invalidate on the control? The ""Moved"" and ""LocationChanged"" events apparently deal with the movement of the control within its parent container not with a change in screen location. I believe the same thing will occur when the control is overlapped by another window in Windows XP but I haven't tested that yet. Same question applies under that circumstance. I am currently drawing directly to the control's Graphics object which I believe is the root of the problem. Would it be better to draw to the Control's BackGroundImage as an attempt to sidestep this issue? Note: This is a significant rephrasing of an earlier question which was phrased very badly. I felt that starting a new question was the thing to do rather than try to repair the nonsense I had written. EDIT: It turns out that the taskbar is not causing the problem just the bottom screen edge. I had thought that the taskbar was causing problems as well but it looks like I was wrong I'll copy the comment from your first question: how are you doing your painting and why does the taskbar present a problem? Normally your control shouldn't care if some other window goes on top of it. when the control is moved to the edge of the screen or moved so that the Vista Taskbar overlaps it the screen edge/taskbar edge is being drawn to the control leaving ugly lines over the paintable area of the control. I am doing my painting in the OnPaint method. When other forms pass over my control there is no visible problem at least in Vista but when the control reaches a screen edge or the taskbar the screen edge is drawing on my control. I'll work on getting some screenshots. Yes you can draw control's background to give illusion problem free drawing. Are you trying to draw on the clipped area (Clip Rectangle) of the graphics object or using whole surface of it? I think whenever another window or object hides an other object windows sends it repaint message and with that it passes the area that needs to be redrawn (clip rectangle).  While making screenshots of the error occurring it became obvious that the problem was that I was drawing an area described by the ClipRectangle of the Paint event and not by the ClipRectangle of the control itself. The ClipRectangle of the Paint event describes the area that is being revealed by a single Movement event it does not describe the area of the bounds of the Control. I simply happened to move the control fast enough that it became obvious that the entire control was being drawn into the revealed space. I altered the code to draw to an area corresponding to the size of the control and everything worked fine. Another PEBKAC question. Oh well at least the error was found. Reread trailblazer's answer I realised it's actually correct. I had misread ""clip rectangle"" as ""client rectangle""."
258,A,Where can I get graphics for startup screens? I really like to have some graphic on my startup/login screen when starting my applications. In one of my past employments we had this on startup/login screen: Image missing: Head bangs on computer Where do you get your graphics for this purpose and what are your favorites. We have an official logo for our core products and in release that's what we use for the splash screen. During development we put funny pictures that we find from around the internet into a folder on a shared network drive. The debug build will randomly pick a picture from that folder if it can access it and use that as the splash screen. Some favs: Seriously though whatever you end up going with for your official splash screen make sure you don't violate copyright etc etc...  I'm a big fan of using fotolia.com. For a couple of dollars you get licensed professional quality images and graphics. Another good source for icons such as above is iconfinder.net. There's tons of free for-business icons there. +1 Good suggestions  You should get a graphic designer to create a splash screen for your product. You should be careful not to use others' copyrighted imagery in your software unless you know it is licensed under a non-restrictive license like one of the Creative Commons licenses. Make sure you understand and comply with whatever license your imagery has. Even for internal you should pay for what you use. Those are someone's intellectual property. @durilai: You know you're right. You're probably not supposed to use copyrighted works for any commercial purpose even internally. I'll edit to fix that.  Legally you can use a www.istockphoto.com  For some reason I recently found this and am finding it very uplifting... http://www.istockphoto.com/file_thumbview_approve/1132735/2/istockphoto_1132735-running-man.swf What are you running from? I guess that comes from upcoming spring and my wish to resume jogging sessions after long winter sleep :)
259,A,"Terrain minimap in OpenGL? So I have what is essentially a game... There is terrain in this game. I'd like to be able to create a top-down view minimap so that the ""player"" can see where they are going. I'm doing some shading etc on the terrain so I'd like that to show up in the minimap as well. It seems like I just need to create a second camera and somehow get that camera's display to show up in a specific box. I'm also thinking something like a mirror would work. I'm looking for approaches that I could take that would essentially give me the same view I currently have just top down... Does this seem feasible? Feel free to ask questions... Thanks! One way to do this is to create an FBO (frame buffer object) with a render buffer attached render your minimap to it and then bind the FBO to a texture. You can then map the texture to anything you'd like generally a quad. You can do this for all sorts of HUD objects. This also means that you don't have to redraw the contents of your HUD/menu objects as often as your main view; update the the associated buffer only as often as you require. You will often want to downsample (in the polygon count sense) the objects/scene you are rendering to the FBO for this case. The functions in the API you'll want to check into are: glGenFramebuffersEXT glBindFramebufferEXT glGenRenderbuffersEXT glBindRenderbufferEXT glRenderbufferStorageEXT glFrambufferRenderbufferEXT glFrambufferTexture2DEXT glGenerateMipmapEXT There is a write-up on using FBOs on gamedev.net. Another potential optimization is that if the contents of the minimap are static and you are simply moving a camera over this static view (truly just a map). You can render a portion of the map that is much larger than what you actually want to display to the player and fake a camera by adjusting the texture coordinates of the object it's mapped onto. This only works if your minimap is in orthographic projection. Would using multiple viewports also work? Can I render polygons to the frame buffer? Obviously I'm pretty noob to graphics programming haha. Yes you can draw to an FBO in exactly the same way that you draw normally in your window's frame buffer (the same as rendering to the back buffer before swapping with the front buffer). You can use different gluPerspective/gluLookAt's in each FBO. The benefit is that you don't have to redraw in the FBO if you don't need to for the current frame the FBO can be of any dimensions and it's very fast when used for textures as you don't need to use a glCopyTexImage. You can create many FBOs and switch between them with glBindFramebufferEXT (0 binds the window's framebuffer that your used to).  Well I don't have an answer to your specific question but it's common in games to render the world to an image using an orthogonal perspective from above and use that for the minimap. It would at least be less performance intensive than rendering it on the fly."
260,A,What is the best and easy to use graphics library available for C#? I need an open-source based graphics library to generate jpg graphics from database such as data from MS SQL server. I guess where the data coming from does not matter. The graphics will be something like trend for curve bar pie... What I found are couple of libraries available such as NPLOT and ZedGraph Before I roll my leef on those just want to get guru's opinions on this issue. Any other libraries better than these two? The Microsoft Chart Controls for Microsoft .NET Framework 3.5 are free but not open source. The data binding support appears to be quite comprehensive and charts look quite nice as well. My only experience with ZedGraph was from a couple of years back. It didn't play nice in our partial trust ASP.NET environments so perhaps that's something to consider if you're attracted by it. Kev quick look at the page. It looks cool. Does it allow saving image as jpg? I need to run it as console app to auto generate jpg for html. The Chart class has a SaveImage method and you can save to file/stream and a wide variety of formats including jpeg gif png.
261,A,"Greatest linear dimension 2d set of points Given an ordered set of 2D pixel locations (adjacent or adjacent-diagonal) that form a complete path with no repeats how do I determine the Greatest Linear Dimension of the polygon whose perimeter is that set of pixels? (where the GLD is the greatest linear distance of any pair of points in the set) For my purposes the obvious O(n^2) solution is probably not fast enough for figures of thousands of points. Are there good heuristics or lookup methods that bring the time complexity nearer to O(n) or O(log(n))? I ported the Python code to C#. It seems to work. using System; using System.Collections.Generic; using System.Drawing; // Based on code here: // http://code.activestate.com/recipes/117225/ // Jared Updike ported it to C# 3 December 2008 public class Convexhull { // given a polygon formed by pts return the subset of those points // that form the convex hull of the polygon // for integer Point structs not float/PointF public static Point[] ConvexHull(Point[] pts) { PointF[] mpts = FromPoints(pts); PointF[] result = ConvexHull(mpts); int n = result.Length; Point[] ret = new Point[n]; for (int i = 0; i < n; i++) ret[i] = new Point((int)result[i].X (int)result[i].Y); return ret; } // given a polygon formed by pts return the subset of those points // that form the convex hull of the polygon public static PointF[] ConvexHull(PointF[] pts) { PointF[][] l_u = ConvexHull_LU(pts); PointF[] lower = l_u[0]; PointF[] upper = l_u[1]; // Join the lower and upper hull int nl = lower.Length; int nu = upper.Length; PointF[] result = new PointF[nl + nu]; for (int i = 0; i < nl; i++) result[i] = lower[i]; for (int i = 0; i < nu; i++) result[i + nl] = upper[i]; return result; } // returns the two points that form the diameter of the polygon formed by points pts // takes and returns integer Point structs not PointF public static Point[] Diameter(Point[] pts) { PointF[] fpts = FromPoints(pts); PointF[] maxPair = Diameter(fpts); return new Point[] { new Point((int)maxPair[0].X (int)maxPair[0].Y) new Point((int)maxPair[1].X (int)maxPair[1].Y) }; } // returns the two points that form the diameter of the polygon formed by points pts public static PointF[] Diameter(PointF[] pts) { IEnumerable<Pair> pairs = RotatingCalipers(pts); double max2 = Double.NegativeInfinity; Pair maxPair = null; foreach (Pair pair in pairs) { PointF p = pair.a; PointF q = pair.b; double dx = p.X - q.X; double dy = p.Y - q.Y; double dist2 = dx * dx + dy * dy; if (dist2 > max2) { maxPair = pair; max2 = dist2; } } // return Math.Sqrt(max2); return new PointF[] { maxPair.a maxPair.b }; } private static PointF[] FromPoints(Point[] pts) { int n = pts.Length; PointF[] mpts = new PointF[n]; for (int i = 0; i < n; i++) mpts[i] = new PointF(pts[i].X pts[i].Y); return mpts; } private static double Orientation(PointF p PointF q PointF r) { return (q.Y - p.Y) * (r.X - p.X) - (q.X - p.X) * (r.Y - p.Y); } private static void Pop<T>(List<T> l) { int n = l.Count; l.RemoveAt(n - 1); } private static T At<T>(List<T> l int index) { int n = l.Count; if (index < 0) return l[n + index]; return l[index]; } private static PointF[][] ConvexHull_LU(PointF[] arr_pts) { List<PointF> u = new List<PointF>(); List<PointF> l = new List<PointF>(); List<PointF> pts = new List<PointF>(arr_pts.Length); pts.AddRange(arr_pts); pts.Sort(Compare); foreach (PointF p in pts) { while (u.Count > 1 && Orientation(At(u -2) At(u -1) p) <= 0) Pop(u); while (l.Count > 1 && Orientation(At(l -2) At(l -1) p) >= 0) Pop(l); u.Add(p); l.Add(p); } return new PointF[][] { l.ToArray() u.ToArray() }; } private class Pair { public PointF a b; public Pair(PointF a PointF b) { this.a = a; this.b = b; } } private static IEnumerable<Pair> RotatingCalipers(PointF[] pts) { PointF[][] l_u = ConvexHull_LU(pts); PointF[] lower = l_u[0]; PointF[] upper = l_u[1]; int i = 0; int j = lower.Length - 1; while (i < upper.Length - 1 || j > 0) { yield return new Pair(upper[i] lower[j]); if (i == upper.Length - 1) j--; else if (j == 0) i += 1; else if ((upper[i + 1].Y - upper[i].Y) * (lower[j].X - lower[j - 1].X) > (lower[j].Y - lower[j - 1].Y) * (upper[i + 1].X - upper[i].X)) i++; else j--; } } private static int Compare(PointF a PointF b) { if (a.X < b.X) { return -1; } else if (a.X == b.X) { if (a.Y < b.Y) return -1; else if (a.Y == b.Y) return 0; } return 1; } }  My off-the-cuff solution is to try a binary partitioning approach where you draw a line somwwhere in the middle and check distances of all points from the middle of that line. That would provide you with 2 Presumably Very Far points. Then check the distance of those two and repeat the above distance check. Repeat this process for a while. My gut says this is an n log n heuristic that will get you Pretty Close.  An easy way is to first find the convex hull of the points which can be done in O(n log n) time in many ways. [I like Graham scan (see animation) but the incremental algorithm is also popular as are others although some take more time.] Then you can find the farthest pair (the diameter) by starting with any two points (say x and y) on the convex hull moving y clockwise until it is furthest from x then moving x moving y again etc. You can prove that this whole thing takes only O(n) time (amortized). So it's O(n log n)+O(n)=O(n log n) in all and possibly O(nh) if you use gift-wrapping as your convex hull algorithm instead. This idea is called rotating calipers as you mentioned. Here is code by David Eppstein (computational geometry researcher; see also his Python Algorithms and Data Structures for future reference). All this is not very hard to code (should be a hundred lines at most; is less than 50 in the Python code above) but before you do that -- you should first consider whether you really need it. If as you say you have only ""thousands of points"" then the trivial O(n^2) algorithm (that compares all pairs) will be run in less than a second in any reasonable programming language. Even with a million points it shouldn't take more than an hour. :-) You should pick the simplest algorithm that works.  On this page: http://en.wikipedia.org/wiki/Rotating_calipers http://cgm.cs.mcgill.ca/~orm/diam.html it shows that you can determine the maximum diameter of a convex polygon in O(n). I just need to turn my point set into a convex polygon first (probably using Graham scan). http://en.wikipedia.org/wiki/Convex_hull_algorithms Here is some C# code I came across for computing the convex hull: http://www.itu.dk/~sestoft/gcsharp/index.html#hull  You could maybe draw a circle that was bigger than the polygon and slowly shrink it checking if youve intersected any points yet. Then your diameter is the number youre looking for. Not sure if this is a good method it sounds somewhere between O(n) and O(n^2) Where do you place the center? Probably near the centroid but I bet I could come up with situations where the center of that circle had an important impact on whether you find the correct GLD or not. this is conceptually flawed -- the circumcircle-diameter of an equilateral triangle is sqrt(3) times the GLD which is equal to the side length"
262,A,"Question about JavaScript graphic animation: saving and restoring a ""sprite"" background I am writing a train based game in Facebook Javascript. I have an inverted triangle that represents the train and dashed lines that represent track which connects cities in Europe. The train moves along the track to travel from one city to the next. At this time I redraw the gameboard with every page refresh. This has the effect of always showing the train in its latest position on the track. However using page refresh to redraw the gameboard is turning out to be too CPU intensive and my apps performance is suffering as a result. Hence I need to implement Ajax and some kind of sprite animation for the train. When the train changes position on the track I need to erase the train marker at its old position and redraw it at it's new position. I know how to redraw it but erasing it at it's old position and restoring the background graphics is where I need help. I have no idea how to do this. As a matter of fact I have no knowledge of manipulating graphics in javascript at all (except a basic PlotPixel() routine that creates 1px divs) Please bear in mind that Facebook JS is incompatible with exisiting JS libraries and frameworks. Can anyone offer me some help with this? I would very much appreciate it. Thanks! Here's the relevant code: function drawTrack(x1 y1 x2 y2 c trains) { if(c == '#444') new Dialog().showMessage('dialog' 'in drawTrack: x1='+x1+' y1='+y1+' x2='+x2+' y2='+y2); /** if(trains[0]) { var train_dump = dump(trains[0] 2); new Dialog().showMessage('dialog' 'td='+train_dump); } **/ var xs1 = x1; var ys1 = y1; var xs2 = x2; var ys2 = y2; var step = 1; var dashlen = 4; var middash = parseInt(dashlen/2); var count = 1; var steep = Math.abs(y2 - y1) > Math.abs(x2 - x1); if (steep) { var t = y1; y1 = x1; x1 = t; t = y2; y2 = x2; x2 = t; } var deltaX = Math.abs(x2 - x1); var deltaY = Math.abs(y2 - y1); var error = 0; var deltaErr = deltaY; var xStep; var yStep; var x = x1; var y = y1; var drewDash; if (x1 < x2) { xStep = step; } else { xStep = -step; } if(y1 < y2) { yStep = step; } else { yStep = -step; } var points = 0; var drawFlag = false; while(x != x2) { x = x + xStep; if(xStep > 0) { if(x > x2) { break; } // do not draw dashes near the city markers if(x >= x2-dashlen) { drawFlag = false; } } if(xStep < 0) { if(x < x2) { break; } // do not draw dashes near the city markers if(x <= x2+dashlen) { drawFlag = false; } } error = error + deltaErr; if(2 * error >= deltaX) { y = y + yStep; error = error - deltaX; } if(points < dashlen) { if(drawFlag) { if(steep) { PlotPixel(y x c); drewDash = true; } else { PlotPixel(x y c); drewDash = true; } if(points == middash) { if(trains[0]) { for(var key = 0; key < trains.length; key++) { if(trains[key]['track_unit'] == count) { if(steep) { trainMarker(y x trains[key]); } else { trainMarker(x y trains[key]); } } } } } } points++; } else { drawFlag = !drawFlag; if(drewDash) count++; points = 0; drewDash = false; } } if(trains[0]) { for(var key = 0; key < trains.length; key++) { if(trains[key]['status'] == 'ARRIVED') { if(trains[key]['direction'] == '+') { trainMarker(xs2 ys2 trains[key]); } else { trainMarker(xs1 ys1 trains[key]); } } } } return count; } function trainMarker(x y trainData) { var train = document.createElement('div'); train.setClassName('train'); y -= trainMarkerHeight; x -= parseInt(trainMarkerWidth/2); var trainId = 'train-'+trainData['line'].toLowerCase()+'-'+trainData['player_number']; train.setId(trainId); train.setStyle('left' x + 'px'); train.setStyle('top' y + 'px'); var trainImg = document.createElement('img'); trainImg.setSrc(publicURL + '/images/train_marker_red.gif'); train.appendChild(trainImg); var trainPlayerNum = document.createElement('div'); trainPlayerNum.setClassName('train-player-num'); trainPlayerNum.setId('train-player-'+trainData['player_number']); trainPlayerNum.setTextValue(trainData['player_number']); trainPlayerNum.setStyle('left' '8px'); trainPlayerNum.setStyle('top' '1px'); trainPlayerNum.addEventListener('mouseover' myEventHandler); trainPlayerNum.addEventListener('mouseout' myEventHandler); trainPlayerNum.appendChild(setTrainTooltip(trainData)); train.appendChild(trainPlayerNum); var parentObj = document.getElementById('map'); parentObj.appendChild(train); } This code draws track (dashed lines) from one city's xy coords to another city's xy coords. If a train is present on that line of track it draws the train as well. If the train is present at one of the cites it draws the train at the city. The game itself is at http://apps.facebook.com/rails%5Fdev. Definitely turn your ""game world"" into a div with the map and track on the background image. You can then manipulate an <img> element (your train) to change its location: var train = document.createElement(""img""); train.srx = ""...""; train.style.position = ""absolute""; document.getElementById(""gameboard"").appendChild(train); function moveTrainTo(x y) { train.style.left = x + ""px""; train.style.top = y + ""px""; }  Perhaps instead of manipulating the image itself you can move the train image over top of the position (map image?) using z-index and an absolutely positioned div? This might be easier to implement and more performant than trying to create or modify the actual image data in javascript. [Edit: to show or hide an image the javascript looks like: (given some HTML like:) <img src=""train.jpg"" id=""trainImg"" /> <img src=""background.jpg"" id=""backgroundImg"" /> The javascript looks like: function getTrain() { document.getElementById(""trainImg""); } function showTrain() { getTrain().style.display = """"; } function hideTrain() { getTrain().style.display = ""none""; } ] If I take my train div and call removeChild() on it will that get rid of it and restore the background? I'm having a little trouble following exactly what your code does but I see that it's an algorithm to draw each pixel individually. My suggestion was instead of drawing to use pre-rendered images using the tag. You can move them around by setting the CSS properties in javascript and use display: none to hide them when you want them not to be shown. Using display: none will make the element invisible but if you have to show it again it will not need to be reloaded. levik's example below has the javascript code for setting the style position attributes Edited my response to show javascript code for showing/hiding the train element Probably don't even need to play with z-index of absolute positioning. For a more user friendly experience you could have the train constantly within the center of the div thus only needing to move the background (the cities and tracks if you will) with a simple CSS scroll. Please see my code above and help me understand how your suggestions might apply to the code. Thank you."
263,A,How can I draw a portion of my Java2D simulation that doesn't change to an image/buffer so I don't have to redraw it's primitives each time? How can I draw a portion of my Java2D simulation that doesn't change to an image/buffer so I don't have to redraw it's primitives each time? I have a portion of my Java2D simulation that requires me to draw thousands of small lines. However this portion of the app doesn't change once drawn so it doesn't make sense to re-draw thousands of primitives each loop iteration (doing active rendering). So what object do I use to draw to and preserve it and then allow me to simply draw this whole image to my canvas and then draw on top of it what changes? Note that you should also override getPreferredSize() to return the size of your image or set a preferred size on the JPanel equal to the size of your image. If you don't do this you'll have layout problems with your JPanel subclass.  One could draw out to a BufferedImage then later draw the contents of the BufferedImage to a Swing component like a JPanel. In order to draw to a BufferedImage one would use the createGraphics to obtain the Graphics2D context of the image: BufferedImage img = new BufferedImage(width height type); Graphics2D g = img.createGraphics(); // do drawing using the Graphics2D object. g.dispose(); Then later draw the contents of the BufferedImage to a JPanel by overriding the paintComponent method: public void paintComponent(Graphics g) { super.paintComponent(g); g.drawImage(img 0 0 null); // Draw img onto the JPanel. }
264,A,"How can I get a collection of all the colors in System.Drawing.Color? How can I extract the list of colors in the System.Drawing.Color struct into a collection or array? Is there a more efficient way of getting a collection of colors than using this struct as a base? So you'd do: string[] colors = Enum.GetNames(typeof(System.Drawing.KnownColor)); ... to get an array of all the collors. Or... You could use reflection to just get the colors. KnownColors includes items like ""Menu"" the color of the system menus etc. this might not be what you desired. So to get just the names of the colors in System.Drawing.Color you could use reflection: Type colorType = typeof(System.Drawing.Color); PropertyInfo[] propInfoList = colorType.GetProperties(BindingFlags.Static | BindingFlags.DeclaredOnly | BindingFlags.Public); foreach (System.Reflection.PropertyInfo c in propInfoList) { Console.WriteLine(c.Name); } This writes out all the colors but you could easily tailor it to add the color names to a list. Check out this Code Project project on building a color chart.  Most of the answers here result in a collection of color names (strings) instead of System.Drawing.Color objects. If you need a collection of actual system colors use this: using System.Collections.Generic; using System.Drawing; using System.Linq; ... static IEnumerable<Color> GetSystemColors() { Type type = typeof(Color); return type.GetProperties().Where(info => info.PropertyType == type).Select(info => (Color)info.GetValue(null null)); }  You'll have to use reflection to get the colors from the System.Drawing.Color struct. System.Collections.Generic.List<string> colors = new System.Collections.Generic.List<string>(); Type t = typeof(System.Drawing.Color); System.Reflection.PropertyInfo[] infos = t.GetProperties(); foreach (System.Reflection.PropertyInfo info in infos) if (info.PropertyType == typeof(System.Drawing.Color)) colors.Add(info.Name);  Try this: foreach (KnownColor knownColor in Enum.GetValues(typeof(KnownColor))) { Trace.WriteLine(string.Format(""{0}"" knownColor)); }  Here is an online page that shows a handy swatch of each color along with its name.  In System.Drawing there is an Enum KnownColor it specifies the known system colors. List<>: List allColors = new List(Enum.GetNames(typeof(KnownColor))); Array[] string[] allColors = Enum.GetNames(typeof(KnownColor));  In addition to what jons911 said if you only want the ""named"" colors and not the system colors like ""ActiveBorder"" the Color class has an IsSystemColor property that you can use to filter those out."
265,A,"design exercise preferably using mfc i was told to design a paintbrush program in 2 variation  one that uses lots of space and little cpu and the other vice versa. the idea (as i was told- so not sure) is somehow to save the screen snapshots versus saving XOR maps (which i have no idea what it means) who represent the delta between the painting. can someone suggest a way or add links to related material ? The obvious place to put the screen shots to use would be to implement an ""undo"" command. The simple memory-hog method is to take a snapshot of the screen before each action. If the user hits ""undo"" you can restore the old screen. To save on memory space you save only the difference between the two screens by XORing them together. By itself this doesn't actually save any space but it sets all the unchanged pixels to 0. To save space you'll then need to apply some sort of compression. Given that you can typically expect fairly large areas that are all zero a run-length encoding will probably be quick and effective. For a run-length encoding you'll typically turn a string of identical bytes into two bytes the first holding the length of the run and the second holding the value. For example 75 zeros in a row would be encoded as 75 0. If you wanted to go a step further instead of saving XORed bitmaps you could look into using a metafile. A metafile records the actions taken at the level of Windows GDI calls so (for example) if you drew a red 100x200 rectangle at 10 100 it would record essentially that -- i.e. instead of the twenty thousand pixels it would save an identifier saying what GDI function to execute and the parameters to supply to that function. In a typical case this might average around 15-20 bytes per ""command"" executed. At the same time it does (often) involve more computation -- for example if you draw a circle re-running a metafile requires re-rastering the circle instead of just storing the bits it produced."
266,A,"Interpolating height for a point inside a grid based on a discrete height function I have been wracking my brain to come up with a solution to this problem. I have a lookup table that returns height values for various points (xz) on the grid. For instance I can calculate the height at A B C and D in Figure 1. However I am looking for a way to interpolate the height at P (which has a known (xz)). The lookup table only has values at the grid intervals and P lies between these intervals. I am trying to calculate values s and t such that: A'(s) = A + s(C-A) B'(t) = B + t(P-B) I would then use the these two equations to find the intersection point of B'(t) with A'(s) to find a point X on the line A-C. With this I can calculate the height at this point X and with that the height at point P. My issue lies in calculating the values for s and t. Any help would be greatly appreciated. Is this a Barycentric coordinate test? Do you want to get the height value at point P if ABC and ADC are separate triangles or do you want the real height interpolating the quad ABCD? Both operation doesn't give the same result. What I am trying to do is first determine which triangle P is in and then interpolate the height with relation to either the points ABC or ADC. Try also bilinear interpolation or bicubic interpolation.  Here's an explicit example based on shape functions. Consider the functions: u1(xz) = (x-x_b)/(x_c-x_b) One has u1(x_bz_b) = u1(x_az_a) = 0 (because x_a = x_b) and u1(x_cz_c) = u1(x_dz_d) = 1 u2(xz) = 1 - u1(xz) Now we have u2(x_bz_b) = u2(x_az_a) = 1 and u2(x_cz_c) = u2(x_dz_d) = 0 v1(xz) = (z-z_b)/(z_a-z_b) This function satisfies v1(x_az_a) = v1(x_dz_d) = 1 and v1(x_bz_b) = v1(x_cz_c) = 0 v2(xz) = 1 - v1(xz) We have v2(x_az_a) = v2(x_dz_d) = 0 and v2(x_bz_b) = v2(x_cz_c) = 1 Now let's build new functions as follows: S_D(xz) = u1(xz) * v1(xz) We get S_D(x_d z_d) = 1 and S_D(x_az_a) = S_D(x_bz_b) = S_D(x_cz_c) = 0 S_C(xz) = u1(xz) * v2(xz) We get S_C(x_c z_c) = 1 and S_C(x_az_a) = S_C(x_bz_b) = S_C(x_dz_d) = 0 S_A(xz) = u2(xz) * v1(xz) We get S_A(x_a z_a) = 1 and S_A(x_bz_b) = S_A(x_cz_c) = S_A(x_dz_d) = 0 S_B(xz) = u2(xz) * v2(xz) We get S_B(x_b z_b) = 1 and S_B(x_az_a) = S_B(x_cz_c) = S_B(x_dz_d) = 0 Now define your interpolating function as H(xz) = h_a * S_A(xz) + h_b * S_B(xz) + h_c * S_C(xz) + h_d * S_D(xz) where h_a is the heigh at point A h_b is the height at point B and so on. You can easily verify that H is indeed an interpolating function: H(x_az_a) = h_a H(x_bz_b) = h_b H(x_cz_c) = h_c and H(x_dz_d) = h_d. Now in order to approximate the height at P all you need to do is evaluate H at this point: h_p = H(x_p z_p) The functions S are normally referred to as ""shape functions"". There's one such function for each node you want your interpolated value to depend on and in this case they all satisfy Kronecker's delta property (they take the value one at one node and zero at all other nodes). There are many ways to build shape functions for a given set of nodes. If I remember correctly the construction of 2D shape functions by multiplication of 1D shape functions (as we've done in this case) is called ""tensor product of functions"" (easy in this case because the grid is rectangular). We have ended up with four functions (one per node) all of them linear combinations of {1 x z xz}. If you want to use only three points for your interpolation then you should be able to easily build three shape functions as linear combinations of {1 x z} only but you will loose a 25% of the height information provided by the grid and your interpolant will not be smooth inside the rectangle when h_b != h_d.  Depending on if you want to interpolate between ABC or ABCD the algorithm will change. To interpolate between ABC (which I assume is what you want to do since you draw the diagonal) you will need to find the barycentric coordinates of P relative to ABC x and y positions then apply the barycentric coordinate to the height (z is assumed here) component of those triangles.  What about going this way: find u and v so that  P = B + u(A-B) + v(C-B) If you write this out you'll see that this is a 2x2 linear system with unknowns u and v so I guess you know how to go on from there. Oh and once you have u and v you use the same exact formula as above for the height only this time ABCP will be the heights at these points."
267,A,GD Image library: Range of colour component arguments for TrueColor images I'm trying to output a TrueColor image using GD (specifically bgd.dll) from a C++ program under windows. The API (or at least the examples) seem to suggest that the range of the integer RGB arguments for gdResolveColor spans the values 0-255. Is this correct? I've experimented with higher values and gotten strange results but this could well be to due my own lack of understanding. That is correct. True color uses one byte for each color component (red green and blue). The range of an byte is 0 to 255 hence the range indicated in the GD documentation. So 16777216 (2^24 or 256^3) different colors can be specified using these 3 bytes (24-bits). I'm not sure how GD handles invalid inputs (i.e. a color component over 255). It likely masks the input and you end up with the your submitted value modulo 255. This was me getting my permutations and combinations mixed up.
268,A,(C#) graphics.drawImage has a size limit? How to deal with it? I am attempting to display a very large graphical representation of some data. I am using a bitmap for persistent storage of the image and e.Graphics.DrawImage(myBitmap new Point(00)) in the onPaint of a PictureBox control on my form. I have noticed (and heard mentioned on other sites) that if my image has a height or width greater than 2^15 I get a Parameter not Valid exception but I have not found any official documentation of this limit. Is this 2^15 image size limit a definite official part of Graphics.DrawImage? Are there any simple workarounds to render my entire image onto the form? (Yes the pictureBox is set to the same size as the image or bigger. Side question though should I just be using the onPaint of the form itself instead of a picture box?) You will run into a problem long before you reach this limit around about 10000x10000 pixels you will be using up nearly all your memory for your bitmap. Consider the internal gdi+ bitmap will be 32bppargb you are looking at 4 bytes per pixel x 100000000 = 4GB by my calculation. You should break the image down into tiles or if you are drawing this manually implement a paging solution. I think your math might be off slightly. 2^15(height) * 2^15(width) * 2^2(*bytes* per pixel) = 2^32 which is exactly 4GB. This is the memory limit for 32 bit systems so it seems like a plausible source of the supposed maximum size of a bitmap. Off by a factor of 10 in fact. Thanks. I've seen far smaller bmps bring gdi+ and c# to its knees though ...  You have a few issues here. First it's not fundamentally possible to display an image this large (unless you have some sort of insanely huge monitor or multiple monitor setup) without shrinking it down to a size that will fit on a normal screen. You can perform this size reduction using Graphics.DrawImage but keep in mind that even with a high-quality InterpolationMode the interpolation is only done on at most a few neighboring pixels which means there is a limit to the maximum amount you can reduce an image without serious loss of information (usually down to 25%). Second a Bitmap object in .Net is a lot more complicated than just a simple array of pixels. Bitmaps are sometimes created in video RAM instead of general program memory which limits their maximum size more severely (in the compact framework as one example one of the Bitmap constructors creates the pixel data in video RAM which is limited to 4MB instead of the 32MB normally available to a .NET process). As a result there is no documented maximum size for a Bitmap - this is an internal implementation detail (affected as well by any already-existing bitmaps) that a programmer can only discover the hard way by getting a thrown exception if it's too big. So using a Bitmap to store an arbitrarily large set of data points is not going to work. Your best approach here would probably be to store your data as an ordinary two-dimensional array (of type int[] probably) which could be arbitrarily large without throwing an OutOfMemoryException (although not without making your computer go swapfile-crazy) and then write a custom method that copies from this array into an actual (and practically-sized) Bitmap. You would then copy from this Bitmap to your PictureBox (or more simply just set this Bitmap as the picture box's Image property - it's best to avoid the Paint method whenever possible).
269,A,Why is X11 Composite extension incompatible with Stereo visuals? In the NVIDIA README for the Quadro card X driver there is this comment: Workstation overlays stereo visuals and the unified back buffer (UBB) are incompatible with Composite. These features will be automatically disabled when Composite is detected Is there some fundamental X reason why this is so? Why are quadro cards on Windows able to do translucent windows and have active stereo visuals at the same time? Is someone working on fixing this? It seems like the future is compositing desktops and Stereo visuals are also becoming more popular for some kinds of apps. You should ask this question at http://www.nvnews.net/vbulletin/forumdisplay.php?s=&forumid=14 as only Nvidia develops and supports this binary driver. Composite prevents direct drawing to the viewport and that's where things get complicated. Basically all hardware overlay operations struggle with composite. The clue is that the hardware overlay shouldn't happen on the screen itself but on a off-screen pixmap which then for example can be mapped to a texture by the compositor. For example xv did not work with early compositing (I don't know if it works now). Direct GL rendering also didn't work with early compositing but this seems to be resolved now. I assume nvidia developers don't have the time/priority to make their other proprietary overlay methods work with indirect rendering. In theory it should work if it is not bad design.
270,A,"What's the best 3D graphics library for Java web apps? I'm looking for a 3D graphics library for a Java web app. Could use some recommendations - only open source though. Edit: I don't really care how the graphics are output - Javascript/applets/canvas/flash but I want to write the graphics logic in Java. I'm unable to understand the concept of a ""3D graphics web app"". Bennop do you mean for use in Applets? I don't really care how the graphics are output - Javascript/applets/canvas/flash but I want to write the graphics logic in Java. Have a look at the The Lightweight Java Game Library. It provided developers access to crossplatform libraries such as openGl. And can run in the browser. http://lwjgl.org Here are a few demos: http://lwjgl.org/applet/ http://fabiensanglard.net/Prototyp/index.php http://www.cokeandcode.com/info/tut2d-4.html People are doing awesome projects with this library: http://lwjgl.org/projects.php  JMonkeyEngine is very good.  Here's an article that describes how to use jmonkeyengine in an applet. http://www.streamhead.com/tutorial-jmonkeyengine-applet-hardware-3d-in-the-browser/ (JMonkeyEngine is a rather nice 3d-engine) ""Warning: Visiting this site may harm your computer!"" - Google Chrome While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes. The warning is probably because the page contains an applet and the java plugin is targeted by hackers. And sorry I don't have time to write a nice summary. Feel free to do so."
271,A,"Metric 3d reconstruction I'm trying to reconstruct 3D points from 2D image correspondences. My camera is calibrated. The test images are of a checkered cube and correspondences are hand picked. Radial distortion is removed. After triangulation the construction seems to be wrong however. The X and Y values seem to be correct but the Z values are about the same and do not differentiate along the cube. The 3D points look like as if the points were flattened along the Z-axis. What is going wrong in the Z values? Do the points need to be normalized or changed from image coordinates at any point say before the fundamental matrix is computed? (If this is too vague I can explain my general process or elaborate on parts) Update Given: x1 = P1 * X and x2 = P2 * X x1 x2 being the first and second image points and X being the 3d point. However I have found that x1 is not close to the actual hand picked value but x2 is in fact close. How I compute projection matrices: P1 = [eye(3) zeros(31)]; P2 = K * [R t]; Update II Calibration results after optimization (with uncertainties) % Focal Length: fc = [ 699.13458 701.11196 ] ± [ 1.05092 1.08272 ] % Principal point: cc = [ 393.51797 304.05914 ] ± [ 1.61832 1.27604 ] % Skew: alpha_c = [ 0.00180 ] ± [ 0.00042 ] => angle of pixel axes = 89.89661 ± 0.02379 degrees % Distortion: kc = [ 0.05867 -0.28214 0.00131 0.00244 0.35651 ] ± [ 0.01228 0.09805 0.00060 0.00083 0.22340 ] % Pixel error: err = [ 0.19975 0.23023 ] % % Note: The numerical errors are approximately three times the standard % deviations (for reference). - K = 699.1346 1.2584 393.5180 0 701.1120 304.0591 0 0 1.0000 E = 0.3692 -0.8351 -4.0017 0.3881 -1.6743 -6.5774 4.5508 6.3663 0.2764 R = -0.9852 0.0712 -0.1561 -0.0967 -0.9820 0.1624 0.1417 -0.1751 -0.9743 t = 0.7942 -0.5761 0.1935 P1 = 1 0 0 0 0 1 0 0 0 0 1 0 P2 = -633.1409 -20.3941 -492.3047 630.6410 -24.6964 -741.7198 -182.3506 -345.0670 0.1417 -0.1751 -0.9743 0.1935 C1 = 0 0 0 1 C2 = 0.6993 -0.5883 0.4060 1.0000 % new points using cpselect %x1 input_points = 422.7500 260.2500 384.2500 238.7500 339.7500 211.7500 298.7500 186.7500 452.7500 236.2500 412.2500 214.2500 368.7500 191.2500 329.7500 165.2500 482.7500 210.2500 443.2500 189.2500 402.2500 166.2500 362.7500 143.2500 510.7500 186.7500 466.7500 165.7500 425.7500 144.2500 392.2500 125.7500 403.2500 369.7500 367.7500 345.2500 330.2500 319.7500 296.2500 297.7500 406.7500 341.2500 365.7500 316.2500 331.2500 293.2500 295.2500 270.2500 414.2500 306.7500 370.2500 281.2500 333.2500 257.7500 296.7500 232.7500 434.7500 341.2500 441.7500 312.7500 446.2500 282.2500 462.7500 311.2500 466.7500 286.2500 475.2500 252.2500 481.7500 292.7500 490.2500 262.7500 498.2500 232.7500 %x2 base_points = 393.2500 311.7500 358.7500 282.7500 319.7500 249.2500 284.2500 216.2500 431.7500 285.2500 395.7500 256.2500 356.7500 223.7500 320.2500 194.2500 474.7500 254.7500 437.7500 226.2500 398.7500 197.2500 362.7500 168.7500 511.2500 227.7500 471.2500 196.7500 432.7500 169.7500 400.2500 145.7500 388.2500 404.2500 357.2500 373.2500 326.7500 343.2500 297.2500 318.7500 387.7500 381.7500 356.2500 351.7500 323.2500 321.7500 291.7500 292.7500 390.7500 352.7500 357.2500 323.2500 320.2500 291.2500 287.2500 258.7500 427.7500 376.7500 429.7500 351.7500 431.7500 324.2500 462.7500 345.7500 463.7500 325.2500 470.7500 295.2500 491.7500 325.2500 497.7500 298.2500 504.7500 270.2500 Update III See answer for corrections. Answers computed above were using the wrong variables/values. Each 2D image has a ""Z axis"" which is the vector that's perpendicular to both the X and Y axes of the 2D image. This is conceptually the ""camera direction"" assuming the 2D image was a photo. You have multiple 2D images right? So do they all have the same Z axis vector? Perhaps I'm misunderstanding the kind of problem you're trying to solve. @Laurence They should have the same z-axis... How can I check? Is this the same as the optical/principal axis? I've only ever thought about doing something like this never actually tried it. In the absence of an answer maybe some questions will help you figure things out... :-) Do your 2D images all have the same Z-axis? (ie: are all of the camera directions for all of the images parallel?) If not is the flattening with respect to a single 2D image's Z-axis? If so can you try reordering the input images to see what happens? Have you normalized your resulting coordinate? What you get back from triangulation is usually [x*s y*s z*s s] (homogenous coordinates) so you must make sure to divide by s. Can you post your code? @Laurence Gonsalves Could you elaborate a bit more? @kiguari yes the points are homogeneous form (xyz1) @John that's a lot of code any specific part you want to see? More information required: What is t? The baseline might be too small for parallax. What is the disparity between x1 and x2? Are you confident about the accuracy of the calibration (I'm assuming you used the Stereo part of the Bouguet Toolbox)? When you say the correspondences are hand-picked do you mean you selected the corresponding points on the image or did you use an interest point detector on the two images are then set the correspondences? I'm sure we can resolve this problem :) Could you post the new points selected with cpselect? So did subpixel corners and/or increasing the baseline work? Updated with the new cpselect points t = [-0.8754; 0.2249; -0.4279]. image points differ an average of: x 65px y 89px. Fairly confident updated the question with my calibration information error is about 0.2 pixels. What units is that? Could you post *K*? In fact could you post *K**R**t* and as many `x1` and `x2`? Ah thanks didn't see the calib info while commenting! *Why* is the skew 1.2584? It's normally (and is often assumed to be) **0**. Are you using a normal camera? Poor quality camera? :) Using Microsoft LifeCam NX-6000 Webcam K = [fc(1) alpha_c*fc(1) cc(1); 0 fc(2) cc(2); 0 0 1]. I tried it with alpha_c = 0... still the same results Hmm .. I see your problem are these faces supposed to be orthogonal? Yes the faces should be orthogonal What's your baseline i .e. |t|? I see you've normalized it so what's the actual length? I'm not sure. How do I compute the baseline? And I didn't know it was normalized :) What's the distance between the two cameras in your stereo images? They were moved by hand (two snapshots) so I don't have their exact distance of change Try increasing the baseline I'll keep you updated if I think of anything Doesn't K*[Rt] take care of the normalization? I was referring to *t* being normalized thus not giving me an idea of the length of the baseline. Also how did you get these correspondences? The correspondences were hand picked As in you clicked them on the screen? Hmmm .. maybe you should use an interest point detector like SIFT (try the VL-feat toolbox) - subpixel estimation is important in triangulation. If he clicked them on the screen using some tool like cpselect (which is part of the Image Processing Toolbox) using sub-pixel accuracy then it's fine. I've tried VLFeat's SIFT library but there are some false positives that get through my RANSAC algorithm seems more trouble than its work for now. I do use the optimal triangulation method (12.1 in Hartley/Zisserman's book) so that might compensate for some inaccuracies in my hand picked results.. @jmbr I just read them off the screen using a paint application. If accuracy is that important I'll take a look at cpselect. If you want to handpick them do zoom up the section so you can click them as accurately as possible. I've done manual correspondence matching in the past and it's OK as long as you're careful and use optimization afterwards. Zooming on the image which is something that cpselect allows you to easily do. So I made an observation. The Microsoft LifeCam NX-6000 Webcam has a 2 megapixel (1.3 effective??) CMOS sensor. My calibration images are 800 x 600 pixels. However a 2MP camera can take 1600 x 1200 pixel images. Does this affect the calibration results if everything is kept at 800x600 ? I would suggest using VL-feat to generate the interest points and then hand-picking the correspondences. Then you get the benefit of subpixel corners! But if not do zoom on the patches in question. Also I gather a new set of points from new images. This time I picked points on 3 sides of the cube (top left and right). Again X and Y look good but the Z is near zero (thus making the 3d pts look flat). Maybe the calibration is bad? Also sub-pixel accuracy shouldn't be necessary because the X and Y are obviously (visually) accurate. The lack of accuracy shouldn't only affect the Z axis I used Paint and the zoom tool :) cpselect doesn't seem to guide the matching unless I missed something `cpselect` allows you to zoom and click points on two images resulting in vectors representing the correspondences- you have to decide the matching. I'm still getting the same results is there anything else that could be the culprit ? Setting the skew to zero doesn't affect the reconstruction by much and still exhibits the same problem Did you obtain points with subpixel accuracy using `cpselect`? So when I triangulate I use the essential matrix rather than the fundamental matrix. Also I multiply every image correspondences in the triangulation algorithm by K^-1. My projection matrix for P2 is [R t];. Now I get a varying z value and the reconstruction looks semi-decent. What is going on? I thought the optimal triangulation method (hartley/zisserman 12.1) required the F not E additional has no mention of multiplying the image correspondences by K^-1.  It may be that your points are in a degenerate configuration. Try to add a couple of points from the scene that don't belong to the cube and see how it goes. Added about another 9 points not on the cube. Still the cube looks flat although the newly added points vary in their Z values relative to the cube. However the Z values themselves do not scale with the X and Y values (z values are about 10^3 smaller than X or Y values). Have you tried using the new points (and these points only) to compute the fundamental matrix? If that works then you could replace non-cube points with cube points recompute F and see how this affects the reconstruction. I insist on this approach because I've been bitten in the past by the degeneracy of points on the cube while debugging calibration algorithms. Hope this helps. Hey jmbr what do you think about subpixel correspondence errors in triangulation? Hi Jacob see my comment in the thread corresponding to your answer. The high skewness bothers me too.  ** Note all reference are to Multiple View Geometry in Computer Vision by Hartley and Zisserman. OK so there were a couple bugs: When computing the essential matrix (p. 257-259) the author mentions the correct Rt pair from the set of four Rt (Result 9.19) is the one where the 3D points lay in front of both cameras (Fig. 9.12 a) but doesn't mention how one computes this. By chance I was re-reading chapter 6 and discovered that 6.2.3 (p.162) discusses depth of points and Result 6.1 is the equation needed to be applied to get the correct R and t. In my implementation of the optimal triangulation method (Algorithm 12.1 (p.318)) in step 2 I had T2^-1' * F * T1^-1 where I needed to have (T2^-1)' * F * T1^-1. The former translates the -1.I wanted and in the latter to translate the inverted the T2 matrix (foiled again by MATLAB!). Finally I wasn't computing P1 correctly it should have been P1 = K * [eye(3)zeros(31)];. I forgot to multiple by the calibration matrix K. Hope this helps future passerby's ! Also thanks Jacob and jmbr :) Would you post your whole script for an example purpose. I've been going around in circles for a few weeks trying to get 3d reconstruction to work. Thanks"
272,A,Why and when should you use hoops 3d graphics? My company needs 3D visualization for our commercial applications (CAD mesh manipulation computational geometry). We are tired of true vision 3D (tv3d) which we've been using for years (poor support compatibility problems). Our manager wants to use hoops 3d from tech soft 3D for software development. While I have no experience of it my prejudices are: Overpriced Relatively few users - poor support Old and outdated Am I wrong about Hoops 3d? What is your experience? Is Hoops useful? Advantages? Disadvantages? Several years ago I was involved in a project that used HOOPs. The company didn't want to pay royalties any more and wasn't very convinced that HOOPs was the right product for them so I ported all the HOOPs functionality they used over to OpenGL. I have summarised my experience of this below but you will have to determine what is relevant for your projects. Disadvantages/costs for that project: Needed to write object picking (HOOPs supplied that) Needed to write virtual trackball (HOOPs supplied that) [although sample code to do this is freely available] Had to move some data storage from HOOPs over to our own data structures Advantages for that project: Able to use features of OpenGL such as transparency (although surely HOOPs has that by now?) Lots of resources to find help with OpenGL Better performance (for our case - I don't know if this is still true as it seems HOOPs uses OpenGL/DirectX underneath now) Better support for consumer-priced graphics cards & laptops More flexibility to go beyond what HOOPs thinks you should need to do The big wins were the resources to find help and greater flexibility. thank you that's a big help.
273,A,"Screen scraping an application window and interacting with the mouse and keyboard The other day I found myself addicted to a Flash game and frustrated by the thing at the same time. In a moment of frustration with the game I thought I would make a 'bot' to beat it for me. Well I really wouldn't but it made me realize: I don't know how to interact with another application in a way to do this. Which brings me to the question how would one take screenshots of another running application and interact with it with the keyboard and mouse. Ideally the solution would be in a managed language like c#. When doing the background reading the net was drowning with articles on scraping HTML. There were not many articles on actually screen scraping an application. Diverse answers are appreciated as I’m really looking at surveying what’s out there. UPDATE I'm looking for a way to interface with another application rather than script/macro another application. UPDATE Could something like Xming be used to redirect the interface? http://www.straightrunning.com/XmingNotes/ Perhaps a Terminal Services client? http://www.codeproject.com/KB/cs/RemoteDesktop_CSharpNET.aspx What exactly are you trying to accomplish? If you are looking for a ""visual programming"" approach go for Sikuli. If you are looking for a programmable/selective screen recorder (""screen scraper"") maybe VNC is easiest to use. The server is already there and the client is pretty simple to write (i wrote a simple Java client in ~300 lines of code GUI and all). I was looking for a way to capture screenshots of an application and manipulate the keyboard/mouse from something like c# (as opposed to a static script). The thought was a program would do some image work from the screenshot and then respond with keyboard/mouse input. Sikuli looks great however it's a little different from what I am looking for. I ended up making the bot which did all this and documented it in a post http://www.charlesrcook.com/archive/2010/09/05/creating-a-bejeweled-blitz-bot-in-c.aspx  I have used AutoHotKey for application automation. Thanks for the suggestion but i'm looking to interact with the application more than automate  Check out Sikuli it is basically what you are looking for. It is written in Java however. http://groups.csail.mit.edu/uid/sikuli/ Wow impressive. That's... the coolest thing ever. Another +1 for Sikuli only wished they had a .NET api to programatically access that powerful engine. Maybe someday."
274,A,UIImageView: Rotated and Scaled. How do I save the results? I am trying to save a rotated and scaled UIImageView as an UIImage in the position and with the zoom scale that the user edited too. But I can only manage to save the original image as it was before editing. How can I save the image as it is shown in the UIImageView with the same scaling and rotation? I have read that I need to use UIGraphicsGetCurrentContext() but I get the same original image saved ( but flipped ) and not the rotated one!! Suggestions and hints would be really helpfull. Thank You in advance. Al (UIImage *)getImage { CGSize size = CGSizeMake(250 250); UIGraphicsBeginImageContext(size); CGContextRef context = UIGraphicsGetCurrentContext(); CGMutablePathRef path = CGPathCreateMutable(); // Add circle to path CGPathAddEllipseInRect(path NULL CGRectMake(0 0 250 250)); CGContextAddPath(context path); // ****************** touchImageView is my UIImageView ****************// CGContextDrawImage(context CGRectMake(0 0 250 250) [touchImageView image].CGImage); UIImage *scaledImage = UIGraphicsGetImageFromCurrentImageContext(); // Clip to the circle and draw the logo CGContextClip(context); UIGraphicsEndImageContext(); return scaledImage; } I don't see anything in the code you posted that would rotate an image. Did you forget to add that? It looks like you are on the right track. Now you should use the transform routines CGContextScaleCTM and CGContextRotateCTM to modify your transform matrix appropriately and then (to avoid the flipping) use -[UIImage drawAtPoint:] to draw the UIImage instead of using CGContextDrawImage. You won't be able to do it like that. The UIImage itself won't get rotated when you rotate the UIImageView. You'll have to rotate it yourself before drawing it if you want to save it. See my answer above for how to do this. The rotation takes place in the touchImageView and then I want to save the rorated image as a circle. Sorry if I was unclear.  The view based transforms are applied the output of the context not the context itself. I believe you have to scale and rotate the image context itself using the link textcore graphic's specific functions. (I could be wrong. I can't remember how exactly how I did this in the past.Take this with a grain of salt.)
275,A,"Rotating 3D cube perspective problem Since I was 13 and playing around with AMOS 3D I've been wanting to learn how to code 3D graphics. Now 10 years later I finally think I have have accumulated enough maths to give it a go. I have followed various tutorials and defined screenX (and screenY equivalently) as screenX = (pointX * cameraX) / distance (Plus offsets and scaling.) My problem is with what the distance variable actually refers to. I have seen distance being defined as the difference in z between the camera and the point. However that cannot be completely right though since x and y have the same effect as z on the actual distance from the camera to the point. I implemented distance as the actual distance but the result gives a somewhat skewed perspective as if it had ""too much"" perspective. My ""actual distance"" implementation was along the lines of: distance = new Vector(pointX pointY cameraZ - pointZ).magnitude() Playing around with the code I added an extra variable to my equation a perspectiveCoefficient as follows: distance = new Vector(pointX * perspectiveCoefficient pointY * perspectiveCoefficient cameraZ - pointZ).magnitude() For some reason that is beyond me I tend to get the best result setting the perspectiveCoefficient to 1/sqrt(2). My 3D test cube is at http://vega.soi.city.ac.uk/~abdv866/3dcubetest/3dtest.svg. (Tested in Safari and FF.) It prompts you for a perspectiveCoefficient where 0 gives a perspective without taking x/y distance into consideration and 1 gives you a perspective where x y and z distance is equally considered. It defaults to 1/sqrt(2). The cube can be rotated about x and y using the arrow keys. (For anyone interested the relevant code is in update() in the View.js file.) Grateful for any ideas on this. +1 for a well-written question and a very cool interactive 3D SVG file. Usually projection is done on the Z=0 plane from an eye position behind this plane. The projected point is the intersection of the line (PtEye) with the Z=0 plane. At the end you get something like: screenX = scaling * pointX / (1 + pointZ/eyeDist) screenY = scaling * pointY / (1 + pointZ/eyeDist) I assume here the camera is at (000) and eye at (00-eyeDist). If eyeDist becomes infinite you obtain a parallel projection."
276,A,"Algorithm for drawing an anti-aliased circle? What's a good algorithm for drawing anti-aliased circles? (Filled and not filled.) Bresenham (of the line algorithm fame) also had a circle algorithm. Xiaolin Wu adapted the line algorithm for anti-aliasing and likewise did the same to the circle algorithm. http://en.wikipedia.org/wiki/Xiaolin_Wu%27s_line_algorithm You can find the circle algorithm with this search: http://www.google.com/search?q=Xiaolin%20Wu%20circle Actually what is popularly known as ""Bresenham's circle algorithm"" wasn't Bresenham's own work. http://en.wikipedia.org/wiki/Midpoint_circle_algorithm But still....  Depending on your platform you are using it may be built in - Java certainly has AA built into it's graphics rendering (and has had for a long time). I would be surprised if DotNet did not also have AA built in. Plus the platform likely has subpixel AA which you get for free and which is substantially better than standard AA. agreed - `java.awt.Graphics2D` has AA capability. See http://www.java2s.com/Code/Java/2D-Graphics-GUI/AntiAlias.htm  If you want an easy one make a soft blur from pixel matrix A to pixel matrix B. This is one i've used (here in pseudo-code) anti_alised_matrix[x][y] = point[x][y] / 2 + point[x+1][y]/8 + point[x-1][y]/8 + point[x][y-1]/8 + point[x][y+1]/8; Ofcourse this is applied to grayscale but you can do easily the same in RGB. This is really a very simple one you can also add the diagonals i.e. [x+1][y+1] and split it by 16 or 32.  Create a Graphics object g. Do g.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.AntiAlias; Draw your anti aliased circle with g.FillEllipse or g.DrawEllipse hehe these are funny comments. :) (I would take the easy option if i could but...) Did he ever say he was on .NET? Nope. But did he ever say he was not :?) He asked for an algorithm not a platform-specific sample of how to use something that does it for you. ""how to use something that does it for you"" sounds like an algorithm to me. Maybe a better answer would involve a stick in a sand rope and a nail :?) ""how to use something that does it for you"" sounds like the exact opposite of an ""algorithm"" to me. Algorithms usually involve math and work everywhere you just set a property to a predefined value. ;-) Spikolynn what you gave is a ritual; what he asked for is reason. I actually found this comment useful as I DID need to know how to do this in .NET"
277,A,"How to create a rounded rectangle at runtime in Windows Forms with VB.NET/C#? I woud like to create a filled rounded rectangle at run-time and assign it as content of a PictureBox (already created and hidden) in Windows Forms. Do you have an idea how can I implement it? This method fills a rounded rectangle on a graphics object (VB code) : Public Sub FillRoundedRectangle(ByVal g As Drawing.Graphics ByVal r As Rectangle ByVal d As Integer ByVal b As Brush) Dim mode As Drawing2D.SmoothingMode = g.SmoothingMode g.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.HighSpeed g.FillPie(b r.X r.Y d d 180 90) g.FillPie(b r.X + r.Width - d r.Y d d 270 90) g.FillPie(b r.X r.Y + r.Height - d d d 90 90) g.FillPie(b r.X + r.Width - d r.Y + r.Height - d d d 0 90) g.FillRectangle(b CInt(r.X + d / 2) r.Y r.Width - d CInt(d / 2)) g.FillRectangle(b r.X CInt(r.Y + d / 2) r.Width CInt(r.Height - d)) g.FillRectangle(b CInt(r.X + d / 2) CInt(r.Y + r.Height - d / 2) CInt(r.Width - d) CInt(d / 2)) g.SmoothingMode = mode End Sub To call this function handle the paint event of the picturebox and pass the e.Graphics object as the first argument and the picturebox's bounds as the second argument if you want the rectangle to fill your picture box completely. The d parameter changes the corners' angle I call it with a value of 30 you can try different values... Also here's some code to draw (instead of fill) a rounded rectangle: Public Sub DrawRoundedRectangle(ByVal g As Drawing.Graphics ByVal r As Rectangle ByVal d As Integer ByVal p As Pen) g.DrawArc(p r.X r.Y d d 180 90) g.DrawLine(p CInt(r.X + d / 2) r.Y CInt(r.X + r.Width - d / 2) r.Y) g.DrawArc(p r.X + r.Width - d r.Y d d 270 90) g.DrawLine(p r.X CInt(r.Y + d / 2) r.X CInt(r.Y + r.Height - d / 2)) g.DrawLine(p CInt(r.X + r.Width) CInt(r.Y + d / 2) CInt(r.X + r.Width) CInt(r.Y + r.Height - d / 2)) g.DrawLine(p CInt(r.X + d / 2) CInt(r.Y + r.Height) CInt(r.X + r.Width - d / 2) CInt(r.Y + r.Height)) g.DrawArc(p r.X r.Y + r.Height - d d d 90 90) g.DrawArc(p r.X + r.Width - d r.Y + r.Height - d d d 0 90) End Sub hey Meta-Knight thanks a lot for your lightning anwser! :D The code is working very well. Just another question how can I do if I don't want to paint it immediatly but only when the application is in a certain state? Should I use also in this case the Paint event? You could have a boolean value in your class (let's say it's called mustPaint) that you set when you want the rectangle to be painted then you could add a condition in the Paint event: if mustPaint then [paint rounded rectangle here] end if ok I will try it thanks do you have any idea why the shape is showed only when I do for example Alt-Tab and not before? I solved calling the PictureBox.Refresh methdo manually to trigger the Paint event  The problem with the fill solution that is marked as the answer is it does not work well with non solid/uniform brushes. Here is another one based on the GraphicsPath class wich I think is more reusable: public static void FillRoundedRectangle(Graphics graphics Rectangle rectangle Brush brush int radius) { if (graphics == null) throw new ArgumentNullException(""graphics""); SmoothingMode mode = graphics.SmoothingMode; graphics.SmoothingMode = SmoothingMode.AntiAlias; using (GraphicsPath path = RoundedRectangle(rectangle radius)) { graphics.FillPath(brush path); } graphics.SmoothingMode = mode; } public static GraphicsPath RoundedRectangle(Rectangle r int radius) { GraphicsPath path = new GraphicsPath(); int d = radius * 2; path.AddLine(r.Left + d r.Top r.Right - d r.Top); path.AddArc(Rectangle.FromLTRB(r.Right - d r.Top r.Right r.Top + d) -90 90); path.AddLine(r.Right r.Top + d r.Right r.Bottom - d); path.AddArc(Rectangle.FromLTRB(r.Right - d r.Bottom - d r.Right r.Bottom) 0 90); path.AddLine(r.Right - d r.Bottom r.Left + d r.Bottom); path.AddArc(Rectangle.FromLTRB(r.Left r.Bottom - d r.Left + d r.Bottom) 90 90); path.AddLine(r.Left r.Bottom - d r.Left r.Top + d); path.AddArc(Rectangle.FromLTRB(r.Left r.Top r.Left + d r.Top + d) 180 90); path.CloseFigure(); return path; } and here is the code for Draw only (not fill) based on the same idea: public static void DrawRoundedRectangle(Graphics graphics Rectangle rectangle Pen pen int radius) { if (graphics == null) throw new ArgumentNullException(""graphics""); SmoothingMode mode = graphics.SmoothingMode; graphics.SmoothingMode = SmoothingMode.AntiAlias; using (GraphicsPath path = RoundedRectangle(rectangle radius)) { graphics.DrawPath(pen path); } graphics.SmoothingMode = mode; }  The easiest way to do this is to create a Bitmap on the fly using the Graphics object. The DrawEllipse method should suffice. Then assign that Bitmap as the contents of the PictureBox object."
278,A,"How to overlay a line for an lm object on a ggplot2 scatterplot I have some data calvarbyruno.1<-structure(list(Nominal = c(1 3 6 10 30 50 150 250) Run = structure(c(1L 1L 1L 1L 1L 1L 1L 1L) .Label = c(""1"" ""2"" ""3"") class = ""factor"") PAR = c(1.25000000000000e-05 0.000960333333333333 0.00205833333333334 0.00423333333333333 0.0322333333333334 0.614433333333334 1.24333333333333 1.86333333333333) PredLin = c(-0.0119152187070942 0.00375925114245899 0.0272709559167888 0.0586198956158952 0.215364594111427 0.372109292606959 1.15583278508462 1.93955627756228 ) PredQuad = c(-0.0615895732702735 -0.0501563307416599 -0.0330831368244257 -0.0104619953693943 0.100190275883806 0.20675348710041 0.6782336426345 1.04748729725370)) .Names = c(""Nominal"" ""Run"" ""PAR"" ""PredLin"" ""PredQuad"") row.names = c(NA 8L) class = ""data.frame"") calweight <- -2 for which I've created both a linear and a quadratic lm model callin.1<-lm(PAR~Nominaldata=calvarbyruno.1weight=Nominal^calweight) calquad.1<-lm(PAR~Nominal+I(Nominal^2)data=calvarbyruno.1weight=Nominal^calweight) I can then plot my data values using ggplot2 qplot(PARNominaldata=calvarbyruno.1) But can't work out how to overlay a line representing the two lm objects... Any ideas ? The easiest option is to use geom_smooth() and let ggplot2 fit the model for you. ggplot(calvarbyruno.1 aes(y = PAR x = Nominal weight=Nominal^calweight)) + geom_smooth(method = ""lm"") + geom_smooth(method = ""lm"" formula = y ~ poly(x 2) colour = ""red"") + geom_point() + coord_flip() Or you can create a new dataset with the predicted values. newdata <- data.frame(Nominal = pretty(calvarbyruno.1$Nominal 100)) newdata$Linear <- predict(callin.1 newdata = newdata) newdata$Quadratic <- predict(calquad.1 newdata = newdata) require(reshape2) newdata <- melt(newdata id.vars = ""Nominal"" variable.name = ""Model"") ggplot(calvarbyruno.1 aes(x = PAR y = Nominal weight=Nominal^calweight)) + geom_line(data = newdata aes(x = value colour = Model)) + geom_point() Thierry do you mind posting an image of the results? Thanks!  Earlier I asked a related question and Hadley had this good answer. Using the predict function from that post you can add two columns to your data. One for each model: calvarbyruno.1$calQuad <- predict(calquad.1) calvarbyruno.1$callin <- predict(callin.1) Then it's a matter of plotting the point and adding each model in as a line: ggplot() + geom_point(data=calvarbyruno.1 aes(PAR Nominal) colour=""green"") + geom_line(data=calvarbyruno.1 aes(calQuad Nominal) colour=""red"" ) + geom_line(data=calvarbyruno.1 aes(callin Nominal) colour=""blue"" ) + opts(aspect.ratio = 1) And that results in this nice picture (yeah the colors could use some work):"
279,A,"Set BufferedImage alpha mask in Java I have two BufferedImages I loaded in from pngs. The first contains an image the second an alpha mask for the image. I want to create a combined image from the two by applying the alpha mask. My google-fu fails me. I know how to load/save the images I just need the bit where I go from two BufferedImages to one BufferedImage with the right alpha channel. I edited my answer to give a correct code (doing what you requested!) in an alternative way. Actually I've figured it out. This is probably not a fast way of doing it but it works: for (int y = 0; y < image.getHeight(); y++) { for (int x = 0; x < image.getWidth(); x++) { Color c = new Color(image.getRGB(x y)); Color maskC = new Color(mask.getRGB(x y)); Color maskedColor = new Color(c.getRed() c.getGreen() c.getBlue() maskC.getRed()); resultImg.setRGB(x y maskedColor.getRGB()); } }  I'm too late with this answer but maybe it is of use for someone anyway. This is a simpler and more efficient version of Michael Myers' method: public void applyGrayscaleMaskToAlpha(BufferedImage image BufferedImage mask) { int width = image.getWidth(); int height = image.getHeight(); int[] imagePixels = image.getRGB(0 0 width height null 0 width); int[] maskPixels = mask.getRGB(0 0 width height null 0 width); for (int i = 0; i < imagePixels.length; i++) { int color = imagePixels[i] & 0x00ffffff; // Mask preexisting alpha int alpha = maskPixels[i] << 24; // Shift green to alpha imagePixels[i] = color | alpha; } image.setRGB(0 0 width height imagePixels 0 width); } It reads all the pixels into an array at the beginning thus requiring only one for-loop. Also it directly shifts the green byte to the alpha (of the mask color) instead of first masking the red byte and then shifting it. Like the other methods it assumes both images have the same dimensions.  Your solution could be improved by fetching the RGB data more than one pixel at a time(see http://java.sun.com/javase/6/docs/api/java/awt/image/BufferedImage.html) and by not creating three Color objects on every iteration of the inner loop. final int width = image.getWidth(); int[] imgData = new int[width]; int[] maskData = new int[width]; for (int y = 0; y < image.getHeight(); y++) { // fetch a line of data from each image image.getRGB(0 y width 1 imgData 0 1); mask.getRGB(0 y width 1 maskData 0 1); // apply the mask for (int x = 0; x < width; x++) { int color = imgData[x] & 0x00FFFFFF; // mask away any alpha present int maskColor = (maskData[x] & 0x00FF0000) << 8; // shift red into alpha bits color |= maskColor; imgData[x] = color; } // replace the data image.setRGB(0 y width 1 imgData 0 1); }  I played recently a bit with this stuff to display an image over another one and to fade an image to gray. Also masking an image with a mask with transparency (my previous version of this message!). I took my little test program and tweaked it a bit to get the wanted result. Here are the relevant bits: TestMask() throws IOException { m_images = new BufferedImage[3]; m_images[0] = ImageIO.read(new File(""E:/Documents/images/map.png"")); m_images[1] = ImageIO.read(new File(""E:/Documents/images/mapMask3.png"")); Image transpImg = TransformGrayToTransparency(m_images[1]); m_images[2] = ApplyTransparency(m_images[0] transpImg); } private Image TransformGrayToTransparency(BufferedImage image) { ImageFilter filter = new RGBImageFilter() { public final int filterRGB(int x int y int rgb) { return (rgb << 8) & 0xFF000000; } }; ImageProducer ip = new FilteredImageSource(image.getSource() filter); return Toolkit.getDefaultToolkit().createImage(ip); } private BufferedImage ApplyTransparency(BufferedImage image Image mask) { BufferedImage dest = new BufferedImage( image.getWidth() image.getHeight() BufferedImage.TYPE_INT_ARGB); Graphics2D g2 = dest.createGraphics(); g2.drawImage(image 0 0 null); AlphaComposite ac = AlphaComposite.getInstance(AlphaComposite.DST_IN 1.0F); g2.setComposite(ac); g2.drawImage(mask 0 0 null); g2.dispose(); return dest; } The remainder just display the images in a little Swing panel. Note that the mask image is gray levels black becoming full transparency white becoming full opaque. Although you have resolved your problem I though I could share my take on it. It uses a slightly more Java-ish method using standard classes to process/filter images. Actually my method uses a bit more memory (making an additional image) and I am not sure it is faster (measuring respective performances could be interesting) but it is slightly more abstract. At least you have choice! :-) Some pretty good techniques in this post. Very appreciated! Very nice - the AlphaComposite just saved me hours and hours of work - thank you for taking the time to refactor and post your results."
280,A,"Drawing path by Bezier curves I have a task - draw smooth curve input: set of points (they added in realtime) current solution: I use each 4 points to draw qubic Bezier curve (1 - strart 2 and 3rd - control points 4- end). End point of each curve is start point for the next one. problem: at the curves connection I often have ""fracture"" (angle) Can you tell me how to connect my points more smooth? Thanks! You need to compute first and probably second derivatives for joining points. See http://html5tutorial.com/how-to-join-two-bezier-curves-with-the-canvas-api/  Here is few references which can help you: Draw a Smooth Curve through a Set of 2D Points with Bezier Primitives Curve fitting Line Smoothing and point-weeding"
281,A,"Graphics.Save vs Graphics.BeginContainer How is Graphics.Save different from Graphics.BeginContainer? take a look here: The documentation does not differentiate between calls to BeginContainer/EndContainer and calls to Graphics.Save and GraphicsRestore. In addition there are a few errors in the documentation. [e.g. GraphicsState is incorrectly asserted to be used by BeginContainer] In my use BeginContainer/EndContainer appears to save and restore the current transform. It does not actually save the clipping region as the documentation asserts and it may not save any of the other properties in the graphics objects. With Save/Restore I was actually able to save/restore the clipping region current transform and other settings. It appears to be if not complete more ""complete"" than the container functions. Therefore I suspect a performance/completeness tradeoff with the two different methods. I also doubt whether the documentation is correct in stating that GraphicsState objects (used by Save) are stored in the stack as are GraphicsContainer objects (used by BeginContainer). I suspect that GraphicsState may not even be placed on a stack but I have not tested this hypothesis.  Graphics.Save Method Saves the current state of this Graphics and identifies the saved state with a GraphicsState. Graphics.BeginContainer Method Saves a graphics container with the current state of this Graphics object and opens and uses a new graphics container. Remarks Calls to the BeginContainer method place information blocks on the same stack as calls to the Save method. Just as a Restore call is paired with a Save call a EndContainer method call is paired with a BeginContainer method call. When you call the Restore method all information blocks placed on the stack (by the Save method or by the BeginContainer method) after the corresponding call to the Save method are removed from the stack. Likewise When you call the EndContainer method all information blocks placed on the stack (by the Save method or by the BeginContainer method) after the corresponding call to the BeginContainer method are removed from the stack. See details on http://msdn.microsoft.com/en-us/library/system.drawing.graphics.save.aspx Sounds like similarities. I'm looking for differences. :)  I am not giving you a solution but here are some links for your reference - Graphics container document http://www.dotnetmonster.com/Uwe/Forum.aspx/dotnet-drawing/2252/BeginContainer-and-Save Graphics Containers SUMMARY:The BeginContainer method of the Graphics class creates a container. Each BeginContainer method is paired with an EndContainer method. You can also create nested containers. The following code snippet creates two containers. A call to the Save method saves a GraphicsState object as an information block on the stack and returns it. Sure but why do both exist? When would I want to choose one or the other? What specific elements of the state of the graphics object are saved/restored by each? I don't get it and a link to a page where someone else says that they don't get it doesn't really help sort it out."
282,A,"What does glLoadIdentity() do in OpenGL? I'm new to OpenGL and I'm a little overwhelmed with all of the random functions that I have my in code. They work and I know when to use them but I don't know why I need them or what they actually do. I know that glLoadIdentity() replaces the current matrix with the identity matrix but what exactly does that do? If every program requires it why isn't the identity matrix by default unless otherwise specified? I don't like to have functions in my code unless I know what they do. I should note that I am using OpenGL exclusively for rich 2D clients so excuse my ignorance if this is something very obvious for 3D. Also a little confused about glMatrixMode(GL_PROJECTION) VS glMatrixMode(GL_MODELVIEW). The identity matrix is used to ""initialize"" a matrix to a sane default. One important thing to realize is that matrix multiplications are in a sense additive. For example if you take a matrix that starts with the identity matrix multiply it times a rotation matrix then multiply it times a scaling matrix you end up with a matrix that rotates and scales the the matrices it is multiplied against.  The identity matrix in terms of the projection and modelview matrices essentially resets the matrix back to its default state. As you hopefully know glTranslate and glRotate are always relative to the matrix's current state. So for instance if you call glTranslate you are translating from the matrix's current 'position' not from the origin. But if you want to start over at the origin that's when you call glLoadIdentity() and then you can glTranslate from the matrix which is now located at the origin or glRotate from the matrix which is now oriented in the default direction. I think Boon's answer that it is the equivalent of 1 is not exactly correct. The matrix actually looks like this: 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 That is the identity matrix. Boon is correct mathematically that any matrix multiplied with that matrix (or a matrix that looks like that; diagonal ones all else 0s) will result in the original matrix but I don't believe he explained why this is important. The reason why this is important is because OpenGL multiplies all positions and rotations through each matrix; so when for instance you draw a polygon (glBegin(GL_FACE) some points glEnd()) it translates it to ""world space"" by multiplying it with the MODELVIEW and then translates it from 3D to 2D by multiplying it with the PROJECT matrix and that gives it the 2D points on screen along with the depth (from the screen 'camera') which it uses to draw pixels. But when one of these matrices are the identity matrix the points are multiplied with the identity matrix and therefore are not changed so the matrix has no effect; it does not translate the points it does not rotate them it leaves them as-is. I hope this clarifies a bit more! clear Example with me :)  Identity matrix is the equivalent of 1 for number. As you know any number that multiplies with 1 is itself (e.g. A x 1 = A) The same thing goes for matrix ( MatrixA x IdentityMatrix = MatrixA). So loading an identity matrix is a way to initialize your matrix to the right state before you multiply further matrices into the matrix stack. glMatrixMode(GL_PROJECTION) : deals with the matrices used by perspective transformation or orthogonal transformation. glMatrixMode(GL_MODELVIEW) : deals with matrices used by model-view transformation. That is to transform your object (aka model) to the view coordinate space (or camera space).  The projection matrix is used to create your viewing volume. Imagine a scene in the real world. You don't really see everything around you only what your eyes allow you to see. If you're a fish for example you see things a bit broader. So when we say that we set up the projection matrix we mean that we set up what we want to see from the scene that we create. I mean you can draw objects anywhere in your world. If they are not inside the view volume you won't see anything. When you create the view volume imagine that you create 6 clipping planes that define your field of view. As for the modelview matrix it is used to make various transformations to the models (objects) in your world. Like this you only have to define your object once and then translate it or rotate it or scale it. You would use the projection matrix before drawing the objects in your scene to set the view volume. Then you draw your object and change the modelview matrix accordingly. Of course you can change your matrix midway of drawing your models if for example you want to draw a scene and then draw some text (which with some methods you can work easier in orthographic projection) then change back to modelview matrix. As for the name modelview it has to do with the duality of modeling and viewing transformations. If you draw the camera 5 units back or move the object 5 units forwards it is essentially the same. Hope I've shed some light"
283,A,Java graphics performance I was programming a simple game and wanted to display it via the Java Graphics + Swing API. It felt a little slow of course so I measured how long it took to repaint which was around 32ms. Then I read about accelerated Java graphics and used the method described here: Space Invaders However somehow this is even slower. It now takes around 98ms to redraw. Why is that? Note that I am aware of libraries like LWGL and JOGL but I don't want to use a full-blown OpenGL wrapper for such a graphically simple game. Can you post some of your painting code? and clock/timer logic? The clock/timer logic is simply the difference of two long values obtained via System.currentTimeMillis(). I know it's not a good approach to measure time like this but I just wanted a rough estimate of how long it takes. The painting code simply draws a static image onto the canvas. So it's just g.drawImage(img 0 0 null) where g is obtained via the BufferStrategy variable. You may look at processing.  Try JGame. It's a simple engine for 2D games which uses GL when its available and can produce games that run on anything from a mobile phone to a desktop system.  I can't tell exactly what you are doing. Your question is about Swing but you mention a Canvas and Bufferstrategy. Well in Swing you would use a JPanel which is automatically double buffered so you don't have to worry about that. I tested a simple animation with 1000 moving balls. The Timer was set to fire every 100ms. When the Timer fired the 1000 ball where randomly moved to a new position and a new BufferedImage was created and the panel displaying the image was repainted. The difference in time between the repainting was 110ms implying it only took 10ms to create an new BufferedImage and do the painting. The painting was done a a full screen so no clipping was done. This posting show the example code I tested. If you want to measure the time for something quick the best way is to use System.nanoTime() (added in JDK 1.5) which measures in nanoseconds. It's not as accurate over long periods as System.currentTimeMillis but for short periods it's obviously more efficient.  I am not an expert but Java2D supports different rendering pipelines. On my machine switching to the OpenGL one by setting the system property sun.java2d.opengl=true yielded a substantial increase in rendering speed.  I agree with Nicolas for graphics / interactivity processing is great  Java2D performance is a bit of a dark art. It can vary between platforms but it possible to get it to perform well. Sometimes doing seemingly innocuous things can result in much slower performance. You want to make sure that you are using accelerated images as much as possible. The type of the image can also be significant. I don't know the full details but I do know that my programs were much faster using type 1 images (INT RGB) than other types such as type 5 (3BYTE BGR) because of conversion issues.  First of all you should check and see how much time is spent in your own code vs. how much time is spent actually drawing the frame. You may have some bug or glitch that makes it run more slowly. You should be able to get much better performance then 98ms to draw your screen. Those game libraries will probably be using the same graphics calls as you would. But which containers you use can have a big impact on how fast things draw. I remember a couple months ago working on some double buffered graphics code and how the back buffer was created made a huge difference in performance. In particular make sure you only create background image once or at least only when your canvas resizes. In my draw code I do this:  //create a graphics backplane if(backplane == null || !backplaneSize.equals(this.getSize())){ backplane = this.createImage((int)this.getSize().getWidth() (int)this.getSize().getHeight()); backplaneSize = this.getSize(); } So the back plane only gets created if it's new or if the component is resized. This has a huge impact on speed. I've been doing graphics programming in java since JDK 1.0. Actually I'd never heard of this BufferStrategy thing. Heh. I've never had any trouble getting good frame rates with basic java graphics calls. Another thing to look out for is making sure that Swing isn't trying to clear the background of your component for you. That can be another source of slowdown.  Read this about timers Use an always sleeping thread in the background to lower the system interrupt rate to get much more precise timers! You can take a look at my code in here A small game I've created can be found here long renderStart = System.nanoTime() // render stuff here long renderTime = (System.nanoTime() - renderStart) / 1000000; I downloaded your library (Bonsai). But I've one question: You made a class `GameEngine`. What is the purpose here (Google translation)? thanks (15 chars) Actually I had a problem with my Eclipse SVN Plugin so it did not delete the file remotely. Just today I deleted the file via another SVN Client... which crashed the whole eclipse Project so I had to rebuild it which then again somehow destroyed the repository :D So the file is just a left over from development at sometime I wanted to factor out the actual engine from the applet/gui.  Swing will be slow for a game. Your best bet is probably an SDL wrapper such as jsdl or sdljava. SDL is quite lightweight and supported on many platforms. The nice thing is well-implemented 2d graphics library wrappers implement the Graphics interface so you should be able to try several and see which one performs best for your app without modifying your code. Both jsdl and sdljava are very out of data nowadays. Most people looking for high graphics performance on Java just use OpenGL directly e.g. via LWJGL (http://lwjgl.org/)  A few hints to improve performance: Clip and just draw the area that has changed Don't scale anything when drawing Use the correct buffer strategy Use full screen exclusive mode if appropriate. Swing isn't inherently slow. Most of the confusion about Swing comes from people who don't know or understand about the Event Dispatch Thread (and yes it's a lot of effort to get it right). Regarding your times are you just calling g.drawImage between the time settings? If so what size are you repainting and are you doing any scaling on this call? EDIT: Forgot to mention Pulp Core - a very nice gaming framework for Java.
284,A,Where's Polygon.Double in Java? Once again I'm doing Java graphics (Graphics2D) but I noticed there is no Polygon.Double or Polygon.Float classes whereas there is Rectangle2D.Float and Rectangle2D.Double class. Does anyone know why this is? I just need to draw a triangle using doubles as points. You can use a Path2D (or a Path2D.Float or Path2D.Double) to get the same effect. Thanks. I also need to fill this polygon. Is this possible with Path2D? Yes. `Graphics2D` has a `fill(Shape)` method and `Path2D` is-a `Shape`.
285,A,float vs double on graphics hardware I've been trying to find info on performance of using float vs double on graphics hardware. I've found plenty of info on float vs double on CPUs but such info is more scarce for GPUs. I code with OpenGL so if there's any info specific to that API that you feel should be known let's have at it. I understand that if the program is moving a lot of data to/from the graphics hardware then it would probably be better to use floats as doubles would require twice the bandwidth. My inquiries are more towards how the graphics hardware does it's processing. As I understand it modern Intel CPUs convert float/double to an 80-bit real for calculations (SSE instructions excluded) and both types are thus about equally fast. Do modern graphics cards do any such thing? is float and double performance about equal now? Are there any strong reasons to use one over the other? Most GPUs don't support double floats at all. The support has been added very recently (this generation) and not everywhere: ATI: HD5870 and HD5850 have it at decent speed (not as fast as single though) HD5770 does not have it despite being in the same generation as the HD5870. Nvidia: GT200 based cards have double support but at a double/single ratio that is very low. (8:1 ratio ?) Fermi is supposed to have it at half speed of single... Whenever that ships. For everything else you just don't have double support. So... You should definitely not use double if you don't need it.  Modern graphic cards do many optimizations e.g.: they can even operate on 24-bit floats. As far as I know internally graphic cards don't use doubles as they're built for speed not necessarily precision. From entry on GPGPU on Wikipedia: The implementations of floating point on Nvidia GPUs are mostly IEEE compliant; however this is not true across all vendors. This has implications for correctness which are considered important to some scientific applications. While 64-bit floating point values (double precision float) are commonly available on CPUs these are not universally supported on GPUs; some GPU architectures sacrifice IEEE-compliance while others lack double-precision altogether. There have been efforts to emulate double precision floating point values on GPUs; however the speed tradeoff negates any benefit to offloading the computation onto the GPU in the first place. most recent graphics cards don't operate on 24bit floats anymore. The ones that did were the ATI R300 and derivatives (DX9 based).  In terms of speed GPUs are optimized for floats. I'm much more familiar with Nvidia hardware but in current generation hardware there is 1 DP FPU for every 8 SP FPU. In next generation hardware they're expected to have more of a 1 to 2 ratio instead. My recommendation would be to see if your algorithm needs double precision. Many algorithms don't really need the extra bits. Run some tests to determine the average error that you get by going to single precision and figure out if it's significant. If not just use single. If your algorithm is purely for graphics you probably don't need double precision. If you are doing general purpose computation consider using OpenCL or CUDA.  Doubles are not supported for rendering until DX11: (ie Shader Model 5) http://msdn.microsoft.com/en-us/library/ee418354(VS.85).aspx I suspect OpenGL will be the same.
286,A,"Tetris and pretty graphics Say you're building a Tetris game. As any proper programmer you have your view logic on one side and your business logic on the other side; probably a full-on MVC going on. When the model sends its update() the view redraws itself as expected. But then... if you wanted to add say an animation to vanish a line how would you implement that in the view? Make any assumptions you want---excepting that ""Everything is properly encapsulated"". Another approach would be to combine class MVC with something like differential execution - the 'view' is a model of what is presented but the drawing code compares the stream of events the 'view' creates with the stream from the previous rendering. So if in one stream there's a line and the next there isn't the drawing code can animate the difference. This allows the drawing to be abstracted away from the view . Frequently the 'view' in MVC is a collection of widgets rather than being something which draws the display directly so you end up with nested MVC hierarchies anyway: the application is MVC ( data model view objects app controller ) where the view object has a collection of widgets each of which is MVC ( widget state (eg button pressed ) look and feel/toolkit binding mapping of toolkit events -> widget state ).  Personally I would separate draw the screen as often as possible even if there was no update of the block position. So I would have a loop somewhere with an ""update"" and a ""render"" part. Update plays the ball to the logic which does or does not any update of positions and/or block removal. Render plays the ball to the graphics part which draws the blocks where they should be. Now if there are lines to erase the logic knows and can mark those lines to be removed. I assume here that every piece consists of 4 single blocks and any of these blocks is a single object. Now when this block has the ""die""-flag set you may take some render-parts to vanish the block (let's say 500ms to explode). After this time the object may be disposed and the block a line above falls down. Why 500ms? Well you should definitely use time-based movement as this keeps the game speed the same on different computers. Btw there are already so called game engines which provide such an update-render-loop. For example XNA if you go the .NET line. You may also code your own engine but beware it's not an easy task and it's very time consuming. I did this once and don't expect it to be an engine like the Source Engine ;-)  I've often wondered this myself. My own thoughts have been along this line: 1) The view is given the state of the blocks (shape yada-yada) but with extra ""transitional"" data: 2) The fact that a line must be removed is encoded in the state NOT computed in the view. 3) The view knows how to draw transitions now: No change: state is the same for this particular block Change from ""falling"" to ""locked"": state is ""locked in"" (by a dropping block) Change from ""locked"" to ""remove"": state is ""removed"" (by a line completion) Change from ""falling"" to ""remove"": state is ""removed"" but old state was ""falling""  Most games execute a loop that constantly redraws the view of the game as fast as possible rather than waiting for a change in the model state and then refreshing the view. If you like the model view pattern then it might work well for the view to continue to draw some types of objects after they are removed from the model fading them out over a few milliseconds.  Its interesting to think of a game as an MVC. Thats a perspective I've never taken (for some odd reason) but definitely an intriguing one that makes a lot of sense. Assuming you do implement your Tetris game with an MVC I think there are two things you might want to take into account in regards to communication between your controller and your view: There is state and there are events. Your controller is obviously the central point of interaction for the user. When they issue keyboard commands your controller will interpret them and make the appropriate state adjustments. However sometimes the game will enter a state that coincides with a particular event...such as filling a line with blocks that should now be removed. Scoregraphic has given you a great foundation. Your view should operate on a fixed cycle to maintain consistent speed across computers. But in addition to updating the screen to render new state it should also have a queue of events that it can perform animations in response to. In the case of filling lines in Tetris your controller could issue strongly typed event objects that derive from some kind of base event type into the view event queue which could then be used by the view to perform the appropriate animated responses."
287,A,"2D image rotation few issues I'm adding ""mouse rotation"" to my 2D drawing program. I've got all the code working by basically calculating the rotation angle from the original mouse click to wherever the mouse currently is. I also draw a transparent rectangle that rotates instead of actually rotating the image on every mouse movement event. Now my problem is the drawing of this rectangle. I draw the rectangle from the image's x/y position with its width/height being what the image reports. However after rotating a rectangular image its new width and height is much bigger as these two screenshots should help clarify: During rotation and after rotating then rotating again -- the little ""handles"" show where the images' x/y/width/height extends to In the second screenshot because of the rotation the image has been padded sort of with whitespace (it's hard to describe with text!). E.g. an image that's 200x100 can end up like 150x150 (approximately) after rotating which looks a bit strange when resizing the 2nd time. Does anyone have an idea how to fix this? You'll need to store the original dimensions of the image and the current angle of rotation so that you can effectively back out the rotations correctly. Also you'll need to save the original image data. What's happening now is that your program loses the information about the original image size so it uses what it sees (correctly). What you want is a fresh redraw from the original image data just with a different rotation.  You should probably keep track of the image's current rotation so that you can re-draw the rectangle around the image at its current rotation. If you are going to need to rotate more than 1 thing you will have to keep track of layers and the rotation of each one.  As a rule of thumb never rotate/resize a previously rotated image as the small errors will start creeping in. Generally it is easier to keep a copy of the original image and base ALL changes off that image. For example the first rotate is 5 degrees. The second rotate is 15 degrees. To render the second image rotate the original copy 20 degrees and display that. Not sure if that helps or if I have misread your question. It doesn't matter whether you rotate or scale first if the scaling is uniform and if the center of scaling and rotation is the same. However if you stretch the X and Y at different ratios or if the scaling has a different origin than the rotation then the order matters Hi thanks I was thinking that was the way to do it. I do keep an original image and base all scaling/rotating from that. Which leads to another problem...I also have code for scaling but am not sure how to incorporate it with the rotation code. Each Image class will keep instance rotate/scale variables but should I rotate or rescale first? Another issue is I'm basically using wxWidgets' image rotating code - and my ""rescale handles"" at the image's corners are drawn from the values returned by GetWidth/Height. Should I now be tracking the (outline) rectangle's x/y/width/height after rotating and using those values to determine the handle's positioning?"
288,A,Drag+Drop with physical behaviour I'd like to implement a dragging feature where users can drag objects around the workspace. That of course is the easy bit. The hard bit is to try and make it a physically correct drag which incorporates rotation due to torque moments (imagine dragging a book around on a table using only one finger how does it rotate as you drag?). Does anyone know where I can find explanations on how to code this (2D only rectangles only no friction required)? Much obliged David EDIT: I wrote a small app (with clearly erroneous behaviour) that I hope will convey what I'm looking for much better than words could. C# (VS 2008) source and compiled exe here EDIT 2: Adjusted the example project to give acceptable behaviour. New source (and compiled exe) is available here. Written in C# 2008. I provide this code free of any copyright feel free to use/modify/whatever. No need to inform me or mention me. Which language? Here's a bunch of 2d transforms in C I can read a lot of languages but this will eventually end up in C# or VB.NET. The source at that link indeed talks about transforms but not about how to calculate those transforms from dragging vectors.  This would seem to be a basic physics problem. You would need to know where the click and that will tell you if they are pushing or pulling so though you are doing this in 2D your calculations will need to be in 3D and your awareness of where they clicked will be in 3D. Each item will have properties such as mass and perhaps information for air resistance since the air will help to provide the motion. You will also need to react differently based on how fast the user is moving the mouse. So they may be able to move the 2 ton weight faster than is possible and you will just need to adapt to that as the user will not be happy if the object being dragged is slower than the mouse pointer. Hi James this is going far beyond the scope of what I'm looking for. There's no need to incorporate friction with the groundplane no need to handle objects with different masses no need for collisions with other objects and certainly no need to take air-flow into account. I'm not even sure I want objects to keep on moving after I let go of the mouse. @ David Rutten - if you don't take into account air friction and velocity how will you calculate what should happen? If I click on a certain part of the object and pull what forces will cause it to being to rotate? @James the object has a tendency to rotate around the center of mass. If I pull on the object anywhere besides this point it will both translate and rotate. If the motion is slow enough than the object momentum can be ignored.  Torque is just the applied force projected perpendicular to a vector between the point where the force is applied and the centroid of the object. So if you pull perpendicular to the diameter the torque is equal to the applied force. If you pull directly away from the centroid the torque is zero. You'd typically want to do this by modeling a spring connecting the original mouse-down point to the current position of the mouse (in object-local coordinates). Using a spring and some friction smooths out the motions of the mouse a bit. I've heard good things about Chipmunk as a 2D physics package: http://code.google.com/p/chipmunk-physics/ Okay It's getting late and I need to sleep. But here are some starting points. You can either do all the calculations in one coordinate space or you can define a coordinate space per object. In most animation systems people use coordinate spaces per object and use transformation matrices to convert because it makes the math easier. The basic sequence of calculations is: On mouse-down you do your hit-test and store the coordinates of the event (in the object coordinate space). When the mouse moves you create a vector representing the distance moved. The force exterted by the spring is k * M where M is the amount of distance between that initial mouse-down point from step 1 and the current mouse position. k is the spring constant of the spring. Project that vector onto two direction vectors starting from the initial mouse-down point. One direction is towards the center of the object the other is 90 degrees from that. The force projected towards the center of the object will move it towards the mouse cursor and the other force is the torque around the axis. How much the object accelerates is dependent on its mass and the rotational acceleration is dependent on angular momentum. The friction and viscosity of the medium the object is moving in causes drag which simply reduces the motion of the object over time. Or maybe you just want to fake it. In that case just store the (xy) location of the rectangle and its current rotation phi. Then do this: Capture the mouse-down location in world coordinates When the mouse moves move the box according to the change in mouse position Calculate the angle between the mouse and the center of the object (atan2 is useful here) and between the center of the object and the initial mouse-down point. Add the difference between the two angles to the rotation of the rectangle. @Mark I tried the vector decomposition but I couldn't get it to feel right. In the end I used the trick you mentioned but added an angle damping factor based on the square of the distance between the centroid and the grip point. New source (and compiled exe) is available in the original post. Thanks again man. Thanks Mark the dragging angle must somehow be part of the equation just haven't been able to come up with a good way to incorporate it. (I edited the original post and added a download link to an example app). Do you have an example of how to construct (and solve) such a spring system? I was afraid you'd ask for an example :-) I haven't had to do this for a while but I'll through some equations in up above... Heh no good deed goes unpunished. I've been browsing Chipmunk it is indeed pretty impressive. But I hesitate to add a whole project to my app just to allow for rotating boxes. I'm pretty sure there must be a good way to mimic physical box transformations with a relatively simple algorithm. Thanks again for the effort. @Mark made my day just in time (bedtime for me too). The example app I posted in the original post does exactly the trick you described (I think) and it doesn't work very well. I'll try the vector-decomposition approach tomorrow and post an update here.
289,A,When should a uniform be used in shader programming? In a vertex shader I calculate a vector using only uniforms. Therefore the outcome of this calculation is the same for all instantiations of the vertex shader. Should I just do this calculation on the CPU and upload it as a uniform? What if I have ten such calculations? If I upload a lot of uniforms in this way does CPU-GPU communication ever get so slow that recomputing such values in the vertex shader is actually faster? I actually depends on the uniforms/vertex count ratio. In the case vertices are more than the uniform variables it's probably better to compute uniforms on CPU which is the most frequent scenario. In the case uniforms values are many and their computation is complex (i.e. inverse matrix) while vertices count is low it may be better to off load the CPU and perform computation on GPU. The ratio threshold value is difficoult to determine since many factor influence the shader execution. Normally its better to compute most of uniforms on CPU to offload the shader execution. A note: once you set the uniform variables the shader program mantains their values untill its relinked again.
290,A,"Finding closest non-black pixel in an image fast I have a 2D image randomly and sparsely scattered with pixels. given a point on the image I need to find the distance to the closest pixel that is not in the background color (black). What is the fastest way to do this? The only method I could come up with is building a kd-tree for the pixels. but I would really want to avoid such expensive preprocessing. also it seems that a kd-tree gives me more than I need. I only need the distance to something and I don't care about what this something is. Ok this sounds interesting. I made a c++ version of a soulution I don't know if this helps you. I think it works fast enough as it's almost instant on a 800*600 matrix. If you have any questions just ask. Sorry for any mistakes I've made it's a 10min code... This is a iterative version (I was planing on making a recursive one too but I've changed my mind). The algorithm could be improved by not adding any point to the points array that is to a larger distance from the starting point then the min_dist but this involves calculating for each pixel (despite it's color) the distance from the starting point. Hope that helps //(c++ version) #include<iostream> #include<cmath> #include<ctime> using namespace std; //ITERATIVE VERSION //picture witdh&height #define width 800 #define height 600 //indexex int ij; //initial point coordinates int xy; //variables to work with the array int pu; //minimum dist double min_dist=2000000000; //array for memorising the points added struct point{ int x; int y; } points[width*height]; double dist; bool viz[width][height]; // direction vectors used for adding adjacent points in the ""points"" array. int dx[8]={110-1-1-101}; int dy[8]={01110-1-1-1}; int knXnY; //we will generate an image with white&black pixels (0&1) bool image[width-1][height-1]; int main(){ srand(time(0)); //generate the random pic for(i=1;i<=width-1;i++) for(j=1;j<=height-1;j++) if(rand()%10001<=9999) //9999/10000 chances of generating a black pixel image[i][j]=0; else image[i][j]=1; //random coordinates for starting x&y x=rand()%width; y=rand()%height; p=1;u=1; points[1].x=x; points[1].y=y; while(p<=u){ for(k=0;k<=7;k++){ nX=points[p].x+dx[k]; nY=points[p].y+dy[k]; //nX&nY are the coordinates for the next point //if we haven't added the point yet //also check if the point is valid if(nX>0&&nY>0&&nX<width&&nY<height) if(viz[nX][nY] == 0 ){ //mark it as added viz[nX][nY]=1; //add it in the array u++; points[u].x=nX; points[u].y=nY; //if it's not black if(image[nX][nY]!=0){ //calculate the distance dist=(x-nX)*(x-nX) + (y-nY)*(y-nY); dist=sqrt(dist); //if the dist is shorter than the minimum we save it if(dist<min_dist) min_dist=dist; //you could save the coordinates of the point that has //the minimum distance too like sX=nX; sY=nY; } } } p++; } cout<<""Minimum dist:""<<min_dist<<""\n""; return 0; }  Search ""Nearest neighbor search"" first two links in Google should help you. If you are only doing this for 1 pixel per image I think your best bet is just a linear search 1 pixel width box at time outwards. You can't take the first point you find if your search box is square. You have to be careful  Personally I'd ignore MusiGenesis' suggestion of a lookup table. Calculating the distance between pixels is not expensive particularly as for this initial test you don't need the actual distance so there's no need to take the square root. You can work with distance^2 i.e: r^2 = dx^2 + dy^2 Also if you're going outwards one pixel at a time remember that: (n + 1)^2 = n^2 + 2n + 1 or if nx is the current value and ox is the previous value:  nx^2 = ox^2 + 2ox + 1 = ox^2 + 2(nx - 1) + 1 = ox^2 + 2nx - 1 => nx^2 += 2nx - 1 It's easy to see how this works: 1^2 = 0 + 2*1 - 1 = 1 2^2 = 1 + 2*2 - 1 = 4 3^2 = 4 + 2*3 - 1 = 9 4^2 = 9 + 2*4 - 1 = 16 5^2 = 16 + 2*5 - 1 = 25 etc... So in each iteration you therefore need only retain some intermediate variables thus: int dx2 = 0 dy2 r2; for (dx = 1; dx < w; ++dx) { // ignoring bounds checks dx2 += (dx << 1) - 1; dy2 = 0; for (dy = 1; dy < h; ++dy) { dy2 += (dy << 1) - 1; r2 = dx2 + dy2; // do tests here } } Tada! r^2 calculation with only bit shifts adds and subtracts :) Of course on any decent modern CPU calculating r^2 = dx*dx + dy*dy might be just as fast as this...  I would do a simple lookup table - for every pixel precalculate distance to the closest non-black pixel and store the value in the same offset as the corresponding pixel. Of course this way you will need more memory. The point is to *avoid* lengthy processing operations which are O(num-of-points) Ok I thought preprocessing would be done much more rarely compared to finding the distance(s).  I'm sure this could be done better but here's some code that searches the perimeter of a square around the centre pixel examining the centre first and moving toward the corners. If a pixel isn't found the perimeter (radius) is expanded until either the radius limit is reached or a pixel is found. The first implementation was a loop doing a simple spiral around the centre point but as noted that doesn't find the absolute closest pixel. SomeBigObjCStruct's creation inside the loop was very slow - removing it from the loop made it good enough and the spiral approach is what got used. But here's this implementation anyway - beware little to no testing done. It is all done with integer addition and subtraction. - (SomeBigObjCStruct *)nearestWalkablePoint:(SomeBigObjCStruct)point { typedef struct _testPoint { // using the IYMapPoint object here is very slow int x; int y; } testPoint; // see if the point supplied is walkable testPoint centre; centre.x = point.x; centre.y = point.y; NSMutableData *map = [self getWalkingMapDataForLevelId:point.levelId]; // check point for walkable (case radius = 0) if(testThePoint(centre.x centre.y map) != 0) // bullseye return point; // radius is the distance from the location of point. A square is checked on each iteration radius units from point. // The point with y=0 or x=0 distance is checked first i.e. the centre of the side of the square. A cursor variable // is used to move along the side of the square looking for a walkable point. This proceeds until a walkable point // is found or the side is exhausted. Sides are checked until radius is exhausted at which point the search fails. int radius = 1; BOOL leftWithinMap = YES rightWithinMap = YES upWithinMap = YES downWithinMap = YES; testPoint leftCentre upCentre rightCentre downCentre; testPoint leftUp leftDown rightUp rightDown; testPoint upLeft upRight downLeft downRight; leftCentre = rightCentre = upCentre = downCentre = centre; int foundX = -1; int foundY = -1; while(radius < 1000) { // radius increases. move centres outward if(leftWithinMap == YES) { leftCentre.x -= 1; // move left if(leftCentre.x < 0) { leftWithinMap = NO; } } if(rightWithinMap == YES) { rightCentre.x += 1; // move right if(!(rightCentre.x < kIYMapWidth)) { rightWithinMap = NO; } } if(upWithinMap == YES) { upCentre.y -= 1; // move up if(upCentre.y < 0) { upWithinMap = NO; } } if(downWithinMap == YES) { downCentre.y += 1; // move down if(!(downCentre.y < kIYMapHeight)) { downWithinMap = NO; } } // set up cursor values for checking along the sides of the square leftUp = leftDown = leftCentre; leftUp.y -= 1; leftDown.y += 1; rightUp = rightDown = rightCentre; rightUp.y -= 1; rightDown.y += 1; upRight = upLeft = upCentre; upRight.x += 1; upLeft.x -= 1; downRight = downLeft = downCentre; downRight.x += 1; downLeft.x -= 1; // check centres if(testThePoint(leftCentre.x leftCentre.y map) != 0) { foundX = leftCentre.x; foundY = leftCentre.y; break; } if(testThePoint(rightCentre.x rightCentre.y map) != 0) { foundX = rightCentre.x; foundY = rightCentre.y; break; } if(testThePoint(upCentre.x upCentre.y map) != 0) { foundX = upCentre.x; foundY = upCentre.y; break; } if(testThePoint(downCentre.x downCentre.y map) != 0) { foundX = downCentre.x; foundY = downCentre.y; break; } int i; for(i = 0; i < radius; i++) { if(leftWithinMap == YES) { // LEFT Side - stop short of top/bottom rows because up/down horizontal cursors check that line // if cursor position is within map if(i < radius - 1) { if(leftUp.y > 0) { // check it if(testThePoint(leftUp.x leftUp.y map) != 0) { foundX = leftUp.x; foundY = leftUp.y; break; } leftUp.y -= 1; // moving up } if(leftDown.y < kIYMapHeight) { // check it if(testThePoint(leftDown.x leftDown.y map) != 0) { foundX = leftDown.x; foundY = leftDown.y; break; } leftDown.y += 1; // moving down } } } if(rightWithinMap == YES) { // RIGHT Side if(i < radius - 1) { if(rightUp.y > 0) { if(testThePoint(rightUp.x rightUp.y map) != 0) { foundX = rightUp.x; foundY = rightUp.y; break; } rightUp.y -= 1; // moving up } if(rightDown.y < kIYMapHeight) { if(testThePoint(rightDown.x rightDown.y map) != 0) { foundX = rightDown.x; foundY = rightDown.y; break; } rightDown.y += 1; // moving down } } } if(upWithinMap == YES) { // UP Side if(upRight.x < kIYMapWidth) { if(testThePoint(upRight.x upRight.y map) != 0) { foundX = upRight.x; foundY = upRight.y; break; } upRight.x += 1; // moving right } if(upLeft.x > 0) { if(testThePoint(upLeft.x upLeft.y map) != 0) { foundX = upLeft.x; foundY = upLeft.y; break; } upLeft.y -= 1; // moving left } } if(downWithinMap == YES) { // DOWN Side if(downRight.x < kIYMapWidth) { if(testThePoint(downRight.x downRight.y map) != 0) { foundX = downRight.x; foundY = downRight.y; break; } downRight.x += 1; // moving right } if(downLeft.x > 0) { if(testThePoint(upLeft.x upLeft.y map) != 0) { foundX = downLeft.x; foundY = downLeft.y; break; } downLeft.y -= 1; // moving left } } } if(foundX != -1 && foundY != -1) { break; } radius++; } // build the return object if(foundX != -1 && foundY != -1) { SomeBigObjCStruct *foundPoint = [SomeBigObjCStruct mapPointWithX:foundX Y:foundY levelId:point.levelId]; foundPoint.z = [self zWithLevelId:point.levelId]; return foundPoint; } return nil; }  Yes the Nearest neighbor search is good but does not guarantee you'll find the 'nearest'. Moving one pixel out each time will produce a square search - the diagonals will be farther away than the horizontal / vertical. If this is important you'll want to verify - continue expanding until the absolute horizontal has a distance greater than the 'found' pixel and then calculate distances on all non-black pixels that were located.  You didn't specify how you want to measure distance. I'll assume L1 (rectilinear) because it's easier; possibly these ideas could be modified for L2 (Euclidean). If you're only doing this for relatively few pixels then just search outward from the source pixel in a spiral until you hit a nonblack one. If you're doing this for many/all of them how about this: Build a 2-D array the size of the image where each cell stores the distance to the nearest nonblack pixel (and if necessary the coordinates of that pixel). Do four line sweeps: left to right right to left bottom to top and top to bottom. Consider the left to right sweep; as you sweep keep a 1-D column containing the last nonblack pixel seen in each row and mark each cell in the 2-D array with the distance to and/or coordinates of that pixel. O(n^2). Alternatively a k-d tree is overkill; you could use a quadtree. Only a little more difficult to code than my line sweep a little more memory (but less than twice as much) and possibly faster. I don't see how the sweep algorithm is correct. If the closest pixel is in some diagonal direction then this method would not find it. It would only find the pixels on the two axis going from the point.  As Pyro says search the perimeter of a square that you keep moving out one pixel at a time from your original point (i.e. increasing the width and height by two pixels at a time). When you hit a non-black pixel you calculate the distance (this is your first expensive calculation) and then continue searching outwards until the width of your box is twice the distance to the first found point (any points beyond this cannot possibly be closer than your original found pixel). Save any non-black points you find during this part and then calculate each of their distances to see if any of them are closer than your original point. In an ideal find you only have to make one expensive distance calculation. Update: Because you're calculating pixel-to-pixel distances here (instead of arbitrary precision floating point locations) you can speed up this algorithm substantially by using a pre-calculated lookup table (just a height-by-width array) to give you distance as a function of x and y. A 100x100 array costs you essentially 40K of memory and covers a 200x200 square around the original point and spares you the cost of doing an expensive distance calculation (whether Pythagorean or matrix algebra) for every colored pixel you find. This array could even be pre-calculated and embedded in your app as a resource to spare you the initial calculation time (this is probably serious overkill). Update 2: Also there are ways to optimize searching the square perimeter. Your search should start at the four points that intersect the axes and move one pixel at a time towards the corners (you have 8 moving search points which could easily make this more trouble than it's worth depending on your application's requirements). As soon as you locate a colored pixel there is no need to continue towards the corners as the remaining points are all further from the origin. After the first found pixel you can further restrict the additional search area required to the minimum by using the lookup table to ensure that each searched point is closer than the found point (again starting at the axes and stopping when the distance limit is reached). This second optimization would probably be much too expensive to employ if you had to calculate each distance on the fly. If the nearest pixel is within the 200x200 box (or whatever size works for your data) you will only search within a circle bounded by the pixel doing only lookups and <>comparisons. This is actually the best answer yet. Thank you."
291,A,"java/swing: Shape questions: serializing and combining I have two questions about java.awt.Shape. Suppose I have two Shapes shape1 and shape2. How can I serialize them in some way so I can save the information to a file and later recreate it on another computer? (Shape is not Serializable but it does have the getPathIterator() method which seems like you could get the information out but it would be kind of a drag + I'm not sure how to reconstruct a Shape object afterwards.) How can I combine them into a new Shape so that they form a joint boundary? (e.g. if shape1 is a large square and shape2 is a small circle inside the square I want the combined shape to be a large square with a small circular hole) I think I figured out an answer to the 2nd part of my question: Shape shape1 shape2; shape1 = ...; shape2 = ...; Area area = new Area(shape1); area.subtract(new Area(shape2)); // ""area"" is now a Shape that is the difference between the two shapes.  I believe you can reconstruct a Shape from path information with java.awt.geom.Path2D.Double. However it may not be as efficient as specific implementations. To be serialisable without special work from all classes that have a Shape as a field then you would need to ensure that all constructed shapes serialisable subclasses of the provided Shapes that initialise data in a readObject method. If there are cases where you need to send data to the constructor then you will need ""serial proxies"" (I don't think this is necessary in this case). It might well be better to serialise the underlying model data. Shapes are generally constructed transiently. But how will such a path behave when the input one contains splines or Bezier curves ? Won't it be interpolated and loose its vectorial abilities ? @Riduidel I think `getPathIterator(AffineTransform)` returns those segments but `getPathIterator(AffineTransformdouble)` does not. I guess in general a `Shape` may return an approximate path."
292,A,"Is there any HTML code that would display an ellipse or a rounded rectangle? I am not sure if it's possible at all in HTML but I would still ask it here: Is there any HTML code that would stand for an ellipse or a rounded rectangle? Hilarious. Image tag. Lol. If you use HTML and CSS you can do this. If you don't mind using browser-specific CSS you can get it working in Firefox with -moz Chrome and Safari with -webkit and IE9 and Opera 10.5 with the CSS 3 stuff that does not start with a hyphen. <!DOCTYPE html PUBLIC ""-//W3C//DTD XHTML 1.0 Strict//EN"" ""http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd""> <html xmlns=""http://www.w3.org/1999/xhtml""> <head> <title> Rounded </title> <style type=""text/css""> div { -moz-border-radius-topleft: 6px; -webkit-border-top-left-radius: 6px; border-top-left-radius: 6px; -moz-border-radius-bottomleft: 6px; -webkit-border-bottom-left-radius: 6px; border-bottom-left-radius: 6px; -moz-border-radius-topright: 6px; -webkit-border-top-right-radius: 6px; border-top-right-radius: 6px; -moz-border-radius-bottomright: 6px; -webkit-border-bottom-right-radius: 6px; border-bottom-right-radius: 6px; border:solid 1px black; padding:10px; background-color:#efefef; } </style> </head> <body> <div>I'm rounded!</div> </body> </html> Thank You Chris!!! Note that IE9 doesn't even have a release date yet...I'd guess not until sometime in 2011...  No. There isn't. Your other post contradicts this one! :P  You could get close to either of those by using the trick found here (allows you to render arbitrarily sized/positioned right triangles using divs) Lots and lots of divs with relatively small borders. It would take a long time to hard code all the heights and widths but you could write a script to generate the html code for you. Of course the easiest and quickest (in terms of development time time needed to download the page and likely even rendering time) solution would be to use something other than pure html as the other folks here have already suggested. LOL. Awesome trick. Simpy amazing!!!!  Border radius in CSS3 will allow you to do this in most browsers (apart from IE... /spit). http://www.css3.info/preview/rounded-border/ HTML5 provides a canvas tag which will allow something similar to be drawn using Javascript. Again browser support is still on-going. You'll likely never be able to do what you're asking in pure HTML however. As of 2010/04/10 only Opera 10.5 will do this with CSS3 (and I *think* the IE9 preview). For Chrome 5.0.371.0 Safari 4.0.5 and FF 3.6.3 you need to use the vendor extensions.  On another thought it's quite possible! There you go: http://virkkunen.net/b/oh-dear.html Pure HTML! It doesn't even use any new-fangled CSS or JavaScript or whateverscript. Please don't actually *use* it... WOW Matti that's something!!! Than You very much.  Yes Canvas. But it's really the Canvas HTML tag coupled with Javascript. Read more about CANVAS here http://en.wikipedia.org/wiki/Canvas_element It's pretty sad that it took so long to get a canvas! Not to be a spoilsport but let's not forget this will not work on IE without a load of extra JavaScript for canvas emulation."
293,A,"TStatusBar flickers when calling Update procedure. Ways to painlessly fix this So here is the discussion I have just read: http://www.mail-archive.com/delphi@delphi.org.nz/msg02315.html BeginUpdate and EndUpdate is not thi procedures I need ... Overriding API Call? I tried to get Update procedures code from ComCtrls unit nut did not found... Maybe you could post here a code to fix thi flicker of statusbar compoent if the only text changes in it? I mean - something like TextUpdate or some kind of TCanvas method or PanelsRepaint ... ? The flickering is caused by this code: Repeat BlockRead(Fp BuffArrayDebug[LineIndex] DataCapac TestByteBuff); // DataCapac = SizeOf(DWORD) ProgressBar1.StepIt; if RAWFastMode.Checked then begin // checks for fast mode and modifyies progressbar if BuffArrayDebug[LineIndex] = 0 then begin ProgressBar2.Max := FileSize(Fp) - DataCapac; ProgressBar2.Position := (LineIndex + 1) * DataCapac; LineDecr := True; end; end else begin ProgressBar2.Max := FileSize(Fp); ProgressBar2.Position := LineIndex * DataCapac end; if PreviewOpn.Caption = '<' then begin // starts data copying to preview area if expanded Memo1.Lines.BeginUpdate; if (LineIndex mod DataCapac) > 0 then HexMerge := HexMerge + ByteToHex(BuffArrayDebug[LineIndex]) else begin Memo1.Lines.Add(HexMerge); HexMerge := ''; end; Memo1.Lines.EndUpdate; end; StatusBar1.Panels[0].Text := 'Line: ' + Format('%.7d'[LineIndex]) + ' | Data: ' + Format('%.3d'[BuffArrayDebug[LineIndex]]) + ' | Time: ' + TimeToStr(Time - TimeVarStart); StatusBar1.Update; if FindCMDLineSwitch(ParamStr(1)) then begin TrayIcon.BalloonTitle := 'Processing ' + ExtractFileName(RAWOpenDialog.FileName) + ' and reading ...'; TrayIcon.BalloonHint := 'Current Line: ' + inttostr(LineIndex) + #10#13 + ' Byte Data: ' + inttostr(TestByteBuff) + #10#13 + ' Hex Data: ' + ByteToHex(TestByteBuff); TrayIcon.ShowBalloonHint; end; Inc(LineIndex); Until EOF(Fp); Any ideas? There was comment with this link ( http://www.stevetrefethen.com/blog/UsingTheWSEXCOMPOSITEWindowStyleToEliminateFlickerOnWindowsXP.aspx ) and there is procedure that works ( no flickering whastsoever ) BUT IT IS VVVVVVVEEEEEERRRRRRYYYYYY SLOW!  1 type 2 TMyForm = class(TForm) 3 protected 4 procedure CreateParams(var Params: TCreateParams); override; 5 end; 6 7 ... 8 9 procedure TMyForm.CreateParams(var Params: TCreateParams); 10 begin 11 inherited; 12 // This only works on Windows XP and above 13 if CheckWin32Version(5 1) then 14 Params.ExStyle := Params.ExStyle or WS_EX_COMPOSITED; 15 end; 16 Also - the target is not the form but the StatusBar ... how to assign this method to statusbar? The only time I see flickering in my statusbar is when I'm trying to force the repaint faster than the refresh of the monitor. Is that what your talking about? If not I'm not sure what your seeing. Could you provide a sample program/code that demonstrates the problem? @HX_unbanned: The 4 ms of your monitor are the time to change from light to dark or vice versa. The graphics card does send a new picture 60 times per second (every 167 ms). Unless you are programming a game you should not need to deal with this at all other than making sure that updates are not unnecessarily fast. 20 or 30 times per second is plenty enough. Ok I see. I was thinking about this from the begining but I was ( and am ) afraid that this might leave some glitches on the statusbars text area ( ok ""text area"" is not the right term - better would be stringlists item text ) - like some not redrawn pixels ( leftovers from a character or something like that ) ... What version of Delphi are you using? I don't get flickering in my statusbar. What do you do to make it flicker? LockWindowUpdate will only help if you change the UI a lot and temporarily don't want it to be updated. @Lars: LockWindowUpdate() should not be used for reducing flicker. In fact it shouldn't be used at all in the days of desktop compositing. See http://blogs.msdn.com/oldnewthing/archive/2007/02/22/1742084.aspx http://blogs.msdn.com/oldnewthing/archive/2007/02/23/1747713.aspx and related postings by Raymond Chen. Hm yeah I just added it as possible part of possible solution ... Thanks for links mghie ;) Delphi 2009 Architect @Ryan  Yes excatly that what I am taling about. Do you have some synchronizing code ( my monitor has refresh time at 4ms )... You should check whether setting the TWinControl.DoubleBuffered property to True of the TStatusBar component will make it work. Also you can try enabling this property to the status bar's parent component (probably TForm). It's a blind shot - don't have access to the compiler from here. Another thought is to override the *WM_ERASEBKGND* message without calling inherited. First example found after using google: here. ----- Update after author's comment I finally got access to the compiler and now it's working. We can use the WS_EX_COMPOSITED solution. All you need is is to create your own custom component basing on TCustomStatusBar or just create a class wrapper and create your status bar instance in runtime. Like this: TMyStatusBar = class( TCustomStatusBar ) protected { Flickering work-around } procedure CreateParams( var Params : TCreateParams ) ; override ; end ; TForm1 = class( TForm ) // (...) private FStatusBar : TMyStatusBar ; // (...) end ; ------------- procedure TMyStatusBar.CreateParams( var Params : TCreateParams ) ; begin inherited ; if CheckWin32Version( 51 ) then Params.ExStyle := Params.ExStyle or WS_EX_COMPOSITED ; end ; ------------- { Creating component in runtime } procedure TForm1.FormCreate( Sender : TObject ) ; begin FStatusBar := TMyStatusBar.Create( Self ) ; FStatusBar.Parent := Self ; FStatusBar.Panels.Add ; end ; And it works for me. Good luck! Well I did take a look at this link and as I knew there is doublebuffered property which did not help me .... Also - Question is updated. Please give some other answers / comments!  The most important advise I can give you is to limit the number of status bar updates to maybe 10 or 20 per seconds. More will just cause unnecessary flicker without any benefit for the user - they can't process the information that fast anyway. OK with that out of the way: If you want to use the WS_EX_COMPOSITED extended style for the status bar you have basically three options: Create a descendent class that overrides the CreateParams() method and either install this into your IDE or (if you don't want to have it as its own component in the IDE) create the status bar at runtime. Create a descendent class with the same name TStatusBar in another unit override the CreateParams() method and add this unit after ComCtrls to the form units using status bar controls. This will create an instance of your own TStatusBar class instead of the one in ComCtrls. See this answer for another example of the technique hopefully its clear enough. Use the vanilla TStatusBar class and set the WS_EX_COMPOSITED extended style at runtime. I prefer the third option as the easiest one to experiment with so here's the sample code: procedure TForm1.FormCreate(Sender: TObject); var SBHandle: HWND; begin // This only works on Windows XP and above if CheckWin32Version(5 1) then begin // NOTE: the following call will create all necessary window handles SBHandle := StatusBar1.Handle; SetWindowLong(SBHandle GWL_EXSTYLE GetWindowLong(SBHandle GWL_EXSTYLE) or WS_EX_COMPOSITED); end; end; Edit: If you want your code to properly support recent Windows versions and visual styles you should not even think of handling WM_ERASEBKGND yourself - the usual technique involves an empty handler for that method and drawing the background in the WM_PAINT handler. This doesn't really work for standard controls like TStatusBar as the background has to be drawn somewhere. If you just skip the background drawing in the WM_ERASEBKGND handler you will need to use owner-drawn panels spanning all of the status bar otherwise the background simply won't be drawn and the window underneath will shine through. Besides the code for the owner-drawn panel would probably be very complex. Again a much better course of action would be to untangle the mess in your posted code properly separate worker from display code and reduce the update speed of your status bar texts to something reasonable. There just isn't any sense at all in going past the number of monitor updates per second and even this is sensible only for games and similar visualizations. hm than I could use Shell_Notify() or SHChangeNotify() ? Problem is that I dont know correct API Call syntax to do it ... And - I never read about ShelObj and ShellAPI units in depth ... @mghie - about Edit - emm yeah my goul is to make full ( although pretty curvehands.dll-like ) support for WinXP SP3 and Post-Windows XP versions so I mainly try to go through Shell resources not deal with GDI TCanvas and Graphics classes/units ... @HX_unbanned: Sorry I don't understand what Shell programming has to do with this at all. @mghie - Ok never mind then. I suppose I am thinking the wrong way after all. Don't get confused - I'm just learning delphi. imho all vcl components connect during runetime directly to os shell and takes from them all the resources. the os api then manages shells resources so i think that they are pretty dependent. Maybe I am wrong? If yes please give some links to read and learn. P.S. Sorry if I disinform you and other members and readers. We all are just humans.. @Darius - well take a look at first post - blockread() reads binary file even with optinal check parameter very fast and this frequency is important as this statusbar model can be used as instant debugger. Maybe you should do as mghie suggested: limit the frequency of StatusBar's updates - just call update every n-th time. fix spelling with OR statement mghie ;) Also - there is now some system lag when used this procedure - as warned in my link :( Hm any other solution? Mybe it could be done of course with a bit larger code but with WM_ERASEBKGND ... althought I did search ComCtrl unit but did not find full StatusBar.Update code ... As much as I understand the overriding of this API call would be simply not calling it in practice? In my opinion limiting the frequency of StatusBar updates won't make those data less informational. In fact such updates shouldn't be called that way cause those VCL calls are affecting the performance of whole I/O operation. Much better approach would be using a separate thread for doing this."
294,A,AvoidXferMode to replace a color on canvas I'm trying to replace a color for something that is drawn on a Canvas using AvoidXferMode. From the android docs it looks like it's exactly what I need: AvoidXfermode xfermode will draw the src everywhere except on top of the opColor or depending on the Mode draw only on top of the opColor. What I'm trying is something like this: Paint paint = new Paint(); paint.setColor(Color.RED); canvas.drawPaint(paint); // actually drawing a bitmap here paint.setXferMode(new AvoidXferMode(Color.RED 0 TARGET); paint.setColor(Color.GREEN); canvas.drawPaint(paint); However this just gives a red screen not green as I would expect (replacing the red with green). I guess I'm missing the point some where...Any suggestions? I'm currently having the same problem but I got it working by specifying 255 as tolerance instead. According to the API documentation this is wrong (It should draw the destination EVERYWHERE with this full-tolerance setting) but for some reason the value 255 does exactly what the value 0 should do. It doesn't seem to work for me setting the tolerance to 255 affects the whole image not only the target color...could you post some sample code? Yeah you are right. This depends on the color. My example worked with RED and BLUE on a BLACK background. Tolerance of 255 works then (at least for me) but if you use LTGRAY and DKGRAY for example then it fails. You can find my code here: http://stackoverflow.com/questions/2553694/avoidxfermode-tolerance I gave up with this AvoidXferMode and used PorterDuffXfermode instead with SRC_IN and SRC_OUT mode.  I finally found out what is issue is there are some clues here: AvoidXferMode Tolerance but it really hit me when I read this post http://stuffthathappens.com/blog/2010/06/04/android-color-banding/ by Eric Burke. The tolerance is failing because the view canvas is not in 8888 mode. That means that when you draw a color or bitmap on that canvas the colors get converted to the target pixel format and the color might slightly change. To fix this you can either switch the entire window pixel format as seen in Eric's post or you can draw onto a 8888 back buffer. Unfortunately the link to Eric's post is dead but Roman Guy also has a similar write up here: http://www.curious-creature.org/2010/12/08/bitmap-quality-banding-and-dithering/ @Trevor Updated the post you could have corrected it in stead of downvoting it. The gist of the solution is not in the link but in the answer itself. Link to Eric Burke's page is now dead. Many thanks for clarifying - now upvoted. Sorry but I didn't correct it myself because I genuinely didn't get the complete picture from your answer without being able to see Eric's page. No problem glad it's clear now ;)
295,A,Stretch Bitmap without anti-aliasing I found this example and used it http://stackoverflow.com/questions/1082884/how-do-you-resize-a-bitmap-under-net-cf-2-0/1082890#1082890 However my image looks horrible because of anti aliasing. It no longer looks pixelated as it should. How do i disable anti aliasing? Set the interpolation mode to nearest neighbor: g.InterPolationMode = InterpolationMode.NearestNeighbor
296,A,Background of UINavigationController view turns white My iPhone application uses the camera to take pictures which I suspect is somewhat memory-intensive. The app uses a custom background image for the view of its UINavigationController and after taking a few pictures the background goes all white. Any ideas on what I can do to stop this? Check to see if -didReceiveMemoryWarning is being called in any of your viewControllers. If it's a low memory problem that's probably the culprit.  Expanding on Ben Gottlieb's post is it really necessary for you to have the custom background image? That by itself is a big memory waste; I shutter (eh camera pun) to think of combining the two without releasing the picture before redisplaying the tableView. I understand what you mean. But it's not my choice: the client really wants it.  Unfortunately the project was very specific about having a background image so I had to retain it despite the memory waste. My work-around was to unload the background once you've entered the photos-view and then reload it before you leave. Not the most beautiful solution but it works really well.  I had exactly the same issue and resolved it by overriding UINavigationController. Then in the viewDidLoad method I simply create my background (ImageView) and add it to the view. If your UINavigationController is created from a nib just give it your custom class name in IB. I had to do that as I also have a custom navigationbar which you can only set in IB. **Override `UINavigationController`** how? Override was probably the wrong word. '@interface MyNavigationController : UINavigationController @end'
297,A,"How to mask a square image into an image with round corners in the iPhone SDK? How to mask a square image into an image with round corners? Both the methods work but the differences shows up depending on where you use it. For Ex: If you have a table view with the cells showing an image along with other labels etc. and you use layer to set the cornerRadius the scrolling will take a big hit. It gets jerky. I faced this issue when I was using Layer for an image in a table view cell and was trying to figure out the cause of that jerkiness only to find that CALayer was the culprit. Used the first solution of doing the stuff in drawRect explained by NilObject. That works like a charm with scrolling being smooth as silk. On the other hand if you want to use this in static views like popover view etc. layer is the easiest way to do it. As I said both the methods work well just that you need to decide based on where you want to use it.  I use this method. + (UIImage *)imageWithColor:(UIColor *)color andSize:(CGSize)size; { UIImage *img = nil; CGRect rect = CGRectMake(0 0 size.width size.height); UIGraphicsBeginImageContext(rect.size); CGContextRef context = UIGraphicsGetCurrentContext(); CGContextSetFillColorWithColor(context color.CGColor); CGContextFillRect(context rect); img = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return img; } According to memory management rules the image object returned by `UIGraphicsGetImageFromCurrentImageContext` doesn't need to be retained so you would be leaking memory there. It's the caller of `imageWithColor` which needs to retain it. Maybe you added the retain because of the `NSAutoreelasePool` (which also seems suspicious)?  You can use CoreGraphics to create a path for a round rectangle with this code snippet: static void addRoundedRectToPath(CGContextRef context CGRect rect float ovalWidth float ovalHeight) { float fw fh; if (ovalWidth == 0 || ovalHeight == 0) { CGContextAddRect(context rect); return; } CGContextSaveGState(context); CGContextTranslateCTM (context CGRectGetMinX(rect) CGRectGetMinY(rect)); CGContextScaleCTM (context ovalWidth ovalHeight); fw = CGRectGetWidth (rect) / ovalWidth; fh = CGRectGetHeight (rect) / ovalHeight; CGContextMoveToPoint(context fw fh/2); CGContextAddArcToPoint(context fw fh fw/2 fh 1); CGContextAddArcToPoint(context 0 fh 0 fh/2 1); CGContextAddArcToPoint(context 0 0 fw/2 0 1); CGContextAddArcToPoint(context fw 0 fw fh/2 1); CGContextClosePath(context); CGContextRestoreGState(context); } And then call CGContextClip(context); to clip it to the rectangle path. Now any drawing done including drawing an image will be clipped to the round rectangle shape. As an example assuming ""image"" is a UIImage and this is in a drawRect: method: CGContextRef context = UIGraphicsGetCurrentContext(); CGContextSaveGState(context); addRoundedRectToPath(context self.frame 10 10); CGContextClip(context); [image drawInRect:self.frame]; CGContextRestoreGState(context); up vote because math is cool That looks very useful but I'm not familiar with the CG library. If I have a UIImageView/UIImage where do I go from there? You would want to implement the drawRect method on the view with the code I added above. Some minor corrections - the drawRect example contains two errors. Call addRoundedRectToPath (not addRoundRectToPath) and to save state call CGContextSaveGState (not CGGraphicsSaveGState). It is easier now. The system provides +[UIBezierPath bezierPathWithRoundedRect:cornerRadius] for this as I describe below.  See this Post - Very simple answer How to set round corners in UI images in iphone UIImageView * roundedView = [[UIImageView alloc] initWithImage: [UIImage imageNamed:@""wood.jpg""]]; // Get the Layer of any view CALayer * l = [roundedView layer]; [l setMasksToBounds:YES]; [l setCornerRadius:10.0];  Here is an even easier method that is available in iPhone 3.0 and up. Every View-based object has an associated layer. Each layer can have a corner radius set this will give you just what you want: UIImageView * roundedView = [[UIImageView alloc] initWithImage: [UIImage imageNamed:@""wood.jpg""]]; // Get the Layer of any view CALayer * layer = [roundedView layer]; [layer setMasksToBounds:YES]; [layer setCornerRadius:10.0]; // You can even add a border [layer setBorderWidth:4.0]; [layer setBorderColor:[[UIColor blueColor] CGColor]]; To use these methods you might need to add: #import <QuartzCore/QuartzCore.h> The CoreGraphics Method above posted by NilObject is faster when you are dealing with animating items. Apparently the built in corner radius is not as efficient. Thanks! If anyone runs into trouble with this be sure to #import in your implementation file else none of those methods will fly. Here you're allocating a UIImageView object. This is not an option when you're dealing with images in UITableViews. Thanks MagicSeth. It rocks! :-) it rounds only border - not image itself I always forget about this little gem. Thanks man.  I realize this is old news but just to boil it down a bit: There are two possible questions here: (1) how do I apply rounded corners to a UIView (such as a UIImageView) which will be displayed on screen and (2) how do I mask a square image (that is a UIImage) to produce a new image with rounded corners. For (1) the easiest course is to use CoreAnimation and set the view.layer.cornerRadius property  // Because we're using CoreAnimation we must include QuartzCore.h // and link QuartzCore.framework in a build phases #import <QuartzCore/QuartzCore.h> // start with an image UIImage * fooImage = [UIImage imageNamed:@""foo.png""]; // put it in a UIImageView UIView * view = [UIImageView alloc] initWithImage:fooImage]; // round its corners. This mask now applies to the view's layer's *background* view.layer.cornerRadius = 10.f // enable masksToBounds so the mask applies to its foreground the image view.layer.masksToBounds = YES; For (2) the best way is to use the UIKit graphics operations: // start with an image UIImage * fooImage = [UIImage imageNamed:@""foo.png""]; CGRect imageRect = CGRectMake(0 0 fooImage.size.width fooImage.size.height); // set the implicit graphics context (""canvas"") to a bitmap context for images UIGraphicsBeginImageContextWithOptions(imageRect.sizeNO0.0); // create a bezier path defining rounded corners UIBezierPath * path = [UIBezierPath bezierPathWithRoundedRect:imageRect cornerRadius:10.f]; // use this path for clipping in the implicit context [path addClip]; // draw the image into the implicit context [fooImage drawInRect:imageRect]; // save the clipped image from the implicit context into an image UIImage *maskedImage = UIGraphicsGetImageFromCurrentImageContext(); // cleanup UIGraphicsEndImageContext(); What's tricky about problem (2) is that you might think you could do the whole operation using the view.layer.mask property in CoreAnimation. But you can't because the CALayer renderInContext: method which you'd use to generate a UIImage from the masked layer seems to ignore the mask. Worse the documentation for renderInContext: doesn't mention this and only alludes to the behavior for OSX 10.5. Some further context: the above approach to (2) is using UIKit's wrappers around more basic CoreGraphics functionality. You can do the same thing using the CoreGraphics calls directly – that is what the chosen answer is doing -- but then you need build the rounded rect bezier path manually from curves and lines and you also need to compensate for the fact that CoreGraphics uses a drawing coordinate system which is flipped with respect to UIKit's. This while working well mirrors the image horizontally. i removed the CGContextDrawImage((ctx imageRect theImage.CGImage); and replaced it with a [theImage drawInRect:CGRectMake(00imageRect.size.width imageRect.size.height)]; and this resolved the issue. @zachron thanks for the correction. I have re-written this answer to avoid raw CG entirely. That seems recommended based on reading the ""Drawing with Quartz and UIKit"" section in Apple's http://developer.apple.com/library/ios/#documentation/2DDrawing/Conceptual/DrawingPrintingiOS/GraphicsDrawingOverview/GraphicsDrawingOverview.html This is a far better answer than the accepted one as it uses built-in functionality rather than reinventing the wheel.  Very simple. self.profileImageView.layer.cornerRadius = self.profileImageView.frame.size.width / 2; self.profileImageView.clipsToBounds = YES; For every view there is a bundled layer property. So the first line of the above is to set the corner radius of the layer object (i.e. an instance of CALayer class). To make a circular image from a squared image the radius is set to the half of the width of UIImageView. For instance if the width of squared image is 100 pixels. The radius is set to 50 pixels. Secondly you have to set the clipsToBounds property to YES in order to make the layer works."
298,A,"Plotting color map with zip codes in R or Python I have some US demographic and firmographic data. I would like to plot zipcode areas in a state or a smaller region (e.g. city). Each area would be annotated by color and/or text specific to that area. The output would be similar to http://maps.huge.info/ but a) with annotated text; b) pdf output; c) scriptable in R or Python. Is there any package and code that allows me to do this? It isn't really clear what you want to do. Map by state? Have a map by zipcode? Both? Can you point to an example of what you want to do? Eduardo I edited my question above. Someone may have something more direct for you but I found O'Reilly's 'Data Mashups in R' very interesting... in part it's a spatial mapping of home foreclosure auctions. http://oreilly.com/catalog/9780596804770/ Plus no DRM and only $5...hard to beat that deal!  Depending on your application a long way around might be to use something like this: http://googlemapsmania.blogspot.com/2006/07/new-google-maps-us-zip-code-mashups.html To map your data. If that wasn't quite what you wanted you can get raw zip code shapefiles from census.gov and do it manually which is quite a pain. Also if you haven't seen it this is a neat way to interact with similar data and might offer some pointers: http://benfry.com/zipdecode/ nice too but these are visualizations of zip codes locations/boundaries. I am looking for a flexible way in R or Python to generate maps with custom-colored or text-annotated zip regions.  Daniel Levine at TechCrunch Trends has done nice things with the maps package in R. He has code available on his site too. Paul's suggestion of looking into Processing - which Ben Fry used to make zipdecode - is also a good one if you're up for learning a (Java-like) new language. thanks Matt yes the trends maps are zip code level but instead of shading zipcode areas I actually mapped the zipcodes to lat/long coords. Anyone is welcome to the code though. Note: the Trends maps are zip-code level.  There is a rich and sophisticated series of packages in R to plot do analysis and other functions related to GIS. One place to get started is the CRAN task view on Spatial Data: This is a complex and sometimes arcane world and takes some work to understand. If you are looking for a free very functional mapping application may I suggest: MapWindow ( mapwindow.com) I could not find anything in that CRAN view that would help me visualize zip code statistics on a map. The closest I got was the package muRL.  I am assuming you want static maps. 1) Get the shapefiles of the zip boundaries and state boundaries at census.gov: 2) Use the plot.heat function I posted in this SO question. For example (assumes you have the maryland shapefiles in the map subdirectory): library(maptools) ##substitute your shapefiles here state.map <- readShapeSpatial(""maps/st24_d00.shp"") zip.map <- readShapeSpatial(""maps/zt24_d00.shp"") ## this is the variable we will be plotting zip.map@data$noise <- rnorm(nrow(zip.map@data)) ## put the lab point x y locations of the zip codes in the data frame for easy retrieval labelpos <- data.frame(do.call(rbind lapply(zip.map@polygons function(x) x@labpt))) names(labelpos) <- c(""x""""y"") zip.map@data <- data.frame(zip.map@data labelpos) ## plot it png(file=""map.png"") ## plot colors plot.heat(zip.mapstate.mapz=""noise""breaks=c(-Inf-2-1012Inf)) ## plot text with(zip.map@data[sample(1:nrow(zip.map@data) 10)]  text(xyNAME)) dev.off() Very nice! Thanks for sharing this. I didn't see that question before. Thanks a lot. This was really nontrivial. The links to to shape files at www.census.gov are broken... it took me a while to find them. Try this URL: http://www.census.gov/cgi-bin/geo/shapefiles2010/main. Then use the dropdown to select ""Zip Code Tabulation Areas"" and ""States (and equivalent)."" Do you mind explaining how I can map values from a csv file that has a zip code column and some other data columns to use this (apologies if the answer is obvious but I don't really know R at all)? Specifically I am having trouble figuring out what I should put for `zip.map@data$noise <- rnorm(nrow(zip.map@data))` `labelpos <- data.frame(do.call(rbind lapply(zip.map@polygons function(x) x@labpt)))` and `zip.map@data <- data.frame(zip.map@data labelpos)`  In Python you can use shapefiles from the US census along with the basemap package. Here is an example of filling in states according to population.  Check out this excellent online visualization tool by IBM http://manyeyes.alphaworks.ibm.com/manyeyes/ EDIT FYI ManyEyes uses the Prefuse visualization toolkit for some of its viz. Even though it is a java-based framework they also provide a Flash/ActionScript tool for the web. that's cool. It has some things in common with http://gapminder.org Isnt that the same thing that was presented by Hans Rosling on TED a few years ago nice but manyeyes doesn't answer my question. I think it's very different from gapminder. Wattenberg is a visualization guy Rosling is a social scientist and the different approach shows.  There are many ways to do this in R (see the spatial view); many of these depend on the ""maps"" package. Check out this cool example of the US 2004 election. It ends up looking like this: Here's a slightly ugly example of a model that uses the ""maps"" package with ""lattice"". Andrew Gelman made some very nice plots like this. See for instance this blog post on red states/blue states and this follow up post. Here's a very simple example using the ""gmaps"" package which shows a map of Arrests by state for arrests per 100000 for Murder: require(gmaps) data(USArrests) attach(USArrests) grid.newpage() grid.frame(name=""map"") grid.pack(""map""USALevelPlot(states=rownames(USArrests)levels=Murdercol.fun=reds)height=unit(1'null')) grid.pack(""map""gradientLegendGrob(at=quantile(Murder)col.fun=reds)side=""bottom""height=unit(.2'npc')) detach(USArrests) It seems like these examples are at the county not zipcode level"
299,A,How to extract channel POD-type from a boost::gil homogeneous pixel type? I have a class templated on <PIXEL> assumed to be one of boost::gil's pixel types (for now only either gray8_pixel_t or gray16_pixel_t and I only expect to support homogeneous pixel types e.g rgb8_pixel_t in future). The class needs to get hold of unsigned char or unsigned short as appropriate to the pixel type; I assume this is buried somehwere in the pixel class but none of PIXEL::value_type PIXEL::channel_type or PIXEL::channel_type::value type seems to be what I want. What's the trick ? (I could of course use type-indirection via some template-specialized helper structs to get this info: template <typename PIXEL> struct types_for {}; template <> struct types_for<boost::gil::gray8_pixel_t> {typedef unsigned char channel_type;}; template <> struct types_for<boost::gil::gray16_pixel_t> {typedef unsigned short channel_type;}; but surely GIL must provide something equivalent already if I could just find it...) Aha.. this seems to do the trick: typename boost::gil::channel_type<PIXEL>::type
300,A,"C# GDI+ - Remove just above bottom half of ellipse on gloss method I have a gloss method and I'm trying to get a inverted half moon like effect. On the below code I'd like to remove just above bottom half of this ellipse and then draw it. Does anyone know how I might begin to do that? PS. The gloss is also turning out too white. I've tried messing with the alpha to no avail does anyone know any tricks to make the gloss more subtle? Thank you  /// <summary> /// Applies gloss to clock /// </summary> /// <param name=""e""></param> private void DrawGloss(PaintEventArgs e) { Graphics g = e.Graphics; float x = ((float)_CenterX / 1.1F) / _PI; float y = ((float)_CenterY / 1.2F) / _PI; float width = ((this.ClientSize.Width / 2)) + _hourLength; float height = ((this.ClientSize.Height / 2)) + _hourLength; RectangleF glossRect = new RectangleF( x + (float)(width * 0.10) y + (float)(height * 0.07) (float)(width * .8) (float)(height * 0.4)); LinearGradientBrush gradientBrush = new LinearGradientBrush(glossRect Color.FromArgb((int)50 Color.Transparent) glossColor LinearGradientMode.BackwardDiagonal); g.FillEllipse(gradientBrush glossRect); } FillPie could give you exactly half a circle. Otherwise you'd have to use FillClosedCurve or FillPath to get slightly less than half a circle or draw a half-circle onto an intermediate slightly smaller Bitmap and copy that back onto your main Bitmap with DrawImage. For a more subtle gloss effect I think you just need to change your LinearGradientBrush code to: LinearGradientBrush gradientBrush = new LinearGradientBrush(glossRect Color.Transparent Color.FromArgb((int)50 glossColor) LinearGradientMode.BackwardDiagonal); Your original gradient was going from fully transparent to full glossColor. +1 and FillPath is the most complex (and most powerful) approach. See http://msdn.microsoft.com/en-us/library/b5hek5ky.aspx Now the gloss seems so subtle as to hardly be there at all. :( I wonder if there isn't a happy medium? Thanks for your responses everyone! @stormist: try increasing the number in the ""Color.FromArgb((int)50 glossColor)"" parameter from 50 to 128 (or thereabouts). This number represents the alpha channel value (or the opacity) of the color you're deriving from glossColor and can range from 0 (completely transparent) to 255 (completely opaque or equal to glossColor). A value of 128 would be literally the ""happy medium"". @stormist: and you can thank me by clicking that little checkmark next to my answer. :) I forgot to mention one other useful trick: set the SmoothingMode property of your Graphics object to HighQuality before calling FillPie (or whatever other drawing methods you're using). The default mode is pixelly and generally pretty crappy-looking."
301,A,"Screenshot of the Nexus One from adb? My goal is to be able to type a one word command and get a screenshot from a rooted Nexus One attached by USB. So far I can get the framebuffer which I believe is a 32bit xRGB888 raw image by pulling it like this: adb pull /dev/graphics/fb0 fb0 From there though I'm having a hard time getting it converted to a png. I'm trying with ffmpeg like this: ffmpeg -vframes 1 -vcodec rawvideo -f rawvideo -pix_fmt rgb8888 -s 480x800 -i fb0 -f image2 -vcodec png image.png That creates a lovely purple image that has parts that vaguely resemble the screen but it's by no means a clean screenshot. Using my HTC Hero (and hence adjusting from 480x800 to 320x480) this works if I use rgb565 instead of 8888: ffmpeg -vframes 1 -vcodec rawvideo -f rawvideo -pix_fmt rgb565 -s 320x480 -i fb0 -f image2 -vcodec png image.png Yeah it works for me on the Magic/Dream/Hero but not on the N1. Thanks though. Works on SE Xperia X10 as well.  I believe all framebuffers to date are RGB 565 not 888.  I think rgb32torgb888.py should be  sys.stdout.write(colour[0]) sys.stdout.write(colour[1]) sys.stdout.write(colour[2]) Yes I had to swap the R and B channels to get correct colors from my N1.  A vastly easier solution for ICS is to use the following from the command line adb shell /system/bin/screencap -p /sdcard/screenshot.png adb pull /sdcard/screenshot.png screenshot.png This'll save the screenshot.png file in the current directory. Tested on a Samsung Galaxy SII & SII running 4.0.3. LoL perfect man very thankyou Faster: `adb shell screencap -p \| uuencode o | uudecode -o out.png` (needs linux+uudecode though base64 also works) @Benjamin this question has already >10 answers... and its just an improvement of Ben's answer suitable only for linux users. @ce4 you should write-up your comment as an answer. It is the best answer for this question. The answer by @ce4 takes about 1-2 seconds longer per picture for me so YMMV (unless the intention was being faster to type).  I hope my script might be of any use. I use it on my galaxy tab and it works perfectly but it is possible for you to change the default resolution. It requires the ""zsh"" shell though: #!/bin/zsh # These settings are for the galaxy tab. HRES=600 VRES=1024 usage() { echo ""Usage: $0 [ -p ] outputfile.png"" echo ""-- takes screenshot off your Galaxy Tab Android phone."" echo "" -p: portrait mode"" echo "" -r X:Y: specify resolution e.g. -r 480:640 specifies that your cellphone has 480x640 resolution."" exit 1 } PORTRAIT=0 # false by default umask 022 [[ ! -w . ]] && { echo ""*** Error: current directory not writeable."" usage } [[ ! -x $(which mogrify) ]] && { echo ""*** Error: ImageMagick (mogrify) is not in the PATH!"" usage } while getopts ""pr:"" myvar do [[ ""$myvar"" == ""p"" ]] && PORTRAIT=1 [[ ""$myvar"" == ""r"" ]] && { testhres=""${OPTARG%%:*}"" # remove longest-matching :* from end testvres=""${OPTARG##*:}"" # remove longest-matchung *: from beginning if [[ $testhres == <0-> && $testvres == <0-> ]] # Interval: from 0 to infinite. Any value would be: <-> then HRES=$testhres VRES=$testvres else echo ""Error! One of these values - '${testhres}' or '${testvres}' - is not numeric!"" usage fi } done shift $((OPTIND-1)) [[ $# < 1 ]] && usage outputfile=""${1}"" blocksize=$((HRES*4)) count=$((VRES)) adb pull /dev/graphics/fb0 fb0.$$ /bin/dd bs=$blocksize count=$count if=fb0.$$ of=fb0b.$$ /usr/bin/ffmpeg -vframes 1 -vcodec rawvideo -f rawvideo -pix_fmt rgb32 -s ${VRES}x${HRES} -i fb0b.$$ -f image2 -vcodec png ""${outputfile}"" if (( ${PORTRAIT} )) then mogrify -rotate 270 ""${outputfile}"" else mogrify -flip -flop ""${outputfile}"" fi /bin/rm -f fb0.$$ fb0b.$$ I was not very clear -- it will (probably) work on ANY android if you provide the resolution.  On the MyTouch Slide 3G I ended up with the red and blue channels swapped in my screenshots. Here's the correct ffmpeg incantation for anyone else in that situation: (the notable part: -pix_fmt bgr32) ffmpeg -vframes 1 -vcodec rawvideo -f rawvideo -pix_fmt bgr32 -s 320x480 -i fb0 -f image2 -vcodec png image.png Thanks to Patola for the handy shell script! At least for my phone no mogrification is necessary to correctly orient for portrait mode (320x480) and so the end of his script becomes: # assuming 'down' is towards the keyboard or usb jack # in landscape and protrait modes respectively (( ${PORTRAIT} )) || mogrify -rotate 270 ""${outputfile}"" /bin/rm -f fb0.$$ fb0b.$$  rgb565 instead of 8888 also work on emulator  It seems that frame buffer of N1 uses RGB32 encoding (32 bits per pixel). Here is my script using ffmpeg: adb pull /dev/graphics/fb0 fb0 dd bs=1920 count=800 if=fb0 of=fb0b ffmpeg -vframes 1 -vcodec rawvideo -f rawvideo -pix_fmt rgb32 -s 480x800 -i fb0b -f image2 -vcodec png fb0.png Another way derived from ADP1 method described here http://code.lardcave.net/entries/2009/07/27/132648/ adb pull /dev/graphics/fb0 fb0 dd bs=1920 count=800 if=fb0 of=fb0b python rgb32torgb888.py <fb0b >fb0b.888 convert -depth 8 -size 480x800 RGB:fb0b.888 fb0.png Python script 'rgb32torgb888.py': import sys while 1: colour = sys.stdin.read(4) if not colour: break sys.stdout.write(colour[2]) sys.stdout.write(colour[1]) sys.stdout.write(colour[0])  This might be related to the issue Reading binary Data from adb shell's stdout where adb attempts to do LF to CRLF conversion for you (it's probably just the windows version of adb). I personally had trouble with it conversion \n to \r\r\n so as a way to convert this it's good to either to use the code at [1] or to to use. for me running it with (in cygwin): adb shell 'cat /dev/graphics/fb0' | perl -pi -e 's/\r\r\n/\n/g' seemed to help other than that try comparing the Width & height to the size of the file. File size should be evenly divisible by Width * height if that's not the case then either the adb tool is automatically doing things for you or it's a more exotic format then rgb545 or rgb8888. if it's just a color issue (ie: everything in the result image is in the right spot) then you might want to consider swapping the Red & Blue channels as some systems (in general) use byte order BGRA instead of RGBA.  A little elaborate/excessive but it handles both screencap and framebuffer scenarios (as well as figuring out the resolution too). #!/bin/bash # # adb-screenshot - simple script to take screenshots of android devices # # Requires: 'ffmpeg' and 'adb' to be somewhere in the PATH # # Author: Kevin C. Krinke <kevin@krinke.ca> # License: Public Domain # globals / constants NAME=$(basename $0) TGT=~/Desktop/${NAME}.png SRC=/sdcard/${NAME}.png TMP=/tmp/${NAME}.$$ RAW=/tmp/${NAME}.raw FFMPEG=$(which ffmpeg) ADB=$(which adb) DD=$(which dd) USB_DEVICE="""" # remove transitory files if exist function cleanup () { [ -f ""${RAW}"" ] && rm -f ""${RAW}"" [ -f ""${TMP}"" ] && rm -f ""${TMP}"" [ -z ""$1"" ] && die ""aborting process now."" exit 0 } # exit with an error function die () { echo ""Critical Error: $@"" exit 1 } # catch all signals and cleanup / dump trap cleanup \ SIGHUP SIGINT SIGQUIT SIGILL SIGTRAP SIGABRT SIGEMT SIGFPE \ SIGKILL SIGBUS SIGSEGV SIGSYS SIGPIPE SIGALRM SIGTERM SIGURG \ SIGSTOP SIGTSTP SIGCONT SIGCHLD SIGTTIN SIGTTOU SIGIO SIGXCPU \ SIGXFSZ SIGVTALRM SIGPROF SIGWINCH SIGINFO SIGUSR1 SIGUSR2 # adb is absolutely required [ -x ""${ADB}"" ] || die ""ADB is missing!"" # cheap getopt while [ $# -gt 0 ] do case ""$1"" in ""-h""|""--help"") echo ""usage: $(basename $0) [-h|--help] [-s SERIAL] [/path/to/output.png]"" exit 1 ;; ""-s"") [ -z ""$2"" ] && die ""Missing argument for option \""-s\"" try \""${NAME} --help\"""" HAS_DEVICE=$(${ADB} devices | grep ""$2"" ) [ -z ""${HAS_DEVICE}"" ] && die ""No device found with serial $2"" USB_DEVICE=""$2"" ;; *) [ -n ""$1"" -a -d ""$(dirname $1)"" ] && TGT=""$1"" ;; esac shift done # prep target with fire [ -f ""${TGT}"" ] && rm -f ""${TGT}"" # tweak ADB command line if [ -n ""${USB_DEVICE}"" ] then ADB=""$(which adb) -s ${USB_DEVICE}"" fi # calculate resolution DISPLAY_RAW=$(${ADB} shell dumpsys window) HRES=$(echo ""${DISPLAY_RAW}"" | grep SurfaceWidth | head -1 | perl -pe 's/^.*\bSurfaceWidth\:\s*(\d+)px\b.*$/$1/') VRES=$(echo ""${DISPLAY_RAW}"" | grep SurfaceHeight | head -1 | perl -pe 's/^.*\bSurfaceHeight\:\s*(\d+)px\b.*$/$1/') RES=${HRES}x${VRES} # check for screencap binary HAS_SCREENCAP=$(${ADB} shell ""[ -x /system/bin/screencap ] && echo 1 || echo 0"" | perl -pe 's/\D+//g') if [ ""$HAS_SCREENCAP"" == ""1"" ] then # use screencap to get the image easy-peasy echo -n ""Getting ${RES} screencap... "" ( ${ADB} shell /system/bin/screencap ${SRC} 2>&1 ) > /dev/null [ ""$?"" != ""0"" ] && die ""Failed to execute screencap"" ( ${ADB} pull ${SRC} ${TMP} 2>&1 ) > /dev/null [ ""$?"" != ""0"" ] && die ""Failed to pull png image"" ( ${ADB} shell rm ${SRC} 2>&1 ) > /dev/null [ ""$?"" != ""0"" ] && die ""Failed to remove png image"" mv ${TMP} ${TGT} echo ""wrote: ${TGT}"" else # fetch a framebuffer snapshot # ffmpeg is only needed if device is pre-ICS [ -x ""${FFMPEG}"" ] || die ""FFMPEG is missing!"" [ -x ""${DD}"" ] || die ""DD is missing!"" echo -n ""Getting ${RES} framebuffer... "" ( ${ADB} pull /dev/graphics/fb0 ${RAW} 2>&1 ) > /dev/null [ ""$?"" != ""0"" ] && die ""Failed to pull raw image data"" # calculate dd parameters COUNT=$((HRES*4)) BLOCKSIZE=$((VRES)) ( ${DD} bs=${BLOCKSIZE} count=${COUNT} if=${RAW} of=${TMP} 2>&1 ) > /dev/null [ ""$?"" != ""0"" ] && die ""Failed to realign raw image data"" ( ${FFMPEG} -vframes 1 -vcodec rawvideo -f rawvideo -pix_fmt rgb32 -s ${RES} -i ${TMP} -f image2 -vcodec png ${TGT} 2>&1 ) > /dev/null [ ""$?"" != ""0"" ] && die ""Failed to encode PNG image"" echo ""wrote: ${TGT}"" fi # exit app normal cleanup 1  Actually there is another very simple ability to grab screenshot from your android device: write simple script 1.script like this: # Imports the monkeyrunner modules used by this program from com.android.monkeyrunner import MonkeyRunner MonkeyDevice # Connects to the current device returning a MonkeyDevice object device = MonkeyRunner.waitForConnection() # Takes a screenshot result = device.takeSnapshot() # Writes the screenshot to a file result.writeToFile('1.png''png') and call monkeyrunner 1.script.  If you have dos2unix installed then the below adb shell screencap -p | dos2unix > screen.png This is my favorite method - very simply! I set up an alias for everything but the filename for easy use whenever I need to take a screenshot.  A way to completely automatize this process is to create a script which adds the curent timestamp to the filename. This way you don't have to write the filename yourself all your screenshots have a different name and your screenshots are sorted by time. Example of bash script : #! /bin/bash filename=$(date +""_%Y-%m-%d-%H:%M"") /PATH_TO_ANDROID_SDK/platform-tools/adb -d shell screencap -p | perl -pe 's/\x0D\x0A/\x0A/g' > screenshot$filename.png This will create a file named like screenshot_2014-01-07-10:31.png"
302,A,Selecting proper toolkit for a 2D simulation project in Java I am looking for a toolkit that will allow me to design widgets containing 2D graphics for an elevator simulation in Java. Once created those widgets will be integrated with SWT Swing or QtJambi framework. Background information: I am developing an Elevator Simulator for fun. My main goal is to increase my knowledge of Java Eclipse IDE and more importantly concurrency. It is certainly fun and I enjoyed implementing this State Machine Pattern. Anyway I am at a point where I would like to see the Elevator on the screen and not limit myself to log its operations on the console. I will probably choose SWT Swing or QtJambi for the UI controls but I am wondering what to do with the graphical part of the simulation. You could get some abstract graphics out of a Graph visualisation tool such as JGraph. You could use this to visualise which state your elevator is in. However i'm not sure how flexible these sort of graph visualisation tools are and whether you can add your own graphics and animations.  You can use an SWT canvas (or Swing canvas or OpenGL canvas via JOGL ...) and set it up as an Observer of your simulation and whenever the simulation state changes you can redraw the new state. Yes this is certainly an option but do some specialized toolkit for those kind of simulation exist so I don't have to start from scratch? But at a basic level all you are doing is drawing a rectangle at a certain height within a lift-shaft rectangle. Maybe with doors opening and closing. Canvas as Observer is all you need.  Are you sure you actually want to be using widgets. Would using Graphics2D+friends and your own abstractions not be a better fit? This may be one option. Doing the graphic myself and then encapsulating into a widget. But I am wondering if some simulation graphic toolkits already exist.
303,A,"Scaling Image to multiple sizes for Deep Zoom Lets assume I have a bitmap with a square aspect and width of 2048 pixels. In order to create a set of files need by Silverlight's DeepZoomImageTileSource I need to scale this bitmap to 1024 then to 512 then to 256 etc down to 1 pixel image. There are two I suspect naive approaches:- For each image required scale the original full size image to the required size. However it seems excessive to be scaling the full image to the very small sizes. Having scaled from one level to the next discard the original image and scale each sucessive scaled image as the source of the next smaller image. However I suspect that this would generate images in the 256-64 range with poor fidelity than using option 1. Note unlike with the Deep Zoom Composer this tool is expected to act in an on-demand fashion hence it needs to complete in a reasonable timeframe (tops 30 seconds). On the pluse side I'm only creating a single multiscale image not a pyramid of mutliple high-res images. I am outside my comfort zone here any graphics experts got any advice? Am I wrong about point 2? Is point 1 reasonably performant and I'm worrying about nothing? Option 3? What you are essentially trying to do is create MipMaps (see http://en.wikipedia.org/wiki/Mipmap). If you start out with a power of 2 square image then scaling down the image to half the size then using the scaled down image to reduce its size by 2 again should give the same results as taking the original image and scaling it down by a factor of 4. Each pixel in the half sized image will be the average of 4 pixels in the original image. Each pixel in the quater sized image will be the average of 16 pixels. It doesn't matter if you take the average of 16 pixels or the average of 4 pixels which where the average of 4 other pixels. So I'd say you'd be fine with successively scaling images down as you mentioned in option 2. If you want to be sure then try both ways and compare the images. Thanks Martin. I see your point. I guess my attempt at making the question simple was a bit misleading. Actually the images will be typical sizes generated by scanners and digital cameras.  I realize this question is old and maybe there's a reason you haven't done this but if you get Microsoft's free Deep Zoom Composer it comes with a DLL for making Deep Zooms ""DeepZoomTools.dll"". It will create Deep Zoom compositions of a single image or composites of many images. (The class ImageCreator does the heavy lifting of resizing images). You should investigate the licensing implications if you're developing a commercial application but reusing code is always better than writing it yourself. I wasn't aware of that component. This project is currently working fine but I certainly will investigate whether swapping out my own code with this dll is a possibility. The fact that the tool itself is free and MS have actively encouraged devs to use it in their own apps leads me to believe that there should be no commercial issues with its use. Thanks for the heads up. ;)  About 1: This seems the best way. And it is not that excessive if you don't reload the source image each time. About 2: Yes you would looses (some) quality by scaling in steps. I think 30 sec should be sufficient to scale a picture (a few times). Any optimization would be in the area of caching the results. Thanks Henk I think in the absence of any other strong opinions (and have done a little testing) I'll go with option 1 since its the simplest to implement."
304,A,"OpenGL: Calculating z-value for 1:1 pixel ratio I have a 256x256 texture in my view frustum that I need to move to a z-position where the texture is replicated on-screen at ACTUAL size. What I have so far is:  const float zForTrueScale = -((itemSize/2) / tanf(DEGREES_TO_RADIANS(fieldOfView/2)) ) * 2; where itemSize is the size of my texture in world space (2.0 units). This is to calculate what Z (adjacent) is when itemSize/2 (opposite) is 1.0. I thought this would work. This formula shows a texture consistently too small by about 10% no matter what FOV I use. Thanks EDIT: I am moving around images in 3D space and need a seamless transition to the correct z-distance. I can't use orthagonal projection it has to be in a view frustum. Is your framebuffer square ? If not then you have a horizontal field of view and a vertical one. Are you using the correct one ? Also how do you know for sure you want it to be 2.0 units in world coordinates ? If your end-goal is to map it to pixels then you need to take the viewport into account and work back from that. I also agree with Pike65 that I don't get where the *2 is coming from. Yes apologies you are right about the trig... I guess my code is more broken than I thought! My view frustum is setup with a square aspect ratio and the texture vertices are just -1.0 to 1.0 both ways. I am working with ES so using a triangle strip just two triangles. Not doing anything to the model matrix starting with a blank slate... The *2 is because the trig is using the length of the opposite (half the item width) and then at the end I multiply it back out to get the full length. I could have just not halved the item width to begin with but it read better to me at the time. Anyway my question is ""how do I take the viewport into account and work back from that"". My framebuffer is square. I found that by adding 13.5/FOV to the end result gives me an accurate enough result for any FOV.. but I don't understand where 13.5 comes from it's just a horrible magic number sat in my code at the moment. For the trig: /2 makes sense to get half the item width. but tan() is half-item/z. There is no *2 involved there. What are your view/proj matrices looking like at the time of draw ? what quad do you pass in to draw the texture ?  Without knowing too much about your situation it's almost certainly easier to push an orthogonal projection onto the projection matrix stack draw your texture and then pop it off again. No I am moving around images in 3D space and need a seamless transition to the correct z-distance. I can't use orthagonal projection it has to be in 3D. Ah OK. Sorry - misunderstood. In that case: my trig is pretty rusty but is the multiplication at the end needed?  You can do this slightly more easily than messing about with fields of view as the info you need is held directly in the projection matrix. Thus given your projection matrix will look like this: xScale 0 0 0 0 yScale 0 0 0 0 -(zf + zn)/(zf - zn) -(2 * zf) / (zf-zn) 0 0 -1 0 Where z-Far (zf) and the z-Near (zn) xScale and yScale are known. To choose the correct Z-Depth we'll start off with making sure w ends up as 1. This way when we do the divide by w it won't change anything. Fortunately w is very easy to get at. It is simply the negative of the input z. Thus an input of -1 into z will return a w of 1. We'll assume you are using a resolution of 1024x768 and that the texture you want at the correct scale is 256x256. We'll also further assume that your rectangle is setup with top left at a position of -1 1 and bottom right at 1 -1. So lets plug these in and work out the z. if we use the following: -1 1 -1 1 1 -1 -1 1 we will get out something as follows. 1 / xScale -1 / yScale someZThatIsn'tImportant 1 -1 / xScale 1 / yScale someZThatIsn'tImportant 1 The viewport transform transforms those values such that -1 1 is 0 0 and 1 -1 is 1024768 So we can see that works by doing ((x + 1) / 2) * 1024 and ((1 - y) / 2) * 768 So if we assume an xScale of 3/4 and a yScale of 1 we can see that by plugging that in we'll get the following: For top left: x = -3/4 => ((-3/4 + 1) / 2) * 1024 => 128 y = 1 => ((1 - 1) / 2) * 768 => 0 For bottom right: x = 3/4 => ((3/4 + 1) / 2) * 1024 => 896 y = -1 => ((1 + 1) / 2) * 768 => 768 You can thus see that we have a 768x768 pixel image centered in the screen. Obviously to get 256x256 we need to get the w to be 3 so that post w divide we have those coordinates a 3rd of the size ((xScale * 1024) / 256 should be equal to (yScale * 768) / 256 to get a square projection. So if our final coordinates are as follows: -1 1 -3 1 and 1 -1 -3 1 we will get the following out (after w-divide): -0.25 0.333 unimportantZ 1 and 0.25 -0.333 unimportantZ 1 Run those through the screen equations above and we get For top left: x = -0.25 => ((-0.25 + 1) / 2) * 1024 => 384 y = 0.3333 => ((1 - 0.3333) / 2) * 768 => 256 For bottom right: x = 0.25 => ((0.25 + 1) / 2) * 1024 => 640 y = -0.333 => ((1 + 0.33333) / 2) * 768 => 512 640 - 384 = 256 512 - 256 = 256 Thus we now have the final rect at the correct pixel size... Wow nice answer thanks  Although it would be slow you could just use glDrawPixels() and be done with it. ...not if it's performance critical though!"
305,A,"Tackling alpha blend in OpenGL for better performance Since having blends is hitting perfomance of our game we tried several blending strategies for creating the ""illusion"" of blending. One of them is drawing a sprite every odd frame resulting in the sprite being visible half of the time. The effect is quit good. (You'd need a proper frame rate by the way else your sprite would be noticeably flickering) Despite that I would like to know if there are any good insights out there in avoiding blending in order to better the overal performance without compromising (too much) of the visual experience. It is not quite clear from your question what kind of application of blending hits your game's performance. Generally blending is blazingly fast. If your problems are particle system related then what is most likely to kill framerate is the number and size of particles drawn. Particularly lots of close up (and therefore large) particles will require high memory bandwidth and fill rate of the graphics card. I have implemented a particle system myself and while I can render tons of particles in the distance I feel the negative impact of e.g. flying through smoke (that will fill the entire screen because the viewer is amidst of it) very much on weaker hardware.  Is it the actual blending that's killing your performance? (i.e. video memory bandwidth) What games commonly do these days to handle lots of alpha blended stuff (think large explosions that cover whole screen): render them into a smaller texture (e.g. 2x2 smaller or 4x4 smaller than screen) and composite them back onto the main screen. Doing that might require rendering depth buffer of opaque surfaces into that smaller texture as well to properly handle intersections with opaque geometry. On some platforms (consoles) doing multisampling or depth buffer hackery might make that a very cheap operation; no such luck on regular PC though. See article from GPU Gems 3 for example: High-Speed Off-Screen Particles. Christer Ericson's blog post overviews a lot of optimization approaches as well: Optimizing the rendering of a particle system  Excellent article here about rendering particle systems quickly. It covers the smaller off screen buffer technique and suggest quite a few other approaches. You can read it here"
306,A,"How to render text into a box and replace excess with three dots I render text with a custom table cell renderer and I want to trim the text drawn if it would exceed the current cell width switching to a ""This text is to long..."" kind of representation where the three dots at the end never leaves the bounding box. My current algorithm is the following: FontMetrics fm0 = g2.getFontMetrics(); int h = fm0.getHeight(); int ellw = fm0.stringWidth(""\u2026""); int titleWidth = fm0.stringWidth(se.title); if (titleWidth > getWidth()) { for (int i = se.title.length() - 1; i > 0; i--) { String tstr = se.title.substring(0 i); int tstrw = fm0.stringWidth(tstr); if (tstrw + ellw < getWidth()) { g2.drawString(tstr 2 h); g2.drawString(""\u2026"" 2 + tstrw h); break; } } } else { g2.drawString(se.title 2 h); } Is there a better way of determining how many characters should I drawString() before switching to the ... (u2026) character? I can't think of another way to achieve the desired effect but you should be able to replace the 2 drawString() calls with one. It's been a while since I've written Java but I think the following should work. BTW I changed the variable names so I can read the code. :p FontMetrics metrics = g2.getFontMetrics(); int fontHeight = metrics.getHeight(); int titleWidth = metrics.stringWidth(se.title); if (titleWidth > getWidth()) { String titleString; for (int i = se.title.length() - 1; i > 0; i --) { titleString = se.title.substring(0 i) + ""\u2026""; titleWidth = metrics.stringWidth(titleString); if (titleWidth < getWidth()) { g2.drawString(titleString 2 fontHeight); break; } } } else { g2.drawString(se.title 2 fontHeight); }  This is the default behaviour of the default renderer for a table so I'm not sure I understand the reason for doing this. But I've created a renderer for dots on the left (""... text is too long"") which shows a more complete way to do what you want since it takes into account a possible Border on the renderer and the intercell spacing of the renderer. Check out the LeftDotRenderer. Thanks. The reason is that the first line (a title) is drawn in 16pt text and the second and third line (some details) are drawn as normal text within the cell. The LeftDotRenderer differs mainly in that it counts the size per character with instead of the entire string width. I will test it."
307,A,"Collapse/Expand Images Ok so this is super basic. I just got done implementing the collapsible panel using the MS AJAX Toolkit and was wondering if anyone knew where to get the collapse and expand images that they use in their demo? One would think these images would be distributed with the toolkit...if so - where are they found? Any recommendations for collapse/expand images in open-source-creative-commons domain? you can set them using the properties ExpandedImage=""~/images/collapse.jpg"" CollapsedImage=""~/images/expand.jpg"" the images can be found inside the toolkit if you want to add your custom image you can get them from http://www.iconfinder.net  you can find them if you view the source code on the sample page... no they're not included though. this is so you can provide your own images. on a side note have you tried implementing this functionality using jquery? :)"
308,A,Apps that support both DirectX 9 and 10 I have a noobish question for any graphics programmer. I am confused how some games (like Crysis) can support both DirectX 9 (in XP) and 10 (in Vista)? What I understand so far is that if you write a DX10 app then it can only runs in Vista. Maybe they have 2 code bases -- one written in DX9 and another in DX10? But isn't that an overkill? They have two rendering pipelines one using DX9 calls and one using DX10 calls. The APIs are not compatible though a majority of any game engine can be reused for either. If you want some Open Source examples of how different rendering pipelines are done look at something like Ogre3d which supports OpenGL DX9 and (soon)DX10 rendering.  The rendering layer of games is usually a fairly well isolated/abstracted part of the whole application. As far as the game engine is concerned each frame you are simply building up a list of conceptual objects (trees characters etc.). If the game engine chooses to render a particular object then it's up to the rendering layer how to actually translate that intent into DX draw calls. A DX10 rendering will generate a different set of draw calls to a DX9 layer but conceptually they are still performing the same action - 'render this tree'. Rendering is nicely abstracted because it's rare that you want to get any information back from the rendering layer once the 'render this tree' action is performed the game engine will just assume that the rendering looks correct. There is little need to handle different potential results from DX9/DX10 rendering calls because 99.9% of the information is going from the engine to the graphics system and the 0.1% that comes back likely takes the same form between the two APIs. The application setup is a little more icky because you've got to ask the system whether or not DX10 is supported and gracefully fall back on DX9 otherwise but this is standard fare for application setup (in the same way that the game has to pick a resolution refresh rate input device etc.).  It is likely that they have an abstraction layer and they develop against that. At run-time they instantiate the DX9 or DX10 wrapping concrete engines. I imagine their abstraction is positioned very close to the DirectX layer and simply provides DX9 with sensible manual implementations of DX10 functions or enhances DX9 logic when running on DX10.
309,A,"Calculating the Bounding Rectangle at an Angle of a Polygon I have the need to determine the bounding rectangle for a polygon at an arbitrary angle. This picture illustrates what I need to do: The pink rectangle is what I need to determine at various angles for simple 2d polygons. Any solutions are much appreciated! Edit: Thanks for the answers I got it working once I got the center points correct. You guys are awesome! By ""various angles"" do you mean the bounding rectangle has to be at a certain angle or the shape inside has to be at a certain angle? You'll have to fix some angles here else there are multiple solutions. The angle of the bounding rectangle is what varies. I've thought about rotating the polygon in the reverse direction then fitting a rectangle around it and rotating the points of the rectangle back but I think I'm getting messed up by identifying the center points correctly. Just rotate both about 00. You don't need the center point To get a bounding box with a certain angle rotate the polygon the other way round by that angle. Then you can use the min/max x/y coordinates to get a simple bounding box and rotate that by the angle to get your final result. From your comment it seems you have problems with getting the center point of the polygon. The center of a polygon should be the average of the coordinate sums of each point. So for points P1...PN calculate: xsum = p1.x + ... + pn.x; ysum = p1.y + ... + pn.y; xcenter = xsum / n; ycenter = ysum / n; To make this complete I also add some formulas for the rotation involved. To rotate a point (xy) around a center point (cx cy) do the following: // Translate center to (00) xt = x - cx; yt = y - cy; // Rotate by angle alpha (make sure to convert alpha to radians if needed) xr = xt * cos(alpha) - yt * sin(alpha); yr = xt * sin(alpha) + yt * cos(alpha); // Translate back to (cx cy) result.x = xr + cx; result.y = yr + cx; Beat me to the centre-point calculation. +1 for having the right answer. For this algorithm does it matter what point you rotate about? No it doesn't matter if you just want to know the size of the bounding box but it will help with the placement of the bounding box around the polygon. Even for bounding box placement it seems like as long as you rotate the box & shape back around the same point once you've placed it it'll work. I think your yr = yt * sin(alpha) + yt * cos(alpha); is wrong don't you want it to be: yr = xt * sin(alpha) + yt * cos(alpha); ? To make it work I had to rotate the rectangle using the centerx and y of the polygon which is where I was going wrong. I was rotating the rectangle around its center rather than the original center point of the poly. Thanks! Thanks corrected the yr formula. Glad I could help you. Emmett you're right about the rotation this saves some calculations.  To get a rectangle with minimal area enclosing a polygon you can use a rotating calipers algorithm. The key insight is that (unlike in your sample image so I assume you don't actually require minimal area?) any such minimal rectangle is collinear with at least one edge of (the convex hull of) the polygon.  To get the smallest rectangle you should get the right angle. This can acomplished by an algorithm used in collision detection: oriented bounding boxes. The basic steps: Get all vertices cordinates Build a covariance matrix Find the eigenvalues Project all the vertices in the eigenvalue space Find max and min in every eigenvalue space. For more information just google OBB ""colision detection"" Ps: If you just project all vertices and find maximum and minimum you're making AABB (axis aligned bounding box). Its easier and requires less computational effort but doesn't guarantee the minimum box.  I'm interpreting your question to mean ""For a given 2D polygon how do you calculate the position of a bounding rectangle for which the angle of orientation is predetermined?"" And I would do it by rotating the polygon against the angle of orientation then use a simple search for its maximum and minimum points in the two cardinal directions using whatever search algorithm is appropriate for the structure the points of the polygon are stored in. (Simply put you need to find the highest and lowest X values and highest and lowest Y values.) Then the minima and maxima define your rectangle. You can do the same thing without rotating the polygon first but your search for minimum and maximum points has to be more sophisticated. This is correct. The bounding box should not be calculated for the polygon with 0 rotation and then rotating the bound. This might yield too big bbox. I'm assuming AABB (axis aligned)."
310,A,Code or formula for intersection of two parabolas in any rotation I am working on a geometry problem that requires finding the intersection of two parabolic arcs in any rotation. I was able to intesect a line and a parabolic arc by rotating the plane to align the arc with an axis but two parabolas cannot both align with an axis. I am working on deriving the formulas but I would like to know if there is a resource already available for this. I'd first define the equation for the parabolic arc in 2D without rotations:  x(t) = ax² + bx + c y(t) = t; You can now apply the rotation by building a rotation matrix:  s = sin(angle) c = cos(angle) matrix = | c -s | | s c | Apply that matrix and you'll get the rotated parametric equation: x' (t) = x(t) * c - s*t; y' (t) = x(t) * s + c*t; This will give you two equations (for x and y) of your parabolic arcs. Do that for both of your rotated arcs and subtract them. This gives you an equation like this:  xa'(t) = rotated equation of arc1 in x ya'(t) = rotated equation of arc1 in y. xb'(t) = rotated equation of arc2 in x yb'(t) = rotated equation of arc2 in y. t1 = parametric value of arc1 t2 = parametric value of arc2 0 = xa'(t1) - xb'(t2) 0 = ya'(t1) - yb'(t2) Each of these equation is just a order 2 polynomial. These are easy to solve. To find the intersection points you solve the above equation (e.g. find the roots). You'll get up to two roots for each axis. Any root that is equal on x and y is an intersection point between the curves. Getting the position is easy now: Just plug the root into your parametric equation and you can directly get x and y.  Unfortunately the general answer requires solution of a fourth-order polynomial. If we transform coordinates so one of the two parabolas is in the standard form y=x^2 then the second parabola satisfies (ax+by)^2+cx+dy+e==0. To find the intersection solve both simultaneously. Substituting in y=x^2 we see that the result is a fourth-order polynomial: (ax+bx^2)^2+cx+dx^2+e==0. Nils solution therefore won't work (his mistake: each one is a 2nd order polynomial in each variable separately but together they're not).  It's easy if you have a CAS at hand. See the solution in Mathematica. Choose one parabola and change coordinates so its equation becomes y(x)=a x^2 (Normal form). The other parabola will have the general form: A x^2 + B x y + CC y^2 + DD x + EE y + F == 0 where B^2-4 A C ==0 (so it's a parabola) Let's solve a numeric case: p = {a -> 1 A -> 1 B -> 2 CC -> 1 DD -> 1 EE -> -1 F -> 1}; p1 = {ToRules@N@Reduce[ (A x^2 + B x y + CC y^2 + DD x + EE y +F /. {y -> a x^2 } /. p) == 0 x]} {{x -> -2.11769} {x -> -0.641445} {x -> 0.379567- 0.76948 I} {x -> 0.379567+ 0.76948 I}} Let's plot it: Show[{ Plot[a x^2 /. p {x -10 10} PlotRange -> {{-10 10} {-5 5}}] ContourPlot[(A x^2 + B x y + CC y^2 + DD x + EE y + F /. p) == 0 {x -10 10} {y -10 10}] Graphics[{ PointSize[Large] Pink Point[{x x^2} /. p /. p1[[1]]] PointSize[Large] Pink Point[{x x^2} /. p /. p1[[2]]] }]}] The general solution involves calculating the roots of: 4 A F + 4 A DD x + (4 A^2 + 4 a A EE) x^2 + 4 a A B x^3 + a^2 B^2 x^4 == 0 Which is done easily in any CAS.
311,A,How to mix two ARGB pixels? How can I mix two ARGB pixels ? Example Here A is (Red with Alpha) and B is ( Blue with Alpha ). This might give you some clues http://stackoverflow.com/questions/1892020/mixing-two-rgb-color-vectors-to-get-resultant though it is RGB and not ARGB the concept is similar. Hi why is FA = 19? Also what type of 'mixing' do you mean? Something like this? http://lectureonline.cl.msu.edu/~mmp/applist/RGBColor/c.htm @o.k.w It is totally different. @Sunny yea I can see why it's different.The FA=19 is puzzling me. With FA he means the final alpha channel (the transparency). Right but transparency is generally from 0 to 1 unless 19 is supposed to mean 0.19 there @danben In computer transparancy is between 0 to 255. Taken from the same Wikipedia article where you got the image: Translating to values which range from 0 to 255: rOut = (rA * aA / 255) + (rB * aB * (255 - aA) / (255*255)) gOut = (gA * aA / 255) + (gB * aB * (255 - aA) / (255*255)) bOut = (bA * aA / 255) + (bB * aB * (255 - aA) / (255*255)) aOut = aA + (aB * (255 - aA) / 255)  It seems like this is what you want: http://en.wikipedia.org/wiki/Alpha%5Fcompositing#Alpha%5Fblending but I'm a little confused by your notation since wikipedia says that argb values should range from 0.0 to 1.0. So I don't think this formula will give you FA=19. Can you clarify? Edit: now that you took out the business about FA=19 I'm inclined to go with that formula. +1 for Alpha Blending. This seems like the required method. You can use also values from 0 to 255 for the RGB components; if you need to use the values to calculate something then it's probably better to use values between 0 and 255. The formula can be changed to work with values between 0 and 255 though. I think the formula expects each value to be a percentage so you could use 0.0-1.0 for the formula and then multiply by 255 afterward. Thanks danben I will take care to convert this formula from (0 to 1.0) to ( 0 to 255 ).
312,A,How can I access the palette of a TPicture.Graphic? I have searched the web for hours but I can not find anything about how to get the palette from a TPicture.Graphic. I also need to get the color values so I can pass these values to a TStringList for filling cells in a colorpicker. Here is the code that I currently have: procedure TFormMain.OpenImage1Click( Sender: TObject ); var i: integer; S: TStringList; AColor: TColor; AColorCount: integer; N: string; Pal: PLogPalette; HPal: hPalette; begin if OpenPictureDialog1.Execute then begin Screen.Cursor := crHourGlass; try Pal := nil; try S := TStringList.Create; ABitmap.Free; // Release any existing bitmap ABitmap := TBitmap.Create; Image1.Picture.LoadFromFile( OpenPictureDialog1.Filename ); ABitmap.Canvas.Draw( 0 0 Image1.Picture.Graphic ); GetMem( Pal Sizeof( TLogPalette ) + Sizeof( TPaletteEntry ) * 255 ); Pal.palversion := $300; Pal.palnumentries := 256; for i := 0 to 255 do begin AColor := Pal.PalPalEntry[ i ].PeRed shl 16 + Pal.PalPalEntry[ i ].PeGreen shl 8 + Pal.PalPalEntry[ i ].PeBlue; N := ColorToString( AColor ); S.Add( N ); end; HPal := CreatePalette( Pal^ ); ABitmap.Palette := HPal; Memo1.Lines := S; finally; FreeMem( Pal ); end; S.Free; finally; Screen.Cursor := crDefault; end; end; end; I am drawing to the canvas of ABitmap with the image contained in Image1.Picture.Graphic because I want to support all TPicture image types such as Bitmap Jpeg PngImage and GIfImg. Any assistance would be appreciated. Am I on the correct path or is something different needed? I have edited your question title and removed the first sentences as the problem seems to be with getting access to an existing palette only not with creating one. The code you posted does nothing really. You either have to read the palette back from the bitmap before you can access it or you need to create a palette and assign it to a bitmap - your code does neither. The following code is more or less yours with fields fBitmap and fBitmapPalEntries for the results of the operation. I commented all the lines that I changed:  if OpenPictureDialog1.Execute then begin Screen.Cursor := crHourGlass; try Pal := nil; try S := TStringList.Create; fBitmap.Free; // Release any existing bitmap fBitmap := TBitmap.Create; // if you want a 256 colour bitmap with a palette you need to say so fBitmap.PixelFormat := pf8bit; Image1.Picture.LoadFromFile( OpenPictureDialog1.Filename ); fBitmap.Canvas.Draw( 0 0 Image1.Picture.Graphic ); // access the palette only if bitmap has indeed one if fBitmap.Palette <> 0 then begin GetMem( Pal Sizeof( TLogPalette ) + Sizeof( TPaletteEntry ) * 255 ); Pal.palversion := $300; Pal.palnumentries := 256; // read palette data from bitmap fBitmapPalEntries := GetPaletteEntries(fBitmap.Palette 0 256 Pal.palPalEntry[0]); for i := 0 to fBitmapPalEntries - 1 do begin AColor := Pal.PalPalEntry[ i ].PeRed shl 16 + Pal.PalPalEntry[ i ].PeGreen shl 8 + Pal.PalPalEntry[ i ].PeBlue; N := ColorToString( AColor ); S.Add( N ); end; // doesn't make sense the palette is already there // HPal := CreatePalette( Pal^ ); // fBitmap.Palette := HPal; Memo1.Lines := S; end; finally; FreeMem( Pal ); end; S.Free; finally; Screen.Cursor := crDefault; end; end; Support for palettes with less entries is easy you just need to reallocate the memory after you know how many entries there are something like ReallocMem(Pal SizeOf(TLogPalette) + SizeOf(TPaletteEntry) * (fBitmapPalEntries - 1)); Creating a palette would only be necessary if you want to write a bitmap in pf4Bit or pf8Bit format. You would need to determine the 16 or 256 colours that are palette entries possibly by reducing the number of colours (dithering). Then you would fill the palette colour slots with the colour values and finally use the two lines I commented out from your code. You have to make sure that the pixel format of the bitmap and the number of palette entries match. Thank-you for your answers. I am studying the code rewrite by mghie. Any ideas how to get pixelformat of fBitmap since it is not loaded directly from a file?  A wonderful resource of graphics alogithms is available at efg's reference library which includes a specific section dealing with just color. Specifically this article (with source) discusses counting the available colors and might be of the best use.  I don't know myself but you might take a look at XN Resource Editor which does display palette information is written in Delphi and has source available. i have spent hours with this source code some of which is incomplete I think. The code is for Delphi 7 and is very complicated.... at least for me it is.  Thank-you all.... especially mghie. We managed to get the code to work very well for bmp png and gif files and pf1bit pf4bit pf8bit pf16bit and pf24bit images. We are still tesing the code but so far it seems to work very well. Hopefully this code will help other developers as well. var i: integer; fStringList: TStringList; fColor: TColor; fColorString: string; fPal: PLogPalette; fBitmapPalEntries: Cardinal; begin if OpenPictureDialog1.Execute then begin Screen.Cursor := crHourGlass; try fPal := nil; try fStringList := TStringList.Create; Image1.Picture.LoadFromFile( OpenPictureDialog1.Filename ); if Image1.Picture.Graphic.Palette <> 0 then begin GetMem( fPal Sizeof( TLogPalette ) + Sizeof( TPaletteEntry ) * 255 ); fPal.palversion := $300; fPal.palnumentries := 256; fBitmapPalEntries := GetPaletteEntries( Image1.Picture.Graphic.Palette 0 256 fPal.palPalEntry[ 0 ] ); for i := 0 to fBitmapPalEntries - 1 do begin fColor := fPal.PalPalEntry[ i ].PeBlue shl 16 + fPal.PalPalEntry[ i ].PeGreen shl 8 + fPal.PalPalEntry[ i ].PeRed; fColorString := ColorToString( fColor ); fStringList.Add( fColorString ); end; end; finally; FreeMem( fPal ); end; if fStringList.Count = 0 then ShowMessage('No palette entries!') else // add the colors to the colorpicker here fStringList.Free; finally; Screen.Cursor := crDefault; end; end;
313,A,"In a DDS file can you detect textures with 0/1 alpha bits? In my engine I have a need to be able to detect DXT1 textures that have texels with 0 alpha (e.g. a cutout for a window frame). This is easy for textures I compress myself but I'm not sure about textures that are already compressed. Is there an easy way to tell from the header whether a DDS image contains alpha? I agree with the accepted answer. Your job may be made a bit easier by using the ""squish"" library to decompress the blocks for you. http://www.sjbrown.co.uk/?code=squish  As far as I know there's no way to tell from the header. There's a DDPF_ALPHAPIXELS flag but I don't think that will get set based on what's in the pixel data. You'd need to parse the DXT1 blocks and look for colours that have 0 alpha in them (making sure to check that the colour is actually used in the block too I suppose).  DDS is a very poor wrapper for DXT (or BTC) data. The header will not help you. Plain original DXT1 did not have any alpha. I believe d3d nowadays does actually decode DXT1 with alpha though. Every DXT1 block looks like this: color1(16 bits) color2(16 bits) indices(32 bits). If the 16 bit color1 value is less than color2 (just a uint16 comparison nothing fancy!) the block has no alpha. Otherwise it does. So to answer you question: Skip the header read 16 bits a read 16 bits b if a>b there is alpha. otherwise skip 32 bits and repeat until eof. Other DXT formats like DXT5 always have alpha. It is very rare that people rely on the DXT1 alpha trick because some hw (intel..) does not support it reliably."
314,A,"How do I flip a polygon by flipping the array? I have 2 arrays of ints I am using to create a polygon (that looks like a fish). What do I need to do to the arrays to flip the polygon horizontally? x = new int[] { 0 18 24 30 48 60 60 54 60 48 30 24 0 }; y = new int[] { 0 18 6 0 0 12 18 24 24 36 36 30 36 }; Have you tried to make any changes? If yes what were they what happened? If no why not? Yes I have. I have tried flipping the x axis which distorts the fish. I have tried flipping both which does nothing. I have also tried subtracting 60 from all x values which just bounces them 60 pxs. And I have tried subtracting x from 60 which seemed to do nothing. `x[i] = 60 - x[i]` should work. What do you mean it did ""nothing""? What exactly did it do? the fish are moving from left to right and they just continue to move to the right with no noticeable change I think you should post your code. or look at your code closer there must be something else wrong. You need to find the maximum value of the x array. In this case it is 60. Then set each x coordinate to 60 - x using a loop like this: for (i = 0; i < NUMBER_OF_POINTS; i++) { x[i] = MAX_X - x[i]; } it is sending the fish back to the otherside of the sceen maybe post your code. after playing around with a few other things this appears to be working there was another bug in the code that was making it jump to the other side of the screen. Ah great. I love it when bugs make fish jump.  I think what you mean by 'flip' is to make a fish that is headed left to right change into one that is right to left. This means that you are effectively reflecting the fish around the line x=a where a is the horizontal coordinate of the mid point of the fish. In this case a=(max(x[])-min(x[]))/2. For each point we check if it is to the left or right of x=a. If it is to the left we simple change it so that it is now the same distance to the right otherwise we change it so that it is to the same distance to the left. I think the following (untested) code will work. I'm keeping all values as ints so a slight distortion might occur. But it should be easy to tweak the code until the distortion disappears. int max_x=-1; int min_x=Integer.MAX_VALUE; for (int v:x){ max_x=Math.max(max_xv); min_x=Math.miN(min_xv); } int mid=(max_x-min_x)/2; int[] reflected_x=new int[x.length]; for(int i=0;i<x.length;i++){ int diff=Math.abs(x[i]-mid); if (x[i]<mid) reflected_x[i]=mid+diff; else reflected_x[i]=mid-diff; } Why the down vote?  http://en.wikipedia.org/wiki/Rotation%5Fmatrix A reflection and a rotation are fundamentally different operations. But looking up linear algebra isn't a bad idea for the OP. It shouldn't be rotation it should be scaling of X coordinates by factor of -1. I confused reflaction in a point with reflaction in a line.  You might find it easier to use the graphics class' translate and scale methods rather than manipulating the array contents.  Modify x coordinate using this formulae x = 60 - x This is correct but it would probably be nice to explain to the OP how he could arrive at this answer If the OP lacks the basic understanding of coordinate systems he should not be doing any gfx coding at all.  You might find this a lot easier if you were using a Point2D abstraction instead of two arrays of ints. These aren't separate entities they're related. So why are you writing code as if they had no relationship at all? Where are your Point and Polygon classes? Where's the abstraction? If I understand what you mean by ""flipping"" horizontally I think you want reflection about the y-axis. If that's true all you have to do is change the signs of all the x-coordinates and you're done. So a vector with endpoints A (xa ya) and B (xb yb) becomes (-xa ya) and (-xb yb)."
315,A,"Is it possible to do a kind of Link / Button style using CSS with special effects like shadows outlining of text and/or gradient? Is it possible to make such buttons (http://img225.imageshack.us/img225/6452/buttonslw9.jpg) using CSS? It should be Menu and PHP would just feed the text to html/css and css should take care of the design. Maybe I want too much out of CSS - especially with that red outline of the text.. ? Any ideas how i can achieve such results without doing those buttons manually in Graphical Editor? There is no cross-browser way to do this (as you said especially with the red text outline) but the Webkit and Gecko teams are implementing some cool CSS things like gradients embossing with experimental CSS properties. You might see what jQuery can do for you. It does some pretty cool CSS-like things that go beyond CSS.  ""Pure"" solution is possible in latest Safari with text-shadow -webkit-text-stroke and -webkit-gradient properties (explained in Safari blog). You could also use SVG + CSS background-image in Opera 9.5 and Safari. A practical solution that works in more than a couple of cutting-edge browsers is to generate images on the server side with GD library. would GD library do what i need? he said it could"
316,A,"How to erase an area in a BitmapData object? Flex 3 ActionScript 3 Flash player 9. I have a picture in a BitmapData object. And an array of points. I nead to erase the part of the picture inside a polygon specified by the points. In other words draw a polygon specified by the points and fill it with transparency. Any ideas on how it can be done? For a rectangle you can use fillRect. For a polygon you are gonna have to draw the polygon in a totally different color (than other colors in the bitmap) and use floodFill - but I don't know how to draw a polygon. There is no method in bitmap data class to draw lines. Another option would be to write your own logic to find pixels inside the polygon and use setPixel32 method to set their alphas to zero. This wikipedia page describes algorithms to find if a point is inside a given polygon. You might find it useful. Are there any other options? Like masking the polygon. I guess it is possible just don't know how  Got it working with the following code:  var shape:Shape = new Shape(); shape.graphics.beginFill(0x000000 1); // solid black shape.graphics.moveTo(points[0].x points[0].y); points.forEach(function (p:Point i:int a:Array):void { shape.graphics.lineTo(p.x p.y); }); shape.graphics.endFill(); data.draw(shape null null ""erase""); +1 That's a good one. Consider accepting your own answer so that this question appears answered in the listings. Ooohhh... nice. I didn't know you could do that :-p"
317,A,"How to recognize rectangles in this image? I have a image with horizontal and vertical lines. In fact this image is the BBC website converted to horizontal and vertical lines. My problem is that I want to be able to find all the rectangles in the image. I want to write a computer program to find all the rectangles. Does anyone know how to do this or suggest ideas on how to get started? This task is easy for me as a person to find the visual rectangles but I am not sure how to describe it as a program. Image at http://www.ironnine.com/rectangles.png is the BBC website here http://www.bbc.co.uk/ Thanks Philip Update to this I wrote the code which converts the BBC website image to the horizontal and vertical line the problem is these lines do not completely meet at the corners and sometimes they do not completely form a rectangle. Thanks! neat. what program is used to create the image? Could you use its source code - which identifies boxes in order to draw them - for your own purposes? Or do you only have the final .png image to work with? Your sample image is helpful in that it brings up an important question. There are many shapes there that approximate rectangles but are incomplete due to gaps often at the corners. Are you looking for all perfect rectangles or shapes that mostly approximate rectangles? The later will be more difficult to determine. Can you please share your results on this and how did you arrive at the above graphic? another approach would be to find ANY colored pixel on the image then go with while(pixel under current is colored) { lowest pixel coordinate = pixel under current current = pixel under } then do the same upwards. now u have defined a single line. then use ends of the lines to approx match lines into rectangles. if they are not pixel perfect you could do some kind of tresholding.  Assuming it's a reasonably noise free image (not a video of a screen) then one of the simple floodfill algorithms should work. You might need to run a dilate/erode on the image to close up the gaps. The normal way to find lines is a Hough transform ( then find lines at right angles) Opencv is the easiest way. Take a look at this question http://stackoverflow.com/questions/279410/opencv-object-detection-center-point  iterate from left to right until you hit a color pixel then use modified flood fill algorithm. more info on the algo flood fill @ wiki  Opencv (image processing and computer vision library written in c) has implementation for hough transform (the simple hough transform find lines in an image while the generalized one finds more complex objects) so that could be a good start. For the rectangles which do have closed corners there are also corner detectors such as cornerHarris which can help. I ran the houghlines demo provided with opencv and here's the result on the image you gave (detected lines marked in red):  To get from the image you have with the nearly touching horizontal and vertical lines to just the rectangles: Convert to binary (i.e. all lines are white the rest is black) Perform a Binary dilation (here you make every pixel that touches a white pixel in the source image or is a white pixel in the source image white. Touch is straight only (so each pixel ""touches"" the pixels to its left right above and below it) this is called ""4-connected"" repeat step 3 a few times if the gaps between the ends are larger then 2 pixels wide but not too often! Perform a skeleton operation (here you make every pixel in the output image black if it is a white pixel in the source image that touches at least one black pixel and the white pixels it touches (in the source image) all touch eachother. Again touch defined with 4-connectedness. See sample below. Repeat step 4 untill the image doesn't change after a repeat (all white pixels are line ends or connectors) This will with a bit of luck first show the boxes with thick fat lines leaving thick fat artifacts all over the image (after step 3) and then then after step 5 all thick fat artifacts will have been removed while all boxes remain. You need to tweek the number of repeats in step 3 for best results. If you're interested in image morphology this is the book of a really good introductory course I took. Sample: (0=black 1=white pixels in the center of each 3x3 block are being considered input left output right) 011 => 011 011 => 001 all other white pixels touch so eliminate 011 => 011 010 => 010 010 => 010 top pixel would become disconnected so leave 010 => 010 010 => 010 010 => 000 touches only one white pixel so remove 000 => 000 010 => 010 111 => 111 does not touch black pixels leave 010 => 010 010 => 010 011 => 011 other pixels do not touch. so leave 000 => 000  I believe you are looking for the generalized Hough transform.  The flood fill would work or you could use a modification of an edge tracking algorithm. what you do is: create a 2d array (or any other d2 data struct)- each row represents a horizontal pixel line on screen and each column a vertical line iterate through all the pixels left to right and whenever you find a coloured one add its coordinates to the array iterate through the array and findying lines and storing the begin and end pixel for each one (different data structure) knowing that the begin of each line is its left/top pixel you can easily check to see if any 4 lines comprise a rectangle  There are several different approaches to your problem. I'd use a morphological image processing tool like this one. You will have the flexibility to define ""rectangle"" even something that not ""exactly closed"" (where the fill algorithm will fail). Another possibility could be to use a machine learning approach which basically is more data-driven than definition-driven like the previous one. You'll have to give your algorithm several ""examples"" of what a rectangle is and it will eventually learn (with a bias and an error rate).  In computer vision there is a algorithm called Generalized Hough Transform which maybe can solve your problem. There should be open source code having implemented this algorithm. Just search for it."
318,A,"Best API for simple 2D graphics with Java I'm not sure what the best api for simple 2d graphics with Java is. I know java.awt.Graphics2D was the standard but has it been replaced? Swing is the new API for Java GUI apps but it seems a bit heavy for what I want. What I really want is something like the C SDL library. Processing.org has some good easy-to-use 2D stuff (and 3D). It has a PApplet class that implements Applet from AWT together with a bunch of useful operations and works well together with Java2D. If you just want to mess around with 2d graphics it has a ""sketchpad IDE"" where you don't need to put it in your java IDE if you just want to experiment with it.  A Java binding to SDL can be found here: http://sdljava.sourceforge.net/  If you want to have the least work possible if you're building a game (or even if not) use http://slick.cokeandcode.com/  Piccolo can be a good choice for drawing graphics. It is a 2D graphics toolkit that supports zoomable user interface. Available for both Java and .Net.  Java 2D (Graphics2D and friends) is indeed the best choice that I know of. Swing is actually implemented on top of Java 2D so yes if you want non-GUI-type graphics Java 2D is the way to go. Original link was dead I updated it to point to the new Java 2D tutorials at Oracle's site."
319,A,glPushName() and GL_TRIANGLE_STRIP I am trying to implement selecting a node in a terrain (represented by triangle strips). However I cannot use glPushName() between glBegin/glEnd so I can only save the whole strip in a name buffer. Any other ideas how could I select a specific node/triangle with the mouse? I know that by creating GL_TRIANGLE objects instead of the triangle strip would solve my problem but wouldn't that be much much slower? Thanks in advance OpenGL's selection mode has been deprecated you should use a CPU-based ray picking algorithm instead (if you really care about picking performance). http://www.opengl.org/resources/faq/technical/selection.htm
320,A,"A dynamic array of a class inside another separate class? I'm working on a robot localization simulator and I created a class called ""landmark"". The end result is going to be a robot that is always centered and always faces the top of the screen. As it turns the birds eye view map will rotate around the robot. To accomplish this I'm assuming I can rotate one class and have all elements inside rotate as well. So the landmark class has properties xy label and radius. This is suppose to simulate a tree location in a forest. To test everything I need ""forest data"" and I wrote a script to generate 100 trees in a 100m x 100m area. The script automatically generates values within an acceptable range for xy radius. The generated data is stored in an object called tempForest and is 100x3. Ideally I want to create a class called ""landmarks"" (plural) that has 100 landmark instances inside. How would I instantiate 100 instances of landmark in one instance of landmarks using that randomly generated data? Ideally I'd just type treeBeacons = landmarks(); and it would randomly populate 100 (user definable set in config file) instances with x y radius data. I'm not sure how to deal with a dynamic array of class ""Landmark"" inside another single class ""landmarks."" Any ideas? I'd create a class 'landmarks' that has a property 'fixedPositions' a property 'viewDirection' and a dependent property 'apparentPositions'. If you type treeBeacons=landmarks; you can have the constructor fill in fixedPositions which is the list of positions of your trees. You then set treeBeacons.facing to whatever direction the robot is facing and you can get the forest relative to the robot as treeBeacons.apparentPositions. I don't think it is necessary to have your trees be objects given your description. However if your trees really need to be individual objects you have the constructor of landmarks create objects instead of coordinates and store them in fixedPositions (or trees) instead. Let me just suggest that you do not use both landmarks and landmark as different variable names. At least the common people like me have a very hard time telling the two apart. classdef landmarks properties fixedPositions %# positions in a fixed coordinate system. [ x y radius ] facing = 0;%# direction in which the robot is facing end properties (Dependent) apparentPositions end methods function obj = landmarks(numberOfTrees) %# set obj.fixedPositions here depending on the number of trees. end function out = get.apparentPositions(obj) %# rotate obj.positions using obj.facing to generate the output end function plotMap(objfixedOrApparent) %# plots the map either using fixed or apparent coordinates (good for testing) end end end @pinnacler: Yes fixedPositions is the output from the tree generator. Since apparentPositions is a dependent function it is re-generated whenever it is requested using the most recent values for 'facing' - thus there is no need to use events. So fixed positions would be the 100x3 output from the tree generator? Would apparent positions change if heading changes? I assume I would just add an event listener to apparent positions and fire it off when I change heading?"
321,A,"how to make java JPanel and graphics2d transparent? Well the title is quite self explanatory. I want to build two panels one on-top of each other in layers using java. I want the top layer to contain a JPanel which will contain a graphics2d object. I'd like both the JPanel and graphics2d to have transparent background (I still want the content drawn by the graphics2d visible). Does anyone have an idea how it can be done? Post your SSCCE. We can't guess what you are doing. Call setOpaque(false) on the JPanel - that will not paint the JPanel's background. Depending on what method you're overriding to get at the Graphics2D (JPanel's don't contain a Graphics2D object like a component - a Graphics2D object is used to paint the JPanel) - if it's paintComponent() you should read the JavaDocs for JComponent - and call super.paintComponent(g) first so that opacity is honored - and then do the rest of your painting. Working example: package com.stackoverflow.opaque; import javax.swing.*; import java.awt.*; import java.awt.event.ActionListener; import java.awt.event.ActionEvent; public class OpaqueExample extends JFrame { private JLayeredPane layers; private JPanel up down; private JButton toggleOpaque; public OpaqueExample() { layers = new JLayeredPane(); down = new JPanel(); down.setBackground(Color.GREEN); down.setBounds(100 100 200 200); layers.add(down new Integer(1)); up = new JPanel() { public void paintComponent(Graphics og) { super.paintComponent(og); Graphics2D g = (Graphics2D)og; GradientPaint gradient = new GradientPaint(0 0 Color.BLUE 10 0 Color.WHITE true ); Polygon poly = new Polygon(); poly.addPoint(10 10); poly.addPoint(100 50); poly.addPoint(190 10); poly.addPoint(150 100); poly.addPoint(190 190); poly.addPoint(100 150); poly.addPoint(10 190); poly.addPoint(50 100); poly.addPoint(10 10); g.setPaint(gradient); g.fill(poly); g.setPaint(Color.BLACK); g.draw(poly); } }; up.setBackground(Color.RED); up.setBounds(150 150 200 200); layers.add(up new Integer(2)); getContentPane().add(layers BorderLayout.CENTER); JPanel buttonPanel = new JPanel(new FlowLayout(FlowLayout.CENTER)); toggleOpaque = new JButton(""Toggle Opaque""); toggleOpaque.addActionListener(new ActionListener() { public void actionPerformed(ActionEvent e) { up.setOpaque(!up.isOpaque()); layers.repaint(); } }); buttonPanel.add(toggleOpaque); getContentPane().add(buttonPanel BorderLayout.EAST); } public static void main(String[] args) { JFrame f = new OpaqueExample(); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); f.setSize(500 500); f.setLocationRelativeTo(null); f.setVisible(true); } } You should be overriding paintComponent() rather than paint() for a Swing component (other than some special cases). Not sure what to say about the rest... since no code is posted I don't know exactly what you're doing. I'll post a short example that works. Hi Nate I've been checking out if its possible to send private message here. Wanted to thank you in a better way. Your example code was simple and made me understand. I was using my graphics2d and noticed that I was calling clearRect which left me constantly with a white screen. after changing this and adjusting to your code sample it worked! once again thank you. well I am using just regular paint. When I test try only a single jpanel without the graphics2d and set setOpaque(false) it still hides the lower JPanel. I am using JLayeredPane by the way."
322,A,How do I take C programming beyond the console? I'm trying to learn some graphics programming using C. What would be the best way for a beginner to start? I'd like to how to make programs that use graphics and images that can be run directly from a command line prompt and don't rely on a windowing system like X to execute. Thanks Mike It's a platform-specific question although there are some cross-platform libraries for this. What platform? What sort of graphics do you want to do? 2D 3D? Do you want to be writing low-level image processing or high-level vector graphics? And by running from a command prompt do you mean you just want to output images into files instead of displaying them directly on the screen? I'm programming on Ubuntu Linux. I'm thinking just 2D graphics for now just to learn the ropes. I'd probably try to make some graphical style menu using graphics I create in Photoshop or GIMP. When I say running from a command line I mean programming it in a way that wouldn't require being run inside a windowing system like Swing or Windows Forms. Look into libsdl - Simple DirectMedia Layer. Although on Linux it can use X11 for displaying output it can also directly use a framebuffer device. It's designed to be simple for pixel-bashing game-type programming and supports a wide variety of platforms. I'll check it out thanks!  There's also Allegro if you're not a fan of SDL. It's somewhat more fully-featured for simple vector graphics; SDL is mainly a cross-platform framebuffer until you add extension libraries.  Learn some GUI toolkit like Qt or GTK this way you will make modern GUI applications. he specifically said he didn't want to require a windowing system like X  Check out the FLTK GUI toolkit. It is small and easy to learn.
323,A,"background colour in opengl I want to change background color of the window after pressing the button but my program doesn't work can somebody tell me why thanks in advance int main(int argc char* argv[]) { glutInit(&argc argv); glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB); glutInitWindowSize(800 600); glutInitWindowPosition(30050); glutCreateWindow(""GLRect""); glClearColor(1.0f 0.0f 0.0f 1.0f); <--- glutDisplayFunc(RenderScene); glutReshapeFunc(ChangeSize); glutMainLoop(); system(""pause""); glClearColor(0.0f 1.0f 0.0f 1.0f); <--- return 0; } After pressing ""what button""? Since `glutMainLoop()` will never return how do you expect to get to your `system(""pause"")` statement and the second `glClearColor` call? @tafa: how can I change my code to see this effect? I would imagine you don't have an OpenGL context at that point that you call glClearColor. But... ...I've never used glut before so a quick look at the docs suggests that you will actually have a context after glutCreateWindow so perhaps that's not it. As my comment on your question says I'm curious about the second call to glClearColor and how you think you will reach it. That's more likely the cause of the problem. To do anything on a key press I believe you would have to register a callback using glutKeyboardFunc. sorry but I didn't understand I'm beginner I'm workin in Eclipse  glClearColor does not do any clearing itself -- it just sets what the color will be when you do actually clear. To do the clearing itself you need to call glClear with (at least) COLOR_BUFFER_BIT. Edit: it's been quite a while since I used glut so the details of this could be wrong but if memory serves to change the screen color in response to pressing a key on the keyboard you'd do something like this: void keyboard (unsigned char key int x int y) { // we'll switch between red and blue when the user presses a key: GLfloat colors[][3] = { { 0.0f 0.0f 1.0f} {1.0f 0.0f 0.0f } }; static int back; switch (key) { case 27: exit(0); default: back ^= 1; glClearColor(colors[back][0] colors[back][1] colors[back][2] 1.0f); glutPostRedisplay(); } } void draw() { glClear(GL_COLOR_BUFFER_BIT); // other drawing here... } int main() { // glutInit glutInitDisplayMode etc. glutDisplayFunc(draw); glutKeyboardFunc(keyboard); glutMainLoop(); } Basically you do all your drawing in whatever function you pass to glutDisplayFunc. Almost anything else just changes the state then calls PostRedisplayFunc(); to tell glut that the window needs to be redrawn. Warning: as I said it's been a while since I used glut and I haven't tested this code. It shows the general structure of a glut program to the best of my recollection but don't expect it to work exactly as-is. I did itbut it still doesn't work may be problem with glutMainLoop? This solution is fine. Try using double buffer? glutMainLoop is fine. Tell me if it works with double buffering"
324,A,FFT for Spectrograms in Python How would I go about using Python to read the frequency peaks from a WAV PCM file and then be able to generate an image of it for spectogram analysis? I'm trying to make a program that allows you to read any audio file converting it to WAV PCM and then finding the peaks and frequency cutoffs. Python's wave library will let you import the audio. After that you can use numpy to take an FFT of the audio. Then matplotlib makes very nice charts and graphs - absolutely comparable to MATLAB. It's old as dirt but this article would probably get you started on almost exactly the problem you're describing (article in Python of course). matplotlib can also compute the spectrogram directly with the command `specgram`. that looks like it will be exactly what I need. thank you :)  Loading WAV files is easy using audiolab: from audiolab import wavread signal fs enc = wavread('test.wav') or for reading any general audio format and converting to WAV: from audiolab import Sndfile sound_file = Sndfile('test.w64' 'r') signal = wave_file.read_frames(wave_file.nframes) The spectrogram is built into PyLab: from pylab import * specgram(signal) Specifically it's part of matplotlib. Here's a better example.  from pylab import * specgram(signal) is the easiest. Also quite handy in this context: subplot But be warned: Matplotlib is very slow but it creates beautiful images. You should not use it for demanding animation even less when you are dealing with 3D  If you need to convert from PCM format to integers you'll want to use struct.unpack.
325,A,"Is it possible to dither a gradient drawable? I'm using the following drawable: <?xml version=""1.0"" encoding=""utf-8""?> <shape xmlns:android=""http://schemas.android.com/apk/res/android"" android:shape=""rectangle"" > <gradient android:startColor=""@color/content_background_gradient_start"" android:endColor=""@color/content_background_gradient_end"" android:angle=""270"" /> </shape> The problem is that I get severe banding on hdpi devices (like the Nexus One and Droid) since the gradient goes from the top of the screen to the very bottom. According to http://idunnolol.com/android/drawables.html#shape_gradient there isn't a ""dither"" attribute for a gradient. Is there anything I can do to smooth the gradient? Note: adding dither=""true"" to shape doesn't seem to work. Hi all i have the same problem there is one solution which works but it's not very good. getWindow().setFormat(PixelFormat.RGBA_8888); getWindow().addFlags(WindowManager.LayoutParams.FLAG_DITHER); It works for me but the problem is that the whole windows is dithered. I was looking to find a way to dither only the gradient but i couldn't find anything. android:dither=""true"" in xml is not working and GradientDrawable.setDither(true) is also not working. So any ideas how can i dither only the gradient ? The selected answer didn't help me yours did! Have a Galaxy S with Android 2.2.  I wrote the documentation you referenced. I've taken another look at the code and unfortunately there's no way to enable dithering on a GradientDrawable except by explicitly calling GradientDrawable.setDither() in code. (The way the codes looks technically you could include the Gradient as the only child of a <selector> and enable dithering on the entire selector; however it's definitely a hack.) I'm not convinced enabling dithering will actually solve your problem as dithering (at least as it's noted in the official Android docs) are for solving banding problems when the device has too small of a color palette. This seems to be a banding problem due to the size of the gradient.  I faced a very similar problem last year and came to no useful conclusion on the android-developers list. However a while ago I discovered — after trying <gradient> and all sorts of Drawables with various dither attributes and manually creating dithered PNGs — that if I manually create a new image using GIMP and specify the density at this point (i.e. explicitly entering 120 or 240 etc) when creating the image it looks great even on hdpi devices. And this is despite it being a grayscale gradient with not so many colours. The PNG when saved ends up being comparatively large (at least for 240dpi) but it looks great."
326,A,"Prerequisite to know before working on cocos2d-iphone or game programmign in general Ok i am also one of those who are trying to make their first game esp for Iphone but i have no clue from where to start. I never work on such application before (graphics stuff). It's totally different paradigm for me. I google around and came to know that opengl will be required then it cam out there is something called cocos2d-iphone is around to make games also. But i really dont know what i should read before even looking into opengl and cocos2d-iphone because they look totally alien to me. What really makes/construct a game? there are sound characters movement. How you can make those sound and characters? what skills are required to make these things? Every lesson of opengl starts from making those triangles but i really dont get how i can make a GAME from these triangle knowledge? I know i am asking many questions within one but i dont know how to put it otherwise. Is there any guide out there which can teach some guy like me how a game is actually made i mean from start to the end! Lets take this game for example: http://www.youtube.com/watch?v=oovCmnuKA1A No this game has a voice then he is using the ""accelerometer"" to move the big thing around or using touch event to move that big thing which hold other 3 objects from dropping. There is lot of art work is required how it is made? Sorry i am totally naive about this so may have asked some stupid question. Sorry. i have no clue from where to start This set of iPhone game tutorials by Michael Daley is a fantastic place to start. He walks you through the process of building your own game engine much like Cocos2D. Along the way you can learn a lot about: Objective C and iPhone programming Game Loops Sprite Sheets Sound Effects Animation Tile Maps Particle Generators and lots of other game development concepts that the triangle tutorials won't give you. Cocos2D is really powerful and has classes for handling all of these things but they aren't much use to you when you don't know what they are to begin with - these tutorials will help with that. Then just as Goz said it's about setting some goals and going for it. When you're starting out it doesn't matter if you use scratchy hand drawn sprites and crackly self recorded sound effects. Just study up start making something on your own and trawl the forums when you run into trouble. Best of luck. Update: Just found this link to a free chapter of the iPhone Games Projects book http://www.apress.com/book/downloadfile/4448 It shows you how to use Cocos2D to make a tetris style game. Great link in the update. Thanks!  What really makes/construct a game? there are sound characters movement. How you can make those sound and characters? what skills are required to make these things? To make sounds requires either buying a clip library or recording some sounds yourself. If you want to get really into it you can load those recorded samples and mess them about in a piece of audio software such as Adobe audition. There are cheaper but not as good alternatives running down to Audacity which is free but quite limited by comparison to audition. To make graphics requires a paint package of some sort. You could use anything. Whatever you can draw well in. Adobe Photoshop is a favourite but again is very expensive. You can potentially buy in sprites from someone or if you aren't aiming to do anything more than a learning project you could just rip off sprites from another game. Well to make sounds you either need a mic or some serious sound design skills. Depends on how ""good"" you want them. Good enough is a lto less hassle. As for the art work that requires some artistic skills. Not something that can be learnt in my experience. You either have them or you don't. Beyond that there is some work that comes to making the characters ""feel"" real. This requires programming abilities. Again the simpler your characters are the less knowledge you need. You will still require a reasonable amount of programming know-how and basic maths skills though. Every lesson of opengl starts from making those triangles but i really dont get how i can make a GAME from these triangle knowledge? There are many free sprite engines out there. Do a google search for sprite engines. You may be surprised how much you can have taken off you by doing this. No this game has a voice then he is using the ""accelerometer"" to move the big thing around or using touch event to move that big thing which hold other 3 objects from dropping. Bear in mind that using an accelerometer is harder than it seems. I've not used the iPhone but on Wii you just get a set of 3 scalars out that indicate a sort of direction vector. You can then plug this direction vector into your game to make things happen. This isn't a simple bit of programming though. It always surprises me how much time you can lose to getting the ""feel"" of something like this right. Good luck! Don't let the above put you off. Its totally doable. Set realistic targets and expect it to take you a lot longer than you imagine and you will learn so much from it. Well it looks to me that game development can only be done in team with different skill set not possible for single guy like me to make anything :( There are always ways. If you want to learn start off making something like tetris. You'll learn a lot and after doing that you may find yourself in a better position to approach something more complex. i havent been able to find a said ""sprite engine"" for mac any pointers?"
327,A,"how do I create a line of arbitrary thickness using Bresenham? I am currently using Bresenham's algorithm to draw lines but they are (of course) one pixel in thickness. My question is what is the most efficient way to draw lines of arbitrary thickness? The language I am using is C. Retagged ""language-agnostic"" since the implementation language is not really relevant. here is a related SO question: http://stackoverflow.com/questions/101718/drawing-a-variable-width-line-in-opengl-no-gllinewidth @banister: you got some demo to share with us? For my embedded thermal printer application using Bresenham's algorithm the line was way too thin. I don't have GL or anything fancy. I ended up simply decrementing the Y value and drawing more lines under the first one. Each number of thickness added another line. Very quick to implement and made for the desired results printing from monochrome bitmap to the thermal. Every dot has another dot below it... no calculations; the line was visibly thicker. I ended up using 3 lines each with a Y value less than previous. Any y less than y minimum was change to Y minimum. Sorry I miss confused about controlling the thickness and controlling the width of the line. I liked your idea how did you test/calculate the desired distance between lines so that you mimic the thickness ? was it by trial and error ?  For best accuracy and also good performance for thicker lines especially you can draw the line as a polygon. Some pseudo code: draw_line(x1y1x2y2thickness) Point p[4]; angle = atan2(x2-x1y2-y1); p[0].x = x1 + thickness*cos(angle+PI/2); p[0].y = y1 + thickness*sin(angle+PI/2); p[1].x = x1 + thickness*cos(angle-PI/2); p[1].y = y1 + thickness*sin(angle-PI/2); p[2].x = x2 + thickness*cos(angle-PI/2); p[2].y = y2 + thickness*sin(angle-PI/2); p[3].x = x2 + thickness*cos(angle+PI/2); p[3].y = y2 + thickness*sin(angle+PI/2); draw_polygon(p4) And optionally a circle could be drawn at each end point.  I assume that you would draw horizontal spans from one bounding line to another and compute the x-value of each of the lines by Bresenham's method as you go (in a single loop). Haven't tried it. The end points may need some attention lest they look oddly cut-off. yes this was the strategy that i had in mind too and i agree that the end-points will need some thought.  Here is a paper and Delphi implementation of a modified version of Bresenham's algorithm for drawing thickened lines. You may also want to take a look at Anti-Grain Geometry a library for high-quality and high-performance software rendering of 2D graphics. Take a look at the demo page to get an idea of what it can do.  http://members.chello.at/~easyfilter/bresenham.html The example at the bottom of this link is javascript but should be easy enough to adapt to C. It's a fairly straightforward antialiasing algorithm to draw lines of variable thickness.  I think the best way is to draw a rectangle rather than a line since a line with width is a two dimensional object. Tring to draw a set of parallel lines to avoid overdraw (to reduce write bandwidth) and underdraw (missing pixels) would be quite complex. It's not too hard to calculate the corner points of the rectangle from the start and end point and the width. I didn't got your solution. Can you elaborate please.  Some simple routes to use: for any width n where n is odd. for any point p plotted also plot the points above/below it for n/2 (if the line is > 45 degree angle draw side to side instead). not really a proper line of the right thickness more like an italic pen but very fast. for start point p(xy) pick the points t0 and b such that they are centred on p but n pixels apart. for the end point do the same resulting in t1 b1. Draw lines from t0 -> t1 t1->b1 b1 -> t0 b0 -> t1. Fill in the resulting rectangle. The trick here is picking the points such that they appear orthogonal to the path direction. for each point p on the line instead of drawing a point draw a circle. This has the advantage of making the end points 'clean' no matter what the orientation. there should be no need to render any circles in solid except for the first. somewhat slow  Take another Bresenham loop and use it to modify the start and end position of original line in rectangular direction. Problem is to efficently find the right starting point and not to draw any pixel twice (or skip a pixel) while drawing next line. Working and tested C code is available from Google Code C code . The image ""Accelerator + gyroscope + compass display"" on Project Home shows 2 example lines. -I am not allowed to add an link or image here :-( If I could I would upvote this answer more because it's the only one that directs the reader to a clean working implementation in C - just as the OP asked for."
328,A,"Where can I get freely available audio graphics and other resources for games? I've done a google search of this topic but so far haven't found anything satisfactory. From your experience what's the best place to get game resources like sprites backgrounds sound effects music etc.? To be more specific I'm looking for more of sound effects and music which I'm currently lacking more than graphics. However for graphics I've tried getting random graphics from different sites but they just don't match. I don't want to copy one entire graphics package either. The resources should be free and easy to obtain. The products I intend to make are free if not open source and are unlikely to receive widespread attention or produce profit for myself so I'd like something that I can use and distribute freely. I don't have enough graphics and musical knowledge to attempt to create resources from scratch and don't know anyone willing to do so. I'm working with Java. I'm sure I can read all kinds of file formats with it or if not I can always use software to convert resources. Is this for a specific project or just a general question did you have a theme in mind? Do you want to be able to sell the end product or is just for personal use? It's actually somewhere in between. Although I don't intend to profit from my product I'm looking to create a reasonably appealing one. not a programming question. Try http://www.gamedev.net/ true although i'm still interested in getting an answer. Totally a programming question and stackoverflow is the easiest question and answer website. Best answers rise to the top. No digging For free sounds music textures and art: OpenGameArt.org GarageGames.com sells a lot of that kind of thing... 3d models textures background music and so on. http://garagegames.com and specifically http://www.garagegames.com/products/browse HTH edit: whoops I didn't see the ""free"" requirement! Do a search on ""creative commons"" and you'll find lots of music at least and some graphics. Hmm their products are fairly expensive and more than I'm willing to pay for my personal use. Some of them are expensive but many are $30 or less. It's far from out of reach but for sure more than free. :-)  In terms of graphics Daniel Cook of Lost Garden produces some seriously high quality reusable game art that is free for both personal and commercial work (read his license details). Here's the index of his free graphics related posts just hit the ""read more"" link at the bottom of an article and you'll find links to the downloads. edit: in terms of sound effects Soundrangers is pretty decent if you have something specific in mind but it can quickly get expensive. For a complex game if you're wanting a rich user experience you'll need dozens if not hundreds of sound effects. At a couple of bucks a pop that adds up real quick. A lot of places (including Soundrangers) offer thematic sound packs which give you a little more bang for your buck but it's still not free. GameDev also has a listing of audio resources. For music I think your options are better. Depending on what kind of thing you're looking for (ambient instrumental vocal etc). I would seriously think about approaching local independent musicians and using existing tracks that they have. They're likely to let you use their music for free (properly accredited of course) or at a reasonable cost. I can't hear any of the previews at soundrangers. Did anyone else have this problem?  Don't know if this is the type of thing you're looking for but Game Sprite Archives has a huge huge collection of SNES/NES/Anything pre-playstation 1 sprite rips. I would be surprised if much of the content on that site is legal to use.  If you're looking for interesting textures I would suggest checking out Filter Forge. You can download the filters for use in Adobe Photoshop or you can potentially use the sample images on the site to create texture maps for various types of terrains and materials.  I just discovered this site the other day while looking for some sound effects: http://www.soundrangers.com/ It looks like they're royalty-free but most of the sounds cost a buck or two. Looks like some sounds are free though.  Clipart Open Clip Art Textures ImageAfter CG Textures OpenFootage Texture Hound  I recommend Sound Snap they allow 5 free downloads for a month for the free accounts and more if you sign up. I have been using them for the past couple of months for the games I have developed.  There's http://www.freesound.org/ Most stuff there has a license that is incompatible with say Fedora for instance though if you ask the copyright holder sometimes they'll license things under a different license. Music is harder to come by than sound effects. you could try digging around on archive.org say here: http://www.archive.org/details/muzic Also check out sfxr http://www.cyd.liu.se/~tompe573/hp/project_sfxr.html though the sounds that it makes are pretty old school sounding -- and if that's what you're looking for it's cool otherwise it can make some place holder sound effects. +1 freesound looks awesome thanks for the link!"
329,A,"What are the differences between the OpenGL GTK and QT libraries? I understand the purpose of GTK QT or other graphic toolkits. But I don't understand the role of OpenGL. Is it just another GUI-library or does it refer to something more fundamental? If so what is it and when should I use it in day-by-day hacking? OpenGL is graphics API upon which more complex user-facing graphics libraries are built. If you are interested in graphics development and in particular 3D graphics then it is well worth making it's acquaintance. It's and extraordinarily powerful system that can do everything from draw a line through render Quake to handle photo-realistic atmospheric rendering. Your starting point for all things OpenGL related is OpenGL.org which has links to everything you need. If you prefer a textbook and that's not a bad idea with OpenGL then the definitive text is 'The OpenGL programming guide' colloquially known as 'The Red Book' on account of it's cover. You may wish to supplement this with 'OpenGL Shading Language' (or The Orange Book) which does what it says. One of the nice things about OpenGL is that there is a simple test toolkit - GLUT - which has been implemented in most languages with bindings and is extraordinarily easy to get up and running to experiment and generally hack around with. So go get rendering your teapot!  OpenGL is a library for drawing graphics on a low level. It excels mainly at 3D graphics. The most important competitor to OpenGL is Direct3D (not the whole DirectX). GL is only really powerful if you have it hardware accelerated. Today almost every desktop or laptop solution has GL acceleration but on some systems where the graphics card is integrated into the chipset it is still rather slow. For day-by-day hacking GL can serve many unusual purposes. The reason is that you can leverage the GPU's processing power for graphic related tasks. For anything non-graphical newer languages like CUDA exist as well to access the GPU. While GL's real strengths lie in 3D scene rendering it is often also used for 2D manipulation or composition. The reason is that 2D acceleration architectures of graphics cards often don't reach the power (freedom) or performance of GL. That's why sometimes GL is used for video output (mainly scaling) or blending tasks even if specific 2D acceleration for these tasks would be available. Mac OS X is a prominent example of a whole desktop (optionally) being rendered through GL to allow appealing effects without high CPU load. But it can also be used for example to blend anti-aliased text glyphs onto a textured background. As soon as you do a graphical performance critical task you will probably want the GPU to help out (possible even if you don't draw on screen) and then OpenGL is the clean standardized cross-platform answer to your needs. Today's GUI toolkits mostly don't offer you GPU-accelerated drawing but the inclusion of GL rendering into their widgets. So you can use them for UI drawing and include your performance critical realtime graphics where it suits. Building a whole UI inside GL is a pain but there are also solutions for that available (typical use cases are games with in-game UI). Why is it that 2D acceleration is generally slower? Wouldn't it make sense for things to go faster with only 2 axes? First most of the additional operations in 3D like the more complicated matrix operations are basically implemented in hardware therefore they are 'for free' as compared to more simple 2D operations. The drawing itself is then done in 2D for both cases (see Scanline algorithm). Second 3D acceleration is generally faster because the GPU vendors put much more effort in optimizing drivers for GL and DirectX instead of 2D hardware acceleration; especially for platforms other than Windows. Most GPU benchmarks are 3D gaming benchmarks. That's where the competition in the market takes place.  It's something more fundamental. OpenGL lets you perform low-level drawing (think ""draw a line from here to here"" rather than ""draw a button""). OpenGL operations are also often hardware accelerated and thus very efficient and the OpenGL API allows you to easily work with 3D. Choosing between OpenGL and a GUI toolkit like QT or GTK really depends on whether you're trying to draw UI elements for your application or want very fine control over how your graphics are drawn. Sometimes GUI toolkits employ OpenGL under the covers in order to draw their on-screen elements.  GTK QT MFC wxwidget etc are windowing toolkits - they provide controls such as dialog boxes widgets buttons etc to create gui apps. One of the controls they generally supply is a canvas that you can draw on with graphics commands (drawline draw circle etc). They probably also supply an OpenGL canvas as well that you can draw on with OpenGL commands. OpenGL (in simple terms) is a language for displaying 3D objects it is very powerful cross platform and somewhat complicated (especially if your maths is weak!) BUT it is far better than trying to do any of this 3D stuff yourself! It can be frustrating at times especially if you do something wrong and it just draws a blank screen. Just to make things a little more complex there are also windowing toolkits written in OpenGL where OpenGL provides a set of buttons dialogs etc in the scene AND there are times when (behind the scenes) all the GUI elements used by something like QT are actually being drawn by OpenGL!"
330,A,How do you create an off-center PerspectiveCamera in WPF? I'm trying to more or less recreate Johnny Lee's Wii head tracking app but using an augmented reality toolkit for the tracking and WPF for graphics. To do this I need to create a perspective camera using the top bottom right and left parameters to create my viewing frustum instead of field of view and aspect ratio (to those familiar with OpenGL I want to use the WPF equivalent of glFrustum instead of gluPerspective) The problem is those options don't seem to be available on WPF's PerspectiveCamera class. I could probably create the projection matrix manually if I had to and use MatrixCamera but I'd like to avoid that. Does anyone know of a better way to do this? I meant some method that does the math for me instead of forcing me to do it like I had to in my answer below. What do you mean by 'better'? I never did find a built-in way to do this so I wrote my own. The math behind it can be found in the OpenGL glFrustum docs. If anyone else ever runs into this problem this should work for you: public Matrix3D CreateFrustumMatrix(double left double right double bottom double top double near double far) { var a = (right + left) / (right - left); var b = (top + bottom) / (top - bottom); var c = -(far + near) / (far - near); var d = -2 * far * near / (far - near); return new Matrix3D( 2 * near / (right - left) 0 0 0 0 2 * near / (top - bottom) 0 0 a b c -1 0 0 d 0); } Just set MatrixCamera.ProjectionMatrix to the return value of that method and you're all set.
331,A,"some links for graphics in C I'm looking for some tutorials which can teach about graphics on C I tryed find it but all I can find are discussions about special topics I'm beginner thanks in advance how about OpenGL? There is a ton of material about that. openGl I know nothing about it how can I connect it with C? What platform are you trying to work on? And what are you trying to achieve? ""Graphics"" can be a lot of things. Are you trying to work on game type stuff? Are we talking 2d or 3d here? Is it image manipulation or just displaying them? OpenGL and DirectX will let you do pretty much anything you need to do and interface directly to the hardware drivers but may well be overkill for many things. ok now I want to write something simple on C using openGl 2d what can U suggest I don't know even from which point to start for expamle how can I draw circle or square using C? Unfortunately there is not really any ""draw circle"" function. You're going to need actually learn some api. If you want to learn open gl there are a lot of good tutorials available online. However for 2D stuff openGL may be a bit overkill. Being more specific about your final goal would help a lot.  SDL Cairo If you need something more specific than those then feel free to ask a more specific question. I know a lot about programming on C but I know nothing about graphics on C from which point to start maybe book some materials?"
332,A,"Drawing part of a Bézier curve by reusing a basic Bézier-curve-function? Assuming I'm using some graphic API which allows me to draw bezier curves by specifying the 4 necessary points: start end two control points. Can I reuse this function to draw x percent of the 'original' curve (by adjusting the control points and the end point)? Or is it impossible? Unnecessary information should someone care: I need the whole thing to draw every n % of the original bezier curve with different color and/or line style I'm using Java's Path2D to draw bezier curves: Path2D p = new GeneralPath(); p.moveTo(x1 y1); p.curveTo(bx1 by1 bx2 by2 x2 y2); g2.draw(p); In an answer to another question I just included some formulas to compute control points for a section of a cubic curve. With u = 1 − t a cubic bezier curve is described as B(t) = u3 P1 + 3u2t P2 + 3ut2 P3 + t3 P4 P1 is the start point of the curve P4 its end point. P2 and P3 are the control points. Given two parameters t0 and t1 the part of the curve between these parameters is described by the new control points Q1 = u0u0u0 P1 + (t0u0u0 + u0t0u0 + u0u0t0) P2 + (t0t0u0 + u0t0t0 + t0u0t0) P3 + t0t0t0 P4 Q2 = u0u0u1 P1 + (t0u0u1 + u0t0u1 + u0u0t1) P2 + (t0t0u1 + u0t0t1 + t0u0t1) P3 + t0t0t1 P4 Q3 = u0u1u1 P1 + (t0u1u1 + u0t1u1 + u0u1t1) P2 + (t0t1u1 + u0t1t1 + t0u1t1) P3 + t0t1t1 P4 Q4 = u1u1u1 P1 + (t1u1u1 + u1t1u1 + u1u1t1) P2 + (t1t1u1 + u1t1t1 + t1u1t1) P3 + t1t1t1 P4 Note that in the parenthesized expressions at least some of the terms are equal and can be combined. I did not do so as the formula as stated here will make the pattern clearer I believe. You can simply execute those computations independently for the x and y directions to compute your new control points. Note that a given percentage of the parameter range for t in general will not correspond to that same percentage of the length. So you'll most likely have to integrate over the curve to turn path lengths back into parameters. Or you use some approximation.  What you need is the De Casteljau algorithm. This will allow you to split your curve into whatever segments you'd like. However since you're dealing with just cubic curves I'd like to suggest a slightly easier to use formulation that'll give you a segment from t0 to t1 where 0 <= t0 <= t1 <= 1. Here's some pseudocode: u0 = 1.0 - t0 u1 = 1.0 - t1 qxa = x1*u0*u0 + bx1*2*t0*u0 + bx2*t0*t0 qxb = x1*u1*u1 + bx1*2*t1*u1 + bx2*t1*t1 qxc = bx1*u0*u0 + bx2*2*t0*u0 + x2*t0*t0 qxd = bx1*u1*u1 + bx2*2*t1*u1 + x2*t1*t1 qya = y1*u0*u0 + by1*2*t0*u0 + by2*t0*t0 qyb = y1*u1*u1 + by1*2*t1*u1 + by2*t1*t1 qyc = by1*u0*u0 + by2*2*t0*u0 + y2*t0*t0 qyd = by1*u1*u1 + by2*2*t1*u1 + y2*t1*t1 xa = qxa*u0 + qxc*t0 xb = qxa*u1 + qxc*t1 xc = qxb*u0 + qxd*t0 xd = qxb*u1 + qxd*t1 ya = qya*u0 + qyc*t0 yb = qya*u1 + qyc*t1 yc = qyb*u0 + qyd*t0 yd = qyb*u1 + qyd*t1 Then just draw the Bézier curve formed by (xaya) (xbyb) (xcyc) and (xdyd). Note that t0 and t1 are not exactly percentages of the curve distance but rather the curves parameter space. If you absolutely must have distance then things are much more difficult. Try this out and see if it does what you need. Edit: It's worth noting that these equations simplify quite a bit if either t0 or t1 is 0 or 1 (i.e. you only want to trim from one side). Also the relationship 0 <= t0 <= t1 <= 1 isn't a strict requirement. For example t0 = 1 and t1 = 0 can be used to ""flip"" the curve backwards or t0 = 0 and t1 = 1.5 could be used to extend the curve past the original end. However the curve might look different than you expect if you try to extend it past the [01] range. Edit2: More than 3 years after my original answer MvG pointed out an error in my equations. I forgot the last step (an extra linear interpolation to get the final control points). The equations above have been corrected. +1 Some commentary stating explicitly that points a and d are points on the original curve that define the extent of a new partial curve would be helpful. Also points b and c are newly defined control points (i.e. only loosely related to the original control points). @Bob Cross: Noted. For the curious point b lies along the tangent of the curve at point a and point c lies along the tangent of the curve at point d. In fact a=p(t0) b=a+p'(t0)/3 c=d-p'(t1)/3 and d=p(t1) where p(t) is the point on the curve at t and p'(t) is the 1st derivative of the curve at t. @Naaff your curve only uses polynomials of degree two so it cannot describe points on a degree three curve (i.e. cubic bezier curve) exactly. Noticed this when [another question](http://stackoverflow.com/q/11703283/1468366) tried to build on this. @MvG You're right. I had inadvertently left off the linear interpolation step (which when combined with the quadratic equations is equivalent to a cubic). Thanks I've fixed this above. For those interested here is a geometric interpretation of the equations above: Qa=(qxaqya) and Qb=(qxbqyb) are on the quadratic curve formed by the first three cubic control points. Similarly Qc=(qxcqyc) and Qd=(qxdqyd) are on the quadratic curve formed by the last three cubic control points. (xaya) and (xbyb) are on the line between Qa and Qc while (xcyc) and (xdyd) are on the line between Qb and Qd. Being new to Bezier curves I would love an edit that further explained the meaning of parameter space from: ""Note that t0 and t1 are not exactly percentages of the curve distance but rather the curves parameter space."" Parameter space is just the range of the variable t. It can be related to the length of the curve but generally not simply. You can think of it sort of like ""time"" as the curve is traced out. See also: http://en.wikipedia.org/wiki/Parametric_equation"
333,A,"Multi-Dimensional Graphing Libraries for Java I have been working on projects that deal with 2-Dimensional graphing UML diagramming and relational graphing (node-link diagrams). I am interested in moving to projects that deal more with 3-Dimensional graphing of scatter plots and 3-Dimensional navigation. Could someone suggest a few books or libraries that would help me get started? EDIT: I don't think I was very clear in my question. The focus of my work is information and scientific visualization so I need something that is not only good for drawing but also for visualizing data. Google for a start: http://java.sun.com/javase/technologies/desktop/java3d/ java3d isn't being maintained anymore. How about using one of the Java frontends for GNUplot? classes for producing plots with the Unix/Linux gnuplot tool JavaPlot jgnuplot Another possibility would be to use the Java/R interface and let R do the heavy lifting inside your Java app. One last option is JMathPlot. I looked into using JMathPlot and I like it alot. Thank you for the suggestion.  JFreeChart is a very good graphing library for Java. JFreeChart is one of the graphing libraries I use for 2D graphs however; I don't think it can be used for 3D graphing.  Try my open source 2D/3D java graphing library DataDisplay. Any feedback you can give would be just swell. https://sourceforge.net/p/msu-doe-dd/  MathGL is GPL plotting library (can plot 2- and 3-ranged data) and in principle it may have Java interface (using SWIG). But I never test it since I'm not familiar with Java :(  JOGL (Java Open GL) is the standard 3d library for java. The OpenGL ""Red Book"" is the standard Book for Open GL programming. I dunno if there are any good packages for doing 3d plotting out of the box though. You might need to write the visualization code yourself. It's not that hard. I think I might use JMathPlot for scientific information and OpenGL for everything else. Thank you for the suggestions I just started looking at OpenGl and it seems like it would be good for drawing but would it be effective for data visualization?  How about the 3D functionality of processing? Or is that too generic for your needs? I always had the impression that http://processing.org was only used for prototyping and not for robust applications. Is this the case? The point of processing is that it allows anyone to get a quick visual feedback based on their scripts which allows people to prototype their crazy ideas such as ""Hey what if I multiply sin curve with tanh!?"" That means it's good/great for prototyping but it's not the only thing it can do. I think processing is suitable for production use. It might be too generic for your specific use though. I doesn't have easy ""plot3d"" like commands such as gnuplot R or matlab"
334,A,InterpolationMode.High I am trying to figure out the difference between High and HighQualityBicubic InterpolationMode in .Net (System.Drawing.Drawing2D.InterpolationMode.High). Does High mean that it will pick the best algorithm based on the image and what you are doing with it? Is it just an alias for HighQualityBicubic? Something else? According to the MSDN documentation High - Specifies high quality interpolation. HighQualityBicubic - Specifies high-quality bicubic interpolation. Prefiltering is performed to ensure high-quality shrinking. This mode produces the highest quality transformed images. There are a lot of seemingly duplicate entries like this in .NET especially System.Drawing. I imagine they're there for possible future additions. HighQualityBicubic specifically requests Bicubic whereas High will use the best available at the time. Someone might discover a super-awesome resizing algorithm tomorrow that puts all other before it to shame. High would allow .NET.NEXT to use that. Just a theory though. There's probably docs on it somewhere but it's bedtime ;) I ended up setting the value to High and checking the value after that and it was HighQualityBicubic. Guess that answers my question :). Thanks for your response.  I set Graphics.InterpolationMode and read back its value for every possible value. I found that Default and Low become Bilinear and that High becomes HighQualityBicubic.
335,A,"Full-justification with a Java Graphics.drawString replacement? Does anyone know of existing code that lets you draw fully justified text in Java2D? For example if I said drawString(""sample text here"" x y width) is there an existing library that could figure out how much of that text fits within the width do some inter-character spacing to make the text look good and automatically do basic word wrapping? Although not the most elegant nor robust solution here's an method that will take the Font of the current Graphics object and obtain its FontMetrics in order to find out where to draw the text and if necessary move to a new line: public void drawString(Graphics g String s int x int y int width) { // FontMetrics gives us information about the width // height etc. of the current Graphics object's Font. FontMetrics fm = g.getFontMetrics(); int lineHeight = fm.getHeight(); int curX = x; int curY = y; String[] words = s.split("" ""); for (String word : words) { // Find out thw width of the word. int wordWidth = fm.stringWidth(word + "" ""); // If text exceeds the width then move to next line. if (curX + wordWidth >= x + width) { curY += lineHeight; curX = x; } g.drawString(word curX curY); // Move over to the right for next word. curX += wordWidth; } } This implementation will separate the given String into an array of String by using the split method with a space character as the only word separator so it's probably not very robust. It also assumes that the word is followed by a space character and acts accordingly when moving the curX position. I wouldn't recommend using this implementation if I were you but probably the functions that are needed in order to make another implementation would still use the methods provided by the FontMetrics class. Thanks - I actually started working on a similar method and there are a number of similarities in our approach. I'm also adding some logic that can tweak the inter-character spacing. This is great. Exactly what I needed right now to finish a project at that last minute. (Not a serious project going into production) @Tone I'm glad I could help :)  For word wrapping you might be interested by How to output a String on multiple lines using Graphics. No justification here not sure if it is easy (or impossible!) to add..."
336,A,"Java Graphics Font - How to make sure the characters lie in particular area? I have an Image. On the bottom portion of the image I would like to create a colored strip say of height 100. I am already done with creating the strip and I can basically write strings in that portion (e.g. copyright of the image etc). The following is my method. public static BufferedImage captionMyImage(BufferedImage sourceImage) { int height=sourceImage.getHeight(); int width=sourceImage.getWidth(); System.out.println(""Image Height: ""+height); System.out.println(""Image Width: ""+width); int min=20; //fifty pixels int newHeight=(int) (0.1*height>min ? 1.1*height : height+min); int difference=newHeight-height; System.out.println(""new height:""+newHeight); System.out.println(""Difference:""+difference); BufferedImage bgImage=new BufferedImage(widthnewHeightBufferedImage.TYPE_INT_RGB); /**Create a Graphics from the background image**/ Graphics2D g = bgImage.createGraphics(); //g.drawRect(0 0 width ((int)0.1*height>min) ? (int)1.1*height : height+min); g.setColor(Color.RED); g.fillRect(0 0 bgImage.getWidth() bgImage.getHeight()); g.setColor(Color.YELLOW); /**Set Antialias Rendering**/ g.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON); /** * Draw background image at location (00) * You can change the (xy) value as required */ g.drawImage(sourceImage 0 0 null); int strX=sourceImage.getWidth()/2; int strY=sourceImage.getHeight()+difference/2; System.out.println(""X: ""+strX); System.out.println(""Y: ""+strY); g.setFont(new Font(g.getFont().getName()Font.PLAIN20)); g.drawString(""(c)www.sanjaal.com""strX strY); g.dispose(); return bgImage; } I know the way I have calculated the values for x and y for drawString() method is just a simple one and has problem that the text goes outside the boundary sometimes (depending on image size and text length) I would like to make sure that the text in the image on the bottom strip that I have defined always aligns to the right portion (boundary) of the image but does not go out of boundary. How can I achieve that? Remember the text length can be dynamic. Would the java Graphics experts out there share your ideas on how this can be achieved? String msg = ""(c)www.sanjaal.com""; int margin = 2; g.drawString(msg getWidth() - g.getFontMetrics().stringWidth(msg) - margin strY); Thank you for a quick reply. This is exactly what I am looking for."
337,A,"Running graphics display on multiple systems keeping synched I have a series of systems on a LAN running a synchronized display routine. For example think of a chorus line. The program they ran is fixed. I have each ""client"" download the entire routine and then contact the central ""server"" at fixed points in the routine for synchronization. The routine itself is mundane with perhaps 20 possible instructions. Each client runs the same routine but they can be doing completely different things at any one time. One part of the chorus line can be kicking left another part kicking right but all in time with each other. Clients can join and drop out at any time but they're all assigned a part. If no-one is there to run the part it just doesn't get run. This is all coded in C# .Net. The client display is a Windows Forms application. The server accepts TCP connections and then services them round-robin fashion keeping a master clock of what's going on. The clients send a signal that says ""I've reached sync-point 32"" (or 19 or 5 or whatever) and waits for the server to acknowledge and then moves on. Or the server can say ""No you need to start at sync-point 15"". This all works great. There is a minor bit of delay between the first and last clients to hit a sync-point but it's hardly noticeable. Ran for months. Then the Specification changed. Now the clients need to respond to near real-time instructions from the server -- it's no longer a pre-set dance program. The server is going to be sending instructions out and the dance program is made up on the fly. I get the fun job of re-designing the protocol the servicing loops and the programming instructions. My toolkit includes anything in a standard .Net 3.5 toolbox. Installing new software is a pain in the arse since so many systems (clients) can be involved. I'm looking for suggestions on keeping the clients synced (some sort of latching system? UDP? Broadcast?) distribution of the ""dance program"" anything that might make this easier than a traditional Client/Server TCP arrangement. Keep in mind that there are time/speed limitations going on as well. I could put the dance program in a network database but I'd have to shove instructions in fairly quickly and there'd be a lot of readers using a rather thick protocol (DBI SqlClient etc..) to get a small bit of text. That seems overly complex. And I still need something to keep them all displaying in sync. Suggestions? Opinions? Wild-ass speculation? Code examples? PS: Answers may not get marked as ""correct"" (since this isn't a ""correct"" answer) but +1 votes for good suggestions for sure. I did something similar (quite a while back) with synchronizing a bank of 4 displays each run by a single system receiving messages from a central server. The architecture we finally settled on after a fair amount of testing involved having one ""master"" machine. In your case this would be having one of your 20 clients that acts as the master and have it connect to the server via TCP. The server then would send the entire series of commands for the series through to that one machine. That machine then used UDP to broadcast real-time instructions to each of the other machines (the 19 other clients on its LAN) to keep their displays up to date. We used UDP for a couple reasons here - there was lower overhead involved which helped keep the total resource usage down. Also since you're updating in real-time if one or two ""frames"" was out of sync it was never noticable at least not noticeable enough for our purposes (having a human sitting and interacting with the system). The key point to this working smoothly though is having an intelligent communication means between the main server and the ""master"" machine - you want to keep the bandwidth as low as possible. In a case like yours I'd probably come up with a single binary blob that had the current instruction set for the 20 machines in its smallest form. (Maybe something like 20 bytes or 40 bytes if you need it etc). The ""master"" machine would then worry about translating this out to the other 19 machines and itself. There are some nice things about this - the server has a much easier time transmitting to one machine in the cluster instead of every machine in the cluster. This let us for example have one single centralized server ""drive"" multiple clusters efficiently without having ridiculous hardware requirements anywhere. It also keeps the client code very very simple. It just has to listen for a UDP datagram and do whatever it says - in your case it sounds like it would have one of 20 commands so the client becomes very very simple. The ""master"" server is the trickiest. In our implementation we actually had the same client code on it as the other 19 (as a separate proces) and one ""translation"" process that took the blob broke it into 20 pieces and transmitted them. It was fairly simple to write and worked very well."
338,A,"Picking in java 2d I am using java2d to draw a simple graph at the moment I have implemented picking by calling contains(MousePoint) for each object/shape this works but scales linearly. Is there a more efficient method for picking in java2d? Yes although the full answer would be too long for this space. First of all unless you have a lot of nodes then linear will most likely be fine and you shouldn't change anything unless performance is already seen to suffer. Second what you want in general is to apply some sort of hierarchical decomposition such as a quadtree. This is a way of using more memory (and more time up front amortized during searches) to eliminate items from consideration in a so-called ""broad phase"". Some diligence on the web will help as will the book ""Real-Time Collision Detection"" by Christer Ericson. Thats the direction I was thinking but that requires me to implement it myself. Maybe it is better to use java3d and use the scenegraph and picking already implemented. +1 for ""optimize when you need to"".  As long as your only selecting areal shapes (rectangles circles) it should work with the contains() method. There's just one pitfall just in case you have overlapping shapes and you point to a place where shapes actually overlap. But that's a question of requirement whether you want to select all shapes the one on top or the first shape you find in your collection. The contains() method will not work in case you want to select Line2D type shapes. They don't have an area so the contains() method always return false. But there's already a solution on SO for this problem."
339,A,"Owner Draw ListView ""smearing"" when scrolling I have an ownerdrawn ListView that ""smears"" when I scroll. It only affects the last displayed item as it moves into the visible are of the list... It looks like: Blah Blah Blah ...have all been drawn on top of each other 1 pixel apart. The code in the DrawItem event is of the form Rectangle rect = new Rectangle(e.Bounds.X + mIconSize.Width e.Bounds.Y e.Bounds.Width e.Bounds.Height); e.Graphics.DrawString(episode.ToString() this.Font mBlackBrush rect); I'm completely stumped. Any ideas gratefully appreciated! Dave It's not a duble buffering issue (I tried that) thanks for the ideas though! In detail view drawing do all of your drawing in DrawSubItem(...). The problem is drawItem is getting called for the first item and DrawSubitem is also for the same item ... with slightly different bounds. That sounds sensible I'll try that!  also int the form's own properties you can enable DoubleBuffer. additionally there are a couple of commands you can use. More information can be found by searching DoubleBuffer C# at google (sorry as a new user I can't post links).  This is a known bug in the ListView control.  You can enable double buffering for ListView by deriving from it and setting DoubleBuffered = true. There's a noticeable reduction in flicker especially in Tile view once you turn Double Buffering on. Turns out it was a double buffering problem. I needed to create a class inheriting from ListView and turn double buffering on in the constructor"
340,A,Video Synthesis - Making waves patterns gradients I'm writing a program to generate some wild visuals. So far I can paint each pixel with a random blue value: for (y = 0; y < YMAX; y++) { for (x = 0; x < XMAX; x++) { b = rand() % 255; setPixelColor(xyrgb); } } I'd like to do more than just make blue noise but I'm not sure where to start (Google isn't helping me much today) so it would be great if you could share anything you know on the subject or some links to related resources. I used to do such kind of tricks in the past. Unfortunately I don't have the code :-/ You'll be amazed of what effects bitwise and integer arithmetic operators can produce: FRAME_ITERATION++; for (y = 0; y < YMAX; y++) { for (x = 0; x < XMAX; x++) { b = (x | y) % FRAME_ITERATION; setPixelColor(xyrgb); } } Sorry but I don't remember the exact combinations so b = (x | y) % FRAME_ITERATION; might actually render nothing beautiful. But you can try your own combos. Anyway with code like the above you can produce weird patterns and even water-like effects. Bitwise gives some interesting results and thanks for the FRAME_ITERATION part that's the crucial bit that my code was missing.  Waves are usually done with trig functions (sin/cos) or tables that approximate them. You can also do some cool water ripples with some simple math. See here for code and an online demo. Thanks I completely forgot about trig functions.
341,A,"Where can I go to get tips / advice on graphic design I am the only guy on a big project - so I have to do everything include the graphical parts such as icons and images. I'm doing my best but I need some help and my company won't be keen to hiring a real graphic artist to do the work. I am having trouble scaling down my logo to something that will work in an Icon at 32pix where would I go to get some tips? Go to alistapart - lots of interesting stuff there. K  One of the ""sister sites"" to Stack Overflow is doctype for Web Design Q&A. This is probably a good place for such questions."
342,A,"Alpha blending colors in .NET Compact Framework 2.0 In the Full .NET framework you can use the Color.FromArgb() method to create a new color with alpha blending like this: Color blended = Color.FromArgb(alpha color); or Color blended = Color.FromArgb(alpha red green  blue); However in the Compact Framework (2.0 specifically) neither of those methods are available you only get: Color.FromArgb(int red int green int blue); and Color.FromArgb(int val); The first one obviously doesn't even let you enter an alpha value but the documentation for the latter shows that ""val"" is a 32bit ARGB value (as 0xAARRGGBB as opposed to the standard 24bit 0xRRGGBB) so it would make sense that you could just build the ARGB value and pass it to the function. I tried this with the following: private Color FromARGB(byte alpha byte red byte green byte blue) { int val = (alpha << 24) | (red << 16) | (green << 8) | blue; return Color.FromArgb(val); } But no matter what I do the alpha blending never works the resulting color always as full opacity even when setting the alpha value to 0. Has anyone gotten this to work on the Compact Framework? Apparently it's not quite that simple but still possible if you have Windows Mobile 5.0 or newer. Wow...definitely not worth it if I have to put all that code in (and do native interop!) Good to know though thanks for the link.  There is a codeplex site out there that seems to do the heavy lifting of com interop for you:  CE 6.0 does not support alpha blending. WM 5.0 and above do but not through the .NET CF you will need to P/Invoke GDI stuff to do so. There are ready-made solutions out there however if you are interested i can dig the links out tomorrow at the office. I have to work with CE 6.0 currently so i don't have them on my mind. If you are using CE 6.0 you can use pseudo-transparency by reserving a transparency background color (e.g. ff00ff or something similiarly ugly) and using that in your images for transparent areas. Then your parent controls must implement a simple interface that allows re-drawing the relevant portion on your daughter controls' graphics buffer. Note that this will not give you a true ""alpha channel"" but rather just a hard on-off binary kind of transparency. It's not as bad as it sounds. Take a look at the OpenNETCF ImageButton for starters. If you are going to use this method i have a somewhat extended version of some simple controls with this technique lying around if you are interested. One additional drawback is that this technique relies on the parent control implementing a special interface and the daugther controls using it in drawing. So with closed-source components (i.e. also the stock winforms components) you are out of luck. CE 6.0 *does* support alpha blending. It may be that your specific OS build doesn't but support is in the OS and can be included in an OS image.  Apparently it's not quite that simple but still possible if you have Windows Mobile 5.0 or newer.  i take this code and i add this device.RenderState.AlphaBlendEnable = true; device.RenderState.AlphaFunction = Compare.Greater; device.RenderState.AlphaTestEnable = true; device.RenderState.DestinationBlend = Blend.InvSourceAlpha; device.RenderState.SourceBlend = Blend.SourceAlpha; device.RenderState.DiffuseMaterialSource = ColorSource.Material; in the initialized routine and it work very well thank you"
343,A,"Is it possible to get high quality graphics in 2D? I'm considering between using 2D and 3D for a game am leaning towards 2D but most of the 2D games that I see look like they're hand drawn and the graphics don't really look good. Is it possible to create slick-looking graphics in 2D at all? I was thinking of animations such as: Rays coming out of the hands/eyes of a player that attack the opponent Fire coming out of dragon's mouth A glow coming from a player or an orb that glows Fireballs being produced during fights Are these things possible in 2D without that 'amateur'/'hand drawn'/DOS game kind of look in them? Remember getting any high quality graphic is going to take work. High quality 3D textures are going to take work as are high quality 2D graphics. Look at World Of Goo for an example of how pretty 2D games can be. http://2dboy.com/games.php Yes you can make very nice slick looking 2D graphics. I was just gonna write the same thing I'll express my agreement with an upvote =)  I don't see why not. All my DVDs draw to a 2D screen and they don't look hand drawn at all :-) What do you mean by DVDs?  Many old games I love are done in 2D and they do looked slick. For example Megaman X Command & Conquer Tiberium Sun Street Fighter. You can always do the things you mentioned in 2D but you may need extra work for specific effect compared to doing it in 3D and vice versa. It is just a matter of trade-offs. Choose the right tool for the right job :)  It sure is possible. If you have an XBox 360 take a look at Geometry Wars Evolved 2. And lots of iPhone games too have sparkingly nice 2D graphics.  For StarCraft they built 3D models of everything and then converted them to sprites so they could use high-quality 3D artwork with the speed of 2D rendering.  you're mixing 'quality' with 'realism'. if you draw in 2D you're about as restricted as Raphael Cezanne Monet etc. they really got nice 2D things there. if you go 3D you get perspective and shadows for free so you don't have to do them manually. but the 'quality' of the result can be just as amateur if you don't take care.  All of my Physle games are 2D and while slick may not be my exact goal they all have particle effects and high res textures over Illustrator vector art which generally does the trick. Can the same affect be achieved in Photoshop at all? Photoshop will not create vector art but is very useful for creating and editing textures.  One of the main reason that 2D graphics don't look as good us that they are usually low budget games. And there is a reason that most low budget games do things in 2D: It is a heck of a lot easier to make a decent game in 2D than 3D. Doing graphics in 3D requires that you actually make a 3D model of your character which is much more time consuming and requires finding rarer/more specific skill-sets than if you just grab your random friend with some artistic abilities to do your art for you (possibly for free). Not to mention how much more time it takes to program a 3D engine. As other have said there is no reason why 2D can't look as good as 3D. Go with 2D and get your project done rather than try for SuperawesomeWTFBBQ 3D graphics and never be able to actually finish your project b/c of the time requirements. very helpfulty :)  ""Hand drawn"" isn't a bad thing at all just look at Braid who looks awesome while actually playing (the screenshots just doesn't make it justice)  Street Fighter II? Seriously the graphics in an app are as good as they are drawn."
344,A,"Multiple ordered translate / scale transforms in GDI+ I have a number of graphics objects which I am plotting in a graphics context in a windows forms application. There is some interaction with the ui element in which the paths are rendered which allows the user to pan zoom and set an origin for the zoom point. The question I have is is it possible to set up a sequence of transform operations on the graphics object as follows? [1] Apply a translate transfrom (to shift paths to the origin point for the scale transform) [2] Apply scale transform [3] Apply a translate transform (to shift the path back to the correct location) It seems I can only order individual transform operations types (translate scale etc) so the two translate transforms will not be applied at the correct point (either side of the scale operation). Is there a way to do this? Alternatively is it possible to set an origin for the scale transform? I did mess around with nested graphicscontainers but they didn't seem to help. Thanks Max yes. you can. use the Matrix object. http://en.csharp-online.net/GDIplus_Graphics_Transformation%E2%80%94Matrix_Class_and_Transformation  Code: private void pictureBox1_Paint(object sender PaintEventArgs e) { Bitmap bmp = new Bitmap(300 300); Graphics g = Graphics.FromImage(bmp); System.Drawing.Drawing2D.Matrix matrix = new System.Drawing.Drawing2D.Matrix(); g.DrawString(""this is a string"" SystemFonts.DefaultFont Brushes.Black new Point(50 50)); matrix.Rotate(30); // or use RotateAt(...) specifying your rotation point g.Transform = matrix; g.DrawString(""this is a 30 rotated string"" SystemFonts.DefaultFont Brushes.Black new Point(50 50)); matrix.Reset(); matrix.Translate(50 50); g.Transform = matrix; g.DrawString(""this is a 50; 50 translated string"" SystemFonts.DefaultFont Brushes.Black new Point(50 50)); pictureBox1.Image = bmp; } you can use Matrix to transform GraphicPath or Graphics objects."
345,A,"How to draw graphics/text on top of another application I want to enhance an application but there is no 3:e party API available. So basically the idea is to draw graphics/text on top of the applications windows. There are problems with z order clipping and directing mouse clicks either to my application or the other application. What is an elegant way of doing this? Example image here. It is a trading application where my application wants to add extra information into the trading application's windows. [URL=http://img104.imageshack.us/my.php?image=windowontop.png][/URL] Can you be more specific as to what you want to do? There's no elegant way of putting text on another application's window and getting it to stay there only ugly ways. But you may be able to achieve what you want some other way depending on your specific situation. added image to demonstrate what I want to do There are problems with z order clipping and directing mouse clicks either to my application or the other application. These are all tasks that the window manager was designed to deal with. You should create a layered window on top of the apps' windows. See also: Prevent repainting of window in C++  You might want to draw it on top of directX for games http://spazzarama.com/2011/03/14/c-screen-capture-and-overlays-for-direct3d-9-10-and-11-using-api-hooks/  There are no nice ways to do this but one approach that may work for you is to hook the application in question using SetWindowsHookEx(...) to add a GetMsgProc which draws your overlay in response to WM_PAINT messages. The basic idea is that you're drawing YOUR graphics right after the application finishes its own drawing. In your main app: .... HMODULE hDllInstance = LoadLibrary(""myFavoriteDll""); HOOKPROC pOverlayHook = (HOOKPROC)GetProcAddress(hDllInstance ""OverlayHook""); SetWindowsHookEx(WH_GETMESSAGE pOverlayHook hDllInstance threadId); Off in a DLL somewhere: LRESULT CALLBACK OverlayHook(int code WPARAM wParam LPARAM lParam) { //Try and be the LAST responder to WM_PAINT messages; //Of course if some other application tries this all bets are off LRESULT retCode = CallNextHookEx(NULL code wParam lParam); //Per GetMsgProc documentation don't do anything fancy if(code < 0) return retCode; //Assumes that target application only draws when WM_PAINT message is //removed from input queue. if(wParam == PM_NOREMOVE) return retCode; MSG* message = (MSG*)lParam; //Ignore everything that isn't a paint request if(message->message != WM_PAINT) return retCode; PAINTSTRUCT psPaint; BeginPaint(message->hwnd &psPaint); //Draw your overlay here ... EndPaint(message->hwnd &psPaint); return retCode; } This is all win32 so your C# code will be p/invoke heavy and correspondingly quite ugly. Your DLL must be unmanaged as well (if you intend to inject into a process other than your own) making this an even nastier solution. This would solve your issue with z-order and clipping issues as you're rendering into the window itself. However if the application you're targeting does any drawing outside of the WinProc responding to WM_PAINT things fall apart; this is not an entirely uncommon occurence."
346,A,How do I pick the control point when using CGContextAddCurveToPoint to make a line graph with curves I'm making a line graph on the iphone using core graphics and instead of having a jagged chart I want to smooth it out like in good old math class. What's the formula to pick where to put the control points for CGContextAddCurveToPoint?  CGFloat cp2x = (x + x + prevX); CGFloat cp1y = (prevY + prevY + y); CGFloat cp1x = (prevX + prevX + x); CGFloat cp2y = (y + y + prevY); CGContextAddCurveToPoint(context cp1x cp1y cp2x cp2y x y); This code almost works but doesn't take into account 3 points. I ended up doing something like this that worked well: CGPoint prevItemPosition2 = [self positionForItem: prevItem2 andMaxItem:maxItem inItems: items]; CGPoint prevItemPosition1 = [self positionForItem: prevItem1 andMaxItem:maxItem inItems: items]; CGFloat cpAngle = atan2f((prevItemPosition2.y - prevItemPosition1.y) (prevItemPosition2.x - prevItemPosition1.x)); cpAngle += M_PI; CGFloat magnitude = sqrtf(powf(prevItemPosition1.x - itemPosition.x 2) + powf(prevItemPosition1.y - itemPosition.y 2)) / 3; CGPoint angleComponents = CGPointMake(cos(cpAngle) * magnitude sin(cpAngle) * magnitude); CGPoint cp = CGPointMake(prevItemPosition1.x + angleComponents.x prevItemPosition1.y + angleComponents.y); CGContextAddQuadCurveToPoint(context cp.x cp.y itemPosition.x itemPosition.y); Could you explain a bit more in details about the prevItem2 items the method positionForItem:andMaxItem:inItems: etc? Thanks. prevItemPosition2 is two point previous in the loop prevItemPosition1 is 1 point previous to the current one in the loop. positionForItem:andMaxItem loops through the list of objects in question and returns a CGPoint where x=width_of_graph/count(items) and y=height_of_graph/biggest_y_value*y_value. It makes it so the graph fills the whole space allocated to it.
347,A,"XNA: Getting a struct as a texture to the GPU I use XNA as a nice easy basis for some graphics processing I'm doing on the CPU because it already provides a lot of the things I need. Currently my ""rendertarget"" is an array of a custom Color struct I've written that consists of three floating point fields: R G B. When I want to render this on screen I manually convert this array to the Color struct that XNA provides (only 8 bits of precision per channel) by simply clamping the result within the byte range of 0-255. I then set this new array as the data of a Texture2D (it has a SurfaceFormat of SurfaceFormat.Color) and render the texture with a SpriteBatch. What I'm looking for is a way to get rid of this translation process on the CPU and simply send my backbuffer directly to the GPU as some sort of texture where I want to do some basic post-processing. And I really need a bit more precision than 8 bits there (not necessarily 32-bits but since what I'm doing isn't GPU intensive it can't hurt I guess). How would I go about doing this? I figured that if I gave Color an explicit size of 32 bytes (so 8 bytes padding because my three channels only fill 24 bits) through StructLayout and set the SurfaceFormat of the texture that is rendered with the SpriteBatch to SurfaceFormat.Vector4 (32 bytes large) and filled the texture with SetData<Color> that it would maybe work. But I get this exception: The type you are using for T in this method is an invalid size for this resource. Is it possible to use any arbitrarily made up struct and interpret it as texture data in the GPU like you can with vertices through VertexDeclaration by specifying how it's laid out? I think I have what I want by dumping the Color struct I made and using Vector4 for my color information. This works if the SurfaceFormat of the texture is also set to Vector4."
348,A,java graphic library i'm trying to develop an interface like Windows Task Manager -> Performance Tab using Java. Can you suggest any Java Graphic Library to ease my development ? try swing or awt ;) I wanted to ask sth like this : http://geosoft.no/graphics/index.html This does not provide an answer to the question. To critique or request clarification from an author leave a comment below their post - you can always comment on your own posts and once you have sufficient [reputation](http://stackoverflow.com/faq#reputation) you will be able to [comment on any post](http://stackoverflow.com/privileges/comment).  If your question is about how to generate charts like the processor usage chart in Windows Task Manager / Performance have a look at for example JFreeChart. Java has a powerful 2D graphics API built-in see Trail: 2D Graphics in Sun's Java Tutorials. thank you very much..  As mentioned by penguru the G 2D graphics library could ease the java 2D usage : G is a generic graphics library built on top of Java 2D in order to make scene graph oriented 2D graphics available to client applications in a high level easy to use way.  SWT is another option: http://www.eclipse.org/swt/  http://en.wikipedia.org/wiki/Java2D Or for more advanced graphics you can use OpenGL http://en.wikipedia.org/wiki/Java%5FOpenGL
349,A,How to initialize Pango under Win32? Having downloaded Pango and GLib from the GTK+ Project's Win32 downloads page and having created and configured a Win32 project under Visual Studio 2005 so it points to the proper lib and include directories how do you initialize Pango for rendering to a Win32 window? Should the first call be to pango_win32_get_context()? Calling that function causes the application to hang on that call as the function never returns. What should be the first call? What other calls are needed to initialize Pango for Win32 and render a simple text string? Are there any examples available online for rendering with Pango under Win32? Pango is a GObject based library. As such you need to make sure that the glib dynamic type system is initialized before using any of its functionality. This can be done by calling g_type_init() (either directly or indirectly via something like gtk_init()). Could this be your problem? Thanks. That took care of the problem with hanging on the call to pango_win32_get_context(). Now to see if I can render some text. Additional advice is welcome.
350,A,"Java 2D game graphics Next semester we have a module in making Java applications in a team. The requirement of the module is to make a game. Over the Christmas holidays I've been doing a little practice but I can't figure out the best way to draw graphics. I'm using the Java Graphics2D object to paint shapes on screen and calling repaint() 30 times a second but this flickers terribly. Is there a better way to paint high performance 2D graphics in Java? Java OpenGL (JOGL) is one way. JOGL is nice but I doubt I can persuade other members of the team to use it. The teams are seeded across all skill levels and while I'#m the kind of person who makes games in their spare time and writes concurrent code for fun other people in the group are going to want to keep things as simple as possible (unfortunately)  There is a simple way to optimize your program. Get rid of any complex code and just use JComponent instead Canvas and paint your objects on it. Thats all. Enjoy it...  What you want to do is to create a canvas component with a BufferStrategy and render to that the code below should show you how that works I've extracted the code from my self written Engine over here. Performance solely depends on the stuff you want to draw my games mostly use images. With around 1500 of them I'm still above 200 FPS at 480x480. And with just 100 images I'm hitting 6k FPS when disabling the frame limiting. A small game (this one has around 120 images at once at the screen) I've created can be found here (yes the approach below also works fine as an applet.) import java.awt.Canvas; import java.awt.Color; import java.awt.Graphics2D; import java.awt.GraphicsConfiguration; import java.awt.GraphicsEnvironment; import java.awt.Toolkit; import java.awt.Transparency; import java.awt.event.WindowAdapter; import java.awt.event.WindowEvent; import java.awt.image.BufferStrategy; import java.awt.image.BufferedImage; import javax.swing.JFrame; import javax.swing.WindowConstants; public class Test extends Thread { private boolean isRunning = true; private Canvas canvas; private BufferStrategy strategy; private BufferedImage background; private Graphics2D backgroundGraphics; private Graphics2D graphics; private JFrame frame; private int width = 320; private int height = 240; private int scale = 1; private GraphicsConfiguration config = GraphicsEnvironment.getLocalGraphicsEnvironment() .getDefaultScreenDevice() .getDefaultConfiguration(); // create a hardware accelerated image public final BufferedImage create(final int width final int height final boolean alpha) { return config.createCompatibleImage(width height alpha ? Transparency.TRANSLUCENT : Transparency.OPAQUE); } // Setup public Test() { // JFrame frame = new JFrame(); frame.addWindowListener(new FrameClose()); frame.setDefaultCloseOperation(WindowConstants.DO_NOTHING_ON_CLOSE); frame.setSize(width * scale height * scale); frame.setVisible(true); // Canvas canvas = new Canvas(config); canvas.setSize(width * scale height * scale); frame.add(canvas 0); // Background & Buffer background = create(width height false); canvas.createBufferStrategy(2); do { strategy = canvas.getBufferStrategy(); } while (strategy == null); start(); } private class FrameClose extends WindowAdapter { @Override public void windowClosing(final WindowEvent e) { isRunning = false; } } // Screen and buffer stuff private Graphics2D getBuffer() { if (graphics == null) { try { graphics = (Graphics2D) strategy.getDrawGraphics(); } catch (IllegalStateException e) { return null; } } return graphics; } private boolean updateScreen() { graphics.dispose(); graphics = null; try { strategy.show(); Toolkit.getDefaultToolkit().sync(); return (!strategy.contentsLost()); } catch (NullPointerException e) { return true; } catch (IllegalStateException e) { return true; } } public void run() { backgroundGraphics = (Graphics2D) background.getGraphics(); long fpsWait = (long) (1.0 / 30 * 1000); main: while (isRunning) { long renderStart = System.nanoTime(); updateGame(); // Update Graphics do { Graphics2D bg = getBuffer(); if (!isRunning) { break main; } renderGame(backgroundGraphics); // this calls your draw method // thingy if (scale != 1) { bg.drawImage(background 0 0 width * scale height * scale 0 0 width height null); } else { bg.drawImage(background 0 0 null); } bg.dispose(); } while (!updateScreen()); // Better do some FPS limiting here long renderTime = (System.nanoTime() - renderStart) / 1000000; try { Thread.sleep(Math.max(0 fpsWait - renderTime)); } catch (InterruptedException e) { Thread.interrupted(); break; } renderTime = (System.nanoTime() - renderStart) / 1000000; } frame.dispose(); } public void updateGame() { // update game logic here } public void renderGame(Graphics2D g) { g.setColor(Color.BLACK); g.fillRect(0 0 width height); } public static void main(final String args[]) { new Test(); } } Thanks I've asked a question regarding whether I can use this in a Swing application here http://stackoverflow.com/questions/1966707/is-it-possible-to-perform-active-rendering-in-java-swing-without-being-on-the-edt I'm getting some strange results with the screen not clearing properly. I'm drawing a full screen filled black rectangle before calling my draw method. However the screen isn't properly cleared from the last frame. Any ideas what I might be doing wrong? Thanks!!! This is very intresting. The FPS limiting also. The game you made is VERY NICE! I hardcoded height and width to 800/600 and scale to 1. I managed to fix it myself although my end code looks quite different to the above code! Interesting is strategy.show() safe to call from outside the EDT? Short test with a second thread says yes it is safe. For the try/catch that's only there because Toolkit.getDefaultToolkit().sync() MAY throw an exception in rare cases. Hm I have no problems with the code above may be something in your code changed the width/height variables? You can also achieve the same with AWT's Frame so you won't hurt the ""don't mix Swing and AWT"" recommendation.  I think you made an override from paint(Graphics g)? This is not the good way. Use the same code but in paintComponent(Graphics g) instead of paint(Graphics g). A label you can search is doublebuffer. That is what will be done automatically by overriding paintComponent. so I can literally just copy the code from paint to paint component and everything will work the same except it'll be double buffered? yes that is what I mean. The Feast's answer describes what happening. But Java had already a solution build-in. What The Feast does is just avoiding to use `paintComponent` and making his own solution.  The flickering is due to you writing direct to the screen. Use a buffer to draw on and then write the entire screen in 1 go. This is Double Buffering which you may have heard of before. Here is the simplest form possible. public void paint(Graphics g) { Image image = createImage(size + 1 size + 1); Graphics offG = image.getGraphics(); offG.setColor(Color.BLACK); offG.fillRect(0 0 getWidth() getHeight()); // etc See the use of the off screen graphics offG. It is expensive to create the off screen image so I would suggest creating it only on first call. There's other areas you can improve this further eg creating a compatible image using clipping etc. For more fine tuned control of animation you should look into active rendering. There's a decent page I have bookmarked discussing game tutorials here. Good luck!"
351,A,Does GdkRectangle have an activate event? I want to make a GdkRectangle clickable so that I can select it and get a dot in every corner of the rectangle implying that the user can move or resize the rectangle. Is there an event that triggers when a GdkRectangle is clicked? What's the syntax of the accompanying g_signal_connect command? GdkRectangle isn't meant to be used like that. It isn't a widget but just a way to refer to a rectangular area of pixels. You should probably use something like GtkLayout but without knowing exactly what you're doing I can't say for sure.
352,A,"How do I destruct data associated with an object after the object no longer exists? I'm creating a class (say C) that associates data (say D) with an object (say O). When O is destructed O will notify C that it soon will no longer exist :( ... Later when C feels it is the right time C will let go of what belonged to O namely D. If D can be any type of object what's the best way for C to be able to execute ""delete D;""? And what if D is an array of objects? My solution is to have D derive from a base class that C has knowledge of. When the time comes C calls delete on a pointer to the base class. I've also considered storing void pointers and calling delete but I found out that's undefined behavior and doesn't call D's destructor. I considered that templates could be a novel solution but I couldn't work that idea out. Here's what I have so far for C minus some details:  // This class is C in the above description. There may be many instances of C. class Context { public: // D will inherit from this class class Data { public: virtual ~Data() {} }; Context(); ~Context(); // Associates an owner (O) with its data (D) void add(const void* owner Data* data); // O calls this when he knows its the end (O's destructor). // All instances of C are now aware that O is gone and its time to get rid // of all associated instances of D. static void purge (const void* owner); // This is called periodically in the application. It checks whether // O has called purge and calls ""delete D;"" void refresh(); // Side note: sometimes O needs access to D Data *get (const void *owner); private: // Used for mapping owners (O) to data (D) std::map _data; }; // Here's an example of O class Mesh { public: ~Mesh() { Context::purge(this); } void init(Context& c) const { Data* data = new Data; // GL initialization here c.add(this new Data); } void render(Context& c) const { Data* data = c.get(this); } private: // And here's an example of D struct Data : public Context::Data { ~Data() { glDeleteBuffers(1 &vbo); glDeleteTextures(1 &texture); } GLint vbo; GLint texture; }; }; P.S. If you're familiar with computer graphics and VR I'm creating a class that separates an object's per-context data (e.g. OpenGL VBO IDs) from its per-application data (e.g. an array of vertices) and frees the per-context data at the appropriate time (when the matching rendering context is current). So if I understand correctly you want reference counting? Thanks all. In my program there is an update thread and a render thread. The render thread is responsible for rendering twice once for each eye to create a stereo image. Each 3D object O has eye-specific rendering data D for each context (i.e. for each O there are two copies of D). The problem: O can be created or destroyed in the update thread but O's eye-specific rendering data must be destroyed in the render thread and only when rendering the associated eye. C needs to know if O is destructed so that in the render thread it will know to destroy both instances of D. The question is rather vague on the requirements so it's hard to give a good concrete answer. I hope the following helps. If you want the data to disappear immediately when its owner dies have the owner delete it (and notify C if the C instances need to know). If you want C to do the deletion at its leisure your solution looks fine. Deriving from Data seems to me the right thing to do. (Of course it is crucial that ~Data() be virtual as you have done.) What if D is an array of objects? There are two interpretations of this question. If you mean that D is always an array let it be an array (or vector<>) of pointers to Data. Then in C::purge() walk the vector and delete the objects. If you mean that D could be an array of objects but could also be a single object there are two ways to go. Either decide that it is always an array (possibly of size 1) or that it is a single object (derived from Data) which can be a class wrapping the array of the actual objects (or pointers to them). In the latter case the wrapper class destructor should walk the array and do the deletions. Note that if you want the array (or vector<>) to contains the actual objects not pointers to them (in which case you won't have to walk the array and delete manually) then you'll have the following limitations. 1. All objects in the array will have to be of the same actual type. 2. You will have to declare the array to be of that type. This will lose you all the benfits of polymorphism.  What you're looking for is Boost::shared_ptr or some similar smart-pointer system.  To answer the question ""What if D is an array of objects ?"" I'd suggest a vector<> but you'd have to associate it with D:  struct D_vector :D { vector<whatever> vw; };"
353,A,"Java image display I am trying to create a GUI that will coexist with several images. There is one background image that will take up the most room. On top of the background image several other images will be displayed in varying locations. These locations will be updated every 5 seconds or so so speed is not a huge factor. So all I need is the ability to display or remove images from display at the coordinates of my choosing upon each update. I am using netbeans to put it all together. Here is the most reasonable code I have come across to get the job done so far but I don't know how to put it into practice or whether it can accomplish what I need. A short simple explanation of what a JPanel is would also be helpful. public class ImagePanel extends JPanel { private Image img; public void setImage(String img) { setImage(new ImageIcon(img).getImage()); } public void setImage(Image img) { int width = this.getWidth(); int height = (int) (((double) img.getHeight(null) / img.getWidth(null)) * width); this.img = img.getScaledInstance(width height Image.SCALE_SMOOTH); } @Override public void paintComponent(Graphics g) { g.drawImage(img 0 0 this); } } And here is the code from the main class that I have so far. I can easily add code to determine whether or not given images should be displayed and where they need to be but I don't know where to put what. public class ClientWindow extends javax.swing.JFrame { /** Creates new form ClientWindow */ public ClientWindow() { initComponents(); imagePanel1.setImage(""C:\\Documents and Settings\\Robert\\Desktop\\ClientServer\\Poker Table Art\\TableAndChairs.png""); } /** This method is called from within the constructor to * initialize the form. * WARNING: Do NOT modify this code. The content of this method is * always regenerated by the Form Editor. */ @SuppressWarnings(""unchecked"") // <editor-fold defaultstate=""collapsed"" desc=""Generated Code""> private void initComponents() { jScrollPane1 = new javax.swing.JScrollPane(); jTextField1 = new javax.swing.JTextField(); jScrollPane2 = new javax.swing.JScrollPane(); jTextArea1 = new javax.swing.JTextArea(); jCheckBox2 = new javax.swing.JCheckBox(); imagePanel1 = new Pokertable.ImagePanel(); imagePanel3 = new Pokertable.ImagePanel(); jPanel1 = new javax.swing.JPanel(); setDefaultCloseOperation(javax.swing.WindowConstants.EXIT_ON_CLOSE); setTitle(""Not Logged In""); getContentPane().setLayout(null); jTextField1.addKeyListener(new java.awt.event.KeyAdapter() { public void keyTyped(java.awt.event.KeyEvent evt) { jTextField1KeyTyped(evt); } }); jScrollPane1.setViewportView(jTextField1); getContentPane().add(jScrollPane1); jScrollPane1.setBounds(0 540 170 22); jTextArea1.setColumns(20); jTextArea1.setRows(5); jScrollPane2.setViewportView(jTextArea1); getContentPane().add(jScrollPane2); jScrollPane2.setBounds(0 440 166 96); jCheckBox2.setText(""Sit Out Next Hand""); jCheckBox2.addActionListener(new java.awt.event.ActionListener() { public void actionPerformed(java.awt.event.ActionEvent evt) { jCheckBox2ActionPerformed(evt); } }); getContentPane().add(jCheckBox2); jCheckBox2.setBounds(0 410 113 23); imagePanel1.add(imagePanel3); imagePanel1.add(jPanel1); getContentPane().add(imagePanel1); imagePanel1.setBounds(0 0 520 370); pack(); }// </editor-fold> private void jCheckBox2ActionPerformed(java.awt.event.ActionEvent evt) { // TODO add your handling code here: } private void jTextField1KeyTyped(java.awt.event.KeyEvent evt) { // TODO add your handling code here: } /** * @param args the command line arguments */ public static void main(String args[]) { java.awt.EventQueue.invokeLater(new Runnable() { public void run() { new ClientWindow().setVisible(true); } }); } // Variables declaration - do not modify private Pokertable.ImagePanel imagePanel1; private Pokertable.ImagePanel imagePanel3; private javax.swing.JCheckBox jCheckBox2; private javax.swing.JPanel jPanel1; private javax.swing.JScrollPane jScrollPane1; private javax.swing.JScrollPane jScrollPane2; private javax.swing.JTextArea jTextArea1; private javax.swing.JTextField jTextField1; // End of variables declaration } For simplicity suppose that I want to have the TableAndChairs.png as a stable background image and the Button.png image (the D with a circle around it) to move to a new location every so often. read a swing tutorial and some things might get clearer for you :) If each of your panels were one of these ImagePanels and you simply had it take another pair of ints as argument your final paint method would simply paint your n different image panels onto the main graphics object: public class Example extends JApplet{ ImagePanel[] images; //whatever code you want for the rest of your applet public void paint(){ for(ImagePanel i : images) images.paintComponent(); } } } and you would fix ImagePanel to be: @Override public void paintComponent(Graphics g) { g.drawImage(img x y this); } this is an application not an applet. will that affect the fact that you are overriding the paint() method? I don't believe so. The underlying idea is the same though. I only have JApplet code but if you post your main class I can give you some more help for the images themselves there isn't really any logic to them so events don't need to be handled. But i'm not sure WHERE code goes to update everything and how to control the properties of the components being updated. Yeah. You should read some tutorials or play about (I learned by playing about). but your main problem is that you don't seem to have overridden the paint method. The signature is public void paint(Graphics g) -- either that or you need to be calling repaint. I'm not entirely sure. Also it might be easier to be using a JApplet and then put that into a JFrame to make it an application  Use your ImagePanel to display the background image. Then set the panel to use a null layout manager. Now you can add an ImageIcon to a JLabel and add the JLabel to the panel. Make sure you use: label.setSize( label.getPreferredSize() ); otherwise the label won't appear. By default the label will appear at location (0 0) although you can change this using the setLocation(...) method. To do the animation you can use a Swing Timer and set it to fire every 5 seconds. When the Timer fires you move the image by changing its location. If you don't know how to use a Timer then read the section from the Swing tutorial on ""How to Use Timers"". They are being updated sporadically not necessarily every x seconds. This is a game being written so every time someone takes an action the display needs to be updated. It doesn't change my answer of course some ""event"" has to happen for the image to move. That event could be regularly scheduled (by using a Timer) or is could be a user controlled event (by clicking a button for example). The point is whenever the event occurs you have to do ""something"". Without knowing the details of what you are doing I'm just suggestion that ""something"" should be setting the location of the icon so it looks like its moving around the board from player to player.  just an idea: use an additional class for holding the images and corresponding coordinates and put them into a list. public class MyImage { // or SubImage? private Image image; private int x; private int y; public MyImage(Image image int x int y) { this.image = image; this.x = x; this.y = y; } public void paint(Graphics g ImageObserver obs) { g.drawImage(image x y obs); } // getters and setters } public class ImagePanel extends JPanel { private Image img; private final List<MyImage> images = new ArrayList<MyImage>(); // setImage methods public void addMyImage(MyImage image) { images.add(image); } // or/and public void addMyImage(Image image int x int y) { images.add(new MyImage(image x y)); } @Override public void paintComponent(Graphics g) { g.drawImage(img 0 0 this); // check if img is null? for (MyImage image : images) { image.paint(g this); } } Why the downvote? not much helpful downvoting without comment... or didn't you like the comment I did?  A better approach doing this would be by extending JComponent and overriding the paintComponent method to draw your custom Table and Chairs component. You could use the foundations of Circle Trigonometry to calculate each circle dynamically. I have done a quick example to show you how this could be simple done using Java2D. There is a lot of performance tweaks that could be done such as caching clipping etc. The reason why did this code is for you to understand how simple it is to draw them yourself. The reasons why to do this you have more control over what is being drawn and the location its being drawn. public class JTableChairs extends JComponent { private final int TABLE_DIAMETER = 300; private final int CHAIR_DIAMETER = 50; private final int CHAIR_TABLE_BOUND = 50; private final int CANVAS_WIDTH = 500; private final int NUMBER_OF_CHAIRS = 10; private final int TABLE_X = (CANVAS_WIDTH - TABLE_DIAMETER) / 2; private final int TABLE_Y = TABLE_X; private final double TABLE_CENTER_X = TABLE_DIAMETER / 2.0 + TABLE_X; private final double TABLE_CENTER_Y = TABLE_DIAMETER / 2.0 + TABLE_Y; private final double TABLE_RADIUS = TABLE_DIAMETER / 2.0 + CHAIR_TABLE_BOUND; public JTableChairs() { super(); } @Override public void paintComponent(Graphics g) { g.setColor(Color.WHITE); g.fillRect(0 0 getSize().width getSize().height); // Draw Table. g.setColor(Color.BLACK); g.fillOval(TABLE_X TABLE_Y TABLE_DIAMETER TABLE_DIAMETER); // Draw each chair using normal circle trig functions. for (int i = 1; i <= NUMBER_OF_CHAIRS; i++) { Rectangle r = getSeatCoordinates(i); g.drawOval(r.x r.y r.width r.height); g.drawString(String.valueOf(i) r.x r.y); } } public Rectangle getSeatCoordinates(int seat) { // Clamp from 0 to number of chairs to make sure it doesn't // go over/under bounds. seat = seat < 1 ? 1 : (seat > NUMBER_OF_CHAIRS ? NUMBER_OF_CHAIRS : seat) - 1; // Calculate the coordinates using trig functions. double arcAngle = Math.toRadians(360 / NUMBER_OF_CHAIRS * seat); int x = (int)(TABLE_CENTER_X + TABLE_RADIUS * Math.cos(arcAngle) - CHAIR_DIAMETER / 2.0); int y = (int)(TABLE_CENTER_Y + TABLE_RADIUS * Math.sin(arcAngle) - CHAIR_DIAMETER / 2.0); return new Rectangle(new Point(x y) new Dimension(CHAIR_DIAMETER CHAIR_DIAMETER)); } } Say you need to do some listener so when you press any circle you want to highlight it. Using collision detection in 2D you can do that. Note the ugly way doing it is the following:  public boolean setSeat(Point coord) { // Quick collision detection. This is bad performance since its // checking each point. for (int i = 1; i <= NUMBER_OF_CHAIRS; i++) { Rectangle r = getSeatCoordinates(i); if (r.contains(coord)) { Graphics g = getGraphics(); g.setColor(Color.RED); g.drawRect(r.x r.y r.width r.height); g.dispose(); return true; } } return false; } And in the constructor you can add a mouse listener: addMouseListener(new MouseAdapter() { @Override public void mouseReleased(MouseEvent e) { setSeat(e.getPoint()); } }); I hope you learned a thing or two :x"
354,A,Anyone know a good tool (ASP.NET or compatible) for generating Sprites and the corresponding CSS from a set of images? I'm at the end of an ASP.NET project and we've been using YSlow to analyze our site. There are a ton of images and we'd like to combine some of them into a single request. Any suggestions? There are a good few online sprite & css generators. I assume youll approach this as a one off excercise rather than wanting to integrate it into your build cycle. Here is one for starters Thank you that's just what I was looking for!
355,A,"OpenGL: projecting mouse click onto geometry I have this view set: glMatrixMode(GL_MODELVIEW); //Switch to the drawing perspective glLoadIdentity(); //Reset the drawing perspective and I get a screen position (sx sy) from a mouse click. Given a value of z how can I calculate x and y in 3d-space from sx and sy? libqglviewer has a good selection framework if that's what you're looking for  This is best answered by the most authoritative source OpenGL's web site. Putting the name of the source in your answer would be helpful rather than having to click on the link to find out.  This is actually dependent on the projection matrix not the model-view matrix. http://www.toymaker.info/Games/html/picking.html should help you out -- it's D3D-centric but the theory is the same. If you're looking to do mouse-based picking though I suggest using the selection rendering mode. Edit: Mind you the model-view matrix does come into play but since yours is identity it's a nonissue.  You should use gluUnProject: First compute the ""unprojection"" to the near plane: GLdouble modelMatrix[16]; GLdouble projMatrix[16]; GLint viewport[4]; glGetIntegerv(GL_VIEWPORT viewport); glGetDoublev(GL_MODELVIEW_MATRIX modelMatrix); glGetDoublev(GL_PROJECTION_MATRIX projMatrix); GLdouble x y z; gluUnProject(sx viewport[1] + viewport[3] - sy 0 modelMatrix projMatrix viewport &x &y &z); and then to the far plane: // replace the above gluUnProject call with gluUnProject(sx viewport[1] + viewport[3] - sy 1 modelMatrix projMatrix viewport &x &y &z); Now you've got a line in world coordinates that traces out all possible points you could have been clicking on. So now you just need to interpolate: suppose you're given the z-coordinate: GLfloat nearv[3] farv[3]; // already computed as above if(nearv[2] == farv[2]) // this means we have no solutions return; GLfloat t = (nearv[2] - z) / (nearv[2] - farv[2]); // so here are the desired (x y) coordinates GLfloat x = nearv[0] + (farv[0] - nearv[0]) * t y = nearv[1] + (farv[1] - nearv[1]) * t; Sorry for the noob comment. Is the third parameter in gluUnproject calls always same(0 and 1) or does it depend on the zNear and zFar values when we're setting the projection matrix with gluPerspective?"
356,A,"Displaying SVG in OpenGL without intermediate raster I have some simple SVG artwork (icon and glyph kind of things) which I want to display in an OpenGL app (developing in C++ on Debian using Qt). The obvious solution is to use the ImageMagick libs to convert the SVGs to raster images and texture map them onto some suitable polygons (or just use good old glDrawPixels). However I'm wondering if there's anything out there which will translate the SVG directly to a sequence of OpenGL calls and render it using OpenGL's lines polygons and the like. Anyone know of anything which can do this ? hmm.. tricky problem when bezier curves are involved. would be nice to see a solution as the two technologies complement each other nicely. This question has been asked again at http://stackoverflow.com/questions/6287650/rendering-svg-with-opengl-and-opengl-es/ http://lii-enac.fr/~conversy/research/sauvage/index.html This seems the closest to what I originally had in mind. The pythonness of it might not actually be a problem as I'm toying with doing the app front-end using PyQt4 rather than pure C++.  It looks like Inkscape has some interesting export options you may find useful. They include DXF PovRay EPS PS (PostScript) XAML Latex and OpenDocument Drawing (ODG). Perhaps there is a converter for one of those and you could use Inkscape as an intermediary. DXF in particular is a likely candidate since it is a common 3D format already. assimp is a good candidate for loading DXF. Inkscape doesn't have yet a full implementation of SVG so I would imagine that its export options are also partial. Well actually all the SVG I'm using is created in Inkscape anyway so as long as it can export all it's own stuff... no problem. You're going to more limited by the output format than SVG anyway. DXF is like a million years old it isn't going to support crop marks or complex gradients.  My answer is going to about displaying vector graphics wtih OpenGL in general because all solutions for this problem can support rather trivially SVG in particular although none support animated SVGs (SMIL). Since there was nothing said about animation I assume the question implied static SVGs only. Since 2011 the state of the art is Mark Kilgard's baby NV_path_rendering which is currently only a vendor (Nvidia) extension as you might have guessed already from its name. There are a lot of materials on that: https://developer.nvidia.com/nv-path-rendering Nvidia hub but some material on the landing page is not the most up-to-date http://developer.download.nvidia.com/devzone/devcenter/gamegraphics/files/opengl/gpupathrender.pdf Siggraph 2012 paper http://on-demand.gputechconf.com/gtc/2014/presentations/S4810-accelerating-vector-graphics-mobile-web.pdf GTC 2014 presentation http://www.opengl.org/registry/specs/NV/path_rendering.txt official extension doc You can of course load SVGs and such https://www.youtube.com/watch?v=bCrohG6PJQE. They also support the PostScript syntax for paths. You can also mix path rendering with other OpenGL (3D) stuff as demoed at: https://www.youtube.com/watch?v=FVYl4o1rgIs https://www.youtube.com/watch?v=yZBXGLlmg2U NV_path_rendering is now used by Google's Skia library behind the scenes when available. (Nvidia contributed the code in late 2013 and 2014.) One of the cairo devs (who is an Intel employee as well) seems to like it too http://lists.cairographics.org/archives/cairo/2013-March/024134.html although I'm not [yet] aware of any concrete efforts for cairo to use NV_path_rendering. An upstart having even less (or downright no) vendor support or academic glitz is NanoVG which is currently developed and maintained. (https://github.com/memononen/nanovg) Given the number of 2D libraries over OpenGL that have come and gone over time you're taking a big bet using something not supported by a major vendor in my humble opinion.  SVGL appears to address this but has been dormant for several years. Still you might be able to find some code of value there. The ""sauvage"" project mentioned by ""dru"" below appears to be the successor. Same author but python rather than C++.  Qt can do this. QSvgRenderer can take an SVG and paint it over a QGLWidget Its possibly you'll need to fiddle around with the paintEvent() abit if you want to draw anything else on the QGLWidget other than the SVG. Thanks very interesting (should have noticed that one myself). It's unclear to me from the documentation whether the vector aspect would be preserved completely in QPainter-on-QGLWidget or whether some intermediate rasterization happens. Ought to be easy enough to find out using GLintercept/GLtrace though... QPainter doesn't usually do any rasterization. There's no reason for it to start doing it in the case of SVG. I also note that enabling automatic multisampling in the Nvidia drivers also makes the SVG graphics look very nice and antialiased (which wouldn't happen if Qt was just blitting some software-rendered bitmap). After getting GLTrace to work with Qt4 apps on my platform (simple fix sent to author) I can confirm the SVG is being drawn on the GLWidget using an impressive number of gl*calls. Trolltech provide a nice example of combining 2D & 3D worlds at http://doc.trolltech.com/4.4/opengl-overpainting.html. The only thing I can imagine wanting to do further would be capturing the SVG render calls in a GL display list (although the only reason to do that would be performance and if that became a problem I'd be more likely to cache rendered bitmaps instead). I'd send a feature request for that using the bug report web form anyway.  there's also tkzinc as a possibility"
357,A,"In terms of OO design can two-d and the three-d point classes be derived from single base class? I`m currently working out the design for simple graphic editor who support trivial operations for two-dimensional and three-d shapes. The point is I want to render prototype of these shapes as MsPaint does. And at the moment it is rendering I need to store somewhere pixels from the canvas which get covered by prototype just in case when prototype changes to restore their state on the canvas. So I want all my shapes to support this kind of buffering (Graphic Operation is the base class for all rendering operations): public abstract class Shape: GraphicOperation { protected List<SomePoint> backup; public Shape(Color c): base(c) { } public Color FigureColor { get { return color; } protected set { color = value; } } public abstract void renderPrototype(Bitmap canvasToDrawOn); } The main idea is that in terms of OO design it would be great to provide the support of the buffer on base class (Shape) level I mean for TwoDShape and ThreeDShape classes this list must be initialized in different way - for TwoDShape with TwoDPoint instances and for ThreeDShape with ThreeDPoint instances. But to do this SomePoint must be base class for both two-dimensional point and three-dimensional point classes. Is it acceptable in terms of OO to derive both these classes from single base class? May be there are too many words but I just wanted the problem to be clear for everyone. Edit: btw is this the good idea to derive point classes from their king of shapes? I personally see no other options but may be it would be better if I derive it directly from shape? Now it is: public abstract class TwoDShape : Shape { protected List<SomePoint> backup; public TwoDShape(Color c) : base(c) { } } public class TwoDPoint: TwoDShape { //... } and the same is for ThreeDPoint. I don't see any reason not to. But if the only difference between Shapes is in the type of Points they contain why not make the Shape class itself a template or a ""generic"" in Java parlance? it`s C# actually but I don`t think making template classes is what I will get benefit from in terms of flexible OO design  What do 2d and 3d points have in common exactly? If it is just ""when drawing them onto a surface they replace pixels which should be backed up"" that sounds more like composition than inheritance to me. Both 2d and 3d shapes have a buffer of pixels they're displacing. Anyway can you always use both 2d and 3d shapes when the base class is expected? If so give them a common base class. Otherwise don't. Base your class design on how the objects are used. If they are used in a context where they have a common base implement it as a common base class. If it is just a matter of ""I can't be bothered adding a data member to both"" inheritance is probably not the right tool. but all my shapes - both point classes included - are derived from Shape - so they`ll got this list as inherited member.  I can't think of any way to do this that doesn't violate the Liskov substitution principle in some subtle (or not-so-subtle) way. This is a classic example of why not to go crazy with inheritance just because two classes share some fields e.g. here or here. In the end even if you make it work it'll probably have so many gotchas and restrictions on the methods you do write I can't imagine it'll be a net win. Very good answer I think this is what I wanted to say but phrased much better.  Couldn't you make ThreeDimensionalPoint derive itself from TwoDimensionalPoint? public class ThreeDimensionalPoint : TwoDimensionalPoint { } I could see a lot of reusable code (methods/properties) in 2D that will be the same in 3D... And you might want to look at a 3D from a 2d standpoint and then all you would need to do is cast it. Just my thougts... Hm I can't think of much that'd be reusable between the two. Care to elaborate? A 3d shape is *not* a 2d shape. It can be projected onto a 2d plane but it is not a two dimensional shape. I can but it will be not so reusable decision and the most important thing is I must bring this list with restored points down one level in hierarchy - so it`s not so flexible either jalf - I agree in terms of OO derive one from another is not the better idea 2d point xy 3d point xyz I don't know I definitely not a graphics/games expert... I agree with jalf. If anything a 2d point should inherit from a 3d point because a 2d point IS-A 3d point. Good sub-classes generally constrain the concept the base class represents not extend it (e.g square inherits from rectangle inherits from polygon inherits from shape). timday: square only inherits from rectangle if both are immutable -- that's the *other* classic example of ""things that look like simple inheritance but are actually full of gotchas"". :-)  A simpler solution and one that will avoid some subtle design issues is to just treat all points as 3D and simply ignore the Z coordinate if you're working within a 2D context. You can consider a 2D shape to simply be a 3D shape that happens to lie on a flat plane.  There's an entirely general rule of OOD that states: Favor composition over inheritance. What this means in practice is that you shouldn't use inheritance for code reuse. It is still legal and valid to use inheritance if the motiviation for doing so is to take advantage of polymorphism. In your case if you can manage to write the abstract class/interface SomePoint in such a way that you can deal only with it on that level and never need to downcast you should definitely go for it. If on the other hand you find that you need to downcast instances to say TwoDPoint or ThreeDPoint then you gain nothing from inheritance and you would be breaking the Liskov Substitution Principle. If this is the case you should consider a design where you can implement reuse without resorting to inheritance - perhaps using a Service or a Strategy.  In games the 2d point and 3d pointtypically don't inherit from eachother but to be honest I can't think of any fundamental reason for this. So yeah it should be fine as for naming they are typically vector2 and vector3 so they can hold dimensions points and colors all in the same object."
358,A,multi layer screen like photoshop in .Net I want to write a utility that it works like Adobe Photoshop. if you work with this product you meet a capability that we able to create a new layer and modify it easily. now i should write a code that draw a shape in run-time and insert several other shape with own Right-Click options. I don't want specifically it it should create a simple shape(it's a symbole e.g AND Gate) and control it. what is your opinion? OoOo it written in C# 3.5 thanks in advance. Like Paint.Net? maybe you should take a look at Paint.Net. you can get the source code ( http://blog.getpaint.net/2007/12/15/paintnet-v320-source-code-now-available/) Paint.Net stopped offering source code some time ago Paint.NET is an Open Source Photoshop-like program with layers written in C#. http://www.getpaint.net You may want to start by looking there at the app and source code for ideas. You will also want to decide if you are creating vector-based or bitmap-based image editing app. Good luck. I don't want specifically it it should create a simple shape(it's a symbole e.g AND Gate) and control it. Take a look at http://www.easydiagram.net/. It's an open source diagram control for .NET. It may give you some of what you're looking for as well. Paint.Net is not open source. Check thier site. @Nissan Fan: Paint.Net has not shared it source code in a few years now. CHECK THE SITE Yes it is. Open source means anyone can use the source code but it doesn't mean anyone can contribute to it.  I use usually Gimp. It is good programm for free:-)
359,A,"What is the best tool to allow web users to create flowcharts/diagrams? I'm creating a web application in which teachers need to be able to easily create educational diagrams flowcharts basically: boxes and circles with text in them text labels ability to upload and embed graphics easy to make lines with arrows etc. lots of ready-made icons and clipart would be nice free and opensource would be nice Basically I'm looking for a website RichTextBox editor but with basic Visio functionality which would allow the user to: click on ""create diagram"" create it WYSIWYG with the mouse save it (on the server) use it in the application It could use Javascript/jQuery Flash or Silverlight doesn't matter as long as it runs in a browser. Have you looked at GoDiagram? I've not tried it but it's the only component I've seen that fulfils this need.  Gliffy have a free option that is supported by ads. As well as having a decent API. I like the look of the service but haven't used them yet. (it's in the pipeline...)  look at Microsoft Chart Controls for Microsoft .NET Framework 3.5.  The Firefox Pencil Add-on allows users to make diagrams and save them as .png files: No clue about points myself and actually I don't care really about that. I have been looking at Processing.js with the encanvas.js for something similar to what you are proposing - have to develop your own handling for that but seems possible. I tried all of these out and finally found the yEd editor which is the best answer to this question. I posted links to all the tools I found and choose the yEd one as the final answer so that other people who find this question will know what best answers this question and since I assume Stackoverflow will exist from now to the end of time this is a place for me to store links for myself to refer to later I don't think you even get points for answering your own question do you? smells like a plug to just answer your own question to me  I suggest flash. I have seen sites that implement this kind of stuffs easily in Flash. You can achieve the same using html and JavaScript too but it will need painful programming. Silverligt in my opinion is not matured yet. But I am sure we can see some improvements soon. And may be you can consider Java applets too. Check the site www.geni.com. they have a family tree building interface there done in Flash. I think it will need ""painful programming"" in all cases... :-) The level of pain depending on your familiarity with the language/system and the knowledge of routing algorithms. True! And JavaScript is a bit more notorious for pain :)  I think your best option could be to use Google Docs Sketchy (the drawing feature in Google docs) is very powerful and I think will do what you want.  https://bubbl.us/beta is very easy to use and create PNGs but quite limited in features the google docs solution is good (thanks @Dave Beer) but the size of the screen is static you can't make large PNGs: http://superuser.com/questions/76630/is-there-a-way-to-change-the-size-of-the-drawing-area-in-google-docs-insert-dra Also not to be missed: yEd: http://www.yworks.com/en/products%5Fyed%5Fabout.html I did some more research on this and choose yEd as the tool we will use for the project here are some examples of what yEd Google Docs and Firefox Pencil can do: http://tanguay.info/web/examples/googleDocsImageDraw/ There are lots of these if you keep looking around here some more each with their advantages and disadvantages: http://www.xmind.net/downloads http://live.gnome.org/Dia http://yuml.me"
360,A,"How to draw a circle in Flex MXML file? In my MXML file I have a tab navigator with three vboxes. <mx:TabNavigator width=""624"" height=""100%""> <mx:VBox label=""Currents Quote"" width=""100%""> </mx:VBox> <mx:VBox label=""Quote Comparison"" width=""100%""> </mx:VBox> <mx:VBox label=""Reports"" width=""100%""> </mx:VBox> </mx:TabNavigator> Inside the VBox ""Current Quote"" I want a circle to be drawn. How can I achieve it? There can be one more option to create a circle eclipse Create Box with background color (if you want to fill it with any color) and with specific width and height and provide corner radius with exactly half of width example : <mx:Box cornerRadius=""3"" borderStyle=""solid"" borderColor=""0x5F4C0B"" backgroundColor=""0x5F4C0B"" width=""6"" height=""6"" />  Try the following code:  <s:Ellipse x=""60"" y=""60"" width=""80"" height=""80""> <s:stroke> <s:LinearGradientStroke rotation=""90""> <s:entries> <s:GradientEntry color=""white"" ratio=""0.5""/> <s:GradientEntry color=""black"" ratio=""0.5"" /> </s:entries> </s:LinearGradientStroke> </s:stroke> </s:Ellipse>  Embellishing for drag and drop etc... Yes pretty much anything is possible. If you don't want to use a pie chart (which I recommend you look at because it might make a lot of the drawing code very simple) then you need to embellish the MyCircle class above to trap the mouseDown event and to enable dragging from it using a DragSource object. package mypackage { class MyCircle extends UIComponent { ...snip... // to initiate a drag from this object private function MouseDown(e:MouseEvent):void { var ds:DragSource = new DragSource(); if (data) { // I'm adding the object's data to it but you need to decide what you want in here ds.addData(data ""MyDragAction""); mx.managers.DragManager.doDrag(this ds e); } } // to handle a drop private function HandleDrop(e:DragEvent):void { mx.managers.DragManager.acceptDragDrop(e.currentTarget); // you can get at whatever you put in the drag event here } } } You'll need to set these functions (and whichever else you wind up supporting for drag/drop) as event listeners on this object. You can do that in the object's constructor you just have to make sure you are listening for the right event. You have some digging to do and the adobe docs are your friend but it is all perfectly possible. As you add more to MyCircle you will get greater benefit from having it factored out into a separate control.  There's no MXML circle defined so you have to create a circle control yourself and you can then include it in your MXML. package mypackage { class MyCircle extends UIComponent { public var x:int; public var y:int; public var radius:int; override protected function updateDisplayList(unscaledWidth:Number unscaledHeight:Number):void { graphics.drawCircle(x y radius); } } } then in your MXML you can use your custom control if you declare a namespace to refer to it at the top of your containing control... <mx:VBox label=""Currents Quote"" width=""100%""> <mycontrols:MyCircle x=""30"" y=""30"" radius=""30""/> </mx:VBox> Code above was typed into the StackOverflow editor so check it works! There's a lot of help on the web for extending UIComponent and Sprite. The Adobe docs are pretty comprehensive. EDIT: Include your controls as a namespace in the enclosing control or application <mx:Application xmlns:mx=""http://www.adobe.com/2006/mxml"" xmlns:mycontrols=""mypackage.*"" > <mx:Script> HTH Like a pie chart but not that.. I need to load images n values into the separate sectors of this wheel like structure.. enabling Drag n drop is also required but I don't know till what extent it is possible. Can u tell me if its possible? see my second answer. Is the Circle control to be created as an .as file? yes as a separate class in its own file If I create as an .as file It is not being included as a component with a name space?? If you are using FlexBuilder then create an ActionScript class in a source folder in your project. Then it will be visible to the IDE in code completion or in the design editor. If you include the custom control in your MXML file the IDE will automatically add a default namespace for you. If you are doing this all manually then you need to give the class a package name and refer to that package in the namespace declarations at the top your MXML file. I'll add a declaration syntax to my answer. it shows an error `1071: Syntax error: expected a definition keyword (such as function) after attribute public not x.` in the line `public x:int;` Can you tell what's wrong? My code is not correct above (I did include a disclaimer and I flip between Java and AS3 so I am prone to get it wrong when typing code in here). I'll correct it but you really ought to know how to do that yourself it is the very basics of the language construct. I have corrected the errorsthe method signature in the function didnt matchand needed to add var for the variables. Also have to add line style to make the circle visible. `graphics.lineStyle(1 0x000000);` Thanks for ur help :-) Also can u guide me in how to divide the circle into sectors? I need the circle that is created to be divided to 6 sectors. Do you really want a pie chart?  So in Flex this is a possibility: var mySprite:Sprite = new Sprite(); mySprite.graphics.beginFill(0xFFCC00); mySprite.graphics.drawCircle(30 30 30); this.addChild(mySprite); That should work :)  Take a look at Degrafa. While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes. ""Link-only answers can become invalid if the linked page changes."" - Yeah the website disappeared."
361,A,"GDI GradientFill not working on offscreen bitmap I am trying to use the GDI GradientFill function to draw on a offscreen bitmap then BitBlt that to the screen. But I always get a black bitmap... if I GradientFill directly to the screen it works. Below is a sample app to see what I mean. #pragma comment(lib ""msimg32.lib"") #include <windows.h> const CHAR c_szWndClass[] = ""GradientTestWnd""; const CHAR c_szWndTitle[] = ""GradientTest""; const int c_nWndWidth = 1024; const int c_nWndHeight = 768; int WINAPI WinMain( HINSTANCE hInstance HINSTANCE hPrevInstance LPSTR lpCmdLine int nCmdShow ) { WNDCLASSEX wcx; ZeroMemory(&wcx sizeof(wcx)); wcx.cbSize = sizeof(wcx); wcx.style = CS_HREDRAW | CS_VREDRAW | CS_OWNDC; wcx.lpfnWndProc = DefWindowProc; wcx.hInstance = hInstance; wcx.lpszClassName = c_szWndClass; RegisterClassEx(&wcx); HWND hwndMain = CreateWindowEx( 0 c_szWndClass c_szWndTitle WS_OVERLAPPEDWINDOW CW_USEDEFAULT CW_USEDEFAULT c_nWndWidth c_nWndHeight NULL NULL hInstance NULL); ShowWindow(hwndMain SW_SHOW); HDC hdc; hdc = GetDC(hwndMain); HDC hdcOffscreen = CreateCompatibleDC(hdc); HBITMAP bitmap = CreateCompatibleBitmap(hdcOffscreen c_nWndWidth c_nWndHeight); HBITMAP old_bitmap = (HBITMAP) SelectObject(hdcOffscreen bitmap); TRIVERTEX vertices[2]; ZeroMemory(&vertices sizeof(vertices)); vertices[0].Red = 0xFF00; vertices[0].Green = 0x0000; vertices[0].Blue = 0x0000; vertices[0].x = 0; vertices[0].y = 0; vertices[1].Red = 0x0000; vertices[1].Green = 0x0000; vertices[1].Blue = 0xFF00; vertices[1].x = c_nWndWidth; vertices[1].y = c_nWndHeight; GRADIENT_RECT rects[1]; ZeroMemory(&rects sizeof(rects)); rects[0].UpperLeft = 0; rects[0].LowerRight = 1; // This works //GradientFill(hdc vertices 2 rects 1 GRADIENT_FILL_RECT_V); // This doesn't GradientFill(hdcOffscreen vertices 2 rects 1 GRADIENT_FILL_RECT_V); BitBlt(hdc 0 0 c_nWndWidth c_nWndHeight hdcOffscreen 0 0 SRCCOPY); Sleep(5000); SelectObject(hdcOffscreen old_bitmap); DeleteObject(bitmap); DeleteDC(hdcOffscreen); return 0; } the problem here is actually due to the initial state of the device context from which you are creating the compatible bitmap - in this line: HBITMAP bitmap = CreateCompatibleBitmap(hdcOffscreen c_nWndWidth c_nWndHeight); hdcOffscreen should instead be hdc - this is because the device context created here: HDC hdcOffscreen = CreateCompatibleDC(hdc); has by default a 1x1 monochrome bitmap selected into it - when you attempt to create a compatible bitmap from that you will get a monochrome bitmap also. so if you do this instead: HBITMAP bitmap = CreateCompatibleBitmap(hdc c_nWndWidth c_nWndHeight); you should see your gradient :) old question that appears to be answered but i thought id just help clarify on exactly why its not working! details/links: http://msdn.microsoft.com/en-us/library/dd183489%28VS.85%29.aspx When the memory DC is created its display surface is exactly one monochrome pixel wide and one monochrome pixel high http://msdn.microsoft.com/en-us/library/dd183488%28v=VS.85%29.aspx The color format of the bitmap created by the CreateCompatibleBitmap function matches the color format of the device identified by the hdc parameter hth :) Thanks I just tested it and it works!  Create a DIB instead of a compatible bitmap. Replace HBITMAP bitmap = CreateCompatibleBitmap(hdcOffscreen c_nWndWidth c_nWndHeight); with BITMAPINFO BitmapInfo; memset(&BitmapInfo 0 sizeof(BITMAPINFOHEADER)); BitmapInfo.bmiHeader.biSize = sizeof(BITMAPINFOHEADER); BitmapInfo.bmiHeader.biWidth = c_nWndWidth; BitmapInfo.bmiHeader.biHeight = c_nWndHeight; BitmapInfo.bmiHeader.biPlanes = 1; BitmapInfo.bmiHeader.biBitCount = 32; BitmapInfo.bmiHeader.biCompression = BI_RGB; HBITMAP bitmap = CreateDIBSection(hdcOffscreen &BitmapInfo DIB_RGB_COLORS NULL NULL 0); This also works but the hdc one is even better."
362,A,Drawing an iso line of a 2D implicit scalar field I have an implicit scalar field defined in 2D for every point in 2D I can make it compute an exact scalar value but its a somewhat complex computation. I would like to draw an iso-line of that surface say the line of the '0' value. The function itself is continuous but the '0' iso-line can have multiple continuous instances and it is not guaranteed that all of them are connected. Calculating the value for each pixel is not an option because that would take too much time - in the order of a few seconds and this needs to be as real time as possible. What I'm currently using is a recursive division of space which can be thought of as a kind of quad-tree. I take an initial very coarse sampling of the space and if I find a square which contains a transition from positive to negative values I recursively divide it to 4 smaller squares and checks again stopping at the pixel level. The positive-negative transition is detected by sampling a sqaure in its 4 corners. This work fairly well except when it doesn't. The iso-lines which are drawn sometimes get cut because the transition detection fails for transitions which happen in a small area of an edge and that don't cross a corner of a square. Is there a better way to do iso-line drawing in this settings? Can you sample the field into an image and use the OpenCV contouring routines to extrract the filedlines  I've had a lot of success with the algorithms described here http://members.bellatlantic.net/~vze2vrva/thesis.html which discuss adaptive contouring (similar to that which you describe) and also some other issues with contour plotting in general. There is no general way to guarantee finding all the contours of a function without looking at every pixel. There could be a very small closed contour where a region only about the size of a pixel where the function is positive in a region where the function is generally negative. Unless you sample finely enough that you place a sample inside the positive region there is no general way of knowing that it is there. If your function is smooth enough you may be able to guess where such small closed contours lie because the modulus of the function gets small in a region surrounding them. The sampling could then be refined in these regions only.
363,A,"Best algorithm for matching colours. I have an array of around 200 colours in RGB format. I want to write a program that takes any RGB colour and tries to match a colour from the array that is most ""similar"". I need a good definition for ""similar"" which is as close as possible to human perception. I also want to show some information about matching accuracy. For example black-white: 100% and for a similar colour with a slightly different hue: -4%. Do I need to use neural networks? Is there an easier alternative? Is the question about a suggestion as to what may be a good similarity function or is it about an algorithm to quickly find the most similar color(s) in the the array relative to a give color ? Both. If first I need some definition of similarity before I can try crating algorithm. I think ""perceptually similar"" is what I was looking for. I was looking for the thing but having not found a lot answers around I decided to create this little library. https://github.com/sebastienjouhans/c-sharp-colour-utilities Please be very careful when posting answers that promote your own work. Make sure that you actually answer the question here and only use your blog/source as backup and reference. At the moment this is likely to be flagged as spam.  No you do not need neural networks here! Simply consider an HSL color value a vector and define a weighted modulus function for the vector like this: modulus = sqrt(a*H1*H1 + b*S1*S1 + c*L1*L1); where abc are weights you should decide based on your visual definition of what creates a bigger difference in perceived color - a 1% change in Hue or a 1% change in Saturation I would suggest you use a = b = 0.5 and c = 1 Finally find out the range your modulus would take and define similar colors to be those which have their moduli very close to each other (say 5%) That's a good simple alternative. The conversion from RGB to HSL is a lot simpler than the conversion from RGB to Lab. :) Ok so I will try it first. Crimson can you check the math on your modulus there? I don't think it's right. You want something more like `a * (H1 - H2)**2 + `... yeah? @hobbs - it would be better to calculate both moduli and then compare them rather than just compute the modulus of the difference vector But you don't want to multiply the hues etc. of the two different colors do you? Hi I was wondering what you mean by ""find out the range your modulus would take""? Do we just compare the two moduli of colors using abs(mod1 - mod2) or something or does ""find out the range"" mean something different?  Convert all of the colors to the CIE Lab color space and compute the distance in that space deltaE = sqrt(deltaL^2 + deltaA^2 + deltaB^2) Colors with the lowest deltaE are the most perceptually similar to each other. Thank You that is exactly what I need. Keep in mind you don't need to do the sqrt - sqrt is an increasing function therefore this step is superfluous. You're right if you're doing nothing more than sorting the square of the distance is as good as the distance itself. If you want to compare ""how different' then leave it in. CIE Lab is used in exactly this manner to do nearest-color calculations in all the major color management systems such as the ones from Apple Microsoft and Adobe. It's a very interesting topic. What's a good low value for deltaE from your experience? i.e. when deltaE < 14 colors are perceptually similar. @MariusAndreiana Using the usual scaling of Lab values a difference of about deltaE=1 is the smallest difference that people can see and the biggest difference representable on a monitor is maybe 150-200. It's up to you to decide where ""similar"" ends and ""not similar"" starts :)  I'd also point out the least squares method just as something slightly simpler. That is you take the difference of a number square it then sum all these squared differences."
364,A,"Where is the textmode video buffer if it isn't at 0xB8000? About 15 years ago I used to amuse myself and annoy my CS teacher by writing bad code that would directly modify the text on the monitor. This was/is easily done by accessing video memory at 0xB8000 on VGA-equipped PC-compatibles. Fast forward to today I decided to try out my old trick through a debug port that gives me access to read physical memory. To my gratification it still worked on the first platform I tried it on. Then dishearteningly I discovered that it doesn't work on many other systems. It seems that the systems it doesn't work on all have UMA (shared-memory) graphics. So a question for all of you BIOS writers low-level OS guys and video driver gurus - if I'm in regular 80x25 color textmode on a PC-compatible system with shared graphics memory in real mode and the contents of 0xb8000 - 0xB8FFF are all 0xFF instead of giving me what's on the screen where did the screen buffer go? Just to confirm you are in true real mode and not a ""DOS"" console running on Windows correct? Real honest-to-god real mode (it's a BIOS setup screen that I'm capturing). The CGA (Color Graphics Adapter) and MDA (Monochrome Display Adapter) cards used different regions. One (color) is at 0xB8000; but monochrome starts at 0xB0000. Remember you could have both displays active on the machine at once. See DOS Memory Map (although it has a typo in the offset for CGA - should be 8000h not 0800h). As far as VGA (Video Graphics Array) goes its memory starts lower at the 640K boundary at 0xA0000 and continues for 64K - but it can go beyond as the card could have up to 256K. Some sample code for programming VGA is here. But it isn't quite so simple as it has multiple video modes. You might try here for some help. Are you sure you have VGA and not EGA or XGA or Super-VGA? All of those have slightly different semantics... But still I'm pretty sure any of those should put text either at 0xB0000 or 0xB8000. What happens when you type either MODE MONO or MODE CO80 -- does that affect the display? Those would switch from one adapter text mode to the other under DOS. I've scanned everything from 0xB0000 to 0xBFFFF - it's all junk (0xFF). The memory at 0xC0000 is the VGA bios as expected; that lets me know that my method of accessing memory is working but doesn't help me grab the screen contents...  Ah if it's a BIOS screen then all bets are off. The fact that it's a BIOS on a motherboard with integrated graphics leans things even more in favor of video strangeness. In all likelyhood ""classic"" VGA functionality is for all intents emulated. With an external video card this would be invisible to the BIOS - it still sees a VGA video card. But a BIOS hardwired to the video card wouldn't need to setup the fake VGA layer and old school 20 bit address space memory mapping just to stick some text on the screen. Instead the BIOS may interface directly with the card (via some proprietary mode) until the operating system is actually going to be started up at which point it does memory mapping itself. This has a major advantage in that for a card that shares main memory you don't get a blank screen on startup if your RAM is incorrectly installed (instead the video cards propritary BIOS mode could use a buffer normally reserved for some other purpose allowing you to get into the BIOS and see 0MB of RAM installed) Yep - I was just informed by a coworker that Phoenix bios is using mode 12h. The video information is there at 0xA0000 but it's all pixels no ascii characters. Bummer. Right answer but a bummer ;-)"
365,A,Quaternion math for rotation? I'm drawing a flat disk using gluDisk() in my scene. gluDisk() draws the disk facing the positive Z axis but I want it to be facing some arbitrary normal I have. Clearly I need to use glRotate() to get the disk facing properly but what should be the rotation? I remember this can be calculated using Quaternions but I can't seem to remember the math. Quaternions describe a rotation about an axis. <wxyz> will rotate around the axis <xyz> some amount depending on the balance between the magnitude of w and the magnitude of the vector. <cos θ/2 x*sin θ/2 y*sin θ/2 z*sin θ/2> where |<x y z>| = 1 For example rotating it to face the positive Y-axis instead you need to rotate it 90° around the X-axis. The vector would be <0 1 0> and the quaternion would be <cos 90° 0 sin 90° 0> = <0 0 1 0>. To rotate the figure from facing the positive Z-axis to facing the vector <xyz> you need to find the rotation-vector and angle of rotation. To find the rotation axis you can take the cross-product of a current vector and where you want it to be. If it is facing the positive Z-axis the current vector would be <0 0 1>. If you want it to face <xyz> the rotation-axis would be <0 0 1> x <x y z> = <-y x 0> and the angle would be arctan(sqrt(x^2+y^2)z). The quaternion becomes <cos(θ/2) -y*sin(θ/2) x*sin(θ/2) 0> where θ = arctan(sqrt(x^2+y^2) z)  The solution should be pretty straightforward and shouldn't require quarternions. The axis of rotation to get from Normal1 to Normal2 must be orthogonal to both so just take their vector cross-product. The amount of rotation is easily derived from their dot-product. This value is |A|.|B|.cos(theta) but as the two normal vectors should be normalised it will give cos(theta) so just take the inverse cosine to get the rotation amount. The resulting vector and angle are the required parameters for glRotate() - there's no need to calculate the actual rotation matrix yourself. p.s. don't forget that glRotate() needs the angle in degrees but the normal C trig functions work in radians. Thank you. that worked perfectly.  Rotation around an arbitrary axis: Given angle r in radians and unit vector u = ai + bj + ck or [abc] define: q0 = cos(r/2) q1 = sin(r/2) a q2 = sin(r/2) b q3 = sin(r/2) c and construct from these values the rotation matrix:  ( q0^2+q1^2 - q2^2 - q3^2 | 2*(q1*q2 - q0*q3) | 2*(q1*q3 + q0*q2) ) Q =( 2*(q2*q1 + q0*q3) | (q0^2 - q1^2 + q2^2 - q3^2) | 2*(q2*q3 - q0*q1) ) ( 2*(q3*q1 - q0*q2) | 2*(q3*q2 + q0*q1) | q0^2 - q1^2 - q2^2 + q3^2 ) To find the rotation you need to do you can calculate the cross product between the current vector and the target vector. You will obtain the orthogonal vector (which will be your rotation vector to create the quaternion) and the length of this vector is the sin of the angle you have to compensate so that the start and target vector overlap.
366,A,OpenGL ES as a 2D Platform I've seen a lot of bandying about what's better Quartz or OpenGL ES for 2D gaming. Neverminding libraries like Cocos2D I'm curious if anyone can point to resources that teach using OpenGL ES as a 2D platform. I mean are we really stating that learning 3D programming is worth a slight speed increase...or can it be learned from a 2D perspective? Cribbed from a similar answer I provided here: You probably mean Core Animation when you say Quartz. Quartz handles static 2-D drawing within views or layers. On the iPhone all Quartz drawing for display is done to a Core Animation layer either directly or through a layer-backed view. Each time this drawing is performed the layer is sent to the GPU to be cached. This re-caching is an expensive operation so attempting to animate something by redrawing it each frame using Quartz results in terrible performance. However if you can split your graphics into sprites whose content doesn't change frequently you can achieve very good performance using Core Animation. Each one of those sprites would be hosted in a Core Animation CALayer or UIKit UIView and then animated about the screen. Because the layers are cached on the GPU basically as textures they can be moved around very smoothly. I've been able to move 50 translucent layers simultaneously at 60 FPS (100 at 30 FPS) on the original iPhone (not 3G S). You can even do some rudimentary 3-D layout and animation using Core Animation as I show in this sample application. However you are limited to working with flat rectangular structures (the layers). If you need to do true 3-D work or want to squeeze the last bit of performance out of the device you'll want to look at OpenGL ES. However OpenGL ES is nowhere near as easy to work with as Core Animation so my recommendation has been to try Core Animation first and switch to OpenGL ES only if you can't do what you want. I've used both in my applications and I greatly prefer working with Core Animation.  GL is likely to give you better performance with less CPU usage battery drain and so on. 2D drawing with GL is just like 3D drawing with GL you just don't change the Z coordinate. That being said it's easier to write 2D drawing code with Quartz so you have to decide the trade-off.
367,A,"Inner shadow in Core Graphics I want to do something similar to Photoshops inner shadow effect in Core Graphics. If I draw/fill a path with this effect I want get something similar to the following: How would I use your InnerShadowDrawing.m to draw an inner shadow on a UILabel's text? Here are the layers you need to create to make this image from back to front: The base color in this case a white background. The shadow. The shape casting the shadow. This is made by finding the bounding box of the inner shape expanding that box by more than the width of the shadow then cutting a hole in the box with the inner shape. Clipping these with the inner shape. Then finally drawing the surrounding colored shape in this case a rectangle with the inner shape cut out. Note: Depending upon the expected look the shape casting the shadow may or may not be the same shape filling the foreground color. A thin section between the inner shape and the outer shape would cast a reduced shadow. If that effect is not desired a larger outer shape would be required to get the consistent inner shadow. Also the explicit clipping of the shadow is required in case the shadow extends beyond the outer shape. To draw a shape with a hole in the middle like this example shape you'll want to draw a path with two subpaths. One subpath would be the outer box and the other would be the inner irregular shape. If you're using the default nonzero winding number rule you'll want to specify the points for the outer box in the opposite direction than the inner irregular shape. For example specifying the outer box's points in clockwise order would require specifying the inner shape's points in counter-clockwise order. Refer to the Quartz 2D Programmer's Guide's section on Paths for more details. Thank but my main problem is: how do you ""cut out"" things in Core Graphics in the required way? I edited my answer to add more information on drawing paths with holes."
368,A,generating a 3d graphics c++ I have found an interesting application on the net and i am using it for my end year study project. http://www.cl.cam.ac.uk/~sjeh3/wii/ the video in the link explains my goal. But i am having issue using it. the example of rendering the trajectory on a 3d axis is using Corba (omniorb) and i believe open inventor. but there isn't any idl file. and i don't know if it's possible to use it. My question is : Is it possible to render a 3d real time graphics using a lib in c++ making it easy and fast to implement? i tried using matlab engine or matlab simulation with tcpip communication but i am having issues with these technics so i am searching for another way. Does anybody have an idea ? sincerely Hugo Take a look at OpenGL.  You might also look at SDL (which uses OpenGL). Edit (re: comments) For the plotting aspect you could look at VTK and/or MayaVI (which puts a Python scripting wrapper around VTK). i took a look but there isn't any real lib for a scatter plot whitout having to do plenty of coding. but i am surely wrong !
369,A,How to generate events from graphics generated by Java2D I have made an Ellipse with the help of java.awt.geom.Ellipse2D Now I want that whenever user clicks on that ellipse an event is generated so that I can listen to that event and peform subsequent tasks based on the ellipse which generated that event. Here is a simple example of an object drawing program that demonstrates click drag and multiple selection. Also consider JGraph which is a much more advanced library for graph visualization. See also this [example](http://stackoverflow.com/a/11944233/230513).  I'm going to assume this is a question asking a way to listen to mouse clicks which are made on an ellipse drawn on some Swing component using Graphics2D.draw. The simple answer is there is no way to generate mouse events from graphics drawn on a surface. However here's an alternative approach: Store the Ellipse2D objects from which the ellipses were drawn from in a List. Register a MouseListener on the Swing component where the user is to click on. From the MouseEvents generated from the mouse clicks determine the location at which the mouse was clicked (using MouseEvent.getPoint) and check if the mouse click occurred in any of the Ellipse2Ds contained in the aforementioned List using the Ellipse2D.contains method.  I don't think this is possible without lots of handcoded stuff (letting the canvas or whatever listen to mouse events and calculating yourself if the ellipse was clicked on). If you want to do more like that consider scenegraph. With that the ellipse would be an object in its own right and you can register event listeners. Edit as response to comment: Scenegraph: https://scenegraph.dev.java.net/ google for more resources: scenegraph java And yes. Scenegraph ist part of the JavaFX stuff but works nicely with pure Java (no FX) could you please provide a link to scenegraph. I haven't heard it before. Can I draw ellipse from that on a swing GUI.
370,A,"How to draw subsection of text using .net graphics I'm doing custom drawing in datagridview cells and I have items that can vertically span across multiple cells. An item displays text and the issue at hand is how can I draw just the cell's part of the text? I have the item's rectangle and the cellBounds. Currently I am drawing all the text on each cell paint i.e. I'm drawing over cells other than the one I'm currently painting from. This requires me to clear out the previous text (so it doesn't get blurry and bolded)...so I'm actually drawing the string twice per cell paint. Not very efficient. //get the actual bounds of this entire item spanning across multiple cells RectangleF sRectF = GetItemRectF(startX + leftMargin + 2 widthForItem cellBounds calItem); //we clear it out first otherwise the text looks bolded if we keep drawing a black string over and over //todo should figure out how to only draw this cells section? cellBounds subsection of sRectF somehow graphics.DrawString(calItem.Description new Font(""Tahoma"" 8) new SolidBrush(itemBackColor) sRectf); graphics.DrawString(calItem.Description new Font(""Tahoma"" 8) new SolidBrush(Color.Black) sRectF); Could I draw the string on some temp graphics and then snatch out the cell bounds part and draw that on the actual graphics? Is there a better way? Thanks Answer Region tempRegion = graphics.Clip; graphics.Clip = new Region(cellBounds); graphics.DrawString(calItem.Description new Font(""Tahoma"" 8) new SolidBrush(Color.Black) sRectF); graphics.Clip = tempRegion; I don't think I quite understand the visual effect you intend to have. Is the text for the item supposed to overlap multiple cells or clipped to a single cell? If it's supposed to be clipped to the cell you can set your clipping area using Graphics.Clip to clip to a specified rectangle. If the problem is related to smearing due to not clearing the buffer you can use FillRectangle to clear a region cheaper than drawing text. Ahh the Clip is what I wanted...thanks."
371,A,Blackberry setting a clipping region/area In J2ME we have the clipRect() and setClip() to clip a region in the Graphics area. What is the equivalent apis available in BlackBerry and how do we use it? See the blackberry API. pushRegion() for the clipRect() don't know of exact setClip replacement. http://www.blackberry.com/developers/docs/3.6api/net/rim/device/api/ui/Graphics.html Actually I found that out in the Graphics class itself if you provide the width height and the offsets in the drawBitmap method it automatically clips and anything beyond that width and height will not be painted or rather gets cropped Thx for sharing!
372,A,"Fast rectangle to rectangle intersection What's a fast way to test if 2 rectangles are intersecting? A search on the internet came up with this one-liner (WOOT!) but I don't understand how to write it in Javascript it seems to be written in an ancient form of C++. struct { LONG left; LONG top; LONG right; LONG bottom; } RECT; bool IntersectRect(const RECT * r1 const RECT * r2) { return ! ( r2->left > r1->right || r2->right left || r2->top > r1->bottom || r2->bottom top ); } I think you've made a typo in your copy/paste Well this is where its from and it looks the same to me -- http://tekpool.wordpress.com/2006/10/11/rectangle-intersection-determine-if-two-given-rectangles-intersect-each-other-or-not/ The original article has a typo. `r2->right left` doesn't make sense. It's might be broken due to HTML escaping issues. I'm curious how you think the above code would be different in a ""modern"" form of C++. I'm sure the missing characters are `<` symbols due to html escaping. This is how the .NET Framework implements Rectangle.Intersect public bool IntersectsWith(Rectangle rect) { if (rect.X < this.X + this.Width && this.X < rect.X + rect.Width && rect.Y < this.Y + this.Height) return this.Y < rect.Y + rect.Height; else return false; } Or the static version: public static Rectangle Intersect(Rectangle a Rectangle b) { int x = Math.Max(a.X b.X); int num1 = Math.Min(a.X + a.Width b.X + b.Width); int y = Math.Max(a.Y b.Y); int num2 = Math.Min(a.Y + a.Height b.Y + b.Height); if (num1 >= x && num2 >= y) return new Rectangle(x y num1 - x num2 - y); else return Rectangle.Empty; }  This is how that code can be translated to JavaScript. Note that there is a typo in your code and in that of the article as the comments have suggested. Specifically r2->right left should be r2->right < r1->left and r2->bottom top should be r2->bottom < r1->top for the function to work. function intersectRect(r1 r2) { return !(r2.left > r1.right || r2.right < r1.left || r2.top > r1.bottom || r2.bottom < r1.top); } Test case: var rectA = { left: 10 top: 10 right: 30 bottom: 30 }; var rectB = { left: 20 top: 20 right: 50 bottom: 50 }; var rectC = { left: 70 top: 70 right: 90 bottom: 90 }; intersectRect(rectA rectB); // returns true intersectRect(rectA rectC); // returns false Just to add/confirm - this is testing three boxes that are 20px x 20px except rectB which is 30px x 30px if r1 and r2 are identical the intersectRect function would return false Fantastic Eloquent Implementation. Love it! +1 worked perfectly for my game.  function intersect(a b) { return (a.left <= b.right && b.left <= a.right && a.top <= b.bottom && b.top <= a.bottom) } This assumes that the top is normally less than bottom (i.e. that y coordinates increase downwards). Good and working solution but should be a bit slower since all conditions have to be evaluated. The other solution is done as soon as one of the conditions evaluates to true. This one is also done as soon as one of the conditions evaluates to false. I.e. in the exact same cases as the other one. +1 Correct don't know what I was thinking. This is better as it removes the negation action. +1 as much neater than the accepted answer. if using half-open ranges (i.e. rectangle includes top and left but does not include bottom and right as is common in many graphics systems) changing `<=` to `<` should work."
373,A,How to find a normal vector pointing directly from virtual world to screen in Java3D? I think it can be done by applying the transformation matrix of the scenegraph to z-normal (0 0 1) but it doesn't work. My code goes like this: Vector3f toScreenVector = new Vector3f(0 0 1); Transform3D t3d = new Transform3D(); tg.getTransform(t3d); //tg is Transform Group of all objects in a scene t3d.transform(toScreenVector); Then I tried something like this too: Point3d eyePos = new Point3d(); Point3d mousePos = new Point3d(); canvas.getCenterEyeInImagePlate(eyePos); canvas.getPixelLocationInImagePlate(new Point2d(Main.WIDTH/2 Main.HEIGHT/2) mousePos); //Main is the class for main window. Transform3D motion = new Transform3D(); canvas.getImagePlateToVworld(motion); motion.transform(eyePos); motion.transform(mousePos); Vector3d toScreenVector = new Vector3f(eyePos); toScreenVector.sub(mousePos); toScreenVector.normalize(); But still this doesn't work correctly. I think there must be an easy way to create such vector. Do you know what's wrong with my code or better way to do so? Yes you got my question right. Sorry that I was a little bit confused yesterday. Now I have corrected the code by following your suggestion and mixing two pieces of code in the question together: Vector3f toScreenVector = new Vector3f(0 0 1); Transform3D t3d = new Transform3D(); canvas.getImagePlateToVworld(t3d); t3d.transform(toScreenVector); tg.getTransform(t3d); //tg is Transform Group of all objects in a scene t3d.transform(toScreenVector); Thank you.  If I get this right you want a vector that is normal to the screen plane but in world coordinates? In that case you want to INVERT the transformation from World -> Screen and do Screen -> World of (00-1) or (001) depending on which axis the screen points down. Since the ModelView matrix is just a rotation matrix (ignoring the homogenous transformation part) you can simply pull this out by taking the transpose of the rotational part or simple reading in the bottom row - as this transposes onto the Z coordinate column under transposition.
374,A,"How to convert a 3D point into 2D perspective projection? I am currently working with using Bezier curves and surfaces to draw the famous Utah teapot. Using Bezier patches of 16 control points I have been able to draw the teapot and display it using a 'world to camera' function which gives the ability to rotate the resulting teapot and am currently using an orthographic projection. The result is that I have a 'flat' teapot which is expected as the purpose of an orthographic projection is to preserve parallel lines. However I would like to use a perspective projection to give the teapot depth. My question is how does one take the 3D xyz vertex returned from the 'world to camera' function and convert this into a 2D coordinate. I am wanting to use the projection plane at z=0 and allow the user to determine the focal length and image size using the arrow keys on the keyboard. I am programming this in java and have all of the input event handler set up and have also written a matrix class which handles basic matrix multiplication. I've been reading through wikipedia and other resources for a while but I can't quite get a handle on how one performs this transformation. http://stackoverflow.com/questions/701504/perspective-projection-help-a-noob/701978#701978 A very good interactive example of  how three D points are projected on two d space this http://www.mathdisk.com/pub/safi/worksheets/Perspective_Projection Did you get any further with this? I've got some questions about the Bezier part of this task [here](http://stackoverflow.com/questions/14558814/how-do-bezier-patches-work-in-the-utah-teapot) and [here](http://stackoverflow.com/questions/14807341/how-to-do-a-space-partitioning-of-the-utah-teapot). I'm not sure at what level you're asking this question. It sounds as if you've found the formulas online and are just trying to understand what it does. On that reading of your question I offer: Imagine a ray from the viewer (at point V) directly towards the center of the projection plane (call it C). Imagine a second ray from the viewer to a point in the image (P) which also intersects the projection plane at some point (Q) The viewer and the two points of intersection on the view plane form a triangle (VCQ); the sides are the two rays and the line between the points in the plane. The formulas are using this triangle to find the coordinates of Q which is where the projected pixel will go Actually quite the opposite - I have found several diagrams and explanations as to what a projection plane is but I haven't started coding anything because I don't have a formula to work with. I'm not even sure what the ""forumlas"" are that you're referring to. @Zachary -- The formulas contained in the other two answers the Wikipedia page etc. Basically X' = (X-C1)*C2/Z + C3 and likewise for Y.  You can project 3D point in 2D using: Commons Math: The Apache Commons Mathematics Library with just two classes. Example for Java Swing. import org.apache.commons.math3.geometry.euclidean.threed.Plane; import org.apache.commons.math3.geometry.euclidean.threed.Vector3D; Plane planeX = new Plane(new Vector3D(1 0 0)); Plane planeY = new Plane(new Vector3D(0 1 0)); // Must be orthogonal plane of planeX void drawPoint(Graphics2D g2 Vector3D v) { g2.drawLine(0 0 (int) (world.unit * planeX.getOffset(v)) (int) (world.unit * planeY.getOffset(v))); } protected void paintComponent(Graphics g) { super.paintComponent(g); drawPoint(g2 new Vector3D(2 1 0)); drawPoint(g2 new Vector3D(0 2 0)); drawPoint(g2 new Vector3D(0 0 2)); drawPoint(g2 new Vector3D(1 1 1)); } Now you only needs update the planeX and planeY to change the perspective-projection to get things like this:  I see this question is a bit old but I decided to give an answer anyway for those who find this question by searching. The standard way to represent 2D/3D transformations nowadays is by using homogeneous coordinates. [xyw] for 2D and [xyzw] for 3D. Since you have three axes in 3D as well as translation that information fits perfectly in a 4x4 transformation matrix. I will use column-major matrix notation in this explanation. All matrices are 4x4 unless noted otherwise. The stages from 3D points and to a rasterized point line or polygon looks like this: Transform your 3D points with the inverse camera matrix followed with whatever transformations they need. If you have surface normals transform them as well but with w set to zero as you don't want to translate normals. The matrix you transform normals with must be isotropic; scaling and shearing makes the normals malformed. Transform the point with a clip space matrix. This matrix scales x and y with the field-of-view and aspect ratio scales z by the near and far clipping planes and plugs the 'old' z into w. After the transformation you should divide x y and z by w. This is called the perspective divide. Now your vertices are in clip space and you want to perform clipping so you don't render any pixels outside the viewport bounds. Sutherland-Hodgeman clipping is the most widespread clipping algorithm in use. Transform x and y with respect to w and the half-width and half-height. Your x and y coordinates are now in viewport coordinates. w is discarded but 1/w and z is usually saved because 1/w is required to do perspective-correct interpolation across the polygon surface and z is stored in the z-buffer and used for depth testing. This stage is the actual projection because z isn't used as a component in the position any more. The algorithms: Calculation of field-of-view This calculates the field-of view. Whether tan takes radians or degrees is irrelevant but angle must match. Notice that the result reaches infinity as angle nears 180 degrees. This is a singularity as it is impossible to have a focal point that wide. If you want numerical stability keep angle less or equal to 179 degrees. fov = 1.0 / tan(angle/2.0) Also notice that 1.0 / tan(45) = 1. Someone else here suggested to just divide by z. The result here is clear. You would get a 90 degree FOV and an aspect ratio of 1:1. Using homogeneous coordinates like this has several other advantages as well; we can for example perform clipping against the near and far planes without treating it as a special case. Calculation of the clip matrix This is the layout of the clip matrix. aspectRatio is Width/Height. So the FOV for the x component is scaled based on FOV for y. Far and near are coefficients which are the distances for the near and far clipping planes. [fov * aspectRatio][ 0 ][ 0 ][ 0 ] [ 0 ][ fov ][ 0 ][ 0 ] [ 0 ][ 0 ][(far+near)/(far-near) ][ 1 ] [ 0 ][ 0 ][(2*near*far)/(near-far)][ 0 ] Screen Projection After clipping this is the final transformation to get our screen coordinates. new_x = (x * Width ) / (2.0 * w) + halfWidth; new_y = (y * Height) / (2.0 * w) + halfHeight; Trivial example implementation in C++ #include <vector> #include <cmath> #include <stdexcept> #include <algorithm> struct Vector { Vector() : x(0)y(0)z(0)w(1){} Vector(float a float b float c) : x(a)y(b)z(c)w(1){} /* Assume proper operator overloads here with vectors and scalars */ float Length() const { return std::sqrt(x*x + y*y + z*z); } Vector Unit() const { const float epsilon = 1e-6; float mag = Length(); if(mag < epsilon){ std::out_of_range e(""""); throw e; } return *this / mag; } }; inline float Dot(const Vector& v1 const Vector& v2) { return v1.x*v2.x + v1.y*v2.y + v1.z*v2.z; } class Matrix { public: Matrix() : data(16) { Identity(); } void Identity() { std::fill(data.begin() data.end() float(0)); data[0] = data[5] = data[10] = data[15] = 1.0f; } float& operator[](size_t index) { if(index >= 16){ std::out_of_range e(""""); throw e; } return data[index]; } Matrix operator*(const Matrix& m) const { Matrix dst; int col; for(int y=0; y<4; ++y){ col = y*4; for(int x=0; x<4; ++x){ for(int i=0; i<4; ++i){ dst[x+col] += m[i+col]*data[x+i*4]; } } } return dst; } Matrix& operator*=(const Matrix& m) { *this = (*this) * m; return *this; } /* The interesting stuff */ void SetupClipMatrix(float fov float aspectRatio float near float far) { Identity(); float f = 1.0f / std::tan(fov * 0.5f); data[0] = f*aspectRatio; data[5] = f; data[10] = (far+near) / (far-near); data[11] = 1.0f; /* this 'plugs' the old z into w */ data[14] = (2.0f*near*far) / (near-far); data[15] = 0.0f; } std::vector<float> data; }; inline Vector operator*(const Vector& v const Matrix& m) { Vector dst; dst.x = v.x*m[0] + v.y*m[4] + v.z*m[8 ] + v.w*m[12]; dst.y = v.x*m[1] + v.y*m[5] + v.z*m[9 ] + v.w*m[13]; dst.z = v.x*m[2] + v.y*m[6] + v.z*m[10] + v.w*m[14]; dst.w = v.x*m[3] + v.y*m[7] + v.z*m[11] + v.w*m[15]; return dst; } typedef std::vector<Vector> VecArr; VecArr ProjectAndClip(int width int height float near float far const VecArr& vertex) { float halfWidth = (float)width * 0.5f; float halfHeight = (float)height * 0.5f; float aspect = (float)width / (float)height; Vector v; Matrix clipMatrix; VecArr dst; clipMatrix.SetupClipMatrix(60.0f * (M_PI / 180.0f) aspect near far); /* Here after the perspective divide you perform Sutherland-Hodgeman clipping by checking if the x y and z components are inside the range of [-w w]. One checks each vector component seperately against each plane. Per-vertex data like colours normals and texture coordinates need to be linearly interpolated for clipped edges to reflect the change. If the edge (v0v1) is tested against the positive x plane and v1 is outside the interpolant becomes: (v1.x - w) / (v1.x - v0.x) I skip this stage all together to be brief. */ for(VecArr::iterator i=vertex.begin(); i!=vertex.end(); ++i){ v = (*i) * clipMatrix; v /= v.w; /* Don't get confused here. I assume the divide leaves v.w alone.*/ dst.push_back(v); } /* TODO: Clipping here */ for(VecArr::iterator i=dst.begin(); i!=dst.end(); ++i){ i->x = (i->x * (float)width) / (2.0f * i->w) + halfWidth; i->y = (i->y * (float)height) / (2.0f * i->w) + halfHeight; } return dst; } If you still ponder about this the OpenGL specification is a really nice reference for the maths involved. The DevMaster forums at http://www.devmaster.net/ have a lot of nice articles related to software rasterizers as well. @madas +1 Thanks for explanation. What does this mean? ""you perform Sutherland-Hodgeman clipping by checking if the x y and z components are inside the range of [-w w]."" Namely the -w and w. Where do those values come from?  To obtain the perspective-corrected co-ordinates just divide by the z co-ordinate: xc = x / z yc = y / z The above works assuming that the camera is at (0 0 0) and you are projecting onto the plane at z = 1 -- you need to translate the co-ords relative to the camera otherwise. There are some complications for curves insofar as projecting the points of a 3D Bezier curve will not in general give you the same points as drawing a 2D Bezier curve through the projected points.  I think this will probably answer your question. Here's what I wrote there: Here's a very general answer. Say the camera's at (Xc Yc Zc) and the point you want to project is P = (X Y Z). The distance from the camera to the 2D plane onto which you are projecting is F (so the equation of the plane is Z-Zc=F). The 2D coordinates of P projected onto the plane are (X' Y'). Then very simply: X' = ((X - Xc) * (F/Z)) + Xc Y' = ((Y - Yc) * (F/Z)) + Yc If your camera is the origin then this simplifies to: X' = X * (F/Z) Y' = Y * (F/Z) I actually did edit it - I had a mistake in there at first. Sorry about that. I read this - but it doesn't quite make sense. In your first two equations you have Xc -Xc and Yc-Yc...isn't that going to always equal zero regardless of what the other values are? @Zachary -- Huh? I see no ""Xc - Xc"" in his answer (and it's never been edited).  I know it's an old topic but your illustration is not correct the source code sets up the clip matrix correct. [fov * aspectRatio][ 0 ][ 0 ][ 0 ] [ 0 ][ fov ][ 0 ][ 0 ] [ 0 ][ 0 ][(far+near)/(far-near) ][(2*near*far)/(near-far)] [ 0 ][ 0 ][ 1 ][ 0 ] some addition to your things: This clip matrix works only if you are projecting on static 2D plane if you want to add camera movement and rotation: viewMatrix = clipMatrix * cameraTranslationMatrix4x4 * cameraRotationMatrix4x4; this lets you rotate the 2D plane and move it around..- No you are wrong. It depends on whether you are using column or row-major notation. You're thinking of OpenGL which uses column-major.  You might want to debug your system with spheres to determine whether or not you have a good field of view. If you have it too wide the spheres with deform at the edges of the screen into more oval forms pointed toward the center of the frame. The solution to this problem is to zoom in on the frame by multiplying the x and y coordinates for the 3 dimensional point by a scalar and then shrinking your object or world down by a similar factor. Then you get the nice even round sphere across the entire frame. I'm almost embarrassed that it took me all day to figure this one out and I was almost convinced that there was some spooky mysterious geometric phenomenon going on here that demanded a different approach. Yet the importance of calibrating the zoom-frame-of-view coefficient by rendering spheres cannot be overstated. If you do not know where the ""habitable zone"" of your universe is you will end up walking on the sun and scrapping the project. You want to be able to render a sphere anywhere in your frame of view an have it appear round. In my project the unit sphere is massive compared to the region that I'm describing. Also the obligatory wikipedia entry: Spherical Coordinate System"
375,A,"Getting drawable area of an AWT frame in Mac OS X? I have subclassed java.awt.Frame and have overridden the paint() method as I wish to draw the entire contents of the window manually. However on the graphics object (00) corresponds to the upper left hand corner of the window inside the title bar decoration not the first drawable pixel. Can I determine the co-ordinate of the first drawable pixel (ie the height of the decoration) in a cross-platform manner avoiding using a Mac OS X-specific fudge factor? Will I be forced to nest a Panel component in order to find the actual drawable area of the window? Here my code fails to centre the blue square inside the paintable area of the window: @Override public void paint (Graphics g) { g.setColor(Color.BLUE); g.setPaintMode(); g.fillRect(30 30 getWidth()-60 getHeight()-60); } You can find the frame insets by calling the getInsets method (defined in Container). Frame insets are discussed at the top of the Frame API docs.  So you want to paint the whole area and don't want a title bar at all? Assuming that you use JDk 1.4 (at least) then you can declare the frame to be ""undecorated"" (java.awt.Frame#setUndecorated(boolean)). This way no title bar is created and therefore the frames-paintable area is the same as the frames-consumed area."
376,A,Ratio of multipass textures compared to the color map I have 4 textures used in a multi-pass effect with the color map at the highest quality - which for this example is 512x512. Currently the Normal Specular & Parallax maps are the same dimensions. The additional maps are taking a significant portion of VRAM. This seems incredibly wasteful. My intention is to reduce the size of the multipass maps. Color - 512x512 Normal - 256x256 (50% of original to keep enough detail) Specular - 256x256 (50% of original once again to maintain detail) Parallax - 128x128 (25% of original as a blurred parallax could be beneficial - or so I'm told) My question is can I reduce the ratio's of the normal specular & parallax maps further? Do these ratios seem reasonable? What ratios should I be using? Thanks Chris You might be better to post this one up on gamedev.net tbh i'd want to maintain the normal map sizes and hammer down the colour map. All the detail in the final image comes from the normal map very little actually comes form the colour map. Beyond that its texture compression all the way. You are right the detail really is in the normal map. And the specular map would likely depend on the level of detail. Thanks Goz.
377,A,Can the ZedGraph charting library for .NET be recommended? I am working on a project for my company and I need to integrate some graphs of different types and average complexity to C# in the process of studying stock markets. I found this free library on the Internet ZedGraph. If you came across it do you recommend using it? And how well is it supported? ZedGraph does not appear to be supported by the original developers anymore. However you can find it as part of other projects where updates have been made. For example per this discussion on a ZedGraph project discussion list: So I highly optimized ZedGraph for all the curves and objects. Basically I optimized how it uses GDI and specifically made it only draw objects that will fit in the the chart. So it scrolls and zooms now extremely efficiently even if I have many millions of objects on the chart. Plus it users nearly zero CPU when it's running in real time as slower speeds for tracking financial charts. I fixed a few defects also. You can find a fork of the repo with the changes here. Link is broken do you know if we can still find this optimized version?  ZedGraph does not support 3D graphing such as Surface Chart: Implied Vol Surface  I hate to be a killjoy but I wouldn't recommend ZedGraph. I was working with it a couple years ago and noticed that the support was provided by a single enthusiast that seemed abused by everyone wanting his freeware with no compenstation or contributions. It's a curse faced by a lot of FOSS authors. It doesn't look like the software has been updated in over a year now and the help forum is full of queries with no responses. It looks like the author lost interest and walked. If you use ZedGraph do it because you want to maintain the underlying code and because you want to contribute back to the user/developer community. If you have no interest in contributing and you can't maintain it for yourself be prepared for things that don't work and simply never will. An enterprising company could sponsor development of the software and offer for-fee support but you need to decide for yourself if it's good enough to do that and if there is a real revenue model. I hope that helps someone. Why do something have to be under constant development to be useful? ZedGraph just works. Ya got the source code it is as good as your own. C#/.NET developers are some of the most common these days anyway.  I strongly recommend the Microsoft Chart Controls For .NET Framework 3.5 over ZedGraph. MS Chart may only be used in conjunction with a valid Windows license. For some products clients or people this is a complete no-go. nevertheless this is a good find and deserves +1 imo. Thanks Jamie Ide.  Do you mean Zedgraph rather than Zgraph? Zedgraph's homepage is here and is described in a CodeProject article here. If you are talking about Zedgraph I can recommend it I have relatively little experience in C# but quite a lot in data visualization. I found it straightforward to get Zedgraph up and running and producing good looking charts. Zedgraph is very good for 2D charting I'm still looking for an equivalent for 3D plotting.  Although development seems to have stalled WPF Dynamic Data Display looked promising.  I can recommend ZedGraph. I have been using it with great success for several years in MSQuant for most plots: mass spectrum display recalibration error plots LC peak plots quantitation profiles and others. Here are some screen-shots from MSQuant where ZedGraph has been used: Scatter plot with trendline X-Y plot with the actual data points shown line connection data points Sticks plot with overlayed annotation (TextBoxes in fact) Several plots in the same window types as in 2. and 3. (the two plots in the bottom half) Closer look at type 2. Collage type 2. and code in Visual Studio The source code that is behind the first plot can be found in Source code for MSQuant: frmRecalibrationVisualisation.vb MSQuant/msquant/src/GUI/forms/frmRecalibrationVisualisation.vb.. In contrast to many other charting libraries ZedGraph can also be used for scientific/math oriented plots/charts (for example scatter plots) and not only for business type plots/charts. Stock market applications may also need scatter plots. In ZedGraph there is built-in support for the user to zoom in (infinite) and zoom out pan (drag while holding down the Ctrl key) save the plot to a file or copy it to the clipboard. There is one thing I am missing in ZedGraph: the ability for the user to select items in the plot in order to perform some action on those selected items (for example computing some number accepting them as verified or marking them as outliers to the application program). Don't be put off by the state of ZedGraph's development. ZedGraph is mature is of very high quality and can be used as-is. There is supposed to be a new team behind its further development.
378,A,"What IDE should I use for Java Graphics? I have had 2 courses in Java programming. We only used a pico editor on a sun server. I am transferring schools but they asked me to cover several chapters on Java graphics before my Advanced Java classes this fall. What options do I have for free or cheep IDEs to use for Java graphics? I don't think it matters at all. Java source code editors should be able to deal with any Java source no matter what it does. Personally I'd go for jEdit (with whichever plugin it is that gives tabs for switching files) on Linux.  I don't know of any IDE features that are really going to stand out with it comes to doing graphics in Java. Is there something in particular you're looking to get out of an IDE when doing graphics? Without any graphics-specific features then you're back to choosing between the primary Java IDEs: Eclipse Netbeans IntelliJ IDEA (not free although there's a trial and an early-access program)  eclipse The best all-around IDE that is widely used in industry. It has many plugins that can suit almost any specific need. ...but does it have any *specific* advantages vs NetBeans/IDEA/pico when it comes to graphics programming? You simply assert that it's the ""best all-around IDE"". no it doesnt have anything special that i know of. however since they all solve his problem equally well why not start using the one that is used in more institutions? one question that i can be sure to get on an interview is: ""do you have experience with eclipse?"" I know people that would say *exactly* the same about Intellij. ditto too bad it's expensive  I had great success with Eclipse and the Visual Editor plugin but that was many years ago. I suspect it or its successor is still a decent choice.  NetBeans has evolved a lot over the last couple years. Check it out here."
379,A,OpenGL Video RAM Limits I have been trying to make a Cross-platform 2D Online Game and my maps are made of tiles. My tileset which I render the tiles from is quite huge. I wanted to know how can I disable hardware rendering or at least making it more capable. Hence I wanted to know what are the basic limits of the video ram as far as I know Direct3D has a texture size limits (by that I don't mean the power-of-two texture sizes). If you want to use a software renderer link against Mesa. You can get an estimate of the maximum texture size using these methods.
380,A,"Books/Tutorials to Learn SVG Does anyone here know SVG? If so how did you learn it? Any books/tutorial pointer will be beneficial. Also I am a programmer not a designer so I want to pick up some skills there too. You could have a look at the SVG tutorial at w3schools. They also have a SVG reference there. Another very good source is SelfSVG but it is in german.  In answer to your question: Yes I do know SVG - or at least a subset of it. How I learnt it: I had a specific task I wanted to solve and SVG seemed like the easiest course. It was a mapping system so I needed a small subset of the functions and then to output the image as jpg. For that I used Batik looking through their examples seeing how the images are constructed in XML is surprisingly helpful and asking questions on forums/mailing lists. Also making mistakes is helpful if frustrating at times.  http://en.wikibooks.org/wiki/SVG might help.  The best reference book I've seen on SVG is SVG Essentials by J. David Eisenberg. I used that book to learn SVG. I also used Firefox to view SVGs and Inkscape to create them.  You can practice SVG paths with this utility: Spark Path Utility  I learned it with the spec: Scalable Vector Graphics on W3C. See SVG 1.1 specification. A bit dry but that's the real thing. They also provide lot of links to resources. Of course I recommend to read the XML specification first... :-) A bit of JavaScript knowledge can help too.  I learned it developing SVG Tiny software mostly by reading the spec. SVG Tiny is basically a subset of full SVG and is focused on use in mobile phones and other ""devices"". Adding to the links from previous answers KevLinDev has a bunch of beginner-friendly tutorials. Please don't flame me into oblivion for recommending my (now former) employer's software but I honestly think you could learn a bit by playing around with Ikivo Animator and looking at the output SVG. Animator supports simple drawing and animation. It's not free but has a 90 day trial period.  I found SVGbasics to be a good starting point. I second Rich's choice for SVG Essentials. And for the ultimate details you want to have the SVG specification within reach. As specifications go it's a rather dull read but it contains a number of examples too.  One method that you might want to try is by direct experimentation. Inkscape is an open source SVG editor that lets you directly view and manipulate the XML tree while editing graphically. So in this way you can experiment with things and see how they affect the XML that gets generated."
381,A,What are the graphics editors that can be built upon? I want to start off with a capable vector graphics drawing/editing program and extend it to create a visual designer for a project I'm working on. Do you know of a graphics editor that can be built upon? Maybe open source? Users should already be able to freely draw and color graphics and any form of grouping / arranging elements is a plus. You could use Inkscape or Karbon14 for vector graphics. For inkscape you have access to the API and there is a proposal for a Python extention. If you don't need colours you could easely use the Dia Python Plugin. As a follow up I have written a very small extension for Inkscape which allows you to type a few lines of python script directly into inkscape and iterate it over a selection etc. http://www.smanohar.com/inkscape.php
382,A,"Graphical Bugs while programmatically creating UILabel UITextField etc I programmatically created a login view and a table view. Under certain circumstances which i cannot reproduce there occur some strange visual bugs: Text with some letters having a different font size than others A button where the border is not initially shown but only if you click on it. In the same case the border of the button can be shown in a UITextField instead of the place holder ( see screenshot ) A label showing up not in the right place but in the above mentioned textfield upside down ( i did not manage to screenshot this one yet ) Below i added two screenshots with most of the app blacked out only showing the bugs:    Here you can see that text gets mixed with different font sizes. This errors occur not regularly or reproduce-able. I added the code of the login screen below:  // Implement loadView to create a view hierarchy programmatically without // using a nib. - (void)loadView { UIView *myView = [[UIView alloc] initWithFrame:CGRectMake(0 0 320 480)]; UIFont* font = [UIFont systemFontOfSize:14]; float midLine = 100.0; float height = 30.0; float borderY = 70.0; float borderX = 10.0; float width = 320.0 - borderX * 2.0; float paddingX = 10.0; float paddingY = 5.0; CGRect usernameLabelRect = CGRectMake(borderX borderY midLine height); CGRect usernameFieldRect = CGRectMake(borderX + midLine + paddingX borderY width - midLine - paddingX - 40  height); CGRect passwordLabelRect = CGRectMake(borderX borderY + height + paddingY midLine height); CGRect passwordFieldRect = CGRectMake(borderX + midLine + paddingX borderY + height + paddingY width - midLine - paddingX - 40 height); CGRect loginButtonRect = CGRectMake(borderX + 40 borderY + height*2 + paddingY*2 width - 80 height); //CGRect warningRect = CGRectMake(borderX // borderY * height * 3.0 + paddingY *3.0 // width height); CGRect warningRect = CGRectMake(borderX 175 width 40); UILabel* usernameLabel = [[UILabel alloc] initWithFrame:usernameLabelRect]; usernameLabel.textAlignment = UITextAlignmentRight; usernameLabel.font = font; usernameLabel.text = @""Username:""; UILabel* passwordLabel = [[UILabel alloc] initWithFrame:passwordLabelRect]; passwordLabel.textAlignment = UITextAlignmentRight; passwordLabel.font = font; passwordLabel.text = @""Password:""; usernameField = [[UITextField alloc] initWithFrame:usernameFieldRect]; [usernameField setBorderStyle:UITextBorderStyleRoundedRect]; //[usernameField setPlaceholder:@""<Username>""]; usernameField.font = font; usernameField.autocapitalizationType = UITextAutocapitalizationTypeNone; usernameField.autocorrectionType = UITextAutocorrectionTypeNo; passwordField = [[UITextField alloc] initWithFrame:passwordFieldRect]; passwordField.font = font; [passwordField setBorderStyle:UITextBorderStyleRoundedRect]; //[passwordField setPlaceholder:@""<Password>""]; passwordField.autocapitalizationType = UITextAutocapitalizationTypeNone; passwordField.autocorrectionType = UITextAutocorrectionTypeNo; passwordField.secureTextEntry = YES; loginButton = [UIButton buttonWithType:UIButtonTypeRoundedRect]; loginButton.frame = loginButtonRect; [loginButton setTitle: @""Einloggen"" forState:UIControlStateNormal]; [loginButton setTitleColor:[UIColor blackColor] forState:UIControlEventTouchDown]; loginButton.contentVerticalAlignment = UIControlContentVerticalAlignmentCenter; loginButton.contentHorizontalAlignment = UIControlContentHorizontalAlignmentCenter; [loginButton addTarget:self action:@selector(OnLoginClicked:) forControlEvents:UIControlEventTouchUpInside]; [loginButton setBackgroundColor:[UIColor clearColor]]; warningLabel = [[UILabel alloc] initWithFrame:warningRect]; warningLabel.font = font; warningLabel.numberOfLines = 3; warningLabel.text = @""Wenn Sie die Applikation jetzt schließen so werden Ihre Logindaten verworfen.""; [myView addSubview:usernameLabel]; [myView addSubview:passwordLabel]; [myView addSubview:usernameField]; [myView addSubview:passwordField]; [myView addSubview:loginButton]; [myView addSubview:warningLabel]; /* [usernameLabel release]; [passwordLabel release]; [usernameField release]; [passwordField release]; [loginButton release]; [warningLabel release]; */ self.view = myView; [myView release]; } Any help is greatly appreciated =) Thank you! here is another screenshot: http://uppix.net/b/f/2/a5993440c07753b851701068330be.png As you can see the button ""Einloggen"" has no button shown in this case. Usually it has. You can also see the placeholder of the username-Label to have the ""border"" shown instead of its actual placeholder The login screen you posted is a prime example of why to use the Interface Builder. You're basicly just placing some controls and setting a lot of properties which could have been handled by Interface Builder. Making a seperate Xib for the login view and loading it when needed is the way to go. Setting up your view in code yourself is error prone & it's a lot more visual. Making it easier to spot issues up front due to overlapping controls & the like. Why do you need to set this up in code? The problem was that i called UIKit messages from Not-The-Main-Thread which causes all kinds of graphical problems. Solved it by using performSelectorOnMainThread: in several places in my code =)  I can't work all of your screen positioning in my head but it looks to me like many of your fields overlap : is this intentional ? Maybe that is the cause ? I normally create views in code but this might be one case for IB! You haven't shown any code for the table view however I've seem similar issues before where you add subviews to a cell then next time the cell is deQueued with DequeueReusableCell you add subviews again without checking if they are there or clearing them. This results in all sorts of overlaps just like the first screenshot."
383,A,"Raster graphics in xterm? No not ASCII graphics see the screenshot here: http://en.wikipedia.org/wiki/W3m How is that even possible? I checked the source and it only prints character sequences. However I am unable to find any reference to graphic drawing or image embedding escape sequences in xterm documentation or elsewhere. w3m also seems to be the only software doing this. There are vector graphics in Tektronix emulation but this is done in VT mode. Maybe I am looking for the wrong thing? Any idea? btw w3m also works on TTY There is MLTerm which supports Sixel format (not ReGis commands). It's available for both Linux and Windows. Otherwise according to man xterm on Ubuntu 12.04 xterm supports Tektronix graphics which (the man page does tell) use ReGIS commands or Sixel bitmap format depending on compilation options (I believe it's Sixel for the XTerm in the Ubuntu package repository). One thing to know is that ReGIS is an instruction set while Sixel is a bitmap format. As a side note there exists a Python package to use Sixel aware terminal emulators: PySixel. I'm interested in the topic too and may update this answer in the future with other relevant terminal emulators entries.  It's a cheat. Note that this feature works only in ""supported terminals"" -- and by that it meant xterm and rendering directly on the xterm window via xv. Or not! Just checked the sources the file that interests us is here. It's still a hack -- via X11 and GTK! That page is awful it is 1995 again! :) While w3m has an option to display through xv I don't think the two are related. I don't even have an xv package. @jbcreix : yeah had the same '95 feeling ;> -- From the w3m page - Q : How do I change the default image viewer? A : By default w3m uses xv to view images. If you want to change it into let's say 'display' add the following line to ~/.mailcap or /etc/mailcap. Check your viewer! @jbcreix : ""use the source Luke"" -- found it ;> Oh it was writing its own ""w3m image protocol"". That's what you get reading a file with no comments. Interesting but in the end it's just a hack. :(  Higher-end models of Dec VT terminals support ReGis and Sixel graphics commands. If the xterm emulation is good enough maybe that's how it was done? xterm doesn't seem to support these modes. At least all references I found were to them not working."
384,A,"Are there any sources for getting affordable graphics and sounds for software products and games? When making software specially games those resources like graphics and sounds are something ""freaky"" that's out of range of the developers brain and feasibility. I mean...sound effects like cool beeps: Who in the world can make them? Almost nobody of us I guess ;) So: Is there any good legal ressource for this kind of content which allow to use them in Apps? How do all those developers make those cool and nice apps with nice music nice sound and nice graphic without getting sued right away? Where do they get their high-quality contents? http://stackoverflow.com/questions/742569/best-place-for-game-audio-graphics-etc"
385,A,"HCI: UI beyond the WIMP Paradigm With the popularity of the Apple iPhone the potential of the Microsoft Surface and the sheer fluidity and innovation of the interfaces pioneered by Jeff Han of Perceptive Pixel ... What are good examples of Graphical User Interfaces which have evolved beyond the Windows Icons ( Mouse / Menu ) and Pointer paradigm ? NUI Group people work primarily on multi-touch interfaces and you can see some nice examples of modern more human-friendly designs (not counting the endless photo-organizing-app demos ;) ).  How about mouse gestures? A somewhat unknown relatively new and highly underestimated UI feature. They tend to have a somewhat steeper learning curve then icons because of the invisibility (if nobody tells you they exist they stay invisible) but can be a real time saver for the more experienced user (I get real aggrevated when I have to browse without mouse gestures). It's kind of like the hotkey for the mouse.  Sticking to GUIs puts limits on the physical properties of the hardware. Users have to be able to read a screen and respond in some way. The iPhone for example: It's interface is the whole top surface so physical size and the IxD are opposing factors. Around Christmas I wrote a paper exploring the potential for a wearable BCI-controlled device. Now I'm not suggesting we're ready to start building such devices but the lessons learnt are valid. I found that most users liked the idea of using language as the primary interaction medium. Crucially though all expressed concerns about ambiguity and confirmation. The WIMP paradigm is one that relies on very precise definite actions - usually button pressing. Additionally as Nielsen reminds us good feedback is essential. WIMP systems are usually pretty good at (or at least have the potential to) immediately announcing the receipt and outcome of a users actions. To escape these paired requirements it seems we really need to write software that users can trust. This might mean being context aware or it might mean having some sort of structured query language based on a subset of English or it might mean something entirely different. What it certainly means though is that we'd be free of the desktop and finally be able to deploy a seamlessly integrated computing experience.  People are used to WIMP the other main issue is that most of the other ""Cool"" interfaces require specialized hardware. Although I see your point I must add that once the mouse too was specialized hardware...  Technically the interface you are looking for may be called Post-WIMP user interfaces according to a paper of the same name by Andries van Dam. The reasons why we need other paradigms is that WIMP is not good enough especially for some specific applications such as 3D model manipulation. To those who think that UI research builds only cool-looking but non-practical demos the first mouse was bulky and it took decades to be prevalent. Also Douglas Engelbart the inventor thought people would use both mouse and (a short form of) keyboard at the same time. This shows that even a pioneer of the field had a wrong vision about the future. Since we are still in WIMP era there are diverse comments on how the future will be (and most of them must be wrong.) Please search for these keywords in Google for more details. Programming by example/demonstration In short in this paradigm users show what they want to do and computer will learn new behaviors. 3D User Interfaces I guess everybody knows and has seen many examples of this interface before. Despite a lot of hot debates on its usefulness a part of 3D interface ongoing research has been implemented into many leading operating systems. The state of the art could be BumpTop. See also: Zooming User Interfaces Pen-based/Sketch-based/Gesture-based Computing Though this interface may use the same hardware setup like WIMP but instead of point-and-click users command through strokes which are information-richer. Direct-touch User Interface This is ike Microsoft's Surface or Apple's iPhone but it doesn't have to be on tabletop. The interactive surface can be vertical say wall or not flat. Tangible User Interface This has already been mentioned in another answer. This can work well with touch surface a set of computer vision system or augmented reality. Voice User Interface Mobile computing Wearable Computers Ubiquitous/Pervasive Computing Human-Robot Interaction etc. Further information: Noncommand User Interface by Jakob Nielsen (1993) is another seminal paper on the topic.  I would recommend the following paper: Jacob R. J. Girouard A. Hirshfield L. M. Horn M. S. Shaer O. Solovey E. T. and Zigelbaum J. 2008. Reality-based interaction: a framework for post-WIMP interfaces. In Proceeding of the Twenty-Sixth Annual SIGCHI Conference on Human Factors in Computing Systems (Florence Italy April 05 - 10 2008). CHI '08. ACM New York NY 201-210. see DOI  If you want some theoretical concepts on GUIs consider looking at vis by Tuomo Valkonen. Tuomo has been extremely critical of WIMP concept for a long he has developed ion window manager) which is one of many tiling window managers around. Tiling WMs are actually a performance win for the user when used right. Vis is the idea of an UI which actually adapts to the needs of the particular user or his environment including vision impairment tactile preferences (mouse or keyboard) preferred language (to better suit right-to-left languages) preferred visual presentation (button order mac-style or windows-style) better use of available space corporate identity etc. The UI definition is presentation-free the only things allowed are input/output parameters and their relationships. The layout algorithms and ergonomical constraints of the GUI itself are defined exactly once at system level and in user's preferences. Essentially this allows for any kind of GUI as long as the data to be shown is clearly defined. A GUI for a mobile device is equally possible as is a text terminal UI and voice interface.  vim! It's definitely outside the realm of WIMP but whether it's beyond it or way behind it is up to judgment!  I'm not in journalism; I write software for a living. All good software has HCI at it's heart. Conceptualising how HCI might change could keep you in a job.  Are you only interested in GUIs? A lot of research has been done and continues to be done on tangible interfaces for example which fall outside of that category (although they can include computer graphics). The User Interface Wikipedia page might be a good place to start. You might also want to explore the ACM CHI Conference. I used to know some of the people who worked on zooming interfaces; the Human Computer Interaction Lab an the University of Maryland also has a bunch of links which you may find interesting. Lastly I will point out that a lot of innovative user interface ideas work better in demos than they do in real use. I bring that up because your example as a couple of commenters have pointed out might if applied inappropriately be tiring to use for any extended period of time. Note that light pens were for the most part replaced by mice. Good design sometimes goes against naive intuition (mine anyway). There is a nice rant on this topic with regard to 3d graphics on useit.com."
386,A,"How to work around a very large 2d array in C++ I need to create a 2D int array of size 800x800. But doing so creates a stack overflow (ha ha). I'm new to C++ so should I do something like a vector of vectors? And just encapsulate the 2d array into a class? Specifically this array is my zbuffer in a graphics program. I need to store a z value for every pixel on the screen (hence the large size of 800x800). Thanks! You could do a vector of vectors but that would have some overhead. For a z-buffer the more typical method would be to create an array of size 800*800=640000. const int width = 800; const int height = 800; unsigned int* z_buffer = new unsigned int[width*height]; Then access the pixels as follows: unsigned int z = z_buffer[y*width+x];  There's the C like way of doing: const int xwidth = 800; const int ywidth = 800; int* array = (int*) new int[xwidth * ywidth]; // Check array is not NULL here and handle the allocation error if it is // Then do stuff with the array such as zero initialize it for(int x = 0; x < xwidth; ++x) { for(int y = 0; y < ywidth; ++y) { array[y * xwidth + x] = 0; } } // Just use array[y * xwidth + x] when you want to access your class. // When you're done with it free the memory you allocated with delete[] array; You could encapsulate the y * xwidth + x inside a class with an easy get and set method (possibly with overloading the [] operator if you want to start getting into more advanced C++). I'd recommend getting to this slowly though if you're just starting with C++ and not start creating re-usable fully templated classes for n-dimension arrays which will just confuse you when you're starting off. As soon as you get into graphics work you might find that the overhead of having extra class calls might slow down your code. However don't worry about this until your application isn't fast enough and you can profile it to show where the time is lost rather than making it more difficult to use at the start with possible unnecessary complexity. I found that the C++ lite FAQ was great for information such as this. In particular your question is answered by: http://www.parashift.com/c++-faq-lite/freestore-mgmt.html#faq-16.16 if you cast the newed array to a ""pointer to array of 800 ints"" (int[800] *buff; // ??) you can avoid the [x*w+y] kludge @BCS: That does not work there is no array of 800 pointers. There is a bug in this code it will fail when width and height are not equal. array[x*xwidth + y] should be array[y*xwidth + x].  Well building on what Niall Ryan started if performance is an issue you can take this one step further by optimizing the math and encapsulating this into a class. So we'll start with a bit of math. Recall that 800 can be written in powers of 2 as: 800 = 512 + 256 + 32 = 2^5 + 2^8 + 2^9 So we can write our addressing function as: int index = y << 9 + y << 8 + y << 5 + x; So if we encapsulate everything into a nice class we get: class ZBuffer { public: const int width = 800; const int height = 800; ZBuffer() { for(unsigned int i = 0 *pBuff = zbuff; i < width * height; i++ pBuff++) *pBuff = 0; } inline unsigned int getZAt(unsigned int x unsigned int y) { return *(zbuff + y << 9 + y << 8 + y << 5 + x); } inline unsigned int setZAt(unsigned int x unsigned int y unsigned int z) { *(zbuff + y << 9 + y << 8 + y << 5 + x) = z; } private: unsigned int zbuff[width * height]; }; Can you say ""premature optimisation""? Why not leave those kind of fancy tricks up to the compiler? Are 2 extra adds and 3 shifts really faster than just doing a multiply? Wouldn't the compiler notice a multiply by a const and do this itself if it really were quicker? Also if you change the const you have to remember to rewrite the shift/add code. I think that creating a buffer row pointers lookup table would be faster anyway. You can create it in the class constructor based on buffer size.  Every post so far leaves the memory management for the programmer. This can and should be avoided. ReaperUnreal is darn close to what I'd do except I'd use a vector rather than an array and also make the dimensions template parameters and change the access functions -- and oh just IMNSHO clean things up a bit: template <class T size_t W size_t H> class Array2D { public: const int width = W; const int height = H; typedef typename T type; Array2D() : buffer(width*height) { } inline type& at(unsigned int x unsigned int y) { return buffer[y*width + x]; } inline const type& at(unsigned int x unsigned int y) const { return buffer[y*width + x]; } private: std::vector<T> buffer; }; Now you can allocate this 2-D array on the stack just fine: void foo() { Array2D<int 800 800> zbuffer; // Do something with zbuffer... } I hope this helps! EDIT: Removed array specification from Array2D::buffer. Thanks Andreas for catching that! I completely disagree about memory management _for certain environments_. Depending on your allocation patterns the default allocator can fragment memory terribly leading to performancee that's hard to diagnose. I also don't understand the need to turn an array into a class. Obfuscation IMHO. For certain environments... OK. But it's easy enough to change Array2D::buffer to use a custom allocator -- well if you know the default allocator is terrible and that there are better ones then you'd likely also know how to add the custom allocate. As far as using a class... well in order to avoid using the stack and avoid using manual memory management you have to use a class. That's all there is to it. If you are open to manual memory management use 'new' to allocate a 2-D array. How does this work? I get ""only static const integral data members can be initialized within a class"" error when I try to compile.  This article talks about the subject.  Kevin's example is good however: std::vector<T> buffer[width * height]; Should be std::vector<T> buffer; Expanding it a bit you could of course add operator-overloads instead of the at()-functions: const T &operator()(int x int y) const { return buffer[y * width + x]; } and T &operator()(int x int y) { return buffer[y * width + x]; } Example: int main() { Array2D<int 800 800> a; a(10 10) = 50; std::cout << ""A(10 10)="" << a(10 10) << std::endl; return 0; }  You need about 2.5 megs so just using the heap should be fine. You don't need a vector unless you need to resize it. See C++ FAQ Lite for an example of using a ""2D"" heap array. int *array = new int[800*800]; (Don't forget to delete[] it when you're done.) You don't have to 'new' the memory. Presumably the zbuffer is going to need to live across functions. Declare it global (or if you have abstraced to some class in the class I suppose). You can do something like 'int zBuffer[WIDTH*HEIGHT];' assuming that the width and height don't change. Granted but I'm not going to suggest using a global because that's not a good solution in general -- it doesn't give you much flexibility in terms of scope and lifetime. It may work well for this questioner so go ahead and leave your own answer.  You can allocate array on static storage (in file's scope or apply 'static' modifier in function scope) if you need only one instance. int array[800][800]; void fn() { static int array[800][800]; } This way it will not go to the stack and you not have to deal with dynamic memory.  I might create a single dimension array of 800*800. It is probably more efficient to use a single allocation like this rather than allocating 800 separate vectors. int *ary=new int[800*800]; Then probably encapsulate that in a class that acted like a 2D array. class _2DArray { public: int *operator[](const size_t &idx) { return &ary[idx*800]; } const int *operator[](const size_t &idx) const { return &ary[idx*800]; } }; The abstraction shown here has a lot of holes e.g what happens if you access out past the end of a ""row""? The book ""Effective C++"" has a pretty good discussion of writing good multi dimensional arrays in C++. operator[] returning a int pointer? Maybe this is what you meant: int* operator[](size_t y) { return &ary[y*width]; } Then you could write: my2DArray[y][x] Although better practice would be to use operator(): int& operator[](int x int y) { return ary[y*width+x]; } Good point  however I don't really agree that overloading the function call operator is necessarily better practice. If you are wrapping an array you should make it look like an array. The C++ FAQ has some justifications for using operator(). http://www.parashift.com/c++-faq-lite/operator-overloading.html#faq-13.11 the single dim array will save little or no space over the multi dim version Is that FAQ meant to be a set of reasons for why you shouldn't use array syntax or why the C++ standard sucks? I found it unconvincing either way. The single dim array may or may not save space the point is that it should be a lot faster to make only one large allocation.  Or you could try something like: boost::shared_array<int> zbuffer(new int[width*height]); You should still be able to do this too: ++zbuffer[0]; No more worries about managing the memory no custom classes to take care of and it's easy to throw around.  One thing you can do is change the stack size (if you really want the array on the stack) with VC the flag to do this is /F. But the solution you probably want is to put the memory in the heap rather than on the stack for that you should use a vector of vectors. The following line declares a vector of 800 elements each element is a vector of 800 ints and saves you from managing the memory manually. std::vector<std::vector<int> > arr(800 std::vector<int>(800)); Note the space between the two closing angle brackets (> >) which is required in order disambiguate it from the shift right operator (which will no longer be needed in C++0x)."
387,A,C# Threads and this.Invalidate() I'm developing a Windows Mobile app (Compact Framework 2.0 SP1) and this code it's generating me an error:   public Image Imagen { get { return imagen; } set { imagen = value; this.Invalidate(); } }  The code is called from a new thread. I've tried to solve using **InvokeRequired:   public Image Imagen { get { return imagen; } set { imagen = value; if (this.InvokeRequired) this.Invoke(this.Invalidate); else this.Invalidate(); } }  But the line this.Invoke(this.Invalidate); doesn't compile. How can I solve the problem? The first error is that you can interact with controls created on another thread. Thank you! My solution for Compact Framework 2.0 SP1 is this:  ... delegate void InvocadorMetodos(); ... public Image Imagen { get { return imagen; } set { imagen = value; if (this.InvokeRequired) { InvocadorMetodos invalida = Invalidar; this.Invoke(invalida); } else this.Invalidar(); } }  Thank you!  Try this. Long winded version of why this is necessary. Instead of taking a specific Delegate type as a parameter the Invoke method takes the type System.Delegate. This type does not providing typing for a strongly typed signature. It's the base delegate class and instead provides a common mechanism for invoking all delegates. Unfortunately when passing a method name as a Delegate source in C# it must be passed to a specific delegate type. Otherwise C# doesn't know what type of delegate to create under the hood and unlike VB it won't generate anonymous delegate types. This is why you need a specific delegate type like MethodInvoker in order to call the function. EDIT Manually defined MethodInvoker since it doesn't exist in the Compact Framework public delegate void MethodInvoker(); public Image Imagen { get { get return imagen; } set { imagen = value; if (this.InvokeRequired) this.Invoke(new MethodInvoker(this.Invalidate)); else this.Invalidate(); } } This Winform is for windows mobile so I use Compact Framework that it hasn't got methodinvoker. @VansFannel answer updated to deal with that @JaredPar: Thank you!  The function Invoke expects an delegate so you can use the built in Action delegate: public Image Imagen { get { return imagen; } set { imagen = value; if (this.InvokeRequired) this.Invoke(new Action(this.Invalidate)); else this.Invalidate(); } } It doesn't work. Action needs a T generic Type. @VansFannel System.Action (no-generics) was added in .Net 3.5 @Stormenet: I use Compact Framework 2.0 SP1 Sorry missed that :)  Invalidate doesn't need an invoke. The invalidate only includes a paint message to be processed by the main thread with the rest of the pending messages. But the paint is not done when you call to invalidate and the control is not changed by this thread so you don't need to use an invoke for it. If you need to ensure that the control is refreshed maybe the invalidate is not enough and you need to call to the update too.
388,A,Radial plotting algorithm I have to write an algorithm in AS3.0 that plots the location of points radially. I'd like to input a radius and an angle at which the point should be placed. Obviously I remember from geometry coursework that 2(pi)r will give me an x on the point at the radius distance. The problem is I need to produce the x y and the calculation is a bit more complicated. A small push (or answer) would be wonderful. Thanks. Conversion from polar coordinates (rtheta) to cartesian coordinates (xy): x = xc + r cos(theta) y = yc + r sin(theta) where r = radius and theta = angle in radians and (xcyc) is the center of the circle x = x0 + r*cos(theta); y = y0 + r*sin(theta). (x0 y0) is the center of the circle. :) Thanks :) - will correct!
389,A,Resizing gif images I have a gif image 720 * 40 pixels used as a footer on a website. I need to extend the height of the gif by 10 pixels. I was unable to resize to this using Office Picture Manager. What is the best way to achieve this? This belongs on SuperUser.com as it is not programming related. You could also try an online solution like this  Use an image editor such a photoshop or gimp or paint.net. Create a new image sized 730 * 40 and paste your original to it.
390,A,Is current graphic only available in view's drawRect:? I tried invoking UIGraphicsGetCurrentContext() in other places other than in drawRect. It give me a NULL. Is it true that I can can only get current context in UIView's drawRect: only? Yes outside of drawRect the default context is nil. Before drawRect is called a view will push its context onto the stack and pop it after drawRect ends.
391,A,"Learning about low-level graphics programming I'm interesting in learning about the different layers of abstraction available for making graphical applications. I see a lot of terms thrown around: At the highest level of abstraction I hear about things like C# .NET pyglet and pygame. Further down I hear about DirectX and OpenGL. Then there's DirectDraw SDL the Win32 API and still other multi-platform libraries like WxWidgets. How can I get a good sense of where one of these layers ends and where the next one begins? What is the ""lowest possible level"" way of creating a window in Windows in C? What about C++? (A code sample would be divine.) What about in X11? Are the Windows implementations of OpenGL and DirectX built on top of the Win32 API? Where can I begin to learn about these things? There's another question on SO where Programming Windows is suggested. What about for Linux? Is there an equivalent such book? I'm aware that this is very low-level and that there are many friendlier tools available but I would like to at least learn the basics of what's going on beneath the surface. As much as I'd like to begin slinging windows and vectors right off the bat starting with something like pygame is too high-level for me; I really need to make the full conceptual circuit of how you draw stuff on a computer. I will certainly appreciate suggestions for books and resources but I think it would be stupendously cool if the answers to this question filled up with lots of different ways to get to ""Hello world"" with different approaches to graphics programming. C? C++? Using OpenGL? Using DirectX? On Windows XP? On Ubuntu? Maybe I ask for too much. ""What about for Linux? Is there an equivalent such book?"" - There is one now - http://www.amazon.com/The-Linux-Programming-Interface-Handbook/dp/1593272200 - very rarely does a book have a 100% 5-star rating and 39 is a reasonably good sample size. Michael Abrash's Graphics Programming 'Black Book' is a great place to start. Plus you can download it for free! this is a broken link Thanks I've updated the link. Available at http://www.drdobbs.com/high-performance-computing/184404919 (for now :) ). Thank you for the recommendation I have a feeling this will be a huge help.  Right off the bat I'd say ""you're asking too much."" From what little experience I've had I would recommend reading some tutorials or getting a book on either directX or OpenGL to start out. To go any lower than that would be pretty complex. Most of the books I've seen in OGL or DX have pretty good introductions that explain what the functions/classes do. Once you get the hang of one of these you could always dig in to the libraries to see what exactly they're doing to go lower. Or if you really absolutely MUST learn the LOWEST level... read the book in the above post.  The lowest level would be the graphics card's video RAM. When the computer first starts the graphics card is typically set to the 80x25 character legacy mode. You can write text with a BIOS provided interrupt at this point. You can also change the foreground and background color from a palette of 16 distinctive colors. You can use access ports/registers to change the display mode. At this point you could say load a different font into the display memory and still use the 80x25 mode (OS installations usually do this) or you can go ahead and enable VGA/SVGA. It's quite complicated that's what drivers are for. Once the card's in the 'higher' mode you'd change what's on screen by accessing the memory mapped to the video card. It's stored horizontally pixel by pixel with some 'dirty regions' of pixels that aren't mapped to screen at the end of each line which you have to compensate for. But yeah you could copy the pixels of an image in memory directly to the screen. For things like DirectX OpenGL. rather than write directly to the screen commands are sent to the graphics card and it updates its screen automatically. Commands like ""Hey you draw this image I've loaded into the VRAM here here and here"" or ""Draw these triangles with this transformation matrix..."" take a fraction of the time compared to pixel by pixel . The CPU will thank you. DirectX/OpenGL is a programmer friendly library for sending those commands to the card with all the supporting functions to help you get it done smoothly. A more direct approach would only be unproductive. SDL is an abstraction layer so without bothering to read up on it I'd guess it would have different ways of working on each system. On one it might use semi-direct screen writing another Direct3D etc. Whatever's fastest as long as the code stays cross-platform..able. The GDI/GDI+ and XWindow system. They're designed specifically to draw windows. Originally they drew using the pixel-by-pixel method (which was good enough because they'd only have to redraw when a button was pressed or a window moved etc.) but now they use Direct3D/OpenGL for accelerated drawing (and special effects). Optimizations depend on the versions and implementations of these libraries. So if you want the most power and speed DirectX/openGL is the way to go. SDL is certainly useful for getting the most from a cross-platform environment and integrates with OpenGL anyway. The windowing system comes last but don't underestimate it. Especially with the stuff Microsoft's coming up with lately. Heh heh Na the lowest level is VHDL or Verilog design of a VGA controller! Actually check this out http://neil.franklin.ch/Projects/SoftVGA/ That's just using a micro controller!  To be a good graphics and image processing programmer doesn't require this low level knowledge but i do hate to be clueless about the insides of what i'm using. I see two ways to chase this - high-level down or bottom-level up. Top-down is a matter of following how the action traces from a high-level graphics operation such as to draw a circle to the hardware. Get to know OpenGL well. Then the source to Mesa (free!) provides a peek at how OpenGL can be implemented in software. The source to Xorg would be next first to see how the action goes from API calls through the client side to the X server. Finally you dive into a device driver that interfaces with hardware. Bottom up: build your own graphics hardware. Think of ways it could connect to a computer - how to handle massive numbers of pixels through a few byte-size registers how DMA would work. Write a device driver and try designing a graphics library that might be useful for app programmers. The bottom-up way is how i learned years ago when it was a possibility with the slow 8-bit microprocessors. The direct experience with circuitry and hardware-software interfacing gave me a good appreciation of the difficult design decisions - e.g. to paint rectangles using clever hardware in the device driver or higher level. None of this is of practical everyday value but provided a foundation of knowledge to understand newer technology.  libX11 is the lowest level library for X11. I believe the opengl/directx talk to the driver/hardware directly (or emulate unsupported ops) so they would be the lowest level library. If you want to start with very low level programming look for x86 assembly code for VGA and fire up a copy of dosbox or similar.  see Open GPU Documentation section: http://developer.amd.com/documentation/guides/Pages/default.aspx HTH  If you really want to start at the bottom then drawing a line is the most basic operation. Computer graphics is simply about filling in pixels on a grid (screen) so you need to work out which pixels to fill in to get a line that goes from (x0y0) to (x1y1). Check out Bresenham's algorithm to get a feel for what is involved. If a line is drawn by filling pixels on a grid how can drawing a line be more basic than ... setting a pixel? :)  On MSWindows it is easy: you use what the API provides whether it is the standard windows programming API or the DirectX-family API's: that's what you use and they are well documented. In an X windows environment you use whatever X11-libraries that are provided. If you want to understand the principles behind windowing on X I suggest that you do this nevermind that many others tell you not to it will really help you to understand graphics and windowing under X. You can read the documentation on X-programming (google for it). (After this exercise you would appreciate the higher level libraries!) Apart from the above at the absolutely lowest level (excluding chip-level) that you can go is to call the interrupts that switch to the various graphics modes available - there are several - and then write to the screen buffers but for this you would have to use assembler anything else would be too slow. Going this way will not be portable at all. Another post mentions Abrash's Black Book - an excellent resource. Edit: As for books on programming Linux: it is a community thing there are many howto's around; also find a forum join it and as long as you act civilized you will get all the help you can ever need."
392,A,"C#: Graphics DrawString to Exactly Place Text on a System.Label I have overridden the OnPaint method of my Label control in VS2008: void Label_OnPaint(object sender PaintEventArgs e) { base.OnPaint(e); Label lbl = sender as Label; if (lbl != null) { string Text = lbl.Text; e.Graphics.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.AntiAlias; if (myShowShadow) { // draw the shadow first! e.Graphics.DrawString(Text lbl.Font new SolidBrush(myShadowColor) myShadowOffset StringFormat.GenericDefault); } e.Graphics.DrawString(Text lbl.Font new SolidBrush(lbl.ForeColor) 0 0 StringFormat.GenericDefault); } } This works but I really want to find out how to center the text both vertically and horizontally. I've heard of the MeasureString() method but my ""Text"" complicates matters because it could include page breaks. Could someone guide me with how to do this? Here is the code i'm using at the moment SizeF size; string text = ""Text goes here""; size = e.Graphics.MeasureString(text font); x = (lineWidth / 2) - (size.Width / 2); y = top; e.Graphics.DrawString(text font Brushes.Black x y);  You can call TextRenderer.DrawText with the HorizontalCenter and VerticalCenter flags. You also have to use the overload that takes a `Rectangle`. TrueType hinting is liable to ruin the shadow effect. @SLaks: How would I get the DeviceContext (first parameter) for a System.Label object? Pass `e.Graphics` from the `Paint` event handler. (`Graphics` implements `IDeviceContext`)  Alternatively you can create your own StringFormat object and pass it in using an overload of DrawString that supports a RectangleF: StringFormat formatter = new StringFormat(); formatter.LineAlignment = StringAlignment.Center; formatter.Alignment = StringAlignment.Center; RectangleF rectangle = new RectangleF(0 0 lbl.Width lbl.Height); e.Graphics.DrawString(Text lbl.Font new SolidBrush(lbl.ForeColor) rectangle formatter); Both are answers but I can only mark one. This one has code for others to follow up with.  I just wanted to add (a year later) a tool I created because StringAlignment turned out to be not very dependable. It turns out to be very similar to Neo's version. The code below does an excellent job of centering the text both vertically and horizontally. Also I wrote it with various overloads so that different options could be supplied to make this control behave exactly like I want. Here are my overloads: private static void DrawCenter(Label label Graphics graphics) { DrawCenter(label.Text label label.Location label.ForeColor graphics); } private void DrawCenter(string text Label label Graphics graphics) { DrawCenter(text label label.Location label.ForeColor graphics); } private static void DrawCenter(string text Label label Point location Graphics graphics) { DrawCenter(text label location label.ForeColor graphics); } private static void DrawCenter(string text Label label Point location Color fontColor Graphics graphics) { Rectangle rect = new Rectangle(location label.Size); SizeF lSize = graphics.MeasureString(text label.Font rect.Width); PointF lPoint = new PointF(rect.X + (rect.Width - lSize.Width) / 2 rect.Y + (rect.Height - lSize.Height) / 2); graphics.DrawString(text label.Font new SolidBrush(fontColor) lPoint); } To use these for the Label's OnPaint event simply modify my original code in the question to following: private void Label_OnPaint(object sender PaintEventArgs e) { base.OnPaint(e); Label lbl = sender as Label; if (lbl != null) { string txt = lbl.Text; e.Graphics.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.AntiAlias; if (myShowShadow) { // draw the shadow first! Point offset = new Point(lbl.Location.X - 1 lbl.Location.Y - 1) DrawCenter(txt lbl offset myShadowColor e.Graphics); } DrawCenter(lbl e.Graphics); } } For a Print_Document event I have a version that will also print a box around the label if there is already a box around it in the designer: private static void DrawCenter(string text Label label Point location Color fontColor Graphics graphics) { Rectangle rect = new Rectangle(location label.Size); SizeF lSize = graphics.MeasureString(text label.Font rect.Width); PointF lPoint = new PointF((rect.Width - lSize.Width) / 2 (rect.Height - lSize.Height) / 2); graphics.DrawString(text label.Font new SolidBrush(fontColor) lPoint); if (label.BorderStyle != BorderStyle.None) { using (Pen p = new Pen(Color.Black)) { graphics.DrawRectangle(p rect); } } } If you find this at all useful give me a +1. ~Joe"
393,A,FillRectangle has no effect on Graphics from Bitmap? I have code: using (Graphics g = control.CreateGraphics()) { Bitmap bm = new Bitmap(r.Width r.Height g); using (Graphics gbm = Graphics.FromImage(bm)) { gbm.FillRectangle(Brushes.Green r); form.BackgroundImage = bm; form.BackgroundImageLayout = ImageLayout.Zoom; } } But that FillRectangle doesn't seem to have any effect. Any idea? Have you tried making FillRectangle the last graphics related thing done in this block? The subsequent Background stuff is likely causing OnPaint to fire again resulting in your rectangle being painted over. Depending on what method this code snippet appears in this will probably happen every time a repaint occurs.  Perhaps you could try copying your code into a new blank project to see if FillRectangle still has no effect. Maybe something elsewhere in your code is the culprit. Yes like an idiot that I am I forgot the Rectangle r is located outside the bitmap.
394,A,"wpf 2d high performance graphics Basically I want GDI-type functionality in WPF where I can write pixels to a bitmap and update and display that bitmap through WPF. Note I need to be able to animate the bitmap on the fly by updating pixels in response to mouse movements. I've read that InteropBitmap is perfect for this as you can write to pixels in memory and copy the memory location to the bitmap--but I don't have any good examples to go by. Does anyone know of any good resources tutorials or blogs for using the InteropBitmap or some other classes for doing high-performance 2D graphics in WPF? The context of the rest of the app is WPF. If you're doing pixel-by-pixel stuff do you really need WPF? Here's what I found: I created a class that subclasses Image. public class MyImage : Image { // the pixel format for the image. This one is blue-green-red-alpha 32bit format private static PixelFormat PIXEL_FORMAT = PixelFormats.Bgra32; // the bitmap used as a pixel source for the image WriteableBitmap bitmap; // the clipping bounds of the bitmap Int32Rect bitmapRect; // the pixel array. unsigned ints are 32 bits uint[] pixels; // the width of the bitmap. sort of. int stride; public MyImage(int width int height) { // set the image width this.Width = width; // set the image height this.Height = height; // define the clipping bounds bitmapRect = new Int32Rect(0 0 width height); // define the WriteableBitmap bitmap = new WriteableBitmap(width height 96 96 PIXEL_FORMAT null); // define the stride stride = (width * PIXEL_FORMAT.BitsPerPixel + 7) / 8; // allocate our pixel array pixels = new uint[width * height]; // set the image source to be the bitmap this.Source = bitmap; } WriteableBitmap has a method called WritePixels which takes an array of unsigned ints as pixel data. I set the source of the image to be the WriteableBitmap. Now when I update the pixel data and call WritePixels it updates the image. I store the business point data in a separate object as a List of Points. I perform transformations on the list and update the pixel data with the transformed points. This way there's no overhead from Geometry objects. Just FYI I connect my points with lines drawn using something called Bresenham's algorithm. This method is extremely fast. I'm updating about 50000 points (and connecting lines) in response to mouse movements with no noticeable lag.  Here's a blog post on using Web cameras with InteropBitmap. It includes a full source code project demonstrating the InteropBitmap's usage. Love this quote from the blog: ""... WPF performance in terms of imaging is sucks""."
395,A,Optimally place a pie slice in a rectangle Given a rectangle (w h) and a pie slice with a radius less or equal to the smaller of both sides (w h) a start angle and an end angle how can I place the slice optimally in the rectangle so that it fills the room best (from an optical point of view not mathematically speaking)? I'm currently placing the pie slice's center in the center of the rectangle and use the half of the smaller of both rectangle sides as the radius. This leaves plenty of room for certain configurations. Examples to make clear what I'm after based on the precondition that the slice is drawn like a unit circle (i.e. 0 degrees on positive X axis then running clock-wise): A start angle of 0 and an end angle of PI would lead to a filled lower half of the rectangle and an empty upper half. A good solution here would be to move the center up by 1/4*h. A start angle of 0 and an end angle of PI/2 would lead to a filled bottom right quarter of the rectangle. A good solution here would be to move the center point to the top left of the rectangle and to set the radius to the smaller of both rectangle sides. This is fairly easy for the cases I've sketched but it becomes complicated when the start and end angles are arbitrary. I am searching for an algorithm which determines center of the slice and radius in a way that fills the rectangle best. Pseudo code would be great since I'm not a big mathematician. Firstly your pie slice doesn't have a radius? It goes to infinity? Secondly what do you mean by fills the rectangle. If you want to cover the entire rectangle the center of the angle is going to be outside the rectangle. But I don't think that's what you mean so I'm pretty perplexed. Just noticed the bit about the pie slice being drawn like a unit circle (I assume this means a radius of either .5 or 1.) And I assume after that that the pie slice is going to fit completely in the rectangle and you want it visually centered? So you and want to find the center (x y) and radius (r) of an arc given by a start and stop angle that maximizes the area of the arc within a bounding box (w h)? clahey I've tried to clarify this in my original question. Unit circle was referring to the starting angle and direction and the radius has to be determined but would be less or equal the smaller of both sides. Judge exactly. Sorry for not being so precise English is not my native language. Instead of pseudo code I used python but it should be usable. For this algorithm I assume that startAngle < endAngle and that both are within [-2 * PI 2 * PI]. If you want to use both within [0 2 * PI] and let startAngle > endAngle I would do: if (startAngle > endAngle): startAngle = startAngle - 2 * PI So the algorithm that comes to mind is to calculate the bounds of the unit arc and then scale to fit your rectangle. The first is the harder part. You need to calculate 4 numbers: Left: MIN(cos(angle) 0) Right: MAX(cos(angle) 0) Top: MIN(sin(angle)0) Bottom: MAX(sin(angle)0) Of course angle is a range so it's not as simple as this. However you really only have to include up to 11 points in this calculation. The start angle the end angle and potentially the cardinal directions (there are 9 of these going from -2 * PI to 2 * PI.) I'm going to define boundingBoxes as lists of 4 elements ordered [left right top bottom] def IncludeAngle(boundingBox angle) x = cos(angle) y = sin(angle) if (x < boundingBox[0]): boundingBox[0] = x if (x > boundingBox[1]): boundingBox[1] = x if (y < boundingBox[2]): boundingBox[2] = y if (y > boundingBox[3]): boundingBox[3] = y def CheckAngle(boundingBox startAngle endAngle angle): if (startAngle <= angle and endAngle >= angle): IncludeAngle(boundingBox angle) boundingBox = [0 0 0 0] IncludeAngle(boundingBox startAngle) IncludeAngle(boundingBox endAngle) CheckAngle(boundingBox startAngle endAngle -2 * PI) CheckAngle(boundingBox startAngle endAngle -3 * PI / 2) CheckAngle(boundingBox startAngle endAngle -PI) CheckAngle(boundingBox startAngle endAngle -PI / 2) CheckAngle(boundingBox startAngle endAngle 0) CheckAngle(boundingBox startAngle endAngle PI / 2) CheckAngle(boundingBox startAngle endAngle PI) CheckAngle(boundingBox startAngle endAngle 3 * PI / 2) CheckAngle(boundingBox startAngle endAngle 2 * PI) Now you've computed the bounding box of an arc with center of 00 and radius of 1. To fill the box we're going to have to solve a linear equation: boundingBox[0] * xRadius + xOffset = 0 boundingBox[1] * xRadius + xOffset = w boundingBox[2] * yRadius + yOffset = 0 boundingBox[3] * yRadius + yOffset = h And we have to solve for xRadius and yRadius. You'll note there are two radiuses here. The reason for that is that in order to fill the rectangle we have to multiple by different amounts in the two directions. Since your algorithm asks for only one radius we will just pick the lower of the two values. Solving the equation gives: xRadius = w / (boundingBox[1] - boundingBox[0]) yRadius = h / (boundingBox[2] - boundingBox[3]) radius = MIN(xRadius yRadius) Here you have to check for boundingBox[1] - boundingBox[0] being 0 and set xRadius to infinity in that case. This will give the correct result as yRadius will be smaller. If you don't have an infinity available you can just set it to 0 and in the MIN function check for 0 and use the other value in that case. xRadius and yRadius can't both be 0 because both sin and cos would have to be 0 for all angles included above for that to be the case. Now we have to place the center of the arc. We want it centered in both directions. Now we'll create another linear equation: (boundingBox[0] + boundingBox[1]) / 2 * radius + x = xCenter = w/2 (boundingBox[2] + boundingBox[3]) / 2 * radius + y = yCenter = h/2 Solving for x and y the center of the arc gives x = w/2 - (boundingBox[0] + boundingBox[1]) * radius / 2 y = h/2 - (boundingBox[3] + boundingBox[2]) * radius / 2 This should give you the center of the arc and the radius needed to put the largest circle in the given rectangle. I haven't tested any of this code so this algorithm may have huge holes or perhaps tiny ones caused by typos. I'd love to know if this algoritm works. edit: Putting all of the code together gives: def IncludeAngle(boundingBox angle) x = cos(angle) y = sin(angle) if (x < boundingBox[0]): boundingBox[0] = x if (x > boundingBox[1]): boundingBox[1] = x if (y < boundingBox[2]): boundingBox[2] = y if (y > boundingBox[3]): boundingBox[3] = y def CheckAngle(boundingBox startAngle endAngle angle): if (startAngle <= angle and endAngle >= angle): IncludeAngle(boundingBox angle) boundingBox = [0 0 0 0] IncludeAngle(boundingBox startAngle) IncludeAngle(boundingBox endAngle) CheckAngle(boundingBox startAngle endAngle -2 * PI) CheckAngle(boundingBox startAngle endAngle -3 * PI / 2) CheckAngle(boundingBox startAngle endAngle -PI) CheckAngle(boundingBox startAngle endAngle -PI / 2) CheckAngle(boundingBox startAngle endAngle 0) CheckAngle(boundingBox startAngle endAngle PI / 2) CheckAngle(boundingBox startAngle endAngle PI) CheckAngle(boundingBox startAngle endAngle 3 * PI / 2) CheckAngle(boundingBox startAngle endAngle 2 * PI) if (boundingBox[1] == boundingBox[0]): xRadius = 0 else: xRadius = w / (boundingBox[1] - boundingBox[0]) if (boundingBox[3] == boundingBox[2]): yRadius = 0 else: yRadius = h / (boundingBox[3] - boundingBox[2]) if xRadius == 0: radius = yRadius elif yRadius == 0: radius = xRadius else: radius = MIN(xRadius yRadius) x = w/2 - (boundingBox[0] + boundingBox[1]) * radius / 2 y = h/2 - (boundingBox[3] + boundingBox[2]) * radius / 2 edit: One issue here is that sin[2 * PI] is not going to be exactly 0 because of rounding errors. I think the solution is to get rid of the CheckAngle calls and replace them with something like: def CheckCardinal(boundingBox startAngle endAngle cardinal): if startAngle < cardinal * PI / 2 and endAngle > cardinal * PI / 2: cardinal = cardinal % 4 if cardinal == 0: boundingBox[1] = 1 if cardinal == 1: boundingBox[3] = 1 if cardinal == 2: boundingBox[0] = -1 if cardinal == 3: boundingBox[2] = -1 CheckCardinal(boundingBox startAngle endAngle -4) CheckCardinal(boundingBox startAngle endAngle -3) CheckCardinal(boundingBox startAngle endAngle -2) CheckCardinal(boundingBox startAngle endAngle -1) CheckCardinal(boundingBox startAngle endAngle 0) CheckCardinal(boundingBox startAngle endAngle 1) CheckCardinal(boundingBox startAngle endAngle 2) CheckCardinal(boundingBox startAngle endAngle 3) CheckCardinal(boundingBox startAngle endAngle 4) You still need IncludeAngle(startAngle) and IncludeAngle(endAngle) Fixed the parameter order. I just checked and in python at least that modulo operation is always going to be positive. Other languages may have different rules if the first number is negative and the second positive. clahey thanks alot. I've voted for Judge's answer since he has also thought of the centering but your solution looks good to me as well even though the CheckCardinal method has some glitches (check the parameter ordering and the modulo operation should probably add a Math.abs(...) around it?!).  Just consider a circle and forget the filling. The bounds will either be the center of the circle the endpoints or the points at 0 90 180 or 270 degrees (if they exist in this slice). The maxima and minima of these seven points will determine your bounding rectangle. As far as placing it in the center calculate the average of the max and min for both the rectangle and the pie slice and add or subtract the difference of these to whichever one you want to move. Or the center... @algorithmist - thanks for the correction. I've fixed my answer. If your angle was 100° your width calculation would either be a little under or a lot over. Same goes for height and an angle of 190°. You have to do the trigonometry. If we're talking 0--100 then the leftmost point is the second endpoint the rightmost is 0/the first endpoint the topmost is 90 and the bottommost is the center. I don't understand your criticism---my understanding of the OP is that the slice cannot be rotated. @Seth and algorithmist - I don't think it matters where the start and endpoints are etc. For all curves the extrema are either at critical points (where the derivative is zero or doesn't exist) or at end points.  The extrema of the bounding box of your arc are in the following format: x + x0 * r = 0 x + x1 * r = w y + y0 * r = 0 y + y1 * r = h The values x0 x1 y0 and y1 are found by taking the minimum and maximum values of up to 7 points: any tangential points that are spanned (i.e. 0 90 180 and 270 degrees) and the end points of the two line segments. Given the extrema of the axis-aligned bounding box of the arc (x0 y0) (x1 y1) the radius and center point can be calculated as follows: r = min(w/(x1-x0) h/(y1-y0) x = -x0 * r y = -y0 * r Here is an implementation written in Lua: -- ensures the angle is in the range [0 360) function wrap(angle) local x = math.fmod(angle 2 * math.pi) if x < 0 then x = x + 2 * math.pi end return x end function place_arc(t0 t1 w h) -- find the x-axis extrema local x0 = 1 local x1 = -1 local xlist = {} table.insert(xlist 0) table.insert(xlist math.cos(t0)) table.insert(xlist math.cos(t1)) if wrap(t0) > wrap(t1) then table.insert(xlist 1) end if wrap(t0-math.pi) > wrap(t1-math.pi) then table.insert(xlist -1) end for _ x in ipairs(xlist) do if x < x0 then x0 = x end if x > x1 then x1 = x end end -- find the y-axis extrema local ylist = {} local y0 = 1 local y1 = -1 table.insert(ylist 0) table.insert(ylist math.sin(t0)) table.insert(ylist math.sin(t1)) if wrap(t0-0.5*math.pi) > wrap(t1-0.5*math.pi) then table.insert(ylist 1) end if wrap(t0-1.5*math.pi) > wrap(t1-1.5*math.pi) then table.insert(ylist -1) end for _ y in ipairs(ylist) do if y < y0 then y0 = y end if y > y1 then y1 = y end end -- calculate the maximum radius the fits in the bounding box local r = math.min(w / (x1 - x0) h / (y1 - y0)) -- find x & y from the radius and minimum extrema local x = -x0 * r local y = -y0 * r -- calculate the final axis-aligned bounding-box (AABB) local aabb = { x0 = x + x0 * r y0 = y + y0 * r x1 = x + x1 * r y1 = y + y1 * r } return x y r aabb end function center_arc(x y aabb w h) dx = (w - aabb.x1) / 2 dy = (h - aabb.y1) / 2 return x + dx y + dy end t0 = math.rad(60) t1 = math.rad(300) w = 320 h = 240 x y r aabb = place_arc(t0 t1 w h) x y = center_arc(x y aabb w h) print(x y r) Example output: These calculations will put the pie slice in the upper left corner instead of centering it. Apparently I have completely misunderstood your question... Where did you specify that the arc should be centered? Maximizing the area of the arc and placing it in the center of the rectangle are contradictory. I updated the code sample with a function to center the arc's AABB within the rectangle. This does not place the center of mass in the center of the rectangle just the extents of the arc. I don't see how maximizing the area of the arc and placing it in the center of the rectangle are contradictory. You just make the biggest arc that would fit and put half of the extra space on either side. I've been thinking about what it might mean to center the center of mass of the arc and in that case it may very well lead to a smaller arc. I am curious what the center of mass of a filled arc is though. Oh I see what you mean. In this test case the radius is limited by the height. So the x value no longer needs to be at the far left. Well once you have x y & r from this algorithm you can calculate the true bounding box of the arc and adjust x & y so as to place the it in the center of the rectangle. Judge thanks alot. This is EXACTLY what I was looking for. Your support and commitment is greatly appreciated! I don't know Lua yet but I think I get the points.  I would divide the problem into three steps: Find the bounding box of a unit pie slice (or if a radius is given the actual pie slice centered at (0 0)). Fit the bounding box in your rectangle. Use the information about fitting the bounding box to adjust the center and radius of the pie slice. When I have time I may flush this out with more details.
396,A,"How to create a repeating background with xaml elements in silverlight? I want to create an area in my application that looks draggable. Usually you see this done with a background of small dots or squares or sometimes lines. I'm using Silverlight and I want to simply create a background that is a set of repeating small rectangles. I honestly cannot figure out how to generate a background with xaml. I'd rather not have to create every little rectangle -- this will also cause the control not to scale. Is there some way to repeat xaml elements to form a pattern? This would be similar to CSS repeating backgrounds but I would like to use xaml instead of images. You can use a brush like this: <Rectangle> <Rectangle.Fill> <LinearGradientBrush EndPoint=""66"" StartPoint=""22"" SpreadMethod=""Repeat"" MappingMode=""Absolute""> <GradientStop Color=""#FFAFAFAF"" Offset=""0""/> <GradientStop Color=""#00FFFFFF"" Offset=""1""/> <GradientStop Color=""#00FFFFFF"" Offset=""0.339""/> </LinearGradientBrush> </Rectangle.Fill> </Rectangle> I've nicked this particular example from the excellent blacklight project you'll need to play around with all the different settings to see what does what. I'm guessing a radial brush will allow you to get dots etc. I think they created it in blend as all the numbers were crazy decimals until I sanitised them a bit. Thanks! I found the same snippet. I am wondering how to do it in 2 dimensions to get a rectangular pattern (like you said radialBrush might help). But this gets me on the right track. It still would be interesting to have a xaml repeater of sorts. I guess I could just write my own layout control to do that. I think a repeater wouldn't be very good performance wise (e.g. a thousand elements in a listbox takes a second or two to render)."
397,A,"mouseover banner controlled by text I'm trying to create an effect where rolling the mouse over some text will cause an image in another part of the page to change to another image until you the mouse moves away from the text. Does anyone know of a simple way to do this? I'd prefer using CSS only but will use js if it is necessary. Welcome to StackOverflow - let me be the first to reward you with 12 points for the thoughtful question! Getting this type of interaction between the mouse and various elements shouldn't be approached with CSS alone. You will need to use Javascript. The following example uses the jQuery Framework (simply addon for raw javascript) to change the source of an image when you hover over a paragraph: $(""p.magicParagraph"").hover( function() { $(""img.magicImage"").attr(""src"" ""image2.jpg""); } function() { $(""img.magicImage"").attr(""src"" ""image1.jpg""); } ); This code binds a set of events to the hovering of any paragraph having the className ""magicParagraph"". The first function is what will be done when you move over the paragraph and the second is what will be done when you move off of the paragraph. This would function with the following markup: <p class=""magicParagraph"">Hover over me to change the image!</p> <p><img src=""image1.jpg"" class=""magicImage"" /></p> I think I'll go with this! Thank you. I'll also try the one below so thank you guy who responded with the CSS only version as well! @Alex C: Welcome to the site. I agree you should probably go with the standard Javascript solution rather than CSS only - but you did ask for CSS only so that's what I gave you. :-P Remember to accept an answer after testing - use the green tick. It's OK to wait 24 hours before doing so if you want to see answers from more people.  It is possible to get interaction between various element using CSS alone but it's not easy. As far as I can see it places some constraints on your document structure (perhaps someone with more knowledge of CSS selectors than myself can see ways around this). Please consider this to be a proof of concept rather than a complete solution. Note that this requires CSS Level 2 support. <html> <head> <style> img { float: right } p.magicParagraph + img { display: none } p.magicParagraph:hover + img { display: block } p.magicParagraph + img + img { display: block } p.magicParagraph:hover + img + img { display: none } </style> </head> <body> <p class=""magicParagraph"">Hover over me to change the image!</p> <img src=""image1.png"" /> <img src=""image2.png"" /> </body> </html>"
398,A,"When using Direct3D how much math is being done on the CPU? Context: I'm just starting out. I'm not even touching the Direct3D 11 API and instead looking at understanding the pipeline etc. From looking at documentation and information floating around the web it seems like some calculations are being handled by the application. That is instead of simply presenting matrices to multiply to the GPU the calculations are being done by a math library that operates on the CPU. I don't have any particular resources to point to although I guess I can point to the XNA Math Library or the samples shipped in the February DX SDK. When you see code like mViewProj = mView * mProj; that projection is being calculated on the CPU. Or am I wrong? If you were writing a program where you can have 10 cubes on the screen where you can move or rotate cubes as well as viewpoint what calculations would you do on the CPU? I think I would store the geometry for the a single cube and then transform matrices representing the actual instances. And then it seems I would use the XNA math library or another of my choosing to transform each cube in model space. Then get the coordinates in world space. Then push the information to the GPU. That's quite a bit of calculation on the CPU. Am I wrong? Am I reaching conclusions based on too little information and understanding? What terms should I Google for if the answer is STFW? Or if I am right why aren't these calculations being pushed to the GPU as well? EDIT: By the way I am not using XNA but documentation notes the XNA Math Library replaces the previous DX Math library. (i see the XNA Library in the SDK as a sheer template library). ""Am I reaching conclusions based on too little information and understanding?"" Not as a bad thing as we all do it but in a word: Yes. What is being done by the GPU is generally dependent on the GPU driver and your method of access. Most of the time you really don't care or need to know (other than curiosity and general understanding). For mViewProj = mView * mProj; this is most likely happening on the CPU. But it is not much burden (counted in 100's of cycles at the most). The real trick is the application of the new view matrix on the ""world"". Every vertex needs to be transformed more or less along with shading textures lighting etc. All if this work will be done in the GPU (if done on the CPU things will slow down really fast). Generally you make high level changes to the world maybe 20 CPU bound calculations and the GPU takes care of the millions or billions of calculations needed to render the world based on the changes. In your 10 cube example: You supply a transform for each cube any math needed for you to create the transform is CPU bound (with exceptions). You also supply a transform for the view again creating the transform matrix might be CPU bound. Once you have your 11 new matrices you apply the to the world. From a hardware point of view the 11 matrices need to be copied to the GPU...that will happen very very fast...once copied the CPU is done and the GPU recalculates the world based on the new data renders it to a buffer and poops it on the screen. So for your 10 cubes the CPU bound calculations are trivial. Look at some reflected code for an XNA project and you will see where your calculations end and XNA begins (XNA will do everything is possibly can in the GPU). ""Generally you make high level changes to the world maybe 20 CPU bound calculations"" : Even for more complex scenes? Would you then redesign your math library (say pushing CPU bound calculations to the GPU as well)? Or is it simply a flawed design to have a high number of CPU bound calculations? (Also note my edit using C++ not C# + XNA). Using the CPU is not evil: The number of CPU calculations you can perform and still maintain 60 FPS (the minimum target FPS for a good user experience IMHO) is absolutely insane on a modern dual core CPU and with 64bit 6 core 12 path 3.5GHz CPUs available for < $1000usd it only gets better. Of course allowable % of CPU time is completely up to you. There are several libraries for pushing general calculation off to a GPU see: http://stackoverflow.com/questions/1249892/c-perform-operations-on-gpu-not-cpu-calculate-pi . The use of the GPU as math co-processor is definitely on the move. I don't consider the CPU to be evil but I just find it odd that calculations related to rendering graphics are being left to the CPU and then there's a cutting edge trend to move such calculations to the very device that accelerates rendering. Regardless it's good to have confirmation about the calculations on the CPU. Thanks for taking the time to answer my question. @zirgen: ""odd..left to the cpu""...It has not been that long since you didn't have a choice...everything happened in the CPU. The way something gets done on a give platform is always effected by the history of the platform and some of the time (ok most) history wins out over ""The Right Way.""...Cheers"
399,A,What are good resources for computer graphics basics? During Flex programming I recently ran into several questions (about box models ways to join lines and misaligning pixels [on doctype]) regarding computer graphics and layout where I felt that I lacked some basic background on things like concepts like the box model approaches mapping real numbers to a pixel raster (like font anti-aliasing) conventions found across drawing engines like do you count y coordinates from top or bottom and why I feel that reading some basic Wikipedia articles books or tutorials on these subjects might help in phrasing my questions more specifically and debugging my code more systematically. I have repeatedly found myself writing tiny test apps in Flex just to find out how the APIs do very basic stuff. My assumption would be that if I knew the right vocabulary and some general concepts I could solve these questions much faster. Related q [here](http://stackoverflow.com/questions/1960434/where-can-i-find-computer-graphics-video-lectures). I don't have much experience in this field myself but I'd suggest to have a look at Ke-Sen Huang's Home Page CG Online Tutorials The Cornell University's CG page The Arizona State University Introduction to Computer Graphics The Brown University Introduction to Computer Graphics
400,A,How to implement a gradient color picker in code? I'm wanting to program something like Photoshop's gradient color picker. I've been googling but can't find anything. Do you guys know how to do it? Any specific color model (ie. rgb lab cmyk etc.)? If you are thinking web based you could get some ideas from this JQuery plugin http://www.eyecon.ro/colorpicker/ @prodigitialson - RGB It's a bit out of date (and .NET if that's your choice of framework) but in the past I've implemented an app with this ColorPicker.  I remember hacking up something like it with a static image and getPixel.
401,A,"Options to create editable graph or scheme with Perl? I'm searching for a way to create complex/simple graph using Perl. The known modules/applications I've checked are: GraphViz Graph-Easy aiSee etc. Each way I walked new problems appeared. If my need is to create graph dependencies that can be edited live have a directed compass mode work fine and are readable with massive data use can be used through the terminal to convert from input format (GDL etc.) to output format (PNG BMP HTML etc.) – what are the various applications that can handle all of these requests? This question is a follow-up of How can I convert connection data lines to block of schemes using Perl?. Thanks YoDar. What do you mean by ""new problems appear""? Perhaps the problems are not with the packages you are trying but something else. ""new problems appear"" means that I cant combine in one module/application all the needs I wrote. Each module/application has it's benefits and disadvantage. Again I'm searching for some pure graphic module/application to satisfy my needs. It might be an overkill for your application but you might also want to look into PerlMagik."
402,A,"How do I interpolate normals on Catmull-Clark Subdivision Surfaces I'm using CCSS to generate smooth surfaces. I've been using the regular subdivision rules to interpolate the surface/vertex normal but I think this may be wrong. Are there different stencils to interpolate normals? The ""normals"" from the control mesh are not really normals to begin with. They're just made-up vectors at each vertex and not something you want to interpolate. Instead use the derivative stencils which yield tangent vectors in two directions. Once you have your tangent vectors cross them to get a normal. The derivative stencils are: 1 4 1 0 (0) 0 -1 -4 -1 and -1 0 1 -4 (0) 4 -1 0 1"
403,A,"How to convert .ICO to .PNG? What tool can I use to convert a .ICO file to a .PNG file? I'm surprised that nobody has mentioned that ICO files are really just BMPs. a BMP is a single bitmap and starts with a BITMAPFILEHEADER structure. An ICO is a collection of bitmaps+other stuff like masks and starts with an ICONDIR structure. (http://msdn.microsoft.com/en-us/library/ms997538.aspx). So yes ICOs and BMPs are both bitmap containers but they aren't the same thing. OMG.. 36 upvotes for this.. http://converticon.com/ is also a candidate. This is browser-based (flash) for those interested. +1  There is an online conversion tool available at http://www.html-kit.com/favicon/. In addition to generating the .ico it will also give you an animated .gif version.  I don't know where I would be without IrFanView. Fantastic for batch converting images including ico to png.  One quick option is to download Paint.net and install the Icon/Cursor plugin. You can then open .ico files with Paint.net edit them and save them to .png or another format. For batch processing I second the suggestions of ImageMagick or IrFanView.  http://www.gimp.org/ free and powerful way to make large resolution .ico files 1024x1024 or greater work with win7 at least I've tested that :) just save and type .ico :) transparency is easy load a new image and select advanced options background color->transparency  I did it this way in C# does the job nicely #region Usings using System; using System.IO; using System.Linq; // Next namespace requires a reference to PresentationCore using System.Windows.Media.Imaging; #endregion namespace Imagetool { internal class Program { private static void Main(string[] args) { new Ico2Png().Run(@""C:\Icons\"" @""C:\Icons\out\""); } } public class Ico2Png { public void Run(string inPath string outPath) { if (!Directory.Exists(inPath)) { throw new Exception(""In Path does not exist""); } if (!Directory.Exists(outPath)) { Directory.CreateDirectory(outPath); } var files = Directory.GetFiles(inPath ""*.ico""); foreach (var filepath in files.Take(10)) { Stream iconStream = new FileStream(filepath FileMode.Open); var decoder = new IconBitmapDecoder( iconStream BitmapCreateOptions.PreservePixelFormat BitmapCacheOption.None); var fileName = Path.GetFileName(filepath); // loop through images inside the file foreach (var frame in decoder.Frames) { // save file as PNG BitmapEncoder encoder = new PngBitmapEncoder(); encoder.Frames.Add(frame); var size = frame.PixelHeight; // haven't tested the next lines - include them for bitdepth // See RenniePet's answer for details // var depth = frame.Thumbnail.Format.BitsPerPixel; // var path = outPath + fileName + size + depth +"".png""; var path = outPath + fileName + size + "".png""; using (Stream saveStream = new FileStream(path FileMode.Create)) { encoder.Save(saveStream); } } } } } } Thanks nawful I'm glad someone else out there thinks learning to code is still a worthwhile thing to do. of course there is another python answer. I flagged this question for mmod attention and they said ""helpful"" but the thread got no change ! if this question is fit for SO then this is the only correct answer.. This is a great answer and I've upvoted it thanks Peter. But there is a problem here. The ICO file may contain multiple images of the same size for example 32 x 32 but with different bit depths. This code will write and re-write the same output file for each image and the last-written output file may not be the one with the best bit depth. This can be fixed by adding the bit depth to the output filename so different files are written for different bit depths. See my answer down at the end of this thread for more information.  Free: @icon sushi is very good for working with icons: Features icon sushi can convert image files into icon files and vice versa. Support for Windows Vista large icons. (convert large image with PNG compression) Support for Windows XP 32bit Icons. Support for Multiple-Icon which contains some icons in a file. Edit Alpha channel and Transparency-Mask. Open 1x1 to 256x256 size of images. Open 1/4/8/24/32bits color images. Open: ICO/BMP/PNG/PSD/EXE/DLL/ICL Convert into: ICO/BMP/PNG/ICL Copy to / Paste from Clipboard. Found this one as the only free tool from all above.  This is probably a rather silly answer but if you only need one icon you could just take a screenshot of the icon in the folder and chop out the part you want. Make sure the icon is showing the size you want and has a white background of course. If you are using a decent screenshot application like SnagIt or WinSnap a region snap should take care of it within a few seconds. Note that this won't give you transparency.  Check out http://iconverticons.com/ - iConvert allows you to easily convert Windows ico to Mac OS X icns SVG to Windows icons PNG ico to Mac OS X ico JPG images to Windows icons and much more.  Another alternative would be IrfanView ... which is why two previous posters mentioned it (in 2008). Please at least do a Ctrl+F to see if your answer exists already. Please :-)  Icon Convert is another online tool with resize option. That site doesn't appear to have a way to convert from an icon (ICO file) to an image only the other way around.  http://convertico.org/ allows users to convert multiple ico files to PNG GIF or JPG files in one step. Consider expanding your answer with some excerpt from the referenced page. See [How to Answer](http://stackoverflow.com/questions/how-to-answer) for details why bare links are not considered good answers.  Here is some C# code to do it based very much on the answer on this thread by ""Peter"". (If you find this answer useful please up-vote Peter's answer.)  /// <summary> /// Method to extract all of the images in an ICO file as a set of PNG files. The extracted /// images are written to the same disk folder as the input file with extended filenames /// indicating the size of the image (16x16 32x32 etc.) and the bit depth of the original /// image (typically 32 but may be 8 or 4 for some images in old ICO files or even in new /// ICO files that are intended to be usable in very old Windows systems). But note that the /// PNG files themselves always have bit depth 32 - the bit depth indication only refers to /// the source image that the PNG was created from. Note also that there seems to be a bug /// that makes images larger than 48 x 48 and with color depth less than 32 non-functional. /// /// This code is very much based on the answer by ""Peter"" on this thread: /// http://stackoverflow.com/questions/37590/how-to-convert-ico-to-png /// /// Plus information about how to get the color depth of the ""frames"" in the icon found here: /// http://social.msdn.microsoft.com/Forums/en-US/e46a9ad8-d65e-4aad-92c0-04d57d415065/a-bug-that-renders-iconbitmapdecoder-useless /// </summary> /// <param name=""iconFileName"">full path and filename of the ICO file</param> private static void ExtractImagesFromIconFile(string iconFileName) { try { using (Stream iconStream = new FileStream(iconFileName FileMode.Open)) { IconBitmapDecoder bitmapDecoder = new IconBitmapDecoder(iconStream BitmapCreateOptions.PreservePixelFormat BitmapCacheOption.None); foreach (BitmapFrame bitmapFrame in bitmapDecoder.Frames) { int iconSize = bitmapFrame.PixelHeight; int bitDepth = bitmapFrame.Thumbnail.Format.BitsPerPixel; string pngFileName = Path.GetDirectoryName(iconFileName) + Path.DirectorySeparatorChar + Path.GetFileNameWithoutExtension(iconFileName) + ""-"" + iconSize + ""x"" + iconSize + ""-"" + bitDepth + "".png""; using (Stream saveStream = new FileStream(pngFileName FileMode.Create)) { BitmapEncoder bitmapEncoder = new PngBitmapEncoder(); bitmapEncoder.Frames.Add(bitmapFrame); bitmapEncoder.Save(saveStream); } } } } catch (Exception ex) { MessageBox.Show(""Unable to extract PNGs from ICO file: "" + ex.Message ""ExtractImagesFromIconFile"" MessageBoxButtons.OK MessageBoxIcon.Error); } } have edited my answer to include the bitdepth  In the terminal on mac: convert favicon.ico favicon.png Yes that's [ImageMagick](http://stackoverflow.com/a/37594).  ConvertICO.com has always worked fine for me. +1 Just used this and it worked perfectly. But due to it's online nature it's probably only good for images that you aren't too protective about.  In case anyone want to convert with Python Imaging Library (PIL) in memory from a file or url from cStringIO import StringIO import Image import urllib def to_png(path mime=""png""): if path.startswith(""http:""): url = urllib.quote(url) input = StringIO() input.write(urllib.urlopen(url).read()) input.seek(0) else: input = open(path).read() if input: out = StringIO() image = Image.open(input) image.save(out mime.upper()) return out.getvalue() else: return None Fantastic! This makes using ipython with --profile sh the best possible method for me to do this :)  XnView is a great graphics utility for Windows/Mac/Linux (free) (download page) that will let you browse images batch convert transform resize rotate take screenshots etc. It can do your XYZ to ICO conversion where XYZ is almost any format under the sun.  ImageMagick can convert practically any widely used image format to another. http://www.imagemagick.org/script/index.php see http://www.imagemagick.org/script/convert.php in particular There are ImageMagick bindigs for most popular languages. Yes this is super easy. Just ran ""convert favicon.ico favicon.png"" and it worked fine. To batch convert using ImageMagick (if you have cygwin installed) : `ls *.ico | xargs -I % /c/ImageMagick/convert.exe % %.png` Note that if there are multiple icons in the ico file then the files are numbered. E.g. `convert favicon.ico icon.png` gives `icon-0.png` and `icon-1.png` if there are two files in it.  I just ran into this issue. FYI open the .ico in paint and save as .png. Worked for me!  If you'r not looking for something programmatic then just 'Print Screen' and crop.  Google has an ico to png converter I saw it on reddit the other day. http://www.google.com/s2/favicons?domain=stackoverflow.com thanks for good link totally awesome @Copas I think for the most part yes but I believe there are ways to use tags and direct it to another location. @Copas: no it's not. The above converts the ICO to PNG whereas browsing to favicon.ico manually obviously does not.  The version of Paint that ships with Windows 7 will convert Icons to PNG JPEG ect... now.  Note: This was free when this question was asked but apparently it's a paid app now. @Sean Kearon should change the ""correct answer"" now. You can use IcoFX ($59) It is an all-in-one solution for icon creation extraction and editing. It is designed to work with Windows XP Windows Vista and Macintosh icons supporting transparency. [@icon sushi](http://www.towofu.net/soft/e-aicon.php) is a perfect free alternative. @Radu D IcoFX was once free. How on earth is a $59 payware application the ""correct answer"" for this question? @stolsvik It was free on Sep 2008 but time changes everything apparently. I wish I can ""unanswer"" this. IcoFX 2 is paid; IcoFX 1 is free. The free version is no longer hosted on the developer's website but you can download it from [FileHippo](http://www.filehippo.com/download_icofx/6890/)"
404,A,Image cropping in Silverlight 3 I've found a solution that looks quite elegant but i don't understand its behavior. If i apply the cropping algorithm before adding my image to Canvas.Children collection it does not work. Why? Ideally i need a function like Image Crop(Image source int X int Y int width int heigh) which returns a new cropped Image instance and does not require source/cropped images to be placed to Canvas. Is it possible? You can use WriteableBitmapEx available as NuGet package. void Crop(Image source string imageRelativePath int x int y int width int heigh) { WriteableBitmap wb = BitmapFactory.New(1 1) .FromResource(imageRelativePath) .Crop(x y width heigh); wb.Invalidate(); source.Source = wb; }  I would guess that the problem you are running into is that when you are calling Render on the WriteableBitmap the image source has not downloaded yet so the Image renders nothing into the WriteableBitmap. To workaround this you will need to wait for the image source to download; you can handle the Image.ImageOpened event to get notified when this happens. I found the same issue when I wrote the blog post. In the end I opted for the simpler sample code and just added it to the canvas. I have update the blog post with your question. Thanks Thanks! I've set Build Action for my image to 'Content' and retrieved it using Application.GetResourceStream(). All works fine now!
405,A,Byte codes for pixel maps for Ascii characters? Anyone who ever had to draw text in a graphics application for pre-windows operating systems (i.e. Dos) will know what I'm asking for. Each ASCII character can be represented by an 8x8 pixel matrix. Each matrix can be represented by an 8 byte code (each byte used as a bit mask for each line of the matrix 1 bit representing a white pixel each 0 a black pixel). Does anyone know where I can find the byte codes for the basic ASCII characters? Thanks BW Here's a useful Wikipedia article: http://en.wikipedia.org/wiki/Code_page_437 IBM DOS and MS-DOS used a charset in the BIOS where each character was 8x16 pixels not 8x8 @mads: this I wasn't aware of thanks. I had 8x8 inmind because I drew my own 8x8 characters for a project back in *gulp* 1991 http://cone3d.gamedev.net/cone3d/gfxsdl/tut4-2.gif you could parse/process this bitmap and get the byte matrixes (matrices?) from this I could do that . . . I could also do my own dentristy with piece of string some razor blades and a pliers . . . but strangely I dont that :)  It depends on the font. Search Google for 8x8 pixel fonts and you'll find a lot of different ones. Converting from an image to a byte code table is trivial. Loop through each image 8x8 block at at time reading the pixels and setting the bytes. Do that post the byte codes and the correct answer goes to you. I don't have a fortune of time to spend on this. Thanks.  There are some good ones here; maybe not 8x8 but still easy parse +1 Exactly the type of thing I'm looking for thanks.  Would this do? Hope this helps. +1 It might well do thanks
406,A,"Drag & Drop Shapes on Canvas I used MouseDragElementBehavior to let user drag & drop Shapes on a Canvas: MouseDragElementBehavior dragBehavior = new MouseDragElementBehavior(); dragBehavior.Attach(myShape); Now I would like to know when myShape was moved and when myShape was dropped. Could you please suggest a code that prints the mouse coordinates when myShape is moved and prints ""Dropped"" when myShape is dropped ? Thank you so much ! MouseDragElementBehavior class has DragBegun DragFinished Dragging events. I believe subscribing to those would do what you need"
407,A,drawing arrows on a line defined by a set of points( for tracking) I have a set of points which define a route and I must draw them so a vehicle's moving direction is denoted. The points may be from a curve and I need to draw some arrows. I want to draw arrows on the route to define which arrow vehicle goes. I have a mapviewr java applet and the last I must do is this work I want to define arrows on every 10 points on the route. A thing like this: no why you want to know? So the text is from a homework assignment. Please tag it as homework. i am working on an AVL project and it is not a homwork thanks for your attention did the image come with the homework assignment? I would suggest you to take a look at Java2D shapes :they allow you to define any kind of ... well ... shape and draw them on a given path. You can take a look at this tutorial or at Sun tutorial.  In addition to what Riduidel said you should also try to get the angle that results from the intersection of the line with OX. Using that angle you could calculate a transformation matrix to draw the arraws neatly aligned to the line.  i found the solution at Drawing Arrows thank you  Pete Kirkham posted a full working example a few months ago in How to draw a directed arrow in Java?
408,A,"What's a simple/fast way to display half-star ratings in an iphone view I have a (float) rating value as a percentage from 0..100 (where 50 = Just OK 0 = terrible and 100=best). What's a simple way to display this as a 5 star rating on the iphone with the following requirements: simple (ideally just using drawing operations based on a png of a single star or five of them and without needing to resort to photoshop.) reasonably fast (this is part of a cell in a table view) includes half stars (or more fine grained) displays something reasonable for a rating of 0 (or close to 0) (This is display only so I don't need it to respond to touch events though that would be nice - currently I'm just using a slider to capture the rating in the first place) I've come up with a simple way to do this without using any PNGs at all. I've subclassed a UIView and then in the drawRect I've done this: - (void)drawRect:(CGRect)rect { // Drawing code CGContextRef context = UIGraphicsGetCurrentContext(); [textColor set]; NSString* stars=@""★★★★★""; rect=self.bounds; UIFont *font = [UIFont boldSystemFontOfSize: 16]; CGSize starSize = [stars sizeWithFont: font]; rect.size=starSize; [@""☆☆☆☆☆"" drawInRect:rect withFont: font]; CGRect clip=rect; clip.size.width=clip.size.width*rating/100; CGContextClipToRect(contextclip); [stars drawInRect:rect withFont:font]; } (those strings are 5 empty and 5 full stars in case they are only displaying on my mac. I just entered them within IB using the 'special characters' menu) I've added a textColor (UIColor*) and rating (int) property so that is where those are coming from. To add the view within IB I just change the class type to the name of my UIView subclass. Then I can add the view to any container within IB.  I would consider having two UIImageViews layered directly on top of each other. The bottom image view would contain a PNG of five 'empty' stars - the top layer a PNG of five 'full' stars. I would resize the width of the top layer depending on the rating to expose the empty starts underneath. You can determine a suitable width of the top layer with something like the following: newStarLayerWidth = fullStarLayerWidth * (percentage / 100) You would need to ensure that the image in the top layer is not resized and is aligned left by setting the views contentMode to UIViewContentModeLeft as well as set clipsToBounds = YES otherwise your image will not clip based on the frame size. If this is not fast enough I would take the same approach but draw the cell directly as described here - however you may find that this is not necessary. +1. Good solution wow that's a lot more clever than what I was going to do"
409,A,"Is there a table of OpenGL extensions versions and hardware support somewhere? I'm looking for some resource that can help me decide what OpenGL version my game needs at minimum and what features to support through extensions. Ideally a table of the following format:  1.0 1.1 1.2 1.2.1 1.3 ... multitexture - ARB ARB core core texture_float - EXT EXT ARB ARB ... (Not sure about the values I put in but you get the idea.) The extension specs themselves at opengl.org list the minimum OpenGL version they need so that part is easy. However many extensions have been accepted and became core standard in subsequent OpenGL versions but it is very hard to find when that happened. The only way I could find is to compare the full OpenGL standards document for each version. On a related note I would also very much like to know which extensions/features are supported by which hardware to help me decide what features I can safely use in my game and which ones I need to make optional. For example a big honkin' table like this:  MAX_TEXTURE_IMAGE_UNITS MAX_VERTEX_TEXTURE_IMAGE_UNITS ... GeForce 6xxx 8 4 GeForce 7xxx 16 8 ATi x300 8 4 ... (Again I'm making the values up.) The table could list hardware limitations from glGet but also support for particular extensions and limitations of such extension support (e.g. what floating-point texture formats are supported in hardware). Any pointers to these or similar resources would be hugely appreciated! There's a good database from the open source game 0 A.D. available here. This is great very comprehensive and much better than the others thank you!  You might want to take a look at GLView. This lets you view what extensions are available on your hardware and they have a database of what hardware has supported what extensions. For many of the ARB extensions the database also contains a note about ""promoted to core feature in OpenGL X.Y"". Thanks! That looks good but it's Windows and OS X only... I'll try to find a machine I can run this on. (I hope the program is better than the website though.)  Geeks3D.com has a good list of what OpenGL extensions are supported by which version of the NVIDIA and AMD/ATI drivers. This is good for checking for compatibility as well. Thanks that is useful. Pity it's only for fairly recent cards.  While not directly answering either of your questions Chris Dragan has a database for DirectX caps / OpenGL extensions supported by graphic adapters. What do you mean ""not directly answering""? This is pretty good! Unfortunately that database does not have many cards listed but it gives an impression of what's generally available."
410,A,"How is the default constructor of System.Drawing.Graphics removed? When I try to create an object of Graphics why doesn't the following work? System.Drawing.Graphics graphicsObj = new System.Drawing.Graphics(); (I am aware that I could create a private System.Windows.Forms.Panel Obj; and then do CreateGraphics() if I wanted it to work) I tried to find a custom constructor for Graphics but I couldn't find one. Where did Microsoft define it or how did it block it? Microsoft did not give the Graphics object constructors. The only way to create an instance is through the static methods such as CreateGraphics() or FromImage(). That is why your code does not work. Also as a sidenote the Graphics object cannot be inherited from. Actually Microsoft did give the Graphics class constructors otherwise the compiler would have generated a public default constructor. Microsoft just did not make any of the constructors they added public. Agreed! There are indeed constructors. Just none that can be publicly accessed  Default constructors are only created by the C# compiler if there are no other declared constructors. In this case it looks like all constructors are internal or private. Basically you don't construct one yourself - you ask for one from an image control or whatever or get given one for paint events etc.  It is intuitively obvious that Graphics cannot have a default constructor. You always want what you draw to be visible somewhere. A default constructor could not select the destination of the drawing. Ways to get a Graphics object: Graphics.FromImage(). You'll draw into a bitmap. Common when resizing images or to create a ""canvas"". Control.CreateGraphics(). Let's you draw directly to the screen. Always wrong instead use: Paint event. The e.Graphics argument lets you draw to the screen. PrintPage event. For the PrintDocument class e.Graphics ends up on a piece of paper. Graphics.FromHdc(). Use in low-level P/Invoke code draws to a Windows' device context. Graphics.FromHwnd(). As above draws directly to a native window. In summary: Draw to the screen with the Paint event Draw to the printer with the PrintPage event Draw to a bitmap with FromImage()"
411,A,Graphics.DrawImage logo scaling incorrectly on a large image I'm doing some standard code I am applying a logo to an image and it is working fine. The source image is always 1024 x 768 as the code before this takes the image and resizes it (creates a new file based on the original). The logo is applied correctly on some images I have that are 2288 x 1712. If I use an image of 3264 x 2448 then the logo is added at the correct start co ordinates but carries on the x and y axis. The logo should have a 10px gap between the sides. The 2 letters in the logo that you can see are also far larger than the source image logo. If I take the image that is doing the wrong behaviour (3264 x 2448) and change it to 2288 x 1712 and then run the code it outputs the correct result! I do not understand because the variable sourceImg is always the 1024 x 768 version so why should resizing the original image have an impact? Image sourceImg = Image.FromFile(Path.Combine(filepathfilename)); Image logo = Image.FromFile(watermark); Graphics g = Graphics.FromImage(sourceImg); g.DrawImage( logo sourceImg.Width - horizontalPosition - logo.Width sourceImg.Height - verticalPosition - logo.Height ); g.Dispose(); logo.Dispose(); sourceImg.Save(Path.Combine(filepath filename)); sourceImg.Dispose(); The overload of the DrawImage that you are using takes the PPI values of the images to calculate the size. As the PPI value of the logo is lower than the PPI value of the image the logo is scaled up so that their relative physical size (inches instead of pixels) are the same. Use an overload where you specify the size in pixels: g.DrawImage( logo sourceImg.Width - horizontalPosition - logo.Width sourceImg.Height - verticalPosition - logo.Height logo.Width logo.Height );
412,A,How to set line spacing Graphics.DrawString I arrive to output a string on multiple lines inside a retangle but haven't find a way to reduce or enlarge the line spacing. How to do that? This Microsoft forum posting may be helpful: http://forums.microsoft.com/MSDN/ShowPost.aspx?PostID=1507414&SiteID=1 This shows how MeasureString can be used to determine how much of your text will fit on each line then using this to progressively render the entire rectangle's contents line by line. Unfortunately I don't think there's a built-in line spacing property so you'll have to go for the manual approach. The post's author uses the font's Height * 1.5. It's also worth researching StringFormatFlags - you'll need to make sure both your DrawString and MeasureString calls use the same StringFormat so the rendering and measurement are consistent: http://msdn.microsoft.com/en-us/library/system.drawing.stringformatflags.aspx  This MSDN should help you. Line spacing is a result of the Font you are using. You may need to break your DrawString commands up into multiple calls if you need custom line spacing.
413,A,"A few x86 Assembly language questions I've just started using assembly language (felt like learning something new) and have run into a few questions (so far) that all the tutorials I've been looking through don't answer or are too old to know. 1) I've tried a few searches (maybe I just don't know the right keywords) but I can't find an updated list of graphics modes for changing screen resolutions etc. The best I've found is: Assembler Tutorial and I'd hardly think that 640x480 is the best resolution assembly language can use. Does anyone know of a more updated tutorial I can use? Edit: Interrupt 10h is old and doesn't quite support more than 640x480 2) Is it possible to ""mov"" a value from a variable to another variable without moving it to a register first? Example: jmp start n1 dw 0 n2 dw 0 res dw 0 start: mov n15 mov n26 mov resn1 add resn2 ...etc... Edit: It is not possible. You cannot go from memory to memory without using registers. 3) Going with question 1 is there a way to detect what graphics mode a user is currently using so that I can change it and change it back after? (I assume there is but am not sure how to do it.) Edit: Need to query OS for graphics settings. I'm not sure about 1 and 3 (though if they're BIOS calls that _is_ the best that you can do) 2 is not possible. There is no ""move this memory location to this memory location"" command; you have to use registers. Thanks Michael... I was going through the tutorial and thought I'd change the code to be slightly more efficient than shown. It was working well till I tried that and couldn't figure out if I had made a mistake or if it wasn't possible to start. Regarding your first question interrupt 10 is very old and likely not used beyond resolutions of 640x480. A different part of the software stack is now used; i.e. you would have to interrogate Windows to get the current screen resolution. Is there another interrupt that is newer than 10 but does something similar? I don't know if you would know but how does Windows do it? I can only assume that they would use assembly or a language that compiles to assembly... Hence why I figured Assembly could do it.  This rather verbose post contains a lot of details of how to use assembler to drive DirectX in Windows. DirectX is the key API family for graphics these days you won't come far using DOS-era interrupts and programming the VGA framebuffer directly.  Interrupt 10h is basically an operating system function call (actually it runs BIOS code). Internally it reads/writes video memory as well as various registers on the graphics card. To get an idea of what sort of stuff happens ""within"" interrupt 10h check this out. When you run a DOS program under Windows it is run in a virtual DOS machine. Windows doesn't actually let it touch the graphics card but lets it play with a virtual one. Usually this only extends as far as VGA screen modes (and sometimes only text mode) i.e. what you have is a virtual VGA card (not a modern graphics card). For this reason in 16-bit assembly language under Windows you just can't use the full capabilities of modern graphics cards. Yes sure assembly language can let you do anything the graphics card can do. But only if either: your program has unrestricted access to the graphics hardware (e.g. you're writing a Windows or Linux device driver or are executing in pure DOS or your own kernel) or your program goes through the appropriate operating system interface. If you're still interested in assembly language I would suggest you try to write a toy kernel. Doing this you'll learn a mountain of things. Leave a comment if you want further information. Of course it's difficult! But it's very rewarding if you enjoy solving tough challenges. One way to start is by putting some code in the boot sector of a floppy disk or flash drive and booting a computer from it. And slowly expand its functionality. Get yourself a copy of HelpPC (good reference for BIOS interrupts data formats and memory use) and check out http://en.wikibooks.org/wiki/X86_Assembly/Bootloaders Thanks for the input... I think I might take a look at playing with a kernel. Is it difficult for a beginner?  For questions #1 and #3 look into the VESA BIOS Extensions. This was something of a standard for dealing with ""Super VGA"" modes popular in the 90s. As for #2 in general the answer is no you can't MOV memory to memory. But it's not strictly true: there is MOVS (move string) which moves a byte word or dword from DS:SI to ES:DI. Usually this instruction is used in conjunction with a REP prefix to move blocks of memory. Also assuming you have a stack set up you can move memory-to-memory without clobbering a register by pushing and popping: PUSH [mem1] POP [mem2]"
414,A,BlackBerry - How to create a sub image from a larger image? I need to create a new smaller image from a larger image at runtime. The smaller image size is fixed (square) and represents a specific region of the larger image (the smaller image is a subset of the larger image). Image format doesn't matter. Thanks. You can use this function: Bitmap[] splitImage(Bitmap bitmap int rCnt int cCnt) { Bitmap[] result = new Bitmap[rCnt * cCnt]; int w = bitmap.getWidth() / cCnt; int h = bitmap.getHeight() / rCnt; for (int i = 0; i < rCnt; i++) for (int j = 0; j < cCnt; j++) { Bitmap bitmapPart = new Bitmap(w h); Graphics g = new Graphics(bitmapPart); g.drawBitmap(0 0 w h bitmap w * j h * i); result[i * cCnt + j] = bitmapPart; } return result; } Take a look at full source of Puzzle game for BB on Google Code You're welcome! Thanks for the answer Max. This is very helpful.
415,A,"Best way to stamp an image with another image to create a watermark in ASP.NET? Anyone know? Want to be able to on the fly stamp an image with another image as a watermark also to do large batches. Any type of existing library or a technique you know of would be great. I have had good luck with ImageMagick. It has an API for .NET too.  This will answer your question: http://www.codeproject.com/KB/GDI-plus/watermark.aspx Good luck! I was going to recommend using GDI plus as well good job. Good article shows adding text and image-based watermarks.  here is my full article: http://forums.asp.net/p/1323176/2634923.aspx use the SDK Command Prompt and navigate the active folder to the folder containing the below source code... then compile the code using  vbc.exe watermark.vb /t:exe /out:watermark.exe this will create an exe in the folder.. the exe accepts two parameters: ex.  watermark.exe ""c:\source folder"" ""c:\destination folder"" this will iterate through the parent folder and all subfolders. all found jpegs will be watermarked with the image you specify in the code and copied to the destination folder. The original image will stay untouched. // watermark.vb --  Imports System Imports System.Drawing Imports System.Drawing.Drawing2D Imports System.Drawing.Imaging Imports System.IO Namespace WatermarkManager Class Watermark Shared sourceDirectory As String = """" destinationDirectory As String = """" Overloads Shared Sub Main(ByVal args() As String) 'See if an argument was passed from the command line If args.Length = 2 Then sourceDirectory = args(0) destinationDirectory = args(1) ' make sure sourceFolder is legit If Directory.Exists(sourceDirectory) = False TerminateExe(""Invalid source folder. Folder does not exist."") Exit Sub End If ' try and create destination folder Try Directory.CreateDirectory(destinationDirectory) Catch TerminateExe(""Error creating destination folder. Invalid path cannot be created."") Exit Sub End Try ' start the magic CreateHierarchy(sourceDirectorydestinationDirectory) ElseIf args.Length = 1 If args(0) = ""/?"" DisplayHelp() Else TerminateExe(""expected: watermark.exe [source path] [destination path]"") End If Exit Sub Else TerminateExe(""expected: watermark.exe [source path] [destination path]"") Exit Sub End If TerminateExe() End Sub Shared Sub CreateHierarchy(ByVal sourceDirectory As String ByVal destinationDirectory As String) Dim tmpSourceDirectory As String = sourceDirectory ' copy directory hierarchy to destination folder For Each Item As String In Directory.GetDirectories(sourceDirectory) Directory.CreateDirectory(destinationDirectory + Item.SubString(Item.LastIndexOf(""\""))) If hasSubDirectories(Item) CreateSubDirectories(Item) End If Next ' reset destinationDirectory destinationDirectory = tmpSourceDirectory ' now that folder structure is set up let's iterate through files For Each Item As String In Directory.GetDirectories(sourceDirectory) SearchDirectory(Item) Next End Sub Shared Function hasSubDirectories(ByVal path As String) As Boolean Dim subdirs() As String = Directory.GetDirectories(path) If subdirs.Length > 0 Return True End If Return False End Function Shared Sub CheckFiles(ByVal path As String) For Each f As String In Directory.GetFiles(path) If f.SubString(f.Length-3).ToLower = ""jpg"" WatermarkImage(f) End If Next End Sub Shared Sub WatermarkImage(ByVal f As String) Dim img As System.Drawing.Image = System.Drawing.Image.FromFile(f) Dim graphic As Graphics Dim indexedImage As New Bitmap(img) graphic = Graphics.FromImage(indexedImage) graphic.DrawImage(img 0 0 img.Width img.Height) img = indexedImage graphic.SmoothingMode = SmoothingMode.AntiAlias graphic.InterpolationMode = InterpolationMode.HighQualityBicubic Dim x As Integer y As Integer Dim source As New Bitmap(""c:\watermark.png"") Dim logo As New Bitmap(source CInt(img.Width / 3) CInt(img.Width / 3)) source.Dispose() x = img.Width - logo.Width y = img.Height - logo.Height graphic.DrawImage(logo New Point(xy)) logo.Dispose() img.Save(destinationDirectory+f.SubString(f.LastIndexOf(""\"")) ImageFormat.Jpeg) indexedImage.Dispose() img.Dispose() graphic.Dispose() Console.WriteLine(""successfully watermarked "" + f.SubString(f.LastIndexOf(""\"")+1)) Console.WriteLine(""saved to: "" + vbCrLf + destinationDirectory + vbCrLf) End Sub Shared Sub SearchDirectory(ByVal path As String) destinationDirectory = destinationDirectory + path.SubString(path.LastIndexOf(""\"")) CheckFiles(path) For Each Item As String In Directory.GetDirectories(path) destinationDirectory += Item.SubString(Item.LastIndexOf(""\"")) CheckFiles(Item) If hasSubDirectories(Item) destinationDirectory = destinationDirectory.SubString(0destinationDirectory.LastIndexOf(""\"")) SearchDirectory(Item) destinationDirectory += Item.SubString(Item.LastIndexOf(""\"")) End If destinationDirectory = destinationDirectory.SubString(0destinationDirectory.LastIndexOf(""\"")) Next destinationDirectory = destinationDirectory.SubString(0destinationDirectory.LastIndexOf(""\"")) End Sub Shared Sub CreateSubDirectories(ByVal path As String) destinationDirectory = destinationDirectory + path.SubString(path.LastIndexOf(""\"")) For Each Item As String In Directory.GetDirectories(path) destinationDirectory += Item.SubString(Item.LastIndexOf(""\"")) Directory.CreateDirectory(destinationDirectory) Console.WriteLine(vbCrlf + ""created: "" + vbCrlf + destinationDirectory) If hasSubDirectories(Item) destinationDirectory = destinationDirectory.SubString(0destinationDirectory.LastIndexOf(""\"")) CreateSubDirectories(Item) destinationDirectory += Item.SubString(Item.LastIndexOf(""\"")) End If destinationDirectory = destinationDirectory.SubString(0destinationDirectory.LastIndexOf(""\"")) Next destinationDirectory = destinationDirectory.SubString(0destinationDirectory.LastIndexOf(""\"")) End Sub Shared Sub TerminateExe(ByVal Optional msg As String = """") If msg """" Console.WriteLine(vbCrLf + ""AN ERROR HAS OCCURRED //"" + vbCrLf + msg) End If Console.WriteLine(vbCrLf + ""Press [enter] to close..."") 'Console.Read() End Sub Shared Sub DisplayHelp() Console.WriteLine(""watermark.exe accepts two parameters:"" + vbCrLf + "" - [source folder]"") Console.WriteLine("" - [destination folder]"") Console.WriteLine(""ex."" + vbCrLf + ""watermark.exe """"c:\web_projects\dclr source"""" """"d:\new_dclr\copy1 dest"""""") Console.WriteLine(vbCrLf + ""Press [enter] to close..."") Console.Read() End Sub End Class End Namespace"
416,A,Real time dynamic shadows to complement deferred shading? I currently have a deferred rendering system setup and can render point lights and directional lights. My question is what are my options for different forms of shadowing which can make shadows based on point lights and directional lights which can maybe make use of a deferred shading setup? There's not really anything special about deferred rendering that requires unique shadowing techniques. Most of the standard approaches to producing shadows work just fine with deferred rendering schemes. Shadow mapping is the most common shadowing algorithm in use today in real time applications like games. Stencil shadows were popular a few years ago but have fallen out of favour a bit due to various limitations. Shadow mapping works fine with deferred shading. The basic algorithm for shadow mapping is reasonably straightforward: Render depth to a render target texture from the point of view of the light (you need to render 6 faces of a cube map for a point light). When you perform the lighting pass for the light for each pixel transform the world space position to the light's view space and compare the depth to the depth stored in the shadow map. If it is greater than the shadow map depth then the point is in shadow otherwise light it normally. As usual with rendering the devil is in the details. In order to get good quality shadows you need to set up your shadow map projection carefully to make best use of the pixels available in the shadow map (search for perspective shadow maps cascaded shadow maps). Due to precision issues you will get various artifacts with self shadowing surfaces ('shadow acne' is the most common one) and there are various strategies to reduce the incidence of these artifacts (depth bias projection matrix tweaks). In order to reduce the blocky pixelated look of shadow maps with insufficient resolution you will probably want to perform some shadow map filtering. Percentage Closer Filtering is the standard approach and there are various ways to go about performing PCF. You also face the standard quality/performance tradeoff with shadows and dynamic shadowing tends to be one of the most expensive aspects of scene rendering. There are numerous tricks and techniques to attempt to maximize the performance of shadow mapping. Recently there have been a number of variations on the basic shadow mapping algorithm that attempt to give better looking results for a given shadow map resolution by approaching the filtering problem differently. Variance Shadow Maps and Exponential Shadow Maps are two recent techniques that have some nice benefits but some unique problems of their own. Nvidia's developer site is a good resource for graphics programming in general and has lots of articles on shadowing. Take a look at the HTML versions of the GPU Gems books available on the site as well they have various articles on shadowing including the definitive article on Variance Shadow Maps in GPU Gems 3.
417,A,"Is there any graphics library in a higher level than OpenGL I am looking for a graphics library for 3D reconstruction research to develop my specific viewer based on some library. OpenGL seems in a low level and I have to remake the wheel everywhere. And I also tried VTK(visualization toolkit). However it seems too abstract that I need to master many conceptions before I start. Is there any other graphics library? I prefer to program in python. So I would like the library has a python wrapper. I think something like O3D would be better. But O3D is for javascript and it seems that Google already stops the development. @Xavier Ho:It seems very difficult to identify what sort of tools required. I just feel that PyOpenGL makes me consider a lot of low level detail which is annoying. Any opinions on the various python graphic libraries that have been developed? What requirements are you looking for? @James Black I only tried PyOpenGL and Python-VTK. The former is just a simple wrapper of COpenGL and the latter is over-complicated. I am looking for a library that is lightweight and object-oriented. Did you combine PyOpenGL with NumPy and/or SciPy? A lot of maths are already done there and what sort of tools are you looking at avoiding the wheel re-invention? It is. Eventually you'll have to keep to reuse the old code that wraps PyOpenGL otherwise you should probably look at PyProcessing. You could try mlab / Mayavi (a wrapper for VTK). There's some examples here: http://code.enthought.com/projects/mayavi/docs/development/html/mayavi/mlab.html Is it very documented? A big problem of VTK is the lack of good tutor and only have C++ documentation. The documentation looks OK. I haven't actually used mlab but I was looking at it a while ago. The function you are likely to be looking for is ""triangular mesh"" which is here: http://code.enthought.com/projects/mayavi/docs/development/html/mayavi/auto/mlab_helper_functions.html#enthought.mayavi.mlab.triangular_mesh  I have no personal experience with this but I have heard some decent things about Pyglet  Have you tried Pyglet with PyOpenGL? The two goes very well together. Wheaties' suggest is quite good as well although PyOgre also has a steep learning curve as it is indeed higher-level. On another thought there is also PyGame which is a Python wrapper of SDL. I personally prefer PyOpenGL and you can use WxPython or PyQT to create your rendering context. Also there is PyProcessing which is still in early stages but very very nifty. I have skimmed pyglet and it is really powerful.  Panda3D seems to be a nice 3D graphics library designed to be used in Python although it's mostly game oriented. I've browsed the manuals a few times and it's very polished and of a high quality it has even been used in some big studio's games (like Disney's Pirates of the Caribbean online if I remember well).  I used openGL with C++ a few years back - found it quite low level. I also have used Java3D which seemed to be a bit higher level. If you are not stuck on using python - try Java3D - very simple to get up and running."
418,A,"Selling commercial software for Mono If the Mono project is successful it will pave the way for commercial software on non-Windows platforms. I am interested in the prospect of writing and selling commercial software for the Mono platform along the lines of our existing Smoke Vector Graphics (OCaml) and F# for Visualization (.NET) products. Are any commercial library developers already building upon Mono and if so are they turning a profit from it? Also will it be feasible to write the software in Microsoft's F# language or will Mono have trouble with ILX? My figures speak against it we developed Qide 10 years ago and got 4 or so buys. We got at least a few hundred time more on Windows. The state of tools on Linux can just be named bad. Agreed you have wonderful things there but if you use GPLd software you will drown in their license stuff. There does exist one debugger really and one C compiler it gdb and gcc despite the efforts of Intel and if you come along into some less well known language you got nothing. Ever tried ProjectCenter (Objective C development environment)?  the debuggers are mostly clis and you have to type info reg to get info about registers. DDD works very funny it's one tools that while scrolling did not get that right you scroll up you have to scroll the mouse wheel down. It's also unbelievable slow to scroll it's just as if the BOFH wants to make a joke of you. Well I could argue about the even sader state on IBM AIX. What you have to pay to IBM is way beyond any reason... So maybe you're luckier than we are. But I'm mostly fed up with trying to earn money with ""application"" development on Linux. The best I can say is that Linux works well for setting up net infrastructure there you got decent payments but with programming tools forget it. Regards Well Piotr I wrote do it better and in fact I'm currently for some Mono stuff but that does not change anything in the simple fact that the state of development tools very fragmented and in many languages not existant. Feel free to proove me wrong. So give me a usable IDE for Erlang Ocaml Haskell or even for Mono itself on Linux.... I did So I encourage you to do better Mihai If that was really all that happened you should write it off as a failed business strategy - you clearly didn't identify your target market. very cute downvotes and interestingly now other answer to my last comment. Looks very trolling to me ;-( Seems that I had to delete my old comment to be able to create a new one. Old comment: This answer hardly seems relevant. First of all something that happened 10 years ago has no relation to what the industry looks right now. Secondly Mono is not a GPL software. Update: You should probably read the question again. It's about feasibility of selling commercial software for Mono. It's not about Erlang Ocaml or Haskell and it's not about Linux's readiness for mainstream adoption. About the IDE - have a good look at the latest version of MonoDevelop. If that is really that great. You can surely point me out to commercial Mono offers I know but one that is from Unitiymedia. But I guess you have tons of other examples.  Linux people are notoriously thrifty so I'd consider the ROI. Do you really want to spend your resources to target a group that has less than 10% market adoption and out of that 10% only 1% would be interested in your product and only %0.01 percent would pay for it? Well no one uses mono because there's really not a whole lot of need yet. If there were a need more people would use it. We're already making as much money from books and consultancy around Linux as we are from Windows in total. 10% of the market... that's a lot of people enough to make you a millionaire ;) If they were going for mass market appeal I doubt they'd be writing toolkits for OCaml and F#  Mono is a perfectly valid platform for running commercial software as a lot of companies have already proven. Some of them you can see here but there is a lot of which you will never hear about as they are running Mono in embedded environments (Sandisk Salsa mp3 player). From the latest news Electronic Arts is going to use Mono for Sims3. How is that for an argument? One of the main points of Mono is minimizing the effort for developers coming from Windows to Linux. In most cases no additional effort is required to make the same software that you already have on Windows run on Linux MacOS and other platforms in Mono. Just to clarify some things that other people answering your question conveniently forgot about. Most of Mono (recently even the compiler) is licensed under MIT/X11 license which allows you to pretty much deploy it in under and conditions you see fit. There is no GPL ""cancer"" that some people seem to be so afraid of. Personally I have been playing with F# and Gtk# in Mono and I loved the experience. More about it here. This was possible due to the fact that the F# team has made sure that F# can run on Mono and they provided a simple Linux installer in their release. This should also be a signal that Mono is regarded as serious alternative to .Net even by Microsoft.  The Mono project lists a number of successful commercial projects here and I would particularly point out Unity as being one of the more notable ones."
419,A,"Blend mode on a transparent and semi transparent background In general the ""normal"" blend mode equation looks like this: D = Sa * S + D * (1.0 - Sa) where D is destination color Sa is source alpha and S is source color. Now this works fine with fully opaque destination but I'd like to know how you would handle that with semi and fully transparent destination. When blending the source over a fully transparent destination the source pixel (a pixel being color and alpha) will be unchanged and not blended like in the equation before and if the destination background is fully opaque the above equation should be applied but I can't find a nice way to handle the situations where destination alpha is in between 0 and 1. For example if you blend a white pixel with 50% alpha on a transparent background the color should not tend to that transparent color value (which is more or less in an undefined state) the destination color should be full white and not 50% (after alpha multiplication) which is what you get after applying the above equation (if D is made the same color as S which was something I thought of). Most blending formulas are used on static images where the destination colors alpha is not stored. If the alpha is available then the final alpha is simply going to be the arithmetic mean of the source alpha and the destination alpha. You can calculate the average alpha and then simply use that in the place of 'Sa' in your formula. Mind telling us what this is for? Calculating the destination alpha is not a problem it's the color that pose a problem. It's to blend 1 semi transparent image over a second semi transparent image and save the result into a PNG file with transparency. In photoshop I can do that by loading the two images merging the two layers and save to a PNG. I still don't see why averaging the alpha channels wouldn't produce the proper color?  This equation is a simplification of the general blending equation. It assumes the destination color is opaque and therefore drops the destination color's alpha term. D = C1 * C1a + C2 * C2a * (1 - C1a) where D is the resultant color C1 is the color of the first element C1a is the alpha of the first element C2 is the second element color C2a is the alpha of the second element. The destination alpha is calculated with: Da = C1a + C2a * (1 - C1a) The resultant color is premultiplied with the alpha. To restore the color to the unmultiplied values just divide by Da the resultant alpha. This does not work. For example if the destination color is black and the source color is white. Destination alpha is 0 and source alpha is 0.5 then you end up with D = 1.0 * 0.5 + 0.0 * 0.0 * (1.0 - 0.5) which is 1.0 * 0.5 with is alike multiplying the source color with it's alpha. This is not what I want I'd like the source color to be untouched if the destination alpha is 0. This is how this blending mode works. You'll want a different blending mode if you want different behavior. There are oodles of other blending modes. There's a good list at http://illusions.hu/effectwiki/doku.php?id=list_of_blendings Also the resulting color is premultiplied by the alpha. You can revert to the unmulitiplied colors by dividing the resulting colors by the resulting alpha. Which as I think further about this is what I think you want. Ho yea me stupid you are right the solution is simply to divide by the resulting alpha at the end. Thanks."
420,A,"Drawing and clearing the desktop canvas with Delphi I'm trying to draw over the whole screen by using the desktop canvas and painting to it directly. The problem is I can't clear that desktop canvas. I've tried setting the canvas.pen.style to psNotXOR and draw over the old image but unfortunately this is not reliable enough and some left overs are still present in some conditions. My need is to draw a selection rectangle around a window / control when the mouse is over it. I've done some stuff like this in the past and I've never come up with an acceptable solution. Having a think about it though you could send the desktop HWND an invalidate command. Because the desktop is a modified listview control you should able to use something like procedure InvalidateDesktop; begin RedrawWindow(GetDesktopWindow 0 0 RDW_INVALIDATE); //if that doesn't work then try this: //Sendmessage(GetDesktopWindow wm_setredraw 1 0); end; That might do what you want I haven't tested it as I've just knocked it up for the example. The visible listview is just a child of the `HWND` returned by `GetDesktopWindow()` so that's probably not going to work as-is. I did say I hadn't tested it. As mghie anticipated this does not work unfortunately. Thanks for the answer though Ryan.  This is how we do it in our app:  FBitmap.Canvas.Brush.Color := clWhite; FBitmap.Canvas.FillRect(FBitmap.Canvas.ClipRect); Ah gotcha. Didn't read the question properly. -1 that's not the ""clearing"" the OP asked for. Instead it justs paints a white rectangle.  the problem is that Windows won't return me the control behind the mouse I think you need to hook the mouse event messages for this - the hooked message then gives you the window handle that the mouse is over. Look up SetWindowsHookEx(WH_MOUSE) and MOUSEHOOKSTRUCT. I'm already using a global mouse hook to retrieve the window handle but when I create a top level window even if it is transparent Windows always selects its handle :(  You don't write on what OS you have problems with the artefacts after clearing. At least with desktop composition activated it is a very bad idea to draw directly to the desktop and to do XOR painting (see ""Drawing To and Reading From the Screen -- Baaaad!"" in this blog post). Apart from the negative performance implications you can't be sure what other painting happens at the same time and what effects and animations alter the displayed content so a simple XOR may not be enough to completely erase everything. One possible way to implement it would be a transparent overlay window of desktop size and to draw your rubber band selector over that. Invalidating the whole window if the size changes should be enough no need to erase the old selection line. If the overlay is removed the line will be gone too. Desktop composition will make sure that no flicker occurs. However switching applications while selecting an area will be problematic you need to catch this and immediately cancel the selection. Edit: I just tested it with Delphi 2009 and with the following test app: a form with FormStyle set to fsStayOnTop and with AlphaBlend set to True with an overridden CreateParams() method to add the WS_EX_TRANSPARENT extended style flag I can pass all mouse clicks through to the underlying windows while being able to draw into a window on top of them. This should get you started. Well selecting doesn't work and the top level window is always the one behind the mouse. Sorry don't understand. Do you start selection by just left clicking the mouse on the desktop and start dragging it? If so how do you catch this with a global (low level) mouse hook? Or do you create the overlay (when in a special selection mode) and start selection with a mouse click on it? Maybe you can expand your question with some more detail. Note that you can make transparent windows ""click-through"" which is probably what you need to do. Google for it there was a DiLascia article some years ago on it. I thought about it but the problem is that Windows won't return me the control behind the mouse then. Or will it ? Come to think about it if the canvas is transparent this might do the trick. I'll try that. Thank you for the very detailed and helpful answer. This is exactly what I needed. Thank you for the very helpful answers."
421,A,Defining Light Coordinates I took a Computer Graphics exam a couple of days ago which had extra credit question like the following: A light can be defined in one of two ways. It can be defined in world coordinates e.g. a street light or in the viewer (eye coordinates) e.g. a head-lamp worn by a miner. In either case the viewpoint can freely change. Describe how the light should be transformed different in these two cases. Since I won't get to see the results of this until after spring break I thought I would ask here. It seems like the analogies being used are misleading - could you not define a light source that is located at the viewers eye in world coordinates just as well as you could in eye coordinates? I've been doing some research on how OpenGL handles light and it seems as though it always uses eye coordinates - the ModelView matrix would be applied to any light in world coordinates. In that case the answer may just be that you would have to transform light defined in world coordinates into eye coordinates using something like the ModelView matrix while light defined in eye coordinates would only need to be transformed by the projection matrix. Then again I could be totally under thinking (or over thinking this). Another thought I had is that it determines which way you render shadows but that has more to do with the location of the light and its type (point directional emission etc) than what coordinates it is represented in. Any ideas? The light's position is transformed by the modelview matrix that was active at the moment the light was defined. If the modelview matrix was identity at that point you'll get the light in eye coordinates. If the modelview matrix was the inverse of your camera matrix you'll get the light in world space. It's too bad you can't accept two answers since it was really the combination of both. Don't worry about it. You're welcome.  In the case of the street light the world coordinates will stay constant when the viewpoint moves. In the case of the head-lamp the eye coordinates will stay constant when the viewpoint moves.
422,A,"How does an unsharp mask work? I've been playing around with image processing lately and I'd like to know how the unsharp mask algorithm works. I'm looking at the source code for Gimp and it's implementation but so far I'm still in the dark about how it actually works. I need to implement it for a project I'm working on but I'd like to actually understand the algorithm I'm using. Has anyone managed to replicate Photoshop's Unsharp Mask? Unsharp Mask works by generating a blurred version of the image using a Gaussian blur filter and then subtracting this from the original image (with some weighting value applied) i.e. blurred_image = blur(input_image) output_image = input_image - blurred_image * weight  The key is the idea of spatial frequency. A Gaussian filter passes only low spatial frequencies so if you do something like: 2*(original image) - (gaussian filtered image) Then it's effect in the spacial frequency domain is: (2 * all frequencies) - (low frequencies) = (2 * high frequencies) + (1 * low frequencies). So in effect an 'unsharp mask' is boosting the high frequency components of the image --- the exact parameters of the gaussian filter size and the weights when the images are subtracted determine the exact properties of the filter. For more information have a read of ~page 70 of this document.  I wasn't sure how it worked either but came across a couple of really good pages for understanding it. Basically it goes like this: What's the opposite of a sharpened image? A blurry one. We know how to blur an image. Duplicate the original image and perform some Gaussian blurring. This is the Radius slider on most USM dialogs. Well if we subtract away the blurriness we should be left with the parts that are high-contrast! Think about it: if you blur a sky it still looks like a sky. Subtract the pixels and you get sky - sky = 0. If you blur a Coke logo you get a blurry Coke logo. Subtract it and you're left with the edges. So do the difference Well what makes things look sharper? Contrast. Duplicate the original image again and increase the contrast. The amount by which you increase the contrast is the Amount or Intensity slider on most USM dialogs. Finally put it all together. You have three things at this point: A high contrast version of your original image The difference of the blurred image and your original (this layer is mostly black). This layer is the unsharp mask The original The algorithm goes like this: Look at a pixel from the unsharp mask and find out its luminosity (brightness). If the luminosity is 100% use the value from the high-contrast image for this pixel. If it is 0% use the value from the original image for this pixel. If it's somewhere in-between mix the two pixels' values using some weighting. Optionally only change the value of the pixel if it changes by more than a certain amount (this is the Threshold slider on most USM dialogs). Put it all together and you've got your image! Here's some pseudocode: color[][] usm(color[][] original int radius int amountPercent int threshold) { // copy original for our return value color[][] retval = copy(original); // create the blurred copy color[][] blurred = gaussianBlur(original radius); // subtract blurred from original pixel-by-pixel to make unsharp mask color[][] unsharpMask = difference(original blurred); color[][] highContrast = increaseContrast(original amountPercent); // assuming row-major ordering for(int row = 0; row < original.length; row++) { for(int col = 0; col < original[row].length; col++) { color origColor = original[row][col]; color contrastColor = highContrast[row][col]; color difference = contrastColor - origColor; float percent = luminanceAsPercent(unsharpMask[row][col]); color delta = difference * percent; if(abs(delta) > threshold) retval[row][col] += delta; } } return retval; } Note: I'm no graphics expert but this is what I was able to learn from the pages I found. Read them yourself and make sure you agree with my findings but implementing the above should be simple enough so give it a shot! References Sharpening Using an Unsharp Mask Understanding the Digital Unsharp Mask 3.7 Unsharp Mask Unsharp masking So of course the non-graphics expert (me) would think of the long imperatively programmed way of doing this. It turns out that maybe a sort of inverse Gaussian blur convolution would work (http://photo.net/bboard/q-and-a-fetch-msg.tcl?msg_id=000Qi5) though I'm not sure that their convolution matrix is identical to my description (mine more closely follows the film process described on Wikipedia). See also: http://photo.net/bboard/q-and-a-fetch-msg?msg_id=001Mod Have a look here: http://stackoverflow.com/questions/13296645/opencv-and-unsharp-masking-like-adobe-photoshop/23322820#23322820  Unsharp is usually implemented as a convolution kernel which detects edges. The result of this convolution is added back in to the original image to increase edge contrast which adds the illusion of additional ""sharpness"". The exact kernel used varies quite a bit from person-to-person and application-to-application. Most of them have this general format:  -1 -1 -1 g = -1 8 -1 -1 -1 -1 Some leave the diagonals out sometimes you get higher weighs and the whole kernel is scaled and some just try different weights. They all have the same effect it in the end it's just a question of playing until you find one that you like the end result of. Given an input image I the output is defined as: out = I + c(I * g) where * is the 2D convolution operator and c is some scaling constant usually above 0.5 and less than 1 so you avoid blowing out any more channels than you have to."
423,A,Flex 2D graphics learning path for Java/GWT developer? I have been developing with Java for several years and last six months I have been building GWT based application. But I have almost no Flash or Action Script experience and I would like to try out some hobby programming with Flex. Especially 2D graphics and image manipulation would be interesting topics. Adobe's tutorials for Java developers are naturally the starting point but I would need some pointers for graphics. Something like drawing 2D graphics based on data from the server or modifying image uploaded by user. How much can be done on the Flex and what must be done on the server side? For drawing you don't need the 'full' flex library flex will mostly provide you with a nice set of GUI components and some ways to integrate with your back-end server easily. If you just want to draw something check out the Graphics object http://livedocs.adobe.com/flex/3/langref/flash/display/Graphics.html - it works similar to the java Graphics object. If you want to modify an image uploaded by the user you are of course already at the server (because the image was uploaded) - however the new flash player (version 10) allows some manipulations of local data as well so it might even be possible to show a modified image that was not even uploaded see http://www.mikechambers.com/blog/2008/08/20/reading-and-writing-local-files-in-flash-player-10/ for example. Flex/AS3 will feel a bit weird to a java programmer - parts of it are very easy and other parts will be frustrating tooling support is worse slow compiler no support for running unittests without jumping through lots of hoops (don't get me started...). But as a deployment platform I must say I like it a lot.
424,A,"Animated GIF in SWT table/tree viewer cell http://www.java2s.com/Code/Java/SWT-JFace-Eclipse/DisplayananimatedGIF.htm describes how to display an animated GIF in SWT - in general. While the code works and is easily comprehensible I'm facing serious issues displaying an animated GIF in a SWT/JFace table/tree viewer cell with that technique. -> all code below Essentially I implemented my own OwnerDrawLabelProvider which creates an ImageLoader in paint(Event Object) and starts an animation thread. The problem seems to be that this animation thread is not the UI thread and I don't know which GC or Display instance to use in its run() method. I tried creating a separate GC instance in the thread's constructor - derived from event.gc - but the thread fails writing to that GC as soon as I step out of the debugger...  Sat Jan 9 22:11:57 192.168.1.6.local.home java[25387] : CGContextConcatCTM: invalid context 0x0 2010-01-09 22:12:18.356 java[25387:17b03] It does not make sense to draw an image when [NSGraphicsContext currentContext] is nil. This is a programming error. Break on _NSWarnForDrawingImageWithNoCurrentContext to debug. This will be logged only once. This may break in the future. Sat Jan 9 22:12:41 192.168.1.6.local.home java[25387] : CGContextConcatCTM: invalid context 0x0 How do I need to handle this situation? Below are the relevant code sections:  /* Called by paint(Event Object). */ private void paintAnimated(final Event event final ImageLoader imageLoader) { if (imageLoader == null || ArrayUtils.isEmpty(imageLoader.data)) { return; } final Thread animateThread = new AnimationThread(event imageLoader); animateThread.setDaemon(true); animateThread.start(); } private class AnimationThread extends Thread { private Display display; private GC gc; private ImageLoader imageLoader; private Color background; public AnimationThread(final Event event final ImageLoader imageLoader) { super(""Animation""); this.display = event.display; /* * If we were to simply reference event.gc it would be reset/empty by the time it's being used * in run(). */ this.gc = new GC(event.gc.getDevice()); this.imageLoader = imageLoader; this.background = getBackground(event.item event.index); } @Override public void run() { /* * Create an off-screen image to draw on and fill it with the shell background. */ final Image offScreenImage = new Image(this.display this.imageLoader.logicalScreenWidth this.imageLoader.logicalScreenHeight); final GC offScreenImageGC = new GC(offScreenImage); offScreenImageGC.setBackground(this.background); offScreenImageGC.fillRectangle(0 0 this.imageLoader.logicalScreenWidth this.imageLoader.logicalScreenHeight); Image image = null; try { /* Create the first image and draw it on the off-screen image. */ int imageDataIndex = 0; ImageData imageData = this.imageLoader.data[imageDataIndex]; image = new Image(this.display imageData); offScreenImageGC.drawImage(image 0 0 imageData.width imageData.height imageData.x imageData.y imageData.width imageData.height); /* * Now loop through the images creating and drawing each one on the off-screen image before * drawing it on the shell. */ int repeatCount = this.imageLoader.repeatCount; while (this.imageLoader.repeatCount == 0 || repeatCount > 0) { switch (imageData.disposalMethod) { case SWT.DM_FILL_BACKGROUND: /* Fill with the background color before drawing. */ offScreenImageGC.setBackground(this.background); offScreenImageGC.fillRectangle(imageData.x imageData.y imageData.width imageData.height); break; case SWT.DM_FILL_PREVIOUS: // Restore the previous image before drawing. offScreenImageGC.drawImage(image 0 0 imageData.width imageData.height imageData.x imageData.y imageData.width imageData.height); break; } imageDataIndex = (imageDataIndex + 1) % this.imageLoader.data.length; imageData = this.imageLoader.data[imageDataIndex]; image.dispose(); image = new Image(this.display imageData); offScreenImageGC.drawImage(image 0 0 imageData.width imageData.height imageData.x imageData.y imageData.width imageData.height); // Draw the off-screen image. this.gc.drawImage(offScreenImage 0 0); /* * Sleeps for the specified delay time (adding commonly-used slow-down fudge factors). */ try { int ms = imageData.delayTime * 10; if (ms I posted the same problem to the SWT newsgroup http://www.eclipse.org/forums/index.php?t=tree&th=160398 Can't you let a LabelProvider return different images and then call viewer.update(...) on the elements you want to animate. You can use Display.timerExec to get a callback instead of having a separate thread. See my answer here for how you can change colors. You should be able to do something similar with images. You can just get the Display from the control. GC gc = new GC(myTable.getDisplay()). Or if you don't need to resize them. ImageData[] imageDatas = new ImageLoader().load(new FileInputStream(""myAnimated.gif"")); Image[] images = new Image[imageDatas.length]; for(int n=0;nAfter many hours of frustrating trial-and-error a co-worker came up with a feasible solution. My initial approaches to have this implemented in a totally self-contained LabelProvider failed miserably. One approach that didn't work was to override LabelProvider#update() and to call timerExec(100 new Runnable() {...viewer.update()... from within that method. The ""life""-cycle of that is hard to control and it uses too many CPU cycles (10% on my MacBook). One of the colleague's ideas was to implement a custom TableEditor: a label with an image (one frame of the animated GIF) but no text. Each TableEditor instance would start its own thread in which it updates the label's image. This works quite well but there's a separate ""animation"" thread for each animated icon. Also this was a performance killer consumed 25% CPU on my MacBook. The final approach has three building blocks an OwnerDrawLabelProvider which paints either a static image or the frame of an animated GIF an animation thread (the pace maker) it calls redraw() for the column which contains the animated GIFs and it also calls update() and the viewer's content provider that controls the animation thread. Details in my blog http://www.frightanic.com/2010/02/09/animated-gif-in-swt-tabletree-viewer-cell/."
425,A,"Encode complex number as RGB pixel and back How is it better to encode a complex number into RGB pixel and vice versa? Probably (logarithm of) an absolute value goes to brightness and an argument goes to hue. Desaturated pixes should receive randomized argument in reverse transformation.  Something like: 0 -> (000) 1 -> (25500) -1 -> (0255255) 0.5 -> (12800) i -> (2552550) -i -> (2550255) (000) -> 0 (255255255) -> e^(i * random) (128128128) -> 0.5 * e^(i *random) (0128128) -> -0.5 Are there ready-made formulas for that? Edit: Looks like I just need to convert RGB to HSB and back. Edit 2: Existing RGB -> HSV converter fragment:  if (hsv.sat == 0) { hsv.hue = 0; // ! return hsv; } I don't want 0. I want random. And not just if hsv.sat==0 but if it is lower that it should be (""should be"" means maximum saturation saturation that is after transformation from complex number). I was reading something on this very topic earlier this week but I can't find it now - you're on the right track though - magnitude -> brightness phase -> hue. Use HSV model and then convert to RGB. Yes I'll use HSV. First a couple of comments on your question: 1+i -> (5102550) ? 'brightness' and 'hue' are usually components of an HSV colour model rather than RGB Perhaps you could use polar form of complex numbers and map them to a colour wheel such as HSV with V fixed ? Since the magnitude of a complex number can be infinite you might have trouble mapping them to any colour model all of which (well all the ones I know of that is) are bounded. Perhaps you could use a non-Euclidean geometry for mapping complex numbers into the unit circle. But to answer your question I've never come across any ready-made solution to your problem. We wait to learn from SO. Magnitude is limited (logarithm is cropped to upper or lower bound if exceed). Yes I use polar form. I was looking to RGB-HSV-RGB converter (probably already found it). I only need to know how much random part should I add in the reverse transformation. Well if you'd asked for what you wanted you'd have got more useful answers.  You will have many problems: |RGB|=2^24 and |C|=? Supposing your complex numbers set is finite and discrete i think the best method for RGB System is like this: Cartesian Axis: Polar Axis: i think it will be unique and a little semantic: something like counter grids and coloring of geographical maps and medical images analysis. you can use HSL system and polar axis instead: what you want to do with this encoding? I'm converting sound to picture and back. Sound is split to the intersecting fragments each of them FFTed resulting numbers are logarithmed. Grayscale variant already works and sounds (argument is just discarded at conversion to the picture and randomized at conversion from picture). It works but sounds blurred. I want to save argument information in colour of pixes as both are circular. @ Vi: are you trying to implement a sound graph? i'm nut sure but this could help: http://faculty.washington.edu/dillon/PhonResources/javoice/vowjavoice2.html read it's algorithm. @ Vi: which encoding structure you are converting to images? What is ""encoding structure""? Current chain is Sound -> Chunks -(FFT)-> Complex number arrays -> Image rows -> Image. I just want to add colour to the image (and argument to complex number arrays).  It seems like the OP had a different question in mind... but anyway here is one place this is incorporated. It is GPL code so you can use it if you want. Sage's complex_to_rgb function does this with Numpy and Python. Mathematica has such complex plots (Sage's were inspired by them in fact) so presumably it has something like this internally as well though perhaps not code."
426,A,"Good reasons NOT to design websites with a vector graphics application? All the (web)designers I've known or worked with used photoshop almost exclusively and yet now that I wanna try my hand at web-design for a personal project I feel compelled to use a vector graphics application (namely Inkscape). The idea of the actual design process just seems more rational with VG given that I'll be experimenting with laying out and composing graphical 'objects' it seems to make more sense to be able to act upon these as 'wholes' rather than having to deal with the pixels that make them up. HTML5 and svg seem like another big upside but I'm worried I may be looking at this the wrong way due to lack of experience since I haven't seen anyone designing like this and resources on the matter are scarce. So are there any reasons why this would be a bad idea ? no I've designed sites in Illustrator before. And Photoshop has lots of vector features integrated. The only thing your really need to keep in mind is color palette. And how you will be exporting your files/slices to use in your site. In the end you are exporting to pixel based images.  Not at all. I use Illustrator all of the time for designing websites. It even has slicing features built-in & ""save for web"" to optimize file sizes. Colleagues of mine with design backgrounds tend to prefer Illustrator. Those who are technically focused tend to use Photoshop. Some people use both. That's just been my experience. Regardless you can use anything you like. SVG hasn't quite caught on like I wanted. Pure vector sites are tough without relying on Flash or Silverlight. I sometimes do hybrid sites but never with SVG. The support has never been great enough for me to feel comfortable. Eventually this will change. It took years for CSS to catch on and gain adequate support. I expect the same will be true for HTML5 & SVG. indeed svg support is still lacking but the promises are exciting ! have you had any experience with svgweb by any chance ? no i haven't. thanks for mentioning it. i'll definitely be giving it a look. :) your ever played with Google's O3D? add an extra dimension and it's quite similar."
427,A,Low-hanging graphics programming fruits? I'm currently working on a tile-based game in Java2D and I was thinking of adding some cheap eye candy. For example implementing a simple particle system (maybe something like this) for explosions and/or smoke. Do you have any suggestion for relatively easy to program effects that wouldn't require drawing a lot (or at all) of new art? Tutorials and code samples for said effects would also be most welcome! -Ido. PS - if absolutely necessary I could switch to something like LWJGL/JOGL or even Slick - but I rather stay with Java2D. Implementing blurs and other image filtering effects are fairly simple to perform. For example to perform a blur on a BufferedImage one can use the ConvolveOp with a convolution matrix specified in a Kernel: BufferedImageOp op = new ConvolveOp(new Kernel(3 3 new float[] { 1/9f 1/9f 1/9f 1/9f 1/9f 1/9f 1/9f 1/9f 1/9f } )); BufferedImage resultImg = op.filter(originalImg resultImage); Not quite sure when a blur effect is needed but it may come in handy some time. But I'd say it's a low-hanging fruit for its ease of implementation. Here's some information on convolution matrices. It can be used to implement effects such as sharpen emboss edge enhance as well. Motion blur can be made to look really nice in 2D and its very simple to achieve. Thanks I'll use that! Any more effects like that you know of?  Transparency effects (e.g. for smoke) can make a big difference without too much effort. No idea if this can be done in Java2d though.  Filthy Rich Clients describes in great detail a number of very nice Java2D/Swing effects. It also gives an excellent theoretical background to these effects. I'm not sure how much low-hanging fruit there is but it's a great resource for browsing. One possibility might be to do something with alpha compositing. Maybe combine alpha composite with Timing Framework. Depending on the rules of your game it might even be important to gameplay to selectively and time-dependently make objects semi-transparent.  Anything that looks kind of realistic things bouncing off of other things rolling off etc. can be kind of cool and if your game is a side scrolling 2D rather than top-down 2D you might be able to use a ready made physics engine like Box2D to do something cool with very little effort. Here's a Java port of Box2D that you could use for it. I used Phys2D for stuff like that before. Sadly it's a top-down game. Ah well it was worth mentioning. In that case I would definitely go the particle route. But ONLY after the game is finished enough to be playable. Getting it in people's hands to play is always your highest priority.  Performing a pixelation effect is a low-hanging fruit operation on a BufferedImage. This can be performed in two steps: Determine the color of the one block of the pixelation. Fill in the block of pixelation on the image. Step 1: Determine the color: public static Color determineColor(BufferedImage img int x int y int w int h) { int cx = x + (int)(w / 2); int cy = y + (int)(h / 2); return new Color(img.getRGB(cx cy) true); } In the determineColor method the pixel color from the center of the BufferedImage is determined and is passed back to the caller. Step 2: Fill in the pixelation block with determined color: BufferedImage sourceImg = ...; // Source Image. BufferedImage destimg = ...; // Destination Image. Graphics g = destImg.createGraphics(); int blockSize = 8; for (int i = 0; i < sourceImg.getWidth(); i += blockSize) { for (int j = 0; j < sourceImg.getHeight(); j += blockSize) { Color c = determineColor(sourceImg i j blockSize blockSize); g.setColor(c); g.fillRect(i j blockSize blockSize); } } g.dispose(); Although there is quite a bit of code this effect is intellectually a low-hanging fruit -- there isn't much complex that is going on. It is basically finding the center color of a block and filling a box with that color. This is a fairly naive implementation so there may be better ways to do it. The following is a before and after comparison of performing the above pixelation effect:
428,A,"White lines around round border at high zoom level If I zoom large enough in xamlpadx (500%) I see while lines around the inside of the border. In my app this even happens at normal zoom level in some controls. Why is that and more important what can I do about it? This is my XAML code: <Border Background=""#000000"" BorderBrush=""#000000"" BorderThickness=""4"" CornerRadius=""4""> <StackPanel> </StackPanel> </Border> I removed everything until I was sure that no padding/margin/border is involved in the flawed display. EDIT 2: I shouldn't have accepted the answer so soon. I noticed that in same cases the error is as before although I used SnapsToDevicePixels and set the interior color of the control to the same color as the border. I hope microsoft will fix this soon! http://blog.pixelingene.com/?p=526 has an alternative workaround using background to emulate the border. I have the same behavior and I think that it is an error of the WPF rendering engine. You get slightly better results by adding SnapsToDevicePixels=""True"" to the Border: the white lines remain only in the rounded corners.  This indeed seems a bug in WPF. However for your specific case you can use two workarounds: <Border Background=""#000000"" BorderThickness=""0"" CornerRadius=""4""> <StackPanel Margin=""4""> </StackPanel> </Border> Or <Border Background=""#000000"" BorderThickness=""0"" CornerRadius=""4""> <Border Background=""#000000"" BorderThickness=""0"" CornerRadius=""4"" Margin=""4""> <StackPanel> </StackPanel> </Border> </Border> This didn't help either  This behavior is inherent in the way WPF works. It's not a bug. Note that your coordinates are all doubles not integers: These are not precise units. If you zoom in enough you will see errors. The following is from memory from about a year ago but I think it's right: If you're drawing to the wpf rendering context by hand you can use the 'guidelines' system to make things line up. This is how the buttons and things are drawn for the stock UI styles. Take a look at the reference source (http://blogs.msdn.com/rscc/default.aspx) and you can see for yourself. There's also a good msdn page on this subject in general: http://msdn.microsoft.com/en-us/library/aa970908.aspx I have no idea. Very curious. But where can white lines come from even if the background of the control is also black?"
429,A,Java tiled paint I'm writing a game and I want an area of the screen to have a tiled paint drawn on it. Using a TexturePaint java supports texturing a Shape with a tiled texture this is really simple and performs pretty well. However I want my tiled texture to be rotated before drawing it as a fill to the shape - this is theoretically possible by subclassing TexturePaint and applying a transform However this performs really badly. Does java in some way support doing this? If not is there any reasn that subclassing texturePaint might perform really badly? Unfortunately it's a university game development project which means that we simply don't have time to learn any new APIs and of course the only API the entire team knows is the java2D one. :( What platform are you developing for? If it's one that supports OpenGL (or any variant of it) I would recommend using that. The Java2D API is not designed for games and this will probably not be the only performance problem you encounter. How does your current implementation look like? We ended up hand tiling the texture tiles to make one really massive tile and then drawing that rotated. Reducing the number of tiles vastly improves the performance Your best off just simply rotating the BufferedImage before throwing it at TexturePaint. AffineTransform texture = new AffineTransform(); texture.rotate(radians bufferedImage.getWidth()/2 bufferedImage.getHeight()/2); AffineTransformOp op = new AffineTransformOp(texture AffineTransformOp.TYPE_BILINEAR); bufferedImage = op.filter(bufferedImage null); You should be able to subclass TexturePaint without recieving a preformance drop I can only assume that your rotation code is causing the drop. I hope this helps.
430,A,"Algorithm for Polygon Image Fill I want an efficient algorithm to fill polygon with an Image I want to fill an Image into Trapezoid. currently I am doing it in two steps 1) First Perform StretchBlt on Image 2) Perform Column by Column vertical StretchBlt Is there any better method to implement this? Is there any Generic and Fast algorithm which can fill any polygon? Thanks Sunny Are you trying to distort the image to the shape of the polygon or simply clip the image to the shape of the polygon? Hi Michael I want to distort the image to the shape. I can't help you with the distortion part but filling polygons is pretty simple especially if they are convex. For each Y scan line have a table indexed by Y containing a minX and maxX. For each edge run a DDA line-drawing algorithm and use it to fill in the table entries. For each Y line now you have a minX and maxX so you can just fill that segment of the scan line. The hard part is a mental trick - do not think of coordinates as specifying pixels. Think of coordinates as lying between the pixels. In other words if you have a rectangle going from point 00 to point 22 it should light up 4 pixels not 9. Most problems with polygon-filling revolve around this issue. ADDED: OK it sounds like what you're really asking is how to stretch the image to a non-rectangular shape (but trapezoidal). I would do it in terms of parameters s and t going from 0 to 1. In other words a location in the original rectangle is (x + w0*s y + h0*t). Then define a function such that s and t also map to positions in the trapezoid such as ((x+t*a) + w0*s*(t-1) + w1*s*t y + h1*t). This defines a coordinate mapping between the two shapes. Then just scan x and y converting to s and t and mapping points from one to the other. You probably want to have a little smoothing filter rather than a direct copy. ADDED to try to give a better explanation: I'm supposing both your rectangle and trapezoid have top and bottom edges parallel with the X axis. The lower-left corner of the rectangle is <x0y0> and the lower-left corner of the trapezoid is <x1y1>. I assume the rectangle's width and height are <wh>. For the trapezoid I assume it has height h1 and that it's lower width is w0 while it's upper width is w1. I assume it's left edge ""slants"" by a distance a so that the position of its upper-left corner is <x1+a y1+h1>. Now suppose you iterate <xy> over the rectangle. At each point compute s = (x-x0)/w and t = (y-y0)/h which are both in the range 0 to 1. (I'll let you figure out how to do that without using floating point.) Then convert that to a coordinate in the trapezoid as xt = ((x1 + t*a) + s*(w0*(1-t) + w1*t)) and yt = y1 + h1*t. Then <xtyt> is the point in the trapezoid corresponding to <xy> in the rectangle. Now I'll let you figure out how to do the copying :-) Good luck. P.S. And please don't forget - coordinates fall between pixels not on them. Hi Mike Thanks for your reply I have implemented trapezoid blt but I think ur method is looking optimized than mine. I can't understand your ""coordinate mapping between the two shapes"" funda. Can u explain it in better way or can you give me some reference to explore it more. Hi Mike I know how to do Polygon Fill Can u please explain me how can I do Image fill in Polygon.  Would it be feasible to sidestep the problem and use OpenGL to do this for you? OpenGL can render to memory contexts and if you can take advantage of any hardware acceleration by doing this that'll completely dwarf any code tweaks you can make on the CPU (although on some older cards memory context rendering may not be able to take advantage of the hardware). If you want to do this completely in software MESA may be an option."
431,A,difference between SDL and GLUT I am learning the Opengl graphic programming at Eclipse. Can someone tell me the difference between GLUT application and SDL application so that I can dig into either one of them? Tks. SDL is a larger library for larger needs. If you're building a simple demo of a rendering mechanism GLUT is way better than SDL it takes care of a lot of the details that SDL would otherwise require. However if you're developing a serious application SDL is more likely to be the tool you need to use as GLUT abstracts more than a real application would normally want to. SDL gives you much more control over everything.  SDL is a full multimedia library which supports graphics sound input networking plus there are a lot third-party add-ons. GLUT is a simple GUI library.
432,A,"C# / Silverlight / WPF / Fast rendering lots of circles I want to render a lot of circles or small graphics within either silverlight or wpf (around 1000-10000) as fast and as frequently as possible. If I have to go to DX or OGL that's fine but I'm wondering about doing this within either of those two frameworks first (read: it's OK if an answer is WPF-only or Silverlight-only). Also if there is a way to access DX through WPF and render on a surface that way I would be interested in that as well. So what's the fastest way to draw a load of circles? They can be as plain as necessary but they do need to have a radius. Currently I'm using DrawingVisual and a DrawingContext.DrawEllipse() command for each circle then rendering the visual to a RenderTargetBItmap but it becomes very slow as the number of circles rises. By the way these circles move every frame so caching isn't really an option unless you're going to suggest caching the individual circles . . . But their sizes are dynamic so I'm not sure that's a great approach. Just curious why are your drawing 10000 dynamic sized circles? It's for a game. They can all grow and shrink and whatnot. Note by editing I don't get any ownership of the question. Also the trend on SO is to **not** have greetings (e.g. ""hi"" ""hello"" ""I hope you can help me"") or signatures (e.g. ""-Walt W""). For more info see the post on Meta StackOverflow regarding this: http://meta.stackexchange.com/questions/28416/what-is-the-policy-on-signatures-and-links-in-answers-for-stack-overflow-question @casperOne I can't omit greetings :). Sooorry :) In Silverlight 3.0+ most likely you will use WriteableBitmap for this. In WPF it also may be a good choice. I wrote two demo applications in Silverlight. They may be little buggy but they demonstrate the point. Hello world application. Definitely can be optimized. Performance isn't that good but that's because I made something stupid. I believe it has ~2 500 ellipses: Slide show application. I can't recall number of objects here but it's way more than 10 000. I'm wondering if it's okay to say ""way more"" in English. If it's silly I meant a lot more :). way more works :) I'll look at these though I'm looking for pretty high framerates and there it looks like it's taking a second per frame for the 2500 ellipses. Thank you :). A second isn't a framerate there. It's initialization time. Framerate is pretty high. No freezing no jumping all smooth. Alright well I'll definitely take a look later thanks.  If you want speed do not use draw Ellipse - pre generate your circles for sizes and colours and use write bitmap and you will get full BitBlt acceleration. This may be difficult depending on your background  transparency needs or anti aliasing but it will be orders of magnitudes faster. Using anti aliased line drawing is slow but is ok for most thigs since it odesnt change the results can be cached and you get multiple frames with the result. However if you want to draw as fast as possible use non transparent Bitmaps.  Check out this article by Charles Petzold. It describes how to do pretty much exactly what you're looking for. I see what you mean; sorry earlier look was pretty quick. I'll look at it more. This might work though I'm a little hesitant to re-implement my circles as a datapoint scatter plot which I would think would be optimized for static content as opposed to the dynamic content I'm talking about. Ok this is the accepted answer because it utilizes more of the framework (which is nice; it allows for more built-in drawing possibilities) and I can get decent speeds with a lot of drawings. Frankly the article pointed to isn't the absolute best... I'd recommend http://msdn.microsoft.com/en-us/library/ms749021.aspx  http://msdn.microsoft.com/en-us/library/bb613591%28VS.100%29.aspx  and http://www.michaelflanakin.com/Weblog/tabid/142/articleType/ArticleView/articleId/1015/WPF-Performance-Tips.aspx instead. I think you missed the point. The article isn't really about the scatter plot it's about common ways to speed up displaying a lot of simple items in WPF and the potential pitfalls along the way.  While you have accepted an answer already if you find you need to use DirectX9 (inter-op with later versions is a bit harder but can still be done (depending on hardware)) you can use the D3DImage component. Which is detailed on CodeProject Here. The other option is you can just use a host a WinForms control and use the hWnd of that for D3D device creation.  Check out the WriteableBitmapEx library for Silverlight which will surely work with WPF too. The circle functionality was introtuced in this blog post including a sample. Question about this - does it support filling the shapes? I only see outlines in the demos. Which may not be so bad as a note but I was curious. At the moment there's no support for filling but it's on the list and will be implemented in the near future. Although I have a lot of other projects as well. :)"
433,A,A native sdk function to scale Bitmap? Is there some native android SDK function which takes bitmap and desired new bitmap dimensions and then returns scaled bitmap? Bitmap.createScaledBitmap() corrupts the image. I think it changes the encoding to RGB_565 which means that the resulting bitmap is useless garbage.  You might take a look at: Bitmap.createScaledBitmap() Oh wow great. Thank you this is it :)
434,A,"How do you copy from a rectangular source region to a non-rectangular/non-paralellogram destination region in .NET? I know how to use the DrawImage() method of the Graphics object to copy from a rectangular source region to a rectangular destination region and how to copy to a paralellogram region defined by a three-element Point[] array. Is there any way in .NET to copy from a rectangular source region to a 4-sided non-rectangular destination region (which is defined by four arbitrary points)? Update. Here is some sample code that copies an image to a parallelogram region: using (Graphic g = this.CreateGraphics()) { List<Point> pts = new List<Point>(); pts.Add(new Point(0 0)); pts.Add(new Point(100 0)); pts.Add(new Point(10 100)); g.DrawImage(pbSource.Image pts.ToArray()); } If I add a fourth point I get a ""Not Implemented"" exception. Update 2. It is possible to do this entirely in .NET: but you have to do it yourself pixel-by-pixel. Graphics courtesy of Jonbert. Sorry know only of the win32 method ;-) @Gamecat: I'll take it. I'm a PInvoking fool. It is possible to do this entirely in .NET: but you have to do it yourself pixel-by-pixel. Graphics courtesy of Jonbert.  As far as I can tell there isn't a straightforward (built-in) way to do this kind of image transform in .NET. If you look at Anti Aliased Image Transformation (Aaform) over on codeproject you can download his sample in which there is a pure vb implementation of the concept (as well as a vb and c++ sample). There is also Anti-Grain Geometry a free-to-use (for non-commercial) c++ graphics library and a c# wrapper for it. Hope this helps."
435,A,"Draw 2.5D or 3D Map with C# from lines I'm developing a turn-by-turn navigation software for Windows Mobile using C# and .NET CF. I'm able to draw a 2D maps by drawing lines. My problem is I would like to get a 2.5D map like in the picture. I tried non-affine transformation on the 2D rendered image but it is too slow for the Windows Mobile device we are targeting. Could anyone give me a clue on my problem? A very basic linear transformation may be sufficient as the viewport is always going to have the same orientation (i.e. ""tilt""). Something like: # assuming 00 is top left of screen w = 320 # screen width h = 480 # screen height t1 = 0.75 # scale at top of screen t2 = 1.25 # scale at bottom of screen # xy is the initial point # x_y_ is the transformed result x_ = (x - w/2)*(t1+(y/h)*(t2-t1)) + w/2 y_ = y This will multiple x by a smaller factor the higher up the screen it is going from 0.75*x at the top (when y=0) to 1.25*x at the bottom (when y=h). Note that we need to scale x relative to the center of the screen. This could be made almost as fast as directly drawing the lines if necessary factoring out the constant expressions and maybe making it use a lookup table. Maybe try `x_ = (x - w/2)*min(t2max(t1 t1+(y/h)*(t2-t1))) + w/2` or something similar to prevent x_ being warped too much when y is off the screen. I'm facing a new problem with your answer. It works very well when y >= 0 and y <= h. But there are situations when y < 0 or y > h then the line become irregular. Could you please help me with this again?  Use a perspective transformation because it will map straight lines to straight lines. More details in this answer."
436,A,"How to print R graphics to multiple pages of a PDF and multiple PDFs? I know that  pdf(""myOut.pdf"") will print to a PDF in R. What if I want to Make a loop that prints subsequent graphs on new pages of a PDF file (appending to the end)? Make a loop that prints subsequent graphs to new PDF files (one graph per file)? Did you look at help(pdf) ? Usage:  pdf(file = ifelse(onefile ""Rplots.pdf"" ""Rplot%03d.pdf"") width height onefile family title fonts version paper encoding bg fg pointsize pagecentre colormodel useDingbats useKerning) Arguments: file: a character string giving the name of the file. For use with 'onefile=FALSE' give a C integer format such as '""Rplot%03d.pdf""' (the default in that case). (See 'postscript' for further details.) For 1) you keep onefile at the default value of TRUE. Several plots go into the same file. For 2) you set onefile to FALSE and choose a filename with the C integer format and R will create a set of files.  Not sure I understand. Appending to same file (one plot per page): pdf(""myOut.pdf"") for (i in 1:10){ plot(...) } dev.off() New file for each loop: for (i in 1:10){ pdf(paste(""myOut""i"".pdf""sep="""")) plot(...) dev.off() } You don't even need the paste() over filenames -- R can do that for you too; see my answer."
437,A,"Combine multiple images into a single image for later painting with alpha blending I have a graphics system for Java which allows objects to be ""wallpapered"" by specifying multiple images which can have (relatively) complex alignment and resizing options applied. In order to perform adequately (esp. on very low powered devices) I do the image painting to an internal image when the wallpaper is first painted and then copy that composite image to the target graphics context to get it onto the screen. The composite is then recreated only if the object is resized so the only work for subsequent repaints is to copy the clipped region from the composite to the target graphics context. The solution works really well except that when I have PNG images with alpha-channel transparency the alpha channel is lost when painting the composite - that is the composite has all pixels completely opaque. So the subsequent copy to the on-screen graphics context fails to allow what's behind the wallpapered object show through. I did manage to use an RGBImageFilter to filter out completely transparent pixels but I can't see a solution with that to making blended transparency work. Does anyone know of a way I can paint the images with the alpha-channel intact and combined if two pixels with alpha values overlap? What type of Image do you use for the composite image? You should use a BufferedImage and set it's type to TYPE_INT_ARGB which allows translucency. I have just be using java.awt.component.createImage(). Will look into using a BufferedImage and let you know how it goes. This works thanks very much. How I missed BufferedImage I do not know! BufferedImage is the single most useful Image subclass Java has in my opinion. You should always use that when you can. If this image will be used quite often you should enable hardware acceleration on it by calling BufferedImage().setAcceleration() with a parameter greater than 0. Read the API for details."
438,A,"Techniques for dynamic (algorithmic) graphics I'm programming an application for a 32 bit processor with limited memory (512k flash 32k RAM). The display on this device is 128x160 with 16 bit color which would normally consume 40k ram if I were to buffer it on my processor. I don't have that much RAM so I'm looking for techniques tips tricks ideas for generating screen data on the fly. Things that might help: Perhaps you know of a resource for this sort of limitation Maybe you've generated attractive graphics on the fly Is there a generic algorithm I might use to combine elements in program memory (including alpha blending) on the fly while I scan the display Simple vector rendering techniques (or free (bsd/mit/apache) source) ??? I do have a multiplier but no floating point processor. The display itself has a very simple controller and memory for the display - but reads and writes are expensive so I don't want to use that as my workspace if I can avoid it. Some ideas that would combine nice graphics and low memory: Store backgrounds and sprites in flash. Store dynamically generated graphics in RAM using a palette to half the bytes. Use the windowing feature of the LCD driver to only update the part of the screen you need to.  In a way you are in pretty much the same situation game developers where at the time of the Tandys Spectrums and early PCs. So here's my recommendation: You should read Michael Abrash writings on computer graphics. They were written in a time where a floating point co-processor was an optional piece of hardware and they describe a lot of the basic techniques (Bresenham lines etc.) used in the old (supposedly 'bad') software-rendered days. You can read most of his ""Black Book"" here. Additionaly you can probably find a lot of the old BBS files that most people used back in the day to learn graphics programming here. Just search for Graphics Lines and what not. Hope that helps! Update: I also recall using this in my first attempts at drawing things on the screen. Can't tell how much time I spent trying to understand the maths behind it (well to be fair I was like 15 at the time). Very good (and simple) introduction to 3D and a very nice premier on transformations polygon-fillers and interpolation.  What kind of data will you show on the screen? If it is not photographic images you could consider using a palette. For instance: A 256 color palette using 8 bits per pixel would require 20kb (plus 256 x 2bytes for the lookup table) which at least is better than 40kb.  I believe the basic technique for dealing with this kind of situation is to divide the screen into narrow horizontal stripes and only buffer two such stripes in RAM. One stripe will be displayed while you render the next one down. When the scanning 'beam' hits the next stripe (and fires off an interrupt for you to catch) you swap the two and start drawing the next stripe down. A nasty side-effect of this is that that you have hard timing limits on how long you can spend on rendering each stripe. So I guess it would be tempting to stick with something boring but with predictable performance like sprites. Slightly offtopic but this is how the Nintendo DS 3D hardware works. You can see it if you try to render too many polygons around the same y-coordinate - polys will randomly flicker and drop out as the screen-refresh overtakes the rendering hardware. Also I'd second the other poster's suggestion that you use palettised rendering. It's very hard to do fast maths on 16bit pixels but faster in 8bit if you're clever about how you lay out your palette."
439,A,Using Graphics Card instead of GDI+ for Image Manipulation I have a question which may be a pipe-dream but I wanted to know if any of my fellow Stack Overflow'ers could help me with. In the company I work for we do billions of image manipulations each month. Basically we take a massive image slice it into 256 pixels square images color quantize them and save them as pngs - and move onto the next mammoth image. We employ a number techniques to do this as quickly as possible and currently it IS very fast but I think there is a chance we could make it stellar in speed. The application itself is .Net 2.0 loops through the various bytes in the large image reading the bytes for each smaller image and use GDI to save the image after it has run through a quantization algorithm. We have dozens of machines which run this application and all of them have Nvidia Geforce 8 Video cards (or better). Is there a way in which I can use the GPU instead of the CPU to perform any or all of the tasks above? If so how do I do this? Unfortunately I have never coded anything like this before so if anyone can help me I might need it explained quite thoroughly (and slowly). You aren't processing satellite images by any chance? No actually but the principal would be the same. A few technologies to look into: Windows Imaging Components. this isn't exactly what you're after since i don't think it uses the GPU (although i could be wong) but it should be significantly faster than GDI+. Direct2D. this does use the GPU for many drawing operations and integrates well with the Windows Imaging Components. but from your description it's not clear whether the drawing operations optimized by the GPU fit exactly what you need. On top of those you can try using pixel shaders for the image manipulation. this is an area I haven't delved into so i'll leave it for others to comment. To put it another way Windows Imaging Components should reduce the PNG load/save bottleneck significantly. the operations it provides will probably also help slice the image up in a much more optimal way than GDI+. Direct2D and/or a pixel shader should help with the pixel-level manipulation. the pixel shader should only be needed if there's no more direct way to do the color quantize operation you need on the images.  Define massive? (In other words massive is relative.) It is possible use CUDA - NVIDIA GPU's - http://developer.download.nvidia.com/compute/cuda/sdk/website/projects/dxtc/doc/cuda_dxtc.pdf +1 for knowing about CUDA. It seems like CUDA is more about compression of images rather than slicing the images up. While the compression is cool (and *sort of* part of the issue) at the end of the process I still need PNGs outputted. As for massiveness the image I am cutting is 8192px X 1024px.
440,A,"Given a set of points find if any of the three points are collinear What is the best algorithm to find if any three points are collinear in a set of points say n. Please also explain the complexity if it is not trivial. Thanks Bala Is this your homework? This question was discussed in the class and I know the O(N^2) algorithm. I found quite simple algorithm to do it in O(N^2). I want to know if there is an even simpler algorithm. This and another question Algorist asked are both Google interview questions. Are these 2d points? yes there are 2d points. @Algorist: Not sure if you can be notified of my answer hence the ping using a comment. Hope it proves useful. @Algorist - can you share with us the simple O(N^2) algorithm? I haven't found anything simple... @FogleBird This is also a (modified) question from the Introduction to Algorithms text the computational geometry chapter. If you can come up with a better than O(N^2) algorithm you can publish it! This problem is 3-SUM Hard and whether there is a sub-quadratic algorithm (i.e. better than O(N^2)) for it is an open problem. Many common computational geometry problems (including yours) have been shown to be 3SUM hard and this class of problems is growing. Like NP-Hardness the concept of 3SUM-Hardness has proven useful in proving 'toughness' of some problems. For a proof that your problem is 3SUM hard refer to the excellent surver paper here: http://www.cs.mcgill.ca/~jking/papers/3sumhard.pdf Your problem appears on page 3 (conveniently called 3-POINTS-ON-LINE) in the above mentioned paper. So the currently best known algorithm is O(N^2) and you already have it :-) Thank you @Moron. The link was very helpful. @Elazar: I believe it is possible by considering the dual in which lines maps to points and vice versa (ab) <-> y = ax+b . Now the problem corresponds to finding vertices in the dual with degree at least 6. Apparently this is possible in O(n^2) time and O(n^2) space. This book: http://books.google.com/books?id=PBRJ-ruwOQcC has a mention of it on page 94. @Moron - is there an algorithm with time `O(n^2)` which isn't using hash table as the one described by @Strilanc?  Another simple (maybe even trivial) solution which doesn't use a hash table runs in O(n2log n) time and uses O(n) space: Let S be a set of points we will describe an algorithm which finds out whether or not S contains some three collinear points. For each point o in S do: Pass a line L parallel to the x-axis through o. Replace every point in S below L with its reflection. (For example if L is the x axis (a-x) for x>0 will become (ax) after the reflection). Let the new set of points be S' The angle of each point p in S' is the right angle of the segment po with the line L. Let us sort the points S' by their angles. Walk through the sorted points in S'. If there are two consecutive points which are collinear with o - return true. If no collinear points were found in the loop - return false. The loop runs n times and each iteration performs nlog n steps. It is not hard to prove that if there're three points on a line they'll be found and we'll find nothing otherwise. You only need to do the outer loop for half of the points: those with largest `y` coordinate values correct? Also when you said ""there are two consecutive points which are collinear"" you actually meant ""there are two consecutive points which are collinear together with point `o`"" am I understanding you correctly? First observation is correct although it does not change the complexity. Doing so for the other half of the points is equivalent due to symmetry. Note that you have to sort them first which makes the algorithm a bit more complex.. The second point is also correct fixing it Thanks.  A simple O(d*N^2) time and space algorithm where d is the dimensionality and N is the number of points (probably not optimal): Create a bounding box around the set of points (make it big enough so there are no points on the boundary) For each pair of points compute the line passing through them. For each line compute its two collision points with the bounding box. The two collision points define the original line so if there any matching lines they will also produce the same two collision points. Use a hash set to determine if there are any duplicate collision point pairs. There are 3 collinear points if and only if there were duplicates. Right you are. You might want to give more detail in step 2 to clarify that the two collision points are put together to create a ""collision point pair"". Otherwise a reader may confuse the pair (of collision points) in step 3 with the pair (of original points) in step 2 as I did. Your condition is necessary but is it sufficient? Put 4 points at (00) (01) (10) and (11) to define the bounding box. The pair [(0.50.6) (0.50.7)] creates a segment that intersects the box at (0.50) as does the pair [(0.40.1)(0.30.2)]. Yet no three of the 8 points are on any common line. Oh I see you interpreted 'pairs' as two equal collision points instead of two pairs of equal collisions points. It's a sufficient condition because two points define a line. You've only considered one of the points in the collision point pair; the other one will differ so the pair doesn't match."
441,A,rendering an antialiased spiral I have looked at this example using php and GD to piecewise-render a spiral with small arcs. What I would like to do is render an approximation to a spiral that is as mathematically accurate as possible. Inkscape has a spiral tool that looks pretty good but I would like to do the spiral generation programmatically (preferably in Python). I haven't found any drawing libraries (e.g. Cairo) that natively support spiral shapes. If one wanted to render a perfect antialiased spiral would the best way be to just iterate pixel-by pixel over a canvas determining whether each pixel lies within a mathematically-defined spiral arm region (of finite thickness)? In that case one would also have to implement anti-aliasing logic from scratch. Would you integrate the portion of the curve that lies within each pixel box then convert the ratio of filled to empty area to an alpha value? In this instance the quality of the rendering is more important than the rendering time. However evaluating an integral at each pixel strikes me as pretty inefficient. Update: I believe the question I should be asking is this one (for which Yahoo Answers has failed). Wouldn't it be easier to draw a curve (spline?) and just generate plenty of control points? Like that you'd pick up an existing antialiasing engine. [EDIT]: A first approximation to this (done with Tcl/Tk but the code should be easy to convert) gives me this: # Make the list of spiral coordinates set coords {} set theta 0; set r 10 set centerX 200; set centerY 150 # Go out 50 pixels per revolution set pitch [expr {100.0 / 720.0}] for {set i 0} {$i<720} {incr i} { lappend coords [expr { $centerX + $r*sin($theta) }] \ [expr { $centerY + $r*cos($theta) }] set r [expr { $r + $pitch }] # Increment angle by one degree set theta [expr { $theta + 3.1415927/180 }] } # Display as a spline pack [canvas .c -width 400 -height 300] .c create line $coords -tag spiral -smooth 1 I've not made any effort to be efficient in my use of control points. @kostmo: Try that updated answer. You'll probably need to adapt as I'm not sure which language you want to implement in but the point calculations remain the same. Math is math after all...  I haven't found any drawing libraries (e.g. Cairo) that natively support spiral shapes No it's quite an unusual feature; it's only very recently that drawing with spirals has become popular. The code Inkscape uses for this is Spiro. It's mostly Python and can use Cairo to render the beziers that the spirals are approximated into. However evaluating an integral at each pixel strikes me as pretty inefficient. Yes indeed. I downloaded the code to Spiro which appears to be in C. However the bezier approach sounds like the right way and I can do this from PyCairo.
442,A,Transformation (rotation) of image using matrix - design-time vs. run-time rendering results I have a 48x48 image which is rotated using a transformation matrix. For some reason the rotated image in design-time differs from the rotated image in run-time as you can see from this screenshot (design-time on the left run-time on the right): It might be a little bit difficult to spot but if you look closely at the right edge of the blue circle it is about a pixel wider in the image to the right. Note that the image is layered - the white glow in the foreground is the part that's being rotated while the blue ball in the background is static. It seems like the image is offset 1 pixel in run-time when rotating exactly 90 degrees (as in the screenshot) 180 degrees and probably also 270 degrees. The image looks the same with any other rotation angle as far as I can see. Here's a snippet: protected static Image RotateImage(Image pImage Single pAngle) { Matrix lMatrix = new Matrix(); lMatrix.RotateAt(pAngle new PointF(pImage.Width / 2 pImage.Height / 2)); Bitmap lNewBitmap = new Bitmap(pImage.Width pImage.Height); lNewBitmap.SetResolution(pImage.HorizontalResolution pImage.VerticalResolution); Graphics lGraphics = Graphics.FromImage(lNewBitmap); lGraphics.Transform = lMatrix; lGraphics.DrawImage(pImage 0 0); lGraphics.Dispose(); lMatrix.Dispose(); return lNewBitmap; } void SomeMethod() { // Same results in design-time and run-time: PictureBox1.Image = RotateImage(PictureBox2.Image 18) // Different results in design-time and run-time. PictureBox1.Image = RotateImage(PictureBox2.Image 90) } Can anyone explain the reason for this behaviour? Or better yet a solution to make run-time results look like design-time results? It's important for me because this image is part of an animation which is generated from code based on a single image which is then rotated in small steps. In design-time the animation looks smooth and nice. In run-time it looks like it's jumping around :/ I'm using Visual Studio 2005 on Windows Vista Business SP2. I think that should be a problem about the math being performed in floating point; when rounding back to integer coordinates. One question that might be relevant... is your image completely square and the width and height an even number? Maybe the problem lies there. Edit: In the first read I passed through the dimensions of the image... If that wasn't the problem maybe you could do a simple rotation by yourself: angle = 90 : img[i][j] = img[j][w-i] angle = 180: img[i][j] = img[w-i][w-j] angle = 270: img[i][j] = img[w-j][i] (I think the index swappings are ok but a double check won't be bad) I believe the transformation actually supports rotation around a floating point e.g. (24.5 24.5) when using matrices. The Matrix.RotateAt method only accepts a PointF struct anyway. I'm not sure how to use your example but I solved the problem by setting the PixelOffsetMode property of the Graphics object to a higher value. This was the cause of the problem. If I wanted to use other means of rotating an image 90 180 or 270 degrees there is also the RotateFlip method on the Image class. In any case thanks for your input!  It might have something to do with differences in the Graphics object used at design time versus run time like the PixelOffsetMode. Amazing! Changning Graphics.PixelOffsetMode to HighQuality caused the result to be even more satisfying than the current design-time results. The animation now looks perfect. Thanks a lot BlueMonkMN you made my day :) PixelOffsetMode.Half also gets the job done in my case.  How are you specifying your angle of rotation? in Degrees or in Radians? Sometimes I get results like those you describe when I've accidentally specified my angle in degrees when it should've been radians (and as a result the object has been rotated several hundred degrees instead of a few) I don't know for sure given your specific example but its worth suggesting. Hi Phill thank you for your response! Unfornutately this is not the source of the problem. The Matrix.RotateAt() method takes an angle argument that should be specified in degrees which is what I have done.  Hm my consideration is to try: lMatrix.RotateAt(pAngle new PointF((pImage.Width + 1)/2.0f (pImage.Height + 1)/ 2.0f)); I tested your suggestion but it simply offsets the entire animation. The irregularities at 90 180 and 270 degrees rotation still occur in run-time. I'd like to think that the code works just fine as it is since the animation already looks perfect in design-time. What I'm wondering is what's causing the difference in run-time results. That means I was wrong... So does the run-time angle values in 90 180 and 270 placements are absolutely equal to that values? The screenshot was taken at exactly 90 degrees rotation. At exactly 180 and 270 degrees rotation the image also seems to be slightly offset. Really strange. Is your step fixed? If no may be you can change it a bit and see how it will affect run-time?.. Yes if I use a step size that avoids the 90/180/270 angles there is no problem. I have posted a workaround below which ensures that these angles are avoided altogether. I'm still wondering why this is necessary though.
443,A,How to set ViewingPlatform and update TransformGroup? I have a scene inside of a TransformGroup that allows the mouse to zoom/rotate/pan. I need to set the camera position back far enough that I can see the entire scene which I do with the following code:  // Position the position from which the user is viewing the scene ViewingPlatform viewPlatform = universe.getViewingPlatform(); TransformGroup viewTransform = viewPlatform.getViewPlatformTransform(); Transform3D t3d = new Transform3D(); viewTransform.getTransform(t3d); t3d.lookAt(new Point3d(0050) new Point3d(000) new Vector3d(010)); t3d.invert(); viewTransform.setTransform(t3d); Executing the above code works in that I can manipulate the scene with the mouse. However if I swap out this line: t3d.lookAt(new Point3d(0050) new Point3d(000) new Vector3d(010)); with: // Change value from 50 to 90 to push the camera back further t3d.lookAt(new Point3d(0090) new Point3d(000) new Vector3d(010)); I lose the ability to manipulate the screen with the mouse. How can I maintain the capability to transform with the mouse while pushing the camera back further so that I can view the entire screen? Much thanks in advance!  GraphicsConfiguration config = SimpleUniverse.getPreferredConfiguration(); Canvas3D canvas3d = new Canvas3D(config); // Manually create the viewing platform so that we can customize it ViewingPlatform viewingPlatform = new ViewingPlatform(); // **** This is the part I was missing: Activation radius viewingPlatform.getViewPlatform().setActivationRadius(300f); // Set the view position back far enough so that we can see things TransformGroup viewTransform = viewingPlatform.getViewPlatformTransform(); Transform3D t3d = new Transform3D(); // Note: Now the large value works t3d.lookAt(new Point3d(00150) new Point3d(000) new Vector3d(010)); t3d.invert(); viewTransform.setTransform(t3d); // Set back clip distance so things don't disappear Viewer viewer = new Viewer(canvas3d); View view = viewer.getView(); view.setBackClipDistance(300); SimpleUniverse universe = new SimpleUniverse(viewingPlatform viewer); Back clip distance determines how far a distance into the back (rear) of the scene objects will remain visible. .setActivationRadius makes the objects appear range but what does .setBackClipDistance do?
444,A,"What strategies are best for storing art assets in SVN? We are using SVN very successfully for source code in the traditional way: /branches /trunk and /tags. We do not use SVN for our art assets which in a way are like source but really don't have the same needs as source code. I am referring to not only image files (jpeg png etc...) but also PhotoShop files and stock artwork that has been purchased (and shouldn't be lost). What would be the best practice for file structure and procedure for my graphic artists? For our graphics we organize things into projects (like /graphics/marketing/NAILBA/2009/Banner represents our banner for the NAILBA {life insurance} conference this year). The folders /trunk and /branch are not mandatory but they offer a clean way to explore ideas before selecting a single version. Since the HEAD is usually the only version that matters we don't use /tags. Now we don't create that much content (we program a suite of web apps primarily) but this has worked well for our marketing projects (fliers banners websites etc).  That's a good question actually. I'm wondering how other people are doing this. What I usually do is keep track of main PSD files in the SVN repository in a separate folder. Let's say you have a /images containing all your images. In that case I usually set up an /images/source containing the latest PSD files. It's a bit annoying on the first checkout but PSD are not updated as often as source code so that's not too bad. Of course you have to exclude these folders when you deploy your website. Another way my company handle this is having all that on a network hard drive. We keep track of changes with a file structure like this: /Project/Assets/Design/ / Round 1 / Round 2 / Round n The ""Round"" folder countain a version of the PSD files and the JPG exports associated to it. It is more efficient if you don't have too many rounds of changes because if you have 100 revisions it's getting hard to manage correctly."
445,A,How do I determine the intersection point of two lines in GDI+? I'm using .NET to make an application with a drawing surface similar to Visio. The UI connects two objects on the screen with Graphics.DrawLine. This simple implementation works fine but as the surface gets more complex I need a more robust way to represent the objects. One of these robust requirements is determining the intersection point for two lines so I can indicate separation via some kind of graphic. So my question is can anyone suggest a way to do this? Perhaps with a different technique (maybe GraphViz) or an algorithm? If you rotate your frame of reference to align with the first line segment (so the origin is now the start of the first line and the vector for the first line extends along the X-axis) the question becomes where does the second line hit the X-axis in the new coordinate system. This is a much easier question to answer. If the first line is called A and it is defined by A.O as the origin of the line and 'A.V' being the vector of the line so that A.O + A.V is the end point of the line. The frame of reference can be defined by the matrix:  | A.V.X A.V.Y A.O.X | M = | A.V.Y -A.V.X A.O.Y | | 0 0 1 | In homogeneous coordinates this matrix provides a basis for the frame of reference that maps the line A to 0 to 1 on the X-axis. We can now define the transformed line B as: C.O = M*(B.O) C.V = M*(B.O + B.V) - C.O Where the * operator properly defined for homogeneous coordinates (a projection from 3 space onto 2 space in this case). Now all that remains is to check and see where C hits the X-axis which is the same as solving Y side of the parametric equation of C for t: C.O.Y + t * C.V.Y = 0 -C.O.Y t = -------- C.V.Y If t is in the range 0 to 1 then C hits the X-axis inside the line segment. The place it lands on the X-axis is given by the X side of the parametric equation for C: x = C.O.X + t * C.V.X If x is in the range 0 to 1 then the intersection is on the A line segment. We can then find the point in the original coordinate system with: p = A.O + A.V * x You would of course have to check first to see if either line segment is zero length. Also if C.V.Y = 0 you have parallel line segments. If C.V.X is also zero you have colinear line segments.  The representation of lines by y = mx + c is problematic for computer graphics because vertical lines require m to be infinite. Furthermore lines in computer graphics have a start and end point unlike mathematical lines which are infinite in extent. One is usually only interested in a crossing of lines if the crossing point lies on both the line segments in question. If you have two line segments one from vectors x1 to x1+v1 and one from vectors x2 to x2+v2 then define: a = (v2.v2 v1.(x2-x1) - v1.v2 v2.(x2-x1)) / ((v1.v1)(v2.v2) - (v1.v2)^2) b = (v1.v2 v1.(x2-x1) - v1.v1 v2.(x2-x1)) / ((v1.v1)(v2.v2) - (v1.v2)^2) where for the vectors p=(pxpy) q=(qxqy) p.q is the dot product (px * qx + py * qy). First check if (v1.v1)(v2.v2) = (v1.v2)^2 - if so the lines are parallel and do not cross. If they are not parallel then if 0<=a<=1 and 0<=b<=1 the intersection point lies on both of the line segments and is given by the point x1 + a * v1 Edit The derivation of the equations for a and b is as follows. The intersection point satisfies the vector equation x1 + a*v1 = x2 + b*v2 By taking the dot product of this equation with v1 and with v2 we get two equations: v1.v1*a - v2.v1*b = v1.(x2-x1) v1.v2*a - v2.v2*b = v2.(x2-x1) which form two linear equations for a and b. Solving this system (by multiplying the first equation by v2.v2 and the second by v1.v1 and subtracting or otherwise) gives the equations for a and b.  ask dr. math Definitely more informative than `dr.evil` thanks for the link!
446,A,"Displaying a rotated string - DataGridView.RowPostPaint I want to display a lengthy rotated string in the background of one of my rows in a DataGridView. However this: private void dataGridView1_RowPostPaint(object sender DataGridViewRowPostPaintEventArgs e) { if (e.RowIndex == 0) { ... //Draw the string Graphics g = dataGridView1.CreateGraphics(); g.Clip = new Region(e.RowBounds); g.RotateTransform(-45); g.DrawString(printMe font brush e.RowBounds format); } } does not work because text is clipped before it's rotated. I've also tried painting on a Bitmap first but there seems to be a problem painting transparent Bitmaps - the text comes out pure black. Any ideas? I figured it out. The problem was that Bitmaps apparently don't have transparency even when you use PixelFormat.Format32bppArgb. Drawing the string caused it to draw over a black background which is why it was so dark. The solution was to copy the row from the screen onto a bitmap draw onto the bitmap and copy that back to the screen. g.CopyFromScreen(absolutePosition Point.Empty args.RowBounds.Size); //Draw the rotated string here args.Graphics.DrawImageUnscaledAndClipped(buffer args.RowBounds); Here is the full code listing for reference: private void dataGridView1_RowPostPaint(object sender DataGridViewRowPostPaintEventArgs args) { if(args.RowIndex == 0) { Font font = new Font(""Verdana"" 11); Brush brush = new SolidBrush(Color.FromArgb(70 Color.DarkGreen)); StringFormat format = new StringFormat { FormatFlags = StringFormatFlags.NoWrap | StringFormatFlags.NoClip Trimming = StringTrimming.None }; //Setup the string to be printed string printMe = String.Join("" "" Enumerable.Repeat(""RUNNING"" 10).ToArray()); printMe = String.Join(Environment.NewLine Enumerable.Repeat(printMe 50).ToArray()); //Draw string onto a bitmap Bitmap buffer = new Bitmap(args.RowBounds.Width args.RowBounds.Height); Graphics g = Graphics.FromImage(buffer); Point absolutePosition = dataGridView1.PointToScreen(args.RowBounds.Location); g.CopyFromScreen(absolutePosition Point.Empty args.RowBounds.Size); g.RotateTransform(-45 MatrixOrder.Append); g.TranslateTransform(-50 0 MatrixOrder.Append); //So we don't see the corner of the rotated rectangle g.DrawString(printMe font brush args.RowBounds format); //Draw the bitmap onto the table args.Graphics.DrawImageUnscaledAndClipped(buffer args.RowBounds); } }"
447,A,Dynamically create R graphics for webpage I've spent a few weeks learning some R and I'm floored at just how slick and powerful it is. I'm using it to plot some data returned from an SQL query and I'd like to be able to share those plots with others I work with through a web portal. I realize I can create a cron job to run the R scripts on the webserver to create the plots daily to be viewed from the website as images. But is there any way I can set things up such that the images are created only when the user views the page? That way I could make a web interface that lets the user select date ranges etc for the SQL query. (and then have R analyse the data and plot it) Any advice? I like to call R to do statistical analysis from Python driven websites with rpy2. .. or from PypeR [http://sourceforge.net/projects/rinpy/files/ and http://www.jstatsoft.org/v35/c02/paper] @radek thanks I didn't know about that module.  I don't do a lot of dynamic html stuff with R but you could start with either one of these rapache: R inside Apache Rpad: Web-based R R FAQ on Web interfaces Also of interest may be brew for mixing R text and R code in web templates. +1 The R FAQ on Web interfaces should be your first read if you want a straight R solution.  Beyond the other suggestions you can use JRI and call R from Java. Another nice option is to use the mediawiki plugin (read about it on the R-Wiki). It's very straightforward and gives you a simple markup for R.
448,A,Explain the TexturedSphere example in Processing The Processing project website has an example of implementing a 3D textured sphere with rotational capabilities. I'm trying to understand the code but I'm having trouble comprehending many of the code blocks since I don't have a background in graphics. Any higher-level explanation of what each block is trying to accomplish perhaps referencing the relevant algorithm would allow me to read up on the concepts and better understand the implementation. It might be easier if you point out a given part that you don't understand ... Are you struggling with the sphere generation? Or is it something else? After just a few minutes looking at the code I'd say the draw() function is called by the Processing runtime system each time the image should be redrawn. This just paints a black background then renders the globe with the renderGlobe() function. The renderGlobe() function sets up the environment for drawing the globe calculating position turing on lights setting the texture to IMAGE etc. Then it calls texturedSphere to draw the globe. After that it cleans up and adjusts the position variables for the next time through. The initializeSphere() function calculates the vertex locations for the sphere. This is simple trigonometry. The texturedSphere() function draws the sphere. First it draws the southern cap which is really a cone a very flat cone. Next it draws rings for each section of the sphere and then tops it off with another cone for the northern cap. Although I haven't gone through the Processing learning materials the headings indicate that if you start from the beginning and try everything in order you'll easily understand this code.
449,A,"How to prevent overdrawing? This is a difficult question to search in Google since it has other meaning in finance. Of course what I mean here is ""Drawing"" as in .. computer graphics.. not money.. I am interested in preventing overdrawing for both 3D Drawing and 2D Drawing. (should I make them into two different questions?) I realize that this might be a very broad question since I didn't specify which technology to use. If it is too broad maybe some hints on some resources I can read up will be okay. EDIT: What I mean by overdrawing is: when you draw too many objects rendering single frame will be very slow when you draw more area than what you need rendering a single frame will be very slow Google [""graphics culling""](http://www.google.com/#q=graphics+culling). You will get much better results than ""overdrawing"". I'm not sure what you mean by ""overdrawing"". Maybe you mean clipping? It is when you draw more than you should. Rendering one frame will be slow. This sounds like a BTree sorting problem - ensuring you only draw vertices (and their corresponding faces) from the top of the tree as sorted according to viewport orientation. Here's a Wiki article all about it: http://en.wikipedia.org/wiki/Hidden_surface_determination  Reduce the number of objects you consider for drawing based on distance and on position (ie. reject those outside of the viewing frustrum). Also consider using some sort of object-based occlusion system to allow large objects to obscure small ones. However this may not be worth it unless you have a lot of large objects with fairly regular shapes. You can pre-process potentially visible sets for static objects in some cases. Your API will typically reject polygons that are not facing the viewpoint also since you typically don't want to draw the rear-face. When it comes to actual rendering time it's often helpful to render opaque objects from front-to-back so that the depth-buffer tests end up rejecting entire polygons. This works for 2D too if you have depth-buffering turned on. Remember that this is a performance optimisation problem. Most applications will not have a significant problem with overdraw. Use tools like Pix or NVIDIA PerfHUD to measure your problem before you spend resources on fixing it.  Question is really too broad :o) Check out these ""pointers"" and ask more specifically. Typical overdraw inhibitors are: Z-buffer Occlusion based techniques (various buffer techniques HW occlusions ...) Stencil test on little bit higher logic level: culling (usually by view frustum) scene organization tehniques (usually trees or tiling) rough drawing front to back (this is obviously supporting technique :o) EDIT: added stencil test has indeed interesting overdraw prevention uses especially in combination of 2d/3d.  It's quite complex topic. First thing to consider is frustum culling. It will filter out objects that are not in camera’s field of view so you can just pass them on render stage. The second thing is Z-sorting of objects that are in camera. It is better to render them from front to back so that near objects will write “near-value” to the depth buffer and far objects’ pixels will not be drawn since they will not pass depth test. This will save your GPU’s fill rate and pixel-shader work. Note however if you have semitransparent objects in scene they should be drawn first in back-to-front order to make alpha-blending possible. Both things achievable if you use some kind of space partition such as Octree or Quadtree. Which is better depends on your game. Quadtree is better for big open spaces and Octree is better for in-door spaces with many levels. And don't forget about simple back-face culling that can be enabled with single line in DirectX and OpenGL to prevent drawing of faces that are look at camera with theirs back-side."
450,A,"Finding the bounding box (axially aligned) of a parametric range of a 3D NURBS surface I'll apologize in advance in case this is obvious; I've been unable to find the right terms to put into Google. What I want to do is to find a bounding volume (AABB is good enough) for an arbitrary parametric range over a trimmed NURBS surface. For instance (uv) between (0.10.2) and (0.40.6). EDIT: If it helps it would be fine for me if the method confined the parametric region entirely within a bounding region as defined in the paragraph below. I am interested in sub-dividing those regions. I got started thinking about this after reading this paragraph from this paper ( http://www.cs.utah.edu/~shirley/papers/raynurbs.pdf ) which explains how to create a tree of bounding volumes with a depth relative to the degree of the surface: The convex hull property of B-spline surfaces guarantees that the surface is contained in the convex hull of its control mesh. As a result any convex objects which bound the mesh will bound the underlying surface. We can actually make a stronger claim; because we closed the knot intervals in the last section [made the multiplicity of the internal knots k − 1] each nonempty interval [ui; ui+1) [vj; vj+1) corresponds to a surface patch which is completely contained in the convex hull of its corresponding mesh points. Thus if we produce bounding volumes for each of these intervals we will have completely enclosed the surface. We form the tree by sorting the volumes according tothe axis direction which has greatest extent across the bounding volumes splitting the data in half and repeating the process. Thanks! Sean You will need to slice out a smaller NURBS surface which only covers the parameter range you are interested in. Using your example which I take to mean you are in the region where the u parameter is between 0.1 and 0.4. Let Pu be the degree of the spline in that parameter (A cubic spline has Pu = 3). You need to perform ""knot insertion"" (There's your Google search term) to get knots of degree Pu located at u=0.1 and u=0.4 Do the same thing on the v parameter to get knots of degree Pv at 0.2 and 0.6. The process of knot insertion will modify (and add to) the array of control points. There's a bit of bookkeeping involved but you can then find the control_points that determine the surface in the parameter patch you just isolated between inserted knots. The convex property then says the surface is bounded by these control points so you can use them to determine your bounding volume. The NURBS reference I like to use for operations like this is: ""The NURBS Book"" by Les Piegl and Wayne Tiller."
451,A,"Maintain margins for drawing after window resize I'm writing an application draws in another application's window (this is under OS X with Cocoa but the question is general enough that I hope it won't be bogged down by operating system / framework issues) and I'm running into a problem which seems like it should have a simple answer but it's driving me absolutely insane. Here's the problem: I draw a rectangle inside another application's window which has to be in a certain location relative to the top and left of the window (i.e. the margins between my rectangle and the top & left of the target window must remain fixed). I can calculate the relative xy coordinates required as a percentage of the window's size which works fine: the rectangle shows up correctly. However the user then resizes the window and I can resize the rectangle correctly by using a transformation based on the ratio of the new height and width to the old height and width but I lose my relative positioning: the rectangle's coordinates are now incorrect (see image - this is a rough mockup of the problem). Now I can't figure out how to calculate the new xy coordinates relative to the top and left of the window. The window isn't square; its resize is constrained by the other application but the aspect ratio width / height changes by some unknown function. When I measure the required y coordinates in relative terms the percentage changes when the window resize. Numerically it looks something like this: Before resize: window size: h=>760 w=>546 rectangle origin: x=>355 y=>84 (e.g. 84/546 = 15.3% of height) After resize: window size: h=>1009 w=>717 rectangle origin should be: ? (I can measure it as something like x=>474y=>99 but I can't predict those values; 99/717 now = 13.8% of height). I've tried every ratio of the two windows' measurements I can think of; I've also run across the idea of translating to the origin scaling and then translating back to avoid the problem of scaling moving the coordinates - but I don't know where to translate back to! This probably has some simple geometric / trigonometric solution but nothing occurs to me no matter how many diagrams I draw. I'm willing to accept the inevitable embarrassment of someone pointing out some one-line solution to this problem if they can just point me in the right direction here! “I draw a rectangle inside another application's window …” How are you doing that? Sorry that sounds weird the way I wrote it. I'm not drawing directly into their window I'm just creating a new window over the specified coordiantes (NSWindow with the level set to NSStatusWindowLevel). For posterity: I really have no idea how Apple (or anyone else) does this - and honestly I think the application I'm trying to interact with was not written with this in mind anyways since it is far from accessible. To solve the problem I just ended up sampling a whole bunch of different (xy) pairs and constructing a linear model to predict the right coordinates for arbitrary coordinates. A little bit of a sledgehammer but it's the best I could come up with.  If you want it to be left units from the left and top units from the top then try the origin as: x: left y: window.frame.size.height - rectangle.frame.size.height - top But ... I don't know what top is after the window resizes. That's the unknown quantity. Or am I missing something here? I can position the rectangle before the move as 546 + 84 (origin is in the top left) but after the window resizes I only have the new height as 717. The new origin should be 717 + x but I don't what to set x as to keep it the same relative distance from the top as before. (I measured it as 99 but I don't know how to calculate that value from what I have.) Didn't you want to keep the distance from the top constant? The margins are constant but the distance from the top isn't because of the window resize. I don't know how to explain it better than this which is probably part of the problem. In Interface Builder it looks like this in the autoresizing controls: http://img221.imageshack.us/img221/7292/ibexample.png though I'm only concerned about the top and left. Note how the margins remain the same even though the window resizes; if this were my own app Cocoa would do that for me but I'm faced with having to calculate this myself now and I'm having trouble. So you want constant height proportionally growing top and bottom margins and constant left and right margins? The rectangle has to remain in the same position relative to the top and left so the margins (expressed in pixels from the top and left) have to change to keep it there e.g. if it were in the center the top and left have to change so that the rectangle stays in the center. I was calling them ""constant"" because I was thinking visually but you're right in that the margins are actually growing / shrinking as the window resizes. (The bottom and right aren't important for this problem - the window has a minimum and maximum size). In other words: I need to know what distance (in pixels) to set the top of the rectangle from the top / left of the window in order to correctly set the margins to keep the rectangle in proportionally the same place as in the IB example diagram."
452,A,"Efficent use of OnPaint I am programming in Visual Studio .Net and using C#. I am creating my own control that draws a wave based on values I get from an analog to digital converter (ADC). I take the incoming points and convert them into X and Y points to properly draw the graph in my control. I have a loop inside my OnPaint method that goes through all the points and calls the DrawLine method between the current point and the next point. However this is very inefficient as some of these graphs have 8192 points and the system actually has nine ADCs that I would like to show at the same time. Every time the page redraws it takes almost a second for all graphs to redraw (especially during debug). On top of that I have functionality that allows you to zoom in and pan across the waves to get a better view (acts a lot like google maps does) and all 9 waves zoom in and pan together. All of this functionality is very ""jerky"" because I am calling invalidate on mousewheel and mousemove. Basically it all works but not as smoothly as I would like. I was wondering if there were a way to create a predrawn object from the data and then just draw a dilated and translated version of the picture in the draw area. Any help would be greatly appreciated even if it is just pointing me in the right direction. Addressing the comments below as I was not clear in my post. The data is NOT streaming in. I request a set of data and then work with that set so a static image is a possible solution. Just wanted to clarify. I am looking into the other suggestions now. You might set DoubleBuffered to true on your control / form. Or you could try using your own Image to create a double buffered effect. My DoubleBufferedGraphics class: public class DoubleBufferedGraphics : IDisposable { #region Constructor public DoubleBufferedGraphics() : this(0 0) { } public DoubleBufferedGraphics(int width int height) { Height = height; Width = width; } #endregion #region Private Fields private Image _MemoryBitmap; #endregion #region Public Properties public Graphics Graphics { get; private set; } public int Height { get; private set; } public bool Initialized { get { return (_MemoryBitmap != null); } } public int Width { get; private set; } #endregion #region Public Methods public void Dispose() { if (_MemoryBitmap != null) { _MemoryBitmap.Dispose(); _MemoryBitmap = null; } if (Graphics != null) { Graphics.Dispose(); Graphics = null; } } public void Initialize(int width int height) { if (height > 0 && width > 0) { if ((height != Height) || (width != Width)) { Height = height; Width = width; Reset(); } } } public void Render(Graphics graphics) { if (_MemoryBitmap != null) { graphics.DrawImage(_MemoryBitmap _MemoryBitmap.GetRectangle() 0 0 Width Height GraphicsUnit.Pixel); } } public void Reset() { if (_MemoryBitmap != null) { _MemoryBitmap.Dispose(); _MemoryBitmap = null; } if (Graphics != null) { Graphics.Dispose(); Graphics = null; } _MemoryBitmap = new Bitmap(Width Height); Graphics = Graphics.FromImage(_MemoryBitmap); } /// <summary> /// This method is the preferred method of drawing a background image. /// It is *MUCH* faster than any of the Graphics.DrawImage() methods. /// Warning: The memory image and the <see cref=""Graphics""/> object /// will be reset after calling this method. This should be your first /// drawing operation. /// </summary> /// <param name=""image"">The image to draw.</param> public void SetBackgroundImage(Image image) { if (_MemoryBitmap != null) { _MemoryBitmap.Dispose(); _MemoryBitmap = null; } if (Graphics != null) { Graphics.Dispose(); Graphics = null; } _MemoryBitmap = image.Clone() as Image; if (_MemoryBitmap != null) { Graphics = Graphics.FromImage(_MemoryBitmap); } } #endregion } Using it in an OnPaint: protected override void OnPaint(PaintEventArgs e) { if (!_DoubleBufferedGraphics.Initialized) { _DoubleBufferedGraphics.Initialize(Width Height); } _DoubleBufferedGraphics.Graphics.DrawLine(...); _DoubleBufferedGraphics.Render(e.Graphics); } ControlStyles I generally set if I'm using it (you may have different needs): SetStyle(ControlStyles.UserPaint true); SetStyle(ControlStyles.AllPaintingInWmPaint true); SetStyle(ControlStyles.DoubleBuffer true); Edit: Ok since the data is static you should paint to an Image (before your OnPaint) and then in the OnPaint use the Graphics.DrawImage() overload to draw the correct region of your source image to the screen. No reason to redraw the lines if the data isn't changing. 10-4 see my edit. Using a static image drawn once and then drawing only the part of the original image you want should be more efficient. Will eliminate the time it takes to (re-)draw the lines when you pan/zoom. Double-buffering doesn't make things redraw faster - it just removes the flicker. What needs to be done here is allowing the data to be panned scaled etc. without re-rendering it all. According to the OP the data is changing as new data comes from the ADC so a static image isn't the solution either. Double buffering will solve the jerkiness he describes and can result in faster drawing. Cory the data is not streaming in. Sorry if I was not clear about that.  You could draw a multiline. I'm not sure what that looks like in C# but it has to be there (Its a GDI/GDI+ based API). That allows you specify all the points in one go and allows Windows to optimize the call a bit (less stack push/pop to stay inside of the draw algorithm rather than returning to your code for each new point). EDIT: but if your data is static then using a double buffered / cached image of your output is more efficient than worrying about the initial drawing of it. Here's a link: http://msdn.microsoft.com/en-us/library/system.windows.shapes.polyline.aspx  I have two points to add: You say you have 8192 points. Your drawing area probably has no more then 1000. I suppose you could ""lower the resolution"" of your graph by adding only every 10th or so line. You could use GraphicsPath class to store all required lines and draw them all at once with Graphics.DrawPath This way you'll avoid using static bitmap (to allow zooming) while still getting some performance improvements.  Create a Bitmap object and draw to that. In your Paint handler just blit the Bitmap to the screen. That will allow you decouple changing the scale from re-rendering the data. Good for Panning but how does this work with Zooming (and text labeling at the same time). When you zoom you can re-render at the new zoom level in the background. The way Google Maps shows you the fuzzy blown-up version of your old view until it streams in the images at the new zoom level.  Just calculate the visible range and draw only these points. Use double buffering. And finally you can create you own realizaion of multiline drawing using raw bitmap data e.g. use LockBits and write pixel colors directly into bytes forming the picture.Use InvalidateRect(..) to redraw a portion of the window."
453,A,"OpenGL fast texture drawing with vertex buffer objects. Is this the way to do it? I am making a 2D game with OpenGL. I would like to speed up my texture drawing by using VBOs. Currently I am using the immediate mode. I am generating my own coordinates when I rotate and scale a texture. I also have the functionality of rounding the corners of a texture using the polygon primitive to draw those. I was thinking would it be fastest to make a VBO with vertices for the sides of the texture with no offset included so I can then use glTranslate glScale and glRotate to move the drawing position for my texture. Then I can use the same VBO with no changes to draw the texture each time. I could only change the VBO when I need to add coordinates for the rounded corners. Is that the best way to do this? What things should I look out for while doing it? Is it really fastest to use GL_TRIANGLES instead of GL_QUADS in modern graphics cards? Thank you for any answer. If you want to draw a textured polygon using VBOs you need to create buffers not only for the vertices but also for the texture coordinates (one UV coordinate per vertex) and for the vertex colors if you want to do vertex lighting (use glColor instead if not). Use glClientActiveTexture(GL_TEXTUREn) before binding the intended texture. Don't forget to use glEnableClientState() for each buffer type you are using before issuing the render call.  For better performance you should not use immediate mode rendering but instead use vertex buffer (on CPU or GPU) and draw using glDrawArrays etc like methods. Scaling/rotating the texture coordinates by modifying texture matrix multiplier is not a performance issue you just set some small values for the entire mesh. The most important bottleneck here is to transfer per-vertex data (such as color position AND texture coordinate(s)) to GPU and that's what vertex buffer objects on the GPU can greatly help. If the mesh that you want to draw to screen is static that is you set it once and do not need to modify (parts of) it later you can generate the texture coordinates for the entire mesh including all corners) only once. You can later modify texture coordinates for the entire mesh using texture matrix multiplier. However if you want to modify different parts of the mesh using different transformations you will have to divide the mesh into parts. From what you have described I understand that you want to draw a rounded-corner rectangle which holds a picture inside which can be accomplished using a single static mesh (a list of per-vertex data). Lastly GL_QUADS are also deprecated in modern OpenGL specifications. They do not offer much functionality anyway so you should switch QUADS to triangles or triangle strips/fans. Thank you. I've managed to get VBOS working for some line code but not for texture drawing. I haven't got a clue how to do it. Everything I tried failed. I guess I need another question. At current it's rendering in the right place it just isn't being rendered properly. It fails for both GL_QUADS and GL_TRIANGLES. I'll make a new question with my VBO code. Is adding colour information to the VBO that important? Is ""glColor"" going to be much of a problem? Thank you for your answer and help. You cannot mix glColor and like with vertex buffer based draw commands (glDrawElements and glDrawArrays basically). You should use glColorPointer glVertexPointer and relatex methods do set pre-defined vertex semantics. If your texture coordinates are OK and if you still do not observe any texturing check if texturing is enabled and also lighting can affect the result. Once you can clearly see a texture tiled on your mesh modifications will be easier. Quads are deprecated since four vertices could not be on the same plane while three vertices (a triangle) always defines a plane. Do you see your objects on where you want them to be? Is it only the in-correct texturing of your models? If you see your objects in the correct position just keep in mind that texture coordinate values are just like vertex position values. Start with something easy (do not modify texture coordinates using glTranslate etc in texture matrix mode) then try to extend it to achieve what you finally want to see."
454,A,"Calculating a LookAt matrix I'm in the midst of writing a 3d engine and I've come across the LookAt algorithm described in the DirectX documentation: zaxis = normal(At - Eye) xaxis = normal(cross(Up zaxis)) yaxis = cross(zaxis xaxis) xaxis.x yaxis.x zaxis.x 0 xaxis.y yaxis.y zaxis.y 0 xaxis.z yaxis.z zaxis.z 0 -dot(xaxis eye) -dot(yaxis eye) -dot(zaxis eye) l Now I get how it works on the rotation side but what I don't quite get is why it puts the translation component of the matrix to be those dot products. Examining it a bit it seems that it's adjusting the camera position by a small amount based on a projection of the new basis vectors onto the position of the eye/camera. The question is why does it need to do this? What does it accomplish? Take a read of the stuff on http://msdn.microsoft.com/en-au/library/bb206269(VS.85).aspx Note this is a [row major left-handed look at matrix](http://msdn.microsoft.com/en-us/library/bb205342(VS.85).aspx) Dot product simply projects a point to an axis to get the x- y- or z-component of the eye. You are moving the camera backwards so looking at (0 0 0) from (10 0 0) and from (100000 0 0) would have different effect.  A transformation 4x4 matrix contains two-three components: 1. rotation matrix 2. translation to add. 3. scale (many engine do not use this directly in the matrix). The combination of the them would transform a point from space A to Space B hence this is a transformation matrix M_ab Now the location of the camera is in space A and so it is not the valid transformation for space B so you need to multiply this location with the rotation transform. The only open question remains is why the dots? Well if you write the 3 dots on a paper you'd discover that 3 dots with X Y and Z is exactly like multiplication with a rotation matrix. An example for that forth row/column would be taking the zero point - (000) in world space. It is not the zero point in camera space and so you need to know what is the representation in camera space since rotation and scale leave it at zero! cheers  It is necessary to put the eye point in your axis space not in the world space. When you dot a vector with a coordinate unit basis vector one of the xyz it gives you the coordinates of the eye in that space. You transform location by applying the three translations in the last place in this case the last row. Then moving the eye backwards with a negative is equivalent to moving all the rest of the space forwards. Just like moving up in an elevator makes you feel lke the rest of the world is dropping out from underneath you. Using a left-handed matrix with translation as the last row instead of the last column is a religious difference which has absolutely nothing to do with the answer. However it is a dogma that should be strictly avoided. It is best to chain global-to-local (forward kinematic) transforms left-to-right in a natural reading order when drawing tree sketches. Using left-handed matrices forces you to write these right-to-left.  The lookat matrix does these two steps: Translate your model to the origin Rotate it according to the orientation set up by the up-vector and the looking direction. The dot product means simply that you make a translation first and then rotate. Instead of multiplying two matrices the dot product just multiplies a row with a column.  Just some general information: The lookat matrix is a matrix that positions / rotates something to point to (look at) a point in space from another point in space. The method takes a desired ""center"" of the cameras view an ""up"" vector which represents the direction ""up"" for the camera (up is almost always (010) but it doesn't have to be) and an ""eye"" vector which is the location of the camera. This is used mainly for the camera but can also be used for other techniques like shadows spotlights etc. Frankly I'm not entirely sure why the translation component is being set as it is in this method. In gluLookAt (from OpenGL) the translation component is set to 000 since the camera is viewed as being at 000 always.  I build a look-at matrix by creating a 3x3 rotation matrix as you have done here and then expanding it to a 4x4 with zeros and the single 1 in the bottom right corner. Then I build a 4x4 translation matrix using the negative eye point coordinates (no dot products) and multiply the two matrices together. My guess is that this multiplication yields the equivalent of the dot products in the bottom row of your example but I would need to work it out on paper to make sure. The 3D rotation transforms your axes. Therefore you cannot use the eye point directly without also transforming it into this new coordinate system. That's what the matrix multiplications -- or in this case the 3 dot-product values -- accomplish. Shouldn't you be creating a view matrix by calculating the inverse world matrix of the camera-orientation?  Note the example given is a left-handed row major matrix. So the operation is: Translate to the origin first (move by -eye) then rotate so that the vector from eye to At lines up with +z: Basically you get the same result if you pre-multiply the rotation matrix by a translation -eye:  [ 1 0 0 0 ] [ xaxis.x yaxis.x zaxis.x 0 ] [ 0 1 0 0 ] * [ xaxis.y yaxis.y zaxis.y 0 ] [ 0 0 1 0 ] [ xaxis.z yaxis.z zaxis.z 0 ] [ -eye.x -eye.y -eye.z 1 ] [ 0 0 0 1 ] [ xaxis.x yaxis.x zaxis.x 0 ] = [ xaxis.y yaxis.y zaxis.y 0 ] [ xaxis.z yaxis.z zaxis.z 0 ] [ dot(xaxis-eye) dot(yaxis-eye) dot(zaxis-eye) 1 ] Additional notes: Note that a viewing transformation is (intentionally) inverted: you multiply every vertex by this matrix to ""move the world"" so that the portion you want to see ends up in the canonical view volume. Also note that the rotation matrix (call it R) component of the LookAt matrix is an inverted change of basis matrix where the rows of R are the new basis vectors in terms of the old basis vectors (hence the variable names xaxis.x .. xaxis is the new x axis after the change of basis occurs). Because of the inversion however the rows and columns are transposed.  That translation component helps you by creating an orthonormal basis with your ""eye"" at the origin and everything else expressed in terms of that origin (your ""eye"") and the three axes. The concept isn't so much that the matrix is adjusting the camera position. Rather it is trying to simplify the math: when you want to render a picture of everything that you can see from your ""eye"" position it's easiest to pretend that your eye is the center of the universe. So the short answer is that this makes the math much easier. Answering the question in the comment: the reason you don't just subtract the ""eye"" position from everything has to do with the order of the operations. Think of it this way: once you are in the new frame of reference (i.e. the head position represented by xaxis yaxis and zaxis) you now want to express distances in terms of this new (rotated) frame of reference. That is why you use the dot product of the new axes with the eye position: that represents the same distance that things need to move but it uses the new coordinate system. So by my understanding the matrix is being set up with the correct translation okay but why the dot products in that computation? Couldn't it just have been -eye.x -eye.y -eye.z ?"
455,A,"Scaling vectors from a center point? I'm trying to figure out if I have points that make for example a square:  * * * * and let's say I know the center of this square. I want a formula that will make it for eample twice its size but from the center  * * * * * * * * Therefore the new shape is twice as large and from the center of the polygon. It has to work for any shape not just squares. I'm looking more for the theory behind it more than the implementation. To do what you want you need to perform three operations: translate the square so that its centroid coincides with the origin of the coordinate system scale the resulting square translate it back.  If you know the center point cp and a point v in the polygon you would like to scale by scale then: v2 = v - cp; // get a vector to v relative to the centerpoint v2_scaled = v2 * scale; // scale the cp-relative-vector v1_scaled = v2_scaled + cp; // translate the scaled vector back This translate-scale-translate pattern can be performed on vectors of any dimension.  I'm not sure there's a clean way to do this for all types of objects. For relatively simple ones you should be able to find the ""center"" as the average of all the X and Y values of the individual points. To double the size you find the length and angle of a vector from the center to the point. Double the length of the vector and retain the same angle to get the new point. Edit: of course ""twice the size"" is open to several interpretations (e.g. doubling the perimeter vs. doubling the area) These would change the multiplier used above but the basic algorithm would remain essentially the same.  If you want the shape twice as large scale the distance of the coordinates to be sqrt(2) times further from the center. In other words let's say your point is at (x y) and the center is (xcent ycent). Your new point should be at (xcent + sqrt(2)*(x - xcent) ycent + sqrt(2)*(y - ycent)) This will scale the distances from the new 'origin' (xcent ycent) in such a way that the area doubles. (Because sqrt(2)*sqrt(2) == 2)."
456,A,Measuring the pixel width of a string I need to measure the pixel width of a string in Cocoa Touch. Can anyone point me to a link that explains how to do this? You should not ask for a generic objective-c answer when this question is really about features in Cocoa for Mac OS X or Cocoa Touch for iPhone OS. Objective-C is just the programming language that both Cocoa and Cocoa Touch uses. Is there any other use of objective-c besides programming for Mac OS or IOS? If so is it significant? Take a look at the NSString Application Kit Additions Reference specifically boundingRectWithSize:options:attributes:. The value returned by that routine should give you the width of your NSString. The documentation is not very clear on how to actually use `boundingRectWithSize:options:attributes:`. It would be nice if you could add an example of how to actually use this method to measure a string with a particular font.  On iPhone OS it is slightly different instead look at the NSString UIKit Additions Reference. The idea is the same as in Cocoa for Mac OS X but there are more methods. For single lines of text use: - (CGSize)sizeWithFont:(UIFont *)font forWidth:(CGFloat)width lineBreakMode:(UILineBreakMode)lineBreakMode And for multiline texts use: - (CGSize)sizeWithFont:(UIFont *)font constrainedToSize:(CGSize)size lineBreakMode:(UILineBreakMode)lineBreakMode The use of a UILineBreakMode as argument for single lines of text can be confusing but this is because the line break is also used to define how to truncate the text. This seems to work fine for usual fonts but it doesn't seem to give the exact bounds for fonts such as Zapfino. Sample code is at http://paste.lisp.org/display/130803 and output screenshot is at http://ScrnSht.com/juixwk. Any idea why and how to solve it? deprecated in iOS7.. the docs don't refer to any replacement.. is there any? The current iOS7 docs (Xcode 5.0.1) do refer to replacements (which tend to be iOS7-only). lineBreakMode options now start with NS. e.g. NSLineBreakByWordWrapping
457,A,What's the best way to generate charts/graphs/data visualizations in Adobe Flex? I'm new to Flex but need to generate some visualizations of some time-based data. Any recommendations for tools that would work well in Flex? Something other that the builtin mx.charts?  Some very interesting charts and they have free-with-logo and pay-and-have-the-source versions is amCharts. They can be used directly in a web page feeding settings by XML and data by CSV (as services by javascript or whatever) and also inside Flex projects.  Be sure to keep an eye on a new project called Axiis that was recently announced by Tom Gonzalez a prominent Flex developer. It's a data visualization framework that should make custom charting very interesting.  I'm using the standard Flex Charting - which is extremely configurable - we've been able to make it look pretty much how we want it to... However also take a look at iLog Elixir - they provide some superb additional charts that you might find useful - particularly the 3D charts. The Radar chart can be very effective in some situations but you really need the RIGHT place to use such a thing. Also they have a good heatmap and pivot charts. Regards Jamie.  If you're willing to pay for it (I think it's in the USD $250 range) Flex Builder Pro is definitely the way to go. The graphing and charting power in the libraries is impressive. It's quite simple to generate nice looking complex graphs with not much coding.  Flex Builder Pro gives you access to the graph visualization library in mx.charts. If you want an open source solution you can try Flare.
458,A,"Mask white color out of PNG image on iPhone I know that there are already 2 relevant posts on that but it's not clear... So the case is this...: I have a UIImage which consists of a png file requested from a url on the net. Is it possible to mask out the white color so it becomes transparent? Everything I have tried so far with CGImageCreateWithMaskingColors returns a white image... Any help guys would be precious :) Ok since I found a solution and it's working fine I am answering my question and I really believe that this will be useful to many people having the same need. First of all I thought that would be nice to extend my UIImages via a category: UIImage+Utils.h #import <UIKit/UIKit.h> @interface UIImage (Utils) - (UIImage*)imageByClearingWhitePixels; @end UIImage+Utils.m #import ""UIImage+Utils.h"" @implementation UIImage (Utils) #pragma mark Private Methods - (CGImageRef) CopyImageAndAddAlphaChannel:(CGImageRef) sourceImage { CGImageRef retVal = NULL; size_t width = CGImageGetWidth(sourceImage); size_t height = CGImageGetHeight(sourceImage); CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB(); CGContextRef offscreenContext = CGBitmapContextCreate(NULL width height 8 0 colorSpace kCGImageAlphaPremultipliedFirst); if (offscreenContext != NULL) { CGContextDrawImage(offscreenContext CGRectMake(0 0 width height) sourceImage); retVal = CGBitmapContextCreateImage(offscreenContext); CGContextRelease(offscreenContext); } CGColorSpaceRelease(colorSpace); return retVal; } - (UIImage *) getMaskedArtworkFromPicture:(UIImage *)image withMask:(UIImage *)mask{ UIImage *maskedImage; CGImageRef imageRef = [self CopyImageAndAddAlphaChannel:image.CGImage]; CGImageRef maskRef = mask.CGImage; CGImageRef maskToApply = CGImageMaskCreate(CGImageGetWidth(maskRef)CGImageGetHeight(maskRef)CGImageGetBitsPerComponent(maskRef)CGImageGetBitsPerPixel(maskRef)CGImageGetBytesPerRow(maskRef)CGImageGetDataProvider(maskRef) NULL NO); CGImageRef masked = CGImageCreateWithMask(imageRef maskToApply); maskedImage = [UIImage imageWithCGImage:masked]; CGImageRelease(imageRef); CGImageRelease(maskToApply); CGImageRelease(masked); return maskedImage; } #pragma mark Public Methods - (UIImage*)imageByClearingWhitePixels{ //Copy image bitmaps float originalWidth = self.size.width; float originalHeight = self.size.height; CGSize newSize; newSize = CGSizeMake(originalWidth originalHeight); UIGraphicsBeginImageContext( newSize ); [self drawInRect:CGRectMake(00newSize.widthnewSize.height)]; UIImage* newImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); //Clear white color by masking with self newImage = [self getMaskedArtworkFromPicture:newImage withMask:newImage]; return newImage; } @end Finally I am able to use it like this: (After I have imported UIImage+Utils.h of course) UIImage *myImage = [...]; //A local png file or a file from a url UIImage *myWhitelessImage = [myImage imageByClearingWhitePixels]; // Hooray!  It is not really possible at least no easy way certainly no API call. It is basically an image editing problem usually done with Photoshop. Most images that have a transparent background were created that way."
459,A,Glass Effect - Artistic Effect I wish to give an effect to images where the resultant image would appear as if we are looking at it through a textured glass (not plain/smooth)... Please help me in writing an algo to generate such an effect. Here's an example of the type of effect I'm looking for The first image is the original image and the second image is the output im looking for. I can't offer you a specific example but the gamedev forums & articles sections have lots of gold for image processing 3d rendering and the like. For instance here is an article talking about using convolution matrices to apply similar effects to images that might be a good starting point.  Begin by creating a noise map with dimensions (width + 1) x (height + 1)that will be used displace the original image. I suggest using some sort of perlin noise so that the displacement is not to random. Here's a good link on how to generate perlin noise. Once we have the noise we can do something like this: Image noisemap; //size is (width + 1) x (height + 1) gray scale values in [0 255] range Image source; //source image Image destination; //destination image float displacementRadius = 10.0f; //Displacemnet amount in pixels for (int y = 0; y < source.height(); ++y) { for (int x = 0; x < source.width(); ++x) { const float n0 = float(noise.getValue(x y)) / 255.0f; const float n1 = float(noise.getValue(x + 1 y)) / 255.0f; const float n2 = float(noise.getValue(x y + 1)) / 255.0f; const int dx = int(floorf((n1 - n0) * displacementRadius + 0.5f)); const int dy = int(floorf((n2 - n0) * displacementRadius + 0.5f)); const int sx = std::min(std::max(x + dx 0) source.width() - 1); //Clamp const int sy = std::min(std::max(y + dy 0) source.height() - 1); //Clamp const Pixel& value = source.getValue(sx sy); destination.setValue(x y value); } } Thanks Andreas. This is exactly what I was looking for. Thanks Again
460,A,"How to use mouse drag to move triangle I draw a triangle using mouse drag. After drawing many different triangles I wanted to move some of the triangles into different location using mouse drag however I don't know how to drag the triangle to different location. Help please :=( Peter was right the last time you asked. (link) Here's some psudocode to elaborate on what he was talking about:  on mouse down: for each triangle in reverse order of their drawing if the pointer is within the triangle's area set the ""dragging"" state in a member variable add a reference to the triangle to a member variable record the mouse position in a member variable record the initial position of the triangle in a member variable break end if end for on mouse move: if currently in the ""dragging"" state move the triangle by the same amount the mouse has moved end if on mouse up: if currently in the ""dragging"" state move the triangle by the same amount as the mouse has moved exit the ""dragging"" state end if Thanks the problem solved now :-) everytimes I dragged the mouse new triangle drawn :-( I believe you're forgetting to repaint the background in your paint method. Either call super.paint(g) at the top of it or draw a big rectangle with Graphics::clearRect(xywh) before drawing your triangles."
461,A,"Algorithm for moving a point around an arbitrary non self-crossing closed polygon in N time I'm looking for an algorithm that would move a point around an arbitrary closed polygon that is not self-crossing in N time. For example move a point smoothly around a circle in 3 seconds. For the record a circle is not a polygon--it's the limit of a regular polygon as the number of sides go to infinity but it's not a polygon. What I'm giving you isn't going to work if you don't have defined points. Assuming you have your polygon stored in some format like a list of adjacent vertices do a O(n) check to calculate the perimeter by iterating through them and computing the distance between each point. Divide that by the time to get the velocity that you should travel. Now if you want to compute the path iterate back through your vertices again and calculate from your current position where your actual position should be on the next timestep whatever your refresh time step may be (if you need to move down a different edge calculate how much time it would take to get to the end of your first edge then continue on from there..). To travel along the edge decompose your velocity vector into its components (because you know the slope of the edge from its two endpoints). Oops good point! For some reason I forgot I already had the x/y coordinates and was assuming we just had the angle and total distance. Editing to fix. @DivineWolfwood I made the same mistake / assumption of about needing trigs or sq roots... Jefromi is right although it does get a bit dicey because the number of animation steps to travel a given edge is not always an integer. @Jefromi - you should write this as the answer... simple and concise and it's mostly all that is needed (though you should also normalize that paramaterization so all sides are traversed at constant speed). @mjv: Yeah the easiest way is going to be checking to see if you'll overshoot the end of the edge you're traversing: if you would overshoot instead reduce the length of the time step you're working in by however long it takes to get to the endpoint of the edge and then repeat the process on that next edge. Trigonometry? All the vector math you'll have done here will already be in coordinate form. And if you want to go from point `A = (x_1 y_1)` to point `B = (x_2 y_2)` in time `T` travel along the path `c(t) = A + (B-A)*t/T = (x_1 + (x_2-x_1)*t/T y_1 + (y_2-y_1)*t/T)`.  A little code might answer this with fewer words (though I'm probably too late for any votes here). Below is Python code that moves a point around a polygon at constant speed. from turtle import * import numpy as nx import time total_time = 10. # time in seconds step_time = .02 # time for graphics to make each step # define the polygone by the corner points # repeat the start point for a closed polygon p = nx.array([[0.0.] [0.200.] [50.150.] [200.200.] [200.0.] [0.0.]]) perim = sum([nx.sqrt(sum((p[i]-p[i+1])**2)) for i in range(len(p)-1)]) distance_per_step = (step_time/total_time)*perim seg_start = p[0] # segment start point goto(seg_start[0] seg_start[1]) # start the graphic at this point for i in range(len(p)-1): seg_end = p[i+1] # final point on the segment seg_len = nx.sqrt(sum((seg_start-seg_end)**2)) n_steps_this_segment = int(seg_len/distance_per_step) step = (seg_end-seg_start)/n_steps_this_segment # the vector step # last_point = seg_start for i in range(n_steps_this_segment): x = last_point + step goto(x[0] x[1]) last_point = x time.sleep(step_time) seg_start = seg_end Here I calculated the step size from the step_time (anticipating an graphics delay) but one could calculate the step size from whatever was needed for example the desired speed.   1) Calculate the polygon's perimeter (or estimate it if exact time of circling is not critical) 2) divide the perimieter by the time desired for circling 3) move point around polygon at this speed. Edit following ire and curses' comment. Maybe I read the question too literally I fail to see the difficulty or the points raised by ire_and_curses. The following describes more specifically the logic I imagine for step #3. A more exact description would require knowing details about the coordinate system the structure used to describe the polygon and indication abou the desired/allowed animation refreshing frequency. The ""travellng point"" which goes around the polygon would start on any edge of the polygop (maybe on a vertex as so to make the start and end point more obvious) and would stay on an edge at all time. From this starting point be it predetermined or randomly selected) the traveling point would move towards towards a vertex at the calculated speed. Once there it would go towards the other vertex of the edge it just arrived to and proceed till it returns to the starting point. The equations for calculating the points on a given edge are the same that for tracing a polygon: simple trig or even pythagoras (*). The visual effect is based on refreshing the position of the traveling point at least 15 times or so per second. The refresh frequency (or rather its period) can be used to determine the distance of two consecutive points of the animation. The only less trivial task is to detect the end of a given edge i.e. when the traveling point needs to ""turn"" to follow the next edge. On these occasions a fractional travel distance needs to be computed so that the next point in the animation is on the next edge. Special mention also for extremely short edges as these may require the fractional distance logic to be repeated (or done differently). Sorry for such a verbose explanation for a rather straight forward literally ;-) algorithm... Correction: as pointed out by Jefromi in comment for other response all that is needed with regard to the tracing is merely to decompose the x and y components of the motion. Although we do need Pythagoras for calculating the distance between each vertex for the perimeter calculation and we do need to extrapolate because the number of animation steps on an edge is not [necessarily] a integer. I'm sorry. I read the question differently and thought it referred to moving a point *within* a polygon. If you are restricted to the perimeter as you specify then your solution is fine. I can't tell what the OP intended but your reading sounds more plausible on reflection. I have retracted my -1 and given you +1 instead. Thanks for taking the time to provide the extra explanation. My apologies again. No apologies needed i&c these questions are often ambiguous. In a sense it was a good thing you down rep-ed me a bit as I find it very unfair that some of these easy questions get rewarded so much compared with ones that effectively challenge one's knowledge and imagination... -1: This is one-dimensional. How do you choose directions? What do you do if you hit an edge? How do you ensure you don't cross your own path? How can you cross your path when moving around a polygon ""that is *not self-crossing* ""? And how do you mean hit an edge? You're *on* the edges how can you speak of hitting one? @ire_and_cursres: [How do you choose directions?] If you can calculate the perimeter surely you can handle vectors between points..."
462,A,"Specify Width and Height of Plot I have a panel containing three plots. How can I use par to specify the width and height of the main panel so it is always at a fixed size? I usually set this at the start of my session with windows.options: windows.options(width=10 height=10) # plot away plot(...) If you need to reset to ""factory settings"": dev.off() windows.options(reset=TRUE) # more plotting plot(...) This is Windows specific I think.  You do that in the device e.g. x11(width=4 height=6) and similarly for the file-based ones pdf(""/tmp/foo.pdf"" width=4 height=6) You can read the physical size via par(""cin"") etc but not set it. For completeness `width` and `height` are specified in inch."
463,A,Why does sign matter in opengl projection matrix I'm working on a computer vision problem which requires rendering a 3d model using a calibrated camera. I'm writing a function that breaks the calibrated camera matrix into a modelview matrix and a projection matrix but I've run into an interesting phenomenon in opengl that defies explanation (at least by me). The short description is that negating the projection matrix results in nothing being rendered (at least in my experience). I would expect that multiplying the projection matrix by any scalar would have no effect because it transforms homogeneous coordinates which are unaffected by scaling. Below is my reasoning why I find this to be unexpected; maybe someone can point out where my reasoning is flawed. Imagine the following perspective projection matrix which gives correct results:  [ a b c 0 ] P = [ 0 d e 0 ] [ 0 0 f g ] [ 0 0 h 0 ] Multiplying this by camera coordinates gives homogeneous clip coordinates: [x_c] [ a b c 0 ] [X_e] [y_c] = [ 0 d e 0 ] * [Y_e] [z_c] [ 0 0 f g ] [Z_e] [w_c] [ 0 0 h 0 ] [W_e] Finally to get normalized device coordinates we divide x_c y_c and z_c by w_c: [x_n] [x_c/w_c] [y_n] = [y_c/w_c] [z_n] [z_c/w_c] Now if we negate P the resulting clip coordinates should be negated but since they are homogeneous coordinates multiplying by any scalar (e.g. -1) shouldn't have any affect on the resulting normalized device coordinates. However in openGl negating P results in nothing being rendered. I can multiply P by any non-negative scalar and get the exact same rendered results but as soon as I multiply by a negative scalar nothing renders. What is going on here?? Thanks! I just found this tidbit which makes progress toward an answer: From Red book appendix G: Avoid using negative w vertex coordinates and negative q texture coordinates. OpenGL might not clip such coordinates correctly and might make interpolation errors when shading primitives defined by such coordinates. Inverting the projection matrix will result in negative W clipping coordinate and apparently opengl doesn't like this. But can anyone explain WHY opengl doesn't handle this case? reference: http://glprogramming.com/red/appendixg.html  Well the gist of it is that clipping testing is done through: -w_c < x_c < w_c -w_c < y_c < w_c -w_c < z_c < w_c Multiplying by a negative value breaks this test. Nice answer! I assumed the clipping test would be: -1 < x_c / w_c < 1 but I guess opengl avoids a division operation at the expense of incorrect evaluation when w_c is negative.  Reasons I can think of: By inverting the projection matrix the coordinates will no longer be within your zNear and zFar planes of the view frustum (necessarily greater than 0). To create window coordinates the normalized device coordinates are translated/scaled by the viewport. So if you've used a negative scalar for the clip coordinates the normalized device coordinates (now inverted) translate the viewport to window coordinates that are... off of your window (to the left and below if you will) Also since you mentioned using a camera matrix and that you have inverted the projection matrix I have to ask... to which matrices are you applying what from the camera matrix? Operating on the projection matrix save near/far/fovy/aspect causes all sorts of problems in the depth buffer including anything that uses z (depth testing face culling etc). The OpenGL FAQ section on transformations has some more details. After converting from homogeneous to non-homogeneous coordinates (i.e. by dividing by w_c) the coordinates do still fall between zNear and zFar  because the negative sign of w_c cancels out the inverted sign of {xyz}_c. so I think this is not it. For the same reason the normalized device coordinates are in fact not inverted either. I'm only using the projection matrix for intrinsic camera parameters (fovy aspect axis skew etc); the extrinsic parameters are going into the modelview matrix. So I think this isn't the problem either. I don't know driver implementation details but I'm guessing that is actually not the case for zNear and zFar. Clip testing is likely not range but depth and therefore the numeric value is tested to be greater than zNear (hense the positive requirement in the first place). If the entire coordinate system and the matrix is inverted the test fails as bahbar pointed out.
464,A,"How to change the background of an image with transparent pixels? The exact background color is known. Imagine an anti-aliased font on a green background. Now we want to change the background color of the image to red. So.. How do I extract the ""non-background"" portion of the color from the pixel and add it (or ""blend"") on top of another bg color afterwards? We also know the font color so we know which pixels are fully opaque. It would seem the answer will depend on language and how you want to make the change. For example do you want to change it real-time right after it is downloaded to a page? Or just use photoshop and change it one time? In my WPF app will the background change based on how the application feels (no errors it is green lots of errors angry red :) Every pixel in the image is either the background color the font color or some blend of the two. So you could specify the whole image with the background RGB values the font RGB values and an array of values between 0 and 1 where say 0 means the pixel is the background color 1 means the pixel is the font color. A value like 0.1 means that the pixel is mostly background color with a little font color etc. An expression like: blend_value = ((pixel.R - bg.R) / (font.R - bg.R) + (pixel.G - bg.G) / (font.G - bg.G) + (pixel.B - bg.B) / (font.B - bg.B)) / 3 will give you these values for any pixel in the image. To construct the new RGB values using a new background color you use this value to blend the font color with the new background color. new_pixel.R = blend_value * font.R + (1 - blend_value) * new_bg.R new_pixel.G = blend_value * font.G + (1 - blend_value) * new_bg.G new_pixel.B = blend_value * font.B + (1 - blend_value) * new_bg.B"
465,A,line is erasing in java I make an Application in java and draw rectangle. When I search from combobox the rectangle is under the combobox and when I close combobox some parts of the rectangle which was under the combobox is deleted. How can I make rectangle to be visible after the combobox was closed. repaint the panel in which the rectangle is visible  Suggest you read this tutorial. As suggested there call the super.paint(g) after your own painting.
466,A,"Defining Up in the Direct3D View Matrix when Camera Is Constantly Moving In my Direct3D application the camera can be moved using the mouse or arrow keys. But if I hard code (010) as the up direction vector in LookAtLH the frame goes blank at some orientations of the camera. I just learned the hard way that when looking along the Y-axis (010) no longer works as the Up direction (seems obvious?). I am thinking of switching my up direction to something else for each of these special cases. Is there a more graceful way to handle this? Assuming you can calculate a vector pointing forward (what you are looking at - your position) and a vector pointing right (always on the XZ-plane unless you can roll). Normalize both these vectors then up is forward x right (where x is cross product). In general you can plug in your yaw pitch and roll into a rotation matrix and rotate the axis vectors to get right up and forward but I guess that's what you are using LookAtLH to avoid. See http://en.wikipedia.org/wiki/Rotation_matrix#The_3-dimensional_rotation_matricies  The graceful way to handle this is to use Unit Quaternions. A quaternion is a vector of 4 values that encodes an orientation in 3D space (not a rotation as some articles assert) and a unit quaternion is one where the vector length sqrt(x^2+y^2+z^2+w^2) is 1.0. There are a set of mathematical operations for working with quaternions that are analogous to using matrices to encode rotations with the added bonus that quaternions can never represent an degenerate orientation. You can freely convert quaternions to a 3x3 or 4x4 matrix when you need to feed the result to a GPU. Your problem is that while you are moving your camera you will introduce a little twist into the camera's up direction. By forcing the camera to re-center itself on the (010) vector every iteration you are in effect rotating the camera and then clamping the camera's orientation to remain on the surface of a sphere but when your camera hits the pole of this sphere there is no good direction to call ""up"" and your matrix goes singular and gives you zero-sized polygons (hence the black screen). Quaternions have the ability to interpolate through these poles and come out the other side just fine leaving you with a valid matrix at all times. all you have to do is control the ""twist"". To measure this twist you should read Ken Shoemake's article ""Fiber Bundle Twist Reduction"" in the book Graphics Gems 4. He shows a good way to measure this accumulated twist and how to remove it when it is offensive. The solution I am using presently is this: i) initially define up as (010) or something suitable and store this in a vector ii) when the user changes the camera orientation rotate the stored vector correspondingly and set it as the new up vector"
467,A,"How to draw ""text"" using GWT-Graphics I am using GWT-Graphics to create shapes like rectangle circles etc... Now i am trying to add text to these shapes. Here is how the code looks like: DrawingArea d1 = new DrawingArea(100 100); Ellipse e = new Ellipse(29 20 30 20); Text t = new Text(10 20 ""A""); d1.add(e); d1.add(t); boundaryPanel.add(d1 200 40 ); But when i run the program i am getting the following Error: [ERROR] Uncaught exception escaped java.lang.AssertionError: The style name 'v-text-align' should be in camelCase format at com.google.gwt.dom.client.Style$.assertCamelCase$(Style.java:63) at com.google.gwt.dom.client.Style$.setProperty$(Style.java:42) at com.vaadin.contrib.gwtgraphics.client.impl.VMLImpl.createElement(VMLImpl.java:101) at com.vaadin.contrib.gwtgraphics.client.VectorObject.<init>(VectorObject.java:37) at com.vaadin.contrib.gwtgraphics.client.Shape.<init>(Shape.java:27) at com.vaadin.contrib.gwtgraphics.client.shape.Text.<init>(Text.java:25) at com.e.r.d.client.ERD1$2.onClick(ERD1.java:74) at com.google.gwt.event.dom.client.ClickEvent.dispatch(ClickEvent.java:54) at com.google.gwt.event.dom.client.ClickEvent.dispatch(ClickEvent.java:1) at com.google.gwt.event.shared.HandlerManager$HandlerRegistry.fireEvent(HandlerManager.java:65) at com.google.gwt.event.shared.HandlerManager$HandlerRegistry.access$1(HandlerManager.java:53) at com.google.gwt.event.shared.HandlerManager.fireEvent(HandlerManager.java:178) at com.google.gwt.user.client.ui.Widget.fireEvent(Widget.java:52) at com.google.gwt.event.dom.client.DomEvent.fireNativeEvent(DomEvent.java:116) at com.google.gwt.user.client.ui.Widget.onBrowserEvent(Widget.java:90) at com.google.gwt.user.client.DOM.dispatchEventImpl(DOM.java:1320) at com.google.gwt.user.client.DOM.dispatchEventAndCatch(DOM.java:1299) at com.google.gwt.user.client.DOM.dispatchEvent(DOM.java:1262) at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at com.google.gwt.dev.shell.MethodAdaptor.invoke(MethodAdaptor.java:103) at com.google.gwt.dev.shell.ie.IDispatchImpl.callMethod(IDispatchImpl.java:126) at com.google.gwt.dev.shell.ie.IDispatchProxy.invoke(IDispatchProxy.java:155) at com.google.gwt.dev.shell.ie.IDispatchImpl.Invoke(IDispatchImpl.java:294) at com.google.gwt.dev.shell.ie.IDispatchImpl.method6(IDispatchImpl.java:194) at org.eclipse.swt.internal.ole.win32.COMObject.callback6(COMObject.java:117) at org.eclipse.swt.internal.win32.OS.DispatchMessageW(Native Method) at org.eclipse.swt.internal.win32.OS.DispatchMessage(OS.java:1925) at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:2966) at com.google.gwt.dev.SwtHostedModeBase.processEvents(SwtHostedModeBase.java:264) at com.google.gwt.dev.HostedModeBase.pumpEventLoop(HostedModeBase.java:557) at com.google.gwt.dev.HostedModeBase.run(HostedModeBase.java:405) at com.google.gwt.dev.HostedMode.main(HostedMode.java:232) Any input on this will be of great help. Thank you. This issue should be fixed in the newest version of GWT Graphics. Download this version 0.9.3 from here. Thank you it works. It works only in web mode. In hosted mode only a line appears nothing else."
468,A,"How to compile Cairo for Visual C++ 2008 (Express edition) Most precompiled Windows binaries are made with the MSYS+gcc toolchain. It uses MSVCRT runtime which is incompatible with Visual C++ 2005/2008. So how to go about and compile Cairo 1.6.4 (or later) for Visual C++ only. Including dependencies (pngzlibpixman). I ran into two problems when building on Windows (Visual Studio 2008 GNU Make 3.81): Invalid ""if"" constructs in src/Makefile.sources. Fixed that using sed ""s/^if \([A-Z_]*\)$/ifeq ($(\1) 1)/"" src\Makefile.sources _lround is not available on Windows/MSVC. Worked around that using sed ""s/#define _cairo_lround lround/static inline long cairo_const _cairo_lround(double r) { return (long)floor(r + .5); }/""` (which is probably a poor fix) These issues aside everything works great (for both x86 and x86_64 architectures).  Here are instructions for building Cairo/Cairomm with Visual C++. Required: Visual C++ 2008 Express SP1 (now includes SDK) MSYS 1.0 To use VC++ command line tools a batch file 'vcvars32.bat' needs to be run.  C:\Program Files\Microsoft Visual Studio 9.0\Common7\Tools\vcvars32.bat ZLib Download (and extract) zlib123.zip from http://www.zlib.net/  cd zlib123 nmake /f win32/Makefile.msc dir # zlib.lib is the static library # # zdll.lib is the import library for zlib1.dll # zlib1.dll is the shared library libpng Download (and extract) lpng1231.zip from http://www.libpng.org/pub/png/libpng.html The VC++ 9.0 compiler gives loads of ""this might be unsafe"" warnings. Ignore them; this is MS security panic (the code is good).  cd lpng1231\lpng1231 # for some reason this is two stories deep nmake /f ../../lpng1231.nmake ZLIB_PATH=../zlib123 dir # libpng.lib is the static library # # dll is not being created Pixman Pixman is part of Cairo but a separate download. Download (and extract) pixman-0.12.0.tar.gz from http://www.cairographics.org/releases/ Use MSYS to untar via 'tar -xvzf pixman*.tar.gz' Both Pixman and Cairo have Makefiles for Visual C++ command line compiler (cl) but they use Gnu makefile and Unix-like tools (sed etc.). This means we have to run the make from within MSYS. Open a command prompt with VC++ command line tools enabled (try 'cl /?'). Turn that command prompt into an MSYS prompt by 'C:\MSYS\1.0\MSYS.BAT'. DO NOT use the MSYS icon because then your prompt will now know of VC++. You cannot run .bat files from MSYS. Try that VC++ tools work from here: 'cl -?' Try that Gnu make also works: 'make -v'. Cool.  cd (use /d/... instead of D:) cd pixman-0.12.0/pixman make -f Makefile.win32 This defaults to MMX and SSE2 optimizations which require a newish x86 processor (Pentium 4 or Pentium M or above: http://fi.wikipedia.org/wiki/SSE2 ) There's quite some warnings but it seems to succeed.  ls release # pixman-1.lib (static lib required by Cairo) Stay in the VC++ spiced MSYS prompt for also Cairo to compile. cairo Download (and extract) cairo-1.6.4.tar.gz from http://www.cairographics.org/releases/  cd cd cairo-1.6.4 The Makefile.win32 here is almost good but has the Pixman path hardwired. Use the modified 'Makefile-cairo.win32':  make -f ../Makefile-cairo.win32 CFG=release \ PIXMAN_PATH=../../pixman-0.12.0 \ LIBPNG_PATH=../../lpng1231 \ ZLIB_PATH=../../zlib123 (Write everything on one line ignoring the backslashes) It says ""no rule to make 'src/cairo-features.h'. Use the manually prepared one (in Cairo > 1.6.4 there may be a 'src/cairo-features-win32.h' that you can simply rename):  cp ../cairo-features.h src/ Retry the make command (arrow up remembers it).  ls src/release # # cairo-static.lib cairomm (C++ API) Download (and extract) cairomm-1.6.4.tar.gz from http://www.cairographics.org/releases/ There is a Visual C++ 2005 Project that we can use (via open & upgrade) for 2008.  cairomm-1.6.4\MSCV_Net2005\cairomm\cairomm.vcproj Changes that need to be done: Change active configuration to ""Release"" Cairomm-1.0 properties (with right click menu)  C++/General/Additional Include Directories: ..\..\..\cairo-1.6.4\src (append to existing) Linker/General/Additional library directories: ..\..\..\cairo-1.6.4\src\release ..\..\..\lpng1231\lpng1231 ..\..\..\zlib123 Linker/Input/Additional dependencies: cairo-static.lib libpng.lib zlib.lib msimg32.lib Optimization: fast FPU code  C++/Code generation/Floating point model Fast Right click on 'cairomm-1.0' and 'build'. There are some warnings.  dir cairomm-1.6.4\MSVC_Net2005\cairomm\Release # # cairomm-1.0.lib # cairomm-1.0.dll # cairomm.def  MSYS+gcc toolchain uses the old MSVCRT runtime library (now built into Windows) and Visual C++ 2005/2008 bring their own. It is a known fact that code should not depend on multiple runtimes. Passing things s.a. file handles memory pointers etc. will be affected and will cause apparently random crashes in such scenario. I have not been bitted by this. Then again I don't really target Windows any more either. But I've been told enough to not even try the solution. What could have worked is linking all the dependencies statically into the lib (say Cairomm). Static libs don't have a runtime bound to them do they? But I did not try this. I actually got the VC++ building of all ingredients to work but it took days. I hadn't found the URL you give. Strange in itself; I looked 'everywhere'. Then again it is for Visual Studio 2003.NET so two generations behind already.  The instructions don't seem to work with current version of imlib I wonder if it's worth reasking this question ?  Did you check here: http://cairographics.org/visualstudio/ ? What do you mean 'It uses MSCVRT runtime which is incompatible with Visual C++ 2005/2008' ? What are the exact problems you're having?  I have done this but I don't have any ready-written instructions. My builds are also rather minimal as I haven't needed support for eg. PNG and SVG files I just used it to render generated vector graphics to memory buffers. But what I did was read through the config.h and other files for the UNIX/GNU build system and write my own suited for MSVC and then create a project with the appropriate source files. It probably takes a few hours at best to do this but when you're done it just works ;) Edit: Do see this page it has an MSVC 2003 (7.1) project for building cairo: http://slinavlee.googlepages.com/"
469,A,"How do I position one image on top of another in HTML? I'm a beginning rails programmer attempting to show many images on a page. Some images are to lay on top of others. To make it simple say I want a blue square with a red square in the upper right corner of the blue square (but not tight in the corner). I am trying to avoid compositing (with ImageMagick and similar) due to perfomance issues. I just want to position overlapping images relative to one another. As a more difficult example imagine an odometer placed inside a larger image. For six digits I would need to composite a million different images or do it all on the fly where all that is needed is to place the six images on top of the other one. you could do the odometer with one image using sprites Inline CSS only for clarity here. Use a real stylesheet. <!-- First your background image is a DIV with a background image style applied not a IMG tag. --> <div style=""background-image:(url=YourBackgroundImage);""> <!-- Second create a placeholder div to assist in positioning the other images. This is relative to the background div. --> <div style=""position: relative; left: 0; top: 0;""> <!-- Now you can place your IMG tags and position them relative To the container we just made --> <img src=""YourForegroundImage"" style=""position: relative; top: 0; left: 0;""/> </div> </div>  This is a barebones look at what I've done to float one image over another. <style type=""text/css""> .imgA1 { position:absolute; top: 25px; left: 25px; z-index: 1; } .imgB1 { position:absolute; top: 25px; left: 25px; z-index: 3; } </style> <img class=imgA1 src=""http://full.path.to/imageA.jpg""> <img class=imgB1 src=""http://full.path.to/imageB.jpg""> Source  Ok after some time here's what I landed on: <div style=""position: relative; left: 0; top: 0;""> <img src=""a.jpg"" style=""position: relative; top: 0; left: 0;""/> <img src=""b.jpg"" style=""position: absolute; top: 30px; left: 70px;""/> </div> As the simplest solution. That is: Create a relative div that is placed in the flow of the page; place the base image first as relative so that the div knows how big it should be; place the overlays as absolutes relative to the upper left of the first image. The trick is to get the relatives and absolutes correct. I like this approach as it allows relative positioning. Worked great for me. Thanks for posting your solution. This solved a problem of mine perfectly. such an elegant solutions. wish i could give 10 upvotes  One issue I noticed that could cause errors is that in rrichter's answer the code below: <img src=""b.jpg"" style=""position: absolute; top: 30; left: 70;""/> should include the px units within the style eg. <img src=""b.jpg"" style=""position: absolute; top: 30px; left: 70px;""/> Other than that the answer worked fine. Thanks. This detail solved my problem +1 to this. I've editted Rrichters answer to include the px  You can absolutely position pseudo elements relative to their parent element. This gives you two extra layers to play with for every element - so positioning one image on top of another becomes easy - with minimal and semantic markup (no empty divs etc). markup: <div class=""overlap""></div> css: .overlap { width: 100px; height: 100px; position: relative; background-color: blue; } .overlap:after { content: ''; position: absolute; width: 20px; height: 20px; top: 5px; left: 5px; background-color: red; } Here's a LIVE DEMO +1 for demooooo  @buti-oxa: Not to be pedantic but your code is invalid. The HTML width and height attributes do not allow for units; you're likely thinking of the CSS width: and height: properties. You should also provide a content-type (text/css; see Espo's code) with the <style> tag. <style type=""text/css""> .containerdiv { float: left; position: relative; } .cornerimage { position: absolute; top: 0; right: 0; } </style> <div class=""containerdiv""> <img border=""0"" src=""http://www.gravatar.com/avatar/"" alt="""" width=""100"" height=""100""> <img class=""cornerimage"" border=""0"" src=""http://www.gravatar.com/avatar/"" alt="""" width=""40"" height=""40""> <div> Leaving px; in the width and height attributes might cause a rendering engine to balk. Thanks fixed. Actually I deleted the content-type I had is my test code before posting as un unnecessary detail. Style declaration should be inside anyway and so on. Maybe I should have leaved it there.  Here's code that may give you ideas: <style> .containerdiv { float: left; position: relative; } .cornerimage { position: absolute; top: 0; right: 0; } </style> <div class=""containerdiv""> <img border=""0"" src=""http://stackoverflow.com/Content/Img/stackoverflow-logo-250.png"" alt=""""""> <img class=""cornerimage"" border=""0"" src=""http://www.gravatar.com/avatar/"" alt=""""> <div> I suspect that Espo's solution may be inconvenient because it requires you to position both images absolutely. You may want the first one to position itself in the flow. Usually there is a natural way to do that is CSS. You put position: relative on the container element and then absolutely position children inside it. Unfortunately you cannot put one image inside another. That's why I needed container div. Notice that I made it a float to make it autofit to its contents. Making it display: inline-block should theoretically work as well but browser support is poor there. EDIT: I deleted size attributes from the images to illustrate my point better. If you want the container image to have its default sizes and you don't know the size beforehand you cannot use the background trick. If you do it is a better way to go. This is the best solution for me because there is no need to specify width and height of your images and this will result in a much more re-useability.  The easy way to do it is to use background-image then just put an <img> in that element. The other way to do is using css layers. There is a ton a resources available to help you with this just search for css layers. Great work bro....(Y)"
470,A,"How to create an animated tiled background from a texture atlas with Cocos2d I want to create a CCLayer with an animated tiled background from a larger texture atlas with Cocos2d. I know how to drop a background in a CCLayer. I know how to create an animated CCSprite. I even know how to handle tiled world maps. But I can't find a proper way to combine all these elements in the desired form. How would I do this? Example case: Let's say I have a 512x512 texture atlas. On it are six frames drawn next to each other all of them 32x32 starting from the top left (0.0). Now I would like my CCLayer's background to display a tiled image consisting of 10 by 15 tiles. Plus the tiles themselves should animate with the six frames from the texture atlas. As a bonus the animation itself should be controllable. (I want to be able to speed it up slow it down or reverse it) You should be able to get the sprite from the tile map CCTMXLayer *layer = [map layerNamed:@""Layer""]; CCSprite *tile = [layer tileAt:ccp(x y)]; and run a CCAnimation action on it. It helped. Thanks. :) Could you post an example of the CCAnimation code. I have not got this to work. Thanks"
471,A,"Change the alpha value of a BufferedImage? How do I change the global alpha value of a BufferedImage in Java? (I.E. make every pixel in the image that has a alpha value of 100 have a alpha value of 80) for a nicer looking alpha change effect you can use relative alpha change per pixel (rather than static set or clipping linear) public static void modAlpha(BufferedImage modMe double modAmount) { // for (int x = 0; x < modMe.getWidth(); x++) { for (int y = 0; y < modMe.getHeight(); y++) { // int argb = modMe.getRGB(x y); //always returns TYPE_INT_ARGB int alpha = (argb >> 24) & 0xff; //isolate alpha alpha *= modAmount; //similar distortion to tape saturation (has scrunching effect eliminates clipping) alpha &= 0xff; //keeps alpha in 0-255 range argb &= 0x00ffffff; //remove old alpha info argb |= (alpha << 24); //add new alpha info modMe.setRGB(x y argb); } } }  @Neil Coffey: Thanks I've been looking for this too; however Your code didn't work very well for me (white background became black). I coded something like this and it works perfectly: public void setAlpha(byte alpha) { alpha %= 0xff; for (int cx=0;cx<obj_img.getWidth();cx++) { for (int cy=0;cy<obj_img.getHeight();cy++) { int color = obj_img.getRGB(cx cy); int mc = (alpha << 24) | 0x00ffffff; int newcolor = color & mc; obj_img.setRGB(cx cy newcolor); } } } Where obj_img is BufferedImage.TYPE_INT_ARGB. I change alpha with setAlpha((byte)125); alpha range is now 0-255. Hope someone finds this useful. Looks good. How portable is it? E.g. are the RGB values guaranteed to be held in the lowest 3 bytes of the `color` int?  This is an old question so I'm not answering for the sake of the OP but for those like me who find this question later. AlphaComposite As @Michael's excellent outline mentioned an AlphaComposite operation can modify the alpha channel. But only in certain ways which to me are somewhat difficult to understand: is the formula for how the ""over"" operation affects the alpha channel. Moreover this affects the RGB channels too so if you have color data that needs to be unchanged AlphaComposite is not the answer. BufferedImageOps LookupOp There are several varieties of BufferedImageOp (see 4.10.6 here). In the more general case the OP's task could be met by a LookupOp which requires building lookup arrays. To modify only the alpha channel supply an identity array (an array where table[i] = i) for the RGB channels and a separate array for the alpha channel. Populate the latter array with table[i] = f(i) where f() is the function by which you want to map from old alpha value to new. E.g. if you want to ""make every pixel in the image that has a alpha value of 100 have a alpha value of 80"" set table[100] = 80. (The full range is 0 to 255.) See how to increase opacity in gaussian blur for a code sample. RescaleOp But for a subset of these cases there is a simpler way to do it that doesn't require setting up a lookup table. If f() is a simple linear function use a RescaleOp. For example if you want to set newAlpha = oldAlpha - 20 use a RescaleOp with a scaleFactor of 1 and an offset of -20. If you want to set newAlpha = oldAlpha * 0.8 use a scaleFactor of 0.8 and an offset of 0. In either case you again have to provide dummy scaleFactors and offsets for the RGB channels: new RescaleOp({1.0f 1.0f 1.0f /* alpha scaleFactor */ 0.8f} {0f 0f 0f /* alpha offset */ -20f} null) Again see 4.10.6 here for some examples that illustrate the principles well but are not specific to the alpha channel. Both RescaleOp and LookupOp allow modifying a BufferedImage in-place.  I don't believe there's a single simple command to do this. A few options: copy into another image with an AlphaComposite specified (downside: not converted in place) directly manipulate the raster (downside: can lead to unmanaged images) use a filter or BufferedImageOp The first is the simplest to implement IMO.  I'm 99% sure the methods that claim to deal with an ""RGB"" value packed into an int actually deal with ARGB. So you ought to be able to do something like: for (all xy values of image) { int argb = img.getRGB(x y); int oldAlpha = (argb >>> 24); if (oldAlpha == 100) { argb = (80 << 24) | (argb & 0xffffff); img.setRGB(x y argb); } } For speed you could maybe use the methods to retrieve blocks of pixel values."
472,A,c++ Having multiple graphics options Currently my app uses just Direct3D9 for graphics however in the future I' m planning to extend this to D3D10 and possibly OpenGL. The question is how can I do this in a tidy way? At present there are various Render methods in my code void Render(boost::function<void()> &Call) { D3dDevice->BeginScene(); Call(); D3dDevice->EndScene(); D3dDevice->Present(0000); } The function passed then depends on the exact state eg MainMenu->Render Loading->Render etc. These will then oftern call the methods of other objects. void RenderGame() { for(entity::iterator it = entity::instances.begin();it != entity::instance.end(); ++it) (*it)->Render(); UI->Render(); } And a sample class derived from entity::Base class Sprite : public Base { IDirect3DTexture9 *Tex; Point2 Pos; Size2 Size; public: Sprite(IDirect3DTexture9 *Tex const Point2 &Pos const Size2 &Size); virtual void Render(); }; Each method then takes care of how best to render given the more detailed settings (eg are pixel shaders supported or not). The problem is I'm really not sure how to extend this to be able to use one of what may be somewhat diffrent (D3D v OpenGL) render modes... I'd say if you want a really complete answer go look at the source code for Ogre3D. They have both D3D and OpenGL back ends. http://www.ogre3d.org Basically their API kind of forces you into working in a D3D-ish way creating buffer objects and stuffing them with data then issuing draw calls on those buffers. That's the way the hardware likes it anyway so it's not a bad way to go. And then once you see how they do things you might as well just just go ahead and use it and save yourself the trouble of having to re-implement all that it already provides. :-)  Define an interface that is sufficient for your application's graphic output demands. Then implement this interface for every renderer you want to support. class IRenderer { public: virtual ~IRenderer() {} virtual void RenderModel(CModel* model) = 0; virtual void DrawScreenQuad(int x1 int y1 int x2 int y2) = 0; // ...etc... }; class COpenGLRenderer : public IRenderer { public: virtual void RenderModel(CModel* model) { // render model using OpenGL } virtual void DrawScreenQuad(int x1 int y1 int x2 int y2) { // draw screen aligned quad using OpenGL } }; class CDirect3DRenderer : public IRenderer { // similar but render using Direct3D }; Properly designing and maintaining these interfaces can be very challenging though. In case you also operate with render driver dependent objects like textures you can use a factory pattern to have the separate renderers each create their own implementation of e.g. ITexture using a factory method in IRenderer: class IRenderer { //... virtual ITexture* CreateTexture(const char* filename) = 0; //... }; class COpenGLRenderer : public IRenderer { //... virtual ITexture* CreateTexture(const char* filename) { // COpenGLTexture is the OpenGL specific ITexture implementation return new COpenGLTexture(filename); } //... }; Isn't it an idea to look at existing (3d) engines though? In my experience designing this kind of interfaces really distracts from what you actually want to make :) Because I already have the d3d9 stuff for everything I need I just want the option to extend to other API's. Since I already have classes wrapping most d3d stuff (eg Texture Sprite Mesh) I could just overload those leaving the actaul objects with API independent stuff.
473,A,Where can I get free Vista style developer graphics? What is the best source of free Vista style graphics for application development? I want 32x32 and 16x16 that I can use in a Winforms application. If you're using Visual Studio Professional or above you've got a zip file of icons in your VS path under Common7\VS2008ImageLibrary. Some of the images use the Vista style.  The Tango project has some good icons For areas that only need 16x16 the silk icons from famfamfam are good too Both are Creative Commons licensed  Best place I've found for commercial toolbar icons etc is glyfx.com.
474,A,Java AffineTransform moving origin I would like to move the origin from top left to bottom middle of the component? I have been playing with AffineTransform class could not get it to work? You will need the height and width of the component that you are trying to draw. Assuming you are in the paint(Graphics g) method the simplest way is: paint(Graphics g){ Graphics2D g2 = (Graphics2D)g; g2.translate( component.getWidth()/2.0 component.getHeight()/2.0); //... } this has moved the origin but everything is still upside down. solved. thank you.
475,A,Painting a Canvas in an Applet I currently have a small Java program which I would like to run both on the desktop (ie in a JFrame) and in an applet. Currently all of the drawing and logic are handled by a class extending Canvas. This gives me a very nice main method for the Desktop application: public static void main(String[] args) { MyCanvas canvas = new MyCanvas(); JFrame frame = MyCanvas.frameCanvas(canvas); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); canvas.loop(); } Can I do something similar for the applet? Ideally MyCanvas would remain the same for both cases. Not sure if its important but I am drawing using BufferStrategy with setIgnoreRepaint(true). Edit: To clarify my issue seems to be painting the canvas -- since all the painting is being done from the canvas.loop() call. Why don't you use Java Web Start or JavaFX? Applets are so 90' Well I've already been developing the app for a little while in Java... so JavaFX doesn't really help me much given that I would have to rewrite what I already have :) Didn't really consider web start but I would like the thing to run in browser if at all possible. Generally the way you would have an application that is also an applet is to have your entry point class extend Applet and have its setup add the Canvas to itself etc. etc. Then in the main method version you just instantiate your Applet class and add it to a new Frame (or JApplet / JFrame etc.). See here and here for examples of the technique which essentially boils down to (from the first example):  public static void main(String args[]) { Applet applet = new AppletApplication(); Frame frame = new Frame(); frame.addWindowListener(new WindowAdapter() { public void windowClosing(WindowEvent e) { System.exit(0); } }); frame.add(applet); frame.setSize(IDEAL_WIDTHIDEAL_HEIGHT); frame.show(); } I will look into this.. thanks!  Applet is a Container just add your Canvas there.  Canvas is inappropriate for adding to Swing components. Use JComponent instead (and setOpaque(true)). Swing components should always be manipulated on the AWT Event Dispatch Thread (EDT). Use java.awt.EventQueue.invokeLater (invokeAndWait for applets). You shouldn't do any blocking operations from the EDT so start your own threads for that. By default you are running in the main thread (or applet thread for applets) which is quite separate from the EDT. I suggest removing any dependence from your MyCanvas to JFrame. I also suggest keeping code for applications using frame separate from that using applets. Adding a component to a JApplet is the same as for JFrame (in both cases there is shenanigans where actually what happens is that add actually calls getContentPane().add which can cause some unnecessary confusion). The main difference is that you can't pack an applet. Assuming you use `JApplet` (which is a Swing component) rather than `Applet` the only way to use a `BufferStrategy` in the applet is to add a `Canvas` to it.
476,A,How to represent 18bit color depth to 16bit color depth? I'm porting a software that build from 16bit color depth to 18bit color depth. How can I convert the 16-bit colors to 18-bit colors? Thanks. Please do not create duplicate questions http://stackoverflow.com/questions/1056921/how-to-represent-from-16bit-color-depth-to-18bit-color-depth Assuming that the 16bit color is in 5-6-5 format: // RRRRR-GGGGGG-BBBBB 16bit --> //RRRRR0-GGGGGG-BBBBB0 18bit with the formula below: Color18 = ((Color16 & 0xF800) << 2) | ((Color16 & 0x07E0) << 1) | ((Color16 & 0x1F) << 1); Color18 = ((Color16 & 0xf800) + Color16) << 1 gives the same result and is significantly shorter @Accipitridae I wanted to be obvious (is it? anyway :) how I *pick* the R-G-B values and how I transform them. But probably I should had post also a shorter version of that formula. Thanks for mentioning it.  I don't see this covered in the other answers but you want the conversion from 5 to 6 bits to be such that binary 11111 gets mapped to 111111 since both represent full on. One way to do that is to replicate the leading bit(s). red6 = (red5 << 1) | (red5 >> 4); If you use an arithmetic conversion keep in mind that the maximum values are 31 and 63 not 32 and 64. red6 = (red5 * 63 + 15) / 31; // all ints +15 is for rounding If you care about other intermediate values such as if you want 16 mapped 32 either tweak the formula (e.g. change 15 to 14) or just make a table.  Without knowing the device I can only speculate. Devices are typically Red Green Blue so each color would get 6 bits of variation. That means 64 variations of each color and a total of 262144 colors. Any bitmap can be scaled to this display. If you take each component (say red) normalize it then multiply by 64 you'll have the scaled version. If you are asking something else or want more detail please ask. Update: There are two 16-bit bitmap formats. One is 5-5-5 (5 bits per pixel) and the other is 5-6-5 (green gets an extra bit). I'll assume 5-5-5 for the sake of this conversion. For each color within each pixel you need something like this: NewColor = (oldColor/32.0)*64 This will turn the old color into a number between 0.0 and 1.0 and then scale it up to the new value range. Hi sir thanks for the info. How can I convert from 16-bit representation of the bitmap to 18-bit representation? Just set the upper-most bit's to 0? :/ If the answer we're commenting on is correct that will mean we only get 4 bits of red :( That would work kinda. All of your colors would get darker though. You need to interpolate the values (see my answer). Actually the colors would get darker AND slightly more green haha GMan that wouldn't scale the colors correctly. Everything would become less saturated. very very nice!  To answer the conversion question I'm not familiar with c++ image libraries but I'm sure this code already exists somewhere. But if you'd like you can do it yourself. Each pixel has three channels: red green and blue. In a 16-bit bitmap each channel doesn't have the same number of bits per channel. Typically green is first to get any extra bits because the human eye is most sensitive to that color. It's probably 565 -- 5 for red 6 for green and 5 for blue. For an 18-bit bitmap (as previously stated) each channel has 6 bits. So green is easy! Just copy the value over from the 16 bit format to the 18 bit format. The other two channels are slightly harder. You have to interpolate from 5 to 6: 18_Value = (16_Value / 32) * 64 //integers only of course. You need to round properly to get best results. And there you have it. If you aren't already familiar with the following I'd suggest you research them: Bit Shift Operators Bitmap Stride Pointer Math Byte Alignment --Edit-- Make sure you know how things are laid out in memory; RGB or BGR. It will mess with your head if you do anything more complicated than this and you're thinking about it backwards (since Green is in the middle it's not that important for this conversion). wouldn't it be better to use 32 bit integers for the computation and multply before dividing? (or use doubles?) wow thanks can you give more examples of this conversion i am fairly new to this stuff. thanks. Being integer division this will clamp to an integer at each step and thus skew the numbers a lot. 63 62 61 and 60 all end up the same color. Also you can't divide by 5 and multiply by 6. That's the number of bits. You have to multiply and divide by their numeric values (32 and 64 in decimal). Yes I only meant it as rough pseduo-code. Values should be properly rounded. I'll add it to the answer. I made an edit just now that address this  You will first of all have to access each of the colour components (i.e. extract the R value the G value and the B value). The way to do this will depend totally on the way that the colour is stored in memory. If it is stored as RGB with 5-6-5 bits for the components you could use something like this: blueComponent = (colour & 0x1F); greenComponent = (colour >> 5 ) & 0x3F; redComponent = (colour >> 11) & 0x1F; This will extract the colour components for you then you can use the method outlined above to convert each of the components (I presume 18-bit will use 6 bits per component): blueComponent = (blueComponent / 32.0) * 64; //greenComponent = (greenComponent / 64.0) * 64; // not needed redComponent = (redComponent / 32.0) * 64; Note that with the above code it is important that you use 32.0 or some other method that will ensure that your program represents the number as a floating point number. Then to combine the components into your 18-bit colour use: colour = (redComponent << 12) | (greenComponent << 6) | blueComponent;
477,A,"How to stop Flex / AIR XOR-ing a fill? If you try the following code: g.beginFill(0xFF0000); g.drawRect(0 0 50 50); g.drawRect(25 25 50 50); g.endFill(); You would think that it would draw 2 overlapping red squares. However it doesn't - it draws two red squares except for the overlapping area which is now completely transparent. Any idea how to get around this? Post-Accepted-Answer: Thanks Christophe Herreman! Changing the code to: g.beginFill(0xFF0000); g.drawRect(0 0 50 50); g.endFill(); g.beginFill(0xFF0000); g.drawRect(25 25 50 50); g.endFill(); Worked just as intended! I'd be interested to know if this was ""intended behaviour"" or an actual bug though! All calls prior to endFill() will just store the points of the polygon you want to draw and connect them once endFill() is called. Since the code in your example has an overlapping part it will be filtered out when the actual lines of the polygon are drawn. I actually don't know if this is intended behavior of the Flash player or a bug. To solve this just add a new call to beginFill() before drawing the new rectangle. g.beginFill(0xFF0000); g.drawRect(0 0 50 50); g.beginFill(0xFF0000); g.drawRect(25 25 50 50); g.endFill();  Wouldn't you need to create a second graphic object to apply the second fill to? I bet you really have one strangely shaped graphic object instead of two intersecting rectangles."
478,A,"Is using Dexter's character sprite okay or do I have to . Inspiration -- Southpark game (very popular if you see download count on download.com  did he ask for permission ??) I am making a 2d game based on dexter's lab theme. I've got the sprite of dexter from GSA. basically I'm not an artist so I have to depend on already available sprites backgrounds sfx on websites like GameSpriteArchive etc. But is it okay/legal to use the dexter sprite I have got ? I wish to release it publicly too so shall I have to make lot of changes to do that? Is it possible to get a permission to use the sprite?? My hopes are very less in getting permission. Besides all that my basic plan is - Dexter's sprite from google search Enemy sprites from various GBA/SNES/etc games tiles/objects from these retro games Background art and style from blogs and portfolios of artists behind dexter powerpuff girls and samurai jack probably not legal since Dexter's lab is published by Hanna-Barbera and was created by Genndy Tartakovsky. They would have to grant you a license - but it can't hurt to ask!  I am not a lawyer. This is not legal advice. If you made the sprite yourself you'd be fine. If you got a release to use it from the creator you'd be fine. If it was released into the public domain you'd be fine. Anything else you'd have a definate problem with. There's also the possible problem you'd have even if you create the sprite yourself -- the likeness of the character is likely copyrighted. However that's not as cut-and-dried of an issue. Unfortunately this is one of the things you'd need to ask a real lawyer to get a firm answer on. If it's for your own use and that of some close friends you might be able to get away with hoping you don't get noticed (like most people who speed). If you're planning to include this in something you distribute to the public (even more so if you sell it) you're likely to run into problems. finally i made a game called nincompoop it was to be finished by 30th september so didn't give finishing touches - try here http://www.bidworkz.com/hfn/nincompoop_shaastra.zip  You probably won't have to get permission if they don't notice -- it's the old ""legal unless you get caught"" thing. However I strongly reccomend that you DO get permission from the creators or not use it at all on purely ethical grounds. After all you wouldn't want somebody appropriating your work right? finally i made a game called nincompoop it was to be finished by 30th september so didn't give finishing touches - try here http://www.bidworkz.com/hfn/nincompoop_shaastra.zip"
479,A,"Zoom image to pixel level For an art project one of the things I'll be doing is zooming in on an image to a particular pixel. I've been rubbing my chin and would love some advice on how to proceed. Here are the input parameters: Screen: sw - screen width sh - screen height Image: iw - image width ih - image height Pixel: px - x position of pixel in image py - y position of pixel in image Zoom: zf - zoom factor (0.0 to 1.0) Background colour: bc - background colour to use when screen and image aspect ratios are different Outputs: The zoomed image (no anti-aliasing) The screen position/dimensions of the pixel we are zooming to. When zf is 0 the image must fit the screen with correct aspect ratio. When zf is 1 the selected pixel fits the screen with correct aspect ratio. One idea I had was to use something like povray and move the camera towards a big image texture or some library (e.g. pygame) to do the zooming. Anyone think of something more clever with simple pseudo code? To keep it more simple you can make the image and screen have the same aspect ratio. I can live with that. I'll update with more info as its required. For a given zoom factor I just want the single image frame. Afterwards I'll step thru the zoom factor 0 to 1 to create the animation. Speed is abit more difficult to define - I'm looking for a ""natural"" motion towards the pixel as you go from 0 to 1 zoom factor with a fixed step (It doesn't have to fixed but this would be great). Also I don't mind using existing libraries. Could you first clarify if you want just the final image or animation through series of zoom levels? If you need animation you will need not only to describe max and min zoom levels (0 and 1) but also if the speed at which camera moves is linear or not (linear is a bit unnatural it accelerates and stops immediately). Furthermore since the pixel to which the camera is zooming is in general case not in the center there is not only zooming going on but also a translation of the camera to get over the pixel; this we could assume linear but confirmation would be nice. Also a note - algorithm will be simple but I would rather call using existing libraries more clever. Edit : For art projects you can check this framework : Processing I make it for 1D you start by writing the direct transform from original image to zoomed image with your constraints : As you want a linear transformation it is in the form : D( x ) = a x + b You want : for z = 0 : D( px ) = px D( px + 1 ) = px + 1 for z = 1 : D( px ) = 0 D( px + 1 ) = sw This gives : for z = 0 : a = 1  b = 0  D( x ) = x for z = 1 : a = sw  b = -sw . px  D( x ) = sw.x - sw.px For all z you use a linear combination of the two : D( x ) = z ( sw.x - sw.px ) + ( 1 - z ) ( x ) D( x ) = ( z.sw + 1 - z ).x - z.sw.px Now you write the inverse function to get the original coordinates from the output coordinates : ID( xout ) = ( xout + z.sw.px ) / ( z.sw + 1 - z ) Which allows you to fill the output image from the input image. For each output pixel the value is OriginalPixel[ ID( xout ) ] ( And when ID( xout ) is not in [0..sw] you use the background value ) For 2D the idea is similar but keeping the aspect ratio will need a little more effort. Interesting thought about 1D. I'll try to grok your equations and see if I can get my head around them. Yes I've played with processing very cool probably be the first tool to use if I can't do it by hand.  If color values of original image are given as array image[x][y] Then color values of zoomed image are image[x+zf*(px-x)][y+zf*(py-y)] Regarding the windows size/image size - initial preparation of image should take care of that: zoom the image up to the point that it would not fit the window any more and fill the remaining pixels with your preferred background colour. In python you can do something like def naivezoom(im px py zf bg): out = Image.new(im.mode im.size) pix = out.load() iw ih = im.size for x in range(iw): for y in range(ih): xorg = x + zf*(px - x) yorg = y + zf*(py - y) if xorg >= 0 and xorg < iw and yorg >= 0 and yorg < ih: pix[xy] = im.getpixel( (xorg  yorg) ) else: pix[xy] = bg return out after you set im = Image.open(""filename.ext"") with objects from import Image EDIT: Given stackoverflow logo you will get for zf = 0.3 around point 256 for zf = 0.96 around the same point Images were obtained with following code #!/bin/env python from Tkinter import * import Image import ImageTk def naivezoom(im p zf bg): out = Image.new(im.mode im.size) pix = out.load() iw ih = im.size for x in range(iw): for y in range(ih): xorg = x + zf*(p[0] - x) yorg = y + zf*(p[1] - y) if xorg >= 0 and xorg < iw and yorg >= 0 and yorg < ih: pix[xy] = im.getpixel( (xorg  yorg) ) else: pix[xy] = bg return out class NaiveTkZoom: def __init__(self parent=None): root = Tk() self.im = Image.open('logo.jpg') self.zf = 0.0 self.deltazf = 0.02 self.p = ( 0.1*self.im.size[0]0.1*self.im.size[1]) self.bg = 255 canvas = Canvas(root width=self.im.size[0]+20  height=self.im.size[1]+20) canvas.pack() root.bind('<Key>' self.onKey) self.canvas = canvas self.photo = ImageTk.PhotoImage(self.im) self.item = self.canvas.create_image(10 10 anchor=NW image=self.photo) def onKey(self event): if event.char == ""+"": if self.zf < 1: self.zf += self.deltazf elif event.char == ""-"": if self.zf > 0: self.zf -= self.deltazf self.out = naivezoom(self.im self.p self.zf self.bg) self.photo = ImageTk.PhotoImage(self.out) self.canvas.delete(self.item) self.item = self.canvas.create_image(10 10 anchor=NW image=self.photo) print self.p self.zf if __name__ == ""__main__"": NaiveTkZoom() mainloop() The libraries used and pixel by pixel approach are not the fastest in the world but will give you enough material to play with. Also the above code is not very clean. EDIT2(and3 centered the formula): Here's another attempt added translation but I have a feeling this is not final either (don't have the time to check the formulas). Also the speed of the translation is constant but that may lead to zooming to slow and showing background (if the point to which you are zooming is too close to the edge). I've also added a point on the original image so that it is visible what happens with it without need to paint on original image. #!/bin/env python from Tkinter import * import Image import ImageTk def markImage(im p bg): pix = im.load() pix[ p[0] p[1] ] = bg def naiveZoom(im p zf bg): out = Image.new(im.mode im.size) pix = out.load() iw ih = im.size for x in range(iw): for y in range(ih): xorg = x + zf*(p[0]+0.5-x) + zf*(1-zf)*(p[0]-iw/2) yorg = y + zf*(p[1]+0.5-y) + zf*(1-zf)*(p[1]-ih/2) if xorg >= 0 and xorg < iw and yorg >= 0 and yorg < ih: pix[xy] = im.getpixel( (xorg  yorg) ) else: pix[xy] = bg return out class NaiveTkZoom: def __init__(self parent=None): root = Tk() self.im = Image.open('py.jpg') self.zf = 0.0 self.deltazf = 0.05 self.p = (round(0.3*self.im.size[0]) round(0.3*self.im.size[1]) ) self.bg = 255 markImage(self.im self.p self.bg) canvas = Canvas(root width=self.im.size[0]+20  height=self.im.size[1]+20) canvas.pack() root.bind('<Key>' self.onKey) self.canvas = canvas self.photo = ImageTk.PhotoImage(self.im) self.item = self.canvas.create_image(10 10 anchor=NW image=self.photo) self.change = False def onKey(self event): if event.char == ""+"": if self.zf < 1: self.zf += self.deltazf self.change = True elif event.char == ""-"": if self.zf > 0: self.zf -= self.deltazf self.change = True if self.change: self.out = naiveZoom(self.im self.p self.zf self.bg) self.photo = ImageTk.PhotoImage(self.out) self.canvas.delete(self.item) self.change = False self.item = self.canvas.create_image(10 10 anchor=NW image=self.photo) print self.p self.zf if __name__ == ""__main__"": NaiveTkZoom() mainloop() There is quite a lot in the above that could be improved. :) Nice! Thanks for the equation and sample code. One thing I noticed was that if lets say the selected pixel is somewhere in the top left quadrant then it hugs the top left corner when you zoom in. Instead as you zoom in I'm looking for the selected pixel to center itself. Updated the answer no hugging happening over here are you sure you are applying given formulas properly? Yes! We have a winner! I added +0.5 the line after as well to fix the vertical. Please update your code for the final time! And thanks you shall be rewarded. Thanks for the updated code. I really liked the +/- keys touch! OK - try an image with a red (or any colour marker) pixel on an image and then zoom into it and you'll see that the top left of the selected pixel stays put. What we want is the pixel center itself on the screen. I'm using a 1000x1000 image with a red pixel at 300300. Yes once the pixel is up close you notice that its top left corner is whats centered on the screen. As for the translation its fine for now I can avoid selecting pixels on the extremes. If you want to zoom to the center of the pixel make formulas x + zf*(p[0]+0.5-x) + zf*(1-zf)*(p[0]-iw/2) - the 0.5 centers the pixel Updated the answer again it was missing the translation zooming was straight to the point out of center. Now it translates as it zooms. Let me know if this works for you still it is not perfect; for one I am not sure that the rate of zoom follows the translation properly and secondly when the zoom gets close to 1 the pixel itself is not centered but one of its corners so I might revise the formula once more (don't have time now)  If I understand correctly what you want to do. You can open image in a graphics program (like Gimp) set zoom level at 1 and take a screenshot. Then increase zoom level and take screenshot again etc. Then use mencoder to create AVI from screenshots. If I don't get a grip on the equations then I will look into automating other tools but I know I'll have to try afew tools before finding the one that gives me the effect I'm looking for. You can also probably write a script to run imagemagic with incremental scale levels and fixed croping. Save each scaled image with names like 001.jpg 002.jpg ... and use mencoder to create diferent movies."
480,A,"Graphics library for embedded systems without Linux? It seems that any kind of graphic library like DirectFB or MiniGui requires some sort of underlying operation system like Linux or uClinux. I am challenged with writing a software for a micro controller with just 512kb flash an LCD display and a touchscreen to display and handle some pictures and GUI parts. Do you know any library which just need a pointer to the video memory that also can handle lines images and fonts? You are right but it seems there is no free software (free as in free speech) for this task. A nice open source project to start with. :) Let us know if you found any project beside it's free or not. The uC is a ARM. You didn't specify whether it must be free (as in $$$) or not. If free is not a requirement I have a list as long as my arm to send you from previous projects. By the way is it a PIC32 or ARM uC? Does your platform support C++? Yeah it also supports C++. I working with the gnu compiler collection on a linux machine. How much RAM do you have? My guess is that something like FreeDOS combined with DJGPP as a toolchain and Allegro as a graphics library might conceivably fit into 512k of flash and still do a reasonable job (I'm assuming you have an x86 which has several Mb of ram here) But these things are very x86 specific (Allegro is not though). It is tricky to get a Linux kernel and a useful amount of userspace software inside 512k (but possible to get SOMETHING in) Very tricky. And then you assume there is much more RAM than flash. This microcontroller probably has much less RAM than flash.  The important thing you should be concerned about is the controller of the LCD and touchscreen. There is an abundance of C libraries (not free) for that task. A quick google got me these results: Simplify Technologies and Ramtex. If you want to find something open source then start from your controller's type and search embedded devices forums (even if it isn't ARM you could easily port C code). Some suggestions: AVRFreaks Arm Forums Also some kit manufactures offer an SDK (both with and without Linux) with their boards. Purchasing a board usually gives you the license to use the code. Search for development boards with the same LCD controller.  We have used ""PEG"" the C++ version from Swellsoftware for many years. It is commercial software not free but the underlying screen driver can use just a pointer to graphics memory and they provide many sample drivers for different types of graphics hardware. We wrote our own custom driver(s) for our proprietary hardware using the sample drivers as reference. We have always had some sort of RTOS but I believe PEG+ can also operate without an OS. Check it out here: http://www.swellsoftware.com/ good luck  By the time you incorporate some third party solution you could have just written it yourself. For most if not all environments the screen is just a two dimensional array of pixels. Sometimes palletized sometimes not but that doesnt matter you can write yours however you want. There is tons of free code out there for drawing lines and arcs etc. The killer may be fonts but I think you will find that a third party app will chew up all of your memory just doing fonts you are resource limited so you will want to pre-compute the fonts and just copy the bits. Make a two dimensional array of data do all of your work on your favorite host at first it is trivial to save .bmp files if you want to see what you are drawing and trivial to turn a series of .bmp files into a video if you want to watch some action. If you use generic C and no libc calls (write your own memcpy memset etc) this code will run anywhere on the host for development and on the target. Fonts are going to be your killer you have to pre-compute them but manage to squeeze that info down into as small as you can and at runtime extract the data and copy the bits for each letter into the virtual screen as fast as you can. Or just buy one of the many lcd solutions that do all of this for you and you simply send it commands like draw ""Hello World!"" at some (xy) using blue as the foreground and white as the background. Basically I think the non-os solutions are still going to use too many libraries and be too big for your specific application. 2d arrays of bytes or pixels are trivial to manage yourself. Even if you are writing an application for a desktop platform I would do it this way and at the last minute copy the fully renedered screen update to some os dependent library (allows for maximum portability from one OS or not to another).  You probably need to compress fonts using Run Length Encoding (RLE). See the .pcx file format for examples although it's probably best to design a custom RLE. You didn't specify the bit depth of the LCD but fonts need either one bit per pixel if antialiasing isn't needed or a maximum of three BPP with antialiasing. Every character has to have it's own width because monospaced text is not nice. You should render directly from the RLE compressed font to the screen using an optimized routine. SDL is a very portable graphics libary. It's used in embedded Linux systems but I think it can be used without an OS. The nice thing about SDL is that you can use Windows / Linux to develop and test your UI and later target your embedded system. No changes to application code needed! You could also use the Anti-Grain Geometry library (http://www.antigrain.com/about/index.html) on top of SDL. With a 16 or 24 bit LCD it produces stunning graphics. It might be just a bit too large for your environment because my executable on a ARM/Linux system was about one megabyte. It contained SDL AGG and libfreetype2 for font rendering. AGG is also a little slow but produces beautiful results. This are useful informations! It's a 16 bit color LCD with 640x480 pixel in size and I want using anti-aliasing for fonts and other graphic stuff. I was also considering cairo but it might be also too large.  For the smallest possible footprint you should really consider RamTEX. I have used it on two projects with 8-bit PICS. The ROM space was about 35K in my applications with ~1K for RAM (amount depends on whether you need RAM buffering for the display). ROM space depends on the graphical features you want or need. They provide full source and the one-time price is quite good less than $1000 (note that the prices listed on their web site have to be converted to dollars or whatever your currency is). There are no royalties or per-product restrictions. They provide a number of different size and style fonts and basic drawing calls (line pixels box etc.). It does not have any defined ""objects"" such as buttons or menus but I was able to implement a pop-up menu without too much trouble. It does support ""viewports"" that can be used to define text boxes menus etc each with their own attributes. It also comes with a simulator that runs on a PC so you can develop display code on your desktop before moving to an embedded system.  (old question but I wanted to post my findings on the subject) For high-quality graphics Anti-Grain Geometry is a good choice. It compiles to about 50kB and can be customized to write into all kinds of framebuffers and rendering devices: http://www.antigrain.com/ For user interface Gwen appears a good choice. It is easily portable and can be customized to render either bitmap skinned controls or just rectangle/circle/line forms: https://github.com/garrynewman/GWEN Then if you are also selecting an RTOS NuttX has it's own graphics subsystem and widget toolkit: http://nuttx.sourceforge.net/  512kb is small. Good luck! You might want to try a dsl combined with mplayer. The latter does not need a GUI to display a movie. I guess it could also display images. Nonetheless I fear it will be too much for your flash. Maybe the source of these links will help. those will not fit  Not free but good on low resource systems: http://www.tat.se and their products Kastor and Cascades. It only requires a pointer to video memory malloc and something that looks like a file system. The last two requirements are not absolutely necessary either. No operating system is required. Looks very impressive thanks! But TAT got eaten by a grue sorry by RIM and it seems they no longer sell their products Kastor and Cascades.  If your requirements for interactivity and GUI widgets are very modest (or you're OK with designing your own widgets) have a look at LibGD. Draw the image you want to appear on the screen using the library's functions and then write it to the frame buffer using gdImagePngToSink().  You should give easyGUI a try. easyGUI is a GUI graphics software/library specifically designed to work on small(er) Embedded Systems. No operating system needed. A basic cyclic executive is enough. 512kb of Flash should be more than OK. The library easyGUI provides is very flexible in helping to minimize the amount of Flash you need. Supports fonts graphics bitmaps touch screens and a bunch of video controllers out of the box. Plus it is really cheap (no licensing fees just a flat amount per seat) and comes with a PC program to design screens and generate code. The PC program takes a while to get used to but in the end it is very nice to try certain things out on the PC and then just generate and watch it run on your target. They have a demo app on their website. It is worth checking it out. We're using it - it works well but you need a Windows machine to run the PC side of things."
481,A,How do I create a floor for a game? I'm attempting to build a Lunar Lander style game on the iPhone. I've got Cocos2D and I'm going to use Box2D. I'm wondering what the best way is to build the floor for the game. I need to be able to create both the visual aspect of the floor and the data for the physics engine. Oh did I mention I'm terrible at graphics editing? I haven't used Box2D before (but I have used other 2D physics engines) so I can give you a general answer but not a Box2D-specific answer. You can easily just use a single static (stationary) Box if you want a flat plane as the floor. If you want a more complicated lunar surface (lots of craters the sea of tranquility whatever) you can construct it by creating a variety of different physics objects - boxes will almost always do the trick. You just want to make sure that all your boxes are static. If you do that they won't move at all (which you don't want of course) and they can overlap without and problems (to simulate a single surface). Making an image to match your collision data is also easy. Effectively what you need to do is just draw a single image that more or less matches where you placed boxes. Leave any spots that don't have boxes transparent in your image. Then draw it at the bottom of the screen. No problem.  The method I ended up going with (you can see from my other questions) is to dynamically create the floor at runtime and then draw it to the screen.
482,A,"Text-based game graphics in Python I'm pretty new to programming and I'm creating a simple text-based game.> I'm wondering if there is a simple way to create my own terminal-type window with which I can place coloured input etc. Is there a graphics module well suited to this? I'm using Mac but I would like it to work on Windows as well Thanks You really should go GUI nowadays whenever you want to make games that are not line-based. It is possible to do that in console but setting the terminal buffering modes drawing ANSI graphics and all that is just too complex and you still won't be able to portably do things such as detect two keys being held down at the same time. With GUI all this is very easy. @Tronic: Is there a particular GUI module u can suggest? For making games in Python Pygame is probably the best option. The Tkinter Text Widget will do what you ask. the IDLE main window is implemented as one if you want to play with an example.  For game programming with Python I would always recommend PyGame. It is not very complex and enables you to easily use input graphics and sound. As a start: http://www.penzilla.net/tutorials/python/pygame/  You could use the termcolor library - it that what you're looking for? On Windows things are trickier. See this SO answer - you should resort to win32console and some ctypes. The answer has some code and links to other articles. I thought that didn't work on Windows??? @Jasper: I've updated the answer. HTH Conceptually the `termcolor` thing is uglier imho. It's just that probably no one really bothered bringing the Windows console API to Python in a nicely usable way :-) @Johannes: I suppose this is because on Windows terminals are far less useful than on other platforms They have a defined set of capabilities making things like termcap and terminfo obsolete. And they have a C API instead of a text stream ""API"". That's about all the differences. All the nice things of ECMA-48 that extend beyond moving the cursor around or changing the color are nowhere implemented in terminal emulators anyway."
483,A,"Draw string in canvas I'm having an hard time finding how to draw/print a String in a canvas rotated 90º NOT vertical letters. After some different approaches without success I was trying to follow one that would involve printing the Graphics object to an Image object. As the API is reduced it has been a difficult task. So basically what I'm asking you guys is if you know how to draw a String rotated 90º in a Canvas or if you don't how can I save the graphics object to an Image one so I can follow my ""hint"". Thank you very much! Guilherme I'm assuming this is Java-based? sorry yes it is! Finally one last research in the web and here it is:  //The text that will be displayed String s=""java""; //Create the blank image specifying its size Image img=Image.createImage(5050); //Create an instance of the image's Graphics class and draw the string to it Graphics gr=img.getGraphics(); gr.drawString(s 0 0 Graphics.TOP|Graphics.LEFT); //Display the image specifying the rotation value. For example 90 degrees g.drawRegion(img 0 0 50 50 Sprite.TRANS_ROT90 0 0 Graphics.TOP|Graphics.LEFT); from: http://wiki.forum.nokia.com/index.php/How_to_display_rotated_text_in_Java_ME Thank you!"
484,A,BlackBerry - How to resize and store image? I need to scale image to small and store that image in to blackberry device (as a small image)? scaling an image is straightforward enough if the image is packaged or even sourced over the network or filesystem. public EncodedImage scaleImage(EncodedImage source int requiredWidth int requiredHeight) { int currentWidthFixed32 = Fixed32.toFP(source.getWidth()); int requiredWidthFixed32 = Fixed32.toFP(requiredWidth); int scaleXFixed32 = Fixed32.div(currentWidthFixed32 requiredWidthFixed32); int currentHeightFixed32 = Fixed32.toFP(source.getHeight()); int requiredHeightFixed32 = Fixed32.toFP(requiredHeight); int scaleYFixed32 = Fixed32.div(currentHeightFixed32 requiredHeightFixed32); return source.scaleImage32(scaleXFixed32 scaleYFixed32); } this returns a copy of the image scaled according to the width and height you require it to be. saving it on device you have options: filesystem or persistentStorage? Trying all kinds of different things: many from this website - nothing worked until I tried your solution. Thanks.  See BlackBerry - How to resize image How to save & delete a Bitmap image in Blackberry Storm?
485,A,Sources for visual explanations? Does anyone has sources for visual explanations of algorithms (or maths) that is even more expressive more intuitive maybe aesthetically appealing ? Or animations of algorithms? I agree with NXC: http://www.google.com/search?q=Algorithm+Animations will give you a lot of results relevant to your question. Animal is a cross-platform animation tool for algorithm animation. Animation scripts for a lot of common algorithms and data structures are available in the website's animation repository. Animal was used to teach us many different kinds of algorithms on data structures in my first CS class in college. It's really pretty neat. A few years back I played around with AnimalScript and I remember it being rather straightforward to create new animations. It is still being used in academia and they continue to improve it and the tools around it (mainly for creating new animation scripts). They just don't seem to find the time to update the website often enough though.  To start the discussion. I like the database / transaction deadlock illustrated to a non-technical person. You have also a lot of good animations on the Relevant Algorithm Animations/Visualizations page  Here's one for sorting algorithms. It gives some nice animations to see how each type works their way through the process.  Do a google search for [insert name of algorithm] and 'applet'. Many academics and other such people have made java applets to demonstrate algorithms working. You can probably find an example for just about any published algorithm.
486,A,screenshots of covered/minimized windows I have the following code to take screenshots of a window: HDC WinDC; HDC CopyDC; HBITMAP hBitmap; RECT rt; GetClientRect (hwnd &rt); WinDC = GetDC (hwnd); CopyDC = CreateCompatibleDC (WinDC); hBitmap = CreateCompatibleBitmap (WinDC rt.right - rt.left //width rt.bottom - rt.top);//height SelectObject (CopyDC hBitmap); //Copy the window DC to the compatible DC BitBlt (CopyDC //destination 00 rt.right - rt.left //width rt.bottom - rt.top //height WinDC //source 0 0 SRCCOPY); ReleaseDC(hwnd WinDC); ReleaseDC(hwnd CopyDC); It is someone elses code slightly modified as I am not really familiar with DC and how windows draws stuff to screen. When I have one window slightly covering another the covering window appears on screenshots of the covered one which is kind of inconvenient. Also when the window is minimized this code produces nothing much interesting. Is there any way around this? I would imagine that taking screenshots of a minimized application would be quite difficult but I hope that getting screenshots of covered windows is possible. Perhaps there is a different method of implementing this to get around these problems? No a screenshot is exactly what it sounds like. You'll read the pixels out of the video adapter what you get is what you see. You'll have to restore the window and bring it to the foreground to get the full view. WM_SYSCOMMAND+SC_RESTORE and SetForegroundWindow() respectively. Plus some time to allow the app to repaint its window if necessary. The WM_PRINT message is available to ask a window to draw itself into a memory context. That could handle the overlapped window problem. But that can only work if is your window. And rarely has the expected result programmers don't often implement WM_PRINT correctly. The WM_PRINT thing might be handy and shouldn't be too difficult. Thanks. To be fair you're not likely reading the pixels directly off the video adapter - probably from an internal buffer held by Windows. I could be wrong but I would be surprised if it wasn't done this way.
487,A,"Opacity of a TWinControl? How could one change the opacity of a TWinControl based control? And why didn't they add this capability to TControl/TWinControl level (why only TForm)? I don't how you'd do it. As I understand it Windows only added the translucency to windows which is why TForm can handle it but not individual controls. Remember that most of the VCL is simply a wrapper around the Windows Common Controls and so if Windows doesn't do it the related VCL control rarely will either. Third-party components often then extend the basic functionality. @_J_: You are correct. Why not add this as an answer so that it can be accepted? Controlling the opacity of a window is offered by a feature called layered windows. You can read more about the feature on MSDN. The feature is only available for top-level windows. As far as I can tell Desktop Window Manager (Vista's ""glass"" effect) is also only available for top-level windows. Alternatives available for child windows are to use window regions to mark certain areas of a window as completely transparent or to use bitmap alpha blending to draw the whole control yourself. Thanks for the extra effort for linking.  Thanks Lars. I don't how you'd do it. As I understand it Windows only added the translucency to windows which is why TForm can handle it but not individual controls. Remember that most of the VCL is simply a wrapper around the Windows Common Controls and so if Windows doesn't do it the related VCL control rarely will either. Third-party components often then extend the basic functionality. Please edit this even controls are windows (`HWND` type). The important point is that support isn there only for windows without the `WS_CHILD` style. A form with an overridden `CreateParams` method which sets the `WS_CHILD` style can't be translucent either. mghie thank you I think your comment says it all."
488,A,Blocking Graphics.drawImage I'm using Graphics2D.drawImage to draw an image into a BufferedImage. Then I Use ImageIO.write() to output this to a PNG. Often I don't see certain images that I painted this way. I assume this is because drawImage is an asynchronous operation and I need to wait for it to complete. I've tried implementing an ImageObserver but without success. Can anybody show me how it's done ? You should post a small sample that clarifies your question. If the original code is too big or it is proprietary and you don't want to release it on the net create a tiny sample application that exhibits the problem and edit your question to include the sample. That will allow others to help in debugging. Especially how are you implementing the ImageObserver? Perhaps your general approach is correct but you have a tiny bug you have overlooked. java.awt.MediaTracker is a relatively easy way to make sure that an image has loaded.
489,A,Php Db schema diagram creator Wondering if there is any php script out there which takes an sql db and diagramatically shows the entire thing (along with relationships)? Similar to what most mysql tools offer for desktop. Thank you very much. Look for FabForce DbDesigner. http://fabforce.net/dbdesigner4/  Maybe this can help you Entity-Relationship Diagram building software
490,A,"How to select a line So I'm trying to figure out how to implement a method of selecting lines or edges in a drawing area but my math is a bit lacking. This is what I got so far: A collection of lines each line has two end points (one to start and one to end the line) The lines are drawn correctly on a canvas Mouse clicks events are received when clicking the canvas so I can get the x and y coordinate of the mouse pointer I know I can iterate through the list of lines but I have no idea how to construct an algorithm to select a line by a given coordinate (i.e. the mouse click). Anyone got any ideas or point me to the right direction? // import java.awt.Point public Line selectLine(Point mousePoint) { for (Line l : getLines()) { Point start = l.getStart(); Point end = l.getEnd(); if (canSelect(start end mousePoint)) { return l; // found line! } } return null; // could not find line at mousePoint } public boolean canSelect(Point start Point end Point selectAt) { // How do I do this? return false; } Best way to do this is to use the intersects method of the line. Like another user mentioned you need to have a buffer area around where they clicked. So create a rectangle centered around your mouse coordinate then test that rectangle for intersection with your line. Here's some code that should work (don't have a compiler or anything but should be easily modifiable) // Width and height of rectangular region around mouse // pointer to use for hit detection on lines private static final int HIT_BOX_SIZE = 2; public void mousePressed(MouseEvent e) { int x = e.getX(); int y = e.getY(); Line2D clickedLine = getClickedLine(x y); } /** * Returns the first line in the collection of lines that * is close enough to where the user clicked or null if * no such line exists * */ public Line2D getClickedLine(int x int y) { int boxX = x - HIT_BOX_SIZE / 2; int boxY = y - HIT_BOX_SIZE / 2; int width = HIT_BOX_SIZE; int height = HIT_BOX_SIZE; for (Line2D line : getLines()) { if (line.intersects(boxX boxY width height) { return line; } } return null; } Or just see if ptLineDist() is less than a certain threshold. You'll get less strange results that way for larger ""buffers sizes"". Nice solution - I hadn't heard of the ptLineDist function before.  Well first off since a mathematical line has no width it's going to be very difficult for a user to click exactly ON the line. As such your best bet is to come up with some reasonable buffer (like 1 or 2 pixels or if your line graphically has a width use that) and calculate the distance from the point of the mouse click to the line. If the distance falls within your buffer then select the line. If you fall within that buffer for multiple lines select the one that came closest. Line maths here: http://mathworld.wolfram.com/Point-LineDistance2-Dimensional.html http://stackoverflow.com/questions/849211/shortest-distance-between-a-point-and-a-line-segment  If you use the 2D api then this is already taken care of. You can use Line2D.Double class to represent the lines. The Line2D.Double class has a contains() method that tells you if a Point is onthe line or not. Wow didn't know about the 2D api. That api will take care of a bunch of other things I had problems with before… if I just knew about it before. (>_<); .. are you sure? As I read from the API `contains()` always returns false because `Line2D` has no area I think you can still use the intersect() method if you use a small rectangle as a selection area Thats right. By definition contains() does not work with Line2D but deciding if the line intersects a very small rectangle around the click will work.  Sorry mathematics are still required... This is from java.awt.geom.Line2D: public boolean contains(double x double y) Tests if a specified coordinate is inside the boundary of this Line2D. This method is required to implement the Shape interface but in the case of Line2D objects it always returns false since a line contains no area. Specified by: contains in interface Shape Parameters: x - the X coordinate of the specified point to be tested y - the Y coordinate of the specified point to be tested Returns: false because a Line2D contains no area. Since: 1.2 I recommend Tojis answer"
491,A,Ruby library to generate graphics from text? I'm working in Rails and I'm looking for a library/gem/plugin to generate a graphic from text. Google Maps doesn't allow you to overlay text only graphics so I'm looking for a back door to overlay text. Suggestions? You need something like RMagick though that particular library has been known to leak memory.  Take a look at RMagick. To draw text using RMagick you need to use one the text or annotate methods.  libgd can do this and has bindings for many different languages including Ruby
492,A,"Most underused data visualization Histograms and scatterplots are great methods of visualizing data and the relationship between variables but recently I have been wondering about what visualization techniques I am missing. What do you think is the most underused type of plot? Answers should: Not be very commonly used in practice. Be understandable without a great deal of background discussion. Be applicable in many common situations. Include reproducible code to create an example (preferably in R). A linked image would be nice. Community wiki plz. changed to c.w. I think this is a very useful discussion and am sad it's closed. @AlexBrown: then why not vote to reopen? I can see why the wording of this question may feel as ""not constructive"" but this question resulted in some of the most thoughtful and insightful answers on this topic anywhere on the web. I would love to see these answers updated and extended. This should probably be moved to stats.stackoverflow.com. It's much more suited to that site. Pity no-one mentioned [QQ-plots](http://en.wikipedia.org/wiki/Q-Q_plot) here before this was closed. They're so damn useful! In addition to Tufte's excellent work I recommend the books by William S. Cleveland: Visualizing Data and The Elements of Graphing Data. Not only are they excellent but they were all done in R and I believe the code is publicly available.  Boxplots! Example from the R help: boxplot(count ~ spray data = InsectSprays col = ""lightgray"") In my opinion it is the most handy way to take a quick look at the data or to compare distributions. For more complex distributions there is an extension called vioplot. I loooove violin plots. Beanplot could be mentioned here as well http://www.jstatsoft.org/v28/c01/paper and http://cran.r-project.org/web/packages/beanplot/index.html Boxplots aren't that underused are they? I mean sure in many papers bar charts are used for data that should be boxplotted but they're still pretty common.  Another nice time series visualization that I was just reviewing is the ""bump chart"" (as featured in this post on the ""Learning R"" blog). This is very useful for visualizing changes in position over time. You can read about how to create it on http://learnr.wordpress.com/ but this is what it ends up looking like: A bump chart is a parallel coordinate plot of ranked data. I do like the bump chart for this particular data but have a hard time thinking of more general situations where it would be of use. That said I think we can all agree that the Learning R blog rocks the socks. this reminds me of slopegraph which is good for representing ranking change over time or relationships between rankings: http://charliepark.org/slopegraphs/  We shouldn't forget about cute and (historically) important stem-and-leaf plot (that Tufte loves too!). You get a directly numerical overview of you data density and shape (of course if your data set is not larger then about 200 points). In R the function stem produces your stem-and-leaf dislay (in workspace). I prefer to use gstem function from package fmsb to draw it directly in a graphic device. Below is a beaver body temperature variance (data should be in your default dataset) in a stem-by-leaf display:  require(fmsb) gstem(beaver1$temp)  Violin plots (which combine box plots with kernel density) are relatively exotic and pretty cool. The vioplot package in R allows you to make them pretty easily. Here's an example (The wikipedia link also shows an example): Violin plots are also available via the lattice package: `bwplot(... panel = panel.violin)` ggplot2 version of violin plots coming shortly. https://github.com/wch/ggplot2/wiki/geom_violin  I really agree with the other posters: Tufte's books are fantastic and well worth reading. First I would point you to a very nice tutorial on ggplot2 and ggobi from ""Looking at Data"" earlier this year. Beyond that I would just highlight one visualization from R and two graphics packages (which are not as widely used as base graphics lattice or ggplot): Heat Maps I really like visualizations that can handle multivariate data especially time series data. Heat maps can be useful for this. One really neat one was featured by David Smith on the Revolutions blog. Here is the ggplot code courtesy of Hadley: stock <- ""MSFT"" start.date <- ""2006-01-12"" end.date <- Sys.Date() quote <- paste(""http://ichart.finance.yahoo.com/table.csv?s="" stock ""&a="" substr(start.date67) ""&b="" substr(start.date 9 10) ""&c="" substr(start.date 14) ""&d="" substr(end.date67) ""&e="" substr(end.date 9 10) ""&f="" substr(end.date 14) ""&g=d&ignore=.csv"" sep="""") stock.data <- read.csv(quote as.is=TRUE) stock.data <- transform(stock.data week = as.POSIXlt(Date)$yday %/% 7 + 1 wday = as.POSIXlt(Date)$wday year = as.POSIXlt(Date)$year + 1900) library(ggplot2) ggplot(stock.data aes(week wday fill = Adj.Close)) + geom_tile(colour = ""white"") + scale_fill_gradientn(colours = c(""#D61818""""#FFAE63""""#FFFFBD""""#B5E384"")) + facet_wrap(~ year ncol = 1) Which ends up looking somewhat like this: RGL: Interactive 3D Graphics Another package that is well worth the effort to learn is RGL which easily provides the ability to create interactive 3D graphics. There are many examples online for this (including in the rgl documentation). The R-Wiki has a nice example of how to plot 3D scatter plots using rgl. GGobi Another package that is worth knowing is rggobi. There is a Springer book on the subject and lots of great documentation/examples online including at the ""Looking at Data"" course. nice. Thanks for including the code/image. what is indicated by the vertical position of the 'Z' or bend in each solid black vertical line? Those are month boundaries (months don't end on the same day). That's beautiful. How did you get the month boundaries to happen? @AlexBrown Taking a look at the [R file](http://blog.revolution-computing.com/downloads/calendarHeat.R) it seems they're made painstakingly with `grid.lines`. [working link to R file](http://blog.revolutionanalytics.com/downloads/calendarHeat.R)  Mosaic plots seem to me to meet all four criteria mentioned. There are examples in r under mosaicplot. A better implementation of mosaic plots is in the vcd library (function name 'mosaic'). It has a much more flexible method signature and it is implemented in grid (rather than the 'base' graphics system).  Horizon graphs (pdf) for visualising many time series at once. Parallel coordinates plots (pdf) for multivariate analysis. Association and mosaic plots for visualising contingency tables (see the vcd package)  If your scatter plot has so many points that it becomes a complete mess try a smoothed scatter plot. Here is an example: library(mlbench) ## this package has a smiley function n <- 1e5 ## number of points p <- mlbench.smiley(nsd1 = 0.4 sd2 = 0.4) ## make a smiley :-) x <- p$x[1]; y <- p$x[2] par(mfrow = c(12)) ## plot side by side plot(xy) ## left plot regular scatter plot smoothScatter(xy) ## right plot smoothed scatter plot The hexbin package (suggested by @Dirk Eddelbuettel) is used for the same purpose but smoothScatter() has the advantage that it belongs to the graphics package and is thus part of the standard R installation. For completeness sake you can also obtain this effect in ggplot by using transparency (alpha).) in combination with geom point. is this the same as kernel density estimation or just similar?  Plots using polar coordinates are certainly underused--some would say with good reason. I think the situations which justify their use are not common; i also think that when those situations arise polar plots can reveal patterns in data that linear plots cannot. I think that's because sometimes your data is inherently polar rather than linear--eg it is cyclical (x-coordinates representing times during 24-hour day over multiple days) or the data were previously mapped onto a polar feature space. Here's an example. This plot shows a Website's mean traffic volume by hour. Notice the two spikes at 10 pm and at 1 am. For the Site's network engineers those are significant; it's also significant that they occur near each other other (just two hours apart). But if you plot the same data on a traditional coordinate system this pattern would be completely concealed--plotted linearly these two spikes would be 20 hours apart which they are thought they are also just two hours apart on consecutive days. The polar chart above shows this in a parsimonious and intuitive way (a legend isn't necessary). There are two ways (that i'm aware of) to create plots like this using R (i created the plot above w/ R). One is to code your own function in either the base or grid graphic systems. They other way which is easier is to use the circular package. The function you would use is 'rose.diag': data = c(35 78 34 25 21 17 22 19 25 18 25 21 16 20 26 19 24 18 23 25 24 25 71 27) three_palettes = c(brewer.pal(12 ""Set3"") brewer.pal(8 ""Accent"") brewer.pal(9 ""Set1"")) rose.diag(data bins=24 main=""Daily Site Traffic by Hour"" col=three_palettes) Copying your code I get a very different plot (that is quite ugly); any idea why? I get this warning: 1: In as.circular(xx[ 1]) : an object is coerced to the class 'circular' using default value for the following components: type: 'angles' units: 'radians' template: 'none' modulo: 'asis' zero: 0 rotation: 'counter' rose.diagdata24Daily Site Traffic by Hourthree_palettes I have the same problem. You could do this with a line-plot too. Can be a bit harder to read but it can also be really awesome for more granular data or data that undergoes more than one cycle (e.g. plot ten cycles then plot their average). I also had trouble replicating the plot. I eventually decided it was easier to use ggplot2. I've left a short demo on Rpubs with code and results: http://rpubs.com/mattbagg/circular @doug -- Brilliantly written explanation! Thanks! ggplot2 equivalent: `qplot(y=data x=1:length(data) fill=factor(1:length(data)) stat='identity' geom='bar') + coord_polar()`  I also like Tufte's modifications of boxplots which let you do small multiples comparison much more easily because they are very ""thin"" horizontally and don't clutter up the plot with redundant ink. However it works best with a fairly large number of categories; if you've only got a few on a plot the regular (Tukey) boxplots look better since they have a bit more heft to them. library(lattice) library(taRifx) compareplot(~weight | Diet * Time * Chick data.frame=cw  main = ""Chick Weights"" box.show.mean=FALSE box.show.whiskers=FALSE box.show.box=FALSE ) Other ways of making these (including the other kind of Tufte boxplot) are discussed in this question. That is cool thanks! @daroczig Thanks. One of these days I'll rewrite it to take different configurations of groupings. I've learned a lot since I wrote that function! I like your plots much better than tufte's which are ridiculously hard to read. I still think that Tukey-style boxplots are better although a good compromise might be something like what you have here but with 3px wide lines for the box instead of the 1px offset. And I think a 1px wide horisontal line for the median is probably neater and more exact.  Regarding sparkline and other Tufte idea the YaleToolkit package on CRAN provides functions sparkline and sparklines. Another package that is useful for larger datasets is hexbin as it cleverly 'bins' data into buckets to deal with datasets that may be too large for naive scatterplots. +1 to the sparklines. I'm currently working on a package that is focused on sparkline creation in R-- they make great additions to tables in Sweave reports. Cool! I am not too happy with what Jay has in YaleToolkit and would love to have sparklines in tables! I've just documented a way to produce sparklines only using `plot` over in an update to my [question](http://stackoverflow.com/q/8337980/1036500) with some help from this [Tufte forum post](http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=00037p) The `Hmisc::latex()` version of output from `Hmisc::describe` includes a mini-histogram that gets included in the table.  Check out Edward Tufte's work and especially this book You can also try and catch his travelling presentation. It's quite good and includes a bundle of four of his books. (i swear i don't own his publisher's stock!) By the way i like his sparkline data visualization technique. Surprise! Google's already written it and put it out on Google Code  I really like dotplots and find when I recommend them to others for appropriate data problems they are invariably surprised and delighted. They don't seem to get much use and I can't figure out why. Here's an example from Quick-R: I believe Cleveland is most responsible for the development and promulgation of these and the example in his book (in which faulty data was easily detected with a dotplot) is a powerful argument for their use. Note that the example above only puts one dot per line whereas their real power comes with you have multiple dots on each line with a legend explaining which is which. For instance you could use different symbols or colors for three different time points and thence easily get a sense of time patterns in different categories. In the following example (done in Excel of all things!) you can clearly see which category might have suffered from a label swap. How is a dotplot different from a scatterplot with switched axis one of which is categorical? @gsk3 Didn't mean to sound snarky. In fact I now (after reading more about grammar of graphics and similar works) realize that this higher-level distinction can be quite important for presentation. Thanks for showing this. @DrSAR How is a histogram different than a barchart or a density plot different than a line plot? You can describe many standard chart types in terms of more fundamental geometries (c.f. Bertin's _Semiologie Graphique_) but that doesn't make the insight to plot something a particular way any less unique. In this case you are plotting two pieces of categorical information (one vertically one by the shape of the plotting character) against one piece of continuous data. While in most software packages you would hack a scatterplot to create it it is most emphatically not a scatterplot. @DrSAR And I didn't mean to sound defensive. Nature of SO comments I guess ;-)  Summary plots? Like mentioned in this page: Visualizing Summary Statistics and Uncertainty"
493,A,"Pythagoras tree with g2d I'm trying to build my first fractal (Pythagoras Tree): in Java using Graphics2D. Here's what I have now : import java.awt.*; import java.awt.geom.*; import javax.swing.*; import java.util.Scanner; public class Main { public static void main(String[] args) { int i=0; Scanner scanner = new Scanner(System.in); System.out.println(""Give amount of steps: ""); i = scanner.nextInt(); new Pitagoras(i); } } class Pitagoras extends JFrame { private int powt counter; public Pitagoras(int i) { super(""Pythagoras Tree.""); setSize(1000 1000); setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); setVisible(true); powt = i; } private void paintIt(Graphics2D g) { double p1=450 p2=800 size=200; for (int i = 0; i < powt; i++) { if (i == 0) { g.drawRect((int)p1 (int)p2 (int)size (int)size); counter++; } else{ if( i%2 == 0){ //here I must draw two squares } else{ //here I must draw right triangle } } } } @Override public void paint(Graphics graph) { Graphics2D g = (Graphics2D)graph; paintIt(g); } So basically I set number of steps and then draw first square (p1 p2 and size). Then if step is odd I need to build right triangle on the top of square. If step is even I need to build two squares on free sides of the triangle. What method should I choose now for drawing both triangle and squares ? I was thinking about drawing triangle with simple lines transforming them with AffineTransform but I'm not sure if it's doable and it doesn't solve drawing squares. You do not have to draw triangles only squares (the edges of the squares are the triangle) in this tree. You can make things a lot easier looking into recursion (these types of fractals are standard examples for recursion): In Pseudo-Code drawSquare(coordinates) { // Check break condition (e.g. if square is very small) // Calculate coordinates{1|2} of squares on top of this square -> Pythagoras drawSquare(coordinates1) drawSquare(coordinates2) } And since I often programmed fractals a hint: Draw the fractal itself in a BufferedImage and only paint the image in the paint-method. The paint-Method gets called possibly several times per second so it must be faaaaast. Also do not directly draw in a JFrame but use a Canvas (if you want to use awt) or a JPanel (if you use swing). Well in this case you would run out of space no matter which data structure you use. the problem with recursion is that at some depth in the fractal you're going to run out of stack space. So while elegant it's probably impractical here. good point I'm stupid.  My final solution : import java.awt.*; import java.util.Scanner; import javax.swing.*; public class Main extends JFrame {; public Main(int n) { setSize(900 900); setTitle(""Pythagoras tree""); add(new Draw(n)); setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); setVisible(true); } public static void main(String[] args) { Scanner sc = new Scanner(System.in); System.out.print(""Give amount of steps: ""); new Main(sc.nextInt()); } } class Draw extends JComponent { private int height = 800; private int width = 800; private int steps; public Draw(int n) { steps = n; Dimension d = new Dimension(width height); setMinimumSize(d); setPreferredSize(d); setMaximumSize(d); } @Override public void paintComponent(Graphics g) { super.paintComponent(g); g.setColor(Color.white); g.fillRect(0 0 width height); g.setColor(Color.black); int x1 x2 x3 y1 y2 y3; int base = width/7; x1 = (width/2)-(base/2); x2 = (width/2)+(base/2); x3 = width/2; y1 = (height-(height/15))-base; y2 = height-(height/15); y3 = (height-(height/15))-(base+(base/2)); g.drawPolygon(new int[]{x1 x1 x2 x2 x1} new int[]{y1 y2 y2 y1 y1} 5); int n1 = steps; if(--n1 > 0){ g.drawPolygon(new int[] {x1 x3 x2} new int[] {y1 y3 y1} 3); paintMore(n1 g x1 x3 x2 y1 y3 y1); paintMore(n1 g x2 x3 x1 y1 y3 y1); } } public void paintMore(int n1 Graphics g double x1_1 double x2_1 double x3_1 double y1_1 double y2_1 double y3_1){ int x1 x2 x3 y1 y2 y3; x1 = (int)(x1_1 + (x2_1-x3_1)); x2 = (int)(x2_1 + (x2_1-x3_1)); x3 = (int)(((x2_1 + (x2_1-x3_1)) + ((x2_1-x3_1)/2)) + ((x1_1-x2_1)/2)); y1 = (int)(y1_1 + (y2_1-y3_1)); y2 = (int)(y2_1 + (y2_1-y3_1)); y3 = (int)(((y1_1 + (y2_1-y3_1)) + ((y2_1-y1_1)/2)) + ((y2_1-y3_1)/2)); g.setColor(Color.green); g.drawPolygon(new int[] {x1 x2 (int)x2_1 x1} new int[] {y1 y2 (int)y2_1 y1} 4); g.drawLine((int)x1 (int)y1 (int)x1_1 (int)y1_1); g.drawLine((int)x2_1 (int)y2_1 (int)x2 (int)y2); g.drawLine((int)x1 (int)y1 (int)x2 (int)y2); if(--n1 > 0){ g.drawLine((int)x1 (int)y1 (int)x3 (int)y3); g.drawLine((int)x2 (int)y2 (int)x3 (int)y3); paintMore(n1 g x1 x3 x2 y1 y3 y2); paintMore(n1 g x2 x3 x1 y2 y3 y1); } } }"
494,A,Finding the overlapping area of two rectangles (in C#) Edit: Simple code I used to solve the problem in case anyone is interested (thanks to Fredrik):  int windowOverlap(Rectangle rect1 Rectangle rect2) { if (rect1.IntersectsWith(rect2)) { Rectangle overlap = Rectangle.Intersect(rect1 rect2); if (overlap.IsEmpty) return overlap.Width * overlap.Height; } return 0; } Original Question: I'd like to know a quick and dirty way to check if two rectangles overlap and if they do calculate the area of the overlap. For curiosities sake I'm interested in the case where 1) all the lines in both rectangles are either vertical or horizontal or 2) the general case for any two rectangles but the only answer I really need is case 1. I'm thinking along the lines of: double areaOfOverlap( Rect A Rect B) { if ( A.Intersects(B) ) { // calculate area // return area } return 0; } For A.Intersects() I was thinking of using the separating axis test but if the rectangles have only horizontal and vertical lines is there an even simpler (faster) way to check? And for calculating the area where they intersect is there an quick way to do it if the rectangles only horizontal and vertical lines? Finally this is unrelated to the question but I'd appreciate any advice someone may have on a good book / webpage where I could review the math for computer graphics. I've been out of college for a while and feel like I'm forgetting everything :)! Anyone else have that problem? ( NOTE: I found this question different than this which seems more complicated and doesn't directly answer the question. ) A horizontal rectangle is a vertical rectangle depends which side you consider the top. **if (overlap.IsEmpty)** should be **if (!overlap.IsEmpty)** Maybe I misinterpret your question but doesn't the Rectangle.Intersect method do the job? It returns the intersecting area and then you can easily calculate the area of it. yes that's prefect :). thanks!  Sounds like basic Collision Detection. Have you looked at this page on Wikipedia? Mike edit: Fredrik make his response at the same time I made this one his answer got my upvote ( : Thanks I'll check out the site!
495,A,"Set form backcolor to custom color How can I set a form's backcolor to a custom color (such as light pink) using C# code? I understand the need to change color but light pink?? :) WinForms or WPF? If you want to set the form's back color to some arbitrary RGB value you can do this: this.BackColor = Color.FromArgb(255 232 232); // this should be pink-ish Thanks! much appreciated @MusiGenesis It works fine for me too but it says: ""Virtual member call in constructor"" what does it mean should I do anything about it? :)  With Winforms you can use Form.BackColor to do this. From within the Form's code: BackColor = Color.LightPink; If you mean a WPF Window you can use the Background property. From within the Window's code: Background = Brushes.LightPink;"
496,A,Flex: Points to pixels and back again So I have extended the PlotChart that comes with Flex to have the ability to draw trend-lines. To do this I have to get pixel positions. How can I convert (100px100px) in pixels to a point on the graph such as (0.71.0)? You probably want to create a subclass of ChartElement and add it to annotationElements for your chart. The docs for Flex 2 show an example: Livedocs  You can draw on charts using the CartesianChartCanvas here is the link to the LiveDoc: Drawing on Chart Controls To answer your question more specifically CartesianChartCanvas supports drawing by pixels.  PlotSeries::dataToLocal(...) ended up working fine.
497,A,"How to draw a decent looking Circle in Java I have tried using the method drawOval with equal height and width but as the diameter increases the circle becomes worse looking. What can I do to have a decent looking circle no matter the size. How would I implement anti-aliasing in java or some other method. Two things that may help: Use Graphics2D.draw(Shape) with an instance of java.awt.geom.Ellipse2D instead of Graphics.drawOval If the result is still not satisfactory try using Graphics2D.setRenderingHint to enable antialiasing Example public void paint(Graphics g) { Graphics2D g2d = (Graphics2D) g; Shape theCircle = new Ellipse2D.Double(centerX - radius centerY - radius 2.0 * radius 2.0 * radius); g2d.draw(theCircle); } See Josef's answer for an example of setRenderingHint  As it turns out Java2D (which I'm assuming is what you're using) is already pretty good at this! There's a decent tutorial here: http://www.javaworld.com/javaworld/jw-08-1998/jw-08-media.html The important line is: graphics.setRenderingHints(Graphics2D.ANTIALIASINGGraphics2D.ANTIALIAS_ON); You gotta love links 11 years old hope they will never die. It looks like the constants have been moved from Graphics2D to RenderingHints. See [this example](http://www.java2s.com/Code/JavaAPI/java.awt/Graphics2DsetRenderingHintKeyhintKeyObjecthintValue.htm).  Inability to draw a ""decent looking circle"" is related to the very old bug 6431487. Turning antialiasing on does not help a lot - just check the kind of ""circle"" produced by the drawOval() or drawShape(Eclipse) when the required circle size is 16 pixels (still pretty common for icon size) and antialiasing is on. Bigger antialiased circles will look better but they are still asymmetric if somebody will care to look at them closely. It seems that to draw a ""decent looking circle"" one has to manually draw one. Without antialiasing it will be midpoint circle algorithm (this question has an answer with a pretty java code for it).  Of course you set your radius to what ever you need: @Override public void paint(Graphics g) { Graphics2D g2d = (Graphics2D) g; g2d.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON); g2d.setRenderingHint(RenderingHints.KEY_RENDERING RenderingHints.VALUE_RENDER_QUALITY); Ellipse2D.Double hole = new Ellipse2D.Double(); hole.width = 28; hole.height = 28; hole.x = 14; hole.y = 14; g2d.draw(hole); } You also need to set the position of the ellipse and I think you want g2d.draw not g2d.stroke  you can set rendering hints: Graphics2D g2 = (Graphics2D) g; g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON);"
498,A,How to copy Gdk.image? Is it possible to make a copy of Gdk.image object using lablgtk2 for Ocaml? I tried to find 'copy' or 'clone' methods but failed. Here is my clumsy workaround: let copy_image image = let w h = Image.width image Image.height image in let copy = Image.create ~kind: `FASTEST ~visual: (Image.get_visual image) ~width: w ~height: h in for x = 0 to w-1 do for y = 0 to h-1 do Image.put_pixel copy ~x:x ~y:y (Image.get_pixel image ~x:x ~y:y) done done; copy although pixbuf variant is also clumsy :(  Maybe you could use pixbuf? It can be created from any drawable.
499,A,"Problem with actionscript 3 erasing drawing I have a based image and some sprites on top of the basd image movieclip... Some of the sprites can be drawn by the user using graphics api in actionscript 3. I can draw things on the sprites but I can't create an eraser like brush that can remove part of the unwanted drawings. I try using Alpha but no it doesn't work I have googled about it and come up with the solution: 1) Linebitmapstyle... This solution is not the best one coz I my sprites can be moved so if I use linebitmapstyle it does draw the pixel from the image to the sprite but if the sprite moved the drawn pixel won't change. 2) Masking may not work for me either.... What is the best way of creating the eraser Simon next time you could post you're working code. You may rather want to use a Bitmap to make such things easier to manipulate (unless you need to do scalable vector graphics of course!). To draw shapes you can still use the graphics API to create the shapes. To do so instantiate a ""dummy"" sprite (or another IBitmapDrawable implementation) to create the graphics and then ""copy"" them to the BitmapData the bitmapData.draw() function. This way you can for instance draw with the option BlendMode.ERASE in order remove the pixels of the shape. Example (from the top of my mind) : // creates a bitmap data canvas var bitmapData:BitmapData = new BitmapData(500 500); // creates a bitmap display object to contain the BitmapData addChild(new Bitmap(bitmapData)); // creates a dummy object to draw and draws a 10px circle var brush:Sprite = new Sprite(); // note this is not even added to the stage brush.graphics.beginFill(0xff0000); brush.graphics.drawCircle(10 10 10); // the matrix will be used to position the ""brush strokes"" on the canvas var matrix:Matrix = new Matrix(); // draws a circle in the middle of the canvas matrix.translate(250 250); bitmapData.draw(brush matrix // translates the position 5 pixels to the right to slightly erase the previously // drawn circle creating a half moon matrix.translate(5 0); bitmapData.draw(brush matrixnullBlendMode.ERASE); It works now .. Thank you... great no problem. I have done what you have told and adjusted with my code... I used the lineTo as the brush but doesn't work... I agree with Radu - posting the working code would have been nice. Theo's code is nearly complete but one detail is missing: The ""blendMode"" property of the parent object must be set to ""layer"" for the erasing to work. So assuming you assign the bitmap data to a Bitmap object the full code would be: var bitmap:Bitmap = new Bitmap(bitmapData); bitmap.blendMode = ""layer"";"
500,A,"How to blend color of two sprites with constant alpha in DirectX? Essentially what I want to do (in DirectX) is to take two partially-transparent images and blend them together. This works fine with default blending insofar as they both show up as overlapping etc. However the problem is that the opacity goes up markedly where the two intersect. This causes increasing problems as more sprites overlap. What I'd like to do is keep the blending the same except keep a global opacity for all these sprites being blended regardless of how they overlap. Seems like there would be a render setting for this (all of these sprites are alone in their sprite batch which keeps that part easy) but if so I don't know it. Right now I'm kind of shooting in the dark and I've tried a lot of different things and none of them have looked right at all. I know I probably need some sort of variant of D3DBLENDOP but I just don't know what sort of settings there I really need (I have tried many things but it is all guessing at this stage). Here is a screenshot of what is actually happening with standard blending (the best I can get it): http://arcengames.com/share/FFActual.png Here is a screenshot with a mockup of how I would want the blending to turn out (the forcefields were added to the same layer in Photoshop then given a shared alpha value): http://arcengames.com/share/FFMockup.png This is how I did it in Photoshop: 1. Take the two images and remove all transparency (completely transparent pixels excepted). 2. Combine them into one layer which blends the color but which has no partial alpha at all. 3. Now set the global transparency for that layer to (say) 40%. The result is something that looks kind of blended together color-wise but which has no increase in opaqueness on the overlapped sections. UPDATE: Okay thanks very much to Goz below who suggested using the Z-Buffer. That works! The blending by and large is perfect and just what I would want. The only remaining problem? Using that new method there is a huge artifact around the edge of the force field image that is rendered last. See this: http://www.arcengames.com/share/FFZBuffer.png UPDATE: Below is the final solution in C# (SlimDX) Clearing the ZBuffer to black transparent or white once per frame all has the same effect (this is right before BeginScene is called) Direct3DWrapper.ClearDevice( SlimDX.Direct3D9.ClearFlags.ZBuffer Color.Transparent 0 ); All other sprites are drawn at Z=1 with the ZBuffer disabled for them: device.SetRenderState( RenderState.ZEnable ZBufferType.DontUseZBuffer ); The force field sprites are drawn at Z=2 with the ZBuffer enabled and ZWrite enabled and ZFunc as Less: device.SetRenderState( RenderState.ZEnable ZBufferType.UseZBuffer ); device.SetRenderState( RenderState.ZWriteEnable true ); device.SetRenderState( RenderState.ZFunc Compare.Less ); The following flags are also set at this time to prevent the black border artifact I encountered: device.SetRenderState( RenderState.AlphaTestEnable true ); device.SetRenderState( RenderState.AlphaFunc Compare.GreaterEqual ); device.SetRenderState( RenderState.AlphaRef 55 ); Note that AlphaRef is at 55 because of the alpha levels set in the specific source image I was using. If my source image had a higher alpha value then the AlphaRef would also need to be higher. Best I can tell is that the forcefields are a whole object. Why not render them last in front to back order and with Z-buffering enabled. That will give you the effect you are after. ie its not blending settings thats your problem at all. Edit: Can you use render-to-texture then? IF so you could easily do what you did under photoshop. Render them all together into the texture and then blend the texture back over the screen. Edit2: How about ALPHATESTENABLE = TRUE; ALPHAFUNC = LESS ALPHABLENDENABLE = TRUE; SRCBLEND = SRCALPHA; DESTBLEND = INVSRCALPHA; SEPERATEALPHABLENDENABLE = TRUE; SRCBLENDALPHA = ONE; DESTBLENDALPHA = ZERO; You need to make sure the alpha is cleared to 0xff in the frame buffer each frame. You then do the standard alpha blend. while passing the alpha value straight through to the backbuffer. This is though where the alpha test comes in. You test the final alpha value against the one in the back buffer. If it is less than whats in the backbuffer then that pixel has not been blended yet and will be put into the frame buffer. If it is equal (or greater) then it HAS been blended already and the alpha value will be discarded. That said ... using a Z-Buffer would cost you a load of RAM but would be faster overall as it would be able to throw away the pixels far earlier in the pipeline. Seeing as all the shields would just need to be written to a given Z-plane you wouldn't even need to go through the hell I suggested earlier. If the Z value it receives is less than whats there already then it will render it if it is greater or equal then it will discard it fortunately before the blend calculation is ever performed. That said ... you could also do it by using the stencil buffer which would require a Z-buffer anyway. Anyway ... hope one of those methods is of some help. Edit3: DO you render the forcefield with some form of feathering around the edge? Most likely that edge is caused by the fact that the alpha fades off slightly and then the ""slightly alpha"" pixels are getting written to the z-buffer and hence any subsequent draw doesn't overwrite them. Try the following settings ALPHATESTENABLE = TRUE ALPHAFUNC = GREATEREQUAL // if this doesn't work try less .. i may be being a retard ALPHAREF = 255 To fine tune the feathering around the edge adjust the alpharef but i'd suspect you need to keep it as above. Damn my error ... i forgot that D3D does not support destination alpha testing. In which case your best option really is to use a Z-Buffer. Basically you need to pass a Z-Buffer value directly through to the Z-Buffer. It is basically as easy as setting the Z-Value with an appropriate matrix and and appropriate set of Viewport settings. You can then render everything to z = 2 (for example) and then render all your alpha stuff to z=1 with D3DCMP_LESS set on the Z test and Z-writes enabled. The forcefields are two separate objects that's the problem. And there could be dozens of them some overlapping some not. I'm not using Z-buffering at all given that this is just 2D (even if it is in Direct3D) and all of the sprites have a Z position of 0. These forcefields are in their own batch being sent to the card as a group and blended as a group but that's it. So it seems like I should be able to combine them in some manner given that. I still reckon Z-Buffer methodd would give best performance ... but I've given you an alpha test based alternative :) Wow thanks for all the great suggestions here! SEPERATEALPHABLENDENABLE is new to me that seems promising. However with the settings you have above I get zero drawn. When I then set the AlphaRef (as it is called in SlimDX don't know the name of the C++ equivalent) to 0xff then it shows up exactly like the standard image FFActual above. Not sure why that would be exactly I am not an expert on the Direct3D internals. For the Z buffer that is also something I have never worked with and I don't know exactly how that would work with a 2D game. Basically I tried turning on the Z Buffer (simple enough) but that did not do anything noticeable. Everything has a Z value of 0 so I guess that makes sense. So I changed the Z value of the force fields to be 1 and that didn't change anything either. When it comes to stencil buffers and so on I have no experience with that either; there are tutorials around and I will look at them now but all of them are trying to do really different things from me. You could also easily do it with the stencil buffer ... but getting your head round the Z-buffer will not only be the fastest but the best way. I'll put up some code a bit later. well edit your original question with any further questions and i will endeavour to answer them :) Hey that works! I'm still fighting with some quality issues that it is having but I've got it basically rendering except with a bunch of artifacts and garbage from the clears not being exactly right. But the blending is perfect! I'll mark this as closed as soon as I get the quality issues sorted I may have a last few couple of (hopefully brief) questions to get resolved on this. But great thanks so much already! Added it into the original question with a screenshot of the one remaining artifact. I think we almost have it at long last! :) Well give that a try. Hope that sorts ya out :) I probably won't find out if it does until tomorrow now ... Beautiful! That did it! I'll post the solution and award this now. Thanks for all your help that was great!  You can specify the D3DBLENDOP used when blending the two images together for the alpha channel. It sounds like your using D3DBLENDOP_ADD currently - try switching this to D3DBLENDOP_MAX as that will just use the opacity of the ""most opaque"" image. Okay here is a screenshot of what is actually happening with standard blending (the best I can get it): http://www.arcengames.com/share/FFActual.png Here is a screenshot with a mockup of how I would want the blending to turn out (the forcefields were added to the same layer in Photoshop then given a shared alpha value): http://www.arcengames.com/share/FFMockup.png I just saw your latest note -- don't know why it did not show it before. When I use that linear blend it makes it extremely opaque (just the barest of transparency remaining and it's still uneven alpha on the overlaps). That's one of the combinations I had already tried but I tried it again just to be sure. No dice... post a screenshot? Thanks -- I have tried that actually. It does not seem to change the result at all which is strange. I've also tried messing with the Alpha function and the reference value for it but have not been able to get anywhere with that either. It's curious to me that there is literally NO difference with D3DBLENDOP_MAX versus add it makes me wonder if there is some other setting I need to set in conjunction with it? Nothing I have tried has worked though. Make sure to set D3DRS_ALPHABLENDENABLE=TRUE. If it is this should work provided D3DPMISCCAPS_BLENDOP is supported in your device caps. (Some older graphics cards don't support anything but blend add). For details read the fine print at: http://msdn.microsoft.com/en-us/library/bb172599%28VS.85%29.aspx That is on and I'm using an 8800GTS. I think maybe this is an operations order problem of some sort -- as each image is added to the buffer the buffer's opacity is going up or something like that. I'm really not an expert on this aspect of graphics programming. The problem is that by the time there are around 12 overlaps these partially-transparent images form a completely opaque mass. Make sure to set source and dest blend functions. THere are many options: http://msdn.microsoft.com/en-us/library/bb173417%28VS.85%29.aspx I know -- that's what I'm trying to find out is the options to use. I've tried a huge number of combinations with no results for this specific problem. One option (linear blend between color including alpha values): lpD3DDevice->SetRenderState(D3DRS_SRCBLENDD3DBLEND_SRCCOLOR); lpD3DDevice->SetRenderState(D3DRS_DESTBLENDD3DBLEND_INVSRCCOLOR);  It is hard to tell exactly what you are trying to accomplish from your mock up since both forcefields are the same color; do you want to blend the colors and cap the alpha? Just take one of the colors? Based off the above discussion it isnt' clear if you are setting all the relevant render states: D3DRS_ALPHABLENDENABLE = TRUE (default: FALSE) D3DRS_BLENDOP = D3DBLENDOP_MAX (default: D3DBLENDOP_ADD) D3DRS_SRCBLEND = D3DBLEND_ONE (default: D3DBLEND_ONE) D3DRS_DESTBLEND = D3DBLEND_ONE (default: D3DBLEND_ZERO) It sounds like you are setting the first two but what about the last two? Unfortunately that still doesn't work. This is what I get: http://www.arcengames.com/share/FFZeroOne.png That's actually with D3DRS_DESTBLEND as D3DBLEND_ZERO by mistake but the D3DBLEND_ONE is 100% identical to it. Oh -- and as for what I am trying to accomplish this is how I did it in Photoshop: 1. Take the two images and remove all transparency (completely transparent pixels excepted). 2. Combine them into one layer which blends the color but which has no partial alpha at all. 3. Now set the global transparency for that layer to (say) 40%. The result is something that looks kind of blended together color-wise but which has no increase in opaqueness on the overlapped sections."
501,A,"Planning a 2D tile engine - Performance concerns As the title says I'm fleshing out a design for a 2D platformer engine. It's still in the design stage but I'm worried that I'll be running into issues with the renderer and I want to avoid them if they will be a concern. I'm using SDL for my base library and the game will be set up to use a single large array of Uint16 to hold the tiles. These index into a second array of ""tile definitions"" that are used by all parts of the engine from collision handling to the graphics routine which is my biggest concern. The graphics engine is designed to run at a 640x480 resolution with 32x32 tiles. There are 21x16 tiles drawn per layer per frame (to handle the extra tile that shows up when scrolling) and there are up to four layers that can be drawn. Layers are simply separate tile arrays but the tile definition array is common to all four layers. What I'm worried about is that I want to be able to take advantage of transparencies and animated tiles with this engine and as I'm not too familiar with designs I'm worried that my current solution is going to be too inefficient to work well. My target FPS is a flat 60 frames per second and with all four layers being drawn I'm looking at 21x16x4x60 = 80640 separate 32x32px tiles needing to be drawn every second plus however many odd-sized blits are needed for sprites and this seems just a little excessive. So is there a better way to approach rendering the tilemap setup I have? I'm looking towards possibilities of using hardware acceleration to draw the tilemaps if it will help to improve performance much. I also want to hopefully be able to run this game well on slightly older computers as well. If I'm looking for too much then I don't think that reducing the engine's capabilities is out of the question. I""m planning on more than 256 tiles so yeah Uint8 won't be big enough. Can i make a small point? a UInt16 seems a bit large to store tile types. Perhaps a byte would work better? (unless you are planning to have more than 256 types of tiles) I agree with Kombuwa. If this is just a simple tile-based 2D game you really ought to lower the standards a bit as this is not Crysis. 30FPS is very smooth (research Command & Conquer 3 which is limited to 30FPS). Even still I had written a remote desktop viewer that ran at 14FPS (1900 x 1200) using GDI+ and it was still pretty smooth. I think that for your 2D game you'll probably be okay especially using SDL. Yup i notice that i can tolerate quite slow frame rates (maybe 15-20) on games like Supreme Commander without much problem.  What about decreasing the frame rate to 30fps. I think it will be good enough for a 2D game. While this is certainly possible (And I'll keep it in mind) I'd rather optimize my code than reduce the capabilities of the engine. 30 FPS seems like an absolute minimum as the responsiveness is significantly lower and a 20 ms latency feels pretty much instantaneous compared to 33 ms.  You might consider merging neighbouring tiles with the same texture into a larger polygon with texture tiling (sort of a build process). The animations each run at their own speed. Some tiles animation one step every 8 frames some every 5 frames etc. As well since the actual tilemap can change at a moment's notice that adds another wrinkle into it. Although this would be a good idea with static tiles since some of the tiles are non-static this would mean I'd be constantly rebuilding textures due to how the animations work in the engine. Otherwise I'd use this right away. But wouldn't some be totally static? And even though some might be animated this wouldn't be a problem if they're meant to be synchronous.  I think you will be pleasantly surprised by how many of these tiles you can draw a second. Modern graphics hardware can fill a 1600x1200 framebuffer numerous times per frame at 60 fps so your 640x480 framebuffer will be no problem. Try it and see what you get. You should definitely take advantage of hardware acceleration. This will give you 1000x performance for very little effort on your part. If you do find you need to optimise then the simplest way is to only redraw the areas of the screen that have changed since the last frame. Sounds like you would need to know about any animating tiles and any tiles that have changed state each frame. Depending on the game this can be anywhere from no benefit at all to a massive saving - it really depends on how much of the screen changes each frame.  I think the thing that will be an issue is the sheer amount of draw calls rather than the total ""fill rate"" of all the pixels you are drawing. Remember - that is over 80000 calls per second that you must make. I think your biggest improvement will be to batch these together somehow. One strategy to reduce the fill-rate of the tiles and layers would be to composite static areas together. For example if you know an area doesn't need updating it can be cached. A lot depends of if the layers are scrolled independently (parallax style). Also Have a look on Google for ""dirty rectangles"" and see if any schemes may fit your needs. Personally I would just try it and see. This probably won't affect your overall game design and if you have good separation between logic and presentation you can optimise the tile drawing til the cows come home. I can't cache the output since the layers are fully independent of each other but it would likely be possible to cache the output of all the layers (except sprites) before they're all blitted to the final surface and only update as necessary. Thanks for the idea. Marking dirty won't be too much help since the engine would mean I'd spend more time finding dirty rectangles than I would just blitting dumbly. Yep sure. Just watch your draw call overhead. On a modern GPU I doubt fill rate will be an issue. I'm not sure how much SDL exposes but if I had full access to GL I'd be making some kind of texture atlas that included all the tiles and then indexing UVs into that from a strip of polys that described a row or column on the final buffer.  Make sure to use alpha transparency only on tiles that actually use alpha and skip drawing blank tiles. Make sure the tile surface color depth matches the screen color depth when possible (not really an option for tiles with an alpha channel) and store tiles in video memory so sdl will use hardware acceleration when it can. Color key transparency will be faster than having a full alpha channel for simple tiles where partial transparency or blending antialiased edges with the background aren't necessary. On a 500mhz system you'll get about 6.8 cpu cycles per pixel per layer or 27 per screen pixel which (I believe) isn't going to be enough if you have full alpha channels on every tile of every layer but should be fine if you take shortcuts like those mentioned where possible. The only layer where I can guarantee any chance at not having alpha is on the rearmost layer. The other three layers I have no way of predicting which tiles will and which tiles will not use alpha transparency.  Can you just buffer each complete layer into its view plus an additional tile size for all four ends(if you have vertical scrolling) use the buffer again to create a new buffer minus the first column and drawing on a new end column? This would reduce a lot of needless redrawing. Additionally if you want a 60fps you can look up ways to create frame skip methods for slower systems skipping every other or every third draw phase. Thanks for the input but I ended up switching to openGL 3D acceleration and am now getting over 980 FPS"
502,A,in GDI+ how to pass graphics to image In C# I am drawing things on panel using creategraphics and I need to pass these things to an image or a bitmap how can I do that? thank you You can draw directly to an Image by calling Graphics.FromImage. You can draw a Control to a Bitmap by calling Control.DrawToBitmap By the way the correct way to draw to a control is to handle its Paint event and draw to e.Graphics in it. If you draw a control using CreateGraphics it will be erased when the control is redrawn (eg if you drag a window over it pre-Vista)
503,A,Drawing an empty polygon given a set of points on a Map Overylay (Android 2.1) I've set of n points and I want to draw a n-sided polygon using these points. I tried using android.graphics.path (please see below). Path path = new Path(); Vertex currVtx; for (int j = 0; j < vertices.size(); j++) { currVtx = vertices.get(j); if (!currVtx.hasLatLong()) continue; Point currentScreenPoint = getScreenPointFromGeoPoint( currVtx mapview); if (j == 0) path.moveTo(currentScreenPoint.x currentScreenPoint.y); // vertex. else path.lineTo(currentScreenPoint.x currentScreenPoint.y); } Currently I'm getting a solid (filled with the color of the canvas) polygon with this code. Is there a way that I can get an unfilled polygon. There is an alternative to android.graphics.Path Thanks. Hi! Can you explain how you implemented getScreenPointFromGeoPoint please? I am trying to draw a filled polygon but I don't knwo how the get the screen point from the lat and long I have. If you can share that I woud be greatful Define a Paint object: Paint mPaint = new Paint(); mPaint.setStrokeWidth(2); //2 pixel line width mPaint.setColor(0xFF097286); //tealish with no transparency mPaint.setStyle(Paint.Style.STROKE); //stroked aka a line with no fill mPaint.setAntiAlias(true); // no jagged edges etc. Then draw the path with: yourCanvas.drawPath(pathmPaint); You can look up the paint documentation but there are tons of options to control how its drawn. Thanks ! This works. How can a I get a canvas to be drawn on top of my mapview?
504,A,Efficiently draw a grid in Windows Forms I'm writing an implementation of Conway's Game of Life in C#. This is the code I'm using to draw the grid it's in my panel_Paint event. g is the graphics context. for (int y = 0; y < numOfCells * cellSize; y += cellSize) { for (int x = 0; x < numOfCells * cellSize; x += cellSize) { g.DrawLine(p x 0 x y + numOfCells * cellSize); g.DrawLine(p 0 x y + size * drawnGrid x); } } When I run my program it is unresponsive until it finishes drawing the grid which takes a few seconds at numOfCells = 100 & cellSize = 10. Removing all the multiplication makes it faster but not by very much. Is there a better/more efficient way to draw my grid? Thanks If you just single-step through this in the IDE you would be able to answer your own question. The problem is that you are drawing the X lines for every Y coordinate. You can simplify first by just rendering the Y lines in one loop and then the X lines in another loop. Here is a quick example  for (int y = 0; y < numOfCells; ++y) { g.DrawLine(p 0 y * cellSize numOfCells * cellSize y * cellSize); } for (int x = 0; x < numOfCells; ++x) { g.DrawLine(p x * cellSize 0 x * cellSize numOfCells * cellSize); } As you progress you can use double buffering to reduce any flashing etc. Take a look at Control.SetStyle < br/> http://msdn.microsoft.com/en-us/library/system.windows.forms.control.setstyle.aspx or in the same loop if the grid is a square ;) @Thomas agreed. I struggle with this when answering my view is that the OP was not seeing the basic problem and that is why I go for the obvious first and then once the problem is understood improve from there. But just an opinion. Ugh I don't know what I was thinking with the nested loops. Thanks for the help. Enabling double buffering doesn't seem to do anything but that's okay there's only a little flickering when I start it up. @Joel you should look at Thomas solution which is even more optimized since it only does one loop rather than the 2 above. That's actually the solution I've used since my grid is a square. However I chose your solution as the answer since you gave a little more detail.  You should take a look at XNA for this. It would probably be more efficent to do this in a rendered window instead of a WinForm. XNA is the game framework for C#. Find more information at http://creators.xna.com/  Create a bitmap image of say 100x100 pixels with the grid lines and display it in tiles. Make sure that the seam between tiles doesn't cause a discontinuity in the spaces of the grid.  You don't need nested loops : for (int i = 0; i < numOfCells; i++) { // Vertical g.DrawLine(p i * cellSize 0 i * cellSize numOfCells * cellSize); // Horizontal g.DrawLine(p 0 i * cellSize numOfCells * cellSize i * cellSize); }
505,A,"Right way to dispose Image/Bitmap and PictureBox I am trying to develop a Windows Mobile 6 (in WF/C#) application. There is only one form and on the form there is only a PictureBox object. On it I draw all desired controls or whatever I want. There are two things I am doing. Drawing custom shapes and loading bitmaps from .png files. The next line locks the file when loading (which is an undesired scenario): Bitmap bmp = new Bitmap(""file.png""); So I am using another way to load bitmap. public static Bitmap LoadBitmap(string path) { using (Bitmap original = new Bitmap(path)) { return new Bitmap(original); } } This is I guess much slower but I don't know any better way to load an image while quickly releasing the file lock. Now when drawing an image there is method that I use: public void Draw() { Bitmap bmp = new Bitmap(240320); Graphics g = Graphics.FromImage(bmp); // draw something with Graphics here. g.Clear(Color.Black); g.DrawImage(Images.CloseIcon 16 48); g.DrawImage(Images.RefreshIcon 46 48); g.FillRectangle(new SolidBrush(Color.Black) 0 100 240 103); pictureBox.Image = bmp; } This however seems to be some kind of a memory leak. And if I keep doing it for too long the application eventually crashes. Therefore I have 3 questions: 1.) What is the better way for loading bitmaps from files without locking the file? 2.) What objects needs to be manually disposed in the Draw() function (and in which order) so there's no memory leak and no ObjectDisposedException throwing? 3.) If pictureBox.Image is set to bmp like in the last line of the code would pictureBox.Image.Dispose() dispose only resources related to maintaining the pictureBox.Image or the underlying Bitmap set to it? I don't think there is a real memory leak. The problem is that you don't dispose the old bitmap it is up to the GC to clean the stuff. But there is no deterministic way to say when this will happen. So i think if you're going to loop through a lot of pictures you'll see some memory increase and at some other point it will fall down or resist at one position. I didn't test it but maybe this will help a little to make it more deterministic: public void Draw() { Bitmap bmp = new Bitmap(240320); using(var g = Graphics.FromImage(bmp)) using(var solidBrush = SolidBrush(Color.Black)) { // draw something with Graphics here. g.Clear(Color.Black); g.DrawImage(Images.CloseIcon 16 48); g.DrawImage(Images.RefreshIcon 46 48); g.FillRectangle(solidBrush 0 100 240 103); //Backup old image in pictureBox var oldImage = pictureBox.Image; pictureBox.Image = bmp; //Release resources from old image if(oldImage != null) ((IDisposable)oldImage).Dispose(); } } Update And another idea inspired by jack30lena: public static Bitmap LoadBitmap(string path) { //Open file in read only mode using (FileStream stream = new FileStream(path FileMode.Open FileAccess.Read)) //Get a binary reader for the file stream using (BinaryReader reader = new BinaryReader(stream)) { //copy the content of the file into a memory stream var memoryStream = new MemoryStream(reader.ReadBytes((int)stream.Length)); //make a new Bitmap object the owner of the MemoryStream return new Bitmap(memoryStream); } } The idea behind my second code sample is to get rid of the file handle and copy the file content into memory. Afterwards the Bitmap will taken ownership of the MemoryStream which will be disposed within my first sample by calling the oldImage.Dispose(). By using this approach there should never be more then two images in memory thous leading only to OutOfMemoryExceptions by really big pictures or small amount of RAM. This may prove to be a good solution but the problem is that it seems the GC on WM is either called rarely enough or not at all for some items because the code I posted caused OutOfMemoryException and app crashes and if I add GC.Collect() at the method end memory keeps steady. Very strange. Unfortunately i don't have any experience with Windows Mobile and the Compact Framework but if this will lead to OutOfMemoryExceptions the above code should help to avoid this (if it happens within this code part) I'll look into it. Thanks for suggestions. It seems that your code sample from jack30lena does not dispose/close the MemoryStream. This might result in a leak. @Stegi: That shouldn't be a problem cause the Bitmap takes ownership of the stream. So you can't use a `using` statement for the stream cause otherwise the Bitmap would throw an exception if you try to access the picture after leaving the `using` statement (see [remarks section of ctor](http://msdn.microsoft.com/en-us/library/z7ha67kw.aspx)). But the bitmap itself should (i didn't check it) call Dispose() of the underlying stream when itself will be closed. @Oliver: Really? I think it wouldn't be good if the Bitmap takes control over releasing the stream. What if two classes shares the same injected object? @Stegi: I think this would lead to other problems if both are starting to read/seek in parallel within the same stream. So i think it would be wise to give every Bitmap its own stream (cause in my eyes it will get owner of the stream). @Oliver: I can't find any dispose/close call within the sources so it is required to ensure the closing of the stream outside the Bitmap class e.g. through a using block for releasing ressources! Correct me if I am wrong. @Oliver: Regarding to the other aspect of injection: would you call a delete/free to ressources you have not created with new/malloc (c++)? This will lead into memory management problems of your architecture. See: http://en.wikipedia.org/wiki/Solid_(object-oriented_design). There is no problem in using a stream twice as long as it is not used in parallel (which might be not supported by the stream). You always have to ensure that the stream is at the correct ""position"" if you start reading. @Stegi: I never looked into the sources. So i think your assumption is right but you can't enclose it within a using statement. Otherwise your bitmap will throw an exception if you try to display it after leaving the using block or calling dispose on the stream. In that case you have to return a Tuple and the receiver of this tuple is responsible to call Dispose on both elements instead of only on Bitmap. @Oliver: Did you tried this or is this an assumption? If you put a return with the construction of bitmap into the using block 1. the image is created and ready to use 2. the stream is disposed (is no longer uses (already copied by GPStream)... so it should be safe to use it this way. Nearly the same like copy in the sample LoadBitmap(string path) in this question using a steam. @Stegi: I made a simple test application took the above code sample and replaced the MemoryStream with using block. The caller of the method tried to show the bitmap within a picture box and that throed an exception which lead to all my further assumptions.  1: I dont know if it works in WM but try this: FileStream bitmapFile = new FileStream(""mybitmap.bmp"" FileMode.Open FileAccess.Read FileShare.ReadWrite); Image loaded = new Bitmap(bitmapFile); 2: The SolidBrush must be disposed. There is a general rule for dispose. --> ""every object instanciated by you that implements dispose must be disposed manually exept when the object is a return/ref/out value"" In this case it is better to use a using statement using (new objecttodispose){ ..... } The using statement will ensure the call of Dispose() in any case (exception for example). 3: Dispose() will free the bitmap ressources. Actually Bitmap and Image never close the underlying stream. You'll have an eternal lock on that file until the GC cares about collecting the floating reference."
506,A,"A Simple 2d cross-platform graphics library for c or c++? As in title i need a 2d graphics library that is cross-platform and provides simple functions like in Basic; essentially i only need to paint a pixel a certain color-I do not need hardware acceleration or any kind of 3d support. I've found a couple ones but they're not cross-platform. Anyone knows a solution for me? GTK QT WxWidgets - heavy-weight; FLTK Fox Tk Lua IUP Ultimate++ dlib - lightweight; SDL Cairo - drawing frameworks without GUI widgets  Picasso graphic library you can used: cross platform https://code.google.com/p/picasso-graphic/  One neat engine I came across is Angel-Engine. Info from the project site: Cross-Platform functionality (Windows and Mac) Actors (game objects with color shape responses attributes etc.) Texturing with Transparency ""Animations"" (texture swapping at defined intervals) Rigid-Body Physics A clever programmer can do soft-body physics with it Sound Text Rendering with multiple fonts Particle Systems Some basic AI (state machine and pathfinding) Config File Processing Logging Input from a mouse keyboard or Xbox 360 controller Binding inputs from a config file Python Scripting In-Game Console Some users (including me) have succesfully (without any major problems) compiled it under linux.  Am I missing something to wonder why noone suggests OpenGL? To use it for 2d would be very simple. The OP only wants to color a pixel. It doesn't get simpler than glBegin/glColor/glVertex/glEnd. OpenGL doesn't guarantee identical pixel output across implementations. If the guy is using it for 2d pixel coloring meaning lighting/texturing/etc. is turned off it should absolutely have identical output.  Qt 4.5 GTK+ Cairo and many many more ... [In no particular order.] However if you have any other requirements let us know. BTW: I am not just posting results of a Google query here I have used all of these (and SDL -- wrote my first few games in SDL :) and I'd say without a set of requirements it's very difficult to choose among the ones listed. why qt 4.2? the latest version of qt is also good i reckon :) I had the doc links for 4.2 handy -- so mentioned it in the post. Agreed Qt (4.5) is also an excellent choice.  http://www.allegro.cc/ http://en.wikipedia.org/wiki/Allegro_library  I would recommend DISLIN. It's cross platform has support for many languages and has very intuitive naming of routines. Also just noticed that nobody mentioned PLPLOT also cross platform multi lingual ... @dagw - you mean for commercial use. It is free for non commercial use. While DISLIN seems pretty cool it is worth noting that it cost $180 pr developer for non-commercial use.  What about SDL? Perhaps it's a bit too complex for your needs but it's certainly cross-platform. It worked with minimal effort. The fact that sdl-config exist and that debian had all the stuff preinstalled helped a lot. I agree it's overkill but it's simple enough for what i need. Thanks a lot for everything! SDL is indeed more than Agasa needs but it should be straightforward enough to use."
507,A,"Drawing Library for Ruby I am trying to code a flowchart generator for a language using Ruby. I wanted to know if there were any libraries that I could use to draw various shapes for the various flowchart elements and write out text to those shapes. I would really prefer not having to write code for drawing basic shapes if I can help it. If someone could point me to some reference documentation with examples of using that library it would be great. Thanks! @Mike: Thanks for your pointers and insightful commentary. I appreciate it very much. I had the same question Let me know id there is anything comparable to System.Drawing namespace in .net ImageMagick has a Ruby binding called RMagick (sometimes known by other names depending on the repository). (Link) I haven't used it myself but I believe it will do what you're looking for. You will need to do some coding but it's along the lines of draw.rectangle(x1 y1 x2 y2) draw.polygon(x1 y1...xN yN)  Write up your flowchart as a directed or undirected graph in Graphviz. Graphviz has a language dot that makes it easy to generate graphs. Just generate the dot file run it through Graphiviz and you get your image. graph { A -- B -- C; B -- D; C -- D [constraint=false]; } renders as digraph { A [label=""start""]; B [label=""eat""]; C [label=""drink""]; D [label=""be merry""]; A -> B -> C; C -> D [constraint=false]; B -> D [ arrowhead=none arrowtail=normal]; // reverse this edge } renders as You can control node shapes and much more in Graphviz.  It sounds like you're going to be limited mainly by the capabilities of whatever user agent you're building for; if this is a web project drawing capabilities are going to be dependent on the browser. Since Ruby is running server-side you would at minimum need some JavaScript to allow dragging/zooming etc. There's plenty of examples of JavaScript being used for vector drawing (just google ""javascript graphics library"") but all require coding and I haven't seen any library that abstracts this elegantly.  I am totally sure but check out cairo bindings for ruby. Pango for text. I am investigating then currently and came across this page.  The simple answer is that (at time of writing) what you want almost certainly didn't exist. orry! If you're Windows-based then I'd look for a product in the .NET space where I expect you'd find something. You're probably going to have to pay real money though. I suppose if you're brave you may be able to talk to it using IronRuby. From a non-MS environment I would be hoping for a Java-based solution. As already mentioned within the Web world you're going to be hoping some something JavaScript-based or probably more likely something from the Flash (or maybe even Silverlight?) world. Actually if you want to stay in Ruby and don't mind some risk Silverlight might be a way to go - assuming the DLR stuff actually works (no experience here). Ruby - much as I love it - hardly has the most mature GUI structure around it. Update: Since writing this (approaching 5 years ago) the situation has improved - a little. While I'm still not aware of any specific Ruby graph-drawing libraries there is improved support for Graphviz here (Github) or by gem install ruby-graphviz. I've found that for simple graphs it's quite possible to write the script directly from Ruby. Some libraries do exist for ruby. Although you would have to work only with basic elements. As Nathan pointed out there's https://rubygems.org/gems/rmagick but there's also https://rubygems.org/gems/cairo. Here's a simple tutorial http://www.hokstad.com/simple-drawing-in-ruby-with-cairo.html. And here's another that uses GTK http://zetcode.com/tutorials/rubygtktutorial/cairo/. I would have to agree that it's not the best API but it's a start. ""I need something with Ruby"" ""Try .NET"" WTF."
508,A,"How to use GL_REPEAT to repeat only a selection of a texture atlas? (OpenGL) How can I repeat a selection of a texture atlas? For example my sprite (selection) is within the texture coordinates: GLfloat textureCoords[]= { .1f .1f .3f .1f .1f .3f .3f .3f }; Then I want to repeat that sprite N times to a triangle strip (or quad) defined by: GLfloat vertices[]= { -100.f -100.f 100.f -100.f -100.f 100.f 100.f 100.f }; I know it has something to do with GL_REPEAT and textureCoords going passed the range [01]. This however doesn't work: (trying to repeat N = 10) GLfloat textureCoords[]= { 10.1f 10.1f 10.3f 10.1f 10.1f 10.3f 10.3f 10.3f }; We're seeing our full texture atlas repeated... How would I do this the right way? I'm not sure you can do that. I think OpenGL's texture coordinate modes only apply for the entire texture. When using an atlas you're using ""sub-textures"" so that your texture coordinates never come close to 0 and 1 the normal limits where wrapping and clamping occurs. There might be extensions to deal with this I haven't checked. EDIT: Normally to repeat a texture you'd draw a polygon that is ""larger"" than your texture implies. For instance if you had a square texture that you wanted to repeat a number of times (say six) over a bigger area you'd draw a rectangle that's six times as wide as it is tall. Then you'd set the texture coordinates to (00)-(61) and the texture mode to ""repeat"". When interpolating across the polygon the texture coordinate that goes beyond 1 will due to repeat being enabled ""wrap around"" in the texture causing the texture to be mapped six times across the rectangle. This is a bit crude to explain without images. Anyway when you're texturing using just a part of the texture there's no way to specify that larger texture coordinate in a way that makes OpenGL repeat it inside only the sub-rectangle. Hmm GL_CLAMP for example affects the sub-textures you're describing leading me to the idea texture coordinate mode do apply to parts of a texture atlas. Or am I in the wrong here? (Of course seeing that I can't repeat the way I'd like implies I indeed am in the wrong:))  None of the texture wrap modes support the kind of operation you are looking for i.e. they all map to the full [01] range not some arbitrary subset. You basically have two choices: Either create a new texture that only has the sprite you need from the existing texture or write a GLSL pixel program to map the texture coordinates appropriately.  While this may be an old topic; here's how I ended up doing it: A workaround would be to create multiple meshes glued together containing the subset of the Texture UV's. E.g.: I have a laser texture contained within a larger texture atlas at U[0.05 - 0.1] & V[0.05-0.1]. I would then construct N meshes each having U[0.05-0.1] & V[0.05-0.1] coordinates. (N = length / texture.height; height being the dimension of the texture I would like to repeat. Or easier: the amount of times I want to repeat the texture.) This solution would be more cost effective than having to reload texture after texture. Especially if you batch all render calls (as you should). (OpenGL ES 1.01.12.0 - Mobile Hardware 2011) Interesting nonetheless. Thanks! :)  It can't be done the way it's described in the question. OpenGL's texture coordinate modes only apply for the entire texture. Normally to repeat a texture you'd draw a polygon that is ""larger"" than your texture implies. For instance if you had a square texture that you wanted to repeat a number of times (say six) over a bigger area you'd draw a rectangle that's six times as wide as it is tall. Then you'd set the texture coordinates to (00)-(61) and the texture mode to ""repeat"". When interpolating across the polygon the texture coordinate that goes beyond 1 will due to repeat being enabled ""wrap around"" in the texture causing the texture to be mapped six times across the rectangle. None of the texture wrap modes support the kind of operation as described in the question i.e. they all map to the full [01] range not some arbitrary subset. when you're texturing using just a part of the texture there's no way to specify that larger texture coordinate in a way that makes OpenGL repeat it inside only the sub-rectangle. You basically have two choices: Either create a new texture that only has the sprite you need from the existing texture or write a GLSL vertex program to map the texture coordinates appropriately. Thanks to ""unwind"" ""Jimmy J"" and ""TrayMan"" for your contributions. I voted your answers up and compiled this answer from yours which now describes very well what's going on. >""or write a GLSL vertex program to map the texture coordinates appropriately."" There must be *fragment* program instead.  Can't be done... That's your fixed pipeline talking."
509,A,"Volumetric particles I'm toying with the idea of volumetric particles. By 'volumetric' I don't mean actually 3D model per particle - usually it's more expensive and harder to blend with other particles. What I mean is 2D particles that will look as close as possible to be volumetric. Right now what I/we have tried is particles with additional local Z texture (spherical for example) and we conduct the alpha transparency according to the combination of the alpha value and the closeness by Z which is improved by the fact that particle does not have a single planar Z. I think a cool add would be interaction with lighting (and shadows as well) but here the question is how will the lighting formula look like (taking transparency into account let's assume that we are talking about smoke and dust/clouds and not additive blend) - any suggestions would be welcomed. I also though about adding normal so I can actually squeeze all in two textures: Diffuse & Alpha texture. Normal & 256 level precision Z channel texture. I ask this question to see what other directions can be thought of and to get your ideas regarding the proper lighting equation that might be used. A nice paper on using spherical billboards to represent volumetric effects: http://www.iit.bme.hu/~szirmay/firesmoke_link.htm Doesn't handle particpating media though. Thanks Andy - I read the article at the time thank you for reminding me. Great source!  See Volume Rendering and Voxel.  It sounds like you are asking for information on techniques for the simulation of participating media: ""Participating media may absorb emit and/or scatter light. The simplest participating medium only absorbs light. That means that light passing through the medium is attenuated depending on the density of the medium."" Here are some links to some example images and to Frisvad Christensen Jensen's the SIGGRAPH 2007 paper (including the PDF)."
510,A,Framework/library for simple 2D animation in Java? I want to write a very simple game in Java to demonstrate a wireless controller I've built. I thought of something like Breakout or Pong. Currently I have a prototype Pong implementation that does all animation directly using the AWT functionality. However this is somewhat awkward to program and also a major CPU hog. My question: Can someone recommend a library for Java to display simple 2D animations? What have you used for similar projects? The library should be easy and straight-forward to use -- I'm not looking for something like Java3D. Integrated collision detection would be a pro. This question relates to comparisons of Java 2D frameworks and may be of use. I'm interested as to why your original implementation is a CPU hog. Is that just whilst it's drawing or is it consuming CPU resource all the time ? If the latter it may point to a problem wrt. how you're querying/polling your controllers. Thanks for the link to the post. I guess my application is a CPU hog because I re-draw the screen every 30 ms from a Thread using a double buffer. It's not controller related -- I took the controller-polling out and it still consumes a lot of CPU. I will look into JGame now as it was suggested in the link you mentioned. A dumb question but do you need to redraw the screen every 30ms ? Or only when a controller/ball moves ? Or perhaps only redraw part of the screen ? Anywaybest of luck! @Brian Agnew/rodian. My guess is that he wants smooth animation of a continuously moving ball... hence redraw every 30ms will give you a nice smooth refresh rate (FPS) of 33.333 (anything above 24 is good in my opinion because 24 is used in movie theatres). Makes sense. I meant to say 'redraw the *whole* screen every 30ms' Yes I should have re-drawn the screen in a smarter way but I was too lazy. I now re-wrote the game using JGame a framework mentioned in the link you posted Brian. Thanks for pointing me to the question. No problem. Glad it worked out
511,A,"Desktop graphics - or ""skinned"" windows I'm looking for a way to draw animations right on the desktop. No window frames and with transparent background. I'm using Python in windows XP for it but it doesn't have to be cross platform although it'd be a nice bonus. Does anyone know about a python library that can do this? I found that wxPython was able to do this after all. You always find things when you've stopped looking for some reason. I stumbled upon the shaped window setting when I was looking through the demo file for wxPython. I haven't been able to find any good tutorials for it on the net but the code in the demo should be enough to get started. It should even be platform independent. wxPython official site wxPython downloads (including demo files) At least with my distribution of WXpython the transparent portions of the window will show up white about 1 in 5 times I draw them. Also the transparency is actually just a snapshot of the content behind it so if you move a window behind it you can tell.  If you want a frameless window there are several options. For example pygame can be initialized with the following flag: pygame.init() screen = pygame.display.set_mode(size=(640480) pygame.NOFRAME) Your question doesn't make it clear if you're looking for a transparent surface though. I guess I was a bit presumptuous there. Yes I'm looking for a transparent window without borders."
512,A,c#: Choosing or modifying a control to create draggable graphics on a canvas I'm attempting to write a small app that will do something similar to what is shown here: http://www.soccertutor.com/tour/MASTER_SKIN_team.swf However I've not used WPF/Win Forms very much at all and can't seem to find a control that will be most applicable to the situation. I want to have some sort of grid for the pitch so I can have players cones other items snap to certain parts of the grid and I would like the players to be draggable onto the various grid squares. Thanks very much I look forward to hearing your recommendations/suggestions! You don't use controls for something like this. You use objects in your code that represent a visible item on the screen. Periodically you render an update to the screen. You draw the background then each object. You add mouse hit testing to make the scene interactive to the user. The main game loop calculates new object positions implements object collision detection and game rules. So you think the best way to go about this would be to draw everything as paths etc. ?? This seems very tedious. Yes game programming can be very tedious. Look at something like XNA to take the tedium out of it.
513,A,Any graphics library for unix that can draw histograms? A python program needs to draw histograms. It's ok to use 3rd party library (free). What is the best way to do that? Gnuplot.py lets you use Gnuplot from python.  You can use matplotlib. Thaks. Exactly what's needed.  How much power do you need? How much external weight are you willing to take on? ROOT is accessible in python using PyROOT. Heavy and a lot to learn to get the most out of it but very powerful. Not that much. But thanks for info anyway.
514,A,"Java2D: Capturing an event on a Line object I have a JPanel which has a line circle etc. Now when I click on the line will the event get reported as a line event or a general JFrame event. I need to be able to move the line if the user clicks on the line and moves it. Is this possible in Java2D? Yes but you will need to do some work (see java.awt.Shape). Basically you need to track a list of Shapes. The JPanel will recieve a mouse event which you can translate to (xy) coordinates. You can then call Shape.contains(xy) to see if your various shapes were clicked on. This will work well for Circle Polygon Arc etc; however in the case of Line2D it won't work as easily but you can use Line2D.intersects() with a small rectangle around the mouse click (this is also good UI since you don't want to force the user to click exactly on a pixel that is hard to see).  I created a canvas markup library in Java a few years back and if you don't need to worry about transforms on the canvas (scaling rotation etc.) it is very easy to do. Basically you just need to maintain a collection of the canvas shapes in a List (not a Set because Z order is probably important). The mouse listener will be on your canvas and not on individual shapes. Add new items to the beginning of your collection (or iterate the list backwards later). When the canvas receives a mouse down event iterate through your collection of shapes until you find one that is underneath your mouse coordinates. The easiest way to do this is to have your shapes implement an interface that defines some sort of hitPoint(int x int y) method. That way your rectangles can implement a contains() lines can do intersects() or graphics paths you can account for some hit padding etc. Taking it one step further your shapes should define their own draw(Graphics2D g) method so that you can easily do things like selection boxes or set the paint mode to XOR to make shape 'moving' easier. The paintComponent method of your canvas would just have to iterate through your collection of shapes calling shape.draw(g) on each one passing in the graphics instance provided to the paintComponent method.  There is no such concept as a ""line event"" unless you decide to implement one. I would suggest adding a MouseListener and a MouseMotionListener to the Canvas or JPanel onto which your geometric shapes are drawn. Use the MouseListener's mousePressed(MouseEvent) callback to determine whether a given shape has been clicked on. Once you've established this use MouseMotionListener's mouseDragged(MouseEvent) method to move and redraw the shape as the mouse cursor is moved.  Here's a simple example that demonstrates some of the techniques adduced in other answers."
515,A,"iPhone + Quartz: How to get a water effect ist there any chance to build a water effect with Quartz on the iPhone? Something similar to this one: For that I highly recommend using OpenGL ES. Despite the ""simplicity"" of the problem you addressed in this SO question I recommended OpenGL ES then as well. So you can imagine why I'm advising it again. Check this SO question for the water effect with OpenGL."
516,A,"Java Graphics library I'm looking for a high-level java graphic library for creating artistic text watermarks resize crop image identification and manipulation. ImageMagic is a good example of such library but its java ports are somewhat problematic (they either run imagemagic through JNI or via commandline and are hellish to deploy to servers). Ideally I'd like to have similar functionality to ImageMagic but pure Java and open-source free to use. Has anyone seen something like that? This is for a server-side component. A service that manipulates images of various web formats (png jpg gif etc). Java has its own libraries of course (Graphics2D) but I'm looking for something of higher level. Here are several use cases: Resize and crop images. If it has ""smart resize"" or ""smart crop"" that'll be cool for example seam carving resize or cropping by points of interest in the photo Drawing artistic text on images. Using fonts colors text effects (3d text charcoal and other effects) Embedding watermarks. Layering images using images as background masking with images etc. Image identification such as - number of colors stdev etc. As mentioned Java in its Graphics2D supports all of the above but is too low level so I'm looking for something that's nicer to work with. Thanks! I've used packages that manipulated images that depended on http://java.sun.com/javase/technologies/desktop/media/jai/ I'm not sure if it would meet all your needs. Raw Java2D makes it quite easy to do resizing and cropping (just make sure that you reduce in powers of 2 the backend doesn't handle this automatically). I only found JAI useful for my usage to do color quantization... the rest of the pipeline seemed unnecessary for basic functions.  The Apache SVG Batik library can be used to display some vectorial drawing in SVG. The eclipse project contains a library called Graphical Editing Framework (GEF) shipped with a painting and layout plug-in called Draw2D. see http://whf.poac.ac.cn/codeopen/jiaocheng/java2s/Code/Java/SWT-JFace-Eclipse/2D.htm. I'm actually looking for a bitmap manipulation package not a vector one so both SVG and EMF don't seem like a good fit :(  Java Advanced Imaging ( JAI ) sounds like what you want. From the website: The Java Advanced Imaging API provides a set of object-oriented interfaces that support a simple high-level programming model which lets you manipulate images easily. I found it relatively simple to work with. And the performance was better than spinning off ImageMagic processes. From my experience with it (for Computer Vision and Graphics) it does quite awesome.  Use ImageJ. In addition to being a neat program it can be used as a library."
517,A,"Really Basic Graphics in C# 2.0 Tutorials I work for a ticketing agency and we print out tickets on our own ticket printer. I have been straight coding the ticket designs and storing the templates in a database. If we need a new field adding to a ticket I manually add it and use the arcane co-ordinate system to estimate where the fields should go and how much the other fields need to move by to accomodate new info. We always planned to make this system automate with a simple (I stress the word simple) graphical editor. Basically we don't forsee tickets changing radically in shape any time soon we have one size of ticket and the ticket printer firmware is super simple because it's more of an industrial machine it has about 10 fonts and some really basic sizing interactions. I need to make this editor display a rectangle of the dimensions by pixel of the tickets (can even be actual size) and have a resizable grid which can toggle between superimposition and invisibility on top of the ticket rectangle and represented by dots rather than lines. Then I want to be able to represent fields by drawing rectangles filled with the letter ""x"" that show the maximum size of the field (to prevent overlaps). These fields should be selectable draggable and droppable in a snap to grid fashion. I've worked out the maths of it but I have no idea how to draw rectangles and then draw grids in layers and then put further rectangles full of 'x'es on top of those. I also don't really know much about changing drawn positions in accordance with mouse events. It's simply not something I've ever had to do. All the tutorials I've seen so far presume that you already know a lot about using the draw objects and are seeking to extend a basic knowledge of these things. I just need pointing in the direction of a good tutorial in manipulating floating objects in a picturebox in the first place. Any ideas? What level are you at with C# and WinForms? Have you managed to create a form with controls display it and respond to user interaction? For those of you in need of a guide to this unusual (at least those of us with a BIS background) field I would heartily endorse: http://www.bobpowell.net/faqmain.htm I am now happily drawing graphical interfaces and getting them to respond to control inputs with not too much hassle."
518,A,"Inverse Bilinear Interpolation? I have four 2d points p0 = (x0y0) p1 = (x1y1) etc. that form a quadrilateral. In my case the quad is not rectangular but it should at least be convex.  p2 --- p3 | | t | p | | | p0 --- p1 s I'm using bilinear interpolation. S and T are within [0..1] and the interpolated point is given by: bilerp(st) = t*(s*p3+(1-s)*p2) + (1-t)*(s*p1+(1-s)*p0) Here's the problem.. I have a 2d point p that I know is inside the quad. I want to find the st that will give me that point when using bilinear interpolation. Is there a simple formula to reverse the bilinear interpolation? Thanks for the solutions. I posted my implementation of Naaff's solution as a wiki. See also [this writeup](http://www.iquilezles.org/www/articles/ibilinear/ibilinear.htm). Since you're working in 2D your bilerp function is really 2 equations 1 for x and 1 for y. They can be rewritten in the form: x = t * s * A.x + t * B.x + s * C.x + D.x y = t * s * A.y + t * B.y + s * C.y + D.y Where: A = p3 - p2 - p1 + p0 B = p2 - p0 C = p1 - p0 D = p0 Rewrite the first equation to get t in terms of s substitute into the second and solve for s.  If all you have is a single value of p such that p is between the minimum and maximum values at the four corners of the square then no it is not possible in general to find a SINGLE solution (st) such that the bilinear interpolant will give you that value. In general there will be an infinite number of solutions (st) inside the square. They will lie along a curved (hyperbolic) path through the square. If your function is a vector valued one so you have two known values at some unknown point in the square? Given known values of two parameters at each corner of the square then a solution MAY exist but there is no assurance of that. Remember that we can view this as two separate independent problems. The solution to each of them will lie along a hyperbolic contour line through the square. If the pair of contours cross inside the square then a solution exists. If they do not cross then no solution exists. You also ask if a simple formula exists to solve the problem. Sorry but not really that I see. As I said the curves are hyperbolic. One solution is to switch to a different method of interpolation. So instead of bilinear break the square into a pair of triangles. Within each triangle we can now use truly linear interpolation. So now we can solve the linear system of 2 equations in 2 unknowns within each triangle. There may be one solution in each triangle except for a rare degenerate case where the corresponding piecewise linear contour lines happen to be co-incident.  I think it's easiest to think of your problem as an intersection problem: what is the parameter location (st) where the point p intersects the arbitrary 2D bilinear surface defined by p0 p1 p2 and p3. The approach I'll take to solving this problem is similar to tspauld's suggestion. Start with two equations in terms of x and y: x = (1-s)*( (1-t)*x0 + t*x2 ) + s*( (1-t)*x1 + t*x3 ) y = (1-s)*( (1-t)*y0 + t*y2 ) + s*( (1-t)*y1 + t*y3 ) Solving for t: t = ( (1-s)*(x0-x) + s*(x1-x) ) / ( (1-s)*(x0-x2) + s*(x1-x3) ) t = ( (1-s)*(y0-y) + s*(y1-y) ) / ( (1-s)*(y0-y2) + s*(y1-y3) ) We can now set these two equations equal to each other to eliminate t. Moving everything to the left-hand side and simplifying we get an equation of the form: A*(1-s)^2 + B*2s(1-s) + C*s^2 = 0 Where: A = (p0-p) X (p0-p2) B = ( (p0-p) X (p1-p3) + (p1-p) X (p0-p2) ) / 2 C = (p1-p) X (p1-p3) Note that I've used the operator X to denote the 2D cross product (e.g. p0 X p1 = x0*y1 - y0*x1). I've formatted this equation as a quadratic Bernstein polynomial as this makes things more elegant and is more numerically stable. The solutions to s are the roots of this equation. We can find the roots using the quadratic formula for Bernstein polynomials: s = ( (A-B) +- sqrt(B^2 - A*C) ) / ( A - 2*B + C ) The quadratic formula gives two answers due to the +-. If you're only interested in solutions where p lies within the bilinear surface then you can discard any answer where s is not between 0 and 1. To find t simply substitute s back into one of the two equations above where we solved for t in terms of s. I should point out one important special case. If the denominator A - 2*B + C = 0 then your quadratic polynomial is actually linear. In this case you must use a much simpler equation to find s: s = A / (A-C) This will give you exactly one solution unless A-C = 0. If A = C then you have two cases: A=C=0 means all values for s contain p otherwise no values for s contain p. Nevermind my intuition was wrong. There are two solutions if none of the segments are parallel as you imply one inside the cuadrilateral (if convex) and one outsite. Great answer! How would you explain that there are two set of solutions for each set of (4) points? Intuitively one can tell that there should be a single solution.  Well if p is a 2D point yes you can get it easily. In that case S is the fractional component of the total width of the quadrilateral at T T is likewise the fractional component of the total height of the quadrilateral at S. If however p is a scalar it's not necessarily possible because the bilinear interpolation function is not necessarily monolithic. I might be misunderstanding but wouldn't that only work for axis-aligned rectangles? I edited my post to be more clear. I will add clarification.  One could also use the Gauss-Newton iterative algorithm to solve the following equivalent optimization problem min_(st) 1/2 ||p0*(1-s)*(1-t) + p1*s*(1-t) + p2*s*t + p3*(1-s)*t - p||^2 Here the residual is r = p0*(1-s)*(1-t) + p1*s*(1-t) + p2*s*t + p3*(1-s)*t - p; Differentiating by hand the columns of the Jacobian matrix are J(:1) = -p0*(1-t) + p1*(1-t) + p2*t - p3*t and J(:2) = -p0*(1-s) - p1*s + p2*s + p3*(1-s). With this just iterate q = q - (J^T J)\J^T r. Doesn't require a square root. Doesn't require conditionals. Generalizes to higher dimensions. Converges extremely rapidly - here's a representative table of iterations vs. error on a random trial I did: 0.0610 9.8914e-04 2.6872e-07 1.9810e-14 5.5511e-17 5.5511e-17 5.5511e-17 ... Here's my Matlab code: (I release it to the public domain) function q = bilinearInverse(pp1p2p3p4iter) %Computes the inverse of the bilinear map from [01]^2 to the convex % quadrilateral defined by the ordered points p1 -> p2 -> p3 -> p4 -> p1. %Uses Gauss-Newton method. Inputs must be column vectors. q = [0.5; 0.5]; %initial guess for k=1:iter s = q(1); t = q(2); r = p1*(1-s)*(1-t) + p2*s*(1-t) + p3*s*t + p4*(1-s)*t - p;%residual Js = -p1*(1-t) + p2*(1-t) + p3*t - p4*t; %dr/ds Jt = -p1*(1-s) - p2*s + p3*s + p4*(1-s); %dr/dt J = [JsJt]; q = q - (J'*J)\(J'*r); end end Also for download here https://github.com/NickAlger/MeshADMM/blob/master/bilinearInverse.m Edit: In response to the comment below here is a 3D version. function ss = trilinearInverse(p p1p2p3p4p5p6p7p8) %Computes the inverse of the trilinear map from [01]^3 to the box defined % by points p1...p8 where the points are ordered consistent with % p1~(000) p2~(001) p3~(010) p4~(100) p5~(011) % p6~(101) p7~(110) p8~(111) %Uses Gauss-Newton method. Inputs must be column vectors. iter = 20; tol = 1e-9; ss = [0.5; 0.5; 0.5]; %initial guess for k=1:iter s = ss(1); t = ss(2); w = ss(3); r = p1*(1-s)*(1-t)*(1-w) + p2*s*(1-t)*(1-w) + ... p3*(1-s)*t*(1-w) + p4*(1-s)*(1-t)*w + ... p5*s*t*(1-w) + p6*s*(1-t)*w + ... p7*(1-s)*t*w + p8*s*t*w - p; disp(['k= ' num2str(k) ... ' residual norm= ' num2str(norm(r))... ' [stw]= 'num2str([stw])]) if (norm(r) < tol) break end Js = -p1*(1-t)*(1-w) + p2*(1-t)*(1-w) + ... -p3*t*(1-w) - p4*(1-t)*w + ... p5*t*(1-w) + p6*(1-t)*w + ... -p7*t*w + p8*t*w; Jt = -p1*(1-s)*(1-w) - p2*s*(1-w) + ... p3*(1-s)*(1-w) - p4*(1-s)*w + ... p5*s*(1-w) - p6*s*w + ... p7*(1-s)*w + p8*s*w; Jw = -p1*(1-s)*(1-t) - p2*s*(1-t) + ... -p3*(1-s)*t + p4*(1-s)*(1-t) + ... -p5*s*t + p6*s*(1-t) + ... p7*(1-s)*t + p8*s*t; J = [JsJtJw]; ss = ss - (J'*J)\(J'*r); end end and here's a test script to test it %test_trilinearInverse.m h = 0.25; p1 = [0;0;0] + h*randn(31); p2 = [0;0;1] + h*randn(31); p3 = [0;1;0] + h*randn(31); p4 = [1;0;0] + h*randn(31); p5 = [0;1;1] + h*randn(31); p6 = [1;0;1] + h*randn(31); p7 = [1;1;0] + h*randn(31); p8 = [1;1;1] + h*randn(31); s0 = rand; t0 = rand; w0 = rand; p = p1*(1-s0)*(1-t0)*(1-w0) + p2*s0*(1-t0)*(1-w0) + ... p3*(1-s0)*t0*(1-w0) + p4*(1-s0)*(1-t0)*w0 + ... p5*s0*t0*(1-w0) + p6*s0*(1-t0)*w0 + ... p7*(1-s0)*t0*w0 + p8*s0*t0*w0; ss = trilinearInverse(p p1p2p3p4p5p6p7p8); disp(['error= ' num2str(norm(ss - [s0;t0;w0]))]) Here's the results of running the test script test_trilinearInverse k= 1 residual norm= 0.38102 [stw]= 0.5 0.5 0.5 k= 2 residual norm= 0.025324 [stw]= 0.37896 0.59901 0.17658 k= 3 residual norm= 0.00037108 [stw]= 0.40228 0.62124 0.15398 k= 4 residual norm= 9.1441e-08 [stw]= 0.40218 0.62067 0.15437 k= 5 residual norm= 3.3548e-15 [stw]= 0.40218 0.62067 0.15437 error= 4.8759e-15 One has to be careful about the ordering of the input points since inverse multilinear interpolation is only well-defined if the shape has positive volume and in 3D it is much easier to choose points that make the shape turn inside-out of itself. wish I could upvote twice. thanks a bunch! I like your solution best and upvoted it. However when working in 3d I could not get *rapid* convergence. Instead I get a rather *stalling* decrease of `norm(r)` (or what did you use above?). No problem. I release the 3D code to the public domain as well if anyone wants to use it. Ok I wrote a 3D version that seems to work and added it to my answer. Are you testing with random points? If that is the case it is likely the ""box"" would not be a box but rather some shape that turns inside out itself. Then the problem is not well-defined and there would be multiple local and even global minima. I would expect the method to work so long as you can guarantee that the input points actually form a well-defined interior volume.  Some of the responses have slightly misinterpreted your question. ie. they are assuming you are given the value of an unknown interpolated function rather than an interpolated position p(xy) inside the quad that you want to find the (st) coordinates of. This is a simpler problem and there is guaranteed to be a solution that is the intersection of two straight lines through the quad. One of the lines will cut through the segments p0p1 and p2p3 the other will cut through p0p2 and p1p3 similar to the axis-aligned case. These lines are uniquely defined by the position of p(xy) and will obviously intersect at this point. Considering just the line that cuts through p0p1 and p2p3 we can imagine a family of such lines for each different s-value we choose each cutting the quad at a different width. If we fix an s-value we can find the two endpoints by setting t=0 and t=1. So first assume the line has the form: y = a0*x + b0 Then we know two endpoints of this line if we fix a given s value. They are: (1-s)p0 + (s)p1 (1-s)p2 + (s)p3 Given these two end points we can determine the family of lines by plugging them into the equation for the line and solving for a0 and b0 as functions of s. Setting an s-value gives the formula for a specific line. All we need now is to figure out which line in this family hits our point p(xy). Simply plugging the coordinates of p(xy) into our line formula we can then solve for the target value of s. The corresponding approach can be done to find t as well.  this is my implementation ... I guess it is faster than of tfiniga void invbilerp( float x float y float x00 float x01 float x10 float x11 float y00 float y01 float y10 float y11 float [] uv ){ // substition 1 ( see. derivation ) float dx0 = x01 - x00; float dx1 = x11 - x10; float dy0 = y01 - y00; float dy1 = y11 - y10; // substitution 2 ( see. derivation ) float x00x = x00 - x; float xd = x10 - x00; float dxd = dx1 - dx0; float y00y = y00 - y; float yd = y10 - y00; float dyd = dy1 - dy0; // solution of quadratic equations float c = x00x*yd - y00y*xd; float b = dx0*yd + dyd*x00x - dy0*xd - dxd*y00y; float a = dx0*dyd - dy0*dxd; float D2 = b*b - 4*a*c; float D = sqrt( D2 ); float u = (-b - D)/(2*a); // backsubstitution of ""u"" to obtain ""v"" float v; float denom_x = xd + u*dxd; float denom_y = yd + u*dyd; if( abs(denom_x)>abs(denom_y) ){ v = -( x00x + u*dx0 )/denom_x; }else{ v = -( y00y + u*dy0 )/denom_y; } uv[0]=u; uv[1]=v; /* // do you really need second solution ? u = (-b + D)/(2*a); denom_x = xd + u*dxd; denom_y = yd + u*dyd; if( abs(denom_x)>abs(denom_y) ){ v = -( x00x + u*dx0 )/denom_x; }else{ v2 = -( y00y + u*dy0 )/denom_y; } uv[2]=u; uv[3]=v; */ } and derivation // starting from bilinear interpolation (1-v)*( (1-u)*x00 + u*x01 ) + v*( (1-u)*x10 + u*x11 ) - x (1-v)*( (1-u)*y00 + u*y01 ) + v*( (1-u)*y10 + u*y11 ) - y substition 1: dx0 = x01 - x00 dx1 = x11 - x10 dy0 = y01 - y00 dy1 = y11 - y10 we get: (1-v) * ( x00 + u*dx0 ) + v * ( x10 + u*dx1 ) - x = 0 (1-v) * ( y00 + u*dy0 ) + v * ( y10 + u*dy1 ) - y = 0 we are trying to extract ""v"" out x00 + u*dx0 + v*( x10 - x00 + u*( dx1 - dx0 ) ) - x = 0 y00 + u*dy0 + v*( y10 - y00 + u*( dy1 - dy0 ) ) - y = 0 substition 2: x00x = x00 - x xd = x10 - x00 dxd = dx1 - dx0 y00y = y00 - y yd = y10 - y00 dyd = dy1 - dy0 // much nicer x00x + u*dx0 + v*( xd + u*dxd ) = 0 y00x + u*dy0 + v*( yd + u*dyd ) = 0 // this equations for ""v"" are used for back substition v = -( x00x + u*dx0 ) / ( xd + u*dxd ) v = -( y00x + u*dy0 ) / ( yd + u*dyd ) // but for now we eliminate ""v"" to get one eqution for ""u"" ( x00x + u*dx0 ) / ( xd + u*dxd ) = ( y00y + u*dy0 ) / ( yd + u*dyd ) put denominators to other side ( x00x + u*dx0 ) * ( yd + u*dyd ) = ( y00y + u*dy0 ) * ( xd + u*dxd ) x00x*yd + u*( dx0*yd + dyd*x00x ) + u^2* dx0*dyd = y00y*xd + u*( dy0*xd + dxd*y00y ) + u^2* dy0*dxd // which is quadratic equation with these coefficients c = x00x*yd - y00y*xd b = dx0*yd + dyd*x00x - dy0*xd - dxd*y00y a = dx0*dyd - dy0*dxd  Here's my implementation of Naaff's solution as a community wiki. Thanks again. This is a C implementation but should work on c++. It includes a fuzz testing function. #include <stdlib.h> #include <stdio.h> #include <math.h> int equals( double a double b double tolerance ) { return ( a == b ) || ( ( a <= ( b + tolerance ) ) && ( a >= ( b - tolerance ) ) ); } double cross2( double x0 double y0 double x1 double y1 ) { return x0*y1 - y0*x1; } int in_range( double val double range_min double range_max double tol ) { return ((val+tol) >= range_min) && ((val-tol) <= range_max); } /* Returns number of solutions found. If there is one valid solution it will be put in s and t */ int inverseBilerp( double x0 double y0 double x1 double y1 double x2 double y2 double x3 double y3 double x double y double* sout double* tout double* s2out double* t2out ) { int t_valid t2_valid; double a = cross2( x0-x y0-y x0-x2 y0-y2 ); double b1 = cross2( x0-x y0-y x1-x3 y1-y3 ); double b2 = cross2( x1-x y1-y x0-x2 y0-y2 ); double c = cross2( x1-x y1-y x1-x3 y1-y3 ); double b = 0.5 * (b1 + b2); double s s2 t t2; double am2bpc = a-2*b+c; /* this is how many valid s values we have */ int num_valid_s = 0; if ( equals( am2bpc 0 1e-10 ) ) { if ( equals( a-c 0 1e-10 ) ) { /* Looks like the input is a line */ /* You could set s=0.5 and solve for t if you wanted to */ return 0; } s = a / (a-c); if ( in_range( s 0 1 1e-10 ) ) num_valid_s = 1; } else { double sqrtbsqmac = sqrt( b*b - a*c ); s = ((a-b) - sqrtbsqmac) / am2bpc; s2 = ((a-b) + sqrtbsqmac) / am2bpc; num_valid_s = 0; if ( in_range( s 0 1 1e-10 ) ) { num_valid_s++; if ( in_range( s2 0 1 1e-10 ) ) num_valid_s++; } else { if ( in_range( s2 0 1 1e-10 ) ) { num_valid_s++; s = s2; } } } if ( num_valid_s == 0 ) return 0; t_valid = 0; if ( num_valid_s >= 1 ) { double tdenom_x = (1-s)*(x0-x2) + s*(x1-x3); double tdenom_y = (1-s)*(y0-y2) + s*(y1-y3); t_valid = 1; if ( equals( tdenom_x 0 1e-10 ) && equals( tdenom_y 0 1e-10 ) ) { t_valid = 0; } else { /* Choose the more robust denominator */ if ( fabs( tdenom_x ) > fabs( tdenom_y ) ) { t = ( (1-s)*(x0-x) + s*(x1-x) ) / ( tdenom_x ); } else { t = ( (1-s)*(y0-y) + s*(y1-y) ) / ( tdenom_y ); } if ( !in_range( t 0 1 1e-10 ) ) t_valid = 0; } } /* Same thing for s2 and t2 */ t2_valid = 0; if ( num_valid_s == 2 ) { double tdenom_x = (1-s2)*(x0-x2) + s2*(x1-x3); double tdenom_y = (1-s2)*(y0-y2) + s2*(y1-y3); t2_valid = 1; if ( equals( tdenom_x 0 1e-10 ) && equals( tdenom_y 0 1e-10 ) ) { t2_valid = 0; } else { /* Choose the more robust denominator */ if ( fabs( tdenom_x ) > fabs( tdenom_y ) ) { t2 = ( (1-s2)*(x0-x) + s2*(x1-x) ) / ( tdenom_x ); } else { t2 = ( (1-s2)*(y0-y) + s2*(y1-y) ) / ( tdenom_y ); } if ( !in_range( t2 0 1 1e-10 ) ) t2_valid = 0; } } /* Final cleanup */ if ( t2_valid && !t_valid ) { s = s2; t = t2; t_valid = t2_valid; t2_valid = 0; } /* Output */ if ( t_valid ) { *sout = s; *tout = t; } if ( t2_valid ) { *s2out = s2; *t2out = t2; } return t_valid + t2_valid; } void bilerp( double x0 double y0 double x1 double y1 double x2 double y2 double x3 double y3 double s double t double* x double* y ) { *x = t*(s*x3+(1-s)*x2) + (1-t)*(s*x1+(1-s)*x0); *y = t*(s*y3+(1-s)*y2) + (1-t)*(s*y1+(1-s)*y0); } double randrange( double range_min double range_max ) { double range_width = range_max - range_min; double rand01 = (rand() / (double)RAND_MAX); return (rand01 * range_width) + range_min; } /* Returns number of failed trials */ int fuzzTestInvBilerp( int num_trials ) { int num_failed = 0; double x0 y0 x1 y1 x2 y2 x3 y3 x y s t s2 t2 orig_s orig_t; int num_st; int itrial; for ( itrial = 0; itrial < num_trials; itrial++ ) { int failed = 0; /* Get random positions for the corners of the quad */ x0 = randrange( -10 10 ); y0 = randrange( -10 10 ); x1 = randrange( -10 10 ); y1 = randrange( -10 10 ); x2 = randrange( -10 10 ); y2 = randrange( -10 10 ); x3 = randrange( -10 10 ); y3 = randrange( -10 10 ); /*x0 = 0 y0 = 0 x1 = 1 y1 = 0 x2 = 0 y2 = 1 x3 = 1 y3 = 1;*/ /* Get random s and t */ s = randrange( 0 1 ); t = randrange( 0 1 ); orig_s = s; orig_t = t; /* bilerp to get x and y */ bilerp( x0 y0 x1 y1 x2 y2 x3 y3 s t &x &y ); /* invert */ num_st = inverseBilerp( x0 y0 x1 y1 x2 y2 x3 y3 x y &s &t &s2 &t2 ); if ( num_st == 0 ) { failed = 1; } else if ( num_st == 1 ) { if ( !(equals( orig_s s 1e-5 ) && equals( orig_t t 1e-5 )) ) failed = 1; } else if ( num_st == 2 ) { if ( !((equals( orig_s s  1e-5 ) && equals( orig_t t  1e-5 )) || (equals( orig_s s2 1e-5 ) && equals( orig_t t2 1e-5 )) ) ) failed = 1; } if ( failed ) { num_failed++; printf(""Failed trial %d\n"" itrial); } } return num_failed; } int main( int argc char** argv ) { int num_failed; srand( 0 ); num_failed = fuzzTestInvBilerp( 100000000 ); printf(""%d of the tests failed\n"" num_failed); getc(stdin); return 0; }"
519,A,What's the best version control system for handling projects with graphics? I'm part of a small team (usually just two people) I handle the code he handles the graphic design. In the past I've used CVS to handle version control of the code files and while we've included the graphics in the repository he hasn't derived nearly as much value from it as I have. Are there other packages that provide the better features for supporting graphics? The system would need to have an easy to use GUI interface as I don't think it's fair to expect a graphic designer to learn command-line tools. Additional aspect: The client software needs to run smoothly on OS X (for the designer) and Windows (for the programmer). Do you need a windows based solution? Windows and Mac. SVN; it's a successor to CVS. SVN supports versioning on binary files. SmartSVN is a nice client for the graphic designer. Nice reference to SmartSVN. I had never come across that tool before but looks great. @BEN: SmartSVN is java; I like it for providing a constant interface across the multiple platforms I work with.  If you would like a GIT style version control system with an easy GUI I would use Tortoise HG (which uses Mercurial distributed version control system and almost the exact same styled interface system as Tortoise SVN). It's written in Python and you can use it to connect with BitBucket (which is basically a slightly different clone of GitHub in Django.  Perforce and P4V should work fine. You could also use the P4EXP extension that lets someone submit add to repository etc just by right clicking in a straight Windows file explorer window. For many artists this is sufficient for most of they day to day operations and is deeply familiar. More esoteric and less frequent source control functions can be accessed through the P4V client. Note that P4V also provides thumbnail views automatically that will help artists a lot. You can write your own plugin if you have your own formats - either client side or server side. Yet another alternative is the P4Web interface that lets you do a lot of things conveniently through a web browser. Perforce have put a lot of effort in over the last few years to target artist workflows which is one of the reasons they've made so much inroads into the games development industry. (No I don't work for them!) Perforce also has a nice image diff tool to show changes between revisions of a file. Also the latest version allows you to customize and strip down the P4V UI to reduce the complexity for non power users  TortoiseSVN + Subversion should do the job. I've never had to use it but TortoiseSVN comes with TortoiseIDiff which lets you compare images.  I would recommend Subversion with TortoiseSVN. It integrates into Windows Explorer and allows you to do everything you need from the context menu (update commit branch merge... anything). It can handle all kinds of files and best of all both Subversion and TortoiseSVN are free (along with the other SVN tools).  Subversion Tortoise SVN. It has easy interface and that is importrant when hiring new artists which are not necessery had experience working with subversion before. Basicaly you just need to know two buttons to start working.  I would recommend Subversion which natively supports binary files (and binary diffs). For the GUI interface you can use TortoiseSVN which is as good as it gets.
520,A,How can you tell png8 from a png24 I'm working on a website that uses lots of png24 files for transparency. I need to replace them with png8 files as all the png fix style javascript workarounds for png24 cause IE6 to lock up randomly. See this link to get an idea of the symptoms IE6 displays - http://blogs.cozi.com/tech/2008/03/transparent-pngs-can-deadlock-ie6.html Does anybody know an easy way of targeting existing png24 files to replace them with the png8s? I'm using a OS X and file browsers like Adobe bridge don't show this nor can I find the info on the commandline or the finder. Help! You actually CAN get alpha transparency in PNG-8's but it's super complicated. http://www.sitepoint.com/blogs/2007/09/18/png8-the-clear-winner/ http://www.personal.psu.edu/drs18/blogs/davidstong/2007/09/png8_alpha_transparency_from_f.html It basically creates GIF like substance for IE6 and a real PNG with alpha transparency for better browsers. It's a more pleasant degradation path. Right now I think you need Fireworks but I am exploring other options.  To add to Alnitak answer once you find all PNG24 you want to convert you can batch them all with pngquant: pngquant -v -f -ext .png 256 *.png It will convert all PNG files in place (overwrite them) with PNG8 versions.  JoeBloggs is right you're asking the wrong question. I've never had IE6 break with a good pngfix script. You've either got a bad pngfix script or a bad IE6 install. PNG8 will ruin your nice transparencies and make it look like you're using GIFs.  I think you might be asking the wrong question. PNG8s don't have the true-alpha you've been working hard to fix in IE6. If you replace the PNG24s with PNG8s you are no better off than replacing them with GIFs. Maybe you could test an alternative replacement/fix script - there are some shockingly bad ones out there maybe that's where the problem is? Here's (and I say this tongue in cheek!) the right question. PNG-8 actually supports storing an alpha value with every color on the palate so with he right program and optimization they can look great. See one of the other answers. PNG8 is not the same as GIF as PNG8 can store semi-transparent pixels where GIFs cannot. Photoshop does not have this ability (when trying to Save for Web) but it's very clear when you use Fireworks to export as PNG8.  Perhaps imagemagick helps you out at converting png24 files to png8 files.  The file utility on OSX can tell you the colour depth in a PNG file e.g: % file foo.png foo.png: PNG image data 1514 x 1514 8-bit grayscale non-interlaced
521,A,Calculating Screen Space Error I'm currently working on a Level of detail system for an XNA game the system subdivides triangles with high screen space error and merges triangles with low screen space error. World Space Error is a heuristic which estimates the error that this triangle has so for example if there is a triangle which is on an almost flat surface it'll have a very low screen space error because it's a very good approximation of that surface however if there is a triangle on the surface of a sphere it'll have a higher screen space error because obviously splitting that triangle up into more triangles would get a better approximation of a spherical surface. Screen Space Error is a slight modification of World Space Error basically the error is modified so triangles closer to the camera and closer to the centre of the field of view have a higher error score. What is a good and efficient way to calculate screen space error? Current Solution: Screen Space Error = World Space Error / dot([vector to triangle in question] [camera direction vector]) Blog post about screen space error You should check out these slides which explain screen space error in detail and derive formulae for calculating it. In particular a simple method is given on page 3 (equation 1). Note that the rest of the pages detail improvements to this simple method which might be of interest to you if you're trying to do something more advanced. aha! looks good thanks
522,A,Image not showing in IE but is showing in Mozilla - Odd If you look at this link: http://www.internetworld.co.uk/g/2010/ExhibLogos/ExhibID_137_Graphic1.jpg. In IE8 it doesn't show. In Mozilla it does. I have no idea as to what would cause this issue as this is just a bog standard jpg. This is also happening for other companies other than ourselves on the same site. Can anyone shed any light on it for me? Thanks in advance. Just a guess but your colorspace is likely something that is not supported by IE 8. Hope this helps...  Is it a CMYK JPEG? IE still can't read them. Convert it to RGB and all should be well in the world. Thanks very much. This sorted it.
523,A,"Filling an Area in visual studio C# I'm drawing a circle in C# and i have divided it into some partsi want to fill different parts with different colorsis there anyway to do this? and how?i tried using fillpie() but i couldn't get the arguments to work. here is the code:  int r = 150; g.DrawEllipse(Pens.Black 300 - r 250 - r 2 * r 2 * r); if (p != 0) g.DrawLine(Pens.Black 300 250 300 + r 250); double sum; sum = 0.0; for (int j = 0; j < p; j++) sum += data[j].value; double angle; angle = 0.0; for (int i = 0; i < p; i++) { angle += (double)(data[i].value / sum) * 2.0 * Math.PI; textBox1.Text += sum.ToString() + "" : "" + angle.ToString() + "":"" + Math.Cos(angle).ToString() + ""\r\n""; g.DrawLine(Pens.Black 300 250 300 + (int)(Math.Cos(angle) * r) 250 - (int)(Math.Sin(angle) * r)); //g.FillPie(Brushes.Black 300-r  250 - r r r (float)(angle)(float)(angle+ (data[i].value / sum) * 2.0 * Math.PI)); } this actually divides the circle into different partsi don't know how to fill them the commented line is where i What arguments have you tried? Maybe you would like to edit your question to show some code that you have? Winforms or WPF? Please show some code. Supposing you are using WinForms the MSDN hase some nice and easy example for the FillPie() method. public void FillPieRectangle(PaintEventArgs e) { // Create solid brush. SolidBrush redBrush = new SolidBrush(Color.Red); // Create rectangle for ellipse. Rectangle rect = new Rectangle(0 0 200 100); // Create start and sweep angles. float startAngle = 0.0F; float sweepAngle = 45.0F; // Fill pie to screen. e.Graphics.FillPie(redBrush rect startAngle sweepAngle); } EDIT: It looks like you actually want to draw some kind of pie chart but your code looks way to complicated. Take a look at this article that might give you some help. thanksi know how to work with it nowthis can be closed.  e.Graphics.FillPie(new SolidBrush(Color.Red0 04545030)"
524,A,Direct3D Output on a Printer/Plotter What is the best way to send Direct3D output to a printer or a plotter? I would like to say that sending screenshot or image is only suitable for printers (raster graphic). If you want to draw on the plotter you'll need use the special instruction for the particular model of plotter you intended to draw on. Plotters are restricted to draw only line art as oppose to raster graphics. Take a look at this wikipedia article  I would do it by creating a BMP file from the front buffer and then printing that BMP: http://www.mvps.org/directx/articles/screengrab.htm  I would just take a screenshot of the entire 3d scene and then print it out. Thanks. I found a more detailed explanation here: http://www.mvps.org/directx/articles/tilerender/index.htm
525,A,High-End 2D Java (SE) Graphics Library I am looking for a high-end graphics library for Java Standard Edition. I know some fairly low-level libraries: AWT/Swing JOGL SDL. Are/is there an alternative? My requirements are (atleast): Anti-aliased Fullscreen support Alpha channel Blend modes and Z-depth Rasterized Effects: (motion) blur glow gloom etc. And optionally: Hardware acceleration Vector graphics (scale/rotate/translate in floating point precision) Well documentated easy to get started. I understand if there are no libraries/frameworks matching all requirements if so can you comment on how well it's extendible? Edit: Are there any other alternatives besides Processing? Is Processing usable (and easy?) as a library? All of the effects you ask for can be done with Java 2D fairly simply and Java2D is to some degree hardware accelerated. The book Filthy Rich Clients shows how to implement all of these effects and many others. Painters in the SwingX has also implemented several of these effects. Another possibility is JavaFx. It has a lot of effects built in. A year and a half ago when I tried it it was still a little slow but at least one version has been released since then. It may be more performant now. It's built on top of Java2D so it should be able to take advantage of the hardware acceleration in Java2D. As far as full screen support here's more information.  Not really a graphics library but rather an advanced applet AND graphics library: PulpCore Be sure to grab a quite recent version (maybe even fetch the source via mercurial and build it yourself) because the filter effects you mentioned (blur etc.) aren't available in earlier versions. I know it might not be exactly what you're looking for; it is not a processing alternative but it might be the better choice depending on what you're planning to do. Excellent! Thank you very much exactly what I need! FYI PulpCore doesn't appear to exist any more... @chriswynnyk too bad. From https://code.google.com/p/pulpcore/ : PulpCore is no longer maintained - please use PlayN or libgdx instead.  Take a look at http://processing.org  Maybe Processing (homepage) can be a good tradeoff. it has 4 kinds of backing including OpenGL it has AA it is 2d or 3d it supports hw acceleration it supports transformations its syntax is quite easy to learn it's easily embeddable everything you need to learn is in this page Can processing be used as a library?
526,A,Tweenmax : is it possible to tween a sprite linestyle? I'm using TweenMax to make all tweens fade in a projet. Now my question is : is it possible to tween only linestyle from a Sprite ? For example something like that : TweenMax.to ( my_sprite.graphics.lineStyle 0.5 { tint : 0xFF0000 } ); Thank you. From what I have seen if you wish to alpha the stroke and the fill separately you need to redraw the shape. ie. this post - http://www.actionscript.org/forums/showthread.php3?t=219378 Alternatively depending on the complexity of the Sprite you can just make the outline its own Sprite and tween it interdependently.
527,A,"Debugging image rendering in Visual C++ any helpful add-ins? I often write code that renders images by writing pixels directly into buffers and I often find it hard to get a good overview of what's really going on. The Memory window in Visual Studio's debugger is somewhat a help but I'd really love to see the images graphically. So my question is does anyone know of a debugging extension that can read a chunk of memory as a picture in a specified pixel format and display it graphically? I'm interested in exactly the same task. Is there any progress on the problem? You've accepted answer with link to imdebug but it require source code modifications and contradicts with your own comment: http://stackoverflow.com/questions/1549129/debugging-image-rendering-in-visual-c-any-helpful-add-ins#comment1425286_1549543 doesn't it? I'm developing a open source Add-In Visual Image Assist for VC6/7/8/9 used to show/edit a image. It may be you want too. Are there any updates on the project? It's not even clear how to build latest version. I had not developed it now. I've transfered it to my colleague. If he and I have time we'll improve it.  I've scrapped my old answer since it was irrelevant. The new on also used OpenCV (since I'm trying to display an OpenCV image) but it can be adapted to any framework. The core code is where it takes a memory address address and the number of bytes to read through numrows numcols and byte_size and reads those bytes into a buffer. I'm sure you can adapt that portion of the code for your own needs. #include ""stdafx.h"" #include <windows.h> #include <cv.h> #include <highgui.h> #include <iostream> using namespace std; using namespace cv; void imshow_debug(const LPCVOID address const int numrows const int numcols const int type const int byte_size const int step const string windows_title) { // Initialize unsigned long PID; SIZE_T read_bytes = 0; // Allocate space for the image const int bytes_to_read = numrows*numcols*byte_size; uchar *image_data = new uchar[bytes_to_read]; Mat Image(numrowsnumcolstypeimage_datastep); // Get the handle and PID HWND handle = FindWindowA(0 windows_title.c_str()); if (!FindWindowA(0 windows_title.c_str())) { printf(""Window %s not found!"" windows_title); exit(0); } GetWindowThreadProcessId(handle &PID); /* Get windows PID from a window handle */ HANDLE WindowsProcessHandle = OpenProcess(PROCESS_ALL_ACCESS false PID); // Read the image ReadProcessMemory(WindowsProcessHandleaddressimage_databytes_to_read&read_bytes); if(bytes_to_read != read_bytes) { cout<<""Could not read entire image""<<endl; exit(0); } // Display the image namedWindow(""Image""); imshow(""Image""Image); waitKey(); // Clean up CloseHandle(WindowsProcessHandle); delete[]image_data; } int main(int argc char* argv[]) { imshow_debug((LPVOID)0x03af0370300300CV_64Fsizeof(double)2400""C:\\Documents and Settings\\jacobm\\My Documents\\Visual Studio 2005\\Projects\\Head\\debug\\SSR_VideoComp.exe""); return 0; } I thought I was clear enough that my problem wasn't displaying the images at some point my problem is that I want to display the images while I single-step through the code inside the debugger and want to see how my write pointer moves around. I am after an extension to the debugger that shows what my code does as I step through it. I'm sorry for that and I've *finally* gotten around to implementing it :) For visualization of OpenCV images see https://sourceforge.net/projects/nativeviewer/  What you're asking for is not naturally achieveable in native c++. All the visualization technology inside the visual debugger is organized around the CLR hence either C# or C++/CLI. The one thing that can help in native land is the expression evaluator addin mechanism. Of course it's designed to return a string and go away but it's code so you could in theory run any code including displaying a window and showing the data you care about (after having read it from the debuggee). It's a bit of a bummer to see those great features missing from the native side though.  A colleague of mine wrote this CodeProject article for writing Debugger Visualizers http://www.codeproject.com/KB/showcase/BuildDebuggerVisualizer.aspx It uses our product a .NET imaging toolkit but it could easily be adapted to use .NET image classes instead.  Such a thing exists: A utility for simple printf-style debugging of images in Win32 C/C++ applications. http://billbaxter.com/projects/imdebug/ My coworker raves about it. --chris  Create a class containing your buffer + metadata (width height stride pixelformat). Add ToStream() and FromStream() methods to your class for (de)serializing image (buffer and metadata). Add a ToWpfBitmapSource() to your class. Create a debug visualizer that deserializes your image from the stream converts to WPF's BitmapSource places in an Image control within a Winforms WPF host. This example will help: http://www.codeproject.com/KB/WPF/WPF%5FGlimps.aspx The class can be added in C++ CLI in a seperate DLL."
528,A,What does the Flash VM use under the hood for drawing? In windows what does Flash use under the hood? It's a relatively simple question which I can never find the answer to. Is it GDI (for windows VM implementations) or something else? You don't need to go into any of the new GPU acceleration features of Flash. I just really want to know the inner workings because it's NEVER discussed. I agree with george GDI is very bad for speed. DirectX for Windows and SDL or similar for Linux (note this is an assumption!). In that sense it probably uses a layer that communicates with the native graphics subsystem on whatever platform it's running on.  DirectX mostly. It's hard to achieve good graphics performance with GDI.  On 64-bit Linux the Flash plugin does not link against SDL (according to ldd). It does however link against GTK GDK and Cairo. It appears therefore that it is using either Cairo or raw Xlib calls to do its drawing on Linux. I don't know on Windows. Flash tends to have minimal dependencies but Direct-X may be standard enough that they use it. With some kind of a process examiner to tell you what libraries a process has loaded you could examine a simple web browser embedding Flash and see what system facilities are actually in use. interesting answers all around. Thanks guys. You were one of the last answers but you have the most complete answer so marked as the answer for future generations =)
529,A,"Export JPanel to vector graphics I would like to export the image in my JPanel to a vector graphics file so it can be edited and printed at a higher-than-screen resolution. Essentially I want its paint() function to be called with a destination Graphics that saves the drawing commands into a vector graphic file. What is a good simple way to do this? What libraries are recommended? Which vector format would be best and why? Have a look at The Java EPS Graphics2D package. Many Java programs use Graphics2D to draw stuff on the screen and while it is easy to save the output as a png or jpeg file it is a little harder to export it as an EPS for including in a document or paper. This package makes the whole process extremely easy because you can use the EpsGraphics2D object as if it's a Graphics2D object. The only difference is that all of the implemented methods create EPS output which means the diagrams you draw can be resized without leading to any of the jagged edges you may see when resizing pixel-based images such as JEPG and PNG files. That looks very nice; thanks. I'm not sure EPS is right for me but I may go that route and if I do this certainly looks like a good package for it. I'm going to go with this solution; thank you. It seems that EPS will serve nicely.  Apache Batik will let you paint to a specialised implementation of a Graphics2D object and then export as an scalable vector graphics (.svg) file. You can then view/process/print it using an SVG-enabled browser (Firefox will handle it nativly ISTR IE and others can use plugins). See the SVGGraphics2D object (process documented here) I had forgotten about Batik; thanks. I've seen it before and I think SVG might be a good format for me. It's a long time since I used it but it works very well If I could pick two answers I'd pick this one too; it's just that EPS seems like a better solution for my needs than SVG. Just wanted to mention that Apache SVG has issues with exporting gradients (they are not exported correctly). Also if a graphics itself uses SVG then the export is broken. AFAIK SVGSalamander does not have the option to write to SVG.  this is basically not possible directly as the low level java api works in terms of raster (pixels) and is never stored in vector format. (Check the API of java.awt.Graphics to see what I mean). there are some general purpose program that convert raster to vector formats this is one I found on a quick search: http://autotrace.sourceforge.net/index.html so using such a program you can divide your problem into two smaller problems: convert your JPanel into a bitmap or file (http://www.jguru.com/faq/view.jsp?EID=242020) run autotrace on the file. You can do this since the API will talk in terms of drawRectangle() etc. That gets rendered in different ways depending on the component you're drawing to When you call drawLine() on the specialised Graphics2D implementation it writes the vector representation of that line to the file.  FreeHEP seems to work quite well although it does not appear to be maintained anymore and its bug and forum pages are gone. With just a handful of lines you get a popup dialog that can save any component to a variety of scalable and regular image formats. We have some challenging images using alpha channel rotated text areas bounded by curves and they saved perfectly much better with with VectorGraphics2D. The only problem I've seen so far is in jpeg save which comes out black for all my images. This isn't very important for us given that png works plus all the vector modes but I'm sure it would be an issue for some. I had to add exactly this much code to save in all these modes: public static void showImage(Component comp) { try { ExportDialog export = new ExportDialog(); export.showExportDialog( null ""Export view as ..."" comp ""export"" ); System.err.println(""Image save complete""); } catch(Exception e) { e.printStackTrace(); } } There are a bunch of library jars that must be added as well.  I can recommend the VectorGraphics2D library (LGPL). Although it does not support all the features of Graphics2D I used it successfully for my project. It provides implementations of java.awt.Graphics2D for various vector file formats. It simply exports all paint operations to EPS SVG or PDF files.  The Java EPS mentioned by Pierre looks good but if it isn't you might also like to look at FreeHEP Vector Graphics. Written to allow Java reuse in the High Energy Physics field it includes a vector graphics package done through an implementation of Graphics2D. We've used it to export EPS very successfull for a number of years. Thank you; I wound up using FreeHEP Vector Graphics and found it quite easy to use and pretty good. Convenient and well documented."
530,A,Drawing on top of an image in Javascript I want to let the user draw on an image in a browser. In other words I need both bitmapped graphics and drawing capabilities whether vector or bitmapped. Canvas looks good but is not supported by IE and though there is ExCanvas I wonder if ExCanvas is stable enough for consistent use in IE6 through 8. Or best of all is there an open-source image/drawing library that supports all this out of the box? I found two dozen or so Web-based image editors or drawing tools but none support the requirements. (And I'd like to avoid Flash/Flex/Silverlight/JavaFX.) This is a very clever and very expansive library that I came across a while back: JS-Graphics  You might be able to do it with VML http://en.wikipedia.org/wiki/Vector_Markup_Language  I'd add a little bit to Kieron's answer; Water Zorn's site has a very well featured vector graphics package. I've used it in a large application and it integrated beautifully with .NET and hand written javascript. I also used his drag and drop API to smooth dragging of html elements.  check out dojo x http://dojotoolkit.org/projects/dojox  Even though you said you'd like to avoid it I'd suggest Flash. You could easily use Flash 6 or 7 and these have a > 90% adoption rate. I'd be surprised if you could get that level of support with JavaScript. Flash is truly write once run anywhere which will cut down on your development time.  Use dojox.gfx. It is cross-browser (SVG/VML/Canvas/Silverlight) and it looks like it fits the bill. You can download it from the main Dojo site. You can try its tests and demos. Warning: the latter two links will be slow because the code was geared for debugging problems not for speed (not minimized not combined not compressed served from file server).  Take a look at RaphaelJS... it's a cross browser implementation of drawing functions using Canvas VML or SVG where available. I'm not sure if it lets users draw for themselves out of the box but it might be worth a look.
531,A,"Algorithms for spacing objects visually I am wondering if there are any well-known algorithms that I should be aware of for spacing objects visually. For instance a LINQ to SQL diagram has many tables but automatically spaces them for readability. Is this pretty much a ""place randomly and move if too close/overlap"" type algorithm or is there more to this? Thanks for any advice! Take a look at GraphViz. It might be usable right off the shelf or might be a good starting point.  Roughly you can perform a ""connectedness"" analysis on your graph of objects to determine which is (are) more central; i.e. which have a higher degree of connectivity to other objects. Those go in the center. Figure out your individual sizing of objects determine the amount of space left divide that by the number of items to place and place them based upon that data."
532,A,"What libraries are available to help create 2D Java games for phones? I want to begin developing 2D Java games for phones (on J2ME) therefore I'd like to know if any libraries or ""engines"" exist to help out in the various graphical tasks: Drawing text with pixel fonts? Drawing bitmaps for sprites with multiple frames like animated GIFs? Drawing graphics with code lines beziers flood-filling and gradient fills? Ordering / layering of sprites? Or maybe a great book exists that gives you enough code samples to get off the ground quickly? There was a book released quite a few years ago called Developing Games in Java by David Brackeen. That covers the basics of 2d and 3d development in pure Java as well as how to handle time jumps and update the physical properties of your game characters. It is a good introduction to the topic. Is it compatible to J2ME? Or would it be using outdated techniques?  MIDP (JSR-118) includes the basics (most of the things you listed above) mainly in the javax.microedition.lcdui and javax.microedition.lcdui.game namespaces.  I didn't use it myself but heard some good reference on here. And here is even a list of libraries you might need."
533,A,"Flex Panel not being updated I am drawing a cellular automaton simulation on a Flex Canvas. I alternate computing the state of the automaton with updating the graphics on the Panel. However the updating of the Panel does not seem to be ""keeping up"" with the update of the CA state. I'm wondering whether I am doing something wrong with my handling of the graphics code. The code is below. Sorry it's so lengthy but I don't think the problem will happen with anything simpler (well it probably will but I'm not certain I want to take the time to find it). <?xml version=""1.0"" encoding=""utf-8""?> <mx:Application xmlns:mx=""http://www.adobe.com/2006/mxml"" creationComplete=""init();""> <mx:Script> <![CDATA[ private static const universeRadius:uint = 8; private var universes:Array = new Array(); private var currentUniverse:uint = 0; private var squareHeight:uint; private var squareWidth:uint; private function init():void { squareHeight = myCanvas.height / universeRadius; squareWidth = myCanvas.width / universeRadius; initUniverses(); while (true) { trace(""Calling draw()""); draw(); trace(""Calling updateUniverse()""); updateUniverse(); } } private function initUniverses():void { var universe0:Array = new Array(); var universe1:Array = new Array(); var i:int; for (i = 0; i < universeRadius; i++) { var universe0Row:Array = new Array(); var universe1Row:Array = new Array(); var j:int; for (j = 0; j < universeRadius; j++) { if (Math.random() < 0.5) { universe0Row[j] = 0; } else { universe0Row[j] = 1; } universe1Row[j] = 0; } universe0[i] = universe0Row; universe1[i] = universe1Row; } universes[0] = universe0; universes[1] = universe1; } private function normalize(pos:int):int { return (pos + universeRadius) % universeRadius; } private function updateUniverse():void { var newUniverse:int = 1 - currentUniverse; var i:int; for (i = 0; i < universeRadius; i++) { var j:int; for (j = 0; j < universeRadius; j++) { var dx:int; var dy:int; var count:int = 0; for (dx = -1; dx <= 1; dx++) { var neighborX:int = normalize(i + dx); for (dy = -1; dy <= 1; dy++) { var neighborY:int = normalize(j + dy); if ((dx != 0 || dy != 0) && universes[currentUniverse][neighborX][neighborY] == 1) { count++; } } } var currentCell:int = universes[currentUniverse][i][j]; if (currentCell == 1) { // 1. Any live cell with fewer than two live neighbours // dies as if caused by underpopulation. if (count < 2) { universes[newUniverse][i][j] = 0; } // 2. Any live cell with more than three live neighbours // dies as if by overcrowding. else if (count > 3) { universes[newUniverse][i][j] = 0; } // 3. Any live cell with two or three live neighbours // lives on to the next generation. else { universes[newUniverse][i][j] = 1; } } else { // 4. Any dead cell with exactly three live neighbours // becomes a live cell. if (count == 3) { universes[newUniverse][i][j] = 1; } else { universes[newUniverse][i][j] = 0; } } } } currentUniverse = newUniverse; } private function draw():void { myCanvas.graphics.clear(); myCanvas.graphics.beginFill(0xFFFFFF 1.0); myCanvas.graphics.drawRect(0 0 myCanvas.width myCanvas.height); var i:int; for (i = 0; i < universeRadius; i++) { var j:int; for (j = 0; j < universeRadius; j++) { if (universes[currentUniverse][i][j] == ""1"") { myCanvas.graphics.beginFill(0x000000 1.0); myCanvas.graphics.drawRect( j * squareWidth i * squareHeight squareWidth squareHeight) } } } myCanvas.graphics.endFill(); } ]]> </mx:Script> <mx:Panel title=""Life"" height=""95%"" width=""95%"" paddingTop=""5"" paddingLeft=""5"" paddingRight=""5"" paddingBottom=""5""> <mx:Canvas id=""myCanvas"" borderStyle=""solid"" height=""100%"" width=""100%""> </mx:Canvas> </mx:Panel> </mx:Application> Scary: while (true) { Flex is single-threaded so I think you're never giving the UIComponents a chance to fully update. Alternatives would be to run the draw and update methods on a Timer or run them on the ENTER_FRAME event. Either of these would let the display list updates occur in between your private function cycleUniverse(event:Event):void { updateUniverse(); draw(); } and then either of private var t:Timer; private function init():void { t = new Timer(500); // every 500 ms t.addEventListener(TimerEvent.Timer cycleUniverse); t.start(); } or <mx:Application xmlns:mx=""http://www.adobe.com/2006/mxml"" enterFrame=""cycleUniverse(event)""> I ended up putting the updates on a Timer event and that works fine. I know ""while (true)"" is scary but it's an early version of the code and I planned on changing that later. Thanks! Only scary because of Flex's single-threadedness. Elsewhere while (true) away."
534,A,"problem with TextRenderer.MeasureText Hi I am using TextRenderer.MeasureText() method to measure the text width for a given font. I use Arial Unicode MS font for measuring the width which is a Unicode font containing characters for all languages. The method returns different widths on different servers. Both machines have Windows 2003 and .net 3.5 SP1 installed. Here is the code we used using (Graphics g = Graphics.FromImage(new Bitmap(1 1))) { width = TextRenderer.MeasureText(g word textFont new Size(5 5) TextFormatFlags.NoPadding).Width; } Any idea why this happens? I use C# 2.0 Why _what_ happens? You just posted some code and no problem description. Hi the problem is if u run the code on different machines it returns different widths so if I have multiple servers each server will return a different width which is not acceptable...And not all the machines return different values only some of them..! //-------------------------------------------------------------------------------------- // MeasureText always adds about 1/2 em width of white space on the right // even when NoPadding is specified. It returns zero for an empty string. // To get the precise string width measure the width of a string containing a // single period and subtract that from the width of our original string plus a period. //-------------------------------------------------------------------------------------- public static Size MeasureText(string Text Font Font) { TextFormatFlags flags = TextFormatFlags.Left | TextFormatFlags.Top | TextFormatFlags.NoPadding | TextFormatFlags.NoPrefix; Size szProposed = new Size(int.MaxValue int.MaxValue); Size sz1 = TextRenderer.MeasureText(""."" Font szProposed flags); Size sz2 = TextRenderer.MeasureText(Text + ""."" Font szProposed flags); return new Size(sz2.Width - sz1.Width sz2.Height); } thanks for the update.... Great solution. MSDN must have had info on that but eventually it hasn't.  MeasureText is not known to be accurate. Heres a better way :  protected int _MeasureDisplayStringWidth ( Graphics graphics string text Font font ) { if ( text == """" ) return 0; StringFormat format = new StringFormat ( StringFormat.GenericDefault ); RectangleF rect = new RectangleF ( 0 0 1000 1000 ); CharacterRange[] ranges = { new CharacterRange ( 0 text.Length ) }; Region[] regions = new Region[1]; format.SetMeasurableCharacterRanges ( ranges ); format.FormatFlags = StringFormatFlags.MeasureTrailingSpaces; regions = graphics.MeasureCharacterRanges ( text font rect format ); rect = regions[0].GetBounds ( graphics ); return (int)( rect.Right ); } Hey That works fine..! Thanks a lot.. Hey Thanks for the inputs... Let me try the same.  We had a similar problem several years back. In our case for some reason we had different versions of the same font installed on two different machines. The OS version was the same but the font was different. Since you normally don't deploy a system font with your application setup measuring and output results may vary from one machine to another based on the font version. Since you say ... And not all the machines return different values only some of them..! ...this is something I'd check for."
535,A,c# GDI Edge Whitespace Detection Algorithm I am looking for a solution for detecting edge whitespace of c# bitmap from the c# managed GDI+ library. The images would be either transparent or white most of the 400x pictures are 8000x8000px with about 2000px whitespace around the edges. What would be the most efficient way of finding out the edges x y height and width coordinates? I tried a go pixel by pixel but was finding it very slow. Update to solution --Added left/right/top/bottom bounds Problems with images detail center images now crops any transparent (0%) or white (#FFFFFF) pixels. var top = bitmap.Height; var left = bitmap.Width; var right = 0; var bottom = 0; ... var pData = pData0 + (y * data.Stride) + (x * 4); var xyAlpha = pData[3]; var xyBlue = pData[0]; var xyGreen = pData[1]; var xyRed = pData[2]; if ((xyAlpha > 0) || (xyRed != 255 && xyGreen != 255 && xyBlue != 255)) { if (y < top) top = y; if (y > bottom) bottom = y; if (x < left) left = x; if (x > right) right = x; } ... var cropWidth = right - left; var cropHeight = bottom - top; var cropX = top; var cropY = left; var cacheBitmap = new Bitmap(cropWidth cropHeight PixelFormat.Format32bppArgb); using (var cacheGraphics = Graphics.FromImage(cacheBitmap)) { cacheGraphics.DrawImage(context.Image new Rectangle(0 0 cropWidth cropHeight) cropX cropY cropWidth cropHeight GraphicsUnit.Pixel); } A great GDI+ resource is Bob Powells GDI+ FAQ! You didn't say how you accessed the pixels in the image so I will assume that you used the slow GetPixel methods. You can use pointers and LockBits to access pixels in a faster way: see Bob Powells explanation of LockBits - This will require an unsafe code block - if you don't want this or you do not have FullTrust you can use the trick explained here: Pointerless Image Processing in .NET by J. Dunlap The below code uses the LockBits approach (for the PixelFormat.Format32bppArgb) and will fill the start and end Points with the value where the first and last pixels in an image are discovered that do not have the color described in the argument color. The method also ignores completely transparent pixels which is useful if you want to detect the area of an image where the visible 'content' starts.  Point start = Point.Empty; Point end = Point.Empty; int bitmapWidth = bmp.Width; int bitmapHeight = bmp.Height; #region find start and end point BitmapData data = bmp.LockBits(new Rectangle(0 0 bitmapWidth bitmapHeight) ImageLockMode.ReadOnly PixelFormat.Format32bppArgb); try { unsafe { byte* pData0 = (byte*)data.Scan0; for (int y = 0; y < bitmapHeight; y++) { for (int x = 0; x < bitmapWidth; x++) { byte* pData = pData0 + (y * data.Stride) + (x * 4); byte xyBlue = pData[0]; byte xyGreen = pData[1]; byte xyRed = pData[2]; byte xyAlpha = pData[3]; if (color.A != xyAlpha || color.B != xyBlue || color.R != xyRed || color.G != xyGreen) { //ignore transparent pixels if (xyAlpha == 0) continue; if (start.IsEmpty) { start = new Point(x y); } else if (start.Y > y) { start.Y = y; } if (end.IsEmpty) { end = new Point(x y); } else if (end.X < x) { end.X = x; } else if (end.Y < y) { end.Y = y; } } } } } } finally { bmp.UnlockBits(data); } #endregion  I'd first make sure to use the LockBits method described by Patrick. Second I'd check the pixels on the middle lines to quickly determine the edges. By middle lines I mean if you have say for example a 2000x1000 image you'd look first along horizontal line number 500 (out of 1000) to find the left and right limits then along vertical line number 1000 (out of 2000) to find the top and bottom limits. It should be very fast this way.
536,A,"Reverse Java Graphics2D scaled and rotated coordinates I use Graphics2D in Java to scale and rotate the picture I draw. I now want to be able to tell what the original coordinates were when I click on a certain point in the picture. So given the rotated and scaled coordinates I want to calculate the original ones. Is there a simple way to do this? This will help http://stackoverflow.com/questions/11821381/cant-get-my-coordinates-graphics2d-mouseclick-java If you keep a copy of the AffineTransform you use when you paint the image you can use AffineTransform.inverseTransform(Point2D ptSrc Point2D ptDst) to transform a device space coordinate back to user space Edit: If you capture the current transform of the Graphics2D while painting beware of the Graphics2D being re-used for multiple lightweight children of the same window/panel because then the transform will be relative to the parent component but the mouse coordinates will be relative to the child. You need to capture the changes you make to the transform not its final value. Example: import java.awt.BasicStroke; import java.awt.Color; import java.awt.Component; import java.awt.Dimension; import java.awt.Graphics; import java.awt.Graphics2D; import java.awt.Point; import java.awt.event.MouseAdapter; import java.awt.event.MouseEvent; import java.awt.event.MouseMotionAdapter; import java.awt.geom.AffineTransform; import java.awt.geom.NoninvertibleTransformException; import java.awt.image.BufferedImage; import java.io.IOException; import java.net.MalformedURLException; import java.net.URL; import javax.imageio.ImageIO; import javax.swing.Box; import javax.swing.BoxLayout; import javax.swing.JComponent; import javax.swing.JFrame; public class Main { public static void main(String[] args) throws MalformedURLException IOException { JFrame frame = new JFrame(); Box box = new Box(BoxLayout.Y_AXIS); BufferedImage image = ImageIO.read(new URL(""http://sstatic.net/so/img/logo.png"")); AffineTransform xfrm1 = AffineTransform.getScaleInstance(0.95 1.25); xfrm1.rotate(-0.3); box.add(new ImageView(image xfrm1)); AffineTransform xfrm2 = AffineTransform.getShearInstance(0.1 0.2); xfrm2.scale(1.3 0.9); box.add(new ImageView(image xfrm2)); frame.add(box); frame.pack(); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.setVisible(true); } } @SuppressWarnings(""serial"") class ImageView extends JComponent { @Override public void paintComponent(Graphics g) { Graphics2D g2d = (Graphics2D) g; try { paintXfrm = g2d.getTransform(); paintXfrm.invert(); g2d.translate(getWidth() / 2 getHeight() / 2); g2d.transform(xfrm); g2d.translate(image.getWidth() * -0.5 image.getHeight() * -0.5); paintXfrm.concatenate(g2d.getTransform()); g2d.drawImage(image 0 0 this); } catch (NoninvertibleTransformException ex) { ex.printStackTrace(); } } @Override public Dimension getPreferredSize() { return new Dimension(image.getWidth() * 2 image.getHeight() * 2); } ImageView(final BufferedImage image final AffineTransform xfrm) { this.canvas = image.createGraphics(); canvas.setColor(Color.BLACK); canvas.setStroke(new BasicStroke(3.0f)); this.image = image; this.xfrm = xfrm; addMouseListener(new MouseAdapter() { @Override public void mousePressed(MouseEvent e) { try { mouseDownCoord = e.getPoint(); paintXfrm.inverseTransform(mouseDownCoord mouseDownCoord); } catch (NoninvertibleTransformException ex) { } } @Override public void mouseExited(MouseEvent e) { mouseDownCoord = null; } }); addMouseMotionListener(new MouseMotionAdapter() { @Override public void mouseDragged(MouseEvent e) { Point p = e.getPoint(); try { paintXfrm.inverseTransform(p p); if (mouseDownCoord != null) { canvas.drawLine(mouseDownCoord.x mouseDownCoord.y p.x p.y); for (Component sibling: getParent().getComponents()) { sibling.repaint(); } } mouseDownCoord = p; } catch (NoninvertibleTransformException ex) { ex.printStackTrace(); } } }); } private Graphics2D canvas; private BufferedImage image; private AffineTransform xfrm; private AffineTransform paintXfrm; private Point mouseDownCoord; }  Its not so hard ;-) When you repaint the Component save the AffineTransform after the transforming with g2.getTransform() Then call the function invert() on it In the mouseClicked() event us the following code: Point2D p= trans.transform(new Point2D.Double(evt.getX() evt.getY()) null); System.out.println(""click x=""+p.getX()+"" y=""+p.getY()); Thats it!  It's not clear exactly how you're rotating and scaling. But you're probably using an AffineTransform. Fortunately there's a createInverse() method and a inverseTransform() method. So your code might be AffineTransform transform = AffineTransform.rotate(theta); transform.scale(sx sy); Then to invert you can say Point2D pointInOrigCoords = transform.inverseTransform(clickPointnull);"
537,A,"How do I intialize a Graphics object in Java? this is the code: import java.awt.*; import java.applet.*; public class anim1 extends Applet{ public void paint (Graphics g) { g.drawString(""""400300); } public static void main(String ad[]) { anim1 a=new anim1(); Graphics g1; a.paint(g1); } } It says that g1 is not initialized. But how do I initialize an abstract class? instead of a call to paint(Graphics g) you need to invoke the repaint or update method. But for this your class must belong to the hierarchy in the java.awt.Container. For your class you have overriden the Paint method and in the main you are trying to invoke the paint method. Instead of paint you need to invoke the repaint or update method (if your class is in the hierarchy of java.awt.Container) and the event dispatching system of java invokes your overridden paint method itself.  Well there are two issues here 1:  Graphics g1; a.paint(g1); And you are getting the error that G1 is not initialized. That's because the variable g1 is never set to anything and that causes a compile error. To get the code to compile you would need to at the very least do this:  Graphics g1 = null; a.paint(g1); However that obviously won't help you out too much. You'll get a NullPointerException when you try to run your code. In order to actually cause your graphics to draw you need to this:  anim1 a=new anim1(); Graphics g1 = anim1.getGraphics(); a.paint(g1); However that still won't work because Anim1 will not appear on the screen. To get it to appear on the screen you need something like: import java.awt.*; import javax.swing.*; import java.applet.*; public class So1 extends Applet{ public void paint (Graphics g) { g.drawString(""hello""4030); } public static void main(String ad[]) { JFrame jp1 = new JFrame(); So1 a=new So1 (); jp1.getContentPane().add(a BorderLayout.CENTER); jp1.setSize(new Dimension(500500)); jp1.setVisible(true); } } Now notice we don't actually call the paint() function ourselves. That's handled by the awt which actually picks the graphics context and calls our paint function for us. If you want though you can pass in any graphics object you want and ask it to draw on to that. (so if you want to draw your component onto an image you can do it) (note I changed the classname from anim1 to So1)  An applet doesn't need a main method like a regular Java application does. I'd recommend starting with Sun's Applets Tutorial. In particular you may want to skip ahead to the section Life Cycle of an Applet to see how the Graphics object is handled within the applet.  All you need to do is simply remove the main method like so: import java.awt.*; import java.applet.*; public class anim1 extends Applet { public void paint (Graphics g) { g.drawString(""Hello""100100); } } In order to have the display refresh you would call the method repaint() on an instance of your anim1 object.  You dont initialize a Graphics object. You get the graphics object from a component via Component#getGraphics() method. In your particular case I think repaint() is all you need!!  You should manipulate the graphics of a component within the paint method and invoke repaint() or update() but not the paint method directly. Start here for some more information."
538,A,"Quartz compositions created in Snow Leopard (10.6) doesn't work in Leopard (10.5) despite testing in runtime I have a reasonably advanced (many patches and subpatches) quartz composition that was created in Snow Leopard but doesn't run well (many elements are not rendered) in Leopard. The composition tested OK via Quartz Composer's Test in Runtime option and works fine for both Leopard 32-bits and Leopard 64-bits (menu item ""File | Test in Runtime | Leopard 32-bits"". In an actual Leopard (32-bits) system a lot of elements are not rendered in the quartz composition. Below are the log file excerpt when the composition is run in QuickTime Player under Leopard: QuickTime Player[134] *** <QCNodeManager | namespace = ""com.apple.QuartzComposer"" | 335 nodes>: Patch with name ""/units to pixels"" is missing QuickTime Player[134] *** Message from <QCPatch = 0x06D82880 ""(null)"">:Cannot create node of class ""/units to pixels"" and identifier ""(null)"" QuickTime Player[134] *** Message from <QCPatch = 0x06D7C130 ""(null)"">:Cannot create node of class ""/resize image to target"" and identifier ""(null)"" QuickTime Player[134] *** Message from <QCPatch = 0x06D7C130 ""(null)"">:Cannot create connection from [""outputValue"" @ ""Math_1""] to [""Target_Pixels"" @ ""Patch_2""] The patch units to pixels is a system virtual patch in Snow Leopard (located in /System/Library/Graphics/Quartz Composer Patches/Units to Pixels.qtz) whereas the patch resize image to target is a custom virtual patch located in my home directory. It seems that we can cross out problems in which the composition is referencing a missing virtual patch. I have tested the composition under another user's account and it ran fine which shows that it already embeds the ""resize image to target"" virtual patch that is located in my home directory. I'm really puzzled why the composition passes the Leopard Runtime test but yet fail to run in an actual Leopard OS? Is there a post-processing step that I need to run to the composition file? Is there any way to make this patch more compatible with Leopard? Thanks in advance. Leopard does not support embedded virtual patches (even though the Test In Runtime feature appears to). Go to the File menu and hold the Option key then select Save a Flattened Copy As.... This will convert all virtual patches to normal Macros which Leopard should be able to comprehend. It worked! Thanks so much. Now why do Apple hides an important menu item like that in the first place... not intuitive ^_^"
539,A,Tips for efficient GLSL code Are there any guidelines for writing efficient shaders in GLSL? Does the compiler handle most of the optimization? While many traditional c optimizations work for glsl there does exist some specific optimizations for GLSL. If you are new to shader programming dont spend too much with optm your compiler can do extremely efficient jobs for you. You can gather Some other advanced optm techniques as you dive deeper into graphics programming. good luck.  A few tips are here: Common mistakes in GLSL Also avoid branching whenever possible. That is if and while statements and for statements that have a comparison with a variable for example: for (int i=0; i<n; i++) {} will be slow. However for (int i=0; i<10; i++) {} should be much faster because most of the time the loop is unrolled and when it's not all the shading units are still executing the same code at the same time so there is no performance penalty. Instead of branching try using conditional compilation using the preprocessor. Also check out nVidia and ATI specific #pragmas to tweak the efficiency.
540,A,Transform a Direct3D Mesh I tried to write a TransformMesh function. The function accepts a Mesh object and a Matrix object. The idea is to transform the mesh using the matrix. To do this I locked the vertex buffer and called Vector3::TransformCoordinate on each vertex. It did not produce expected results. The resulting mesh was unrecognizable. What am I doing wrong? // C++/CLI code. My apologies. int n = verts->Length; for(int i = 0; i < n; i++){ verts[i].Position = DX::Vector3::TransformCoordinate(verts[i].Position matrix); } Without contextual code around what you are doing it might be hard to know the exact problem. How is the mesh created? How is verts[] read how is it written? Are you trying to read from a write only vertex buffer? My recommendation would be to try with a very simple translation matrix first and debug the code and see the vertex input and output. See if you receive good data and if it's transformed correctly. If so the problem is in vertex stride stream declaration or something else deeper in the DirectX pipeline. As I said more code would be needed to pinpoint the origin of the problem.  I recommend to use function D3DXConcatenateMeshes. Pass one mesh and one matrix. Result will be transformed mesh. It's quite easy.  I totally agree with Coincoin contextual code would help. And if you want just to draw transformed mesh to the screen you don't need to transform the mesh in this way. You can just change one of world view and projection matrices. This produces expected result. Like in the following sample code.  // Clear the backbuffer to a Blue color. device.Clear(ClearFlags.Target | ClearFlags.ZBuffer Color.Blue 1.0f 0); // Begin the scene. device.BeginScene(); device.Lights[0].Enabled = true; // Setup the world view and projection matrices. Matrix m = new Matrix(); if( destination.Y != 0 ) y += DXUtil.Timer(DirectXTimer.GetElapsedTime) * (destination.Y * 25); if( destination.X != 0 ) x += DXUtil.Timer(DirectXTimer.GetElapsedTime) * (destination.X * 25); m = Matrix.RotationY(y); m *= Matrix.RotationX(x); device.Transform.World = m; device.Transform.View = Matrix.LookAtLH( new Vector3( 0.0f 3.0f-5.0f ) new Vector3( 0.0f 0.0f 0.0f ) new Vector3( 0.0f 1.0f 0.0f ) ); device.Transform.Projection = Matrix.PerspectiveFovLH( (float)Math.PI / 4 1.0f 1.0f 100.0f ); // Render the teapot. teapot.DrawSubset(0); // End the scene. device.EndScene(); This sample is taken from here.
541,A,"Finding points on a line with a given distance I have a question i know a line i just know its slope(m) and a point on it A(xy) How can i calculate the points(actually two of them) on this line with a distance(d) from point A ??? I m asking this for finding intensity of pixels on a line that pass through A(xy) with a distance .Distance in this case will be number of pixels. I would suggest converting the line to a parametric format instead of point-slope. That is a parametric function for the line returns points along that line for the value of some parameter t. You can represent the line as a reference point and a vector representing the direction of the line going through that point. That way you just travel d units forward and backward from point A to get your other points. Since your line has slope m its direction vector is <1 m>. Since it moves m pixels in y for every 1 pixel in x. You want to normalize that direction vector to be unit length so you divide by the magnitude of the vector.  magnitude = (1^2 + m^2)^(1/2) N = <1 m> / magnitude = <1 / magnitude m / magnitude> The normalized direction vector is N. Now you are almost done. You just need to write the equation for your line in parameterized format:  f(t) = A + t*N This uses vector math. Specifically scalar vector multiplication (of your parameter t and the vector N) and vector addition (of A and t*N). The result of the function f is a point along the line. The 2 points you are looking for are f(d) and f(-d). Implement that in the language of your choosing. The advantage to using this method as opposed to all the other answers so far is that you can easily extend this method to support a line with ""infinite"" slope. That is a vertical line like x = 3. You don't really need the slope all you need is the normalized direction vector. For a vertical line it is <0 1>. This is why graphics operations often use vector math because the calculations are more straight-forward and less prone to singularities. It may seem a little complicated at first but once you get the hang of vector operations a lot of computer graphics tasks get a lot easier. thanks for the help it works @ALevy You introduce a variable 'A' but I don't see what 'A' is defined as. What's 'A'? @TomAuger I introduced A in the introductory paragraph of this answer. A is a point on the line. Doesn't matter which point. Any point that is on the line will do.  So if your slope is in a ratio of height/width on the screen (and when I first read the question I didn't realise you were in 2D until you mentioned pixels)... So if we consider a point (ab) it's on your line if: (a-x) / (b-y) = m So now if you know 'a' or 'b' you should be able to work out the other one. Of course you'll have to decide what 'd' means. Is that on one axis or is it the length of the diagonal line? Pythagorus should be able to help you with that one. Rob  Let's call the point you are trying to find P with coordinates px py and your starting point A's coordinates ax and ay. Slope m is just the ratio of the change in Y over the change in X so if your point P is distance s from A then its coordinates are px = ax + s and py = ay + m * s. Now using Pythagoras the distance d from A to P will be d = sqrt(s * s + (m * s) * (m * s)). To make P be a specific D units away from A find s as s = D/sqrt(1 + m * m).  I thought this was an awesome and easy to understand solution: http://www.physicsforums.com/showpost.php?s=f04d131386fbd83b7b5df27f8da84fa1&p=2822353&postcount=4"
542,A,Recommended 3D Programming Aspects for Light/Laser Show Simulator? Hey guys I would like to develop a light/laser show editor and simulator and for this of course I am going to learn some graphics programming. I am thinking about using C# and XNA. I was just wondering what aspects of graphics programming I should research or focus on given the project I am working on. I am new to graphics programming so I don't know much about it but for example I imagine something that I might look into would (possibly?) be volumetric lighting. For example what would be a practical way to go about rendering a 'laser' of varied width/color? I read somewhere to just draw a cylinder and apply a shader to it I would like to confirm that this is the way. Given that this seems like a big project I was thinking about starting off by creating light sources and giving them properties so that I can easily manipulate them. I have (mis)read that only a certain amount of lights can be rendered at any given time I believe eight. Does this only apply to ambient lights? Given this possible limitation and the fact that most of the lights I will use will be directional such as head-lights or lasers what would be a different way to render these? Is that what volumetric lighting would be? I'd just like to get some things clear before I dive into it. Since I'm new to this I probably didn't make the best use of words so if something doesn't make sense please let me know. Thanks and sorry for my ignorance. Although its faster I think its best not to use vanilla hardware lighting because of its limitations. Pixel shaders can help with you task. Also you may want to chose OpenGL because of portability and its clarity in rendering methods. I worked on Direct3D for several years before switching to OpenGL. OpenGL functions and states are easier to learn and rendering methods (like multi-pass rendering) is a lot clear. If you like to code on C# (which I dont recommend for these tasks) you can use CsGL library to access OpenGL functions.  Well if it's just for light show simulations I'd imagine your going to need a lot of custom lighting effects - so regardless if you decide to use XNA or straight DirectX your best bet would be to start by learning shader languages and how to program various lighting effects using them. Once you can reproduce the type of laser lighting you want then you can experiment with the polygons you want to use to represent the lasers. (I've used the cylinder method in some of my work for personal purposes but I'm not sure how well straight cylinders will fit your purpose).  The answer to this depends on the level of sophistication that you need in your display simulation. Computer graphics is ultimately a simulation of the transport of light; that simulation can be as sophisticated as calculating the fraction of laser light deflected by particles in the atmosphere to the viewer's eyepoint or as simple as drawing a line. Try out the cylinder effect and see if it works for your project. If you need something more sophisticated look into shader programming (using Nvidia Cg for example) and volumetric shading as you mentioned; also post-processing glow effects may be useful. For OpenGL I believe there is a limit of 8? light sources in a scene but you could conceivably work around this limit by doing your own shading logic. You can through at the latest SIGGRAPH papers for inspiration to see if there are projects related to lighting effects; some of these may also help you: http://www.cs.rit.edu/~jmg/courses/procshade/20073/papers.html
543,A,Clearing DrawRectangle in Windows Forms I am drawing a rectangle sleeping for a few milliseconds--then I want to clear the rectangle but I can't figure out how. (The rectangle is sitting over a graphic so I can't simply cover it up with another rectangle)  graphics.DrawRectangle(p innerRectangle) System.Threading.Thread.Sleep(75) Next I want to clear the rectange... If the rectangle is sitting completely over the graphic you should be able to just redraw or refresh the underlying graphic. If it is not you will need to redraw the rectangle using the background color and then refresh the underlying graphic.  You need to redraw the graphic (or at least the part of it under the rectangle). If this is a picture box or something similar use Invaldiate() to force a repaint.  I guess it should work to copy the original data from the surface into a temporary bitmap before drawing the rectangle and then draw the bitmap back in place. Update There is already an accepted answer but I thought I could share a code sample anyway. This one draws the given rectangle in red on the given control and restores the area after 500 ms. public void ShowRectangleBriefly(Control ctl Rectangle rect) { Image toRestore = DrawRectangle(ctl rect); ThreadPool.QueueUserWorkItem((WaitCallback)delegate { Thread.Sleep(500); this.Invoke(new Action<Control Rectangle Image>(RestoreBackground) ctl rect toRestore); }); } private void RestoreBackground(Control ctl Rectangle rect Image image) { using (Graphics g = ctl.CreateGraphics()) { g.DrawImage(image rect.Top rect.Left image.Width image.Height); } image.Dispose(); } private Image DrawRectangle(Control ctl Rectangle rect) { Bitmap tempBmp = new Bitmap(rect.Width + 1 rect.Height + 1); using (Graphics g = Graphics.FromImage(tempBmp)) { g.CopyFromScreen(ctl.PointToScreen(new Point(rect.Top rect.Left)) new Point(0 0) tempBmp.Size); } using (Graphics g = this.CreateGraphics()) { g.DrawRectangle(Pens.Red rect); } return tempBmp; }
544,A,Using Graphics.copyArea with transparent image I'm trying to shift a portion of an image currently using g.copyArea(). It works fine with solid colors but the transparent pixels aren't being copied (because they're transparent!). I want to make the color underneath transparent. This image shows what's happening if a start shape was copied but I want the whole area to copy overwriting the all pixels below. This is what I want: BufferedImage b; ... Graphics g = b.getGraphics(); g.copyArea(xywhdxdy); I have considered copying the image to another image clearing the orginal image then copy it back to the new position but there must be a better way? Disclaimer: This is part of a homework project. Actually I do think this is a good homework question. Specific to the point only a problematic part and you showed that you tried before. +1 Use g.setComposite(AlphaComposite.Src) like so: Graphics2D g; ... g.setComposite(AlphaComposite.Src) g.copyArea(xywhdxdy); Thanks to unwind for suggesting the use of Graphics2D.  Are you sure you should be using Graphics? I think it is semi-deprecated and you're supposed to use Graphics2D nowadays. With Graphics2D you can set the background color which might help to prevent the unwanted transparency.
545,A,Draw arrow on line I have this code: CGPoint arrowMiddle = CGPointMake((arrowOne.x + arrowTo.x)/2 (arrowOne.y + arrowTo.y)/2); CGPoint arrowLeft = CGPointMake(arrowMiddle.x-40 arrowMiddle.y); CGPoint arrowRight = CGPointMake(arrowMiddle.x arrowMiddle.y + 40); [arrowPath addLineToScreenPoint:arrowLeft]; [arrowPath addLineToScreenPoint:arrowMiddle]; [arrowPath addLineToScreenPoint:arrowRight]; [[mapContents overlay] addSublayer:arrowPath]; [arrowPath release]; with this output: What have i to add to get the left and right the at same degree of the line + 30°. If someone has the algorithm of drawing an arrow on a line pleas give it. It doesn't matter what programming language it is... Thanks Thanks for respond! In the meanwhile I found also an solution. It's Like this:  double slopy  cosy  siny; double Par = 10.0; //length of Arrow (>) slopy = atan2( ( arrowOne.y - arrowTo.y ) ( arrowOne.x - arrowTo.x ) ); cosy = cos( slopy ); siny = sin( slopy ); //need math.h for these functions   CGPoint arrowMiddle = CGPointMake((arrowOne.x + arrowTo.x)/2 (arrowOne.y + arrowTo.y)/2); [arrowPath addLineToScreenPoint:arrowMiddle]; CGPoint arrowLeft = CGPointMake( arrowMiddle.x + round( - Par * cosy - ( Par / 2.0 * siny ) ) arrowMiddle.y + round( - Par * siny + ( Par / 2.0 * cosy ) ) ); [arrowPath addLineToScreenPoint:arrowLeft]; CGPoint arrowRight = CGPointMake( arrowMiddle.x + round( - Par * cosy + ( Par / 2.0 * siny ) )arrowMiddle.y - round( Par / 2.0 * cosy + Par * siny ) ); [arrowPath addLineToScreenPoint:arrowRight]; [arrowPath addLineToScreenPoint:arrowMiddle]; [[mapContents overlay] addSublayer:arrowPath]; [arrowPath release];  The only problem here is that i draw it like it's an RMPath(route-me framework) and that the arrow gets bigger/smaller when you zoom in/out. But thanks for respond I will look into it which code is the most perform. perfect that's what i was looking for all day around. thanks man.  Here is what you do. First take the vector of the line and normalize it by dividing it by its length — this will give you a vector of length 1 pointing in the direction of the line. Next multiply it by the length you need it to be. Turn it by 120° and -120° to make the arrow. Finally offset it by the coordinates where you want it to be. Here is how it would look like in code: // calculate the position of the arrow CGPoint arrowMiddle; arrowMiddle.x = (arrowOne.x + arrowTo.x) / 2; arrowMiddle.y = (arrowOne.y + arrowTo.y) / 2; // create a line vector CGPoint v; v.x = arrowTo.x - arrowOne.x; v.y = arrowTo.y - arrowOne.y; // normalize it and multiply by needed length CGFloat length = sqrt(v.x * v.x + v.y * v.y); v.x = 40 * (v.x / length); v.y = 40 * (v.y / length); // turn it by 120° and offset to position CGPoint arrowLeft = CGPointApplyAffineTransform(v CGAffineTransformMakeRotation(3.14 * 2 / 3)); arrowLeft.x = arrowLeft.x + arrowMiddle.x; arrowLeft.y = arrowLeft.y + arrowMiddle.y; // turn it by -120° and offset to position CGPoint arrowRight = CGPointApplyAffineTransform(v CGAffineTransformMakeRotation(-3.14 * 2 / 3)); arrowRight.x = arrowRight.x + arrowMiddle.x; arrowRight.y = arrowRight.y + arrowMiddle.y; perrrrrrrrrrrrrrfect!!!!
546,A,KeyListener problem In my apllication i am using a jpanel in which i want to add a key listener. I did it. But it doesnot work. Is it because i am using a swingworker to update the contents of the panel every second. Here is my code to update the panel RenderedImage image = ImageIO.read(new ByteArrayInputStream((byte[]) get())); Graphics graphics = remote.rdpanel.getGraphics(); if (graphics != null) { Image readyImage = new ImageIcon(UtilityFunctions.convertRenderedImage(image)).getImage(); graphics.drawImage(readyImage 0 0 remote.rdpanel.getWidth() remote.rdpanel.getHeight() null); } Does the JPanel have keyboard focus? I suggest you use the InputMap and WHEN_IN_FOCUSED_WINDOW or something similar. Excerpt from How to Use Key Bindings: The WHEN_IN_FOCUSED_WINDOW input maps of all the enabled components in the focused window are searched. Because the order of searching the components is unpredictable That has worked very robustly for me. Have a look at my other post for more information and actual code examples: Keyboard input for a game in Java or this tutorial: Swing: Understanding Input/Action Maps Related question: JPanel not listening to key event when there is a child component with JButton on it
547,A,"Graphics-Related Question: Mesh and Geometry What's the difference between mesh and geometry? Aren't they the same? i.e. collection of vertices that form triangles? In computer graphics 'geometry' is often used for the final solid shape to distinguish it from the original unordered points A mesh is typically a collection of polygons/geometric objects. For instance triangles quads or a mixture of various polygons. A mesh is simply a more complex shape. From Wikipedia: Geometry is a part of mathematics concerned with questions of size shape and relative position of figures and with properties of space IMO a mesh falls under that criteria.  A point is geometry but it is not a mesh. A curve is geometry but it is not a mesh. An iso-surface is geometry but it is not... enfin you get the point by now. Meshes are geometry not the other way around. Geometry in the context of computing is far more limited that geometry as a branch of mathematics. There are only a few types of geometry typically used in computer graphics. Sprites are used when rendering points (particles) line segments are used when rendering curves and meshes are used when rendering surface-like geometry. I didn't get the point. Saying what is not a mesh doesn't explain what a mesh is. A mesh is a set of straight lines between points forming a surface. Both the points and the lines are geometries.  Although this is tagged in ""graphics"" I think the answer connects with the interpretation from computational physics. There we usually think of the geometry as an abstraction of the system that is to be represented/simulated while the mesh is an approximation of the geometry - a compromise we usually have to make to be able to represent the spatial domain within the finite memory of the machine. You can think of them basically as regular or unstructured sets of points ""sprayed"" on a surface or within a volume in space. To be able to do visualization/simulation it is also necessary to determine the neighbors of each point - for example using Delaunay triangulation which allows you to group sets of points into elements (for which you can solve algebraic versions of the equations describing your system). In the context of surface representation in computer graphics I think all major APIs (e.g. OpenGL) have functions which can display these primitives (which can be triangles as given by Delaunay quads or maybe some other elements).  In the context implied by your question: A mesh is a collection of polygons arranged in such a way that each polygon shares at least one vertex with another polygon in that collection. You can reach any polygon in a mesh from any other polygon in that mesh by traversing the edges and vertices that define those polygons. Geometry refers to any object in space whose properties may be described according to the principles of the branch of mathematics known as geometry.  That the term ""geometry"" has different meanings mathematically and in rendering. In rendering it usually denotes what is static in a scene (walls etc.) What is widely called a ""mesh"" is a group of geometrical objects (basically triangles) that describe or form an ""object"" in the scene - pretty much like envalid said it but usually a mesh forms a single object or entity in a scene. Very often that is how rendering engines use the term: The geometrical data of each scene element (object entity) composes that element's mesh."
548,A,GDI+: How do I draw a line that's one inch in length on any device it's drawn on? I need to draw a line one inch long on any device given a Graphics reference to it. I need it to be an inch long regardless of what Transform is set to. Let's assume that the scaling factor of the transform is given by scale in both horizontal and vertical directions. Some C++/CLI code: g->DrawLine(Pens::Black 50.0f 50.0f 50.0f + oneInchEquivalent / scale 50.0f); Now that was not difficult at all! Now all we need to do is calculate oneInchEquivalent. g->DpiX gives me a distance of what looks like one inch on screen but not on the printer. It seems that on printers drawing a line of 100 units with g->PageUnit set to GraphicsUnit::Display will give me a line one inch long. But I really need this to work regardless of the PageUnit setting. In fact changing PageUnit will change the width of the pen!! Edit: I have tentatively accepted the only answer here as it's pretty close to what I am looking for. I updated my answer now with a print sample The answer became rather long after a couple of edits so here is the final result: Setting the PageUnit property of the Graphics object to GraphicsUnit.Pixel and taking in multiplying coordinates with the DpiX and DpiY values will render the expected result on both display and printer devices. private static void DrawInchLine(Graphics g Color color Point start Point end) { GraphicsUnit originalUnit = g.PageUnit; g.PageUnit = GraphicsUnit.Pixel; using (Pen pen = new Pen(color 1)) { g.DrawLine(pen start.X * g.DpiX start.Y * g.DpiY end.X * g.DpiX end.Y * g.DpiY); } g.PageUnit = originalUnit; } You can have it paint on a Form (or some control): using (Graphics g = this.CreateGraphics()) { Point start = new Point(1 1); Point end = new Point(2 1); DrawInchLine(g Color.Black start end); } ...or send the output to a printer: PrintDialog dialog = new PrintDialog(); if (dialog.ShowDialog() == DialogResult.OK) { PrintDocument pd = new PrintDocument(); pd.PrinterSettings = dialog.PrinterSettings; pd.PrintPage += (psender pe) => { Point start = new Point(1 1); Point end = new Point(2 1); DrawInchLine(pe.Graphics Color.Black start end); pe.HasMorePages = false; }; pd.Print(); } This does however rely on setting the PageUnit. You can try CutePDF for a printer emulator. You can see the differences there. I just realized I have a printer standing here; tried to print on it; came out at exactly one inch. Will update with the code. Thanks Fredrik but I need to do this using Math alone without changing the PageUnit setting. Changing PageUnit will affect the width of the pen. See the last part of my question. Ah missed that part... Unfortunately I cannot see a simple solution apart from using `Pixel` as base unit for all drawing and scaling it to the target device using `DpiX` and `DpiY`. I'll accept this anyway. It's pretty informative.
549,A,How to create a picture with animated aspects programmatically Background I have been asked by a client to create a picture of the world which has animated arrows/rays that come from one part of the world to another. The rays will be randomized will represent a transaction will fade out after they happen and will increase in frequency as time goes on. The rays will start in one country's boundary and end in another's. As each animated transaction happens a continuously updating sum of the amounts of all the transactions will be shown at the bottom of the image. The amounts of the individual transactions will be randomized. There will also be a year showing on the image that will increment every n seconds. The randomization summation and incrementing are not a problem for me but I am at a loss as to how to approach the animation of the arrows/rays. My question is what is the best way to do this? What frameworks/libraries are best suited for this job? I am most fluent in python so python suggestions are most easy for me but I am open to any elegant way to do this. The client will present this as a slide in a presentation in a windows machine. The client will present this as a slide in a presentation in a windows machine I think this is the key to your answer. Before going to a 3d implementation and writing all the code in the world to create this feature you need to look at the presentation software. Chances are your options will boil down to two things: Animated Gif Custom Presentation Scripts Obviously an animated gif is not ideal due to the fact that it repeats when it is done rendering and to make it last a long time would make a large gif. Custom Presentation Scripts would probably be the other way to allow him to bring it up in a presentation without running any side-programs or doing anything strange. I'm not sure which presentation application is the target but this could be valuable information. He sounds like he's more non-technical and requesting something he doesn't realize will be difficult. I think you should come up with some options explain the difficulty in implementing them and suggest another solution that falls into the 'bang for your buck' range.  If you are adventurous use OpenGL :) You can draw bezier curves in 3d space on top of a textured plane (earth map) you can specify a thickness for them and you can draw a point (small cone) at the end. It's easy and it looks nice problem is learning the basics of OpenGL if you haven't used it before but that would be fun and probably useful if your in to programing graphics. You can use OpenGL from python either with pyopengl or pyglet. If you make the animation this way you can capture it to an avi file (using camtasia or something similar) that can be put onto a presentation slide.  It depends largely on the effort you want to expend on this but the basic outline of an easy way. Would be to load an image of an arrow and use a drawing library to color and rotate it in the direction you want to point(or draw it using shapes/curves). Finally to actually animate it interpolate between the coordinates based on time. If its just for a presentation though I would use Macromedia Flash or a similar animation program.(would do the same as above but you don't need to program anything)
550,A,"Point Sequence Interpolation Given an arbitrary sequence of points in space how would you produce a smooth continuous interpolation between them? 2D and 3D solutions are welcome. Solutions that produce a list of points at arbitrary granularity and solutions that produce control points for bezier curves are also appreciated. Also it would be cool to see an iterative solution that could approximate early sections of the curve as it received the points so you could draw with it. Unfortunately the Lagrange or other forms of polynomial interpolation will not work on an arbitrary set of points. They only work on a set where in one dimension e.g. x xi < xi+1 For an arbitary set of points e.g. an aeroplane flight path where each point is a (longitude latitude) pair you will be better off simply modelling the aeroplane's journey with current longitude & latitude and velocity. By adjusting the rate at which the aeroplane can turn (its angular velocity) depending on how close it is to the next waypoint you can achieve a smooth curve. The resulting curve would not be mathematically significant nor give you bezier control points. However the algorithm would be computationally simple regardless of the number of waypoints and could produce an interpolated list of points at arbitrary granularity. It would also not require you provide the complete set of points up front you could simply add waypoints to the end of the set as required. Very good point wrt to polynomials.  Google ""orthogonal regression"". Whereas least-squares techniques try to minimize vertical distance between the fit line and each f(x) orthogonal regression minimizes the perpendicular distances. Addendum In the presence of noisy data the venerable RANSAC algorithm is worth checking out too.  There are several algorithms for interpolating (and exrapolating) between an aribtrary (but final) set of points. You should check out numerical recipes they also include C++ implementations of those algorithms.  One way is Lagrange polynominal which is a method for producing a polynominal which will go through all given data points. During my first year at university I wrote a little tool to do this in 2D and you can find it on this page it is called Lagrange solver. Wikipedia's page also has a sample implementation. How it works is thus: you have a n-order polynominal p(x) where n is the number of points you have. It has the form a_n x^n + a_(n-1) x^(n-1) + ...+ a_0 where _ is subscript ^ is power. You then turn this into a set of simultaneous equations: p(x_1) = y_1 p(x_2) = y_2 ... p(x_n) = y_n You convert the above into a augmented matrix and solve for the coefficients a_0 ... a_n. Then you have a polynomial which goes through all the points and you can now interpolate between the points. Note however this may not suit your purpose as it offers no way to adjust the curvature etc - you are stuck with a single solution that can not be changed.  The Catmull-Rom spline is guaranteed to pass through all the control points. I find this to be handier than trying to adjust intermediate control points for other types of splines. This PDF by Christopher Twigg has a nice brief introduction to the mathematics of the spline. The best summary sentence is: Catmull-Rom splines have C1 continuity local control and interpolation but do not lie within the convex hull of their control points. Said another way if the points indicate a sharp bend to the right the spline will bank left before turning to the right (there's an example picture in that document). The tightness of those turns in controllable in this case using his tau parameter in the example matrix. Here is another example with some downloadable DirectX code. A vote for this. I used Catmull in a video game some years ago and it worked a charm. This is exactly what Catmull-Rom was designed for (specifically to get a motion spline to go through the control points).  Have you looked at the Unix spline command? Can that be coerced into doing what you want? Well that is an interesting place to look for a solution! Thanks. I'll check it out.  In the 3D graphics world NURBS are popular. Further info is easily googled.  I came up with the same problem and implemented it with some friends the other day. I like to share the example project on github. https://github.com/johnjohndoe/PathInterpolation Feel free to fork it.  You should take a look at B-splines. Their advantage over Bezier curves is that each part is only dependent on local points. So moving a point has no effect on parts of the curve that are far away where ""far away"" is determined by a parameter of the spline. The problem with the Langrange polynomial is that adding a point can have extreme effects on seemingly arbitrary parts of the curve; there's no ""localness"" like described above."
551,A,"Do ""expert"" programmers and designers really exist It seems like every job posting I see any more and most recruiters I talk to insist that the only way I could qualify for the position posted is if I'm an ""expert"" programmer and designer. And when they say ""programmer and designer"" they seem to mean designer front-end developer and back-end programmer (and most seem to assume that also means sysadmin). So what are your thoughts? Do these mythical master programming designers really exist or am I just being illogical in my confusion thinking that these guys actually expect to find some one who's an ""expert"" programmer and designer willing to work in a ""senior"" position for a barely livable salary. They exist but I think a lot of employers aim really high when writing job postings. The best programmers and designers are like Bruce Lee - they don't use any one particular technique or tool. Instead they are masters of their environment and use what is available to create what is needed. This is something that is very difficult for most people to achieve. My favorite ads require you to have been using best practices in Java for five more years than it has existed. Of course they exist. But they don't go through interviews in order to get hired - employers who expect ""expert"" positions to be filled through interviews are delusional. I don't get the question. Yes they exist. What's the point? What more do you want to know? What does it matter if they exist or don't exist? What are you going to *do* with this information? Why are you asking? Lott he's trying to figure out how to deal with non-technical HR people who throw around terms that they shouldn't. I would hesitate to call myself an expert to a fellow developer I'd call myself a ""super-whiz-kid-guru-dba-designer"" without even blinking if that's what HR wants to hear from me. The local paper had a guy leave. When they advertised to replace him the requirements were insane. There were something like 6 _required_ languages you had to be expert in. I thought man I've got like 30 years of programming and I can't do half of this! I figured that's a $125k senior level job. Nope they wanted to pay like $40k. No wonder the guy left. Turns out whenever he got bored he'd write a new tool in a different language. the test of an ""expert"" programmer is when they program the images of their laptop screen is reflected on their faces and you can actually see the code in reverse :P Unless I've seen their work myself the more ""expert"" someone claims to be the less expert I assume they are. Somebody who on a scale of 0 to 10 is 8+ at each of these skills (designer front-end developer back-end programmer AND sysadmin) might conceivably exist though I've never met one in 30 years of a career mostly spent moving among the upper reaches of most of these professions (not ""designer"" -- in the usual graphical-rich sense of the word -- but my wife's career is different enough from mine that she's had Terry Winograd as a professor and advisor so it's not as if I'm all that unfamiliar with those... I've also worked ""side by side"" with Jeffrey and Asa Raskin [we were consulting in different capacities for the same client] and on other occasions with students of E. Tufte...). If some such ""Renaissance Man [[or Woman]] of the 21st Century"" exist it would be peculiar indeed for him or her not to be a highly-paid consultant entrepreneur or startup co-founder... I guess some personality disorder or (temporary!-) dire financial need might possibly place such a paragon in the position of seeking employment but we're piling improbability upon improbability here in a tower reaching up to the stars...!-) that is very poetic :) ... ""piling improbability upon improbability here in a tower reaching up to the stars""  Most good programmers are humble folk and have a hard time being declared a ""master"" or ""expert"". So don't be distraught because you don't think you're an ""expert"". If you're smart and get things done go for it! Whenever I'm asked the ""rate yourself on a scale of 1 to 10"" question I always tell'em there's no such thing as a 10 and they seem to get confused. But I still maintain when it comes to any skill there's no such thing as a 10 and if anyone says they are a 10 don't even think about hiring them. I agree. I think of Socrates was alive today he'd be an unemployed programmer. I think you could say the same of Einstein. I'm wrong he would be working for Google. ""Today Google announced it's latest project gRelativity which allows users to experience 32 hour days incrementing at .0007 seconds every minute for free!"" 10 in language 'x' is the person who designed and created language 'x' on their own.  Yes ""experts"" do exist and you might even be one in something. The problem is that you're talking to recruiters and you need to understand their perspective. Most recruiters don't know anything about tech jobs beyond the various in-vogue acronyms and buzzwords. The actual companies doing the hiring rarely have the same expectations. Not all recruiters are equal of course (if you find a good one keep the relationship active!) but most of them are simply doing keyword matching between positions and candidates. And most of them will up-sell both the position being offered and the people they submit for it. Take a close look at the job requirement; if you think you'd be a good fit and be able to do the job then go ahead and tell the recruiter you're an ""expert"". For their purposes and for their definition of the word you probably are an expert.  Managers say they want skills but what they really want are results. An ""expert"" is someone who consistently delivers above expectations. To that degree it correlates with experience. But experience does not make one an expert.  Anyone who has really good problem solving and CS skills can do all above mentioned things which make him (very rarely her) an 'expert' programmer.  I sure hope so; otherwise I'm out of a job.  Everyone is an expert until someone better comes along and rubs their nose in the dirt.  A few things here. Recruiters typically aren't technically inclined. I'm not sure why programmers still haven't been put in charge of hiring their own colleagues. The same people who constantly say ""Oh haha I don't understand anything about all that computer stuff."" still want to be in charge of it all. It's very weird. Anyway just look at the job requirements and if you meet them call yourself ""cream cheese"" if that's what they want to hear. On wages if you are a competent developer you should charge a decent rate. There are a lot of companies out there looking for real talent so if you can convince them you've got it they'll be willing to negotiate. I know of two companies right now that are actively looking for developers but can't seem to get any interviewees capable of performing basic algebra. You have to learn how to sell yourself to both types of people. Prepare completely different answers to the same interview questions depending on who you are talking to. HR people want to feel like you have work ethic Tech people want to know about your abilities. Relax. In the midst of an economic recession you have a skill set that is still in very high demand and short supply. It won't always be this way so enjoy it while it lasts.  Jack of all trades master of none? Sorry but this little phrase is now and always has been wrong. In reality in order to be a master at something you have to have a good grasp of the surrounding things. For example to be a master programmer you would have strong skills in database networking and architecture. Along the way you will more than likely have picked up a good knowledge of UI layouts that work or don't work. The point is a ""master"" will by definition be a jack of all trades.  If they're looking for an ""expert"" and you clearly fit the job requirements otherwise then you're clearly an ""expert"" - so go ahead and say so.  Everyone asks for the moon and takes the best that they can when it comes time to really fill the position. If you don't like the pay or description let them go. Otherwise you won't know what they really want until you interview."
552,A,Seamless Transitions of Scale over large distances (3D rendering) What is the best mechanism for handling large scale structures and scenes? Examples being a continent with scale cities and geography or infinity universe style planetary transitions. The biggest problem with working with floating point representations of positions in large scale is that you rapidly lose precision as you get further and further away from the origin. To remedy this you need to express all positions as relative to something else than the origin. The easiest way to do this is to partition the world into a grid and store the positions of all entities something like this: struct Position { int kilometers[3]; // x y and z offset in kilometers float offset[3]; //x y and z offset in meters }; The position of the camera is also stored like this and when it's time to render you do something like this: void getRelativePosition(float& x float& y float& z const Position& origin const Position& object) { x = (object.kilometers[0] - origin.kilometers[0]) * 1000.0f + (object.offset[0] - origin.offset[0]); //Ditto for y and z } //Somewhere later float x y z; getRelativePosition(x y z camera.position() object.position()); renderMesh(x y z object.mesh()); (For simplicity I ignored the orientation of camera and objects in this example since there are no special problems associated with this). If you're working with a continuous world on a galactic scale you can replace the kilometers parameter with a long long (64 bits) giving you an effective range of 1.8 million lightyears. EDIT: To use this for continuous geometry such as terrain etc you have to split the terrain into chunks of size one square kilometers the coordinates of the vertices in the terrain chunk should be in the range [0 1000]. Also in the function getRelativePosition above you could change it so it returns a bool and return false if the difference in kilometers is larger than some threshold (say the distance to your far clip plane). hmmm I'm not sure how this would actually get around the floating point error as at very large scales you would be multiplying 1000 by a very large number anyway and your just swapping one set scale and all its problems for a new one =/ ah I see now what your getting at but it still leaves the problem of very very large structures e.g. planets/mountains/moons @Tom You can't stora a planet as a single mesh se my edited answer ah well there's large scales and large distances now for the kicker drawing large objects that are far away e.g. sitting on one planets surface and drawing the neighbouring planet in the sky Ill give you the question since I think youve answered the original question already! @Tom The problem with large view distance is that the precision of the z-buffer won't be sufficient. It's hard to cram an answer on how to handle this into a comment but basically you'll have to partition the geometry along the z axis and render each partition separately.  Might be worth looking into the tech behind Bing Maps Deep Zoom Similar tech is used by Google Earth which lets you go form Planet view all the way down to street view pretty smoothly. There is obviously a lot of resolution swapping going on as you zoom in and out. nifty how would this be applied to 3D geometry outside of silverlight?  Taken from http://www.gamedev.net/reference/business/features/spotlightFB/ Q:Space. It’s big. It’s REALLY big. It must be daunting to code in such huge scales – how do you go about such a thing? Do you use any special data structures or measurement units to help? A: As you can imagine working with a single type of unit doesn’t work. I use a hierarchical system of units. At the galactic level light-year (LY) units are utilized. At the star system level the kilometer is the base unit and coordinates are represented by double-precision floating point numbers. At render time vertices are generated as single-precision floats but translated into camera space to minimize the loss of precision due to large numbers.
553,A,"Version Control for Graphics Say a development team includes (or makes use of) graphic artists who create all the images that go into a product. Such things include icons bitmaps window backgrounds button images animations etc. Obviously everything needed to build a piece of software should be under some form of version control. Should the graphics people use the same version-control system and repository that the coders do? If not what should they use and what is the best way to keep everything synchronized? http://superuser.com/questions/5917/version-control-for-images @Damian - Good point about the tagging and cross referencing. That's true; while I haven't working with many designers on a software development project I have worked for a company that had a design department and know that this is an issue. Designers are still (perpetually) looking for the perfect system to handle this sort of thing. I think this is more suited to a design department for shared access searching and versioning etc to all assets -- where there is a business incentive to not reinvent the wheel wherever/whenever possible. I don't think it would apply for a project-oriented manner as tagging and cross referencing wouldn't be quite as applicable.  You might want to take a look at Boar: ""Simple version control and backup for photos videos and other binary files"". It can handle binary files of any size. http://code.google.com/p/boar/  We too just put the binaries in source control. We use Git but it would apply just as well to Subversion. One suggestion I have is to use SVGs where possible because you can see actual differences. With binaries (most other image formats) the best you can get is a version history. +1 for the use of SVG. It will also reduce the size of the git folder since it can save differences instead of the all file versions. Adobe products have XML structure inside. I haven't tried it but thinks that it's good to work with them in version control.  In my opinion Pixelapse combined with a backup solution is the best version control software for graphics that I've found thus far. It supports adobe files and a bunch of normal raster images. It has version by version preview. It autosaves when the files update(on save). It works like dropbox but have a great web interface. You can use it in teams and share projects to different people. It also support infinite reviewers which is great for design agencies. And if you want you can publicly collaborate on projects that are ""open"". Unfortunately you can't have a local pixelapse server so for backup my current setup is that I have the Pixelapse folder(like a dropbox folder) inside a git repo for snapshot creation. LayerVault looks better  A lot of the graphics type people will want something more sophisticated than subversion. While it's good for version control they will want a content management system that allows cross-referencing of assets tagging thumbnails and that sort of thing (as well as versioning).  TortoiseSVN can show image revisions side-by-side which is really useful. I've used it with different teams with a great degree of success. The artists loved having the ability to roll back things (after they got used to the concepts). It does take a lot of space though. TortoiseGIT do that as well! \o/  I would definitely put the graphics under version control. The diff might not be very useful from within a diff tool like diffmerge but you can still checkout two versions of the graphic and view them side by side to see the differences. I don't see any reason why the resultant graphics shouldn't be kept in the same version control system that the coders use. However when you're creating graphics using PSD files or PDN files you might want to create a seperate repository for those as they have a different context to the actual end jpeg or gif that is produced and deployed with the developed application.  Github recently introduced ""image view modes"" take a look: https://github.com/blog/817-behold-image-view-modes  Interesting question. I don't have a bunch of experience working directly with designers on a project. When I have it's been through a contractual sort of agreement where they ""delivered"" a design. I have done some of my own design work for both web sites and desktop applications and though I have not used source control in the past I am in the process of implementing SVN for my own use as I am starting to do some paid freelance work. I intend to utilize version/source control precisely the way I would with source code. It just becomes another folder in the project trunk. The way I have worked without source control is to create an assets folder in which all media files that are equivalents of source code reside. I like to think of Photoshop PSD's as graphics source code while the JPEG output for a web site or otherwise is the compiled version. In the case of working with designers which is a distinct possibility I face in the near future I'd like to make an attempt to have them ""check-in"" their different versions of their source files on a regular basis. I'll be curious to read what others with some experience will say in response to this.  Yes having art assets in version control is very useful. You get the ability to track history roll back changes and you have a single source to do backups with. Keep in mind that art assets are MUCH larger so your server needs to have lots of disk space & network bandwidth. I've had success with using perforce on very large projects (+100 GB) however we had to wrap access to the version control server with something a little more artist friendly. I've heard some good things about Alienbrain as well it does seem to have a very slick UI.  With respect to diff and merging I think the version control is more critical for graphics and media elements. If you think about it most designers are going to be the sole owners of a file -- at least in the case of graphics -- or at least I would think that'd be the case. I'd be curious to hear from a designer.  A free and slightly wonky solution is Adobe version Cue its comes with the Adobe Suites up to CS4 and is easy to install and maintain. Offers user level control and is artist friendly. Adobe has discontinued support though for it which is a shame. Adobe Bridge acts as the client between the user and the Version Cue server. If used properly its an inexpensive solution to version control. I use CS3 version cue with CS3 Bridge. Works great for small teams. now it's a part of Creative Cloud  We use subversion. Just place a folder under /trunk/docs for comps and have designers check out and commit to that folder. Works like a champ.  We keep the binary files and images in revision control using Perforce. It's great! We keep a lot of art assets and it scales well for lots of large files. It recognizes binary files the ones that can't be diffed and stores them as full file copies in the back end. It has P4V (cross-platform visual browser) and a thumbnail system so image files can be seen in the browser.  @lomaxx TortoiseSVN includes a program called TortoiseIDiff which looks to be a diff for images. I haven't used it but looks intriguing."
554,A,"Simple graphic display or display widget for Java We're giving a demo in about a week of a program which calculates a couple of moving averages. It would be much cooler if we could display these averages graphically like perfmon instead of printing to the console. I'm looking for the fastest easiest way to get this dynamic line-graph display onto a GUI ) the less programming required the better. In an ideal world it would somehow receive printed output in a certain format and generate the dynamic graph based on that. Barring that a very simple widget free or otherwise that can be implemented in a Java GUI. Keeping in mind I don't have much experience in Swing and always find it to be somewhat non-user friendly. Also what are the possibilities of a browser-based solution? JFreeChart is what you are looking for. It does include a MovingAverage class and you have here a ""Moving Average"" Demo I took the liberty to add a few links and illustration. Feel free to revert if you find those additions inappropriate. Thanks the additions are more than appropriate. Exactly what I was looking for. Thanks I'll give it a try. VonC strikes again!"
555,A,"Polygon Triangulation with Holes I am looking for an algorithm or library (better) to break down a polygon into triangles. I will be using these triangles in a Direct3D application. What are the best available options? Here is what I have found so far: Ben Discoe's notes FIST: Fast Industrial-Strength Triangulation of Polygons I know that CGAL provides triangulation but am not sure if it supports holes. I would really appreciate some opinions from people with prior experience in this area. Edit: This is a 2D polygon. Do you need 2D (triangles) or 3D (tetrahedra)? This is a 2D polygon You can add the holes relatively easily yourself. Basically triangulate to the convex hull of the input points as per CGAL and then delete any triangle whose incentre lies inside any of the hole polygons (or outside any of the external boundaries). When dealing with lots of holes in a large dataset masking techniques may be used to significantly speed this process up. edit: A common extension to this technique is to weed weak triangles on the hull where the longest edge or smallest internal angle exceeds a given value. This will form a better concave hull. This approach would not work: you need to use a constrained triangulation otherwise you may encounter triangles that are partially inside and partially outside a hole. @camille - a triangulated polygon with holes is always constrained. The polygon edges and holes are by definition contraints. If a triangle edge crossed a hole the hole would be partially covered. If it crossed a polygon edge the TIN would not be a triangulation of that polygon.  To give you some more choices of libraries out there: Polyboolean. I never tried this one but it looks promising: http://www.complex-a5.ru/polyboolean/index.html General Polygon Clipper. This one works very well in practice and does triangulation as well as clipping and holes holes: http://www.cs.man.ac.uk/~toby/alan/software/ My personal recommendation: Use the tesselation from the GLU (OpenGL Utility Library). The code is rock solid faster than GPC and generates less triangles. You don't need an initialized OpenGL-Handle or anything like this to use the lib. If you don't like the idea to include OpenGL system libs in a DirectX application there is a solution as well: Just download the SGI OpenGL reference implementation code and lift the triangulator from it. It just uses the OpenGL-Typedef names and a hand full of enums. That's it. You can extract the code and make a stand alone lib in an hour or two. In general my advice would be to use something that alreay works and don't start to write your own triangulation. It is tempting to roll your own if you have read about the ear-clipping or sweep-line algorithm but fact is that computational geometry algorithms are incredible hard to write in a way that they work stable never crash and always return a meaningful result. Numerical roundoff errors will accumulate and kill you in the end. I wrote a triangulation algorithm in C for the company I work with. Getting the core algorithm working took two days. Getting it working with all kinds of degenerated inputs took another two years (I wasn't working fulltime on it but trust me - I spent more time on it than I should have). Wrote all my own TIN stuff as well and agree 100% about the many degenerate cases. Wouldn't ever move from my own libs for this reason Some of the newer CG books out there are excellent though.  I have found the poly2tri library to be exactly what I needed for triangulation. It produces a much cleaner mesh than other libraries I've tried (including libtess) and it does support holes as well. It's been converted to a bunch of languages. The license is New BSD so you can use it in any project. Poly2tri library on Google Code  try libtess2 https://code.google.com/p/libtess2/downloads/list based on the original SGI GLU tesselator (with liberal licensing). Solves some memory management issues around lots of small mallocs.  This is a common problem in finite element analysis. It's called ""automatic mesh generation"". Google found this site with links to commercial and open source software. They usually presume some kind of CAD representation of the geometry to start.  CGAL has the tool you need: Constrained Triangulations You can simply provide boundaries of your polygon (incuding the boundaries of the holes) as constraints (the best would be that you insert all vertices and then specify the constraints as pairs of Vertex_handles). You can then tag the triangles of the triangulation by any traversal algorithm: start with a triangle incident to the infinite vertex and tag it as being outside and each time you cross a constraint switch to the opposite tag (inside if you were previously tagging the triangles as outsider outside if you were tagging triangles as insider before). Its a good enough solution for simple cases. Where you have overlapping holes and holes within holes it falls over. I prefer to have explicit internal and external boundaries. In case you have overlapping holes you should indeed retain the list of holes you have already entered into (instead of just an inside/outside tag). Apart from that it is exactly the same. ""each time you cross a constraint""? How do I figure that out? Do I take each triangle and test it with every available constraint? Is there a quicker way to do this? This is also said in the FAQ of CGAL: http://www.cgal.org/FAQ.html#polygon_triangulation  Another option (with a very flexible license) is to port the algorithm from VTK: vtkDelaunay2D This algorithm works fairly well. Using it directly is possible but requires links to VTK which may have more overhead than you want (although it has many other nice features as well). It supports constraints (holes/boundaries/etc) as well as triangulating a surface that isn't necessarily in the XY plane. It also supports some features I haven't seen elsewhere (see the notes on Alpha values).  CGAL definitly does not support holes it always triangulates the complex hull of the polygon (complete triangulation) You can always decompose them into a set of convex polygons and triangulate them. (Depending on how you get/create your polygons it might be an easy or hard task) How do I decompose them into a set of convex polygons? Did you mean manually? CGAL triangulates the convex hull but it DEFINITELY DOES support internal constraints (such as the boundaries of holes).  Jonathan Shewchuk's Triangle library is phenomenal; I've used it for automating triangulation in the past. You can ask it to attempt to avoid small/narrow triangles etc. so you come up with ""good"" triangulations instead of just any triangulation. I can vouch that Triangle is indeed a great tool. It also won the prestigious ""J. H. Wilkinson Prize for Numerical Software"" given out only once every 4 years. One of the biggest advantages here is that Triangle makes it very easy to construct separate vertex and index buffers of the triangulation. Love it! Changing the selected answer to this one since I have actually got this to work. @Jason The site says ""may not be sold or included in commercial products without a license"". So... it might be possible to obtain a license for commercial use. @agnel-kurian Triangle cannot be used in a commercial application BTW and even meshes generated with it are supposed to include acknowledgements."
556,A,What typeset programming languages exist? Mathematica's notebook front end has provided a hugely productive next-generation development environment for over 13 years particularly suited to technical computing but also more widely applicable. What other languages have IDEs that allow code to be typeset and have inline graphics? Well you are really describing Mathematica as far as I know. For typesetting folks there is still CWEB by Donald Knuth which fits a lot better into your standard latex book. And Self allows in-line graphics. Not quite as inline as Mathematica does but close enough: Self.  I believe Charles Simonyi's Intentional Programming system was intended so support all kinds of domain-specific notations. Typeset mathematical expressions would have been one example of a compatible notation.  Not quite what you're after but Knuth's Literate Programming as implemented in CWEB is at least related.
557,A,C# Drawing Arc with 3 Points I need to draw an arc using GraphicsPath and having initial median and final points. The arc has to pass on them. I tried .DrawCurve and .DrawBezier but the result isn't exactly an arc. What can I do? SOLUTION: After a couple of hours of code writing I managed to draw what I wanted with this algorithm (give 3 Point abc and a GraphicsPath path): double d = 2 * (a.X - c.X) * (c.Y - b.Y) + 2 * (b.X - c.X) * (a.Y - c.Y); double m1 = (Math.Pow(a.X 2) - Math.Pow(c.X 2) + Math.Pow(a.Y 2) - Math.Pow(c.Y 2)); double m2 = (Math.Pow(c.X 2) - Math.Pow(b.X 2) + Math.Pow(c.Y 2) - Math.Pow(b.Y 2)); double nx = m1 * (c.Y - b.Y) + m2 * (c.Y - a.Y); double ny = m1 * (b.X - c.X) + m2 * (a.X - c.X); double cx = nx / d; double cy = ny / d; double dx = cx - a.X; double dy = cy - a.Y; double distance = Math.Sqrt(dx * dx + dy * dy); Vector va = new Vector(a.X - cx a.Y - cy); Vector vb = new Vector(b.X - cx b.Y - cy); Vector vc = new Vector(c.X - cx c.Y - cy); Vector xaxis = new Vector(1 0); float startAngle = (float)Vector.AngleBetween(xaxis va); float sweepAngle = (float)(Vector.AngleBetween(va vb) + Vector.AngleBetween(vb vc)); path.AddArc( (float)(cx - distance) (float)(cy - distance) (float)(distance * 2) (float)(distance * 2) startAngle sweepAngle); do you mean a circular arc or will any arc do? circular I forgot to add that Have you tried the DrawArc method and seeing if u can manipulate your 3 points somehow? maybe Pen blackPen= new Pen(Color.Black 3); // Create rectangle to bound ellipse. Rectangle rect = new Rectangle(initial x initial y final x median y); // Create start and sweep angles on ellipse. float startAngle = 0F; float sweepAngle = 270.0F; // Draw arc to screen. e.Graphics.DrawArc(blackPen rect startAngle sweepAngle); http://msdn.microsoft.com/en-us/library/system.drawing.graphics.drawarc%28VS.71%29.aspx I don't have the angles and I don't know how to calculate them and the points can be in any place (they are exported from AutoMap 3D in FDO inside Oracle)  I would use DrawArc() as suggested by ANC_Michael. To find an arc that passes through 3 points you want to calculate the circumcircle of the triangle formed by the points. Once you have the circumcircle calculate a bounding box for the circle to use with DrawArc using the min/max dimensions (center +/- radius). Now calculate your start and stop angles by translating the points so that the circumcircle is centered on the origin (translate by -circumcenter) and take the dot-product of the normalized start and end vectors with the X-axis: double startAngle = Math.Acos(VectorToLeftPoint.Dot(XAxis)); double stopAngle = Math.Acos(VectorToRightPoint.Dot(XAxis)); Note that DrawArc expects angles clockwise from the X-axis so you should add Math.PI if the calculated vector is above the x-axis. That should be enough information to call DrawArc(). Edit: This method will find a circular arc and not necessarily the 'best fit' arc depending on your expected endpoint behavior.
558,A,"What are Vertex and Pixel shaders? What are Vertex and Pixel shaders? What is the difference between them? Which one is the best? possible duplicate of [Vertex shader vs Fragment Shader](http://stackoverflow.com/questions/4421261/vertex-shader-vs-fragment-shader) A Pixel Shader is a GPU (Graphic Processing Unit) component that can be programmed to operate on a per pixel basis and take care of stuff like lighting and bump mapping. A Vertex Shader is also GPU component and is also programmed using a specific assembly-like language like pixel shaders but are oriented to the scene geometry and can do things like adding cartoony silhouette edges to objects etc. Neither is better than the other they each have their specific uses. Most modern graphic cards supporting DirectX 9 or better include these capabilities. There are multiple resources on the web for gaining a better understand of how to use these things. NVidia and ATI especially are good resources for documents on this topic. Shaders are also written in higher level (non assembly) languages like Cg HLSL and GLSL.  DirectX 10 and OpenGL 3 introduced the Geometry Shader as a third type. In rendering pipeline order - Vertex Shader - Takes a single point and can adjust it. Can be used to work out complex **vertex lighting calcs as a setup for the next stage and/or warp the points around (wobble scale etc). each resulting primitive gets passed to the Geometry Shader - Takes each transformed primitive (triangle etc) and can perform calculations on it. This can add new points take them away or move them as required. This can be used to add or remove levels of detail dynamically from a single base mesh create mathematical meshes based on a point (for complex particle systems) and other similar tasks. each resulting primitive gets scanline converted and each pixel the span covers gets passed through the Pixel Shader (Fragment Shader in OpenGL) - Calculates the colour of a pixel on the screen based on what the vertex shader passes in bound textures and user-added data. This cannot read the current screen at all just work out what colour/transparency that pixel should be for the current primitive. those pixels then get put on the current draw buffer (screen backbuffer render-to-texture whatever) All shaders can access global data such as the world view matrix and the developer can pass in simple variables for them to use for lighting or any other purpose. Shaders are processed in an assembler-like language but modern DirectX and OpenGL versions have built in high-level c-like language compilers built in called HLSL and GLSL respectively. NVidia also have a shader compiler called CG that works on both APIs. [edited to reflect the incorrect order I had before (Geometry->Vertex->Pixel) as noted in a comment.] There are now 3 new shaders used in DirectX 11 for tessellation. The new complete shader order is Vertex->Hull->Tessellation->Domain->Geometry->Pixel. I haven't used these new ones yet so don't feel qualified to describe them accurately. Your rendering pipeline is wrong. The vertices are first processed in the vertex shader and then the primitives they make up go through the geometry shader. I wonder why nobody else has complained about this wrong order yet. The tessellation shader consists of *Hull shader* and *Domain shader*. The in-between *Tessellator* stage is not programmable. So there are only two additional shaders. The GL equivalents would be *Tessellation Control* and *Tessellation Evaluation*.  In terms of development a Pixel shader is a small program that operates on each pixel individually similarly a Vertex shader operates on each vertex individually. These can be used to create special effects shadows lighting etc... Since each Pixel/Vertex is operated on individually these shaders lend themselves to the highly parallel architecture of modern graphics processors.  Vertex and Pixel shaders provide different functions within the graphics pipeline. Vertex shaders take and process vertex-related data (positions normals texcoords). Pixel (or more accurately Fragment) shaders take values interpolated from those processed in the Vertex shader and generate pixel fragments. Most of the ""cool"" stuff is done in pixel shaders. This is where things like texture lookup and lighting take place.  DirectX Specific: Shader: Set of programs which implements addition graphical features to the objects that are not defined in the fixed rendering pipeline. Because of this we can have our own graphical effects according to our needs - ie. We are no longer limited to predefined “fixed” operations. HLSL: (High-Level Shading Language): HLSL is a programming language like C++ which is used to implement shaders (Pixel Shaders / Vertex Shaders). Vertex Shaders: A vertex shader is a program executed on the graphics card’s GPU which operates on each vertex individually. This facilitates we can write our own custom algorithm to work with the vertex's. Pixel Shaders: A pixel shader is a program executed on the graphics card’s GPU during the rasterization process for each pixel. It gives us a facility to access/manipulate individual pixels directly . This direct access to pixels allows us to achieve a variety of special effects such as multitexturing per pixel lighting depth of field cloud simula-tion fire simulation and sophisticated shadowing techniques. Note: Both Vertex Shaders and Pixel Shaders (programs) should be compiled using specific version of compiler before use. Compilation can be done just like a calling an API with required parameters like file name main entry function etc."
559,A,Turning off antialiasing in Löve2D I'm using Löve2D for writing a small game. Löve2D is an open source game engine for Lua. The problem I'm encountering is that some antialias filter is automatically applied to your sprites when you draw it at non-integer positions. love.graphics.draw( sprite x y ) So when x or y is not round (for example x=100.24) the sprite appears blurred. The same happens when the sprite size is not even because (xy) points to the center of the sprite. For example a sprite which is 31x30 big will appear blurred again because its pixels are painted in non-integer positions. Since I am using pixel art I want to avoid this all the way otherwise the art is destroyed by this effect. The workaround I am using so far is to force the coordinates to be round by littering the code with calls to math.floor() and forcing all the sprites to have even sizes by adding a row or column of transparent pixels with the paint program if needed. Is there some command to deactivate the antialiasing I can call at program startup? Another possible work around may be to use math.floor() to round your integers as a cheap workaround.  In case anyone is interested I've been asking in other places and found out that what I am asking is already requested as feature: http://love2d.org/forum/tracker.php?p=2&t=7 So the current version of Löve that I'm using (0.5.0) still doesn't allow to disable the antialias filter but the feature is already in the SVN version of the engine. I would consider marking this answer (or one of the others) as the correct one so that it's more visible to people who visit the question :)  If you turn off anti-aliasing you will just get aliasing hence the name! Why are you drawing at non-integral positions and what do you want it to do about those fractional parts? (Round them to the nearest value? Truncate them? What about if they're negative?) Personally I would leave the low level graphics alone and alter your code to use accessors for x and y that perform the rounding or truncation that you require. This guarantees your pixel art ends up drawn on integer boundaries while keeping the anti-aliasing on that you might need later.
560,A,"line drawing routine How to optimize this line drawing routine ? Will memcpy work faster ? void ScreenDriver::HorizontalLine(int wXStart int wXEnd int wYPos COLORVAL Color int wWidth) { int iLen = wXEnd - wXStart + 1; if (iLen <= 0) { return; } while(wWidth-- > 0) { COLORVAL *Put = mpScanPointers[wYPos] + wXStart; int iLen1 = iLen; while(iLen1--) { *Put++ = Color; } wYPos++; } } Some additional information: - COLORVAL -> uint16_t - platform -> IMX31 ARM Your best bet before doing anything else is to employ whatever low-level profiling tools you have available. At the very least get an overall timing for a hefty test case or 3. Without a baseline measurement you're shooting in the dark. (I should know I'm as guilty of this as anyone else!) That said I note that your code looks like it has a fair bit of overhead per pixel A memset() call could be a win (if COLORVAL is sizeof(char) ). Alternately unrolling the loop may help - this is heavily dependent on you input data machine architecture etc. If your iLen value is reasonably bounded you might consider writing a custom function for each iLen value that is fully unrolled (inline the first few smallish cases in a switch) and call the bigger cases through an array of function pointers. The fastest option of course is usually to resort to assembly.  You could try unrolling the inner loop but really it's only going to matter for lines close to horizontal. For lines that are not close to horizontal it could be you spend more time setting up the table of scan pointers. Frankly for more realistic situations where you have not only colors but widths line-styles and end-styles not to mention drawing modes like XOR and aliasing the way I've seen it done is each ""line"" is really a polygon-fill for which there are pretty fast algorithms (which is actually what your algorithm is) and/or a special-purpose machine-language routine is generated on-the-fly (stored on the stack) because there are too many options to have option-specific special routines and you don't want the algorithm continually questioning pixel-by-pixel what the options are.  I've found through personal experience that memcpy is slightly faster than direct pointer access... but only slightly it isn't usually a ground-breaking optimization.  I think you mean to say ""memset"" instead of ""memcpy"". Replacing this bit of the code: while (iLen--) { *Put++ = Color; } with memset(Put Color iLen); could be faster but so much depends on your target CPU memory architecture and the typical values of iLen encountered. It's not likely to be a big win but if you've got the time I encourage you to measure the alternatives as that kind of exercise is the only way to really understand optimization. Of course this memset() use will only work if COLORVAL is character sized.  No not really. memcpy copies memory that's a read and a write and you don't need the read. memset which only writes only writes bytes so that isn't going to work either unless COLORVAL is also a byte. No leave it as is the compiler should produce a fairly good bit of code. Don't forget that you are probably limited by memory bandwidth.  One of the fastest ways to draw a horizontal line aka fill an array with a value in assembly is to use the stosb stosw stosd instructions. memset is optimized to use stosb. To use dword values we can write code like the one below to draw a line __asm { cld mov eax color mov ecx screen_width mov edi video_buffer rep stosd } But I'm quite sure that your inner while loop will be optimized by the compiler to use the stosd anyway."
561,A,Student Project Ideas: Parallel Computing I've been given a free choice of final project for my software development course I'm quite interested in attempting a distributed programming task My initial thought was to create a simple photon scattering renderer but I don't think I'd get far past rendering platonic solids and metaballs. Any suggestions or interesting areas I might want to explore? Go with your interests and what you know best. When ever I'm attempting a new programming algorithm or data structure or design I try to work it into the framework of a Multi User Dungeon. I've been doing Multi User Dungeon design since I began programming and it's what got me into programming. I know the domain backwards and forward and can instantly think of a use for most algorithms or data structures in that framework. It allows me to focus only on the problem at hand and not on the side problems. If you're very familiar with graphic programming do that. If you're not pick some other domain that you know backwards and forwards and try to find some part of it that would benefit from being distributed. Then use that.  A simple suggestion is to take any NP complete problem with a known good solution and parallelize it to produce at least a single better answer than has been previously documented. But personally I would choose something I am interested in like finding a more accurate equilibrium strategy for poker.
562,A,"PCL raster color graphics distorted. Code included: what am I missing? The function below prints a color raster image to a PCL-5 printer. The function was adapted from a 2-color (1bpp) printing function we had that worked perfectly except for the grainy 2-color printing. The problem is that the image comes out with a large black bar extending from the right of the image to the edge of the page like this:  IMAGE######################################### IMAGE#########AREA COMPLETELY BLACK########### IMAGE######################################### The image itself looks perfect otherwise. Various PCL-to-PDF tools don't show the image at all which leads me to believe I've forgotten do to something. Appropriate resets (\u001bE\u001b%-12345X) were sent before and page-feeds after. Any PCL experts out there? I've got the PCL 5 Color Technical Reference Manual and it's gotten me this far. This last thing is driving me crazy though. *Edit: I now know what command is causing the problem but I don't know why:  stream(""\u001b*r0F""); This should keep the image rotated along with the page (portrait landscape). If I remove this the problem goes away. I can compensate by rotating the bitmap beforehand but I really want to know what caused this!  static void PrintImage() { // Get an image into memory Image original = Image.FromFile(""c:\\temp\\test.jpg""); Bitmap newBitmap = new Bitmap(original original.Width original.Height); stream(String.Format(""\u001b*p{0:d}x*p{1:d}Y"" 1000 1000));// Set cursor. stream(""\u001b*t300R""); // 300 DPI stream(String.Format(""\u001b*r{0:d}T"" original.Height)); // Height stream(String.Format(""\u001b*r{0:d}S"" original.Width)); // Width stream(""\u001b*r3U""); // 8-bit color palette stream(""\u001b*r0F""); // Follow logical page layout (landscape portrait etc..) // Set palette depth 3 bytes per pixel RGB stream(""\u001b*v6W\u0000\u0003\u0000\u0008\u0008\u0008""); stream(""\u001b*r1A""); // Start raster graphics stream(""\u001b*b0M""); // Compression 0 = None 1 = Run Length Encoding // Not fast but fast enough. List<byte> colors = new List<byte>(); for (int y2 = 0; y2 < original.Height; y2++) { colors.Clear(); for (int x2 = 0; x2 < original.Width; x2++) { Color c = newBitmap.GetPixel(x2 y2); colors.Add(c.R); colors.Add(c.G); colors.Add(c.B); } stream(String.Format(""\u001b*b{0}W"" colors.Count)); // Length of data to send streamBytes(colors.ToArray()); // Binary data } stream(""\u001b*rB""); // End raster graphics (also tried *rC -- no effect) } There are a few problems with your code. First off your cursor position code is incorrect it should read: ""\u001b*p{0:d}x1:d}Y"" 1000 1000 This equates to: <esc>*p1000x1000Y you had: <esc>*p1000x*p1000Y When joining PCL commands together you match up the same parameterized value and group and then simply add value + parametrized character + value + parametrized character etc. Ensure that the final parametrized character is a capital letter which signifies the end of the PCL command. Also when defining an image I recommend you also specify the width & hight in decipoints this should help with the scaling of the image (*r3A) on the page so add this (just after your resolution command should be an okay place for it): Int32 deciHeight = original.Height / (int)original.HorizontalResolution * 720; Int32 deciWidth = original.Width / (int)original.VerticalResolution * 720; stream(""\u001b*t{0:d}h{1:d}V"" deciHeight deciWidth)); The other recommendation is to write all of this to file (watch your encodings) and use one of the handful of PCL viewers to view your data vs. always printing it. Should save you some time and a forest or two! I've tried all of them and wold recommend spending the $89 and purchasing pclWorks. They also have a complete SDK if you are going to do a lot of PCL. We don't use that as we hardcode all PCL ourselves but it does look good. As for rotation we've had problems on some device You could just rotate the the jpg first (original.RotateFlip) an then write it out. I don't have much time today but hope that my comments assist. I can test your code on Monday or Tuesday and work with it and post any further comments. Keep in mind that even though PCL is a standard its support from manufacturer to manufacturer and device to device can be a problem and vastly different. When doing basic things most devices seem okay; however if you get into macros or complex graphics you will find difference. The positioning was a cut-and-paste error (yours has a typo too). I decided to forget the PCL rotation and just rotate myself. Solved all of my problems. We do use a couple of PCL viewers here and none showed the problem only the printer itself. I'm still puzzled but have working code."
563,A,"Draw text with a stroke onto an image in .Net I am currently using the System.Drawing.Graphics.DrawString() method in .Net to draw text on top of an image and then save it to a new file: // define image file paths string sourceImagePath = HttpContext.Current.Server.MapPath(""~/img/sourceImage.jpg""); string destinationImagePath = HttpContext.Current.Server.MapPath(""~/img/"") + ""finalImage.jpg""; // create new graphic to draw to Bitmap bm = new Bitmap(200 200); Graphics gr = Graphics.FromImage(bm); // open and draw image into new graphic System.Drawing.Image sourceImage = System.Drawing.Image.FromFile(sourceImagePath true); gr.DrawImage(sourceImage 0 0); sourceImage.Dispose(); // write ""my text"" on center of image gr.SmoothingMode = System.Drawing.Drawing2D.SmoothingMode.AntiAlias; StringFormat sf = new StringFormat(); sf.Alignment = StringAlignment.Center; PrivateFontCollection fontCollection = new PrivateFontCollection(); fontCollection.AddFontFile(HttpContext.Current.Server.MapPath(""~/fonts/HelveticaNeueLTStd-BlkCn.ttf"")); // prototype (1) gr.DrawString(""my text"" new Font(fontCollection.Families[0] 14 FontStyle.Bold) new SolidBrush(Color.FromArgb(255 255 255)) 100 0 sf); fontCollection.Dispose(); // save new image if (File.Exists(destinationImagePath)) { File.Delete(destinationImagePath); } bm.Save(destinationImagePath); bm.Dispose(); gr.Dispose(); This method of adding text on an image does not provide a way (that I know of) to add a stroke to the text. For example what I really need to do is add a 2px stroke of a certain color to the text that is written on the image. How can this be done with the .Net Framework <= v3.5? Not sure if this is exactly what you're looking for but try using a LinearGradientBrush instead of a SolidBrush in your code. I'm not sure what you mean by ""stroke"" in this question but if you want say all pixels on rows 7 and 8 to be red you can use a System.Drawing.TextureBrush instead of a SolidBrush. You would create a Bitmap one pixel wide and tall enough for your font (all black except for rows 7 and 8 which would be red) and then use the constructor for TextureBrush that takes this Bitmap as a parameter.  All text has strokes. If by stroke you mean outline then you create a GraphicsPath and add the string using AddString you can then outline the path using DrawPath rather than FillPath as used in the example on msdn.  private void Form1_Paint(object sender PaintEventArgs e) { // Create a GraphicsPath object. GraphicsPath myPath = new GraphicsPath(); // Set up all the string parameters. string stringText = ""Sample Text""; FontFamily family = new FontFamily(""Arial""); int fontStyle = (int)FontStyle.Italic; int emSize = 96; Point origin = new Point(20 20); StringFormat format = StringFormat.GenericDefault; // Add the string to the path. myPath.AddString(stringText family fontStyle emSize origin format); //Draw the path to the screen. e.Graphics.SmoothingMode = SmoothingMode.AntiAlias; e.Graphics.FillPath(Brushes.BlanchedAlmond myPath); e.Graphics.DrawPath(new Pen(Brushes.Azure 2) myPath); } If by stroke you mean strike then use the Strikeout font style. If by stroke you mean serif then use a different font. Works like a charm! One thing I chose to do is to do the DrawPath call first and then the FillPath. This way the Drawn path doesn't cover the actual filled path and is more of a stroke. Thanks!  A bit of a hack but you could layer two sets of text one atop the other with the lower layer scaled up just slightly. Might not work perfectly for all fonts but it's worth a try."
564,A,OpenGL in my HWND Right now I'm trying to port a Direct3D renderer from my engine. I'm and OpenGL begginer so i dont have much knowlegde about OpenGL as now i can create windows and do my render via glut but i can't use glut for my project because the HWND is created in my code and then sent to the renderer DLL // Where pWindow is already a valid HWND target of the renderer //(Currently Direct3D9 and Direct3D10 pRenderer = pCreateGraphics(800 600 false pWindow); My question is: Is there library similar to GLUT that has a similar behaviour as GLUT but allowing my to use my own window handle? Note: I prefer using a lib instead of reinventing the wheel but i will do if there is not a library that can help me SDL lets you get the window handle. I don't know if the used pixel format can be assumed to be DirectX compatible though.  Well you can have a look at GLFW's source code or NeHe tutorials and grab the OpenGL init code. I'll have a look at GLFW's about NeHe i forgot to add that to my post but I prefer using a lib instead of reinventing the wheel thanks ok well GLFW doesn't allow you to pass your own HWND in fact but having your own opengl context init code isn't much code Maybe not but what i try to achieve is gonna be hard tough xD thanks
565,A,"Using Java's Graphics or Graphics2D classes how do I paint a String? I have a String and I want to paint it onto an image. I am able to paint points and draw lines however even after reading the Text part of the 2D Graphics tutorial I can't figure out how I can take a String and paint it onto my drawing. Unless I'm looking at the wrong tutorial (but it's the one I get whenever I search for anything about Java and painting Strings using Graphics or Graphics2D) I'm still stumped. if you want to play with the shape of your string (eg: fill:red and stroke:blue): Graphics2D yourGraphicsContext=(...); Font f= new Font(""Dialog""Font.PLAIN14); FontRenderContext frc = yourGraphicsContext.getFontRenderContext(); TextLayout tl = new TextLayout(e.getTextContent() f frc); Shape shape= tl.getOutline(null); //here you can move your shape with AffineTransform (...) yourGraphicsContext.setColor(Color.RED); yourGraphicsContext.fill(shape); yourGraphicsContext.setColor(Color.BLUE); yourGraphicsContext.draw(shape);  Check out the following method. g.drawString(); The drawString() method will do what you need. An example use: protected void paintComponent(Graphics g){ g.setColor(Color.BLACK); g.drawString(5 40 ""Hello World!""); } Just remember the coordinates are regarding the bottom left of the String you are drawing. Thanks. Why was there no mention of this in the tutorial that I read? I learned a lot about fonts and stuff though... No idea. It is a pretty basic thing to do in Swing. that's a strange tutorial that seems to contain nothing :D here 1.4's javadoc for Graphics2D it's a much better tutorial than the one you were looking at :D http://java.sun.com/j2se/1.4.2/docs/api/java/awt/Graphics2D.html That wasn't supposed to be a tutorial. It is just a link to the Javadocs for Graphics which has `drawString()` in it."
566,A,"Is it possible to achieve MAX(AsAd) openGL blending? I am working on a game where I want to create shadows under a series of sprites on a grid. The shadows are larger than the sprites themselves and the sprites are animated (i.e. move and rotate). I cannot simply render them into the sprite png or the shadows will overlap adjacent sprites. I also cannot simply put shadows on a lower layer by themselves because when they overlap they will create dark bands at their intersection. These sprites are animated so it is not feasible to render these en masse. Basically I want the sprites' shadows to blend together such that they max out at a set opacity. Example: I believe this is equivalent to an openGL blending of (RsGsBsMax(AsDs)) where I don't really care about RG and B as it will always be the same color in src and dst. However this is not a valid openGL blending mode. Is there an easy way to accomplish this especially in cocos2d-iphone? I would be able to approximate this by making the shadow sprites opaque then applying them both to a parent sprite and making the parent sprite 40% opacity. However the way cocos2d works this only sets the opacity of each child to 40% rather than the combined sprite image which results in the same stripe. OK I think after much research I realize the error in my thinking. First there is a way to do Max Alpha openGL blending and that is using glBlendEquations. You would have to modify the draw method to do this (and make some new attribute for a textureNode to switch between these methods) and use either of these:  glBlendEquationOES( GL_MAX_EXT ); glBlendEquationSeparateOES( GL_FUNC_ADD_OES GL_MAX_EXT ); The first uses max() for RGBA while the second uses standard addition for RGB and max() for alpha. Now the problem is that each sprite and it's children in z-order are drawn to the screen. This means that after the first shadow is drawn it is part of the main screen buffer. In other words its opacity is changed and with max() blending and a standard opaque background it's now painted onto the background so when you draw the next shadow it will do max(As 1) which is 1 because the background is opaque. So I don't achieve the blending I want. It would ""work"" if my background was transparent but then I could not apply any background ""behind"" that we can only add to the buffer. I could draw the shadows first using the max() blend and then do a {ONE_MINUS_DST_ALPHA DST_ALPHA} glBlend. This is an OK workaround but I have other issues that make this difficult. Now the real solution I finally realize is to use a RenderTexture to render the shadows to a separate buffer before applying them to the background. This could have performance implications so we'll see where this goes. Update: OK I have a working solution now. Oddly I ended up not needing this solution after all because in my particular case the banding was not noticeable enough to warrant the effort however this is the solution I came up with that seemed to do what I wanted: First draw a 0% black alpha png to the entire screen. (I don't know if this is necessary. I don't know what the default alpha of the screen is). Draw the shadows to the screen using glEquationSeperateOES( GL_ADD GL_MAX_ENT) and glBlend( GL_ONE GL_ONE ). This will blend all of the shadows together as described taking the max() of the alphas. Because you are compositing to black it is equivalent to use glBlendEquationOES( GL_MAX_ENT ). (X+0 == max(X0)) glBlend( GL_ZERO GL_ONE ) will eliminate the need of step 1 or at least the requirement of that layer to be 0% black. GL_ZERO essentially forces it to be 0% black. Draw the floor which will be over the shadows but use glBlend( ONE_MINUS_DST_ALPHA DST_ALPHA ). This results in exactly the same results as if you added the shadows to the floor with glBlend( SRC_ALPHA ONE_MINUS_SRC_ALPHA ) however if you did it this way you would not be able to blend the shadows together (read why at top of answer). You are done! The nice thing about this method is that the shadow sprites can be animated as usual without having to call ""visit"" on a separate renderTexture. The rest: I also modified CocosNode to allow me to add a shadow to a layer which links to another CocosNode. This way the shadow is rendered as a child of the floor (or the 0% black background sprite for the shadow blending) but is linked to another CocosNode. When a sprite moves if it has a shadow it updates the shadows position as well. This allows me to have all shadows below all objects on the screen and to have the shadows automatically follow the object. Sorry for the long answer. Maybe my solution is kludgy but it seems to work great. Thanks for following up on your question with an informative answer! thanks also! This information is practically nowhere else. :)"
567,A,"Java Image Editor which renders output as source code? Alex explained what I'm looking for much better than I have: You want an existing program that allows you to draw a picture captures what you do as you draw and writes each action as a Java command. When you click the ""Drawl Oval"" tool and click at 00 and then at 5050 it would generate the line g.drawOval(005050). If anybody knows of a program such as this let me know. Thanks. Original question: I've been working with Java and custom drawing using the java.awt.Graphics library lately but find it is taking too much time to write manually. Is there any simple graphics editor (like mspaint) which generates source code? Example: Drawing this: Would generate: public void update(Graphics g) { g.translate(0 0); g.drawOval(0 0 50 50); } Thanks. Can you be a bit more specific? Adobe Illustrator generates EPS files. Since PostScript is a programming language then you can say that Illustrator generates source code. I understand the principle I just didn't get the specific example he gave. I believe that he wants an app like XamlPadX for WPF ie that he draws a shape (maybe in illustrator?) and then that shape gets translated into code. A SVG to Java2D converter is nice but not what I'm looking for. I'm still a bit baffled as to why the SVG to Java2D transcoder doesn't supply what you need. Its not real time but it should be really easy to write a batch file to generate it for you. I didn't understand your example. Your drawing shows a circle with two tangents at right angles but you want that to generate commands to draw a rectangle. How come? While not what you were looking for I should mention that XPM (X Pixmap) format is basically a subset of C programming language. XPM2 simplified it more by removing the trappings of C syntax. XPM3 brought them back again. In a sense XPM image converters are source code generators and translators. You are looking for something similar to output Java AWT but for many real images or photographs it would be complicated to do analysis on the image to find ovaletc and create the code for drawing them with lines and shapes (well unless the image had filters applied to simplify it or was an SVG as someone pointed out). It would probably have to convert to a bitmap of some form and keep it in an array in the generated Java source.  If they are vectors you could use an SVG Editor (eg Inkscape) along with Kirill's SVG to Java2D Transcoder to simplify this. It isn't perfect but Kirill is very responsive in responding to requests for enhancement.  It's unclear what you are asking. Two guesses: You want an existing program that allows you to draw a picture captures what you do as you draw and writes each action as a Java command. When you click the ""Drawl Oval"" tool and click at 00 and then at 5050 it would generate the line g.drawOval(005050). I do not know of any such tool. But the above might help you reword your question so that others can share their knowledge. You want a program that takes an existing bitmap and converts it into a series of commands that will replicate the bitmap. Other than simply outputting pixels such a tool is nearly impossible to write; attempting to decompose an arbitrary picture into simple drawing commands is very hard. In this case I would recommend simply importing the bitmap as a JPG PNG whatever and using drawImage() instead of using Graphics calls."
568,A,"Drawing lines in win32/GDI with a custom pen style? I have a need to do some drawing using win32/GDI (Native not .NET) and I've run into the following problem: I need to draw lines that are ""styled."" For example in the attached image the line marked ""A"" is a straight line as far as my application data is concerned it just needs to be drawn with the additional zigzag as a style. Of course this is easy to do programatically but it gets more complicated when the line can be at any angle (""B"") or even a bezier curve (""C""). Now I could do this all programatically painstakingly doing the math to put a zigzag around each line possibility but that will take a lot of time and more importantly be rather error prone. Is it possible to just give windows/GDI a ""style"" to apply to the line perhaps a bitmap like the one marked ""D"" and have it use that as a pen to draw the lines? If not is there a more flexible and less error-prone way of doing this than writing a bunch of specific drawing code for each of the ""styled"" lines? *Apparently first timers can't post images. Examples can be found at http://i.imgur.com/IC0T2.png There's nothing that'll do this automatically. You'll have to write some code. You might want to look at the LineDDA API in GDI. It might simplify the math your code will need.  This is not possible in Win32 GDI. You will need to do the math yourself. It should be noted however that you can obtain the points used to make up the line or curve which should make it substantially easier. See this ""Hit-Testing"" tutorial for an example. http://msdn.microsoft.com/en-us/library/ms969920.aspx For a bezier curve you would use Path Functions: BeginPath PolyBezier EndPath FlattenPath For straight lines you could use: LineDDA  ExtCreatePen() maybe? I don't know for a fact if it supports zigzagging...  As far as I know there's nothing in GDI or even GDI+ that would support this. The only line options you have are dash-patterns compound-pens dash caps end caps and fill brushes. You might be able to trick one of those functions into drawing something vaguely akin to your wiggles for straight splines but it definitely won't work for curved splines. It shouldn't be too hard to do this however. Sure it will take a day or so but all you have to do is write a line and bezier interpolator divide the curves into equal segments find the tangents at all those segments and alternate left and right. You'll end up with an array of points which can be drawn very quickly as a polyline."
569,A,"When is a Swing component 'displayable'? Is there a way (e.g. via an event?) to determine when a Swing component becomes 'displayable' -- as per the Javadocs for Component.getGraphics? The reason I'm trying to do this is so that I can then call getGraphics() and pass that to my 'rendering strategy' for the component. I've tried adding a ComponentListener but componentShown doesn't seem to get called. Is there anything else I can try? Thanks. And additionally - is it ok to keep hold of the Graphics object I receive? Or is there potential for a new one to be created later in the lifetime of the Component? (e.g. after it is resized/hidden?) Don't save the Graphics object. It could change for many reasons. You need to check up the component hierarchy. Check after AncestorListener.ancestorAdded is called.  I've always used Coomponent.addNotify to know when the component is ready to be rendered.Not sure if is the the best way but it works for me. Of course you must subclass the component. Component.isDisplayable should be the right answer but I know it didn't worked for me as I thought it will(I don't remember why but there was something and I switched to addNotify). Looking in the SUN's source code I can see addNotify fires a HierarchyEvent.SHOWING_CHANGED so this is the best way to be notified.  You can listen for a resize event. When a component is first displayed it is resized from 00 to whatever the layout manager determines (if it has one).  Add a HierarchyListener public class MyShowingListener { private JComponent component; public MyShowingListener(JComponent jc) { component=jc; } public void hierarchyChanged(HierarchyEvent e) { if((e.getChangeFlags() & HierarchyEvent.SHOWING_CHANGED)>0 && component.isShowing()) { System.out.println(""Showing""); } } } JTable t = new JTable(...); t.addHierarchyListener(new MyShowingListener(t)); This is definitely the best answer for my question although I think I'll go with wilums2's resizing suggestion as that's an event I'm going to be listening for anyway. Thanks."
570,A,"Algorithm to blend gradient filled corners in image I need to put an alpha blended gradient border around an image. My problem is in blending the corners so they are smooth where the horizontal and vertical gradients meet. I believe there is a standard algorithm that solves this problem. I think I even encountered it in school many years ago. But I have been unsuccessful in finding any reference to one in several web searches. (I have implemented a radial fill pattern in the corner but the transition is still not smooth enough.) My questions: If there is a standard algorithm for this problem what is the name of it and even better how is it implemented? Forgoing any standard algorithm what's the best way to determine the desired pixel value to produce a smooth gradient in the corners? (Make a smooth transition from the vertical gradient to the horizontal gradient.) EDIT: So imagine I have an image I will insert on top of a larger image. The larger image is solid black and the smaller image is solid white. Before I insert it I want to blend the smaller image into the larger one by setting the alpha value on the smaller image to create a transparent ""border"" around it so it ""fades"" into the larger image. Done correctly I should have a smooth gradient from black to white and I do everywhere except the corners and the inside edge. At the edge of the gradient border near the center of the image the value would be 255 (not transparent). As the border approaches the outside edge the alpha value approaches 0. In the corners of the image where the vert & horiz borders meet you end up with what amounts to a diagonal line. I want to eliminate that line and have a smooth transition. What I need is an algorithm that determines the alpha value (0 - 255) for each pixel that overlaps in the corner of an image as the horizontal and vertical edges meet. If you don't need it to be resizable then you can just use a simple alpha map. However I once used a simple Gaussian fade with the mean at the location where I wanted it to be the last fully-opaque pixels to be. If that makes sense.  Presumably you're multiplying the two gradients where they overlap right? Dunno about a standard algorithm. But if you use a signoid shaped gradient instead of a linear one that should eliminate the visible edge where the two overlap. A simple sigmoid function is smoothstep(t) = t*t*(3 - 2*t) where 0 <= t <= 1 This looks promising. I'm not sure I want to mess with XNA (This is a C# application and I think XNA is the best (only?) way to get smoothstep) but I'm working up a test now with it and will decide if I want to use it or roll my own based on the results. Smoothstep is just the (very simple) function I described; it's been around for yonks and is not exclusive to XNA. I just did an implementation - while it makes the gradient edges nicer the corners have the same issue. The real problem is the optical illusion created by the gradients. There's no 'error' for either linear or smoothstep but you still 'see' a line unless you block the reset of the gradient from your vision."
571,A,"What percentage of Windows boxes have OpenGL support I've been thinking about starting a new graphics project and I want to use Java. Java has wrappers for all of the relevant GL functionality but I wonder how many people including casual users actually have decent GL drivers installed. By decent I mean somewhat stable and fairly new (GL 1.5 support would probably do although the GLSL support that comes with 2.0 would be great). I could DirectX even with Java but I pretty much hate it and this project is supposed to be 'fun'. Also I like the at least near-cross-platformedness of GL. So anyone know of any non-imaginary stats on what percentage of Windows users have the drivers to run a GL app? I'd say that all Windows users have some support for OpenGL. The latest versions usually are reserved for Vista users gamers and developers.  Windows XP comes with OpenGL 1.1 (quite slow though). Windows Vista also comes with OpenGL 1.1 (but for some special applications it has OpenGL 1.4 emulator on top of D3D). When you install a graphics driver on Windows it installs a more OpenGL version. OpenGL 1.5 is roughly ""DX9 shader model 2.0"" capable hardware. How many machines have that kind of hardware depends on your target market. In traditional/hardcore games space almost all will (see Steam Hardware Survey). In more casual/small games space quite a lot of machines have much older hardware (see Unity Hardware Stats - almost 30% in 2009 Q1 are older than ""DX9 shader model 2.0""). Also a lot of machines in that space do not have custom drivers; they use whatever display drivers are shipped in Windows (which do not provide anything more than GL 1.1). Again see Unity Hardware Stats - the most popular driver versions are the ones that come with Windows. Stability wise I'd strongly suggest using D3D9 on Windows instead of OpenGL. Driver quality is much better for D3D9 (less crashes inside drivers less incorrect rendering better performance ...).  The Steam Hardware Survey is probably the best and most detailed source for info about what gamers have. Accurate statistics for the general population will be harder to come by. Instead you should look at this in terms of how recent you want the graphics hardware. For example any ATI chip from the R300 series (Radeon 9550+) onward supports OpenGL 2.0. On the NVidia side any GeForce 6000+ series chip will support OpenGL 2.0 and their predecessors the FX series almost supported OpenGL 2.0. The R300 series and the FX series were both introduced in 2002 so if you know what portion of your target market is using a PC from 2003 or later you'll have a fairly good idea of how widespread OpenGL 2.0 support is among users with discrete graphics. If you want to support integrated graphics (which are the largest segment of the market but aren't particularly common with those who are serious about graphics of any kind) your users will need at least a GMA X3000 for hardware acceleration of OpenGL 2.0 features which means their system has to be from 2006 or later. If you're interested in support on other operating systems any Intel Mac will support OpenGL 2.0 with software fallbacks and hardware acceleration whenever the chip would support it under Windows. On Linux any system with Mesa 7 or later (June 2007 or later) will support OpenGL 2.0 software rendering. Hardware acceleration is less reliable but there are decent open-source drivers for ATI chips from R300 and newer. Ill add that in order to have OpenGL 2.x+ you need to have installed gpu drivers! So if box owner did not installed gpu drivers you are stuck to OpenGL 1.1 (via aware MS decision to not put any better OpenGL in Windows). As of Jully 2011 Steam Surveys claims 75% of computers running Windows as capable of OpenGL3+ <10% have OpenGL 4+ but how much of them have proper gpu drivers?  Every modern day video card supports OpenGL ... Shouldnt you be questioning how many Windows boxes have the Java runtime? Good point I think that more users have OpenGL compatible card than Java runtime as well. Hahaha - made me laugh. Good point though! I was concerned about that until it reached 94% of non-mobile (dont know just windows) users in 2007. I also have no problem requiring people install it (some won't... mostly developers because we can never be reasoned with!). At this point its pretty stale FUD (read: business opportunity). Stability is a bigger concern. The GL problem on the other hand is real but its also going away with the sputtering power of Microsoft. I have more of a problem requiring a driver install because you usually have to restart the computer or at least the browser unlike with Java. Still I laughed :-)  As I recall Windows XP comes with 1.1 support out of the box. Vista upgrades this to 1.5. So you can at least count on those as an absolute minimum. Apart from that the GPU drivers from pretty much any vendor gives you at least 2.0 support. But if I were you I'd reconsider DirectX. I don't know what you hate about it but it does have some advantages. The tool support is vastly better (ie there are tools available. And PIX is nothing short of amazing) the API is up to date and well designed rather than accumulated over 20 years of a committee working at cross purposes and if this is limited to Windows anyway cross-platform doesn't really matter. (On the other hand of course if you do need cross-platform capabilities it doesn't really matter what else DirectX can offer it won't deliver that one killer feature)  Nearly everybody has SOME form of OpenGL support. Experience has shown that the actual drivers involved can be quite poor when dealing with ATI and especially Intel hardware but it will at least work aforementioned bugs notwithstanding. If nothing else Windows can fall back to its built in 1.1 (XP and earlier) or 1.4 (Vista and higher) implementation. It won't work well but it will work."
572,A,Public domain web template/graphics for developers? This is an issue for web developers that aren't or don't have artists. We would really like some nice templates that we can modify for use on Intranet sites. So does anyone have suggestions for public domain web template/graphics for developers? I believe this has been asked before and got closed... but I didn't agree that the original should have been closed Twitter's Bootstrap. Excellent CSS for an Apple-y look. Grid responsive design buttons and now even icons. Not quite public domain or am I missing something? Looks Apache 2 licensed.  Andreas Viklund has great templates that he released into the public domain. http://andreasviklund.com/templates/  As a designer as a well as a developer i in unabashed self interest would say just commission some. Give people some work :-) Then you can reuse and modify them going forward.  Not quite public domain but if you can cope with a small author's attribution on the web pages take a look at http://www.freecsstemplates.org. They're licensed under Creative Commons so you need to leave the attribution in the template but the attribution is generally quite discreet so most people won't notice it.
573,A,"Constructing colours for maximum contrast I want to draw some items on screen each item is in one of N sets. The number of sets changes all the time so I need to calculate N different colours which are as different as possible (to make it easy to identify what is in which set). So for example with N = 2 my results would be black and white. With three I guess I would get all red all green all blue. For all four it's less obvious what the correct answer is and this is where I'm having trouble. EDIT:: The obvious approach is to map 0 to Red 1 to green and all the colours in between to the appropriate rainbow colours then you can get a colour for set N by doing GetRainbowColour(N / TotalSets) so a GetRainbowColour method is all that's need to solve this problem You can read up on the HSL and HSV color models in this wikipedia article. The ""H"" in the acronymns stands for Hue and that is the rainbow you want. It sounds like you want saturation to max out. The article also explains how to convert to RGB color. Looks like a similar question has been asked before here.  The answer to this question is subjective - what is best contrast to someone with full vision is not necessarily best contrast to someone who is colour blind or has limited vision or someone with normal eyesight who is operating the equipment in a dark environment. Physiologically humans have much better resolution for intensity that for hue or saturation. That is why analogue TV digital video and photo compression throw away colour information to reduce bandwidth (4:2:2) - if you put two pixels which are different intensities together it doesn't matter what colour they are - we simply can only sense colour differences on large areas of like intensity. So if the thing you are trying to display has lots of small areas of only a few pixels or you want it to be used in the dark (in the dark the brain boosts the blue cells and we don't see colour as well) or by the 10% of the male population who are colour blind consider using intensity as the main differentiating factor rather than hue. GetRainbowColour would ignore the most important dimension of the human visual sense but can work quite well for continuous data. I didn't know that thanks. However the UI is a debugging UI so I know that none of the people who use it are colour blind. I will however bear this in mind for the future."
574,A,"Windowsforms: How to draw lines/bars on a DataGridView? I'm using a DataGridView in a Windows application (.NET 3.5) showing some colored bars (basically ""tasks in time""): What I need now is to show a custom graphical ""completed"" bar on the cells depending on a percentage value. Here is a photoshopped image: Any hint how I could approach the problem or a creative solution? Thanks! Edit: I had to redraw the cell becuase it gets ""lost."" Here is the (VB.NET) code that works: Private Sub DataGridView_CellPainting(ByVal sender As Object ByVal e As System.Windows.Forms.DataGridViewCellPaintingEventArgs) Handles DataGridView.CellPainting If e.ColumnIndex < FIRST_DATA_COLUMN OrElse e.RowIndex < 0 Then Return If e.Value Is Nothing Then Return Dim BarValue As Integer = DirectCast(e.Value Integer) If BarValue = 0 Then Return Dim BackColorBrush As New SolidBrush(e.CellStyle.BackColor) Dim GridBrush As New SolidBrush(Me.DataGridView.GridColor) Dim GridLinePen As New Pen(GridBrush) ' -- Erase the cell e.Graphics.FillRectangle(BackColorBrush e.CellBounds) ' -- Draw the grid lines (only the right and bottom lines; DataGridView takes care of the others) e.Graphics.DrawLine(GridLinePen e.CellBounds.Left e.CellBounds.Bottom - 1 e.CellBounds.Right - 1 e.CellBounds.Bottom - 1) e.Graphics.DrawLine(GridLinePen e.CellBounds.Right - 1 e.CellBounds.Top e.CellBounds.Right - 1 e.CellBounds.Bottom - 1) ' -- Paint progress bar Dim ProgressBarBrush As New SolidBrush(Color.Green) Dim CellProgressBarRect As New Rectangle(e.CellBounds.X e.CellBounds.Y + CELL_HEIGHT - PROGRESS_BAR_HEIGHT BarValue PROGRESS_BAR_HEIGHT) e.Graphics.FillRectangle(ProgressBarBrush CellProgressBarRect) e.Handled = True End Sub You will have to do a Custom Draw on the Cell. Have a look at the Cell Painting event. Yes I thought I had to do that. A question: if I do a custom draw hooking into cell painting event I ""lose"" the normal layout and have to redraw the cell? The DataGridView is one of the custom draw controls that more or less does it right. The DataGridViewCellPaintingEventArgs has a couple of functions you can call to have WinForms paint certain parts of the cell for you. You could then paint just the background yourself. I don't think you do I think as Eric stated the args allow you to simply override specific properties of the Cell. It works thanks. See my updated question. Glad I could help. You just need to add some logic in the there now to handle the calculation of the percentage to draw. The logic of the percentage is done in another method. It's the cell value. Ah well your done then!"
575,A,Smooth animations using Win32 API - without controlling the message pump I'm currently trying to integrate some animation drawing code of mine into a third party application under the form of an external plugin. This animation code in realtime 3d based on OpenGL and is supposed to render as fast as it can usually at 60 frames per second. In my base application where I'm the king of the world I control the application message pump so that drawing occurs whenever possible. Like that : for (;;) { if (PeekMessage(&msg NULL 0 0 PM_REMOVE)) { do { if (msg.message == WM_QUIT) break; TranslateMessage(&msg); DispatchMessage(&msg); } while (PeekMessage(&msg NULL 0 0 PM_REMOVE)); } draw(); } Now that I'm no more king in the world I have to play nice with the application messages so that it keeps being responsive. To my knowledge as I'm a plugin I can't hijack the whole application message pump ; so I tried various things doing my drawing in WM_PAINT message handler : Use WM_TIMER which doesn't work :I don't know in advance which time step I need (often not fixed) and the timing in not accurate. Call InvalidateRect as soon as I'm done drawing doesn't work : completely prevents the rest of the application of being responsive and doing its own refreshing. Create a 'worker' thread whose only job is to post a user message to the plugin window. This message is posted as soon as the drawing is finished (signaled by an event). The user message handler in turn calls InvalidateRect (see there). So far my last attempt is the better and sometimes work fine. DWORD WINAPI PaintCommandThreadProc(LPVOID lpParameter) { Plugin* plugin = static_cast<Plugin*>(lpParameter); HANDLE updateEvent = plugin->updateEvent(); while (updateEvent == plugin->updateEvent()) { ::WaitForSingleObject(updateEvent 100); ::Sleep(0); if (updateEvent == plugin->updateEvent()) { ::PostMessage(plugin->hwnd() WM_USER+0x10 0 0); } } return 0; } ... LRESULT CALLBACK PluginWinProc(HWND hWnd UINT msg WPARAM wParam LPARAM lParam) { bool processDefault = true; LRESULT result = 0; Plugin* plugin = reinterpret_cast<Plugin*>( GetWindowLong(hWnd GWL_USERDATA) ); switch (msg) { ... case WM_GL_MESSAGE: { ::InvalidateRect( hWnd NULL FALSE ); processDefault = false; result = TRUE; } break; case WM_PAINT: { draw(hWnd); ::SetEvent( plugin->updateEvent() ); processDefault = false; result = TRUE; } break; ... } if (processDefault && plugin && plugin->m_wndOldProc) result = ::CallWindowProc(plugin->m_wndOldProc hWnd msg wParam lParam); return result; } On some occasions the host application still seems to miss messages. The main characteristics of the problem are that I have to press the 'Alt' key for modal dialogs to show up ; and I have to move the mouse to give some processing time to the host application !... Is there any 'industry standard' solution for this kind of as-often-as-you-can animation repaint problem ? Each thread has its own message queue and messages sent to a window arrive in the queue of the thread that created the window. If you create your plugin window yourself you can create it in a separate thread and that way you will have complete control over its message pump. An alternative solution (which imho is better) is to only have OpenGL rendering in a separate thread. All OpenGL calls must occur in the thread that created the OpenGL context. However you can create a window in one thread (your application main thread) but create the OpenGL context in another thread. That way the original application message pumps stays intact and in your rendering thread you can loop forever doing rendering (with calls to SwapBuffers to vsync). The main problem with that second solution is that communication between the plugin WindowProc and the rendering loop must take into account threading (ie. use locks when accessing shared memory). However since the message pump is separate from rendering it can be simultaneous and your message handling is as responsive as it can get. Agreed I can't think of any other way to have the plugin render as fast as possible without adapting the host application. Thanks for this suggestion ! It looks like a very good idea ! I'll give it try ASAP - I never thought that OpenGL rendering could be entirely done in an autonomous thread.
576,A,"Performance-wise: A lot of small PNGs or one large PNG? Developing a simple game for the iPhone what gives a better performance? Using a lot of small (10x10 to 30x30 pixels) PNGs for my UIViews' backgrounds. Using one large PNG and clipping to the bounds of my UIViews. My thought is that the first technique requires less memory per individual UIView but complicates how the iPhone handles the large amount of images as it tries to combine the images into a larger texture or tries to switch between all the small textures a lot. The second technique on the other hand gives the iPhone the opportunity to handle just one large PNG but unnessicarily increases the image weight every UIView has to carry. Am I right about the iPhone's attempts handling the images the way I described it? So what is the way to go? Seeing the answers thus far there is still doubt. There seems to be a trade-off with two parameters: Complexity and CPU-intensive coding. What would be my tipping point for deciding what technique to use? Can the small images be reused e.g. tiling? There is no tiling required but I do reuse a lot. (Think puzzle game with loads of tiles) One large image mainly gives you more work and more headaches. It's harder to maintain but is probably less ram intensive because there is only one structure + data in memory instead of many structures + data. (though probably not enough to notice). Looking at the contents of .app bundles on regular Mac OS it seems the generally approved method of storage is one file/resource per image. Ofcourse this is assuming you're not getting the image resources from the web where the bottleneck would be in http and its specified maximum of two concurrent requests.  If you end up referring back to the same CGImageRef (for example by sharing a UIImage *) the image won't be loaded multiple times by the different views. This is the technique used by the videowall Core Animation demo at the WWDC 07 keynote. That's OSX code but UIViews are very similar to CALayers. The way Core Graphics handles images (from my observation anyway) is heavily tweaked for just-in-time loading and aggressive releasing when memory is tight. Using a large image you could end up loading the image at draw time if the memory for the decoded image that CGImageRef points to has been reclaimed by the system. What makes a difference is not how many images you have but how often the UIKit traverses your code. Both UIViews and Core Animation CALayers will only repaint if you ask them to (-setNeedsDisplay) and the bottleneck usually is your code plus transferring the rendered content into a texture for the graphics chip. So my advice is to think your UIView layout in a way that allows portions that change together to be updated all at the same time which turn into a single texture upload. this falls into the ""great answer"" category Is that what the iPhone does? Turning piles of images into a single large texture? Does that mean the ""large texture"" can change over time? You can think of every UIView as its own texture (in OpenGL terms) yes. That's why moving/fading/scaling UIViews is fast (hardware accelerated) but modifying the contents of the UIViews is slow/expensive (CPU bound). I see. Great help duncan! Thanks!  I would say there is no authoritative answer to this question. A single large image cuts down (slow) flash access and gets the decode done in one go but a lot of smaller images give you better control over what processing happens when... but it's memory hungry and you do have to slice that image up or mask it. You will have to implement one solution and test it. If it isn't fast enough and you can't optimise implement the other. I suggest implementing the version which you personally find easier to imagine implementing because that will be easiest to implement.  One large gives you better performance. (Of cause if you should render all pictures anyway). Another advantage could be smaller size the large PNG will perhaps find some similarities between smaller pictures. To maximize that try to put similar images next to each other. The reason why is what I implied? I'm curious because every UIView I assign that large PNG to has to somehow take that extra heavy weight with it? actually one large image would require more code to slice at runtime and thus add cpu cycles. the added cpu cycles will likely be more costly than just loading smaller images. Also a large image containing multiple smaller images requires a lot more maintenance in both code and graphics. I don't think clipping an image is very computationally expensive on the iPhone. IIRC drawing a clipped image is actually cheaper than drawing the whole image. Also for most apps it seems like the iPhone is more memory-starved than it is CPU-starved and the large image is more RAM-efficient.  One large image will remove any overhead associated with opening and manipulating many images in memory."
577,A,The Need To Restore Graphics Original State When Overwritten paint or paintComponent I realize most of the Java code to overwritten paint or paintComponent most of them doesn't restore the old state of graphics object after they had change the state of graphics object. For example setStroke setRenderingHint... I was wondering whether it is a good practice that we restore back the old state of graphics object before returning from the method. For example public void paintComponent(Graphics g) { super.paintComponet(g); Stroke oldStroke = g.getStroke(); g.setStroke(newStroke); // Do drawing operation. g.setStroke(oldStroke); } Is this a good practice? Or it is over done? You should not alter the Graphics object passed in at all rather perform all your graphics operations on a copy of it which you then dispose. There'll be no need to reset the state at all then. public void paintComponent(Graphics g1) { super.paintComponent(g1); final Graphics2D g = (Graphics2D)g1.create(); try { // ...Whole lotta drawing code... } finally { g.dispose(); } } I'm going to support this answer with the JavaDoc for JComponent that agrees: http://docs.oracle.com/javase/6/docs/api/javax/swing/JComponent.html#paintComponent%28java.awt.Graphics%29  Yes this is a very good practice to follow. You don't pay much in performance (relative to the actual painting operation) and you save yourself a mess of grief if you're making unusual changes to the graphics context. Don't overdo it though -- you probably don't need to worry about color settings for example. The alternative is to assume nothing about the graphics context and set all the necessary properties before every painting in case they're set to something wonky. Try to avoid freely creating and disposing Graphics objects for every operation. Specific properties you should always restore if modified: (because they can do Bad Things and have Unexpected Consequences): Transform - because modifications to this will stack on top of each other and get very very hard to reset. Beware: this is modified by the translate shear scale rotate and transform methods of Graphics2D. Modifying transforms should be used with CAUTION. Stroke -- because (at least in my configuration) leaving this default runs much faster than any setting even if equivalent to default. Don't ask -- it's a result of the Java2D graphics pipelines accelerating the default case using graphics hardware. Clip: will result in weird bugs where only part of the screen draws. Composite: most operations probably don't expect this to be something weird. Properties to not worry about: RenderingHints. These are things you can easy set and restore and generally you want to leave them set a certain way (antialiasing on etc) for the whole time the app is running. Changing RenderingHints will rarely break rendering of components although it might make it uglier. Background color and paint color. Most things will modify these before drawing anyway. Font: likewise. You should find that creating a Graphics object is extremely cheap. Er... yeah corrected. I still think it's not a good practice though because you never know what else is using a Graphics object and you lose your settings for RenderingHints etc.
578,A,Is it possible to use AnimateWindow with AW_BLEND when using a layered window? I am displaying a window using UpdateLayeredWindow and would like to add transition animations. AnimateWindow works if I use the slide or roll effects (though there is some flickering). However when I try to use AW_BLEND to produce a fade effect I not only lose any translucency after the animation (per-pixel and on the entire image) but a default window border also appears. Is there a way to prevent the border from appearing? The only way I've found to successfully fade up/down a window (without the complications you're describing) is by first creating a window with the WS_EX_LAYERED extended style. I then start a timer (30ms) that gradually fades the window by calling something like: SetLayeredWindowAttributes(0 (BYTE)(m_nAnimationCount * WINDOW_ALPHA) LWA_ALPHA); where WINDOW_ALPHA is 23 (seems to look the best) and m_nAnimationCount is a count from 0 to 10 (or 10 to 0 if fading down). If you discover a better way of doing this I'd be interested to find out. UpdateLayeredWindow and SetLayeredWindowAttributes don't really get along :-\  Since I'm using UpdateLayeredWindow SetLayeredWindowAttributes will not work. The diagram here was pretty useful. Instead I just need to call UpdateLayeredWindow in a loop while decreasing the SourceConstantAlpha member of the BLENDFUNCTION structure. In fact the pointer to the BLENDFUNCTION structure the handle to the layered window and the flags are the only things I needed to pass into UpdateLayeredWindow if the alpha is all that is changing.
579,A,How does a GUI Framework work? I have been all over the web looking for an answer to this and my question is this: How does a GUI framework work? for instance how does Qt work is there any books or wibsites on the topic of writing a GUI framework from scratch? and also does the framework have to call methods from the operating systems GUI framework? -- Thank you to any one who takes the time to try to answer this question and forgive me if i misspelled anything. Building a GUI framework isn't a 123 process. All I can say is take a look of some of those open source IDEs like Netbeans source code for example. Look inside the code and then build the whole IDE. I haven't used NetBeans but I looked at a few screenshots and it appears to just use Swing - I don't think this would give a good example of developing a UI framework since it simply uses an existing one not implements its own. GTK+ (http://www.gtk.org/) would be a good example. Hay Thanks for the advice and your prompt response. I will do that.  A GUI framework like Qt generally works by taking the existing OS's primitive objects (windows fonts bitmaps etc) wrapping them in more platform-neutral and less clunky classes/structures/handles and giving you the functionality you'll need to manipulate them. Yes that almost always involves using the OS's own functions but it doesn't HAVE to -- if you're designing an API to draw an OpenGL UI for example most of the underlying OS's GUI stuff won't even work and you'll be doing just about everything on your own. Either way it's not for the faint of heart. If you have to ask how a GUI framework works you're not even close to ready to design one. You're better off sticking with an existing framework and extending it to do the spiffy stuff it doesn't do already. Hay thank you for your response things are starting to make more sense. And I would have to agree with you on that im probably not yet ready to develop one yet but hopefully someday I will be at that level of understanding. But thanks for your answer it helps.
580,A,"Advice on how to be graphically creative I've always felt that my graphic design skills have lacked but I do have a desire to improve them. Even though I'm not the worlds worst artist it's discouraging to see the results from a professional designer who can do an amazing mockup from a simple spec in just a few hours. I always wonder how they came up with their design and more importantly how they executed it so quickly. I'd like to think that all good artists aren't naturally gifted. I'm guessing that a lot of skill/talent comes from just putting in the time. Is there a recommended path to right brain nirvana for someone starting from scratch a little later in life? I'd be interested in book recommendations personal theories or anything else that may shed some light on the best path to take. I have questions like should I read books about color theory should I draw any chance I have should I analyze shapes like an architect etc... As far as my current skills go I can make my way around Photoshop enough where I can do simple image manipulation... Thanks for any advice This question is not related to programming. Drawing is probably what I'd recommend the most. Whenever you have a chance just start drawing. Keep in mind that what you draw doesn't have to be original; it's a perfectly natural learning tool to try and duplicate someone else's work. You'll learn a lot. If you look at all the great masters they had understudies who actually did part of their masters' works so fight that ""it must be original"" instinct that school's instilled in you and get duplicating. (Just make sure you either destroy or properly label these attempts as copies--you don't want to accidentally use them later and then be accused of plagiarism..) I have a couple of friends in the animation sector and one of them told me that while she was going through college the way she was taught to draw the human body was to go through each body part and draw it 100 times each in a completely different pose. This gets you comfortable with the make-up of the object and helps you get intimately knowledgeable about how it'll look from various positions. (That may not apply directly to what you're doing but it should give you an indicator as to the amount of discipline that may be involved in getting to the point you seek.) Definitely put together a library of stuff that you can look to for inspiration. Value physical media that you can flip through over websites; it's much quicker to flip through a picture book than it is to search your bookmarks online. When it comes to getting your imagination fired up having to meticulously click and wait repeatedly is going to be counter-productive.  I too was not born with a strong design skillset in fact quite the opposite. When I started out my philosophy was that if the page or form just works then my job was done! Over the years though I've improved. Although I believe I'll never be as good as someone who was born with the skills sites like CSS Zen Garden among others have helped me a lot. Read into usability too as I think usability and design for computer applications are inextricably entwined. Books such as Don Norman's ""The Design of Everyday Things"" to Steve Krug's ""Don't Make Me Think"" have all helped improve my 'design skills'... slightly! ;-) Good luck with it.  As I mentioned in a thread yesterday I have found working through tutorials for Adobe Photoshop Illustrator InDesign and After Effects to be very helpful. I use Adobe's Kuler site for help with colors. I think that designers spend a lot of time looking at other's designs. Some of the books out there on web site design might help even for designing applications. Adobe TV has a lot of short videos on graphic design in general as well as achieving particular results in one of their tools. I find these videos quite helpful.  That's a difficult thing. Usually people think ""artistic skills"" come from your genes but actually they do not. The bests graphic designer I know have some sort of education in arts. Of course photoshop knowledge will allow you to do things but being interested in art (painting specially) will improve your sensitivity and your ""good taste"". Painting is a pleasure both doing it and seeing it. Learning to both understand and enjoy it will help and the better way to do it is by going to museums. I try to go to as much expositions as I can as well as read what I can on authors and styles (Piccaso Monet Dali Magritte Expresionism Impresionism Cubism etc) that will give you a general overview that WILL help. On the other side... you are a programmer so you shouldn't be in charge of actually drawing the icons or designing the enterprise logo. You should however be familiarized with user interface design specially with ease of use and terms as goal oriented design. Of course in a sufficiently large company you won't be in charge of the UI design either but it will help anyway. I'd recommend the book About Face which centers in goal oriented design as well as going through some user interface methapores and giving some historic background for the matter.  I'm no artist and I'm colorblind but I have been able to do fairly well with track creation for Motocross Madness and other games of that type (http://twisteddirt.com & http://dirttwister.com). Besides being familiar with the toolset I believe it helps to bring out your inner artist. I found that the book ""Drawing on the Right Side of the Brain"" was an amazing eye opening experience for me. One of the tricks that it uses is for you to draw a fairly complicated picture while looking at the picture upside down. If I had drawn it while looking at it right side up it would have looked horrible. I impressed myself with what I was able to draw while copying it while it was upside down. I did this many years ago. I just looked at their website and I think I will order the updated book and check out their DVD.  Inspiration is probably your biggest asset. Like creative writing and even programming looking at what people have done and how they have done will give you tools to put in your toolbox. But in the sense of graphic design (photoshop illustrator etc) just like programmers don't enjoy reinventing the wheel I don't think artwork is any different. Search the web for 'pieces' that you can manipulate (vector graphics: example). Run through tutorials that can easily give you some tricks. Sketch out a very rough idea and look through web images to find something that has already created. It's like anything else that you wish to master or become proficient in. If you want it you've got to practice it over and over and over.  I have a BFA in Graphic Design although I don't use it much lately. Here's my $.02. Get a copy of ""Drawing on the Right Side of the Brain"" and go through it. You will become a better artist/drawer as a result and I'm a firm believer that if you can't do it with pencil/paper you won't be successful on the computer. Also go to the bookstore and pick up a copy of How or one of the other publications. I maintain a subscription to How just for inspiration. I'll see if I can dig up some web links tonight for resources (although I'm sure others will provide some). most importantly carry a sketch book and use it. Draw. Draw. Draw.  Most of artistic talent comes from putting in the time. However as in most skills practicing bad habits doesn't help you progress. You need to learn basic drawing skills (form mainly) and practice doing them well and right (which means slowly). As you practice correctly you'll improve much faster. This is the kind of thing that changes you from a person who says ""It doesn't look right but I can't tell why - it's just 'off' somehow"" to a person who says ""Oops the arm is a bit long. If I shorten the elbow end it'll change the piece in this way if I shorten the hand end it'll change the piece this way..."" So you've got to study the forms you intend to draw and recognize their internally related parts (the body height is generally X times the size of the head the arms and legs are related in size but vary from the torso etc). Same thing with buildings physical objects etc. Another thing that will really help you is understanding light and shadow - humans pick up on shape relationships based on outlines and based on shadows. Color theory is something that will make your designs attractive or evoke certain responses and emotions but until you get the form and lighting right the colors are not something you should stress. One reason why art books and classes focus so much on monochrome drawings. There are books and classes out there for these subjects - I could recommend some but what you really need is to look at them yourself and pick the ones that appeal to you. You won't want to learn if you don't like drawing fruit bowls and that's all your book does. Though you shouldn't avoid what you don't like given that you're going the self taught route you should make it easy in the beginning and then force yourself to draw the uninteresting and bland once you've got a bit of confidence and speed so you can go through those barriers more quickly. Good luck!"
581,A,"WPF video tearing on Windows XP I've created a WPF application using C# that renders about 50 3D elements and animates the camera in order to move around the scene. While it all works I've noticed as the camera moves ugly video tearing occurs. I'm well aware of the standard cause of tearing ie the application is updating frames at a different rate from the monitor/adapter vertical sync. I've also read that the default screen update rate in WPF is 60 frames per second and that on XP WPF updates without regard to the montior v-sync. For certain scenes the tearing is less noticeable but often it is distracting enough to be a real problem. I'm really hoping there is a resolution after all Windows Forms has had double buffering for years. Moving to WPF is actally a backward step for many types of application if there is no resolution. So on Windows XP how can I configure WPF to remove tearing? I'm looking for more helpful answers then ""move to Vista"". This is simply out of the question for many of the future users of this application. I believe Microsoft's response has been that it's fixed on Vista. So move to Vista. I've moved to Linux myself... No problems for me. I'm not sure all the paying customers are going to upgrade from XP to Vista just to run this little app ;) No answers yet surely with the number of WPF developers out there someone else has seen this problem? In the meantime I've investigated/read some more and have come across the Timeline.DesiredFrameRate property. With my display running at 50hz vertical refresh I tried setting this to 50 FPS in the hope this would brings things in sync. Unfortunately this had no effect on the tearing. However I did have to apply it to a single specific animation I created (in code) using BeginAnimation(). I'm not sure how this would affect a ""global"" framerate at all. Instead I would have expected to need to do this at a more global level say on the viewport however I couldn't find DesiredFramerate property on the viewport object. Another area to look at could be the RenderCapability.Tier but I haven't had the time to look in detail yet. [Update] No solution but a useful tip. To set the DesiredFramerate globally (ie for all animations in the viewport) you can override the PropertyMetadata for the Timeline.DesiredFrameRateProperty dependency property: // Set the global desired framerate to 50 frames per second Startup += delegate(object sender StartupEventArgs e) { Timeline.DesiredFrameRateProperty.OverrideMetadata(typeof(Timeline) new PropertyMetadata(50)); }; It's best to place this in the constructor of the standard WPF Application class. Unfortunately this didn't solve my problem because even setting it to FPS it must not be synced accurately enough with monitor v-sync. It is handy to know about though particularly if you have slow moving animations and want to save CPU usage by setting the FPS much lower then the default 60fps. [Update] Setting the DesireFrameRate to some very high value such as 100+ seems to reduce but not remove the flicker. One big drawback however is that the CPU (on my 4 yo PC) goes to 35%."
582,A,"How to make/generate a graph image depending on values in a database? How could I make an image graph from values in a database? The idea came from ""Steam"" which uses a graph to show how many users are online. How could I do the same thing? It seems like the graph is one whole image; not made up of parts. Here's an image of the graph from Steam: You will mainly need to keep a track of how many users were connected at each period of time. Drawing the graph is easy they are some really neat library to help you with it. ;)  I use FLOT with Jquery -> http://code.google.com/p/flot/ I find it easier to work with CSS than GD library to get the layout I want.  +1 for Svisstack's link. Here's another one (less supported less documented but more features): http://www.php.net/imagick I know about ImageMagick being used for scaling images and doing other image processing operations. But how does it help with creating an image from raw data? Yes it can be used to create images from scratch and draw on them.  There are many javascript-tools. For example you can choose jQuery's plugin - jqplot or Dojo charting etc... With those you can generate very good-looking charts/graphs.  The easiest way is using the Google Chart API. You can generate a graph by simply creating a url with your values (basic example)  Use this GD module from PHP. http://php.net/manual/en/book.image.php  if you are the javascript lover use this http://raphaeljs.com/ they keep updating and many advance function. Great find: this turned out to be the best solution for doing some very custom stuff. A bit low level and thus more challenging to work with still worked like a charm for my needs. The demo source code makes it fairly easy to just jump in.  You want to use JPGraph it is a very power PHP graphing library. http://www.aditus.nu/jpgraph/"
583,A,How to read and modify the colorspace of an image in c# I'm loading a Bitmap from a jpg file. If the image is not 24bit RGB I'd like to convert it. The conversion should be fairly fast. The images I'm loading are up to huge (9000*9000 pixel with a compressed size of 40-50MB). How can this be done? Btw: I don't want to use any external libraries if possible. But if you know of an open source utility class performing the most common imaging tasks I'd be happy to hear about it. Thanks in advance. Standard .NET System.Drawing namespace should have all that you need but it probably won't be very efficient. It'll load the whole thing into RAM uncompress it convert it (probably by making a copy) and then re-compress and save it. If you aim for high performance I'm afraid you might need to look into C/C++ libraries and make .NET wrappers for them.  This isn't something I've tried myself but I think you might need to acccess the picture's EXIF information as a start. Check out Scott Hanselman's blog-entry on accessing EXIF information from pictures. Exif-Data is just plain optional meta-data associated with an image. Modifying Exif-Data doesn't change anything on the image. But anyway thanks for your answer.  As far as I know jpg is always 24 bpp. The only thing that could change would be that it's CMY(K?) rather then RGB. That information would be stored in the header. Unfortunately I don't have any means of creating a CMYK image to test whether loading into a Bitmap will convert it automatically. The following line will read the file into memory: Bitmap image = Image.FromFile(fileName); image.PixelFormat will tell you the image format. However I can't test what the file load does with files other than 24bpp RGB jpgs. I can only recommend that you try it out. I believe that jpg can come in 24 and 8 bit depth which is a grayscale image then. Ah - didn't remember about the 8 bit depth but I should have done as I had to deal with texture images in all sorts of formats in a previous job. Again though won't the Bitmap load from file do the conversion for you? I don't know how to test whether it is 24 bit or not ;) I hoped to have asked this implicitly with my question: How do I detect whether the image is 24bit?  The jpeg should start with 0xFF 0xD8. After that you will find various fields in the format: Field identifier 2 bytes Field length excluding field identifier. 2 bytes. Variable data. Parse through the fields. The identifier you will be looking for is 0xFF 0xC0. This is called SOF0 and contains height width bit depth etc. 0xFF 0xC0 will be followed by two bytes for the field length. Immediately following that will be a single byte showing the bit depth which will usually be 8. Then there will be two bytes for height two for width and a single byte for the number of components; this will usually be 1 (for greyscale) or 3. (for color)
584,A,"Calculate the horizon of a curved face? - Not extrema I need to find the 2 points of the visual horizon of a curved face. I have: XYZ of the 4 corner points XYZ of the 2 curved edge bezier points And I need to calculate either: XY of the 2 horizon points XYZ of the 2 horizon points Note: I got a solution the last time I asked this question but it only found the extrema of the curves not the horizon points which changes based on the position and rotation of both curves in respect to each other. The known XYZ points are what is calculated AFTER projection. So don't both about that. Also the ""camera"" of the 3D world is at 000 XYZ pointing at a 000 XYZ angle. In other words the XYZ points are ready for calculation the ""camera"" does not need to be bothered about. Nice visual clue. Care to divulge which software was used to create it? What you're looking for is actually called a silhouette not a horizon. The most simple method of doing this is finding the boundary between the surface parts in which the normal is directed towards the camera (dot product is negative) and the surface parts in which the normal is directed away from the camera (dot product is positive). With a triangle mesh you can do this directly by using the normals. with NURBS you can probably find a closed formula which does this.  Does it work to first rotate the curves so that the connection between the corner points is horizontal and then calculating the extrema? To test it visually you can rotate your example image by about 150 degree: Note the extrema of this curve isn't exactly where you want it but this could be caused by several factors for example the way you marked the horizon points doesn't seem to be that exact.  You don't say how your surface is defined only that it is bounded by two quadratic Bézier curves. There are lots of ways to build such a surface and each way of building it would have a different horizon. So this answer is going to be guesswork. The horizon consists of those points on the surface where the vector from the camera to the point is tangent to the surface as shown here: A quadratic Bézier curve has parametric equation B(t) = (1 − t)2 P0 + 2(1 − t)t P1 + t2 P2 differentiating that with respect to t gives us the tangent to the curve: B′(t) = 2(t − 1) P0 + 2(1 − 2t) P1 + 2t P2 and this is parallel with the vector from the camera (at the origin) to the curve if B(t) × B′(t) = 0 Solve this for t and you'll have the point on the curve at the horizon. How you can extend this to the horizon for the whole surface depends on how your surface is constructed. (Maybe you can just find the horizon points for the curves at each end of the surface and join them with a straight line?)"
585,A,"How can I draw a bitmap rotated in wxPython? How do I draw a bitmap to a DC while rotating it by a specified angle? I agree with Al - he deserves the answer but this (admittedly untested) code fragment should do what you asked for: def write_bmp_to_dc_rotated( dc bitmap angle ): ''' Rotate a bitmap and write it to the supplied device context. ''' img = bitmap.ConvertToImage() img_centre = wx.Point( img.GetWidth()/2 img.GetHeight()/2 ) img = img.Rotate( angle img_centre ) dc.WriteBitmap( img.ConvertToBitmap() 0 0 ) One thing to note though from the docs: ...using wxImage is the preferred way to load images in wxWidgets with the exception of resources... Was there a particular reason to load it as a bitmap rather than a wx.Image? I need to draw it to the screen and the wx.Image doc says I'll need to convert it to bitmap before I could draw it. After a bit of fixing your code worked. Can you either post the fixed result yourself or suggest what you did to fix the above to improve the example? I ended up doing it with a GraphicsContext similar to what Anurag suggested.  I'm not sure that this is the best way of doing it but one option would be to convert it to a wx.Image with ConvertToImage (wxWidgets help) and then use the rotate function (wxWidgets help). You could then (if necessary) convert it back with ConvertToBitmap (wxWidgets help). I couldn't see an obvious function that could be used to apply a coordinate transform to the drawing context (DC) but there may be one in there somewhere... Hope that helps.  Better way would be to use Graphics context if you want a generic rotation e.g. rotate bitmap or text or any other drawing path gc = wx.GCDC(dc) gc.Rotate(angle) gc.DrawText(""anurag"" 100 100) which version and platform you are using? wxPython doc says it does have Rotate and I do use it heavily http://www.wxpython.org/docs/api/wx.GraphicsContext-class.html#Rotate I'm confused. Check this out: http://www.wxpython.org/docs/api/wx.GCDC-class.html What's the connection between GraphicsContext and GCDC? It sounded promising but GCDC doesn't seem to have a `Rotate` method!"
586,A,"BlackBerry - image 3D transform I know how to rotate image on any angle with drawTexturePath: int displayWidth = Display.getWidth(); int displayHeight = Display.getHeight(); int[] x = new int[] { 0 displayWidth displayWidth 0 }; int[] x = new int[] { 0 0 displayHeight displayHeight }; int angle = Fixed32.toFP( 45 ); int dux = Fixed32.cosd(angle ); int dvx = -Fixed32.sind( angle ); int duy = Fixed32.sind( angle ); int dvy = Fixed32.cosd( angle ); graphics.drawTexturedPath( x y null null 0 0 dvx dux dvy duy image); but what I need is a 3d projection of simple image with 3d transformation (something like this) Can you please advice me how to do this with drawTexturedPath (I'm almost sure it's possible)? Are there any alternatives? With the following code you can skew your image and get a perspective like effect:  int displayWidth = Display.getWidth(); int displayHeight = Display.getHeight(); int[] x = new int[] { 0 displayWidth displayWidth 0 }; int[] y = new int[] { 0 0 displayHeight displayHeight }; int dux = Fixed32.toFP(-1); int dvx = Fixed32.toFP(1); int duy = Fixed32.toFP(1); int dvy = Fixed32.toFP(0); graphics.drawTexturedPath( x y null null 0 0 dvx dux dvy duy image); This will skew your image in a 45º angle if you want a certain angle you just need to use some trigonometry to determine the lengths of your vectors. Hi Lucas thanks for you answer. What you provide is a simple skew. Frankly I need to be able do any combined 3d transformation (rotate skew resize). Take a closer look at http://stackoverflow.com/questions/660304/4-point-transform-images  its resized partly not just skewed. As I said there should be some way to do this because: 1) - they provide dvx dux dvy duy args which are behaves like some trans matrix; 2) there are undocumented setMatrix() and setIdentity() methods in net.rim.device.api.ui.Graphics class (unfortunatley can't make them work)  You want to do texture mapping and that function won't cut it. Maybe you can kludge your way around it but the better option is to use a texture mapping algorithm. This involves for each row of pixels determining the edges of the shape and where on the shape those screen pixels map to (the texture pixels). It's not so hard actually but may take a bit of work. And you'll be drawing the pic only once. GameDev has a bunch of articles with sourcecode here: http://www.gamedev.net/reference/list.asp?categoryid=40#212 Wikipedia also has a nice article: http://en.wikipedia.org/wiki/Texture_mapping Another site with 3d tutorials: http://tfpsly.free.fr/Docs/TomHammersley/index.html In your place I'd seek out a simple demo program that did something close to what you want and use their sources as base to develop my own - or even find a portable source library I´m sure there must be a few. Hi Kristoffon thanks for answer! As I understood I can paint color per pixel save it once in bitmap and draw that bitmap on each refresh. That is slow more than draw line per y but can be a solution as well. For proof of concept can you provide blackberry code for perspective transformation like in http://www.wholetomato.com/images/tour/mondoPerspectiveTrans.gif? Texture mapping by hand(so 1 pixel at a time) is way to slow for j2me. J2me is only 'fast' when it can use API functions to do the heavy lifting.  The method used by this function(2 walk vectors) is the same as the oldskool coding tricks used for the famous 'rotozoomer' effect. rotozoomer example video This method is a very fast way to rotate zoom and skew an image. The rotation is done simply by rotating the walk vectors. The zooming is done simply by scaling the walk vectors. The skewing is done by rotating the walkvectors in respect to one another (e.g. they don't make a 90 degree angle anymore). Nintendo had made hardware in their SNES to use the same effect on any of the sprites and or backgrounds. This made way for some very cool effects. One big shortcoming of this technique is that one can not perspectively warp a texture. To do this every new horizontal line the walk vectors should be changed slightly. (hard to explain without a drawing). On the snes they overcame this by altering every scanline the walkvectors (In those days one could set an interrupt when the monitor was drawing any scanline). This mode was later referred to as MODE 7 (since it behaved like a new virtual kind of graphics mode). The most famous games using this mode were Mario kart and F-zero So to get this working on the blackberry you'll have to draw your image ""displayHeight"" times (e.g. Every time one scanline of the image). This is the only way to achieve the desired effect. (This will undoubtedly cost you a performance hit since you are now calling the drawTexturedPath function a lot of times with new values instead of just one time). I guess with a bit of googling you can find some formulas (or even an implementation) how to calc the varying walkvectors. With a bit of paper (given your not too bad at math) you might deduce it yourself too. I've done it myself too when I was making games for the Gameboy Advance so I know it can be done. Be sure to precalc everything! Speed is everything (especially on slow machines like phones) EDIT: did some googling for you. Here's a detailed explanation how to create the mode7 effect. This will help you achieve the same with the Blackberry function. Mode 7 implementation Well there are free simulators. Can you write at least an algorithm for getting those xv yv xu yu values? unfortunately I don't own a blackberry. I do have access to it when I'm at a client doing some porting work (which also involves the blackberry). Don't think I'll be there next week though. =^| I thought my added link explained a lot of the math involved behind it... Did you try to read it? Reinier thanks for answer! To be earnest I'm bad at math that's why I've took this question up to bounty... You answer is really interesting and although it may resolve in performance issues I'll accept it if you provide a Blackberry code for a Mode 7.  Thanks for answers and guidance +1 to you all. MODE 7 was the way I choose to implement 3D transformation but unfortunately I couldn't make drawTexturedPath to resize my scanlines... so I came down to simple drawImage. Assuming you have a Bitmap inBmp (input texture) create new Bitmap outBmp (output texture). Bitmap mInBmp = Bitmap.getBitmapResource(""map.png""); int inHeight = mInBmp.getHeight(); int inWidth = mInBmp.getWidth(); int outHeight = 0; int outWidth = 0; int outDrawX = 0; int outDrawY = 0; Bitmap mOutBmp = null; public Scr() { super(); mOutBmp = getMode7YTransform(); outWidth = mOutBmp.getWidth(); outHeight = mOutBmp.getHeight(); outDrawX = (Display.getWidth() - outWidth) / 2; outDrawY = Display.getHeight() - outHeight; } Somewhere in code create a Graphics outBmpGraphics for outBmp. Then do following in iteration from start y to (texture height)* y transform factor: 1.create a Bitmap lineBmp = new Bitmap(width 1) for one line 2.create a Graphics lineBmpGraphics from lineBmp 3.paint i line from texture to lineBmpGraphics 4.encode lineBmp to EncodedImage img 5.scale img according to MODE 7 6.paint img to outBmpGraphics Note: Richard Puckett's PNGEncoder BB port used in my code private Bitmap getMode7YTransform() { Bitmap outBmp = new Bitmap(inWidth inHeight / 2); Graphics outBmpGraphics = new Graphics(outBmp); for (int i = 0; i < inHeight / 2; i++) { Bitmap lineBmp = new Bitmap(inWidth 1); Graphics lineBmpGraphics = new Graphics(lineBmp); lineBmpGraphics.drawBitmap(0 0 inWidth 1 mInBmp 0 2 * i); PNGEncoder encoder = new PNGEncoder(lineBmp true); byte[] data = null; try { data = encoder.encode(true); } catch (IOException e) { e.printStackTrace(); } EncodedImage img = PNGEncodedImage.createEncodedImage(data 0 -1); float xScaleFactor = ((float) (inHeight / 2 + i)) / (float) inHeight; img = scaleImage(img xScaleFactor 1); int startX = (inWidth - img.getScaledWidth()) / 2; int imgHeight = img.getScaledHeight(); int imgWidth = img.getScaledWidth(); outBmpGraphics.drawImage(startX i imgWidth imgHeight img 0 0 0); } return outBmp; } Then just draw it in paint() protected void paint(Graphics graphics) { graphics.drawBitmap(outDrawX outDrawY outWidth outHeight mOutBmp 0 0); } To scale I've do something similar to method described in Resizing a Bitmap using .scaleImage32 instead of .setScale private EncodedImage scaleImage(EncodedImage image float ratioX float ratioY) { int currentWidthFixed32 = Fixed32.toFP(image.getWidth()); int currentHeightFixed32 = Fixed32.toFP(image.getHeight()); double w = (double) image.getWidth() * ratioX; double h = (double) image.getHeight() * ratioY; int width = (int) w; int height = (int) h; int requiredWidthFixed32 = Fixed32.toFP(width); int requiredHeightFixed32 = Fixed32.toFP(height); int scaleXFixed32 = Fixed32.div(currentWidthFixed32 requiredWidthFixed32); int scaleYFixed32 = Fixed32.div(currentHeightFixed32 requiredHeightFixed32); EncodedImage result = image.scaleImage32(scaleXFixed32 scaleYFixed32); return result; } See also J2ME Mode 7 Floor Renderer - something much more detailed & exciting if you writing a 3D game!"
587,A,"Scale an image nicely in Delphi? I'm using Delphi 2009 and I'd like to scale an image to fit the available space. the image is always displayed smaller than the original. the problem is TImage Stretch property doesn't do a nice job and harms the picture's readability. I'd like to see it scaled like this instead: Any suggestions how best to do this? Tried JVCL but it doesn't seem to have this ability. A free library would be nice but maybe there's a low cost library that does ""only"" this would be good as well. The MadExcept library also has an image resize routine - see this topic. It is ""free for non-commercial usage"". good to know--thank you!  I use GDIPOB.pas's TGPGraphics class if Canvas is TGPGraphics Bounds is TGPRectF and NewImage is TGPImage instance: Canvas.SetInterpolationMode(InterpolationModeHighQualityBicubic); Canvas.SetSmoothingMode(SmoothingModeHighQuality); Canvas.DrawImage(NewImage Bounds 0 0 NewImage.GetWidth NewImage.GetHeight UnitPixel); You can choose the quality VS speed factor by changing the interpolation mode InterpolationModeDefault = QualityModeDefault; InterpolationModeLowQuality = QualityModeLow; InterpolationModeHighQuality = QualityModeHigh; InterpolationModeBilinear = 3; InterpolationModeBicubic = 4; InterpolationModeNearestNeighbor = 5; InterpolationModeHighQualityBilinear = 6; InterpolationModeHighQualityBicubic = 7; and smooting mode: SmoothingModeDefault = QualityModeDefault; SmoothingModeHighSpeed = QualityModeLow; SmoothingModeHighQuality = QualityModeHigh; SmoothingModeNone = 3; SmoothingModeAntiAlias = 4; NOTE: This would require XP or later or bundeling the gdiplus.dll in your installer.  You could try the built-in Delphi ScaleImage from GraphUtil thank you for the tip--i didn't know about ScaleImage!  If you revert to using Win32 API calls you can use SetStretchBltMode to HALFTONE and use StretchBlt. I'm not sure if this is provided using default Delphi calls but that's the way I generally solve this issue. thank you rob. i needed it to use HALFTONE always. btw thank you for doing so much good stuff on stack overflow. thank you; this provided a good balance btwn speed and quality. Delphi sets the stretch mode to Halftone automatically if the destination canvas's bits-per-pixel count is less than or equal to 8 and the source bitmap's bits-per-pixel count is greater than the destination's. Otherwise it uses Stretch_DeleteScans for color bitmaps.  You really really want to use Graphics32. procedure DrawSrcToDst(Src Dst: TBitmap32); var R: TKernelResampler; begin R := TKernelResampler.Create(Src); R.Kernel := TLanczosKernel.Create; Dst.Draw(Dst.BoundsRect Src.BoundsRect Src); end; You have several methods and filters to choose when resampling an image. The example above uses a kernel resampler (kinda low but with great results) and a Lanczos filter as reconstruction kernel. The above example should work for you. Great -- however if you find it slow I'm mostly inclined to think that something was wrong somewhere else. Anyways glad you found a good solution for your problem! it's a cool tool but i found it to be too slow for exactly what i needed. now that i have it i'm sure i'll use it for something else. thank you! Perhaps it was slow for him because he was doing fast updates each second? I had the same performance problem and this was on an Intel Quad Core I5 with plenty of memory. I switched from a kernel resampler to a draft resampler (TDraftResampler) and the performance problem went away. In my application's case I was updating a 1100w x 1100h image 20 times a second and then downsampling and the Lanczos kernel was too slow to keep up. Thanks for the tip about the Graphics32 library though. It's a real blessing and it really helped improve the look of my downsampled image. i was having some issues with it being too slow but the way i have now structured it this is no longer a problem. nice results. thank you!"
588,A,How do I set the color of a single pixel in a Direct3D texture? I'm attempting to draw a 2D image to the screen in Direct3D which I'm assuming must be done by mapping a texture to a rectangular billboard polygon projected to fill the screen. (I'm not interested or cannot use Direct2D.) All the texture information I've found in the SDK describes loading a bitmap from a file and assigning a texture to use that bitmap but I haven't yet found a way to manipulate a texture as a bitmap pixel by pixel. What I'd really like is a function such as void TextureBitmap::SetBitmapPixel(int x int y DWORD color); If I can't set the pixels directly in the texture object do I need to keep around a DWORD array that is the bitmap and then assign the texture to that every frame? Finally while I'm initially assuming that I'll be doing this on the CPU the per-pixel color calculations could probably also be done on the GPU. Is the HLSL code that sets the color of a single pixel in a texture or are pixel shaders only useful for modifying the display pixels? Thanks. First your direct question: You can technically set pixels in a texture. That would require use of LockRect and UnlockRect API. In D3D context 'locking' usually refers to transferring a resource from GPU memory to system memory (thereby disabling its participation in rendering operations). Once locked you can modify the populated buffer as you wish and then unlock - i.e. transfer the modified data back to the GPU. Generally locking was considered a very expensive operation but since PCIe 2.0 that is probably not a major concern anymore. You can also specify a small (even 1-pixel) RECT as a 2nd argument to LockRect thereby requiring the memory-transfer of a negligible data volume and hope the driver is indeed smart enough to transfer just that (I know for a fact that in older nVidia drivers this was not the case). The more efficient (and code-intensive) way of achieving that is indeed to never leave the GPU. If you create your texture as a RenderTarget (that is specify D3DUSAGE_RENDERTARGET as its usage argument) you could then set it as the destination of the pipeline before making any draw calls and write a shader (perhaps passing parameters) to paint your pixels. Such usage of render targets is considered standard and you should be able to find many code samples around - but unless you're already facing performance issues I'd say that's an overkill for a single 2D billboard. HTH.
589,A,"Best graphic library for .NET/Mono I am looking for a high-performance graphic library for .NET and Mono. I have taken a look at Tao framework and while it is very complete it's quite lacking in terms of usability. It should be cross-platform compatible. What other alternatives worked for you? Looks like the Tao framework is offline these days. There is a more modern OpenGL wrapper for .NET/Mono called OpenTK.  OpenGL would be my choice .NET bindings exist from many open source wrappers with OpenGL you're set for cross platform.  I agree with mhutch - OpenTK is a very good .NET/Mono wrapper for OpenGL. For one it has its own vector/matrix math library that among other things contains some of the more useful GLUT functions (I used to miss) like CreatePerspectiveFieldOfView and LookAt. I've found this library especially useful now that the matrix stack is no longer ""kosher"". Finally it comes with a nice sample project so you can study how to use the wrapper without extensive documentation study."
590,A,Binding scattered/overlapping images to a WPF Canvas I am porting a GDI application over to WPF where I displayed several dozen images onto Form then drew polygons circles rectangles etc over the top of these images using GDI Pens and Brushes. I'm starting to get the hang of WPF binding and would like to store all of these images and markup graphics in my ViewModel. My VM contains an ObservableCollection of my custom DrawingEntitys DrawingEntity contains DependencyProperties for BitmapSource Height Width CanvasTopLeftY and CanvasTopLeftX that I update frequently in the collection. I know my binding is working I just can't figure out how to bind and draw this collection onto a Canvas. I've played around with ItemsControl and ItemsSource to death too many different ways to list here. I can display the DrawingEntity.Bitmaps onto the canvas but Canvas.Top won't bind to CanvasTopLeftY in the DrawingEntity everything is overlapped at 00. I think I'm missing an obvious strategy. Any ideas? Code of your binding declarations ? Have a look at the XAML in this answer and make some slight changes to it.
591,A,"Rotate an image around the Y-Axis in Java? I need to rotate a 2d sprite about the y axis. E.g. I have a 2d top-view sprite of an aircraft. When the user turns the aircraft the wings should tilt into (or out of) the screen to show that it is turning. Is there a way to put the image into java3d rotate it and then put it back into a buffered image? Or maybe somehow knows how the pixels should change as they come to / away from the screen and I can just mess with the rasters to accomplish this. I know how to get the resulting x positions of each pixel after a rotation about the y-axis but of course just having this knowledge makes the image look like it gets squished since the pixels overlap after the rotation. note that for 2D games it's not uncommon to pre-store all the rotated versions of your sprite. This was typically done for two reasons: rotating **used** to be an expensive operation and small 2D sprites tend to not look that great once rotated. So the usual way to do this ""back in the days"" was to rotate programmatically the sprite and then have the graphic artist retouch every rotated position. This was typically done on symmetric sprites so you just had to rotate say from 0 to 90 degree then the other positions where X/Y flipped to generate all the 0 to 360 rotations. Yes but I haven't been able to rotate around the Y axis in gimp. I believe you could accomplish something like this using a perspective warp or transformation. JAI (Java Advanced Imaging) has this capability. http://java.sun.com/products/java-media/jai/forDevelopers/jai1_0_1guide-unc/Geom-image-manip.doc.html#58571  If you have it in a BufferedImage format you can use an AffineTransform to rotate it. See http://stackoverflow.com/questions/2257141/problems-rotating-bufferedimage for an example. AffineTransform can only do rotations in the x-y plane. I'm not trying to rotate in the x-y plane.  Perhaps a bit off topic but why not look into a Game Engine for Java? Maybe they have solved this already (and other problems that will encounter in the future e.g. double buffering sound input). You might find that there is already a well tested API for what you want. http://en.wikipedia.org/wiki/List_of_game_engines  Well if you have to rotate an image a Transformation is the way to go just like Tom said. If you are working with vectorial graphics it's just a little math. In this example the aircraft is simply a triangle with an extra line pointing it's heading: public void rotateRight() { heading = (heading + vectorIncrement); } public void rotateLeft() { heading = (heading - vectorIncrement); } public synchronized void render(Graphics2D g) { g.setColor(COLOR_SHIP); // Main line ship g.drawLine((int)xPos (int)yPos (int)(xPos+Math.cos(heading) * width) (int)(yPos+Math.sin(heading) * height) ); g.drawLine((int)xPos (int)yPos (int)(xPos-Math.cos(heading) * width/2) (int)(yPos-Math.sin(heading) * height/2) ); // Wings p = new Polygon(); p.reset(); p.addPoint((int)(xPos+Math.cos(heading) * width) (int)(yPos+Math.sin(heading) * height) ); p.addPoint((int)(xPos+Math.cos((heading+90)%360) * width) (int)(yPos+Math.sin((heading+90)%360) * height) ); p.addPoint((int)(xPos+Math.cos((heading-90)%360) * width) (int)(yPos+Math.sin((heading-90)%360) * height) ); g.drawPolygon(p); } This kind of interpolation can also be applied to images in order to get the desired rotation.  I believe you can achieve the YZ rotation using shear transforms something similar used to draw objects in isometric perspective in design apps such as Adobe Illustrator. Maybe this document will help you the PDF seems to be offline but there's a copy in Google's cache. 3D Volume Rotation Using Shear Transformations We show that an arbitrary 3D rotation can be decomposed into four 2D beam shears. Ah. I haven't tried it out yet but I think this is exactly what I need."
592,A,"Why should ""miter"" joints be slower than others? I'm having a graphics problem on drawing lines in Flash Player where two lines drawn on top of each other with different thickness don't align properly if I use any other JointStyle than MITER. For pictures of the effect and for the graphics oriented part of the question see my post over on doctype. However there's also a second angle on this problem which is: why should drawing the ""mitered"" joints be so much slower than others? This seems to be a problem since at least FP 8 but I couldn't find any detailed info on what the problem might be. Is this just an ordinary bug that didn't get fixed yet or is there something inherently slower about drawing these joints? For example they seem to have something to do with square roots but I seriously lack understanding of what this joint style thing is all about technically. It just looks like some minor detail a graphic designer might worry about. I'm asking because I'm wondering if I can do something to mitergate er mitigate the problem. There are various ways to join two lines: none: free round: draw a circle of radius line width/2 cap: fill in the gap between the lines miter: extrapolate the lines and fill that Miter is the most expensive. If the lines are meeting at an outer angle that is greater than 90 they need to be extrapolated intersected and filled. From the screenshots at your linked post you shouldn't need any joints for drawing a graph. Joints are only important with large stroke widths and for drawing a graph round or cap joints should be perfectly fine. In flash rendering miters is quite fast - it's just the most complicated join to pick. Out of curiosity do you know of any drawing frameworks that handle sharp miter points by clipping the miters at the miter limit rather than lopping off the miter entirely? Lopping off miters entirely when they get too steep is really ugly. Interesting idea! Maybe even lerp from miter to cap in a certain range. Never seen it in a renderer though. My guess would be that people writing vector renderers hate miters anyway :) And the whole concept of miter limits is a bit of a hack to avoid really terrible edge case behavior. I don't think it was designed with animating over the limit in mind. I'll ask around at work about the history of it a bit. Do you have a specific use case/example of where it looks bad? I just tested Winforms miter behavior and it works as I would think it should which looks good. For some reason I was expecting it to follow PostScript's lead which converts overly sharp mitered corners into standard bevel joins which look really bad. Incidentally I was just playing with JavaScript and it appears to use the annoying switch-to-bevel behavior."
593,A,"Java - Graphics to image Well I am a junior java programmer in the last week I've been facing the following problem: I've got an image and a panel. Both image and panel are inside a ScrollPane and they both need to move in the same time. When I try moving them together I get a flickering effect. Moving each of them alone though works fine. I've read about double buffering and decided to implement it. My problem is that my image is very very big therefore drawing it each time from scratch causes my app to get stuck. So instead I've been thinking of the following solution: In my paint function I'd advance the scrollbar of the large image(works fine) draw a new image from my now updated graphics content and draw upon it my panel's content. Though as much as I looked for it on the web I couldn't find an explanation of how to do it. So making it short how do I use my current Graphic object to draw an image from it? Why didn't you just update your original post? - http://stackoverflow.com/questions/1392664/scroll-bar-problems-in-java-layered-panes/1393237#1393237 I think you made an override from the paint method somewhere in your code. You have to doublebuffer the frame: frame.setVisible(true); frame.createBufferStrategy(2); // Two is the number of buffers // Here is the order important: first set visible then createStrategy Then your paint-method in your frame: (Do not override a method) public void updateGraphics() { BufferStrategy bs = getBufferStrategy(); Graphics g = bs.getDrawGraphics(); paint(g); g.dispose(); bs.show(); Toolkit.getDefaultToolkit().sync(); update(g); } This method you can use in a Thread: new Thread(""PainterThread"") { public void run() { while (true) { try { updateGraphics(); Thread.sleep(10); } catch (Exception e) {} } } }.start();  As far as i know you don't have to implement double buffering by hand. As far as i can remember here is a property (e.g setDoubleBuffered(true)) to activate double buffering. with best regards what do I set it on? Any component that contains the components that are flickering. The scrollpane for example or the Frame that contains it. doubleBuffering is enabled by default - calling this won't have any effect.  You could make a panel that extends some JComponent (like JPanel) and override its paint method. Something like this: class MyPanel extends JPanel { @override public void paint(Graphics g) { g.drawImage(myImage 0 0 null); } ... } well i am doing it but since I have both the jpanel and the large image in scrollbars when I try moving them their scrollbars together I get the flickering. How can I avoid this? I'm pretty sure that if you construct the image elsewhere (BufferedImage myImage in the example) _before_ calling g.drawImage it won't flicker. I suspect you may currently be constructing your image within the paint method perhaps successively calling Graphics object's different methods; it will flicker for sure."
594,A,"Graphic programming over video playback I want to make some GUI mockup program for video player so my idea is just to show some menu pictures over real video being playback. I have working program made with C and SDL just to load pictures and make a slideshow but i don´t know how to put this over video with transparencies. Do you have a hint? ps. i usually program with python and C so if there is any solution with this two i highly appreciate. Thanks! Sorry - tried to give an answer but deleted it when it became clear it was wrong. It does have something to do with color keys and SDL_CreateYUVOverlay though. The ""overlay"" basically *is* the video - not your foreground graphics. I guess your graphics are an overlay over an overlay ;-) - the video is overlayed over your apps window. thanks i will investigate this overlay issue that will be useful to superimpose the images over the video. But still pending how to play a video in some of the planes? there should be an easy way to do it but i cannot find anything... I'm just not sure whether you put video into the overlay surface then masked-blit over it or whether you have to do some colour-key trickery where you basically masked-blit over a magenta background so the hardware knows where to put video (where the magenta is) and where to leave your foreground graphics untouched. ok. i understand thanks. But basically then i must somehow (maybe with ffmpeg) decode the video stream to individual images and then fliping it one after another isnt? i find this very anoying i wonder if some existing library will do that job... Suggestion: use Openframeworks http://www.openframeworks.cc/documentation It is in C++ though and not C. Check the documentation for class ofVideoPlayer  I think you can use this tutorial to get a working video player in SDL: http://dranger.com/ffmpeg/tutorial01.html The tutorial is using SDL and ffmpeg. In the ""tutorial 01"" the video frame is converted to 24-bit RGB so you can just do some tricks and translate to a surface. If you want the overlay you can see this library: http://www.libsdl.org/projects/sdlyuvaddon/ There is a function (Get_YUV_From_Surface) to convert a surface to YUV. You will need to modify it to fit your needs but you can see the function to get some ideas.  You could render the video with OpenGL using a textured quad and then do all your 2D drawing on top (with an appropriate 2D projection matrix setup) using GL. SDL OpenGL support Python OpenGL bindings Depending on your codec you are quite likely to end up with YUV data (rather than RGB). Depending on the platform and your hardware you may be able to use a native texture type to avoid having to convert to RGB first. By using alpha blending you can easily come up with some very sophisticated looking UIs. You will probably want to look into something like FreeType to handle the font rendering as this is one area of GL that is platform-specific and fiddly to get working. There are loads of tools scene graphs widget libraries and so on for OpenGL you can use too."
595,A,Resized image degrades in quality I resized an image using Java2D Graphics class. But it doesn't look right.  BufferedImage resizedImage = new BufferedImage(IMG_WIDTH IMG_HEIGHT type); Graphics2D g = resizedImage.createGraphics(); g.drawImage(originalImage 0 0 IMG_WIDTH IMG_HEIGHT null); g.dispose(); Is it possible to scale an image without introducing artifacts? Duplicate of http://stackoverflow.com/questions/543130/java-2d-resize ? You could look into java-image-scaling library. With a quick test it created a better quality down scaled image than using standard Java2D/AWT tools.  This article by Chris Campbell has lots of detailed information on scaling images with Java2D. There are a number of options you can use regarding the quality of the scaling where generally the better the quality the longer the scaling will take (performance versus quality tradeoff). The information in the article will probably help your scaling look better but as @Kevin says in his answer at the end of the day no scaling is going to be absolutely perfect.  Bitmap graphics do not scale well generally speaking. Degradation is particularly notable when you increase the size of the image but even scaling down can introduce undesirable artifacts especially if not scaling by integral factors. The best solution if you need multiple sizes of a single image for display is to either use vector* graphics or take the highest fidelity bitmap you have and scale down and by integral factors. *Note that vector graphics aren't an option for photographs and the like.
596,A,"Managed Direct3D or XNA for non-game related 3D graphics programming? Which is the preferred approach for doing .NET 3D graphics programming: Direct3D or XNA seem to be the current technologies but which is best for non-game related programming? Also has Managed Direct 3D been discontinued? XNA doesn't really seem to be appropriate for non-game development. i thingk you meant ""XNA doesn't really seem to be appropriate unless for game development."" @faulty: Corrected - thanks :) Xna is Microsoft's Managed DirectX. Unfortunately they haven't written much about using it for non game related things but it can be done. Unfortunately I can't find any good resources online that show how to do this right now.  Managed DirectX wasn't so much abandoned as it was rewritten as XNA. True XNA was designed with the Xbox 360 in mind but XNA is as appropriate to use for non-game 3D programming as is any other 3D framework I've used. Don't overlook XNA's enormously convenient content importing pipeline... You can get your models and shaders onto the card and rendering with great ease. As stated above the Creaters Club community is very rich and their Winforms tutorials will likely be of interest to you.  The simple answer here is maybe XNA. And maybe WPF. It depends. If you're looking for anything that's remotely gamelike XNA is a really good solution as it gives you an framework that meshes very well with just about any implementation of a 3d interactive world. In addition to handling all of the 3d visual stuff you get things like collision detection lighting timing and cameras that all hook in very easily. If what you're working on is some kind of 3d virtual space what you have is probably pretty close to being a game anyway and XNA will give you a lot more besides just the graphics end. Otherwise if you're looking for 3d UI or data visualization WPF is probably more of what you want. There's less unnecessary overhead and the other things you get included like commanding or data binding are probably going to be more useful to you anyway. As I understand it it's set to use hardware acceleration on the backend and will handle material shaders and various 3d effects through software if necessary so that the user gets a visually consistent experience. If you've got an example that doesn't really fit well into either of these camps I'd really be interested in hearing about it. Could be a good opportunity for someone who wanted to write a useful graphics library...  I'm not in anyway an expert but when I messed around with both I found XNA way better. I just goof around with my Zune and PC gaming. I didn't have enough time to mess with DirectX and I found way more examples for XNA. You also have a whole community to help you.  Yap too bad MS discontinued MDX. I'm currently using SlimDX for our non-game development. It's not a drop-in replacement for MDX but at least the API design is quite intuitive compare to MDX v1.1's API. I've used MDX v1.1 + .Net v1.1 for version 1 of our product. As for version 2 we're migrating over to .Net 2.0. MDX v1.1 doesn't works well with .Net 2.0 plus it has quite a bit of other inherent problem. Thus we had to scout for alternative. Initially XNA was our best choice. Then because it target gaming market and compatibility with xbox 360 we found that we need to write a lot of workaround/hacks to make it works for us. The worse is interoperability with DirectShow which we use extensively in our product. MS created a huge gap in between existing and the new technology where the latest replacement for MDX and DirectShow is not powerful enough to replace the old ones at the moment and we can't afford wait for the next version. I wouldn't say SlimDX is the best for non-gaming development. But for my case it is. And since it's OpenSource we can always make changes to places where we see fit. There exist quite a will glitches when we use SlimxDX + DirectShow.Net. These glitches doesn't exist when we're using MDX 1.1 + DirectShow.Net. My guess is internally MS put in quite a bit of hacks to make it work. Anyway if you're not using DirectShow at all do consider Ogre3D/MOgre3D. MOgre3D is a manged wrapper for Ogre3D a well built 3D engine. It was our first choice but we can't use it because it doesn't support DirectShow well.  If it's not game related what is it about? If you answer this we might give you a good answer anyway depending on the application WPF might be a good answer. True... I didn't pick up on the ""non-game"" related part... @Peter Lillevold: WPF is not a 3D framework It's a UI framework with full 3D support you can integrate XNA or DirectiX in a WPF application or simply use WPF for 3D ... http://www.youtube.com/watch?v=SgNPHldOEZA WPF gives you full access to pixel and vertex shaders ... also for 3D applications WPF is a adequate framework. I'm not saying it will ever compete with DirectX or XNA for ""game"" development but the question was about ""non game"" development. Check out some cool demos and videos for WPF 3D here: http://www.interknowlogy.com/lab/Pages/Network.aspx I think WPF in this context is currently not an option. WPF have some 3D support but is imo not (yet) a 3D framework that can be compared to either Direct3D Managed DirectX or XNA. This is a terrible non-answer to the question. The answer below mentioning SlimDX is much more informative."
597,A,".NET graphics Ghosting I'm making a sample GUI for a new application we are developing at work. The language has already been decided for me but I am allowed to use any 3rd party DLLs or add-ins or whatever I need in order to make the GUI work as seamlessly as possible. They want it very mac/ubuntu/vista/Windows 7-like so I've come up with some very interesting controls and pretty GUI features. One of which are some growing/shrinking buttons near the top of the screen that increase in size when you mouse over them (it uses the distance formula to calculate the size it needs to increase by). When you take your mouse off of the controls they shrink back down. The effect looks very professional and flashy except that there is a ghosting effect as the button shrinks back down (and the buttons to the right of it since they are fixed-at-the-hip). Here is what the buttons look like in the designer: Here are some code snippets that I am using to do this: pops child buttons underneath when parent is hovered Private Sub buttonPop(ByVal sender As Object ByVal e As System.EventArgs) For Each control As System.Windows.Forms.Control In Me.Controls If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.Y > sender.Location.Y AndAlso control.Location.X >= sender.Location.X AndAlso control.Width < sender.Width AndAlso control.Location.X + control.Width < sender.Location.X + sender.Width Then control.Visible = True End If Next End Sub size large buttons back to normal after mouse leaves Private Sub shrinkpop(ByVal sender As Object ByVal e As System.EventArgs) Dim oldSize As Size = sender.Size sender.Size = New Size(60 60) For Each control As System.Windows.Forms.Control In Me.Controls If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.X > sender.Location.X AndAlso (Not control.Location.X = control.Location.X + (sender.size.width - oldSize.Width)) Then control.Location = New Point(control.Location.X + (sender.size.width - oldSize.Width) control.Location.Y) End If If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.Y > sender.Location.Y AndAlso control.Location.X = sender.Location.X AndAlso control.Width < sender.Width Then control.Location = New Point(control.Location.X control.Location.Y + (sender.size.height - oldSize.Height)) If Windows.Forms.Control.MousePosition.X < control.Location.X Or Windows.Forms.Control.MousePosition.X > control.Location.X + control.Width Then control.Visible = False End If End If Next End Sub increase size of large command buttons based on the mouse location in the button happens on mouse move  Private Sub buttonMouseMovement(ByVal sender As Object ByVal e As System.Windows.Forms.MouseEventArgs) Dim oldSize As Size = sender.Size Dim middle As Point = New Point(30 30) Dim adder As Double = Math.Pow(Math.Pow(middle.X - e.X 2) + Math.Pow(middle.Y - e.Y 2) 0.5) Dim total As Double = Math.Pow(1800 0.5) adder = (1 - (adder / total)) * 20 If Not (sender.size.width = 60 + adder And sender.size.height = 60 + adder) Then sender.Size = New Size(60 + adder 60 + adder) End If For Each control As System.Windows.Forms.Control In Me.Controls If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.X > sender.Location.X AndAlso (Not control.Location.X = control.Location.X + (sender.size.width - oldSize.Width)) Then control.Location = New Point(control.Location.X + (sender.size.width - oldSize.Width) control.Location.Y) End If If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.Y > sender.Location.Y AndAlso control.Location.X >= sender.Location.X AndAlso control.Width < sender.Width AndAlso control.Location.X + control.Width < sender.Location.X + sender.Width AndAlso (Not control.Location.Y = control.Location.Y + (sender.size.height - oldSize.Height)) Then control.Location = New Point(control.Location.X control.Location.Y + (sender.size.height - oldSize.Height)) End If Next End Sub increase size of smaller command buttons Private Sub SmallButtonMouseMovement(ByVal sender As Object ByVal e As System.Windows.Forms.MouseEventArgs) Dim oldSize As Size = sender.Size Dim middle As Point = New Point(22.5 22.5) Dim adder As Double = Math.Pow(Math.Pow(middle.X - e.X 2) + Math.Pow(middle.Y - e.Y 2) 0.5) Dim total As Double = Math.Pow(1012.5 0.5) adder = (1 - (adder / total)) * 15 If Not (sender.size.Width = 45 + adder And sender.size.height = 45 + adder) Then sender.Size = New Size(45 + adder 45 + adder) End If For Each control As System.Windows.Forms.Control In Me.Controls If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.Y > sender.Location.Y AndAlso control.Location.X = sender.location.X AndAlso (Not control.Location.Y = control.Location.Y + (sender.size.height - oldSize.Height)) Then control.Location = New Point(control.Location.X control.Location.Y + (sender.size.height - oldSize.Height)) End If Next End Sub decrease puts command buttons back to correct location and hides them if appropriate  Private Sub SmallShrinkPop(ByVal sender As Object ByVal e As System.EventArgs) Dim oldsize As Size = sender.Size If Not (sender.size.width = 45 AndAlso sender.size.height = 45) Then sender.size = New Size(45 45) End If Dim ChildCounter As Integer = 0 For Each control As System.Windows.Forms.Control In Me.Controls If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.X = sender.location.X AndAlso control.Width = sender.width AndAlso control.Location.Y > sender.location.y Then ChildCounter += 1 control.Location = New Point(control.Location.X control.Location.Y + (sender.size.height - oldsize.Height)) If Windows.Forms.Control.MousePosition.X < control.Location.X Or Windows.Forms.Control.MousePosition.X > control.Location.X + control.Width Then sender.visible = False control.Visible = False End If End If Next If (ChildCounter = 0 AndAlso Windows.Forms.Control.MousePosition.Y > sender.Location.Y + sender.Height) Or (Windows.Forms.Control.MousePosition.X < sender.Location.X Or Windows.Forms.Control.MousePosition.X > sender.Location.X + sender.Width) Then sender.visible = False For Each control As System.Windows.Forms.Control In Me.Controls If control.GetType.ToString = ""Glass.GlassButton"" AndAlso control.Location.X = sender.location.x AndAlso control.Width = sender.width Then control.Visible = False End If Next End If End Sub What I know: If the form didn't have a background image I wouldn't have the ghosting problem. If this were just a normal button I am drawing I probably wouldn't have the ghosting problem. What I've done and how I've tried to fix it: Ensuring the form's doublebuffering is turned on (it was) Manually doublebuffering using the bufferedGraphics class (did not help or made it worse) Convince the designers that it doesn't need background images or the pretty glass buttons (no go) Run Invalidate() on the rectangle containing the form (did not help) Run Refresh() on the form (fixed the ghosting but now the entire screen flashes as it reloads image) Sit in the corner of my cubicle and cry softly to myself (helped the stress but also did not fix issue) What I am looking for are the answers to these questions: Does anyone have any idea how to get rid of the ghosting I'm describing? Should I focus on changing the size less often? should I be focusing on buffering the background image? Are there other technologies I should be using here? Are there ActiveX controls that would be better at this than .NET user inherited ones? Is it possible to make a DirectX user-control to use the graphics card to draw itself? Is there something else I'm not thinking of here? ~~~~~~~~~~~~~~~~~~~~~~~~~ Update 1: 11/17/2009 9:21 AM ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ I've improved the efficiency of the draw methods by first checking to see if they need to be redrawn by checking what the new values will be vs what they already are(code changed above). This fixes some of the ghosting however the core problem still remains to be solved. Wow I think you're helping us out by way of all that generous code and description! About ""Are there other technologies I should be using here?"": *WPF* is the current state-of-the-art of creating ""mac/ubuntu/vista/Windows 7-like"" user interfaces with VB.NET. It looks like you're still using Winforms... Yes I believe WPF was brought up to our management team here but they decided against it because it was ""too new"". So yes I'm stuck in .NET 2005 (they said we could move to 2008 if I had a good reason to) Too new?? WPF might be relatively new but it fits the kind of application you're creating perfectly. Also it's the technology that is going to be supported and enhanced in the future. I'd ask them to reconsider... I'll certainly try to get them to reconsider but I doubt they will its ""way too new"" its like arguing with a brick wall. As a random aside instead of: control.GetType.ToString = ""Glass.GlassButton"" use TypeOf control Is Glass.GlassButton good catch Stu gotta fix the Code Smells. do you have access to Spy++ or a similar tool? If so can you check if there is a WM_ERASEBKGND message being sent to the parent form when the buttons are withdrawn to their smaller size? I don't see anything wrong in the code. Here is something you can try: Draw the background image on a picturebox the size of the form instead of the form itself. You can keep the buttons in front of it and it should look the same. Then you can control the redraw of the background image either using the graphics object in the paint event or using a persistent image with the picture box. To prevent flickering you can only redraw that part of the image that is within the bounds of the buttons (or the modified button and those to its right). This isn't ideal but it might be a usable workaround. This may actually be a viable workaround for the issue. However you can't tell it from the picture but the grey background here is actually an MDI container that will contain windows launched by these buttons. The MDI Container has draw events and such as well and I've already tried invalidating certain sections of it and redrawing portions and the like (and it was no simple task to figure out how to modify the properties on the MDIClient since it isn't an actual form control) So in a sense I've kind of tried what you're saying to the extent I could. Thanks for the idea though!  There is a possible quick fix for your problem. Paste this code into your form:  public partial class Form1 : Form { public Form1() { InitializeComponent(); this.IsMdiContainer = true; foreach (Control ctl in this.Controls) { if (ctl is MdiClient) { ctl.BackgroundImage = Properties.Resources.SampleImage; break; } } } protected override CreateParams CreateParams { get { CreateParams cp = base.CreateParams; cp.ExStyle |= 0x02000000; // Turn on WS_EX_COMPOSITED return cp; } } } The style flag works on XP SP1 and up. It double-buffers your entire form not just each individual control and should eliminate the ghosting effect you see. My management staff decided that there should be a black panel behind the buttons and that they need to be spaced out and don't need to move one another so it really gets rid of the entire issue for me. I believe that your answer was the most correct and inventive solution for the issue so you get the points. The main reason that I believe I was getting a grey background is because our app actually has 4 MDI client areas on it which is really a bigger mess of code than I care to get into in this topic. Thanks for all of your help! I'm pretty sure this fixes the ghosting issue however now I cannot see the background images past the MDI Client control. any ideas? I don't know what that means. Setting the MdiClient's BackgroundImage property works just fine. I updated the sample code to set the MDI client window's background image. This works fine when I try it. Please modify this code so it reproduces your problem. hmm... it isn't working fine on my PC. the grey background is showing instead of the MDIClient's BackgroundImage."
598,A,"Loading an image from memory GDI+ Here's a quick and easy question: using GDI+ from C++ how would I load an image from pixel data in memory? I belive this thread solved your problem: [http://stackoverflow.com/questions/192124/how-do-i-load-and-save-an-image-from-an-sql-server-database-using-gdi-and-c?rq=1][1] Is the ""pixel data in memory"" formatted as a particular image type i.e. bitmap TIF PNG etc.? If so get it into a stream and use System.Drawing.Image.FromStream. Sorry I was being unclear. These are raw pixels. No image format. Ah in that case this wouldn't work then.  There's a bitmap constructor which takes a BITMAPINFO and a pointer to pixel data directly for example: BITMAPINFO bmi; memset(&bmi 0 sizeof(bmi)); bmi.bmiHeader.biSize = sizeof(BITMAPINFOHEADER); bmi.bmiHeader.biWidth = 32; bmi.bmiHeader.biHeight = 32; bmi.bmiHeader.biPlanes = 1; bmi.bmiHeader.biCompression = BI_RGB; bmi.bmiHeader.biBitCount = 24; char data[32 * 32 * 3]; // Write pixels to 'data' however you want... Gdiplus::Bitmap* myImage = new Gdiplus::Bitmap(&bmi data); That's OK for an RGB image if it's a palette image you'd need to allocate a BITMAPINFO with enough space for the RGBQUADS etc etc.  Probably not as easy as you were hoping but you can make a BMP file in-memory with your pixel data: If necessary translate your pixel data into BITMAP-friendly format. If you already have say 24-bit RGB pixel data it is likely that no translation is needed. Create (in memory) a BITMAPFILEHEADER structure followed by a BITMAPINFO structure. Now you've got the stuff you need you need to put it into an IStream so GDI+ can understand it. Probably the easiest (though not most performant) way to do this is to: Call GlobalAlloc() with the size of the BITMAPFILEHEADER the BITMAPINFO and your pixel data. In order copy the BITMAPFILEHEADER BITMAPINFO and pixel data into the new memory (call GlobalLock to get the new memory pointer). Call CreateStreamOnHGlobal() to get an IStream for your in-memory BMP. Now call the GDI+ Image::FromStream() method to load your image into GDI+. Good luck! Good luck! The BITMAPINFO stuff is fussy; looking at a 24-bit BMP file in a hex-editor can be helpful to compare what you're doing with the real thing. (Visual Studio can do this; open the file but use the ""Open With"" option and pick ""Binary Editor""). Ugh you're right; that is far more complex than I would have ever imagined. I hope I can get it working.  This article describes how you can load a GDI+ image from a resource (rc) file. Loading the image from memory should be similar."
599,A,"Draw a polygon in C i need to draw a polygon of ""n"" sides given 2 points (the center and 1 of his vertex) just that i suck in math. I have been reading a lot and all this is what i have been able to figure it (i dont know if it is correct): Ok i take the distance between the 2 points (radius) with the theorem of Pythagoras: sqrt(pow(abs(x - xc) 2) + pow(abs(y - yc) 2)); And the angle between this 2 points with atan2 like this: atan2(abs(y - yc) abs(x - xc)); Where xc yc is the center point and x y is the only vertex know. And with that data i do: void polygon(int xc int yc int radius double angle int sides) { int i; double ang = 360/sides; //Every vertex is about ""ang"" degrees from each other radian = 180/M_PI; int points_x[7]; //Here i store the calculated vertexs int points_y[7]; //Here i store the calculated vertexs /*Here i calculate the vertexs of the polygon*/ for(i=0; i<sides; i++) { points_x[i] = xc + ceil(radius * cos(angle/radian)); points_y[i] = yc + ceil(radius * sin(angle/radian)); angle = angle+ang; } /*Here i draw the polygon with the know vertexs just calculated*/ for(i=0; i<sides-1; i++) line(points_x[i] points_y[i] points_x[i+1] points_y[i+1]); line(points_y[i] points_x[i] points_x[0] points_y[0]); } The problem is that the program dont work correctly because it draw the lines not like a polygon. Someone how know enough of math to give a hand? im working in this graphics primitives with C and turbo C. Edit: i dont want to fill the polygon just draw it. Can you clarify whether you want a filled polygon (in which case the answers about trangles may be relevant) or an unfilled one (where you simple want the perimeter lines as you suggest and drawing lines is all you need I'm not going to just give you the answer but I have some advice. First learn how line drawing works INSIDE AND OUT. When you have this down try to write a filled triangle renderer. Generally filled polygons are drawn 1 horizontal scan line at a time top to bottom. You're job is to determine the starting and stopping x coordinate for every scan line. Note that the edge of a polygon follows a straight line (hint hint)... :)  Consider what 360/sides actually returns if sides is not a factor of 360 (this is integer division - see what 360/7 actually returns). There is no need to use degrees at all - use 2*Math_PI/(double)nsides and work throughout in radians. also you can omit the final line by using the modulus function (module nsides). If you have more than 7 sides you will not be able to store all the points. You don't need to store all the points if you are simply drawing the polygon rather than storing it - just the last point and the current one.  You should be using radians in all your calculations. Here's a complete program that illustrates how best to do this: #include <stdio.h> #define PI 3.141592653589 static void line (int x1 int y1 int x2 int y2) { printf (""Line from (%3d%3d) - (%3d%3d)\n"" x1 y1 x2 y2); } static void polygon (int xc int yc int x int y int n) { int lastx lasty; double r = sqrt ((x - xc) * (x - xc) + (y - yc) * (y - yc)); double a = atan2 (y - yc x - xc); int i; for (i = 1; i <= n; i++) { lastx = x; lasty = y; a = a + PI * 2 / n; x = round ((double)xc + (double)r * cos(a)); y = round ((double)yc + (double)r * sin(a)); line (lastx lasty x y); } } int main(int argc char* argv[]) { polygon (000104); // A diamond. polygon (0010104); // A square. polygon (000108); // An octagon. return 0; } which outputs (no fancy graphics here but you should get the idea): === Line from ( 0 10) - (-10 0) Line from (-10 0) - ( 0-10) Line from ( 0-10) - ( 10 0) Line from ( 10 0) - ( 0 10) === Line from ( 10 10) - (-10 10) Line from (-10 10) - (-10-10) Line from (-10-10) - ( 10-10) Line from ( 10-10) - ( 10 10) === Line from ( 0 10) - ( -7 7) Line from ( -7 7) - (-10 0) Line from (-10 0) - ( -7 -7) Line from ( -7 -7) - ( 0-10) Line from ( 0-10) - ( 7 -7) Line from ( 7 -7) - ( 10 0) Line from ( 10 0) - ( 7 7) Line from ( 7 7) - ( 0 10) I've written the polygon function as per your original specification passing in just the two co-ordinates. As an aside you don't want those abs calls in your calculations for radius and angle because: they're useless for radius (since -n2 = n2 for all n). they're bad for angle since that will force you into a specific quadrant (wrong starting point). Full double precision is about 17 decimal digits. Not enough digits. Try 3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798 @Pete I can't tell whether you're trying to be funny ot not. But even the approximation 355/113 is enough to pinpoint a car (3m) on a circle the size of the Earth. I suspect your version may be enough to locate an atom in a sphere the size of the solar system probably a little bit of overkill for drawing on a monitor. I think more than 8 digits would be excessive for this sort of work--so pardon my lame attempt at a joke :)  /* all angles in radians */ double ainc = PI*2 / sides; int x1 y1; for (i = 0; i <= sides; i++){ double a = angle + ainc * i; int x = xc + radius * cos(a); int y = yc + radius * sin(a); if (i > 0) line(x1 y1 x y); x1 = x; y1 = y; } Or you could save the points in an array and call the DrawPoly routine if you've got one. If you want a filled polygon call FillPoly if you've got one.  I think the main trouble is: atan2(abs(y - yc) abs(x - xc)); is giving you radians not degrees just convert it to degrees and try.  You're trying to draw a filled poly I guess? If you're going to try to draw the polys using a line primitive you're going to have a lot of pain coming to you. dicroce actually gave you some very good advice on that front. Your best bet is to find a primitive that fills for you and supply it a coordinates list. It's up to you to determine the coordinates to give it. Why do you assume it has to be filled? The question makes no mention of that. In fact it specifically attempts to draw the outline of the polygon not a filled version."
600,A,"Is there any way to have an ""inverted"" clip region for painting in Java? I want to fill a region using Graphics.fillRoundRect() but I want a rectangle in the middle of it to not be filled. Essentially given a component of 100x30 I want to set clipping to be the rectangle at 1010 of size 80x10 but have the fill only paint the area outside that 80x10 rectangle. The reason is that I want a border of n pixels with a curved outline painted without affecting the inside component area. The best way I can see so far is to clip to 1010 90x10 and do the fillRoundRect() and then clip to 9010 10x10 and do a fillRect() to fill in the right hand side below and above the corners. If I simple repaint a single line rectangle then I end up with ""spotting"" on the corners because the curves don't quite abut (and/or because AA affects surrounding pixels). EDIT: Caveat - I need a way to do it which will work with J2ME AWT (CDC with Personal Profile 1.1) as well as J2SE. Edit: Another similar question has an an answer I was able to adapt. The code that works correctly for my situation is posted as a self-answer. Don't know why this had an ""objective-c"" tag... @Quinn: Thanks for the correction - I *typed* ""J2ME"" in the Tags field but I suspect that pressing tab from the Tags field to the Edit Summary switched the tag and I didn't notice! I find the tab behavior of the tags field very annoying/counter-intuitive. Please check my answer on this question. It is very similar. EDIT: you might want to check if AlphaComposite is available in j2me. In Java you can simulate the clip by changing the alpha composite mode (i can't remember to which exactly i think its srcIn) and by painting on an image with black and white areas. You might want to check it out. Yes it looks like that will work for J2SE but the solution also needs to work for J2ME (which requirement I didn't include originally sorry). J2ME PP 1.1 does not have the java.awt.geom package. Man! it's a pain having to support handhelds! yes you should change the tags and the title then. in the question i provided check out the answer not by me. Maybe that will work for you. +1 for pointing me to a similar question which had an answer I was able to adapt into a working solution. Everybody wins! Quick note: You used 1 extra point. The point (olit) is unnecessary because you can go from the top left outer corner to the top left inner corner no problem.  I have a similar answer on the other question too which is to use a polygon as an AWT clip. Maybe this is supported in J2ME? You'll need to know the bounds of the rectangle you want to exclude and the outer bounds of your drawing area.  +-------------------+ | clip drawing area | +---+-----------+ | | | excluded | | | | area | | | +-----------+ | | | +-------------------+ EDIT FROM OP. This answer worked for me and the API's are supported on J2ME. The answer of the other question appears to have one mistake - the set of coordinates needs to start a the point on the outer-left and inner top in order to create an enclosed polygon. My final code which worked follows: To create a clipping Shape I used this method: static public Shape getOutsideEdge(Graphics gc Rectangle bb int top int lft int btm int rgt) { int ot=bb.y  it=(ot+top); int ol=bb.x  il=(ol+lft); int ob=(bb.y+bb.height) ib=(ob-btm); int or=(bb.x+bb.width ) ir=(or-rgt); return new Polygon( new int[]{ ol ol or or ol ol il ir ir il il } new int[]{ it ot ot ob ob it it it ib ib it } 11 ); } which I set into the Graphics context and then filled my rectangle: Rectangle tmp=new Rectangle(pxpypwph); gc.setClip(getOutsideEdge(gctmpthicknessthicknessthicknessthickness)); gc.fillRoundRect(pxpypwphRADIUSRADIUS); and then I created the illusion of rounded inside corners by painting a single dot in each corner: gc.setClip(pxpypwph); gc.drawLine((px +thickness )(py +thickness )(px +thickness )(py +thickness )); gc.drawLine((px+pw-thickness-1)(py +thickness )(px+pw-thickness-1)(py +thickness )); gc.drawLine((px +thickness )(py+ph-thickness-1)(px +thickness )(py+ph-thickness-1)); gc.drawLine((px+pw-thickness-1)(py+ph-thickness-1)(px+pw-thickness-1)(py+ph-thickness-1));"
601,A,"Making A Graphic Pie Chart In C# I'm trying to write a Windows Application that shows a pie chart with seven unequal slices (25% 20% 18% 17% 10% 10% 10%) all of them will be colored differently. So far I have made Pens and Brushes with colors attached and drawn a circle. This is what I have so far private void Form1_Paint(object sender PaintEventArgs e) { this.BackColor = Color.White; this.Text = ""Pie Chart""; this.Width = 350; this.Height = 350; Pen black = new Pen(Color.Black); Pen blue = new Pen(Color.Blue); Pen green = new Pen(Color.Green); Pen red = new Pen(Color.Red); Pen orange = new Pen(Color.Orange); Pen pink = new Pen(Color.Pink); Pen purple = new Pen(Color.Purple); Pen magenta = new Pen(Color.Purple); Brush brBlue = blue.Brush; Brush brGreen = green.Brush; Brush brRed = red.Brush; Brush brOrange = orange.Brush; Brush brPink = pink.Brush; Brush brPurple = purple.Brush; Brush brMagenta = magenta.Brush; Graphics g = e.Graphics; g.DrawEllipse(black 20 10 300 300); } My question to you is. What would be the easiest way to draw the wedges of the pie? Don't forget to Dispose your GDI+ resources or put them into a Using block. You really put 110% into this one! This [tutorial](http://forum.codecall.net/csharp-tutorials/7917-tutorial-vs2008-c-pie-chart.html) may be helpful. I will advise you to take a look at ZedGraph. If you want a sample code to actually draw pieChart using GDI you can check this tutorial.. It uses FillPie Method of Graphics class. Thanks for this. I used fill pie and got it done. @Cistoran : You are welcome. Please consider accepting answer that helped you most. At first I couldn't find the button. But now it is done :)  CodeProject.com has several samples. Here's one I've used. Also I would recommend looking into the Google Charts. It will do this for you. While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes.  This isn't a direct answer to you question but why aren't you using the Microsoft chart controls? Scott Gu's post about it Because I don't have administrative rights on this computer to install them :) You don't need to install them. Just copy the DLLs and reference them in your project. This does not provide an answer to the question. To critique or request clarification from an author leave a comment below their post. While this link may answer the question it is better to include the essential parts of the answer here and provide the link for reference. Link-only answers can become invalid if the linked page changes."
602,A,How to scan convert right edges and slopes less than one? I'm writing a program which will use scan conversion on triangles to fill in the pixels contained within the triangle. One thing that has me confused is how to determine the x increment for the right edge of the triangle or for slopes less than or equal to one. Here is the code I have to handle left edges with a slope greater than one (obtained from Computer Graphics: Principles and Practice second edition): for(y=ymin;y<=ymax;y++) { edge.increment+=edge.numerator; if(edge.increment>edge.denominator) { edge.x++; edge.increment -= edge.denominator; } } The numerator is set from (xMax-xMin) and the denominator is set from (yMax-yMin)...which makes sense as it represents the slope of the line. As you move up the scan lines (represented by the y values). X is incremented by 1/(denomniator/numerator) ...which results in x having a whole part and a fractional part. If the fractional part is greater than one then the x value has to be incremented by 1 (as shown in edge.increment>edge.denominator). This works fine for any left handed lines with a slope greater than one but I'm having trouble generalizing it for any edge and google-ing has proved fruitless. Does anyone know the algorithm for that? I'm not reading your question carefully enough to give a good answer but I learned lots about rasterization from Chris Hecker tutorials: http://chrishecker.com/images/9/97/Gdmtex2.pdf I'm assuming that you're doing this as an exercise (homework or otherwise) or else you'd be using Bresenham's algorithm right? Bresenham's is just for drawing line's isn't it? And yes it is for an exercise. I've written some code to convert bezier patches into triangles and I was noticing that some triangles were being shaded correctly and others weren't - when I realized I hadn't considered the general case for filling them. There are 4 cases you need to consider: slope > 1 slope between 0 and 1 slope between -1 and 0 and slope less than -1. You have coded for slope > 1. If you have slope < -1 you can determine that ahead of time compute increment to be +1 or -1 and change x++ to x += increment. For the cases where slope is between -1 and 1 you can have the same loop only with x and y swapped. EDIT: It sounds like you're trying to fill a triangle by scan converting two legs simultaneously. That's a special case that you can use but the general way to scan convert a convex polygon (like a triangle) is to put all the points into a list and then for each scan line draw a line between the min and max column. Here's some pseudocode for the algorithm: // create a list of points that is the union of all segments var points = ScanConvert(x1 y1 x2 y2) .Union(ScanConvert(x2 y2 x3 y3)) .Union(ScanConvert(x1 y1 x3 y3)); // group the points by scan line finding the leftmost and rightmost point on each line var scanlines = from p in points group p by p.Y into scanline select new { y = scanline.Key x1 = scanline.Min(p => p.X) x2 = scanline.Max(p => p.X) } // iterate over the scan lines drawing each segment foreach (var scanline in scanlines) DrawLine(x1 y x2 y); By swapping x and y do you mean swapping them in the loop - so you would have (for x=xmin....) and then edge.y? Or do you mean switching them in the calculation of the numerator/denominator values? Zachary: You would have to swap x and y in all locations including the loop and the numerator/denominator calculation. That doesn't really make a lot of sense to me. What if your left hand edge and your right hand edge are in two different categories of slopes? As far as I can tell you only want to iterate from yMin to yMax one time (filling in pixels as you go) then updating the x value for each edge. If you have to switch x's and y's throughout that seems like it would be impossible to handle situations where your left edge is something like 2 and your right edge is .5. Zachary: you need to rethink your algorithm to make it more general. See my edit. Thanks for the detailed algorithm but unfortunately this exercise requires me to use the pixel-by-pixel conversion. I'll go ahead and accept your answer though...as in most normal case it is best.
603,A,"Switching line properties while using NSBezierPath I'm having some very basic problems changing line color/width when drawing different lines (intended for a chart) with NSBezierPath. The following code should make it clear what I'm trying to do: #import ""DrawTest.h"" @implementation DrawTest - (id)initWithFrame:(NSRect)frameRect { NSLog(@""in 'initWithFrame'...""); if ((self = [super initWithFrame:frameRect]) != nil) { [self drawBaseline]; [self display]; // ...NO! [self drawPlotline]; } return self; } - (void)drawBaseline { NSRect windowRect; int width height; windowRect = [self bounds]; width = round(windowRect.size.width); height = round(windowRect.size.height); theLineWidth=1.0; [[NSColor grayColor] set]; // allocate an instance of 'NSBezierPath'... path = [[NSBezierPath alloc]init]; // draw a HORIZONTAL line... [path moveToPoint:NSMakePoint(0height/2)]; [path lineToPoint:NSMakePoint(widthheight/2)]; } - (void)drawPlotline { theLineWidth=10.0; [[NSColor redColor] set]; // draw a VERTICAL line... [path moveToPoint:NSMakePoint(100125)]; // xy [path lineToPoint:NSMakePoint(100500)]; } - (void)drawRect:(NSRect)rect { NSLog(@""in 'drawRect'...""); // draw the path line(s) // [[NSColor redColor] set]; [path setLineWidth:theLineWidth]; [path stroke]; } - (void)dealloc { [path release]; [super dealloc]; } @end The problem obviously lies in the fact that 'drawRect' only gets called ONCE (after both methods have run) with the result that all lines appear in the last color and line width set. I've tried calling [self display] etc. in the hope of forcing 'drawRect' to redraw the NSView contents between the two method calls but to no avail. Can anyone suggest a basic strategy to achieve what I'm trying to do here please? Any help would be much appreciated. Arigato gozaimashita ;-) You definitely need to read and re-read the Cocoa Drawing Guide as you're fundamentally misunderstanding how Cocoa drawing works. As Yuji points out your view draws only when it's told to by Cocoa. http://developer.apple.com/mac/library/documentation/Cocoa/Conceptual/CocoaDrawingGuide/ Your drawing code in `initWithFrame:` isn't doing anything because your view isn't in a window yet because *it hasn't even finished initializing yet*. You need to read the View Programming Guide for Cocoa in addition to the Cocoa Drawing Guide that Rob Keniger already mentioned. http://developer.apple.com/mac/library/documentation/Cocoa/Conceptual/CocoaViewsGuide/ Thanks Rob and Peter. Yes I'll read the documentation. Unfortunately many of my problems stem from the fact that (1) I'm still struggling to free myself from my old 'proc-C' habits and (2) at age 65 my brain isn't co-operating with me like it used to :-( Appreciate both of your useful inputs. You're twice my age! Keep the good work! Quick answer is: move [self drawBaseline] and [self drawPlotline] inside drawRect. Also you need to call [path stroke] once per color before changing the color. So the pseudo-code is something like -(void)drawRect:(NSRect)rect { NSBezierPath*path1=... construct the path for color red... [[NSColor redColor] set]; [path1 stroke]; NSBezierPath*path2=... construct the path for color blue... [[NSColor blueColor] set]; [path2 stroke]; } Remember: [path moveToPoint:] etc. does not draw the path. It just constructs the path inside the object. Once it's constructed you stroke the path using [path stroke]. Also the color is not the property of the path constructed inside the NSBezierPath instance. So [color set] is not recorded inside your NSBezierPath instance! It's the property of the graphic context. Conceptually 1. you construct the path. 2. you set the color to the context. 3. you stroke the path. In Cocoa it's not that you tell the system when to draw when to refresh etc. Cocoa tells you when to draw by calling your drawRect:. The graphic context on which you draw is not available outside of drawRect: unless you create off-screen context yourself. So don't bother drawing beforehand. Just draw everything inside drawRect . Thanks Yuji I appreciate your help. It's working great now :-)"
604,A,Soft Paint Bucket Fill: Colour Equality I'm making a small app where children can fill preset illustrations with colours. I've succesfully implemented an MS-paint style paint bucket using the flood fill argorithm. However near the edges of image elements pixels are left unfilled because the lines are anti-aliased. This is because the current condition on whether to fill is colourAtCurrentPixel == colourToReplace which doesn't work on the blended pixels at the lines. (the colours are RGB uints) I'd like to add a smoothing/treshold option like in Photoshop and other sophisticated tools but what's the algorithm to determine the equality/distance between two colours? if (match(pixel(xy) colourToReplace) setpixel(xycolourToReplaceWith) How to fill in match()? Here an image (left is situation right is wanted) Here's my current full code:  var b:BitmapData = settings.background; b.lock(); var from:uint = b.getPixel(xy); var q:Array = []; var xx:int; var yy:int; var w:int = b.width; var h:int = b.height; q.push(y*w + x); while (q.length != 0) { var xy:int = q.shift(); xx = xy % w; yy = (xy - xx) / w; if (b.getPixel(xxyy) == from) { //<- want to replace this line b.setPixel(xxyyto); if (xx != 0) q.push(xy-1); if (xx != w-1) q.push(xy+1); if (yy != 0) q.push(xy-w); if (yy != h-1) q.push(xy+w); } } b.unlock(null); well i guess the most natural approach is to calculate the difference between to colors. to achieve a sensible value one should calculate the difference per channel. haven't tested it but the following should work: const perChanThreshold:uint = 5; const overallThreshold:uint = perChanThreshold * perChanThreshold * 3; function match(source:uint target:uint):Boolean { var diff:uint = 0 chanDiff:uint; for (var i:int = 0; i < 3; i++) { chanDiff = (source >> (i * 8)) & 0xFF; diff += chanDiff * chanDiff; } return diff <= overallThreshold; } Thanks I'm doing something like this now. See my answer  Made something that works:  c = b.getPixel(xxyy); if (c == to) continue; if (c != from) d = Math.pow(f1 - (c & 0xFF) 2) + Math.pow(f2 - (c >> 8 & 0xFF) 2) + Math.pow(f3 - (c >> 16 & 0xFF) 2) if (c == from || d < tres) { yes should work perfectly. however please note that this will not perform very good if you call it often (which may be the case for a floodfill). Going back and forth between floats and integers isn't very fast neither are static function calls.
605,A,"C# Drawstring clear rectangle Or no rectangle? possible here is the code i'm using i really need to make it so the text does not appear inside of a box. is that possible? int width = (int)g.MeasureString(line f).Width; int height = (int)g.MeasureString(linef).Height; b = new Bitmap(b new Size(width height)); g = Graphics.FromImage(b); g.Clear(Color.Empty); g.DrawString(linef new SolidBrush(Color.Black) 0 0); b.Save(savepoint+line+"".tif"" System.Drawing.Imaging.ImageFormat.Tiff); g.Flush(); What i mean is there can be no Rectangle around the text that is converted to image. So I need to watch it to the same color to create the illusion there is no box or to never write out that rectangle period. Please explain ""does not appear inside of a box."" Are you saying that you get a border around it that you're trying to remove? Or that you want to mask a rectangular area and only render the text outside that area? Or something else entirely? check edit/update please What do you mean by a box? How do you use the image? Please post a screenshot of what you mean. My test of drawing to a TIFF draws no ""rectangle"" and the code you posted will not draw anything. Yes scratch out something in mspaint and post it to tinypic.com Use the color Transparent for the backgtround and a file format that supports transparency like PNG: var measure = g.MeasureString(line f); int width = (int)measure.Width; int height = (int)measure.Height; using (Bitmap b = new Bitmap(width height)) { using (Graphics bg = Graphics.FromImage(b)) { bg.Clear(Color.Transparent); using (Brush black = new SolidBrush(Color.Black)) { bg.DrawString(line f black 0 0); } } b.Save(savepoint+line+"".png"" System.Drawing.Imaging.ImageFormat.Png); } I notice that you did overwrite your Graphics instance didn't dispose the objects that you create and called Graphics.Flush for no apparent reason... And don't forget to properly parent the control. The ""Transparent"" background of a string really just sets the background to the Parent's. keeps coming back with parameter not valid at using (b = new Bitmap(b new Size(width height))) even otuside of the loop it still places a box around the text @Mike: Where do you get the original graphics object from (variable g)? What loop are you talking about? I do a foreach to get the line. but there is still a blackbox being draw by the DrawString the black outine of the text is the problem. @Mike: The black box has to come from somewhere else. I tried the code and as expected it doesn't draw any box around the text. How are you using the image?"
606,A,"Cross Platform C library for GUI Apps? Free of charge simple to learn/use Cross Platform C library for GUI Apps? Am I looking for Qt? Bonus question: Can I develop with the said library/toolkit on Mac then recompile on PC/Linux? Super Bonus Question: Link to tutorial and/or download of said library. (RE)EDIT: The truth is that I'm in the process of catching up on the C family (coming from web development - XHTML/PHP/MySQL) to learn iPhone development. I do understand that C is not C++ or ObjectiveC but I want to keep the learning curve as simple as possible. Not to get too off topic but I am also on the lookout for good starter books and websites. I've found this so far. I'm trying to kill many birds with one stone here. I don understand that there are platform specific extensions but I will try to avoid those for porting purposes The idea is that I want to write the code on one machine and just compile thrice. (Mac/Win/Linux) If Objective C will compile on Windows and Linux as well as OS X then that's good. If I must use C++ that's also fine. EDIT: Link to QT Please... Yeah I didn't realize how old the question was when I answered. @jmucchielo - No problem. Thanks for your answer. What does desktop GUI programming have to do with iPhone development? If you want to work in phone development desktop GUI programming is the last place you want to start. @jmucchiello - I didn't say Desktop nor did I mention iPhone. OK I mentioned iPhone. This question is a bit stale already but you are right. I know that now. To complete this post Allegro has to be here =) http://www.talula.demon.co.uk/allegro/ Allegro Game Library have many graphics functions and a basic GUI library And an explicit gui (and very simple) Allegro based library http://cgui.sourceforge.net/index.html Both multi-platform  One that I have considered using was the EFL as it's quite fast simple small but powerful. I would recommend diving into Elementary their simplest GUI toolkit and then later on once you get comfortable with it move to EDJE which is not as simple but much more powerful.  Take a look at the IUP Toolkit. It is written largely in C and is also easily bound to Lua.  Qt is a C++ library. Other cross platform libraries that you might consider are wxWidgets (C++) and GTK (C). All three of the presented libraries are fully cross platform. You can also look at Tcl/Tk but that's a toolkit :). GTK is C++ really? Nope corrected. wxWidgets is C++ GTK+ is C. gtkmm is C++ so you could say that GTK is C++. As is Qt Python Ruby php etc. When providing bindings it is hard to bind downwards so C++ is not (conveniently) C as C can (roughly and quite unfairly) be seen as a subset of C++.  Another option is Tk which is a GUI library written in C. It comes with Tcl a scripting language also written in C. These were designed from the ground up to be embedded in C programs.  I'd strongly suggest questioning why your project would need to be in C. There are many benefits to C++ and the idea that C performs intrinsically better is mostly a myth. For some hard data on that check out Bjarne Stroustrup's Learning C++ as a New Language. If for some strange reason you feel you must use C and not C++ then Qt is not for you. It was designed from the ground-up as a C++ library. But if at all possible you should give it a try it has a pleasing design and is well-documented! If you must stick to C then there's always GTK. The underlying API of GTK+ is C but bindings also exist for C++ called GTKmm. I'm not a big fan of it from a design perspective but it does power the Gnome desktop (Ubuntu's default)...and Google is using it for their version of Chrome for Linux. So it has some cred. @HostileFork: Do you troll other language tags informing them that C++ would be a better alternative than C# Perl Ruby Python and Lisp? @mcl: I follow the #qt tag and the OP asked specifically about Qt. I suppose one can say ""NO! Qt is C++ you want GTK+"" and leave it at that and you are free to answer that way if you like. But I personally don't feel GTK is that good and I'd hate for someone to avoid Qt because they hadn't taken the time to consider if their reasons for using C instead of C++ were based on incorrect assumptions. So I include some discussion of that with my answer. Got some ""hard data"" that comes from someone besides the author of the language? Stroustrup is the single least objective observer possible on the question of C vs. C++. @mcl. Agreed. @Hostile Fork : Bit of a sweeping statement! All depends on what your doing the compiler you are using and the target hardware. C++ can ( but not always) lead to single object centric approaches which can limit performance by cache-thrashing. We've learnt this the hard way in the games industry C'mon though the question is ""Cross platform GUI framework""...I've heard this a couple times before from people who know C and just don't want to learn C++. And the Stroustrup paper stands on its own merits regardless of who wrote it. Did you *read* it? Aside from the fact that ""Are you sure you don't want to ask something else?"" isn't an answer the obvious reason to want a C API is because you're not using C but a language that can interface with C like luajit terra D rust and so on. I find that good wrapper libraries rarely exist and would rather have the full power of the native C API. @cib Look [even Ubuntu is ditching GTK for Qt](http://www.omgubuntu.co.uk/2013/03/unity-next-project-announced). I'll wear any downvotes here on a question actually *tagged* [tag:qt] as a badge of pride to be on the right side of history. [You can't touch us. You can't fix us.](http://www.youtube.com/watch?v=5GFqQnqcK-Q) Just leave us alone Neanderthals.  Take a look at the Ecere SDK. It offers a cross-platform GUI toolkit and gives you eC an object-oriented language derived from C (with all of its functionality) that is just great for building GUIs.  If you are looking for a C++ library then Qt basically does what you are looking for. If you want to stick to pure C than Qt is not an option. As a C framework you could use GTK+ it works on Linux Windows and OS X. OK so what will work with c then? @Moshe yes GTK+ is written in C. Doesn't QT require payment for commercial use? Not that he said it's commercial but he talks about mac/win/lin and iPhone. @phkahler: No it doesn't you can use it as LGPL (so you can use it in commercial closed source projects as long as you make modifications of the QT library itself available). There also is a commercial license if you want to make modifications to QT itself that you wouldn't want to make available under LGPL. GTK support for Macs is not very good unless you want to run X11 and that has its own downsides. Note gtk+ now supports OS X without X11(I believe this is no longer labelled experimental and the latest version of gimp for OS X ships a non-x11 build) although I would still probably heavily recommended qt over gtk+ for OS X."
607,A,"Get the graphics card model? I was wondering how I can get the graphics card model/brand from code particularly from DirectX 9.0c (from within C++ code). Thanks for any help! You can use ""DirecX Diagnostic Tool"" API like in sample DxDiagOutput from DX SDK http://msdn.microsoft.com/en-us/library/ee416986%28v=VS.85%29.aspx  At runtime you can query the device model and vendor: In OpenGL use the command glGetString(GL_VENDOR) or GL_RENDERER or GL_VERSION to get the information you're after. In DirectX 9 it appears the info is in the Microsoft config system and is queried from the device database. It's section 3 of this document which also has example code: http://msdn.microsoft.com/en-us/library/bb204848(VS.85).aspx Using the same system you can get such information as the amount of ram the video card has the driver number etc.  Take a look at Chapter 2. Direct3D from my book The Direct3D Graphics Pipeline. See section 2.12 Identifying a Particular Device.  The easiest way in DirectX is through IDirect3D9::GetAdapterIdentifier. Just create a D3DADAPTER_IDENTIFIER9 object pass a pointer to it to GetAdapterIdentifier. DirectX fills out the graphics card description as a string in the Description field. It also includes information on which display device the card is and what driver version you have. You get something like this: Description: ""NVIDIA GeForce GTX 570"" Device: ""\.\DISPLAY1"" Driver: ""nvd3dum.dll"" Thanks got the information I needed and was very useful not sure why this isn't the accepted answer."
608,A,How to convert array of bytes into Image in Java SE What is the right way to convert raw array of bytes into Image in Java SE. array consist of bytes where each three bytes represent one pixel with each byte for corresponding RGB component. Can anybody suggest a code sample? Thanks Mike Eventually I did not find anything better then: for(int y=0; yYou can do this with Raster class like this. It's better because it does not require iterating and copying of byte arays  byte[] raw = new byte[width*heigth*3]; BufferedImage image = new BufferedImage(width height BufferedImage.TYPE_3BYTE_BGR); file.read(frame); DataBuffer buffer = new DataBufferByte(frame frame.length); //The most difficult part of awt api for me to learn SampleModel sampleModel = new ComponentSampleModel(DataBuffer.TYPE_BYTE width height 3 width*3 new int[]{210}); Raster raster = Raster.createRaster(sampleModel buffer null); image.setData(raster); Thanks for posting that. It came in really handy!  Assuming you know the height and width of the image. BufferedImage img=new BufferedImage(width height BufferedImage.TYPE_INT_RGB); for(int r=0; r<height; r++) for(int c=0; c<width; c++) { int index=r*width+c; int red=colors[index] & 0xFF; int green=colors[index+1] & 0xFF; int blue=colors[index+2] & 0xFF; int rgb = (red << 16) | (green << 8) | blue; img.setRGB(c r rgb); } Roughly. This assumes the pixel data is encoded as a set of rows; and that the length of colors is 3 * width * height (which should be valid). so iterating through the array is my best option here? It feels that there got to be some method which accepts array as a parameter and populates/recreate the image in one go. Maybe something to do with Raster class? Not to my knowledge though Raster is unfamiliar to me. If this followed a standard image format you could use the ImageIO class but that's the limit of my knowledge. You can also do: `int rgb = ((int)colors[in] << 24) + ((int)colors[in+1] << 8) + (int)colors[in]` Ahh... those were the magic numbers. When I wrote it the first time I had red at 16 which failed miserably so I went to the Color.getRGB route. oops... sorry it is supposed to be `(0xFF << 24) + ((int)color[in] << 16) ...`. You need to pass the alpha in the int Two unfortunate things about the bit shift method though are that it handles neither the negative values of the components or when red is >127. One of many times that it would be nice to have unsigned data type in java. Which... now the comment about red is invalid. Bah. Though it's odd that it looks at the alpha component at all as it is supposed to be a soley RGB image. That way you can solve sign/unsign problem: frameImg.setRGB(x y ((data[i0]&0xFF) | ((data[i0+1]&0xFF)<<8) | (data[i0+2]&0xFF)<<16))); That I did not know... It doesn't seem like this answer is correct despite being accepted. If `colors` is an RGB array (where R G & B are individual bytes) shouldn't `index` be getting incremented by more than 1 per loop? sgusc colors in the example code is storing RGB as a single combined int; which is a solution to a slightly different but mostly the same problem as the OP  There is a setRGB variant which accepts an int array of RGBA values: BufferedImage img=new BufferedImage(width height BufferedImage.TYPE_INT_RGB); int[] raw = new int[data.length * 4 / 3]; for (int i = 0; i < data.length / 3; i++) { raw[i] = 0xFF000000 | ((data[3 * i + 0] & 0xFF) << 16) | ((data[3 * i + 1] & 0xFF) << 8) | ((data[3 * i + 2] & 0xFF)); } img.setRGB(0 0 width height raw 0 width); The performance characteristics is similar to CoderTao's solution.
609,A,"When is the right time to draw? I just finished essential part of my own personal 2D engine in C++ and I'm kinda deciding how to complete the part where it is actually supposed to display everything on the screen namely when do I call that function which does the job. I don't have much idea of how does the graphic card work my biggest experience is calling bios graphic services to write some stuff on the screen. Could you give me a hint on this please? Or maybe some keywords I should try to google? look up render loop. In a game you will do it in a loop. You can also look up game loop which is a related concept if you're working on a game.  Are you rendering to a back buffer and then trying to display that? Common terminology includes ""flip"" (as in page flipping) or ""present"". If your copying from a back buffer to the screen it might also be a ""blit"" (or blt) from ""bit-block transfer""."
610,A,"Saving System.Drawing.Graphics to a png or bmp I have a Graphics object that I've drawn on the screen and I need to save it to a png or bmp file. Graphics doesn't seem to support that directly but it must be possible somehow. What are the steps? Here is the code: Bitmap bitmap = new Bitmap(Convert.ToInt32(1024) Convert.ToInt32(1024) System.Drawing.Imaging.PixelFormat.Format32bppArgb); Graphics g = Graphics.FromImage(bitmap); // Add drawing commands here g.Clear(Color.Green); bitmap.Save(@""C:\Users\johndoe\test.png"" ImageFormat.Png); If your Graphics is on a form you can use this: private void DrawImagePointF(PaintEventArgs e) { ... Above code goes here ... e.Graphics.DrawImage(bitmap 0 0); } In addition to save on a web page you could use this: MemoryStream memoryStream = new MemoryStream(); bitmap.Save(memoryStream ImageFormat.Png); var pngData = memoryStream.ToArray(); <img src=""data:image/png;base64@(Convert.ToBase64String(pngData))""/> Graphics objects are a GDI+ drawing surface. They must have an attached device context to draw on ie either a form or an image.  Copy it to a Bitmap and then call the bitmap's Save method. Note that if you're literally drawing to the screen (by grabbing the screen's device context) then the only way to save what you just drew to the screen is to reverse the process by drawing from the screen to a Bitmap. This is possible but it would obviously be a lot easier to just draw directly to a Bitmap (using the same code you use to draw to the screen).  Graphics graph = CreateGraphics(); Bitmap bmpPicture = new Bitmap(""filename.bmp""); graph.DrawImage(bmpPicture width height);  You are likely drawing either to an image or on a control. If on image use  Image.Save(""myfile.png""ImageFormat.Png) If drawing on control use Control.DrawToBitmap() and then save the returned image as above. Thanks for the correction - I wasn't aware you can draw directly to the screen. You *can* draw directly to the screen using Graphics which has a constructor that takes a device context - all you need is the screen's device context. Thanks for this it has never occurred to me to try drawing straight to the screen! I would actually recommend *not* drawing directly to the screen since there's really no point to doing so."
611,A,Clutter does not update screen outside of breakpoints I have some code: l1 = clutter.Label() l1.set_position(100100) for i in range(010): l1.set_text(str(i)) time.sleep(1) That is designed to show a count from 1 to 10 seconds on the screen in clutter but I'm getting a strange error. When I run the script normally the screen runs as it should do but there is no text displayed until 10 seconds are up. However When I run with breakpoints in pdb the text shows up just fine. I'm also getting a strange error at the start of the program: do_wait: drmWaitVBlank returned -1 IRQs don't seem to be working correctly. Try adjusting the vlank_mode configuration parameter. But I don't see why that would affect the code out of break points but not in breakpoints. Any help would be greatly appreciated. Have you tried posting (or searching) on the Clutter mailing list? Here's someone who got the same message about drmWaitVBlank for example. My guess is most people on SO wouldn't be familiar with solving Clutter problems. I know I'm not :)  Not sure if you've already figured out the answer to this but: The reason you are having this problem is because you are blocking the main thread (where all the drawing occurs) with your time.sleep() calls preventing the library from redrawing the screen. E.g. your code is currently doing this: Clutter redraws the screen. You loop over ten seconds and change the text ten times. Clutter redraws the screen. If you want to queue something on a timer you should look into gobject.timeout_add.
612,A,"How does GraphicsPath.AddArc use the startAngle and sweepAngle parameters? I am trying to use System.Drawing.Drawing2D.GraphicsPath.AddArc to draw an arc of an ellipse starting at 0 degrees and sweeping to 135 degrees. The issue I am running in to is that for an ellipse the arc drawn does not match up with what I would expect. For example the following code generates the image below. The green circles are where I would expect the end points of the arc to be using the formula for a point along an ellipse. My formula works for circles but not for ellipses. Does this have something to do with polar versus Cartesian coordinates?  private PointF GetPointOnEllipse(RectangleF bounds float angleInDegrees) { float a = bounds.Width / 2.0F; float b = bounds.Height / 2.0F; float angleInRadians = (float)(Math.PI * angleInDegrees / 180.0F); float x = (float)(( bounds.X + a ) + a * Math.Cos(angleInRadians)); float y = (float)(( bounds.Y + b ) + b * Math.Sin(angleInRadians)); return new PointF(x y); } private void Form1_Paint(object sender PaintEventArgs e) { Rectangle circleBounds = new Rectangle(250 100 500 500); e.Graphics.DrawRectangle(Pens.Red circleBounds); System.Drawing.Drawing2D.GraphicsPath circularPath = new System.Drawing.Drawing2D.GraphicsPath(); circularPath.AddArc(circleBounds 0.0F 135.0F); e.Graphics.DrawPath(Pens.Red circularPath); PointF circlePoint = GetPointOnEllipse(circleBounds 135.0F); e.Graphics.DrawEllipse(Pens.Green new RectangleF(circlePoint.X - 5 circlePoint.Y - 5 10 10)); Rectangle ellipseBounds = new Rectangle(50 100 900 500); e.Graphics.DrawRectangle(Pens.Blue ellipseBounds); System.Drawing.Drawing2D.GraphicsPath ellipticalPath = new System.Drawing.Drawing2D.GraphicsPath(); ellipticalPath.AddArc(ellipseBounds 0.0F 135.0F); e.Graphics.DrawPath(Pens.Blue ellipticalPath); PointF ellipsePoint = GetPointOnEllipse(ellipseBounds 135.0F); e.Graphics.DrawEllipse(Pens.Green new RectangleF(ellipsePoint.X - 5 ellipsePoint.Y - 5 10 10)); } GraphicsPath.AddArc does exactly what you ask it to do -- it the arc up to a line projecting from the ellipse center at an exact angle of 135 degrees clockwise from the x axis. Unfortunately this doesn't help when you're using the angle as a direct proportion of a pie chart slice you want to draw. To find out the angle B you need to use with AddArc given an angle A that works on a circle in radians use: B = Math.Atan2(sin(A) * height / width cos(A)) Where width and height are those of the ellipse. In your sample code try adding the following at the end of Form1_Paint: ellipticalPath = new System.Drawing.Drawing2D.GraphicsPath(); ellipticalPath.AddArc( ellipseBounds 0.0F (float) (180.0 / Math.PI * Math.Atan2( Math.Sin(135.0 * Math.PI / 180.0) * ellipseBounds.Height / ellipseBounds.Width Math.Cos(135.0 * Math.PI / 180.0)))); e.Graphics.DrawPath(Pens.Black ellipticalPath); The result should look as follows: Cool! That works. But I still don't understand why it works. What is the logic behind your calculation? Does this have anything to do with polar vs. Cartesian coordinates? GetPointOnEllipse finds a point in two steps: 1. it finds a point on the circumference of a circle with R=1 at an angle A from the X axis at (cos(A) sin(A)) 2. it transforms the point with a stretch transform to (w*cos(A) h*sin(A)) The stretch transform changes the angle! (imagine what the angle would be if we stretched the circle very wide) To find the new angle B note that tan(B)=y/x and hence B=atan(y/x). To avoid dividing by zero we'll use B=atan2(yx) As (xy)=(w*cos(A) h*sin(A)) we get B=atan2(h*sin(A)w*cos(A)) Divide by w to get B=atan2(sin(A)*h/wcos(A)) Makes sense? I agree that AddArc is doing the right thing. Which leads me to believe that theta in the ellipse equation is not an angle measured from the center of the ellipse or it would come up with the same point correct? So what is theta in the ellipse equation? Exactly -- that is correct. As for theta you could say that it tracks the angle on a circle that is 3d-projected to the ellipse using orthographic projection. I don't fully understand your statement: ""The stretch transform changes the angle!"". I can see from the output image that the angle is different. But what I am confused on is the formula for a point on an ellipse. The formula is x = a cos(theta) y = b sin(theta). Does the fact that it draws at a different point mean that the angle being measured is different from the angle being measured by AddArc? I found another thread on this subject. It seems to point to the issue having something to do with polar coords. http://bytes.com/topic/c-sharp/answers/665773-drawarc-ellipse-geometry-repost If we draw a line inclined at 45 degrees to the X axis then for each point on that line x=y always at all times -- wouldn't you say? We could draw that line on the screen or on paper. What about theta=45 however? We know sin(45)=cos(45)=~0.707. For our ellipse a cos(45)=b sin(45) if and only if a=b. If a>b then x>y and therefore won't touch the 45 degrees line on the screen/paper. It's located somewhere to its right. That means that theta does not track the angle you are looking for on the screen. It's confusing because theta ranges between 0 and 360 like a circle (contd.) (contd.) It's also confusing as theta IS the correct angle for precisely 4 points on the ellipse -- when the ellipse crosses the X and Y axes. But at all other points on the ellipse theta is not the correct angle. AddArc does the right thing. When you tell it to end at 45 degrees it precisely hits the 45 degrees line we drew. It ignores the fact that the arc is on the surface of an ellipse that can be described by a cos(theta)b sin(theta) or by any other formula for that matter. +1 for being unlike the vast majority of random math-challenged folks and not only being able to do basic math but hitting geometry home runs with ease!"
613,A,How to copy one Graphics Object into another I am trying to copy the contents of one graphics object to another but the only was I have been able to find is based on using GDI32.DLL which i would rather avoid using if possible. Does anyone know how/if this is possible using managed code? I don't mind if answers are in C# or VB.Net. Here is what I currently have: Private Sub CopyGraphics() Dim srcPic As Graphics = pnl.CreateGraphics Dim srcBmp As New Bitmap(pnl.Width pnl.Height srcPic) Dim srcMem As Graphics = Graphics.FromImage(srcBmp) Dim HDC1 As IntPtr = srcPic.GetHdc Dim HDC2 As IntPtr = srcMem.GetHdc BitBlt(HDC2 0 0 pnl.Width pnl.Height HDC1 0 0 13369376) pnlDraw.BackgroundImage = srcBmp 'Clean Up code omitted... End Sub Strictly speaking it's not possible to copy the contents of a Graphics object anywhere using any method because a Graphics object doesn't contain anything. Why don't use the DrawToBitmap method to draw the control on the bitmap? Dim srcBmp As New Bitmap(pnl.Width pnl.Height) Dim clip As New Rectangle(New Point(0 0) pnl.Size) pnl.DrawToBitmap(srcBmp clip) I did not know about the DrawToBitmap method thanks.
614,A,"Is it possible to translate twice in one OpenGL Matrix? This is a homework question I am sorry but I am lost. What happens is the following I am asked to do a tiling of hexagons. Like the grid map in many Risk games and Wild Arms XF. I understand the current transformation is the matrix that translates the points I give to OpenGL to screen coordinates and if you apply a transformation it moves the the center point usually (00) to wherever. If you use glPushMatrix and glPopMatrix. What it does is make a replica of the current CT matrix and you do operations on it and when you pop it you return to the matrix without the transformation. The problem is the following I am trying to draw a hexagon Translate(Displace I like it better) draw another Hexagon translate and Draw the last hexagon. What happens is the third hexagon dissapears from the face of the viewport and I am only raising it twice. What is funny is that consecutive rotates and scales give me no problems. so a small sample of the code. #include <windows.h> #include <stdio.h> #include <GL/gl.h> #include <GL/glu.h> #include <GL/glut.h> #include <iostream> #include <ctime> using namespace std; void initCT(void) { glMatrixMode(GL_MODELVIEW); glLoadIdentity(); } void drawHexagon() { glBegin(GL_LINE_STRIP); glVertex2i(00); glVertex2i(20 0); glVertex2i(2510); glVertex2i(2020); glVertex2i(020); glVertex2i(-510); glVertex2i(00); glEnd(); } void myInit(void) { initCT(); glClearColor(1.0f1.0f1.0f0.0f); glColor3f(0.0f 0.0f 0.0f); //set the drawin color glPointSize(1.0); //a 'dot'is 4 by 4 pixel glMatrixMode(GL_MODELVIEW); glLoadIdentity(); gluOrtho2D(-100.0 400.0 -400.0 400.0); glViewport(0 0 640 480); } void myDisplay (void) { glClear (GL_COLOR_BUFFER_BIT); glMatrixMode( GL_MODELVIEW ); drawHexagon(); glTranslated(0.0f20.0f1.0f); drawHexagon(); glTranslated(0.0f20.0f1.0f); drawHexagon(); glTranslated(0.0f20.0f1.0f); drawHexagon(); glFlush(); //send all output to display } void main (int argc char **argv) { glutInit (&argc argv); glutInitDisplayMode (GLUT_SINGLE | GLUT_RGB); glutInitWindowSize (640 480); glutInitWindowPosition (100 150); glutCreateWindow (""Hexagon Tiling""); glutDisplayFunc (myDisplay); myInit(); glutMainLoop(); } I have used this code with the similar result only two hexagons draw the others go into the Hitchhikers guide to the galaxy. I changed the colors of each hexagon and only the first two appear. THANK YOU GUYS YOU ARE THE MEN. I am sorry if giving the answer to one upsets the other but you both are right. Also for people using Google this is from Computer Graphics using Open GL there is an error in the translation function that the author gives you in the translate it should be 0 in the Z axis. You're translating behind the camera; Each translation goes up in the y axis by 20 and in the z axis by 1. After the second step your hexagons are behind the camera and can't be seen in the viewport. Try glTranslated(0.0f20.0f0.0f); for each translation; that should help.  Yes you can use a glTranslate multiple times. This will construct a new translate matrix and multiply it onto your current matrix. Some thoughts: Your initCT function looks redundant. gluOrtho2D is usually used to modify the projection matrix. Try something like this: .... glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(-100.0 400.0 -400.0 400.0); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); .... Usually one starts their drawing by resetting their modelview matrix to identity. It's possible that glut does this for you but in case that's not happening try this: .... glClear (GL_COLOR_BUFFER_BIT); glMatrixMode( GL_MODELVIEW ); glLoadIdentity(); drawHexagon(); .... Thank you and Thank You I will take your observations into account and fix the application accordingly. You say that his initCT function is redundant then recommend that he do what's in the initCT? He switches into modelview and loads identity twice. I added a switch to projection and a loadIdentity moved one modelview loadIdentity down and suggested that he remove the other. That's all. Fair enough... That makes sense."
615,A,"How do I fill a region to its bounds with a color on a graphics object? I'm playing around and try to make a Coloring book for my children and I have a lots of black and white line drawings that I use as backgrounds so they they can paint on them. Now I want to add a FILL-function so they can point and click somewhere where its white in the drawing and then let the function fill the whole region within its bounds with a color. It would be nice to have a function that by sending in X Y and a color defined for the boundary and get a region out of it. Then its just about to do e.Graphics.FillRegion (brushregion) to fill it with the color. But I can't find any function that allowes me doing that and I think its too much job to trace the boundarys myself. Have I missed some function in the system.drawing class that does this or do you have any other ideas? Any other ideas for the coloring book-program are also interesting. (Im making the program safe so you can leave the kid in front of the computer without being afraid of the kid accessing any other things on the computer and have different funny sounds when selecting colors and so on) Look into flood fill methods. Wikipedia has plenty of information on the subject here.  If the ""boundary"" is defined by differently-colored pixels and not a mathematical formula or a Path data structure you will have to test and fill individual pixels. There is an article on how to flood fill in .Net at Code Project."
616,A,"Windows Form ""Hole"" How would I go about creating a dynamic ""hole"" in a windows form through which the user could see the actual desktop instead of the form? Right now I've created a translucent form on top of the entire screen and I'm looking to see through that translucent form. Use the form's Region property. Rectangle rect = new Rectangle(Point.Empty this.Size); Region region = new Region(rect); rect.Inflate(-1 * (this.Width / 3) -1 * (this.Height / 3)); region.Exclude(rect); this.Region = region; That should put a hole through your form. Excellent answer! Simple and nice!  As an alternative if you need rectangular form of ""hole"" you can set form's TransparencyKey property to a certain color and then create a Panel with the background of the same color. (That panel will be transparent on run.)"
617,A,"Graphics.DrawRectangle(Pen RectangleF) http://msdn.microsoft.com/en-us/library/system.drawing.graphics.drawrectangle.aspx FillRectangle DrawRectangle FillElipse and DrawEllipse all can take 4 Float (or ""Single"") parameters: x y width height. DrawRectangle is the only one that will not take a RectangleF though. I was wondering if anyone knew why this is. It sure seems like they just plain forgot to overload it. Well it sure does look like an omission to me too. Interestingly there is an overload of DrawRectangles that takes a RectangleF[] array as a parameter. So I suppose you could use this with an array size of one if needed."
618,A,"Creating a Movie from a Series of Plots in R Is there an easy way to create a ""movie"" by stitching together several plots within R? I think you can do this also with the write.gif function in the caTools library. You'd have to get your graph into a multi-frame image first. I'm not sure how to do that. Anyone? Bueller? The classic example of an animated GIF is this code which I didn't write but I did blog about some time ago: library(fields) # for tim.colors library(caTools) # for write.gif m = 400 # grid size C = complex( real=rep(seq(-1.80.6 length.out=m) each=m ) imag=rep(seq(-1.21.2 length.out=m) m ) ) C = matrix(Cmm) Z = 0 X = array(0 c(mm20)) for (k in 1:20) { Z = Z^2+C X[k] = exp(-abs(Z)) } image(X[k] col=tim.colors(256)) # show final image in R write.gif(X “Mandelbrot.gif” col=tim.colors(256) delay=100) Code credit goes to Jarek Tuszynski PhD.  Here's a full example on making an animated GIF ""movie"" from an HDF5 file. The data should be an HDF Dataset of a 3 dimensional array [Nframes][Nrows][Ncolumns]. # # be sure to be run as Administrator to install new packages # source(""http://bioconductor.org/biocLite.R"") biocLite(""rhdf5"") install.packages('caTools') install.packages('fields') library(caTools) library(fields) library(rhdf5) x = h5read(file=""mydata.h5""name=""/Images"") write.gif(x""movie1.gif""col=rainbowdelay=10flip=TRUE)  I'm not sure it is possible in R. I did a project once when data points from R were exported to a MySQL database and a Flex/Flash application picked up those data points and gave animated visualizations.. This is a clever technique Srirangan. I learned many years ago that when someone says 'it's not possible' they mean 'I don't know how to do it'. The clever part of the technique is that in a forum such as SO someone is bound to tell you how to do it. I'm not being sarcastic by the way. I REALLY think it's a good technique and I'm going to try it out. Thanks Srirangan. Sure. But it is still essentially the same thing that I said. R can't do it and you are depending on an external application to do so. I citied the case where I had used Flex/ActionScript Ryan recommended the the use of ImageMagick but in the end you are dependent on an external app. That was my point. I was nowhere claiming that my way was the only way to do it. ;) You don't need a database. In a loop save all your images. Then use a command-line tool to stitch them together; imagemagick is one possibility. Yes this was the easiest way. I guess due to OS modularity it really isn't possible to do this within R unless R is compiled with a special library or such.  Here is one method I found using R help: To create the individual image frames: jpeg(""/tmp/foo%02d.jpg"") for (i in 1:5) { my.plot(i) } dev.off() To make the movie first install ImageMagick. Then call the following function (which calls ""convert"" part of ImageMagick I suppose): make.mov <- function(){ unlink(""plot.mpg"") system(""convert -delay 0.5 plot*.jpg plot.mpg"") } Or try using the ffmpeg function as described in this article (I've found this gives cleaner results): ffmpeg -r 25 -qscale 2 -i tmp/foo%02d.jpg output.mp4 May require a bit of tinkering but this seemed pretty simple once everything was installed. Of course anywhere you see ""jpg"" or ""jpeg"" you can substitute GIF or PNG to suit your fancy. You can even keep jpeg() and dev.off() outside the loop -- if you use an appropriate filename as e.g. jpeg(""/tmp/foo%02d.png"") R will simply create new files during your loop. No need for you to compute the filename. Makes it even easier. You should make Dirk's fix and then accept your own answer. Good solution.  Take a look at either the animation package created by Yihui Xie or the EBImage bioconductor package (?animate).  Here's an example of using Flash to animate graphics created in R: Animate R Graphics with Flash.  I've done some movies using XNview's (freeware graphics viewer) Create Slideshow function. I wanted to show trends through time with spatial data so I just created a series of plots named sequentially [paste() is your friend for all sorts of naming calistethics] then loaded them into XNviews slideshow dialogue and set a few timer variables voila. Took like 5 minutes to learn how to do it and produce some executable graphics. XnView is freeware not open source. I fixed it thanks.  If you wrap your R script within a larger Perl/Python/etc. script you can stitch graphs together with your favorite command-line image stitching tool. To run your R script with a wrapper script use the R CMD BATCH method. Why do you need another language to use a command-line tool? Well where does require a Perl/Python script? Also look at Rscript (and littler) as better alternatives to 'R CMD BATCH'. You don't need another language. You can use a shell like bash. Whatever you want. There are lots of options. I use R CMD BATCH because it is more or less universal across platforms. I found that it is pretty easy once ImageMagick and ffmpeg are installed."
619,A,"Resizing an OpenGL window causes it to fall apart For some reason when I resize my OpenGL windows everything falls apart. The image is distorted the coordinates don't work and everything simply falls apart. I am sing Glut to set it up. //Code to setup glut glutInitWindowSize(appWidth appHeight); glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA); glutCreateWindow(""Test Window""); //In drawing function glMatrixMode(GL_MODELVIEW); glLoadIdentity(); glClear(GL_COLOR_BUFFER_BIT); //Resize function void resize(int w int h) { glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } The OpenGL application is strictly 2D. This is how it looks like initially: http://www.picgarage.net/images/Corre_53880_651.jpeg this is how it looks like after resizing: http://www.picgarage.net/images/wrong_53885_268.jpeg The images are no longer available. You should not forget to hook the GLUT 'reshape' event: glutReshapeFunc(resize); And reset your viewport: void resize(int w int h) { glViewport(0 0 width height); //NEW glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluOrtho2D(0 w h 0); } A perspective projection would have to take the new aspect ratio into account: void resizeWindow(int width int height) { double asratio; if (height == 0) height = 1; //to avoid divide-by-zero asratio = width / (double) height; glViewport(0 0 width height); //adjust GL viewport glMatrixMode(GL_PROJECTION); glLoadIdentity(); gluPerspective(FOV asratio ZMIN ZMAX); //adjust perspective glMatrixMode(GL_MODELVIEW); } And replace the C-style cast with static_cast<> since you're working with C++ :)"
620,A,"what are sparse voxel octrees? I have reading a lot about the potential use of sparse voxel octrees in future graphics engines. However I have been unable to find technical information on them. I understand what a voxel is however I dont know what sparse voxel octrees are or how are they any more efficient than the polygonal techniques in use now. Could somebody explain or point me to an explanation for this? an octree has 8 neighbors because if you imagine a square that was cut into 4 equal quarters like so ______________ | | | | | | |_____|______| | | | | | | |_____|______| then it would be a ""quad""(four)-tree. but in 3 dimensions you have yourself a cube rather then a square so cutting it horizontally vertically and along the Z axis you'll find 8 chunks rather then 4 like so  _____________ / / / | /-----/-----/ | /_____/_____/ | | | | | |/| |-----|-----|/| | | | | |/ |_____|_____|/ hope that makes since.. what makes the SVO unique is that it stores Voxel information which is a point in space that has properties such as Color Normal etc.. the idea behind SVO is to ignore triangles and the need of textures by putting them together into a single SVO which contains the Voxelized Triangle Hull(the Model) and its surface textures all in one object. The reason a Octree is needed here is that otherwise a uniform grid structure would require far to much memory for existing graphics cards to handle.. so using the SVO allows for a sort of Mip-Mapped 3D Texture.. MipMapping basically is the same image but at difference scales one which has more detail and the latest which has the least detail(but look fairly similar from a distance) that way near objects can stream from the SVO with greater detail while further objects stream with less detail.. that is if you're using Ray-Casting.. the further away the ray from the camera the less we dig into our Mega-Texture/SVO But if you think outside the box like ""Euclideon"" with its ""unlimited-detail"" you would just use frustum slicing and plane/aabb intersection with projected UV of our sliced billboard for finding each texels color on the screen opposed to Width*Height pixels shooting out rays with nvidia's naive ""beam optimizations"". PS(sorta off topic): for anyone who doesn't understand how Euclideon does their shi I believe thats the most practical solution and I have reason to back it up(that they DO NOT use ray casting) The biggest mystery they have isn't rendering but storing their data.. RLE simply doesn't cut it.. because some volume/voxel data may be more random and less ""solid"" where RLE is usless also compression of which for me typically requires at least 5 bytes into anything less. they say they output roughly half of what is put in) through their compression.. so they're using 2.5 bytes which is about the same as a Triangle now-adays  actually the 1.15 bits make me suspect they just store things sequentially in some brilliantly simple way. that is if they're only storing the volume data and not things like colour or texture data as well. think about it like this: 1 voxel only needs to be 1 bit: is it there or is it not there? (to be or not to be in other words :P). the octree node it's in is made of 8 voxels and a bit to store whether the thing contains anything at all. that's one bit per voxel plus one per 8. 1 + 1/8 = 1125. add another parent node with 7 siblings and you get 1 + 1/8 + 1/8/8 = 1140625. suspiciously close to the 1.15 they mentioned. although i'm probably way off it may give someone some clue. 1+1/8+1/8/8+1/8/8/8+1/8/8/8/8+1/8/8/8/8/8+1/8/8/8/8/8/8+1/8/8/8/8/8/8/8+1/8/8/8/8/8/8/8/8+1/8/8/8/8/8/8/8/8/8 = 1.14285714179277420043 did some more calculations 1.14285714285714285705 is the most accurate value I get with GNU's basic calculator and mathlib. I guess we can work with that.  Here's a snippet about id Software on this subject. id Tech 6 will use a more advanced technique that builds upon the MegaTexture idea and virtualizes both the geometry and the textures to obtain unique geometry down to the equivalent of the texel: the Sparse Voxel Octree (SVO). It works by raycasting the geometry represented by voxels (instead of triangles) stored in an octree. The goal being to be able to stream parts of the octree into video memory going further down along the tree for nearby objects to give them more details and to use higher level larger voxels for further objects which give an automatic level of detail (LOD) system for both geometry and textures at the same time. Also here's a paper on this. Found more information in this great blog entry. Well voxels alone are not that interesting because for any reasonably detailed modeled you would need extremely huge amounts of voxels (if using an uniform grid). So a hierarchical system is needed which brings us to octrees. An octree is a very simple spatial data structure which subdivides each node into 8 equally large subnodes. A sparse octree is an octree where most of the nodes are empty similar to the sparse matrices that you get when discretizing differential equations i read that before but that really doesnt give any hard technical details. like what is 'sparse' about the trees? why octree and not binary tree or 'hexa' tree? Updated with a paper on this. They are called octree because the space is divided into 8 equally sized subspaces each time. Schnaader - yes but why 8 why not any other number. Olafur - nice paper but it does not seem to describe sparse voxels. An octree node is subdivided by inserting 3 planes that are orthogonal and each perpendicular to one of the three basis vectors x y & z and pass through the center of the volume being divided (for uniform octrees). A volume divided by 3 orthoganal planes has 8 subvolumes. pdeva: Look at the papers/presentation linked in the blog post -- the GigaVoxel paper and the ""Beyond programmable shading"" presentation have all the ""hard technical"" data you are looking for. Hey the paper link is dead as of 19/04/2012 that I checked. Can you share a new link or the title of the paper? Even at a single bit per voxel - a 4096x4096x4096 scene is 8gb... @JustinMeiners no. only if you use a grid system. with an octree your scene would be only as big as you need for your object.  A nVidia whitepaper describes it very detailed~ http://www.nvidia.com/object/nvidia_research_pub_018.html  You can even simply raster all the points you needent raytrace or raycast these days since video cards can project obscene amounts of points. You use an octree because its a cube shape continually dividing making smaller and smaller cubes. (voxels) I have an engine on the way now using a rastered technique and its looking good. For those that say you cant animate voxels I think they really havent thought much about the topic of course its possible. As I see it making the world is alot like ""infinite 3d-coat"" so look up 3d coat and the level design will be very similar to the way this program works. Main draw backs involve streaming speed not being fast enough the raytracing or rastering not quite making 60 fps and plotting the actual voxel objects is very computationally expensive at the moment I can plot a 1024x1024x1024 sphere in about 12 seconds But all these problems could be remedied its an exciting future. My maximum world size at the moment is a meg by a meg by a meg but I actually might make it 8 times bigger than this. Of course the other problem which is actually quite serious is it takes about 100 meg to store a 8192x8192x8192 character even after compression so an environment will be even more than this. Even though saying your going to have characters 8192x8192x8192 is completely absurd compared to what we see in games today... an entire world used to be 8192x8192x8192 :) How you do it by only storing bits per pointer is the pointers are constructed at runtime in video memory... get your mind around that and you could have your own engine. :)"
621,A,"How do bezier handles work? On Wikipedia I found information about bezier curves and made a function to generate the inbetween points for a bezier polygon. I noticed that Expression Design uses bezier handles. This allows a circle to be made with 4 points each with a bezier handle. I'm just not sure mathematically how this works in relation with the formula for bezier point at time T. How do these handle vectors work to modify the shape? Basically what's there relation to the bezier formula? Thanks You do know that a Bezier can only *approximate* a circle right? Here's one explanation: http://www.cgafaq.info/wiki/Bezier_Circle. I know I've seen a better analysis but I can't locate it at the moment. The 4 points of a Bezier segment are the two endpoints of the segment and two handles one per endpoint. The handles determine the initial direction of the line as it leaves the endpoint. The distance from the handle to the endpoint determines the amount of ""pull"" that the handle exerts on the path. Often you will find multiple Beziers connected end to end with the endpoint of one being shared as the starting point of the next. This guarantees an unbroken curve. If the handles on either side of a point are directly across from each other the angle at the joint will match up; if the handles are also the same distance from the point the angle will be completely smooth and there will not be a visible discontinuity at the point. An interesting property of Bezier segments is that the curve will fit entirely within the parallelogram defined by the 4 points. What I have been describing is the most common form of Bezier the cubic. There is also a quadratic which only has a single handle between the two endpoints; the most common application is TrueType fonts.  Basically the 4 points used in the cubic bezier formula are the 2 points the curve is between plus the two points of the handles on that ""side"" of the first two points (1 handle from each of the first points). If there are double handles on each point the handles on the ""opposite"" side of the points from the curve currently being calculated are ignored (they're used for generating the curve that comes out of the opposite side). The actual generation method used for cubic bezier curves is outlined on the Wikipedia page you linked in your question. Thanks I think I should be good to go now!"
622,A,"How to determine width of a string when printed? I'm creating a custom control part of which is using the Graphics class to draw text to the form. Currently I'm using the following code to display it: private float _lineHeight { get { return this.Font.Size + 5; } } private void Control_Paint(object sender PaintEventArgs e) { Graphics g = this.CreateGraphics(); Brush b = new SolidBrush(Colors[7]); g.DrawString(""Hello World!"" this.Font b 0 2); g.DrawString(""This has been a test of the emergency drawing system!"" this.Font b 0 2 + _lineHeight); } I'm currently using fixedwidth fonts and I'd like to know how wide the font will display but there doesn't appear to be any properties for this sort of information. Is there some way of obtaining it? I want it so I can wrap lines properly when displayed. Yes you can use MeasureString from the Graphics class This method returns a SizeF structure that represents the size in the units specified by the PageUnit property of the string specified by the text parameter as drawn with the font parameter. private void MeasureStringMin(PaintEventArgs e) { // Set up string. string measureString = ""Measure String""; Font stringFont = new Font(""Arial"" 16); // Measure string. SizeF stringSize = new SizeF(); stringSize = e.Graphics.MeasureString(measureString stringFont); // Draw rectangle representing size of string. e.Graphics.DrawRectangle(new Pen(Color.Red 1) 0.0F 0.0F stringSize.Width stringSize.Height); // Draw string to screen. e.Graphics.DrawString(measureString stringFont Brushes.Black new PointF(0 0)); } Ah in the Graphics class... Good call I was looking high and low in the Font class for something."
623,A,"How can I modify a bitmap and use it later without storing or drawing I'm trying to do some ocr by myself in C#. I originally come from Java and this is my first ""project"" with C# So I know how you can make different ColorMatrizes to draw a processed bitmap in your application. I also do have those but I want to use the processed picture to better analyze a picture. Those are my methods to get a ImageAttribute public static ImageAttributes ToGrayscale(Bitmap b) public static ImageAttributes ToNegative(Bitmap b) public static ImageAttributes ToSepia(Bitmap b) public static ImageAttributes SetBrightness(Bitmap b float Brightness) public static ImageAttributes SetContrast(Bitmap b float Contrast) This is my method to draw it Graphics g = this.CreateGraphics(); g.DrawImage(bmpnew Rectangle(0 0 bmp.Width bmp.Height) 0 0 bmp.Width bmp.Height GraphicsUnit.Pixel ImageAnalysis.ToGrayscale(bmp)); g.Dispose(); This is what I want: FindLines( setConrast(toGrayscale(bmp)200) ) But I found no method to save the changes permanently to bitmap object. Maybe someone did this before and can help me Instead of drawing to the screen with this Graphics g = this.CreateGraphics(); You create a new bitmap and then you draw onto that bitmap using a Graphics object obtained like this Bitmap bmpNew = new Bitmap( width height ); Graphics g = Graphics.FromBitmap( bmpNew ); thanks i did this before when I tried to draw a rectangle over a the bmp (e.g. marking found lines for debuggin) but didn't realize this would work the same.. Thanks for your help"
624,A,How can I add graphic elements in android using code? How can I add graphic elements in android without the use of drag it into main.xml? I mean using code to make them You can take a look at this tutorial for starters: create Android UI in Java code
625,A,"Is it reasonable to use John Resig's Processing.js? I am thinking about making a website with some fairly intense JavaScript/canvas usage and I have been looking at Processing.js and it seems to me that it would make manipulating the canvas significantly easier. Does anyone know any reasons why I shouldn't use Processing.js? I understand that older browsers won't be able to use it but for now that's ok. Have you looked at pure Javascript canvas libraries like [Fabric.js](http://fabricjs.com)? As mentioned IE is not supported by Processing.js (including IE8 beta). I've also found processing.js to be a bit slow in terms of performance compared to just using canvas (especially if you're parsing a string with Processing language instead of using the javascript API). I personally prefer the canvas API over the processing wrapper because it gives more me control. For example: The processing line() function is implemented like this (roughly): function line (x1 y1 x2 y2) { context.beginPath(); context.moveTo(x1 y1); context.lineTo(x2 y2); context.closePath(); context.stroke(); }; And you'd use it like this (assuming you're using the javascript-exposed API): var p = Processing(""canvas"") p.stroke(255) ////Draw lines.../// p.line(001010) p.line(10102010) //...and so on p.line(100100200200) ////End lines//// Notice that every line() call has to open and close a new path whereas with the canvas API you can draw all the lines within a single beginPath/endPath block improving performance significantly: context.strokeStyle = ""#fff""; context.beginPath(); ////Draw lines.../// context.moveTo(0 0); context.lineTo(10 10); context.lineTo(20 10); //...so on context.lineTo(200 200); ////End lines.../// context.closePath(); context.stroke(); When I tried this out many examples also did not work in Chrome Safari. This should be fixed.. That's a rather contrived example isn't it? In Processing you'd do the same thing with beginShape() vertex() and endShape().  If you're OK with it not working in IE7 then go for it. I've had it working in Firefox 3. It's a slick way to bring Silverlight/Flash effects to your page. My hunch is that libraries like Processing.js will change or be upgraded on a fast track path so get ready to run when they do and keep up with the new features.  It doesn't simplify drawing on your canvas. What it does do is simplify the task of animation if you are using canvas. If you are doing animation and you don't care about full browser support then use Processing.js. If you are not doing animation (if you are doing charting or rounded corners for example) then don't add the overhead of Processing.js. Either way I recommend that you learn how to use the canvas API directly. Understanding the canvas api especially transformations will greatly help you even if you are using Processing.js.  I'd say use Flash instead. More browsers have Flash installed than the number of browsers that work with processing.js. In addition you'll get much better performance from Flash versus using JavaScript (at least for now though there are projects in the works to speed up JS a lot but it's still a little ways off) Ideally Flash Silverlight et al should all be killed off by (at the very least de-facto) standardised in-DOM content and functionality."
626,A,shape back color I have created a User Control using graphic's fill property. It's a circle. It's like a speedometer hence it has many colors and filled using different shapes. I need to have it's empty space to have back color to some thing other than the default color. How to change the back color of the graphic's shape??? Did my last post answer this question? or do you still need more help? Check out Pens Brushes and Colors Graphics g = this.CreateGraphics(); SolidBrush myBrush = new SolidBrush(Color.Red); g.FillEllipse(myBrush ClientRectangle); A more complete example. This will fill the background of the UserControl (based on the size of the user control) to Pink and create a small circle that is filled with an orange backgroung color.  protected override void OnPaint(PaintEventArgs e) { Graphics g = e.Graphics; SolidBrush sb = new SolidBrush(Color.Pink); g.FillRectangle(sb e.ClipRectangle); sb.Dispose(); sb = new SolidBrush(Color.Orange); g.FillEllipse(sb 20 20 20 20); sb.Dispose(); } Is this more what you are looking for? If you want the background color to be the same as the user control change this: SolidBrush sb = new SolidBrush(this.BackColor); //Color.Pink); If i'm not wrong that will draw the shape in RED color. I have drawn the shape in circle it's like a speedometer but I want to set that empty space that is having form color to another color. For example. Graphics g = this.CreateGraphics(); SolidBrush myBrush = new SolidBrush(Color.Red); g.FillEllipse(myBrush ClientRectangle); it creates a rectangle in red shape but inside that rectangle I want some other color. Hope it's clear... Ok I updated my example let me know if thats what you are looking for. Hey that's great I'll try with my code and let you know thanks. Hey that worked!!!! It's great I needed that only Superb!!! :-) Thank you:-):-)
627,A,"drawing Graphics performance issue i am trying to create slider movement graphics. The code explain better this. using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Text; using System.Windows.Forms; using System.Drawing.Drawing2D; namespace temp { public partial class Form1 : Form { public Form1() { InitializeComponent(); } private System.Drawing.Graphics g; private System.Drawing.Pen pen1 = new System.Drawing.Pen(Color.Black 1F); //timer for animation private void Form1_Load(object sender EventArgs e) { Timer a = new Timer(); a.Interval = 60; a.Tick += new EventHandler(a_Tick); a.Start(); } void a_Tick(object sender EventArgs e) { button1_Click(this null); } //draws randomly generated point array. int cnt = 0; private void button1_Click(object sender EventArgs e) { rAr(); g = pictureBox1.CreateGraphics(); g.Clear(Color.Violet); g.PixelOffsetMode = PixelOffsetMode.HighQuality; g.SmoothingMode = SmoothingMode.HighQuality; g.DrawCurve(pen1 points2); cnt++; } Random r = new Random(); Point[] p = new Point[100]; Point[] points2 = new Point[100]; int c = 4; //fills new random point array private void rAr() { points2.CopyTo(p 0); int cc = 1; for (int i = points2.Length - 1; i > 0; i--) { points2[i - 1] = new Point(100 - cc p[i].Y); cc++; } points2[99] = new Point(100 r.Next(1 50)); } } } this code works fine put the problem is that it takes to much cpu and automation is not very smooth. is there some other way to achieve same thing put with much less cpu. Thanks for replays Apart from you not disposing the graphics object and the somewhat odd construct with the timer calling the button_click event handler I don't really see the problem. The program consumes less than 1 % CPU on my laptop. (Premature) Optimisations In addition to the other answers already posted you are attempting to draw a spline curve through 100 points. This is probably a bad idea for two reasons: Firstly fitting a curve to 100 points isn't a particularly fast thing to do under any circumstances. Secondly as the points are all 1 pixel apart there is likely to be no visual benefit to using a curve. Therefore there are several more things you could do that might improve the speed: Try using a PolyLine rather than a curve If you need to use a curve then you probably don't need to use 100 points across 100 pixels of the drawing - using 10 or 20 positions across that width may give you better results and will significantly reduce the curve-fitting work that .net has to do. You may be able to also remove some intermediate points that lie on the line/curve so that you have fewer lines to draw. e.g. if you have points at (1057) (11 57) (1257) then you can drop the middle point and just draw a straight line from (1057) to (1257). Wrong Algorithm? But wait! Before you optimise the code - is it actually the right solution to the problem? It sounds as if the content is not meant to ""change"" but simply scroll sideways. In which case only the new pixels introduced at one side of the image are actually ""changing"" with each frame - the rest simply move sideways. In that case you can scroll (move) the region of the graphic that contains the ""old"" curve image and then just draw the extra pixel or two that have ""scrolled into view"". Scrolling in this manner can be achieved in several different ways (e.g. blitting in the graphics or if the content of the entire window is scrolling by using windows Form scrolling commands) In the same way if the data is not changing but simply ""scrolling"" then rebuilding the array for every frame is unnecessarily expensive. Using a circular buffer you could simply add a new point to the ""end"" of the array and delete one from the beginning without having to reallocate the array or copy all the intervening points at all. So it's all about using the right tool for the job.  You should move all of the graphics code in Button1 to your picturebox's paint event something like this:  private void button1_Click(object sender EventArgs e) { rAr(); pictureBox1.Invalidate(); cnt++; } private void picturebox1_Paint(object sender PaintEventArgs e) { using graphics g = e.Graphics() { g.Clear(Color.Violet); g.PixelOffsetMode = PixelOffsetMode.HighQuality; g.SmoothingMode = SmoothingMode.HighQuality; g.DrawCurve(pen1 points2); } } It's the call to CreateGraphics() thats killing you. Andy's suggestion is the way to go because creating a new Graphics object each time is expensive. You could go one step further and encapsulate all this into a UserControl and override the usercontrol's OnPaint method for a couple less stack calls. Also you should set the controls style or the forms style to DoubleBuffering for less flicker. You might also consider rewriting your rAr function (at least if you will use more points or smaller intervals later) and improve it f.e. using a ringbuffer but it might not be worth it - drawCurve perhaps eats much more CPU cycles."
628,A,"How to transition between two images using a grayscale transition map Imagine you have two images A and B and a third grayscale image T. A and B contain just about anything but let's assume they're two scenes from a game. Now assume that T contains a diamond gradient. Being grayscale it goes from black on the outside to white on the inside. Over time let's assume 256 not further elaborated on ""ticks"" to match the grayscales A should transition into B giving a diamond-wipe effect. If T instead contained a grid of smaller rectangular gradients it would be like each part of the image by itself did a rectangular wipe. You might recognize this concept if you've ever worked with RPG Maker or most visual novel engines. The question ofcourse is how this is done. I know it involves per-pixel blending between A and B but that's all I got. For added bonus what about soft edges? And now the conclusion Final experiment based on eJames's code I'd say you start with image A then on every step I you use the pixels of image A for every position where T is smaller than I and pixels of the image B otherwise. For a soft edge you might define another parameter d and calculate you pixels P like so: For every point (xy) you decide between the following three options: I < T(xy) - d then the point is equal to the point of A T(xy) - d <= I < T(xy) + d then let z = I - (T(xy) -d) and the point is equal to A(xy)(1-z/(2d)) + B(xy)(z/(2d)) I < T(xy) + d then the point is equal to the point of B This produces a linear edge of course you can chose between an arbitrary number of functions for the edge.  The grayscale values in the T image represent time offsets. Your wipe effect would work essentially as follows on a per-pixel basis: for (timeIndex from 0 to 255) { for (each pixel) { if (timeIndex < T.valueOf[pixel]) { compositeImage.colorOf[pixel] = A.colorOf[pixel]; } else { compositeImage.colorOf[pixel] = B.colorOf[pixel]; } } } To illustrate imagine what happens at several values of timeIndex: timeIndex == 0 (0%): This is the very start of the transition. At this point most of the pixels in the composite image will be those of image A except where the corresponding pixel in T is completely black. In those cases the composite image pixels will be those of image B. timeIndex == 63 (25%): At this point more of the pixels from image B have made it into the composite image. Every pixel at which the value of T is less than 25% white will be taken from image B and the rest will still be image A. timeIndex == 255 (100%): At this point every pixel in T will negate the conditional so all of the pixels in the composite image will be those of image B. In order to ""smooth out"" the transition you could do the following: for (timeIndex from 0 to (255 + fadeTime)) { for (each pixel) { blendingRatio = edgeFunction(timeIndex T.valueOf[pixel] fadeTime); compositeImage.colorOf[pixel] = (1.0 - blendingRatio) * A.colorOf[pixel] + blendingRatio * B.colorOf[pixel]; } } The choice of edgeFunction is up to you. This one produces a linear transition from A to B: float edgeFunction(value threshold duration) { if (value < threshold) { return 0.0; } if (value >= (threshold + duration)) { return 1.0; } // simple linear transition: return (value - threshold)/duration; } Congratulations! Precomputing the blending ratios and using direct bitmap manipulation made your method usably fast. Glad to hear it :) And thank you for posting your final code. It's great to see an answer get put into practice like that and now other people can learn from the improvements that you've made. Cheers!"
629,A,"Convert string to Brushes/Brush color name in C# I have a configuration file where a developer can specify a text color by passing in a string:  <text value=""Hello World"" color=""Red""/> Rather than have a gigantic switch statement look for all of the possible colors it'd be nice to just use the properties in the class System.Drawing.Brushes instead so internally I can say something like:  Brush color = Brushes.Black; // Default // later on... this.color = (Brush)Enum.Parse(typeof(Brush) prasedValue(""color"")); Except that the values in Brush/Brushes aren't enums. So Enum.Parse gives me no joy. Suggestions? Note that Color and Brush is not the same thing you seem to be mixing them up Recap of all previous answers different ways to convert a string to a Color or Brush: // best using Color's static method Color red1 = Color.FromName(""Red""); // using a ColorConverter TypeConverter tc1 = TypeDescriptor.GetConverter(typeof(Color)); // ..or.. TypeConverter tc2 = new ColorConverter(); Color red2 = (Color)tc.ConvertFromString(""Red""); // using Reflection on Color or Brush Color red3 = (Color)typeof(Color).GetProperty(""Red"").GetValue(null null); // in WPF you can use a BrushConverter SolidColorBrush redBrush = (SolidColorBrush)new BrushConverter().ConvertFromString(""Red""); Mind adding how you'd do this with WPF's XAML as well?  If you want you can extend this even more and allow them to specify values for the R G and B values. Then you just call Color.FromArgb(int r int g int b);  I agree that using TypeConverters are the best method:  Color c = (Color)TypeDescriptor.GetConverter(typeof(Color)).ConvertFromString(""Red""); return new Brush(c);  Try using a TypeConverter. Example: var tc = TypeDescriptor.GetConverter(typeof(Brush)); Another alternative is to use reflection and go over the properties in SystemBrushes. TypeDescriptor cannot convert from string to Brush. It can convert string to Color though...  String to brush: myTextBlock.Foreground = new BrushConverter().ConvertFromString(""#FFFFFF"") as SolidColorBrush; That's my case here! System.Drawing.ColorTranslator has FromHtml() and ToHtml() Also note that this requires .Net 3.0 or later good one! Mayhe!!  D'oh. After a while of looking I found:  Color.FromName(a.Value) After hitting ""post"". From there it's a short step to:  color = new SolidBrush(Color.FromName(a.Value)); I'll leave this question here for others.... Strictly speaking it's a ""coincidence"" that the static properties on Brushes use the same name as the static properties on Color. However that's probably nothing to worry about.  Brush myBrush = new SolidBrush(Color.FromName(""Red"")); +1 An alternative is: Brush aBrush = new SolidBrush(Color.FromArgb(240 240 240));  You could use reflection for this: Type t = typeof(Brushes); Brush b = (Brush)t.GetProperty(""Red"").GetValue(null null); Of course you'll want some error handling/range checking if the string is wrong."
630,A,"Creating a linear gradient in 2D array I have a 2D bitmap-like array of let's say 500*500 values. I'm trying to create a linear gradient on the array so the resulting bitmap would look something like this (in grayscale): The input would be the array to fill two points (like the starting and ending point for the Gradient tool in Photoshop/GIMP) and the range of values which would be used. My current best result is this: ...which is nowhere near what I would like to achieve. It looks more like a radial gradient. What is the simplest way to create such a gradient? I'm going to implement it in C++ but I would like some general algorithm. This is really a math question so it might be debatable whether it really ""belongs"" on Stack Overflow but anyway: you need to project the coordinates of each point in the image onto the axis of your gradient and use that coordinate to determine the color. Mathematically what I mean is: Say your starting point is (x1 y1) and your ending point is (x2 y2) Compute A = (x2 - x1) and B = (y2 - y1) Calculate C1 = A * x1 + B * y1 for the starting point and C2 = A * x2 + B * y2 for the ending point (C2 should be larger than C1) For each point in the image calculate C = A * x + B * y If C <= C1 use the starting color; if C >= C2 use the ending color; otherwise use a weighted average: (start_color * (C2 - C) + end_color * (C - C1))/(C2 - C1) I did some quick tests to check that this basically worked. +1 what is programming if not applied math? =) Maybe I am stupid but I couldn't make it work. It worked for horizontal and vertical gradients but no luck for angled ones. +1 anyways. Just double-checked and in Mathematica it worked perfectly for all angles I tested so I'm fairly positive the formula itself is valid. What trouble did you have? @ka_ka8 `C` is a coordinate that increases in the direction you want the gradient to run. `C1` and `C2` are the values of this coordinate that correspond to the starting and ending points. @David Zaslavsky can you tell me what's going on in your linear gradient drawing algorithm in Section 3 and 4? I do not know what it is C C1 and C2 (which is calculated there?)  I'll just post my solution. int ColourAt( int x int y ) { float imageX = (float)x / (float)BUFFER_WIDTH; float imageY = (float)y / (float)BUFFER_WIDTH; float xS = xStart / (float)BUFFER_WIDTH; float yS = yStart / (float)BUFFER_WIDTH; float xE = xEnd / (float)BUFFER_WIDTH; float yE = yEnd / (float)BUFFER_WIDTH; float xD = xE - xS; float yD = yE - yS; float mod = 1.0f / ( xD * xD + yD * yD ); float gradPos = ( ( imageX - xS ) * xD + ( imageY - yS ) * yD ) * mod; float mag = gradPos > 0 ? gradPos < 1.0f ? gradPos : 1.0f : 0.0f; int colour = (int)( 255 * mag ); colour |= ( colour << 16 ) + ( colour << 8 ); return colour; } For speed ups cache the derived ""direction"" values (hint: premultiply by the mag).  In your example image it looks like you have a radial gradient. Here's my impromtu math explanation for the steps you'll need. Sorry for the math the other answers are better in terms of implementation. Define a linear function (like y = x + 1) with the domain (i.e. x) being from the colour you want to start with to the colour your want to end with. You can think of this in terms of a range the within Ox0 to OxFFFFFF (for 24 bit colour). If you want to handle things like brightness you'll have to do some tricks with the range (i.e. the y value). Next you need to map a vector across the matrix you have as this defines the direction that the colours will change in. Also the colour values defined by your linear function will be assigned at each point along the vector. The start and end point of the vector also define the min and max of the domain in 1. You can think of the vector as one line of your gradient. For each cell in the matrix colours can be assigned a value from the vector where a perpendicular line from the cell intersects the vector. See the diagram below where c is the position of the cell and . is the the point of intersection. If you pretend that the colour at . is Red then that's what you'll assign to the cell.  | c | | Vect:____.______________ | | I was thinking about something along these lines but I hoped I could find some simplier solution. It took me a hour and half to derive the correct formula to calculate the correct point on the line A->B :) Good stuff if you have a chance post it in your question. I'm sure someone will find it useful.  There are two parts to this problem. Given two colors A and B and some percentage p determine what color lies p 'percent of the way' from A to B. Given a point on a plane find the orthogonal projection of that point onto a given line. The given line in part 2 is your gradient line. Given any point P project it onto the gradient line. Let's say its projection is R. Then figure out how far R is from the starting point of your gradient segment as a percentage of the length of the gradient segment. Use this percentage in your function from part 1 above. That's the color P should be. Note that contrary to what other people have said you can't just view your colors as regular numbers in your function from part 1. That will almost certainly not do what you want. What you do depends on the color space you are using. If you want an RGB gradient then you have to look at the red green and blue color components separately. For example if you want a color ""halfway between"" pure red and blue then in hex notation you are dealing with ff 00 00 and 00 00 ff Probably the color you want is something like 80 00 80 which is a nice purple color. You have to average out each color component separately. If you try to just average the hex numbers 0xff0000 and 0x0000ff directly you get 0x7F807F which is a medium gray. I'm guessing this explains at least part of the problem with your picture above. Alternatively if you are in the HSV color space you may want to adjust the hue component only and leave the others as they are. I am working with grayscale only which significantly simplifies color blending."
631,A,"fast algorithm for drawing filled circles? I am using Bresenham's circle algorithm for fast circle drawing. However I also want to (at the request of the user) draw a filled circle. Is there a fast and efficient way of doing this? Something along the same lines of Bresenham? The language I am using is C. I would just generate a list of points and then use a polygon draw function for the rendering. If he has implemented Bresenham for the open version he is working at a lower layer then using an API...either for learning purposes or to *implement* an API.  I like palm3D's answer. For being brute force this is an amazingly fast solution. There are no square root or trigonometric functions to slow it down. Its one weakness is the nested loop. Converting this to a single loop makes this function almost twice as fast. int r2 = r * r; int area = r2 << 2; int rr = r << 1; for (int i = 0; i < area; i++) { int tx = (i % rr) - r; int ty = (i / rr) - r; if (tx * tx + ty * ty <= r2) SetPixel(x + tx y + ty c); } This single loop solution rivals the efficiency of a line drawing solution.  int r2 = r * r; for (int cy = -r; cy <= r; cy++) { int cx = (int)(Math.Sqrt(r2 - cy * cy) + 0.5); int cyy = cy + y; lineDDA(x - cx cyy x + cx cyy c); }  If you want a fast algorithm consider drawing a polygon with N sides the higher is N the more precise will be the circle. That works but is not fast.  Here's a C# rough guide (shouldn't be that hard to get the right idea for C) - this is the ""raw"" form without using Bresenham to eliminate repeated square-roots. Bitmap bmp = new Bitmap(200 200); int r = 50; // radius int ox = 100 oy = 100; // origin for (int x = -r; x < r ; x++) { int height = (int)Math.Sqrt(r * r - x * x); for (int y = -height; y < height; y++) bmp.SetPixel(x + ox y + oy Color.Red); } bmp.Save(@""c:\users\dearwicker\Desktop\circle.bmp""); Or loop on y and draw horizonal lines. Occasionally there is a reason to choose one or the other but in most cases it does not matter. Either way you use the same Bresenham logic to find the endpoints quickly. All those Math.Sqrt's aren't going to be especially fast... No but you can use Bresenham to avoid that. The basic idea is to ""join the dots"" between the upper and lower points at each x coordinate covered by the circle. Profile to see which is best. If there's a difference at all going horizontal should be better. It gets rid of a multiplication by stride and may result in fewer faults. @AakashM - have you tried it? I find it makes almost no measurable difference whether I compute the `Math.Sqrt` or not. (Obviously I've eliminated the `SetPixel` as well which accounts for ALL the time in the complete program; I just add x and y to a counter variable to ensure they're used). ""All those Math.Sqrts aren't going to be especially fast"" - the disassembly shows that the C#/JIT combo emits the inlined SQRTSD instruction on my 64-bit machine and so it's little wonder that it runs blazingly fast. I can't measure a different between Math.Sqrt and a simple addition. So I think your comment is probably based on pre-FP instruction set guessing! This is surprising to me too as it's been a while since I've had to use Sqrt for anything so I've blogged it here: http://smellegantcode.wordpress.com/2009/07/29/square-roots-are-slow/ That perennial problem - what to do with one's carefully-tuned educated-guesswork engine when the fundamentals change? Well depends if you want to learn about history or get the job done - personally I enjoy both! :)  Here's how I'm doing it: I'm using fixed point values with two bits precision (we have to manage half points and square values of half points) As mentionned in a previous answer I'm also using square values instead of square roots. First I'm detecting border limit of my circle in a 1/8th portion of the circle. I'm using symetric of these points to draw the 4 ""borders"" of the circle. Then I'm drawing the square inside the circle. Unlike the midpoint circle algorith this one will work with even diameters (and with real numbers diameters too with some little changes). Please forgive me if my explanations were not clear I'm french ;) void DrawFilledCircle(int circleDiameter int circlePosX int circlePosY) { const int FULL = (1 << 2); const int HALF = (FULL >> 1); int size = (circleDiameter << 2);// fixed point value for size int ray = (size >> 1); int dY2; int ray2 = ray * ray; int posminposmax; int YX; int x = ((circleDiameter&1)==1) ? ray : ray - HALF; int y = HALF; circlePosX -= (circleDiameter>>1); circlePosY -= (circleDiameter>>1); for (;; y+=FULL) { dY2 = (ray - y) * (ray - y); for (;; x-=FULL) { if (dY2 + (ray - x) * (ray - x) <= ray2) continue; if (x < y) { Y = (y >> 2); posmin = Y; posmax = circleDiameter - Y; // Draw inside square and leave while (Y < posmax) { for (X = posmin; X < posmax; X++) setPixel(circlePosX+X circlePosY+Y); Y++; } // Just for a better understanding the while loop does the same thing as: // DrawSquare(circlePosX+Y circlePosY+Y circleDiameter - 2*Y); return; } // Draw the 4 borders X = (x >> 2) + 1; Y = y >> 2; posmax = circleDiameter - X; int mirrorY = circleDiameter - Y - 1; while (X < posmax) { setPixel(circlePosX+X circlePosY+Y); setPixel(circlePosX+X circlePosY+mirrorY); setPixel(circlePosX+Y circlePosY+X); setPixel(circlePosX+mirrorY circlePosY+X); X++; } // Just for a better understanding the while loop does the same thing as: // int lineSize = circleDiameter - X*2; // Upper border: // DrawHorizontalLine(circlePosX+X circlePosY+Y lineSize); // Lower border: // DrawHorizontalLine(circlePosX+X circlePosY+mirrorY lineSize); // Left border: // DrawVerticalLine(circlePosX+Y circlePosY+X lineSize); // Right border: // DrawVerticalLine(circlePosX+mirrorY circlePosY+X lineSize); break; } } } void DrawSquare(int x int y int size) { for( int i=0 ; i<size ; i++ ) DrawHorizontalLine(x y+i size); } void DrawHorizontalLine(int x int y int width) { for(int i=0 ; i<width ; i++ ) SetPixel(x+i y); } void DrawVerticalLine(int x int y int height) { for(int i=0 ; i<height ; i++ ) SetPixel(x y+i); } To use non-integer diameter you can increase precision of fixed point or use double values. It should even be possible to make a sort of anti-alias depending on the difference between dY2 + (ray - x) * (ray - x) and ray2 (dx² + dy² and r²)  Having read the Wikipedia page on Bresenham's (also 'Midpoint') circle algorithm it would appear that the easiest thing to do would be to modify its actions such that instead of setPixel(x0 + x y0 + y); setPixel(x0 - x y0 + y); and similar each time you instead do lineFrom(x0 - x y0 + y x0 + x y0 + y); That is for each pair of points (with the same y) that Bresenham would you have you plot you instead connect with a line. Very clear. does that really work ? i tried it .. it doesn't totally fill the circle. Am I missing something ? In any case there are some correct answers below. @AJed To know if you're missing something we'd need to see your code in [a new question of your own](http://stackoverflow.com/questions/ask) Wouldn't this cause scan lines in the 2nd/3rd and 6th/7th quadrants to be repeated? In those x changes at every step and the decision variable governs y.  Just use brute force. This method iterates over a few too many pixels but it only uses integer multiplications and additions. You completely avoid the complexity of Bresenham and the possible bottleneck of sqrt. for(int y=-radius; y<=radius; y++) for(int x=-radius; x<=radius; x++) if(x*x+y*y <= radius*radius) setpixel(origin.x+x origin.y+y); I like this answer because it solves the problem in a very direct way. The only problem with this is that it generates a different circle than the midpoint circle algorithm. These circles are ""thinner"" than the equivalent in midpoint which appear to be the more correct shape. Any way to fix that? This sounds horrible but I found that in most cases I can get the exact same circle from this algorithm as midpoint if I modify the check slightly. Those times it doesn't exactly match up it's pretty close. The modification to the check is: x * x + y * y <= range * range + range * 0.8f Great answer worked straight without modification  You can use this: void DrawFilledCircle(int x0 int y0 int radius) { int x = radius; int y = 0; int xChange = 1 - (radius << 1); int yChange = 0; int radiusError = 0; while (x >= y) { for (int i = x0 - x; i <= x0 + x; i++) { SetPixel(i y0 + y); SetPixel(i y0 - y); } for (int i = x0 - y; i <= x0 + y; i++) { SetPixel(i y0 + x); SetPixel(i y0 - x); } y++; radiusError += yChange; yChange += 2; if (((radiusError << 1) + xChange) > 0) { x--; radiusError += xChange; xChange += 2; } } }"
632,A,"What do you use as WPF alternative for Win32 Delphi? If you are sticking with Delphi for Win32 what do you use as GUI framework in order to approach the versatility and performance of the WPF framework on .NET? There are some alternatives out there such as DXScene but it appears to have a problem with font clarity. Graphics32 and AGG are excellent low-level libraries but lack a high-level design environment or IDE-plugin. What do you use to implement a modern vector-based GUI? @Domus: I didn't say the situation was good. But you have to realize that MS isn't really caring for ""native"" development any more. There are ever more APIs that are not or only with difficulties accessible from native apps and WPF seems to be one of them. Have you considered a blend of win32 an wpf? It is possible to mix in some wpf in a win32 application. Take a look at tms interop-component (http://www.tmssoftware.com/site/xv.asp) or even more flexible RemObjects Hydra (http://www.remobjects.com/hydra.aspx). I don't think there's going to be any third party solution for general vector-based UIs on Windows [1] it will have to come from MS and will have to be supported by other vendors. Since MS won't do that for native code it looks like you're out of luck. [1]: And IMHO this is a good thing for user interface consistency at least. So freedom of UI design to .NET developers but better maintain old user interface consistency for native coders? Okay. @mghie - this is complete nonesense. Microsoft said they they believed managed code would become more dominant but the reality is quite the reverse. To quote from: ttp://windowsteamblog.com/blogs/developers/archive/2009/11/18/new-windows-api-code-pack-version.aspx Windows 7 offers new features like the taskbar libraries and the Sensor and Location platform to name a few. ... All these great features are exposed via the Win32 native API. Currently there is no easy way to use these features from managed code applications. @Vegar: No on one hand I don't want to ditch Delphi's advantage of being able to deploy a single exe without too many dependency on external libraries although .NET is pretty much common stuff right now. On the other I'm looking for an alternative to WPF really. Last year I would have answered about DXScene - see also this question. But now wait some weeks and try FireMonkey which is DXScene bought and integrated into XE2 by Embarcadero! Just one huge point: WPF is Windows only whereas FireMonkey is cross-platform - it will run under Mac OS X and even iOS. And in the next XE3 version it should also be able to create desktop Linux application (just as DXScene did). It uses Direct X under Windows and Open GL on other platforms (even iOS). It's not VCL-based not ""native control"" based but draws its own vectorial controls using a ""all-is-container"" modular approach. Thanks to an internal ""theming"" engine it is able to render controls as native under Windows or Mac OS X. It won't use a XAML language both some controls on standard IDE forms or runtime-created controls if needed.  Sorry Domus but WPF is not known as a ""performance"" framework also isn't hardware accelerated. WPF is a layer over windows 3.1 common controls. In the other hand Direct2d is hardware-accelerated 2-D graphics API that provides high performance and high-quality rendering for 2-D geometry bitmaps and text. And is available in Delphi 2010. http://msdn.microsoft.com/en-us/magazine/dd861344.aspx @Francis I'm not sure what the author of this reply meant when he wrote it but WPF 4 will use Direct2D. @Pavel WPF will not use Direct2D directly it will use its own internal retained-mode API (MIL). This impact directly in performance and memory usage compared with C++ or any native code talking directly to direct2d Pardon? WPF is layered on top of Direct 3D. Windows 3.1 common controls?!! WPF is a new code base that makes heavy use of the graphics card. Maybe you're mixing up WPF with Windows Forms which is just a layer on top of Windows?! Direct2D uses an immediate-mode calling pattern (like Direct3D). WPF is a managed code framework which uses a retained-mode calling pattern. Please look at your processor usage while running an WPF application.  I use VCL and DevExpress for all my GUI needs. DevExpress also have a Skin Library if you need a more fancy UI. No no controls. Just a vector-based GUI. If I may ask why do you insist on vector-based UI? What's the gain? Just curious. Scaling rotation complex figure rendering. Basically all controls that are commonly used today were introduced over twenty years ago. The gain is an improved interaction experience with ""controls"" tailored to the task and around the user's mental model. Total freedom of design. Etc. Well that's a good enough reason. I'm happy with Delphi's offerings. But if you follow the saying ""Choosing the right tool for the job"" than maybe Adobe AIR would be the best choice for the job. Net adds to much overhead when compared to AIR. Thanks Mihaela. Do you have any experience with Adobe AIR? Not much I just played with it. But there's a big pile of questions and answers here on that subject. Good luck.  The Delphi VCL still works just fine for me and without WPF's infamous ""learning cliff"". The Delphi VCL doesn't offer any vector-based GUI development -- and not integrated in the IDE. I'm not advocating WPF just looking for an alternative on Win32. This is completely informal and not representing anything official in any way but during CodeRage I had a conversation with David I and among other things we discussed the possibility of an OpenGL-based vector-graphics visual control system in conjunction with cross-platform plans for Delphi. So what does that mean? Absolutely nothing except that David I is thinking about the idea and we might maybe see something like that in a few years.  Actually there are no libs which will allow you to use WFP directly in Delphi. I would suggest you to try and use AGG and build your own lib for such tasks or use AGG's commercial descendant. Well I don't want to use WPF. I'm looking for an alternative library with IDE-plugin. AGG is a 3D library? No AGG is a 2D Library with sub-pixel facility. http://www.aggpas.org/ AGG (and AggPas too) doesn't depend on any graphic API or technology. Basically you can think of AGG as of a rendering engine that produces pixel images in memory from some vectorial data. But of course AGG can do much more than that."
633,A,"Java Graphics not displaying on successive function calls why? I'm making a visualization for a BST implementation (I posted another question about it the other day). I've created a GUI which displays the viewing area and buttons. I've added code to the BST implementation to recursively traverse the tree the function takes in coordinates along with the Graphics object which are initially passed in by the main GUI class. My idea was that I'd just have this function re-draw the tree after every update (add delete etc...) drawing a rectangle over everything first to ""refresh"" the viewing area. This also means I could alter the BST implementation (i.e by adding a balance operation) and it wouldn't affect the visualization. The issue I'm having is that the draw function only works the first time it is called after that it doesn't display anything. I guess I don't fully understand how the Graphics object works since it doesn't behave the way I'd expect it to when getting passed/called from different functions. I know the getGraphics function has something to do with it. Relevant code:  private void draw(){ Graphics g = vPanel.getGraphics(); tree.drawTree(gORIGINORIGIN); } vPanel is what I'm drawing on private void drawTree(Graphics g BinaryNode<AnyType> n int x int y){ if( n != null ){ drawTree(g n.left x-10y+10 ); if(n.selected){ g.setColor(Color.blue); } else{ g.setColor(Color.gray); } g.fillOval(xy2020); g.setColor(Color.black); g.drawString(n.element.toString()xy); drawTree(gn.right x+10y+10); } } It is passed the root node when it is called by the public function. Do I have to have: Graphics g = vPanel.getGraphics(); ...within the drawTree function? This doesn't make sense!! Thanks for your help. This is not the right way of doing it. If you want a component that displays the tree you should make your own JComponent and override the paintComponent-method. Whenever the model (the tree / current node etc) changes you invoke redraw() which will trigger paintComponent. I actually don't think you are allowed to fetch the Graphics object from anywhere else than the argument of the paintComponent method. Try out the following program import java.awt.Graphics; public class FrameTest { public static void main(String[] args) { final JFrame f = new JFrame(""Frame Test""); f.setContentPane(new MyTreeComponent()); f.setSize(400 400); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); f.setVisible(true); new Thread() { public void run() { for (int i = 0; i < 10; i++) { try { sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } f.repaint(); } } }.start(); } } class MyTreeComponent extends JComponent { public void paintComponent(Graphics g) { // Draw your tree. (Using random here to visualize the updates.) g.drawLine(30 30 50 30 + new Random().nextInt(20)); g.drawLine(30 30 50 30 - new Random().nextInt(20)); } } The best starting point is probably http://java.sun.com/docs/books/tutorial/uiswing/painting/index.html So you're saying I should extend the JComponent class and put my drawing code in the paintComponent method which can then be called with redraw? Edit: didn't see your last edit. Yes. That's exactly the way to go :) It's along with the model-view-controller way of thinking: The tree and the current selection etc constitutes the model and the component is the view. Preferably you listen for changes in the model (clicks in the tree etc) and then call `repaint()`. (**Don't** call `paintComponent` yourself! You wouldn't be able to hand it a proper `Graphics`-object.) I should have clarified that I'm not doing updates in real time or interacting with the tree directly with the mouse I'm just using buttons to add delete etc... from the tree. I just need a way to keep the image matched up with the actual tree structure. It seems like I could still do this through the method you describe I'm just not familiar doing it this way. I see. This is still the way to go though. At the same place where you change the data that the visualization depends on and varies with you should invoke `repaint`. Yes I think I sort of understand now the Graphics object can't be passed as an argument into any function with the exception of paintComponent. It seems like I need to move the drawTree function out of the BST implementation (probably wasn't a good idea in the first place) in order to make it work. My original idea had been to modify the BST so I could pull out individual nodes allowing me to do the traversal in the GUI but this would have required more modification of the BST implementation. Yes. You seem to have understood the basic concept :-) Though you may very well pass around the Graphics object as you wish but you must keep in mind that the drawing can never be initialized by you (you wouldn't know what graphics-object to use for drawing!) The drawing can only be initiated by the Swing system however you may *ask* the swing system to initiate the drawing (or calling paintComponent with a proper graphics object) by calling repaint. So basically the key is that if in order to have my own custom drawing function like I wrote above I have to define it in paintComponent and call it with repaint in order for it to work the way I want it to. Yes. Now you got it. Except I would phrase it like ""I have to tell swing to call paintComponent (by calling repaint)"" :) +1 @aioobe: I understand it's an example but shouldn't `repaint()` be invoked on the EDT say via `EventQueue.invokeLater()`?  @aioobe's approach is sound and the example is compelling. In addition to the cited tutorial Performing Custom Painting I would add that drawing should take place on the Event Dispatch Thread (EDT). In the variation below note how the GUI is built using EventQueue.invokeLater. Similarly the actionPerformed() method of javax.swing.Timer invokes repaint() on the EDT to display recently added nodes. A more elaborate example may be found here. import java.awt.*; import java.awt.event.ActionEvent; import java.awt.event.ActionListener; import java.util.ArrayList; import java.util.List; import java.util.Random; import javax.swing.*; public class StarPanel extends JPanel implements ActionListener { private static final Random rnd = new Random(); private final Timer t = new Timer(100 this); private final List<Node> nodes = new ArrayList<Node>(); private static class Node { private Point p; private Color c; public Node(Point p Color c) { this.p = p; this.c = c; } } public static void main(String[] args) { EventQueue.invokeLater(new Runnable() { @Override public void run() { JFrame f = new JFrame(""Star Topology""); f.add(new StarPanel()); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); f.pack(); f.setVisible(true); } }); } public StarPanel() { this.setPreferredSize(new Dimension(400 400)); t.start(); } @Override // Respond to the Timer public void actionPerformed(ActionEvent e) { int w = this.getWidth(); int h = this.getHeight(); nodes.add(new Node( new Point(rnd.nextInt(w) rnd.nextInt(h)) new Color(rnd.nextInt()))); this.repaint(); } @Override public void paintComponent(Graphics g) { int w2 = this.getWidth() / 2; int h2 = this.getHeight() / 2; for (Node n : nodes) { g.setColor(n.c); int x = n.p.x; int y = n.p.y; g.drawLine(w2 h2 x y); g.drawLine(w2 h2 x y); g.drawRect(x - 2 y - 2 4 4); } } } Do I still need to use a timer and/or RNG if my visualization is just responding to button presses? Otherwise that makes sense. No timer needed unless you do animations (but then you're better off with a SwingWorker). The timer simply serves as an example in this case. @primehunter326: @aioobe's comment is correct. Like the `actionPerformed()` method of `javax.swing.Timer` the `process() method of `SwingWorker` runs on the EDT."
634,A,"How do I generate non overlapping circles in a fixed region? What's the best way to generate a known number of non overlapping fixed radius circles in a limited space? You should be more specific. Is the distribution of the circles an issue? Should most of the surface be covered? The easiest way would be to start at x=r y=r for circles of radius r and put them one next to the other row by row until you have enough of them... If you just want as many circles in a small area as possible use hexagonal close packing.  I don't think your problem is well specified. Any constrains on the generation? Here's a rejection algorithm for a random collection of non-overlapping circles of fixed radius: Maintain a list of all circles generate a candidate by generate a random center in the allowed region Test each existing circle in your list for a collision with the candidate and reject if any collision is found. If all pass add this candidate to your list. Goto #1 until you're satisfied with the number generated Edit: read the question more closely the radius is specified...  The way to do this with the least leftover space is to put the centers of the circles on a hexagonal grid like the pattern you'd get if you were making a pyramid of coins.  See Circle packing theorem on wikipedia  You could split the screen as a grid and draw a circle in each ""square"" :)  This will depend on the shape of the space and other restrictions on the nature of the circles which you didn't specify. For example to generate N non-overlapping circles in a rectangle with height Y and length X let the diameter of each circle be either X/10*N or Y/10*N whichever is smaller and let each center be spaced evenly on a horizontal line dividing the rectangle spanning half its length."
635,A,"Java SWT - dissolve (fade) from one image to the next I'm pretty new to Java and SWT hoping to have one image dissolve into the next. I have one image now in a Label (relevant code): Device dev = shell.getDisplay(); try { Image photo = new Image(dev ""photo.jpg""); } catch(Exception e) { } Label label = new Label(shell SWT.IMAGE_JPEG); label.setImage(photo); Now I'd like photo to fade into a different image at a speed that I specify. Is this possible as set up here or do I need to delve more into the org.eclipse.swt.graphics api? Also this will be a slide show that may contain hundreds of photos only ever moving forward (never back to a previous image)---considering this is there something I explicitly need to do in order to remove the old images from memory? Thanks!! I never worked with SWT but in AWT/Swing you can use http://java.sun.com/j2se/1.5.0/docs/api/java/awt/AlphaComposite.html for this purpose. I am not aware of any swt widgets that directly support this type of functionality your best bet would be to extend org.eclipse.swt.widgets.Canvas and adding a org.eclipse.swt.events.PaintListener that can handle drawing 2 overlapping images with a different alpha. So something like this: addPaintListener(new PaintListener() { public void paintControl(PaintEvent e) { e.gc.setAlpha(50); e.gc.drawImage(image1 0 0); e.gc.setAlpha(100); e.gc.drawImage(image2 0 0); } }); After that you would just have to create a thread change redraw your Canvas and change the images/alpha values at some interval. As for your question about removing an Image from memory that is done by calling dispose() on the Image object letting it get garbage collected won't be enough when using swt."
636,A,Modelling an I-Section in a 3D Graphics Library I am using Direct3D to display a number of I-sections used in steel construction. There could be hundreds of instances of these I-sections all over my scene. I could do this two ways: Using method A I have fewer surfaces. However with backface culling turned on the surfaces will be visible from only one side. If backface culling is turned off then the flanges (horizontal plates) and web (vertical plate) may be rendered in the wrong order. Method B seems correct (and I could keep backface culling turned on) but in my model the thickness of plates in the I-section is of no importance and I would like to avoid having to create a separate triangle strip for each side of the plates. Is there a better solution? Is there a way to switch off backface culling for only certain calls of DrawIndexedPrimitives? I would also like a platform-neutral answer to this if there is one. In OpenGL you can switch of backface culling for each triangle you draw: glEnable(GL_CULL_FACE); glCullFace(GL_FRONT); // or glCullFace(GL_BACK); I think something similar is also possible in Direct3D Notice that enabling and disabling backface culling for each triangle separately will be a huge performance drain because it's a serverside state. It is possible in Direct3d with IDirect3DDevice9::SetRenderState() call though as Jaspers pointed out it is a big hit on performance switching that for each mesh. One could turn off culling once and render all the I-sections at once two switches shouldn't be a burden.  If your I-sections don't change that often load all the sections into one big vertex/index buffer and draw them with a single call. That's the most performant way to draw things and the graphic card will do a fast job even if you push half a million triangle to it. Yes this requires that you duplicate the vertex data for all sections but that's how D3D9 is intended to be used.  I would go with A as the distance you would be seeing the B from would be a waste of processing power to draw all those degenerate triangles. Also I would simply fire them at a z-buffer and allow that to sort it all out. If it get's too slow then I would start looking at optimizing but even consumer graphics cards can draw millions of polygons per second.  First off backface culling doesn't have anything to do with the order in which objects are rendered. Other than that I'd go for approach B for no particular reason other than that it'll probably look better. Also this object probably isn't more than a hand full of triangles; having hundreds in a scene shouldn't be an issue. If it is try looking into hardware instancing.
637,A,Drawing several transformed images using Quartz on the iPhone I'm trying to figure out how best to accomplish drawing a scene in an iphone app. Here is the situation: I have a single view that I would like to draw to I would like to have a background image I would like to draw several different images on top of the background each which have separate transforms applied to them (rotation scaling) I would like to draw several blocks of text on top of the background image as well - I need to word break these as well as transform them (rotation scaling) I would like to animate these images and blocks of text being drawn on screen (I don't need to do this in the first iteration but I don't want to choose a path that will preclude me from doing this in the future. I'm not really sure where to start - but because things aren't animated too much it doesn't seem like this has to be really performant so I'm thinking Quartz will be the way to go. Some possible solutions I can think of: Use quartz: Draw the background to a view's cgcontext. Create several CGLayers and draw the text/images to them. Transform the CGlayers (can you do this?) then draw the layers to the main CGContext. use OpenGL: please no. unless I really need to. Ideas? Thanks. Mostly agree with Marco. CA is perfect for this work. Set up a view and add a bunch of layers. A couple of points though. You can directly draw text using the NSString UIStringDrawing category methods. works really well and don't have the overhead of UILabel. Drawing with CoreGraphics is not too slow because CoreAnimation intelligently caches the layer. That is it's only drawn once and then CA animates a copy unless the layer is sent a setNeedsDisplay message. So in the meantime before animation should I just use CGLayers? Also - whats the difference between CoreGraphics and CoreAnimation? Are they both part of Quartz? I'm having trouble distinguishing between the different names.... Anyone have any good resources for CoreAnimation/CALayers/CGLayers? CGLayers are part of Core Graphics. CALayers are part of Core Animation. Neither is part of the other but one may presume that Core Animation uses Core Graphics under the hood. The difference is that Core Animation is designed to make it easy for you to animate things whereas Core Graphics is designed only for you to draw graphics. Great thanks. Got this working.  Use CoreAnimation CALayers for the imags (you can apply tranformations to them) and UILabels for the text. Drawing with CoreGraphics will be too slow for animating.
638,A,"Calculate points to create a curve or spline to draw an ellipse I am working with Dundas maps and need to overlay the map with bubbles depicting some data. I want to add shapes to the map in order to achieve this. I can add a triangle (or any straight-line-polygon) like this: public static void AddShape(this MapControl map List<MapPoint> points Color color string name) { if (points[0].X != points[points.Count - 1].X && points[0].Y != points[points.Count - 1].Y) points.Add(points[0]); var shape = new Shape { Name = name BorderColor = color BorderStyle = MapDashStyle.Solid BorderWidth = 1 Color = Color.FromArgb((int)(255 * (0.3)) color) }; var segments = new[] {new ShapeSegment {Type = SegmentType.Polygon Length = points.Count}}; shape.AddSegments(points.ToArray() segments); map.Shapes.Add(shape); } public static void AddBermudaTriangle(this MapControl map) { var points = new List<MapPoint> { new MapPoint(-80.15 26.0667) new MapPoint(-64.75 32.333) new MapPoint(-66.07 18.41) }; map.AddShape(points Color.Red ""Bermuda Triangle""); } You can see that the Bermuda Triangle overlays the map in red. Now I want to calculate a set of points to pass to my AddShape method that would draw an elipse or circle. I just need a simple algorithm for calculating the x and y coordinates of a given number of points. Perhaps starting with a given point that would represent the centre of the circle. For example: public static void AddCircle(this MapControl map Point centre double radius string name) { var points = new List<MapPoint>(); const int n = 360; for(var i = 0; i < n; i++) { //calculate x & y using n radius and centre double x = 0; double y = 0; points.Add(new MapPoint(x y)); } map.AddShape(points Color.Red name); } I know that the xy calculation is simple trigonometry but I'm suffering a brain freeze. Help! EDIT (Solved using tur!ng's code): public static void AddCircle(this MapControl map Color color MapPoint centre double radius string name) { var points = new List<MapPoint>(); const int n = 360; for(var i = 0; i < n; i++) { var x = (radius * Math.Cos(i * Math.PI / 180)) + centre.X; var y = (radius * Math.Sin(i * Math.PI / 180)) + centre.Y; points.Add(new MapPoint(x y)); } map.AddShape(points color name); } The blue circle (over Greenwich) is distorted because of the map projection over a Robinson grid. Recall that the formula for a circle may be expressed as (x/r)**2 + (y/r)**2 = 1 where x and y are coordinates and r is radius. The formula for an ellipse may be expressed as (x/a)**2 + (y/b)**2 = 1 where a and b are the semimajor and semiminor axes (in no particular order). Choose a and b to give you an ellipse that ""looks good"". You usually want to pick your points around a circle at equal angular steps to make a better looking polygonal approximation to a true circle. For this you use the substitutions x = r cos theta y = r sin theta and run your loop for theta from zero to 2*pi. For your ellipse you'll use x = a cos theta y = b sin theta This gives you an ellipse with the semimajor and semiminor axes parallel to the X and Y axes and centered at the origin. If you want an arbitrary orientation with an arbitrary position you'll need to apply a rotation by an angle phi and a translation. Any good computer graphics text will give you the necessary equations most likely in matrix form. Thanks! I'm working on an elipse implementation now using your suggestions.  Copied from an old C++ program I wrote a long time ago it still runs at dozens of places:  // Approximate arc with small line segments double sa = dp[ix].center.angle(dp[ix].co); double ea = dp[ix].center.angle(dp[ix+1].co); double r = scale * dp[ix].radius; double rot = ea - sa; double inc = rot; if (dp[ix].dir == ROTCW) rot = -rot; if (rot < 0) rot += 2*PI; // Compute rotation increment that generates less than 1/4 pixel error if (r > 2) inc = 2*acos(1-0.25/r); if (inc >= rot || r < 2) addPoint(x y); else { int cnt = int(1 + rot / inc); inc = rot / cnt; if (dp[ix].dir == ROTCW) inc = -inc; for (int jx = 0; jx < cnt; ++jx) { x = offsx + scale * dp[ix].center.x + r * cos(sa); y = offsy + scale * dp[ix].center.y + r * sin(sa); addPoint(x y); sa += inc; } } acos() is the same as Math.Acos(). Thanks the rotation increment is useful to me! Hans If you're still around please would you mind explaining a few things in that code? What type is in the dp array? I'd guess an array of Geometries? What's it's ""co"" attribute? And what's the scale? I'd guess it's double but of what sort of magnitude. I've got a compilable version of that ported to C# but I'm still a long way from working code.   double x = centre.x + radius*Math.cos(2*Math.PI/360 * i); double y = centre.y + radius*Math.sin(2*Math.PI/360 * i); for a circle. Thanks that's spot on! Just testing now..."
639,A,"Writing a 2D RTS game in C#: graphics library options? I am considering writing a 2D RTS in C#/.NET and i was wondering what options are available for a graphics library aside from XNA. What are they? Please move to stack overflow i accidentally asked it here... Ok why does it not show me as asking the question? Have you linked your accounts properly on the ""Accounts"" tab of your user page? Why you would use C#/.NET and NOT use XNA are a total mystery to me... I have at least i think so... I want to thank everyone for their answers i got many good results however Grant Peters convinced me there's nothing better than XNA. If you are looking for opensource community supported library you can try SlimDX TAO framework -- opengl bindings for .Net framework But am not sure if they are matured enough.. but its worth giva try..  Going straight to DirectX is a perfectly valid option although for only 2D you probably need to know a bit too much about 3D graphics to effectively use it. Back when I was into game development I was quite fond of the Artificial Heart graphics engine which was somewhat basic but incredibly easy to use. Unfortunately the website appears to have been replaced with a parked domain. Perhaps the company went out of business. If so it is a shame. For a more extensive catalogue take a look at the DevMaster game engine database  There is always Unity. Uses Mono not MS.Net http://unity3d.com/unity/  I use Gorgon (which uses SlimDX). http://tape-worm.net/?tag=gorgon  I'm sure there are other options out there but you can also take a look at SDL.Net a .Net implementation for SDL.  The answers posted so far (apart from SDL.NET) have all just said to use OpenGL DirectX or some variant of the 2. Personally I wouldn't use either of these for a 2D game when you already have XNA. XNA already has some classes for doing 2D graphics (the main one being the SpriteBatch). If you do use XNA not only can you make a game that runs on Windows but also one for XBox Zune and Silverlight as well (Zune and Silverlight require that you don't use the 3D part of XNA though but that shouldn't worry you). Basically XNA will save you the trouble of writting all of the low level management code that DirectX or OpenGL need (such as handling window switching and a plethora of other problems). I haven't used SDL at all so I can't give you any personal opinions on it but I have heard it is quite good for 2D graphics so I'd recommend looking into SDL.NET or just going with XNA the other suggestions here require a lot more work with little advantage (most people just want to make the game and not worry about the HAL interfaces and getting everything to play nice with the OS). It's just that XNA is easy enough to write simple little samples for but extraordinarily hard to write something decent for. I'm starting to suspect that it is the best option though... Like I said XNA is a lot easier than going with something like OpenGL or DirectX which is what most people here are suggesting. There may be a few engines out there that will be easier but most of those would require you to work in c/c++ (without CLR so you won't get the garbage collection that makes C# so much simpler to develop in) also most engines are geared towards 3D games and it can be more difficult to get them to just do 2D graphics. You're most likely right. Besiddes; if i put enough effort into the ""framework"" for my RTS game the actual development should be simple enough.  To make the most of today's computers' hardware and to keep up with the graphics of other 2D games I would go with a 2D-in-3D solution based on DirectX or OpenGL. You would render sprites as so-called quads (two polygons combined to make a square.) I personally think OpenGL is much more appropriate for a 2D game. You should definitely read through the NeHe tutorials (they're for C++ but the OpenGL parts are still valid. You can find a C# conversion of the code at the bottom of the page of each lesson.) as they are an awesome resource for getting started with making a simple graphics engine for your game. Furthermore a lot of the articles on GameDev have helped me a lot when developing games. They've got all sorts of articles that will help you along ranging from adding lighting effects to your games to adding simple physics."
640,A,Java: AWT on Solaris How is AWT implemented for Solaris? ie: What native libraries if any is it dependent on. For Sun JRE: Old school MToolkit use Motify. From 1.5 I think there is XToolkit which just use xlib. From memory Solaris kept the MToolkit as the default longer than on Linux. It's switchable with the AWT_TOOLKIT environment variable.
641,A,"Is there any 2D renderer library with complete fixed point support for embedded linux? I am working on embedded linux Is there any open source 2D renderer available which can draw on memory scanline based complete fixed-point support. I work in c or cpp programming language. I know one with which satisfy my all needs that is Google Skia which google uses in android and chrome But I found it without documentation not straight-forward compilable not straight-forward usable in 3rd party projects. Regards Sunny. Do you have a CPU planed for this project? Checkout Cairo. I am not sure what you mean by ""complete fixed-point support"" but other than that it seems to meet your requirements. kdubb complete fixed point support means no floating point operations all floating point operations replaced with integer operations with limited accuracy. glad to give you first UP. :-) Welcome to SO. Although the public API uses double internally cairo is 24.8 fixed point since version 1.5.12.  DirectFB. If you want hardware acceleration  directFB is the most portable way to go. But does it uses Fixed Point ?  Allegro is a games library which includes extensive software rendering most of which does not rely on floating point. Additionally it has some trig functions and maths functions which work on fixed-point. It has things like sprite-rotation which don't need floating point. It's all linked from the above site. You're going to be more or less on your own compiling for an embedded platform though; the community is pretty helpful however. Thanks May I have there any reference or tutorial to learn it which you found helpful?  Don't know if it's what you're looking for but there's libcrtxy http://libcrtxy.sourceforge.net/"
642,A,"Validating Form Data and Displaying Graphics with JavaScript I have not really used JavaScript before but I am trying to validate form elements as they are being filled out. I have an X and a Check mark that I am trying to display next to the field to show if it is valid or not. I know that this is partially right because I can use an alert but I am not sure how to alter the fields of the graphics. The JavaScript: function validate_field(field) { var value = document.newAccount.fname; if (value==null||value=="""") { document.fnameX.visibility = visible; document.fnameOK.visibility = hidden; //alert(""FAIL""); return false; } else { document.fnameX.visibility = hidden; document.fnameOK.visibility = visible; //alert(""TRUE""); return true; } } Some of the HTML:  <form action=""XXXXX"" method=""post"" name=""newAccount""> <table width=""78%"" border=""0"" align=""center""> <tr> <td colspan=""2""><strong>Personal Info:</strong> </td> </tr> <tr> <td width=""24%""><div align=""right"">First Name: </div></td> <td width=""76%""> <input type=""text"" onchange=""return validate_field(this)"" onfocus=""return validate_field(this)"" name=""fname"" tabindex=""1"" size=""50""/> <img name=""fnameX"" src=""redx.jpg"" style=""visibility:hidden"" alt=""X"" width=""18"" height=""18"" /><img name=""fnameOK"" src=""checkmark.jpg"" style=""visibility:hidden"" alt=""Ok"" width=""18"" height=""18"" /></td> </tr> <tr> <td><div align=""right"">Last Name: </div></td> <td><input type=""text"" name=""lname"" tabindex=""2"" size=""50""/></td> </tr> </table> </form> Your probably better off using CSS classes for something like this. .valid { padding-right:15px /*or whatever the width of your graphic is*/ background:url('checkmark.jpg') center right no-repeat; /*remember that the url is relative to the stylesheet*/ } .invalid { padding-right:15px background:url('redx.jpg') center right no-repeat; } Then your validate function becomes: function validate_field(field) { var value = field.value; if (value==null||value=="""") { field.parentNode.className = 'invalid'; return false; } else { field.parentNode.className = 'valid'; return true; } } EDIT: Remember to keep accessibility in mind and that unless you are using an alert or making notifications accessible using ARIA attributes users with assistive devices are going to have a tough time with your form. EDIT: Here is a working example from your code where I change the background color of the parent table cell of the form input. EDIT: And here is an example where I got rid of those not so nice layout tables and replaced them with a fieldset and modified the Javascript a little bit. If you aren't familiar with jsbin you can append /edit to the url to see/modify the code like so. Thanks!!! Works great! Sure thing. If you are having trouble with the Javascript you might want to look into jQuery or a similar library. Using jQuery doesn't mean you don't need to know your way around the DOM but it makes simple manipulation like this more straightforward and powerful. Ok. I will take a look at that. Thanks!"
643,A,"Content of a WPF button - unable to put anything but text I designed a nice STOP graphic with red bg and gradient and all that in Expression Design - just like it'll look on a real taperecorder or something. I exported it as a XAML WPF Resource dictionary and got the XAML code. I want this graphic to appear inside the button along with the text ""STOP"". If I directly paste the XAML inside the button tag it is not working. I tried searching but all search results seem to be talking about something called button template and they are setting styles and stuff. that's not what i want. Is it like not everything you design in Expression Design won't go inside a button? How can I put any design XAML as the content of a button? Edit This is what I got from exporting as XAML. Can this be put inside Button tag? <DrawingBrush x:Key=""Layer_1"" Stretch=""Uniform""> <DrawingBrush.Drawing> <DrawingGroup> <DrawingGroup.Children> <GeometryDrawing Geometry=""F1 M 2.720.160004L 16.05330.160004C 17.46720.160004 18.61331.30615 18.61332.72L 18.613312.96C 18.613314.3739 17.467215.52 16.053315.52L 2.7215.52C 1.3061515.52 0.1614.3739 0.1612.96L 0.162.72C 0.161.30615 1.306150.160004 2.720.160004 Z ""> <GeometryDrawing.Pen> <Pen Thickness=""0.32"" LineJoin=""Round"" Brush=""#FFFFFFFF""/> </GeometryDrawing.Pen> <GeometryDrawing.Brush> <LinearGradientBrush StartPoint=""0.5-0.111111"" EndPoint=""0.51.11111""> <LinearGradientBrush.GradientStops> <GradientStop Color=""#FFDDB8B8"" Offset=""0.00465116""/> <GradientStop Color=""#FFA60E0E"" Offset=""0.986046""/> </LinearGradientBrush.GradientStops> </LinearGradientBrush> </GeometryDrawing.Brush> </GeometryDrawing> </DrawingGroup.Children> </DrawingGroup> </DrawingBrush.Drawing> . Post your code ! Rather than using a DrawingBrush how about just using a Rectange? Expression Design is great but the XAML it exports tends to be a little too much for my taste. I recommend adding a rectangle as a resource and then setting the content of your button to the resource as follows: <Grid> <Grid.Resources> <Rectangle x:Key=""Stop"" RadiusX=""3"" RadiusY=""3"" Width=""20"" Height=""20""> <Rectangle.Fill> <LinearGradientBrush StartPoint=""0.51"" EndPoint=""0.50""> <LinearGradientBrush.GradientStops> <GradientStop Color=""#FFAD2323"" Offset=""0""/> <GradientStop Color=""#FFD6A0A0"" Offset=""1""/> </LinearGradientBrush.GradientStops> </LinearGradientBrush> </Rectangle.Fill> </Rectangle> </Grid.Resources> <Button Width=""100"" Height=""30"" Content=""{StaticResource Stop}""/> </Grid> Of course you could also export your design to a .png as well... I thought whatever expression exported was what is possible. I didn't know you could do the same thing witha rectangle. I guess I should study XAML in detail. Thanks :) This should work with the XAML you exported also. Just replace the LinearGradientBrush in Brent's example with your DrawingBrush.  You have defined a Brush. Now you need something to paint on. Try using:  <Button Background=""{StaticResource Layer_1}"" Content=""STOP""> /> You'll probably need to edit the button template to change other button states like pressed MouseOver and so on. Wait that XAML can only go as the background? I already have another gradient as the background. Can this one come next to the STOP? No this was just a quick hack so you can see your brush in action."
644,A,How do draw to a texture in OpenGL Now that my OpenGL application is getting larger and more complex I am noticing that it's also getting a little slow on very low-end systems such as Netbooks. In Java I am able to get around this by drawing to a BufferedImage then drawing that to the screen and updating the cached render every one in a while. How would I go about doing this in OpenGL with C++? I found a few guides but they seem to only work on newer hardware/specific Nvidia cards. Since the cached rendering operations will only be updated every once in a while i can sacrifice speed for compatability. glBegin(GL_QUADS); setColor(DARK_BLUE); glVertex2f(0 0); //TL glVertex2f(appWidth 0); //TR setColor(LIGHT_BLUE); glVertex2f(appWidth appHeight); //BR glVertex2f(0 appHeight); //BR glEnd(); This is something that I am especially concerned about. A gradient that takes up the entire screen is being re-drawn many times per second. How can I cache it to a texture then just draw that texture to increase performance? Also a trick I use in Java is to render it to a 1 X height texture then scale that to width x height to increase the performance and lower memory usage. Is there such a trick with openGL? Consider using a display list rather than a texture. Texture reads (especially for large ones) are a good deal slower than 8 or 9 function calls. So would a display list that draws a full screen gradient be faster than just caching the result and drawing that over and over? It's something you'll want to benchmark yourself but yes I believe so. See Sorren's answer for the rationale.  Look into FBOs - framebuffer objects. It's an extension that lets you render to arbitrary rendertargets including textures. This extension should be available on most recent hardware. This is a fairly good primer on FBOs: OpenGL Frame Buffer Object 101 The whole point of the question is that he can't use FBOs because he wants the lowest possible hardware denominator.  Before doing any optimization you should make sure you fully understand the bottlenecks. You'll probably be surprised at the result.  If you don't want to use Framebuffer Objects for compatibility reasons (but they are pretty widely available) you don't want to use the legacy (and non portable) Pbuffers either. That leaves you with the simple possibility of reading the contents of the framebuffer with glReadPixels and creating a new texture with that data using glTexImage2D. Let me add that I don't really think that in your case you are going to gain much. Drawing a texture onscreen requires at least texel access per pixel that's not really a huge saving if the alternative is just interpolating a color as you are doing now!  Hey there thought I'd give you some insight in to this. There's essentially two ways to do it. Frame Buffer Objects (FBOs) for more modern hardware and the back buffer for a fall back. The article from one of the previous posters is a good article to follow on it and there's plent of tutorials on google for FBOs. In my 2d Engine (Phoenix) we decided we would go with just the back buffer method. Our class was fairly simple and you can view the header and source here: http://code.google.com/p/phoenixgl/source/browse/branches/0.3/libPhoenixGL/PhRenderTexture.h http://code.google.com/p/phoenixgl/source/browse/branches/0.3/libPhoenixGL/PhRenderTexture.cpp Hope that helps! links are dead: the branch 0.3 isn't anymore Here's a link to the equivalent file in the newer version of phoenix: https://github.com/jonparrott/PhoenixCore/blob/master/source/RenderTarget.h  I sincerely doubt drawing from a texture is less work than drawing a gradient. In drawing a gradient: Color is interpolated at every pixel In drawing a texture: Texture coordinate is interpolated at every pixel Color is still interpolated at every pixel Texture lookup for every pixel Multiply lookup color with current color Not that either of these are slow but drawing untextured polygons is pretty much as fast as it gets.
645,A,"Android: Difference between SurfaceView and View? When is it necessary or better to use a SurfaceView instead of a View? updated 05/09/2014 OK. We have official document now. It talked all I have mentioned in a better way. Read more detailed here. Yes the main difference is surfaceView can be updated on the background thread. However there are more you might care. surfaceView has dedicate surface buffer while all the view share one surface buffer that is allocated by ViewRoot. In another word surfaceView cost more resources. surfaceView can not be hardware accelerated (as of JB4.2) while 95% operations on normal View are HW accelerated using openGL ES. More work should be done to create your customized surfaceView. You need to listener to the surfaceCreated/Destroy Event create an render thread more importantly synchronized the render thread and main thread. However to customize the View all you need to do is override onDraw method. The timing to update is different. Normal view update mechanism is constraint or controlled by the framework:You call view.invalidate in the UI thread or view.postInvalid in other thread to indicate to the framework that the view should be updated. However the view won't be updated immediately but wait until next VSYNC event arrived. The easy approach to understand VYSNC is to consider it is as a timer that fire up every 16ms for a 60fps screen. In Android all the normal view update (and display actually but I won't talk it today) is synchronized with VSYNC to achieve better smoothness. Nowback to the surfaceView you can render it anytime as you wish. HoweverI can hardly tell if it is an advantage since the display is also synchronized with VSNC as stated previously.  The main difference is that SurfaceView can be drawn on by background theads but Views can't. They use more resources though so you don't want to use them unless you have to. ""They use more resources though so you don't want to use them unless you have to."" - Who uses more resources? Views or SurfaceViews? Surfaceview use more resources than views  A few things I've noted: SurfaceViews contain a nice rendering mechanism that allows threads to update the surface's content without using a handler (good for animation). Surfaceviews cannot be transparent they can only appear behind other elements in the view hierarchy. I've found that they are much faster for animation than rendering onto a View. For more information (and a great usage example) refer to the LunarLander project in the SDK 's examples section. How time progresses! FYI...A SurfaceView can now be transparent: http://stackoverflow.com/questions/5391089/how-to-make-surfaceview-transparent @Ralphleon : What do you mean by Surface Views cannot be transparent? Can other views overlap the Surface view? For example can I display a ListView on a Surface view temporarily?  Views are all drawn on the same GUI thread which is also used for all user interaction. So if you need to update GUI rapidly or if the rendering takes too much time and affects user experience then use SurfaceView. Official detail answer: http://developer.android.com/guide/topics/graphics/2d-graphics.html Check pierr's answer which contains more detail."
646,A,"Rendering vector shapes in Java I have a series of shapes (Approx. 50) that each have 5 points and a color (with alpha transparency). I want to render these shapes onto a pixel grid. I program as an amateur so I have no idea how I should go about doing this. Can someone give me a starting point or some pseudo-code? Thanks in advance. Java has a pretty nice 2d graphics API included with it. I suggest you take a look at http://java.sun.com/docs/books/tutorial/2d/index.html for a tutorial on the basics. +1 - I tend to suggest the sun tutorial quickly also.  import java.awt.Graphics; import java.awt.Graphics2D; import java.util.List; import java.util.ArrayList; import java.awt.Point; import javax.swing.JFrame; import java.awt.Color; import java.util.Random; import java.awt.Polygon; import java.awt.Shape; public class GraphicsTest extends JFrame { private List<ColoredShape> shapes; private static final int NUM_SHAPES = 50; private static final int NUM_POINTS_PER_SHAPE = 5; private static final int WIDTH = 640; private static final int HEIGHT = 480; private Random randomGenerator; public GraphicsTest(String title) { super(title); setVisible(true); setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); setSize(WIDTH HEIGHT); randomGenerator = new Random(); initShapes(); } private void initShapes() { shapes = new ArrayList<ColoredShape>(NUM_SHAPES); for (int i = 0; i < NUM_SHAPES; i++) { Point[] points = getRandomPoints(); Color color = getRandomColor(); shapes.add(i new ColoredShape(points color)); } } private Point[] getRandomPoints() { Point[] points = new Point[NUM_POINTS_PER_SHAPE]; for (int i = 0; i < points.length; i++) { int x = randomGenerator.nextInt(WIDTH); int y = randomGenerator.nextInt(HEIGHT); points[i] = new Point(x y); } return points; } /** * @return a Color with random values for alpha red green and blue values */ private Color getRandomColor() { float alpha = randomGenerator.nextFloat(); float red = randomGenerator.nextFloat(); float green = randomGenerator.nextFloat(); float blue = randomGenerator.nextFloat(); return new Color(red green blue alpha); } public void paint(Graphics g) { Graphics2D g2 = (Graphics2D) g; for (ColoredShape shape : shapes) { g2.setColor(shape.getColor()); g2.fill(shape.getOutline()); } } public static void main(String[] args) { GraphicsTest b = new GraphicsTest(""Testing polygons""); } private class ColoredShape { private Polygon outline; private Color color; public ColoredShape(Point[] points Color color) { this.color = color; // Would be better to separate out into xpoints ypoints npoints // but I'm lazy outline = new Polygon(); for (Point p : points) { outline.addPoint((int) p.getX() (int) p.getY()); } } public Color getColor() { return color; } public Shape getOutline() { return outline; } } } psychedelic picture :) Thank you! That's a very helpful example."
647,A,"Drawing graphics in java (netbeans ide) I created a new JApplet form in netbeans: public class UI extends javax.swing.JApplet { //generated code... } And a jpanel in design mode named panou: // Variables declaration - do not modify private javax.swing.JPanel panou; How do I get to draw a line on ""panou""? I've been searching for this for 5 hours now so a code snippet and where to place it would be great. (using Graphics2D preferably) Edit: its getting really frustrating I tried this (big chunk of code) Edit2: thanks to Martijn I made it work:) To do custom painting in a JPanel one would need to make a subclass of a JPanel and then overload the paintComponent method: class MyPanel extends JPanel { public void paintComponent(Graphics g) { // Perform custom painting here. } } In the example above the MyPanel class is a subclass of JPanel which will perform whatever custom painting is written in the paintComponent method. For more information on how to do custom painting in Swing components Lesson: Performing Custom Painting from The Java Tutorials have some examples. If one wants to do painting with Java2D (i.e. using Graphics2D) then one could do some painting on a BufferedImage first then draw the contents of the BufferedImage onto the JPanel: class MyPanel extends JPanel { BufferedImage image; public MyPanel() { Graphics2D g = image.createGraphics(); // Do Java2D painting onto the BufferedImage. } public void paintComponent(Graphics g) { // Draw the contents of the BufferedImage onto the panel. g.drawImage(image 0 0 null); } } Further reading: Painting in AWT and Swing Trail: 2D Graphics  Go to design mode Right Click on the panel ""panou"" Click ""Costumize code"" In the dialog select in the first combobox ""costum creation"" add after = new javax.swing.JPanel() this so you see this:   panou = new javax.swing.JPanel(){ @Override public void paintComponent(Graphics g) { super.paintComponent(g); // Do the original draw g.drawLine(10 10 60 60); // Write here your coordinates } }; Make sure you import java.awt.Graphics. The line that you will see is always one pixel thick. You can make it more ""line"" by doing the following: Create this method: public static final void setAntiAliasing(Graphics g boolean yesno) { Object obj = yesno ? RenderingHints.VALUE_ANTIALIAS_ON : RenderingHints.VALUE_ANTIALIAS_OFF; ((Graphics2D) g).setRenderingHint(RenderingHints.KEY_ANTIALIASING obj); } And add after super.paintComponent(g); (in your costum creation) this: setAntiAlias(g true); Edit What you are doing wrong is: you paint the line once (by creating the frame). When you paint the line the frame is also invisible. The first draw is happening when the frame becomes visible. The frame will be REpainted so everything from the previous paint will disapear. Always you resize the frame everything will be repainted. So you have to make sure each time the panel is painted the line also is painted. Martijn thank you very much  I just want to know whether this method can work with netbean IDE ?? yes it works see Martijn's answer."
648,A,"Inside clipping with Java Graphics I need to draw a line using java.awt.Graphis but only the portion of the line that lies outside a rectangle should be rendered. Is it possible to use the Graphics clipping support? Or do I need to calculate the intersection and clip the line myself? You can do this with an AWT clip. You'll need to know the bounds of the rectangle you want to exclude and the outer bounds of your drawing area. The following demo code opens a frame and displays a single panel in it. The panel's paint method sets up an example clip which looks like a rectangle with a rectangular hole in the middle when in fact it's a polygon that describes the area around the area we want to exclude. The clip rectangle should be composed of the bounds of the excluded rectangle and the outer edge of the drawing area but I've left hard-coded values in to keep it simple and illustrate the workings better (I hope!)  +-------------------+ | clip drawing area | +---+-----------+ | | | excluded | | | | area | | | +-----------+ | | | +-------------------+ This method has the benefit over calculating the line intersection manually in that it prevents all AWT painting going into the excluded area. I don't know if that's useful to you or not. My demo then paints a black rectangle over the whole area and a single white diagonal line running through it to illustrate the clip working. public class StackOverflow extends JFrame { public static void main(String[] args) { new StackOverflow(); } private StackOverflow() { setTitle( ""Clip with a hole"" ); setSize( 320300 ); getContentPane().add( new ClipPanel() ); setVisible( true ); } } class ClipPanel extends JPanel { @Override public void paintComponent(Graphics g) { super.paintComponent(g); Polygon clip = new Polygon( new int[]{ 0 100 100 0 0 20 20 80 80 0 } new int[]{ 0 0 60 60 20 20 40 40 20 20 } 10 ); g.setClip(clip); g.setColor( Color.BLACK ); g.fillRect( 0010060 ); g.setColor( Color.WHITE ); g.drawLine( 0010060 ); } } Yeah Savvas Dalkitsis' answer is much better. +1. This answer almost works and uses API's supported by J2ME PP. I found I had to start the shape at 020 to create an enclosed polygon. The complete working code is in my question: http://stackoverflow.com/questions/1273688/is-there-any-way-to-have-an-inverted-clip-region-for-painting-in-java @Banjolity; if you want to put an answer on my question which refers to this answer I will accept it as correct - I think you should get the credit. Also correct. But the other is easier because uses subtract. Thanks.  You need to use the Area class. This example will demonstrate how to do what you ask: import java.awt.BorderLayout; import java.awt.Color; import java.awt.Dimension; import java.awt.Graphics; import java.awt.Graphics2D; import java.awt.geom.Area; import java.awt.geom.Rectangle2D; import javax.swing.JFrame; import javax.swing.JPanel; public class Test extends JPanel { public static void main(String[] args) { JFrame f = new JFrame(); Test t = new Test(); f.getContentPane().setLayout(new BorderLayout()); f.getContentPane().add(tBorderLayout.CENTER); f.pack(); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); f.setVisible(true); } public Test() { setPreferredSize(new Dimension(300 300)); } public void paintComponent(Graphics g) { Graphics2D g2 = (Graphics2D)g.create(); Rectangle2D rectangleNotToDrawIn = new Rectangle2D.Double(100 100 20 30); Area outside = calculateRectOutside(rectangleNotToDrawIn); g2.setPaint(Color.white); g2.fillRect(0 0 getWidth() getHeight()); g2.setPaint(Color.black); g2.setClip(outside); g2.drawLine(0 0 getWidth() getHeight()); } private Area calculateRectOutside(Rectangle2D r) { Area outside = new Area(new Rectangle2D.Double(0 0 getWidth() getHeight())); outside.subtract(new Area(r)); return outside; } } +1 - Much better than mine :) The Area method is better in terms of readability and extensibility (you can create any shape and clip to its outside) but i think it might have some performance issues (the Area class is a bit over the top maybe). You should benchmark it if you need to draw on the clip many times and see if the other method is faster. Yes I'm considering performance as the operation is done on mouse move events. But it's good so far. I may optimize it even more because I need to clip only when the end of the line is inside the rectangle so I don't need to clip the whole outside of the rectangle only two sides of it at most (""right and bottom"" or ""right and top"" or ""just right"" etc). Thanks again This is a great answer for J2SE; for J2ME the other answer by banjollity works. Thanks perfect!"
649,A,"World-Coordinate Issues with gluUnProject() I'm currently calling Trace (method below) from a game loop. Right now all I'm trying to do is get the world coordinates from the screen mouse so I can move objects around in the world space. The values I'm getting from gluUnProject are however; puzzling me. I was using glReadPixel(...) to get the Z value but that produced little to no movement in the object I was drawing and the resulting vector ended up being the same as my cameras location (except for the tiny decimal changes due to mouse movement) so I decided to get rid of the call and replace the Z value with 1. My question is: Does the following code look right to you? Every example I've seen thusfar is either identical or -very- similar but I can't seem to produce correct results even if I lock down the Y axis. If the code is correct then I'm guessing that I'm just not using the resulting vector properly. Should I not be able to draw an object or point directly with the resulting vector or do I have to do something else with it like normalize? The current render mode is GL_RENDER and I am using glFrustum with a NearZ value of 1 and FarZ value of 2048 to create a perspective. There is also a series of viewports created along with scissors with a size and width of 512x768 and positioned in each corner of a 1024x768 window. Trace(...) is called in between rendering of the upper left viewport and is the only perspective projection while the other viewports are orthographic. FOV is set to 45. void VideoWindow::Trace(int cursorX int cursorY) { double objX objY objZ;//holder for world coordinates GLint view[4];//viewport dimensions+pos GLdouble p[16];//projection matrix GLdouble m[16];//modelview matrix GLdouble z;//Z-Buffer Value? glGetDoublev (GL_MODELVIEW_MATRIX m); glGetDoublev (GL_PROJECTION_MATRIXp); glGetIntegerv( GL_VIEWPORT view ); //view[3]-cursorY = conversion from upper left (00) to lower left (00) //Unproject 2D Screen coordinates into wonderful world coordinates gluUnProject(cursorX view[3]-cursorY 1 m p view &objX &objY &objZ); //Do something useful here??? } Any ideas? Edit: I've changed the winZ value to 0.5 instead of 1 which gives a vector thats more reasonable but drawing a point still wasn't matching the mouse. I found out that the value of view[3] was 384 which is correct for the viewport I'm using but I replaced it with 768 (the actual window size) and the point followed the mouse 100%. Further experimentation reveals that I can't use the coordinates to move around a 3D object in the perspective world space using this these coordinates however moving around 3D object in Orthographic space works fine. After looking over the NEHE tutorial on gluUnProject it seemed that the tutorial was identical to the way I had it using glReadPixel. I decided to revert my code and use glReadPixel and everything seems to work nicely. The winz argument to gluUnproject specifies the depth from the camera at which you're ""picking"" your points. As you've stated this coordinate should be in the [0 1] range. Some tutorials like NeHes read out the z coordinate from the depth buffer so that you ""pick"" at the right depth of course for this to work you'll have to do the gluUnproject after you've rendered everything else. Regardless if you set winz to 0.5 or something (not 0 or 1 or the point will end up on the near or far clip plane and maybe culled) and do the following: gluUnProject(cursorX view[3]-cursorY 0.5 m p view &objX &objY &objZ); //Do something useful here??? glPointSize(10); glBegin(GL_POINTS); glColor3f(1 0 0); glVertex3f(objX objY objZ); glEnd(); You should end up with a red blob at the mouse pointer (provided nothing else overdraws it afterwards and you don't have any funny render states which renders the point invisible). Initially the changes you suggested didn't work but after some experimentation I found that changing view[3] (which is 384 the size of my viewport) to 768 (window size) got the point to draw at the mouse coordinate and the 0.5 corrected the position. However the mouse movement only seems to be changing the actual coordinates by a fraction and moving the camera itself is the only real way to change the vector position more than a fraction. For example I have a grid that I want to move an object across but the vector from gluUnProject isn't that different from the camera position.  just a thought but if the third argument to gluUnProject is the z distance to the camera wouldn't any point you draw at that location be on the near clipping plane of your frustum? Better make that z value a bit higher. If I remember correctly I experimented with higher Z values ranging from my farZ (2048) to about 10000. It seemed that the heigher the Z value the less the mouse coordinates had an effect on the point in 3D space. I thought the Z value was really just normalized between 0 (near) and 1 (far) though."
650,A,"Why the leading ""#FF"" in hexadecimal color values? I'm using Expression Blend 3 and writing some of the XAML by hand specifically the color values of controls. I have a list of RGB colors already converted to hexadecimal. I just need to insert the hex value into my XAML. Initially I pasted the hex value from an email into the appropriate properties. Before I could finish Blend started having a fit underlining the color property with a squiggle and a tooltip telling me ""Token is not valid."" After some research I found placing a pound sign (""#"") in front of the hex value resolved this issue. In the process of researching this problem I started chaning colors via the color picker in Blend. I quickly found the values Blend was inserting not only started with the pound sign but also ""FF"". The values I was pasting were valid colors in valid hex format. But when entering the RGB values into Blend and letting Blend insert the hex value I noticed all mycolors were prefixed with ""#FF"". Removing the # as i already pointed out generated errors but removing the ""FF"" seamed to have no effect at all. In the world of hexadecimal colors is the color #5A7F39 really the same as #FF5A7F39 ? Why the FF ? They are two different hex values right? But appear identical onscreen. Why the difference? the 'extra' ff is an alpha value (degree of transparency). If you only have 3 hex pairs the alpha value is assumed to be ff (no transparency). However if you compare #335A7F39 and #FF5A7F39 you should see a difference  That may be the alpha component of the color which represents the opacity (00 -> transparent FF -> opaque). MSDN seems to agree with this : http://msdn.microsoft.com/en-us/library/bb980062%28VS.95%29.aspx 16-bit hexadecimal alpha -- #AARRGGBB I'm a bit confused by how according to the article 1 hex digit represents 8 bits and 2 hex digits represents 16 bits.  I believe the leading FF is the Alpha. 255 (or FF) being 100% opaque and 00 would be transparent."
651,A,"Clear Graphics Object to Black or other RGB I am a rank beginner in C#. I am currently using this code:  objGraphics.Clear(SystemColors.Control); What I want to do is Clear this object to black (or some other RGB color) and I'm stumped as to how to replace the SystemColors.Control with preferably an RGB color spec. I'd probably want to clear to black most of the time. Any help will be much appreciated! Gene I apologize for that. StackOverflow won't let me rate answers even though I have this ID. I'll try to figure out what's wrong with my registration. Any tips? It's got nothing to do with rating answers. Click the check mark next to a good answer. Green = accepted answer. Color black = Color.FromName(""Black""); objGraphics.Clear(black);  objGraphics.Clear(Color.Black); or objGraphics.Clear(Color.FromArgb(r g b)); I see you've sorted it now. Thanks :) Easy! Thanks! StackOverflow won't let me rate answers so I apologize for that."
652,A,"Creating image or graphic in C# from decimal data? Is it possible to create an image or graphic using vector data that has decimal components that vary? And of course if its possible... then how? For example: a vector has two points where point one is {9.56 4.1} and point two is {3.4567892.12345}. Note: the precision varies from number to number. So in the end it seems like the google earth solution is better. Oh sure. For example the Line class will draw a line in WPF. It uses two doubles for each coordinate. X1 Y1 and X2 Y2. If your data is stored in the decimal datatype you can do a simple cast. Keep in mind I'm trying to extrapolate what you're trying to do from your question. How are you planning to draw your vector shapes? What's your overall approach? Um.. Not sure what you mean by ""my approach"". But I""m using C# to solve a specific problem (don't want to discuss on here due to IP issues). I import my initial vector data perform some operations whose output are some other vectors (possible different color). And I want to be able to visualize the output by viewing a jpg or png.  You can draw vectors to a bitmap and then save that bitmap as follows. using System.Drawing; using System.Drawing.Imaging; . . . using (Bitmap bmp = new Bitmap(10 10)) { using (Graphics g = Graphics.FromImage(bmp)) { g.Clear(Color.White); g.DrawLine(Pens.Black new PointF(9.56f 4.1f) new PointF(3.456789f 2.12345f)); } bmp.Save(@""c:\myimage.jpg"" ImageFormat.Jpeg); } +1 for the no nonsense code snippet that features the asker's sample data. Even though this solution worked for a small set of the geo data it started having problems when dealing with larger than normal resolutions and more than 10000 vectors. @Sean Ocha - You didn't mention those requirements in your post. A simple solution mapping the coordinates to a viewport and culling the ones that are too big or too small for the viewport would probably work well. Dude... That's EXACTLY what I was looking for. Thank you!!!"
653,A,"Java2D: Increase the line width I want to increase the Line2D width. I could not find any method to do that. Do I need to actually make a small rectangle for this purpose? Thank you. You should use setStroke to set a stroke of the Graphics2D object. The example at http://www.java2s.com gives you some code examples. The following code produces the image below: import java.awt.*; import java.awt.geom.Line2D; import javax.swing.*; public class FrameTest { public static void main(String[] args) { JFrame jf = new JFrame(""Demo""); Container cp = jf.getContentPane(); cp.add(new JComponent() { public void paintComponent(Graphics g) { Graphics2D g2 = (Graphics2D) g; g2.setStroke(new BasicStroke(10)); g2.draw(new Line2D.Float(30 20 80 90)); } }); jf.setSize(300 200); jf.setVisible(true); } } (Note that the setStroke method is not available in the Graphics object. You have to cast it to a Graphics2D object.) +1 for the illustration! Also consider `g2.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON)` Yes. Good point."
654,A,Java Cross Hatching Texture Any know how to recreate a cross hashing texture in Java? The C# code belows shows how to accomplish this for the .NET framework. The Java snippet is close but I've been unable to correctly rotate the lines by 45 degrees. C# HatchBrush crossHatch = new HatchBrush(HatchStyle.Cross somecolor somecolor); Java BufferedImage bufferedImage = new BufferedImage(5 5 BufferedImage.TYPE_INT_ARGB); Graphics2D g2 = bufferedImage.createGraphics(); g2.setColor(Color.BLUE); g2.fillRect(0 0 5 5); g2.setColor(pinColor); g2.fillOval(0 0 5 5); // paint with the texturing brush Rectangle2D rect = new Rectangle2D.Double(0 0 5 5); g2d.setPaint(new TexturePaint(bufferedImage rect)); g2d.fill(shape); Thanks in advance. Here's one that should cross-hatch at 5-pixel intervals: BufferedImage bufferedImage = new BufferedImage(5 5 BufferedImage.TYPE_INT_ARGB); Graphics2D g2 = bufferedImage.createGraphics(); g2.setColor(backColor); g2.fillRect(0 0 5 5); g2.setColor(stripeColor); g2.drawLine(0 0 5 5); // \ g2.drawLine(0 5 5 0); // / // paint with the texturing brush Rectangle2D rect = new Rectangle2D.Double(0 0 5 5); g2d.setPaint(new TexturePaint(bufferedImage rect)); g2d.fill(shape);
655,A,"How to concatenate icons into a single image with ImageMagick? I want to use CSS sprites on a web site instead of separate image files for a large collection of small icons that are all the same size. How can I concatenate (tile) them into one big image using ImageMagick? Not ImageMagick but here are instructions on how to do this in Python using the Python Image Library: http://29a.ch/2009/5/14/concatenating-images-using-python convert works much better than montage. It arranges images vertically or horizontally and keeps png transparency. convert *.png -append sprites.png (append vertically) convert *.png +append sprites.png (append horizontally) Well I didn't test but it seems that -background None will allow transparent background (see http://www.imagemagick.org/Usage/montage/#bg )  Use the 'montage' tool: here are some instructions  From the page you linked 'montage' is the tool you want. It'll take a bunch of images and concatenate/tile them into a single output. Here's an example image I've made before using the tool:  You are looking for: montage -background transparent -geometry +4+4 *.png sprite.gif +1 for a ready-to-use command  I like this script for automatical sprite/css generation. ""Building CSS sprites with Bash & Imagemagick"" link is dead ... @jedierikb here is cached version http://goo.gl/SxcXQH"
656,A,"Combining Bitmaps / ImageLists (Win32) Is it possible to create an image list from multiple bitmaps or to combine multiple image lists into one. For sake of simplicity same element dimensions and transparent color could be assumed. Reason: I am currently dealing with a very long image list containing four groups of icons and notable ""reserved"" areas between them. For editing it would be easier to split into four bitmaps combine them on the fly and calculate icon indices dynamically. ImageList_Add can add a bitmap with several images to an existing image list. Is that what you're looking for? I've finally got it working using `ImageList_AddMasked` and using `ILC_MASK` for the new image list. (I couldn't get the transparent color to work with `ImageList_Add'). Thanks!"
657,A,"How do I split an image for use in a web page? I have an image (source file in Paint.NET format) and it has two areas that need to be click-able when on the web. I've never done anything like this (I usually don't do very graphical sites). Is there an easy way to cut this image up and put it into a web layout? What about HTML Image Maps?  You could use image maps but they are difficult to author and if you just need a rectangle area consider using invisible absolutely positioned elements over an element with background image. You don't need to cut the image at all just convert it to jpg format. <html><head><title>Clickable Areas</title><style type=""text/css""> #imageArea { position:relative; width:100px; height:60px; background-image:url(areaImage.jpg) } #imageArea a { width:20px; height:20px; position:absolute; } #area1 { top:20px; left:20px; } #area2 { top:20px; left:60px; } </style></head> <body> <div id=""imageArea""> <a href=""link1.htm"" id=""area1""></a> <a href=""link2.htm"" id=""area2""></a> </div> </body> </html> I think this is probably the way I will go. There are only two areas and I want the background centered but I'll just make its container relative position and then absolute positioning inside of it for the invisible hovering links. Thanks  Try CSS sprites. http://css-tricks.com/css-sprites/  You can do an ""image map""-like thing in CSS. A List Apart has a good guide. Essentially you set the image as a background and use CSS to create invisible links and position them over the correct spots."
658,A,"Why does this thumbnail generation code throw OutOfMemoryException on large files? This code works great for generating thumbnails but when given a very large (100MB+) TIFF file it throws OutOfMemoryExceptions. When I do it manually in Paint.NET on the same machine it works fine. How can I improve this code to stop throwing on very large files? In this case I'm loading a 721MB TIF on a machine with 8GB RAM. The Task Manager shows 2GB used so something is preventing it from using all that memory. Specifically it throws when I load the Image to calculate the size of the original. What gives? /// <summary>Creates a thumbnail of a given image.</summary> /// <param name=""inFile"">Fully qualified path to file to create a thumbnail of</param> /// <param name=""outFile"">Fully qualified path to created thumbnail</param> /// <param name=""x"">Width of thumbnail</param> /// <returns>flag; result = is success</returns> public static bool CreateThumbnail(string inFile string outFile int x) { // Mathematically determine Y dimension int y; using (Image img = Image.FromFile(inFile)) // Exception thrown y = (int)((double)img.Height * ((double)x / (double)img.Width)); // Make thumbnail using (Bitmap bmp = new Bitmap(inFile)) using (Bitmap thumb = new Bitmap((Image)bmp new Size(x y))) using (Graphics g = Graphics.FromImage(thumb)) { g.SmoothingMode = SmoothingMode.HighQuality; g.InterpolationMode = InterpolationMode.High; g.CompositingQuality = CompositingQuality.HighQuality; ImageCodecInfo codec = ImageCodecInfo.GetImageEncoders()[1]; EncoderParameters ep2 = new EncoderParameters(1); ep2.Param[0] = new EncoderParameter(Encoder.Quality 100L); g.DrawImage(bmp new Rectangle(00thumb.Width thumb.Height)); try { thumb.Save(outFile codec ep2); return true; } catch { return false; } } } does this happen with every image of this size or just particular instances? I know I get OOM from GDI for certain images regardless of size and it usually ends up that I am not jumping through the right GDI hoops for the stream at hand. Every very large image JPG or TIF. Mostly TIFs just because that's most of my ""imagebase"". Works great for files under 100MB or so and some of my TIFs go into the GBs. tsilb: I snipped the validation code (since it didn't touch the file) and squeezed the formatting to make the code block less intimidating. Please roll back if you think these changes took out anything important. Can you track down which bit causes the OOM exception -- e.g. loading from inFile creating the new Bitmap from the existing one doing the DrawImage performing the Save? Yep accidentally edited that part out - re-added ""exception thrown"" comment (loads image to calculate the Y dimension) - also a bit of refactoring I noticed I could do while reexamining the code. My wild guess is that you are running it as a 32-bit application. Which limits your app's memory usage to a theoretical 2 GB which is really more like 1.5 GB empirically. Occurs both in Prod in a .NET 4.0 app pool IIS 6 x64 OS; and in debug on an x64 machine with vs2010 rc x64... not sure how else to verify that... If you are really running the process in an Windows x64 env you can verify whether the app is running as x86 by opening your task manager and locating your app's process. If it has no *32 my wild guess is wrong. Dang you're right. Looks like 64-bit can be done in VS but it makes debugging near impossible. How about a way to make it work from IIS?  Have you tried with FastImageGDIPlus. It has worked well for me - but I've never tried loading a 721 MB file."
659,A,How to proportional resize image of any type in .NET? Is possible to resize image proportionally in a way independent of the image type (bmp jpg png etc)? I have this code and know that something is missing (but don't know what): public bool ResizeImage(string fileName string imgFileName ImageFormat format int width int height) { try { using (Image img = Image.FromFile(fileName)) { Image thumbNail = new Bitmap(width height img.PixelFormat); Graphics g = Graphics.FromImage(thumbNail); g.CompositingQuality = CompositingQuality.HighQuality; g.SmoothingMode = SmoothingMode.HighQuality; g.InterpolationMode = InterpolationMode.HighQualityBicubic; Rectangle rect = new Rectangle(0 0 width height); g.DrawImage(img rect); thumbNail.Save(imgFileName format); } return true; } catch (Exception) { return false; } } If not possible how can I resize proportional a jpeg image? I know that using this method but don't know where to put this (!). 8-bit PNG and Gif images require re-quantization and re-dithering when being resized (which is non-trivial!). The best solution is to use a managed library that is designed to handle all the formats/edge cases properly maintain aspect ratio etc. [The ImageResizer library](http://imageresizingin.net/) is a proven tool for this task as well as cropping rotating and flipping. It's open-source so if you feel like rolling your own solution just copy & paste. It also allows lots of other useful things via plugins like watermarking disk caching sql blobs S3 reading etc. Came here to improve my home-grown image resizer and discovered The ImageResizer Library. It's EXCELLENT. +1 for Computer Linguist! The url in Computer Linguist's answer above should be: http://imageresizing.net/ First and foremost you're not grabbing the CURRENT height and width of the image. In order to resize proportionately you'll need to grab the current height/width of the image and resize based on that. From there find the greatest attribute and resize proportionately based on that. For instance let's say the current image is 800 x 600 and you wanna resize proportionately within a 400 x 400 space. Grab the greatest proportion (800) and find it's ratio to the new size. 800 -> 400 = .5 Now take that ratio and multiply by the second dimension (600 * .5 = 300). Your new size is 400 x 300. Here's a PHP example (sorry....you'll get it though) $thumb_width = 400; $thumb_height = 400; $orig_w=imagesx($src_img); $orig_h=imagesy($src_img); if ($orig_w>$orig_h){//find the greater proportion $ratio=$thumb_width/$orig_w; $thumb_height=$orig_h*$ratio; }else{ $ratio=$thumb_height/$orig_h; $thumb_width=$orig_w*$ratio; }  I think your code is fine but taking in the width and the height as parameters is where you're going wrong in my opinion. Why should the caller of this method have to decide how big they want the width and the height? I would suggest changing it to a percentage: public bool ResizeImage(string fileName string imgFileName ImageFormat format int percent) { try { using (Image img = Image.FromFile(fileName)) { int width = img.Width * (percent * .01); int height = img.Height * (percent * .01); Image thumbNail = new Bitmap(width height img.PixelFormat); Graphics g = Graphics.FromImage(thumbNail); g.CompositingQuality = CompositingQuality.HighQuality; g.SmoothingMode = SmoothingMode.HighQuality; g.InterpolationMode = InterpolationMode.HighQualityBicubic; Rectangle rect = new Rectangle(0 0 width height); g.DrawImage(img rect); thumbNail.Save(imgFileName format); } return true; } catch (Exception) { return false; } } It might be very relevant for the caller to know the exact dimension they want the image to be...for example if I have an image that is 1024X768 and I want it scaled down to 50X50 for a thumbnail I shouldn't have to calculate the percentage. Personally I would standardize on width/height and provide a percentage overload that ultimately calls out to the width/height version. Best of both and not a lot of extra code. I know this is an old comment but.. I would personally provide width or height as the constraint and maintain the aspect ration so the images doesn't get all distorted. You may even just want it to resize the largest edge to 50 no matter if that's the height or width.
660,A,tangent of two circles I am trying to write some code that that will draw the line which is a tangent between 2 circles. so far i have been able to draw multiple circles and lines between the centers. i have a class which stores the values used in drawing the circles (radius position). what i need is a method in this class to find all posible tangents between 2 circles. any help would be great. this is what i have so far (it could very well be a load of rubbish) public static Vector2[] Tangents(circle c1 circle c2) { if (c2.radius > c1.radius) { circle temp = c1; c1 = c2; c2 = temp; } circle c0 = new circle(c1.radius - c2.radius c1.center); Vector2[] tans = new Vector2[2]; Vector2 dir = _point - _center; float len = (float)Math.Sqrt((dir.X * dir.X) + (dir.Y * dir.Y)); float angle = (float)Math.Atan2(dir.X dir.Y); float tan_length = (float)Math.Sqrt((len * len) - (_radius * _radius)); float tan_angle = (float)Math.Asin(_radius / len); tans[0] = new Vector2((float)Math.Cos(angle + tan_angle) (float)Math.Sin(angle + tan_angle)); tans[1] = new Vector2((float)Math.Cos(angle - tan_angle) (float)Math.Sin(angle - tan_angle)); Vector2 dir0 = c0.center - tans[0]; Vector2 dir1 = c0.center - tans[1]; Vector2 tan00 = Vector2.Add(Vector2.Multiply(tans[0] (float)c2.radius) c1.center); Vector2 tan01 = c2.center; Vector2 tan10 = Vector2.Add(Vector2.Multiply(tans[1] (float)c2.radius) c1.center); Vector2 tan11 = c2.center; } IMHO you should first try to solve the problem with pencil and paper before you start coding. This link at Mathworld seems to be a good starting point. EDIT: The article on this page looks promising. i have done that but it is different when i put it into code
661,A,"Recommendations for a simple 2D graphics python library that can output to screen and pdf? I'm looking for an easy-to-use graphics lib for python that can output to screen as well as pdf. So I would use code to draw some stuff (simple prims like ovals rectangles lines and points) to screen and then when things look good have it output to pdf. Make sense? Thanks! Blue If you use Tkinter you can draw on a Canvas widget then use its .postscript method to save the contents as a PostScript file which you can convert to PDF using ps2pdf. postscript(self cnf={} **kw) Print the contents of the canvas to a postscript file. Valid options: colormap colormode file fontmap height pageanchor pageheight pagewidth pagex pagey rotate witdh x y.  Matplotlib should be able to do it. See event handling here: http://matplotlib.sourceforge.net/examples/event_handling/index.html  You can use the Python Imaging Library for drawing images which can easily be displayed in various UIs e.g. by displaying a jpg. Then use ReportLab. Here's an example which shows how to use ReportLab with an image. I'm not sure what you mean by drawing to ""screen"" i.e. if you're working with a specific UI toolkit. But if it's acceptable to draw and display PDFs without using an intermediate image (jpg etc) then you might consider the PyX library which makes it quite simple to do graphics with PDFs.  This is a tricky question because there are so many libraries available - there is a trade-off between beauty/easiness. What I've done and works great is to produce the Postscript directly it is not difficult at all and you can preview it using Ghostview; converting tyo PDF is trivial (ps2pdf). Learning how to tell Postscript to create lines and circles is extremely simple. If you want more extensibility then go to Matplotlib but beware of the many times when it will ""decide for you what looks best"" even if you don't like it. Good luck.  Creating PDFs is always a pain it doesn't make sense if you do not aim to lose sanity. With that said you are aiming to do two completely different things: when you draw to screen you draw into a raster bitmap while PDFs are mostly dynamic like HTML. (unlike HTML they are more prone to be the same over different platforms but that's beside the point) If you really want to do that the solution might be finding something that outputs PDFs and then showing the generated PDF on screen at every step. I guess that's the only way to have WYSIWYG results.  You could look into matplotlib which is mainly for plotting but you could probably do some basic drawing. Then there is pygame. But I'm not so sure if it can generate a pdf however you can do 2D graphics with it. There is something called ReportLab that can generate pdf's. Here is a bunch of tutorials using it."
662,A,How do I get the resolution of the main monitor in Mac OS X in C++? I have a graphical app that needs to test the resolution of the display it is starting up on in Mac OS X to ensure it is not larger than the resolution. This is done before the window itself is initialized. If there is more than one display it needs to be the primary display. This is the display that hardware accelerated (OpenGL) apps will start up on in Full Screen and is typically the display that has the menu bar at the top. In Windows I can successfully use GetSystemMetrics(). How can I do this on OS X? Using CoreGraphics: CGRect mainMonitor = CGDisplayBounds(CGMainDisplayID()); CGFloat monitorHeight = CGRectGetHeight(mainMonitor); CGFloat monitorWidth = CGRectGetWidth(mainMonitor); More information at Apple's Quartz Display Services Reference. Note for future generations: this code uses Carbon so it will not work in 64-bit applications on Leopard or Snow Leopard. Then what to do for 64 bit application
663,A,"Percentage Error for 2 sets of 3D points I have a reference set of n points and another set which 'approximates' each of those points. How do I find out the absolute/percentage error between the approximation and my reference set. Put in other words I have a canned animation and a simulation. How do I know how much is the 'drift' between the 2 in terms of a single number? That is how good is the simulation approximating the vertices as compared to those of the animation. I actually do something like this for all vertices: |actual - reference|/|actual| and then average out the errors by dividing the number of verts. Is this correct at all? If you sum the formula you have over all vertices (and then divide by the number of verts) you will have calculated the average percentage error in position for all vertices. However this percentage error is probably not quite what you want because vertices closer to the origin will have a greater ""percentage error"" for the same displacement because their magnitude is smaller. If you don't divide by anything at all you will have the average drift in world units which may be exactly what you want: average_drift = sum(1->numvertices |actual - reference|) / numvertices You may want to divide by something more appropriate to your particular situation to get a meaningful unitless number. If you divide average_drift by the height of your model you will have the error as a percentage of the model size which could be useful. If individual vertices are likely to have more error if they are a long distance from a vertex 'parented' to them as could be the case if they are vertices of a jointed model you could divide each error by the length of their parent joint to get the average error normalised for joint orientation -- i.e. what the average drift would be if each joint were of unit length: orientation_drift = sum(1->numvertices |actual - reference| / jointlength) / numvertices  Does this measurement really have to be a percentage value? I'm guessing you have one reference set and then several sets that approximate this set and you want to pick the one that is ""the best"" in some sense. I'd add the squared distances between the actual and the reference: avgSquareDrift = sum(1..n |actual - reference|^2) / numvertices Main advantage with this approach is that we dont need to take apply the square root which is a costly operation."
664,A,"Unsafe C# and pointers for 2d rendering good or bad? I am writing a c# control that wraps DirectX 9 and provides a simplified interface to perform 2d pixel level drawing. .NET requires that I wrap this code in an unsafe code block and compile with the allow unsafe code option. I'm locking the entire surface which then returns a pointer to the locked area of memory. I can then write pixel data directly using ""simple"" pointer arithmetic. I have performance tested this and found a substantial speed improvement over other ""safe"" methods I know of. Is this the fastest way to manipulate individual pixels in a C# .NET application? Is there a better safer way? If there was an equally fast approach that does not require pointer manipulation it would be my preference to use that. (I know this is 2008 and we should all be using directx 3d opengl etc however this control is to be used exclusively for 2d pixel rendering and simply does not require 3d rendering.) I recently was tasked with creating a simple histogram control for one of our thin client apps (C#). The images that I was analyzing were about 1200x1200 and I had to go the same route. I could make the thing draw itself once with no problem but the control needed to be re-sizable. I tried to avoid it but I had to get at the raw memory itself. I'm not saying it is impossible using the standard .NET classes but I couldn't get it too work in the end.  yes that is probably the fastest way a few years ago i had to compare two 1024x1024 images at the pixel level; the get-pixel methods took 2 minutes the unsafe scan took 0.01 seconds.  I have also used unsafe to speed up things of that nature. The performance improvemets are dramatic to say the least. The point here is that unsafe turns off a bunch of things that you might not need as long as you know what you're doing. Also check out DirectDraw. It is the 2D graphics component of DirectX. It is really fast.  Using unsafe pointers is the fastest way to do direct memory manipulation in C# (definitely faster than using the Marshal wrapper functions). Just out of curiosity what sort of 2d drawing operations are you trying to perform? I ask because locking a DirectX surface to do pixel level manipulations will defeat most of the hardware acceleration benefits that you would hope to gain from using DirectX. Also the DirectX device will fail to initialize when used over terminal services (remote desktop) so the control will be unusable in that scenario (this may not matter to you). DirectX will be a big win when drawing large triangles and transforming images (texture mapped onto a quad) but won't really perform that great with single pixel manipulation. Staying in .Net land one alternative is to keep around a Bitmap object to act as your surface using LockBits and directly accessing the pixels through the unsafe pointer in the returned BitmapData object."
665,A,What is the difference between texture filtering and texture sampling? Are these concepts one and the same? I've seen them used in multiple contexts. Texture sampling is the act of retrieving data from a texture. Texture filtering is the algorithm by which a pixel or group of pixels within the texture are fetched (and possibly combined) in order to produce the result of a sampling of a texture.  Some links for you: http://www.extremetech.com/article2/02845115516300.asp http://blogs.msdn.com/shawnhar/archive/2009/09/08/texture-filtering.aspx
666,A,"Direct3D Geometry: Rotation Matrix from Two Vectors Given two 3D vectors A and B I need to derive a rotation matrix which rotates from A to B. This is what I came up with: Derive cosine from acos(A . B) Derive sine from asin(|A x B| / (|A| * |B|)) Use A x B as axis of rotation Use matrix given near the bottom of this page (axis angle) This works fine except for rotations of 0° (which I ignore) and 180° (which I treat as a special case). Is there a more graceful way to do this using the Direct3D library? I am looking for a Direct3D specific answer. Edit: Removed acos and asin (see Hugh Allen's post) Maybe you can use D3DXMatrixLookAtLH ?  No you're pretty much doing it the best way possible. I don't think there is a built-in DirectX function that does what you want. For step 4 you can use D3DXMatrixRotationAxis(). Just be careful about the edge cases such as when |A| or |B| is zero or when the angle is 0° or 180°.  It's probably more of a typo than a thinko but acos(A.B) is the angle not its cosine. Similarly for point 2. You can calculate the sin from the cos using sin^2 + cos^2 = 1. That is sin = sqrt(1-cos*cos). This would be cheaper than the vector expression you are using and also eliminate the special cases for 0/180 degrees. You don't ""lose the sign of cosine"" because you use the cosine as-is in the rotation matrix. What you should really ask is do you get the right sign for sin? In this case (0-180 degrees) you only want the positive square root so it's all good. I am not sure that calculating the sine from sqrt(1-cosine*cosine) will work. This is because cosine*cosine will cause us to lose the sign of the cosine I agree completely! Hugh What about the 0 to -180 range? Negative angles aren't necessary for this particular application. When rotating from B to A the axis vector will be negated rather than the rotation angle. That's how things ended. Thanks again for a very prompt reply.  You might look at the following article from siggraph link text"
667,A,"Polygon math Given a list of points that form a simple 2d polygon oriented in 3d space and a normal for that polygon what is a good way to determine which points are specific 'corner' points? For example which point is at the lower left or the lower right or the top most point? The polygon may be oriented in any 3d orientation so I'm pretty sure I need to do something with the normal but I'm having trouble getting the math right. Thanks! are you asking about the http://en.wikipedia.org/wiki/Convex_hull? Insufficient information provided to really give an answer. If the polygon is rotated arbitrarily in the plane of interest what does lower right mean? We've not been told anything about the polygon. Rectangle? Triangle? Hexagon? perhaps you can refer the following links and download a paper J. E. Boyce D. P. Dobkin R. L. Drysdale III and L. J. Guibas. Finding extremal polygons. SIAM J. Comput. 14:134-147 1985 Are you looking for a bounding box? I'm not sure the normal has anything to do with what you are asking. To get a Bounding box keep 4 variables: MinX MaxX MinY MaxY Then loop through all of your points checking the X values against MaxX and MinX and your Y values against MaxY and MinY updating them as needed. When looping is complete your box is defined as MinXMinY as the upper left MinX MaxY as upper right and so on... Response to your comment: If you want your box after a projection what you need is to get the ""transformed"" points. Then apply bounding box loop as stated above. Transformed usually implies 2D screen coordinates after a projection(scene render) but it could also mean the 2D points on any plane that you projected on to. Sorry I should have been more clear. It's a face in 3d space but it's really just a 2d polygon. So I want to know the specific corners if you were facing the polygon directly aligned with the normal (so it would appear as a 2d polygon). No wonder I can't solve it I can't even explain it. :) ahh then you are looking for the ""transformed"" points  I have a silly idea but at the risk of gaining a negative a point I'll give it a try: Get the minimum/maximum value from each three-dimensional axis of each point on your 2d polygon. A single pass with a loop/iterator over the list of values for every point will suffice simply replacing the minimum and maximum values as you go. The end result is a list that has the ""lowest"" X Y Z coordinates and ""highest"" X Y Z coordinates. Iterate through this list of min/max values to create each point (""corner"") of a ""bounding box"" around the object. The result should be a box that always contains the object regardless of axis examined or orientation (no point on the polygon will ever exceed the maximum or minimums you collect). Then get the distance of each ""2d polygon"" point to each corner location on the ""bounding box""; the shorter the distance between points the ""closer"" it is to that ""corner"". Far from optimal certainly crummy but certainly quick. You could probably post-capture this during the object's rotation by simply looking for the min/max of each rotated x/y/z value and retaining a list of those values ahead of time.  Project it onto a plane and get the bounding box.  I believe that your question requires some additional information - namely the coordinate system with respect to which any point could be considered ""topmost"" or ""leftmost"". Don't forget that whilst the normal tells you which way the polygon is facing it doesn't on its own tell you which way is ""up"". It's possible to rotate (or ""roll"") around the normal vector and still be facing in the same direction. This is why most 3D rendering systems have a camera which contains not only a ""view"" vector but also ""up"" and ""right"" vectors. Changes to the latter two achieve the effect of the camera ""rolling"" around the view vector.  A possible algorithm would be Find the normal which you can do by using the cross product of vectors connecting two pairs of different corners Create a transformation matrix to rotate the polygon so that it is planer in XY space (i.e. normal alligned along the Z axis) Calculate the coordinates of the bounding box or whatever other definition of corners you are using (as the polygon is now aligned in 2D space this is a considerably simpler problem) Apply the inverse of the transformation matrix used in step 2 to transform these coordinates back to 3D space.  If you can assume that there is some constraints regarding the shapes then you might be able to get away with knowing less information. For example if your shape was the composition of a small square with a long thin triangle on one side (i.e. a simple symmetrical geometry) then you could compare the distance from each list point to the ""center of mass."" The largest distance would identify the tip of the cone the second largest would be the two points farthest from the tip of the cone etc... If there was some order to the list like points are entered in counter clockwise order (about the normal) you could identify all the points. This sounds like a bit of computation so it might be reasonable to try to include some extra info with your shapes like the ""center of mass"" and a reference point that is located ""up"" above the COM (but not along the normal). This will give you an ""up"" vector that you can cross with the normal to define some body coordinates for example. Also the normal can be defined by an ordering of the point list. If you can't assume anything about the shapes (or even if the shapes were symmetrical for example) then you will need more data. It depends on your constraints.  If you know that the polygon in 3D is ""flat"" you can use the normal to transform all 3D-points of the vertices to a 2D-representation (of the points with respect to the plan in which the polygon is located) - but this still leaves you with defining the origin of this coordinate-system (but this don't really matter for your problem) and with the orientation of at least one of the axes (if you want orthogonal axes you can still rotate them around your choosen origin) - and this is where the trouble starts. I would recommend using the Y-axis of your 3D-coordinate system project this on your plane and use the resulting direction as ""up"" - but then you are in trouble in case your plan is orthogonal to the Y-axis (now you might want to use the projected Z-Axis as ""up""). The math is rather simple (you can use the inner product (a.k.a. scalar product) for projection to your plane and some matrix stuff to convert to the 2D-coordinate system - you can get all of it by googling for raytracer algorithms for polygons.  You would need more information in order to make that decision. A set of (co-planar) points and a normal is not enough to give you a concept of ""lower left"" or ""top right"" or any such relative identification. Viewing the polygon from the direction of the normal (so that it appears as a simple 2D shape) is a good start but that shape could be rotated to any arbitrary angle. Is there some other information in the 3D world that you can use to obtain a coordinate-system reference? What are you trying to accomplish by knowing the extreme corners of the shape?"
668,A,Draw a shadow behind UIWebView I know similar questions have been asked before so don't get snarky and link to previous answers. The reason I am repeating this is that none of the answers have worked. I have a UIWebView and I want to draw a pretty drop-shadow behind it. I have tried subclassing and using some CoreGraphics goodness in drawRect: but to no avail. Can anyone point me in the right direction? Thanks. You could place a UIImageView under the UIScrollView with a slight offset and give the image view a shadowy-looking image. In this case it would be convenient to use the stretchableImageWithLeftCapWidth:topCapHeight: method of UIImage to get a shadow image that can have nicely blurred edges and corners without distorting if you alter its size. I think the usual quartz shadows are hard to apply to a UIWebView since you don't really have access to the raw drawing code which would be the best place to plug that in. Addendum Here's a sample drop shadow image: . It has 4 pixel cap size all around for use with the above stretchableImage method and an alpha of 80% (so it's 20% transparent in the middle tapering out at the edges). Feel free to use it. They're easy to make with a selection + feathering in your favorite graphics app. Thanks a lot! Very helpful!
669,A,turn a line into a rectangle I have a method that draws a line between two points. This works pretty well but now I want to make this line into a rectangle. How can I get the points on the left and right side of each of the line points to make it into a rectangle that I can draw? It is almost as though I need to somehow figure out how to get perpendicular lines programatically.... watch out IBM has that patented! http://www.forbes.com/asap/2002/0624/044.html that is insane... fat lines should be free for everyone I may have messed up explaining this here. The line is in the middle of the rectangle. So imagine a rectangle with a line cutting it in half lengthwise not diagonal I'm guessing you basically want fat lines? Lets assume the line is specified by two points (x0 y0) and (x1 y1) we then have: float dx = x1 - x0; //delta x float dy = y1 - y0; //delta y float linelength = sqrtf(dx * dx + dy * dy); dx /= linelength; dy /= linelength; //Ok (dx dy) is now a unit vector pointing in the direction of the line //A perpendicular vector is given by (-dy dx) const float thickness = 5.0f; //Some number const float px = 0.5f * thickness * (-dy); //perpendicular vector with lenght thickness * 0.5 const float py = 0.5f * thickness * dx; glBegin(GL_QUADS); glVertex2f(x0 - px y0 + py); glVertex2f(x1 - px y1 + py); glVertex2f(x1 + px y1 - py); glVertex2f(x0 + px y0 - py); glEnd(); Since you're using OpenGL ES I guess you'll have to convert the immediate mode rendering (glBegin glEnd etc) to glDrawElements. You'll also have to convert the quad into two triangles. One final thing I'm a little tired and uncertain if the resulting quad is counterclockwise or clockwise so turn of backface culling when you try this out (glDisable(GL_CULL)). Andreas I have to replace (-dy) by (dy). I think that's because you have e.g. 'y0 + py' not 'y0 - py'. PS Here's a version that has square end caps: http://pastebin.com/KQMc6jp5  Since you've asked this question with OpenGL taged I'll assume that you wish for a solution that is close to OpenGL. As you know you have two points that make up a line. Lets call them x1y1 and x2 y2 Lets also assume that x1 y1 is the top left point and x2 y2 is the bottom right. Your 4 points to plot will be [you need to perserve order to make a rectangle with GL_LINES) Height:y1-y1 Width: x2-x1 (x1 y1) (x1+width y1) (x2 y2) (x2-width y2)  drawRect(xywh) tl = (xy) tr = (x+w y) br = (x+w y+h) bl = (x y+h)
670,A,"Graphics Gems IV. Binary Image Thinning Using Neigborhood Maps What does this expression's algorithm mean? p = ((p<<1)&0666) | ((q<<3)&0110) | (Image->scanLine(y+1)[x+1] != 0); Algorithm ""Binary Image Thinning Using Neigborhood Maps"" in a book ""Graphics Gems IV"":  static int masks[] = {0200 0002 0040 0010}; uchar delete_[512] = { 00000000 00000000 00010011 01110011 00000000 00000000 00111011 00110011 00000000 00000000 00000000 11110011 00000000 00000000 00000000 00110011 00000000 00000000 00000000 11110011 00000000 00000000 10111011 11111111 00000000 00000000 10000000 11110011 00000000 00000000 10111011 11111111 00000000 00000000 00000000 00000000 00000000 00000000 10111011 00110011 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00110011 00000000 00000000 10000000 11110011 00000000 00000000 10111011 11111111 00000000 00000000 10000000 11110011 00000000 00000000 10111011 11111111 }; int xsize ysize; int x y; int i; int count = 1; int p q; uchar *qb; int m; xsize = Image->width(); ysize = Image->height(); qb = (uchar*) malloc (xsize*sizeof(uchar)); qb[xsize-1] = 0; while(count) { count = 0; for (i = 0; i < 4; ++i) { m = masks[i]; p = Image->scanLine(0)[0] != 0; for (x = 0; x < xsize-1; ++x) qb[x] = p = ((p<<1)&0006) | (Image->scanLine(0)[x+1] != 0); // Scan image for pixel deletion candidates. for (y = 0; y < ysize-1; ++y) { q = qb[0]; p = ((q<<3)&0110) | (Image->scanLine(y+1)[0] != 0); for (x = 0; x < xsize-1; ++x) { q = qb[x]; p = ((p<<1)&0666) | ((q<<3)&0110) | (Image->scanLine(y+1)[x+1] != 0); qb[x] = p; if ((p&m)==0 && delete_[p]) { count++; Image->scanLine(y)[x] = 0; } } And now we see the importance of naming constants. C programmers believe meaningful identitifer names lead to poor execution time:) this answer for them who read a book This line has a typo: `p = ((q<<3)&0110) | (Image->scanLine(y+1)[0] != 0);` The `3` should be a `2`. I think... It looks like a bitwise manipulation. Do you not understand the individual operations or do you not understand why that manipulation is being done (if the later a link to a description of the algorithm is essential BTW)? Operators in that line: << right shift operator (move the bits of p to more significant positions according to the RH argument) & in this context the bitwise and | the bitwise or A helpful thing to recall here is that integer literals that start with '0' are expressed in octal which means that you can read the binary digits right out of the on-screen representation (well with a little practice anyway). The constants here are (666)_8 = (110110110)_2 and (0110)_8 = (001001000)_2. This manipulation is a mask 3x3 that walk along the image but don't understand why that manipulation is being done) @dmckee - you got the 0666 bit pattern wrong. 110110110 helps the OP see that it is the inverted bit pattern for 0110 @nobugz: ACK. I'll just put my brain in gear here shall I? @dmckee - surely the number was the source of the problem. @nobugz: Could be. I also have just now answered (666)_10 questions on Stack Overflow. My inner numerologist is screaming. @dmckee - wow 666 answers indeed. What are the odds?  (See commented source code) The variables m p q and elements of the qb array are 9-bit numbers that represent the 3x3-pixel ""neighborhood"" of a pixel. Suppose your image looks like this (each letter represents a pixel which is either 'on' or 'off' (1 or 0 black or white):  ---x--- 0123456 | 0 abcdefg | 1 hijklmn y 2 opqrstu | 3 vwxyz{| The pixel at (xy) location (21) is j. The neighborhood of that pixel is bcd ijk // 3x3 grid centered on j pqr Since each pixel has a binary value the neighborhood can be represented in 9 bits. The neighborhood above can be written out linearly expressed in binary as bcd_ijk_pqr. The grouping of 3 pixels in a row makes octal a good choice since each octal digit represents three bits. Once you have a neighborhood expressed as a 9-bit value you can do bit-manipulation on it. An operation such as ((p << 1) & 0666) takes a neighborhood shifts all bits to the left one position and clears the rightmost column of bits. For example the shift changes bcd_ijk_pqr to cdi_jkp_qr@ (where @ represents a 'null' bit by default 0). Then the mask changes it to cd@_jk@_qr@. Expressed in 3x3 grid form: cd@ jk@ qr@ Essentially the whole grid has been shifted to the left. Similarly an operation such as ((q << 3) & 0110) shifts all bits three positions (moves rows up) and clears the first two columns of bits. So bcd_ijk_pqr becomes ijk_pqr_@@@ and then after the masking becomes @@k_@@r_@@@. The gist of the algorithm is to evaluate the neighborhood of each pixel to determine whether to turn that pixel off (delete it). This line does the evaluation: if ((p&m)==0 && delete_[p]) Everything that precedes that line is done to set up the neighborhood in p. The code is written so that each pixel value is read exactly once per pass. The qb array stores the neighborhood for each pixel in the previous scanline. Note that the elements of qb are only 8-bits wide. This means the upper-left pixel of the neighborhood is omitted. That is not a problem since any time qb is used it gets shifted up a row. So to answer your question about what this line does: p = ((p<<1)&0666) | ((q<<3)&0110) | (Image->scanLine(y+1)[x+1] != 0); It creates the neighborhood of a pixel by merging the following: the neighborhood of the previous pixel on the same line shifted to the left the right column of the neighborhood of the pixel one row higher shifted up the (x+1y+1) pixel of the image put into the ""southwest"" corner For example the neighborhood about j is calculated as: p = bc@_ij@_pq@ | @@d_@@k_@@@ | r bc@ | @@d | @@@ bcd p = ij@ | @@k | @@@ = ijk pq@ | @@@ | @@r pqr"
671,A,Easiest way to create and render 3D model by rotating a 2D silhouette I have a black and white 2D drawing of a silhouette (say a chess piece) that I would like to rotate around an axis to create a 3D object. Then I want to render that 3D object from multiple angles using some sort of raytracing software saving each angle into a separate file. What would be the easiest way to automatically (repeatedly) 1. get a vector path from the 2d drawing 2. create the 3D model by rotating it 3. import it into the raytracer. I haven't chosen a specific raytracer yet but Sunflow has caught my eye. Texturing/bump mapping would be nice but non-essential I found a pretty cool site where you can do this online interactively: http://www.fi.uu.nl/toepassingen/00182/toepassing_wisweb.en.html No great detail revolution but maybe you can find the code and extend it to your needs. Brilliant in it's interactivity. Nice find!  The modeling feature you're looking for is a Lathe. Sunflow can import 3ds files and blender files. I've never used blender but here's a tutorial for using the lathe to make a wine glass. You'd replace the silhouette of the wine glass with your shape: http://www.blendermagz.com/2009/04/14/blender-3d-lathe-modeling-wine-glass/ Blender is FOSS you can down load it here: www.blender.org/download/get-blender/ (can't post more than one link so you'll have to type this one in yourself :-) AFAIK you can script using python in blender - so the process could be automated. I think I'm going to end up using Luxrender (http://www.luxrender.net) and some glue code to convert my models into the right format.
672,A,"""Screen"" effect in Java 2D graphics This is a question that's been bugging me for some time now: In photoshop/GIMP there is a ""screen"" layer composition mode. This mode has bright colours have a strong alpha and dark colours a weak one. Black is entirely transparent white entirely opaque. I would dearly love to be able to replicate this composite using Java 2D graphics but my repeated attempts at trying to coax AlphaComposite into this have failed - and indeed I think this is outside of AlphaComposite's capabilities. A visual example can be seen here . Any ideas on how to do this? Any chance we can get a screenshot of what you're talking about? Now that there is a screenshot you may want to break it up into multiple screenshots as certain browsers (read: Firefox) are putting part of your image underneath the tags box. Whoops I tried to shrink it but ended up making it disappear. Trying to fix it now... Weird the tag was getting stripped. I had to roll it back to the original version. Anyone else want to try? Looks like some reference code is here: http://www.curious-creature.org/2006/09/20/new-blendings-modes-for-java2d/"
673,A,"What's a good affordable ""knob"" library? I'm currently thinking of a new pet project an ""editor"" for MIDI-enabled synths. I've got the MIDI side covered I suppose but what I'm looking for right now is something that can pass for nice ""dials"" and ""knobs"" like you see in Ableton Live Reason Reaktor and so forth. Putting my form full of trackbars is sort of wasteful y'know? So what is a nice affordable .NET 2.0 library that has that sort of graphical components? I've never used this myself but found KineticaRT controls. $67.06. GMSI.Net Knob v2.0. $169.00.  Check out some of these. If none just do search for "".net gauges"". Most of the major UI control libraries have a gauge or dashboard library. http://www.perpetuumsoft.com/Product.aspx?lang=en&pid=44 http://www.brothersoft.com/software_developer/microsoft_.net/.net_dashboard_suite_56186.html http://www.componentone.com/SuperProducts/GaugesSilverlight/ http://www.dundas.com/  1- http://www.codeproject.com/KB/cs/industrial_controls.aspx has some related controls (C#) 2- http://www.c-sharpcorner.com/UploadFile/desaijm/KnobControlusingWindowsForms11182005004925AM/KnobControlusingWindowsForms.aspx is another C# based Knob Control Used the (2) personally on one project and it worked great (though not visually very strong). (1) looks cooler but havn't explored I like the first one. The behaviour of the LBKnob may not be exactly what I need but since it's free and the source is included I might get by with some tweaking. Thanks!"
674,A,"Animate rotating SVG element on webpage So I have an SVG file created in Inkscape embedded in a webpage and I'd like it to rotate slowly. I've tried using Javascript and inserting animation commands directly into the SVG but nothing works. I don't want to load in an entire JS library for this one task. This is what I have so far: <html> <body bgcolor=""#333333""> <embed src=""gear.svg"" id=""gear"" width=""1000"" height=""1000"" style=""position: absolute; top: -500px; left: -500px;"" /> <script type=""text/javascript""> var gear = document.getElementById(""gear""); window.setInterval(function() { // Somehow animate the gear. } 10); </script> </body> </html> Add a <g> element inside your <svg> element that wraps everything inside the <svg> and add <animateTransform type=""rotate"" attributeName=""transform"" values=""0 cx cy;360 cx cy"" dur=""30s""/> as a child of that <g> element and replace ""cx"" and ""cy"" with whatever actual absolute point you want to use as the center of rotation e.g ""100 300"". Should work in the newest generation of web browsers apart from IE9. Animate the rotatation using CSS3 2d transforms noting that you'll have to use at least three different vendor prefixes in the process (-webkit-transform -moz-transform -o-transform). Should work in the newest generation of web browsers unsure about IE9 though. Add a <g> element inside your <svg> element that wraps everything inside the <svg> and then add a <script> inside it that does yourGelement.setAttribute(""transform"" ""rotate("" + (newRotation++) + "" cx cy)"") from a window.setInterval timer as before replace the cx and cy with your center of rotation. This solution should be less than 10 lines of code and should work fine even in older implementations that don't support declarative (SMIL) animations (e.g IE9 Firefox2 Safari3). The firefox 3.7 alpha I have supports at least parts of it: http://blog.dholbert.org/2009/10/smil-enabled-by-default-on-nightly.html. Feel free to test it yourself against the w3c svg testsuite http://dev.w3.org/SVG/profiles/1.1F2/test/harness/ Good complete answer you were faster than me! :-) Does FF3 support Smil animation now? I haven't tried yet.  Interesting topic because AFAIK currently Firefox doesn't support animation in SVG. So I made a little investigation and found a working solution. Tested in Firefox 3.6 IE7 with Adobe plug-in Opera 10.51 Safari 4.0.5 Chrome 5.0. The background of the SVG area has no transparency in IE7 Safari and Chrome... I might try with the object tag (not supported by IE probably need some conditional HTML...). [EDIT] OK I changed to use the more standard object (embed have never been defined in HTML...) except for IE where it isn't well supported by Adobe SVG plugin. The latter allows to add an attribute to have transparency of the embed object. For Webkit-based browsers no transparency: see object embedded in HTML: default background should be transparent bug. The HTML code: <!DOCTYPE html PUBLIC ""-//W3C//DTD HTML 4.01//EN"" ""http://www.w3.org/TR/html4/strict.dtd""> <html lang=""en""> <head> <title>Animating SVG</title> </head> <body bgcolor=""#CCAAFF"" onload=""RotateSVG()""> <!--[if !IE]> --> <object id=""gear"" data=""gear.svg"" type=""image/svg+xml"" width=""500"" height=""500"" style=""position: absolute; top: -250px; left: -250px;""> <!--<![endif]--> <embed id=""gear"" src=""gear.svg"" type=""image/svg+xml"" width=""500"" height=""500"" wmode=""transparent"" style=""position: absolute; top: -250px; left: -250px;""/> <!--[if !IE]> --> </object> <!--<![endif]--> <div onclick=""RotateSVG()"" style=""position: absolute; top: 250px; background-color: #ACF;"">Start / Stop</p> <script type=""text/javascript""> var animator; var angle = 0; function RotateSVG() { if (animator != null) { // Just stop clearInterval(animator); animator = null; return; } var svgTag = document.getElementById(""gear""); var svgDoc = null; try { // Most modern browsers understand this svgDoc = svgTag.getSVGDocument(); } catch (ex) {} // Ignore error if (svgDoc == undefined) { svgDoc = svgTag.contentDocument; // For old Mozilla? if (svgDoc == undefined) { alert(""Cannot get SVG document""); return; } } var gear = svgDoc.getElementById(""gearG""); if (gear == null) { alert(""Cannot find gearG group""); return; } animator = setInterval( function () { angle += 5; gear.setAttribute(""transform"" ""rotate("" + angle + "" 250 250)""); } 100); } </script> </body> </html> The SVG code I used (only the ID is important the SVG is from Mozilla SVG Project): <svg xmlns=""http://www.w3.org/2000/svg"" xmlns:xlink=""http://www.w3.org/1999/xlink"" version=""1.1"" baseProfile=""full""> <!-- http://www.mozilla.org/projects/svg/ --> <g id=""gearG"" fill-opacity=""0.7"" stroke=""black"" stroke-width=""0.1cm""> <circle cx=""6cm"" cy=""2cm"" r=""100"" fill=""red"" transform=""translate(050)"" /> <circle cx=""6cm"" cy=""2cm"" r=""100"" fill=""blue"" transform=""translate(70150)"" /> <circle cx=""6cm"" cy=""2cm"" r=""100"" fill=""green"" transform=""translate(-70150)"" /> </g> </svg>"
675,A,"Detecting coincident subset of two coincident line segments This question is related to: How do I determine the intersection point of two lines in GDI+? (great explanation of algebra but no code) How do you detect where two line segments intersect? (accepted answer doesn't actually work) But note that an interesting sub-problem is completely glossed over in most solutions which just return null for the coincident case even though there are three sub-cases: coincident but do not overlap touching just points and coincident overlap/coincident line sub-segment For example we could design a C# function like this: public static PointF[] Intersection(PointF a1 PointF a2 PointF b1 PointF b2) where (a1a2) is one line segment and (b1b2) is another. This function would need to cover all the weird cases that most implementations or explanations gloss over. In order to account for the weirdness of coincident lines the function could return an array of PointF's: zero result points (or null) if the lines are parallel or do not intersect (infinite lines intersect but line segments are disjoint or lines are parallel) one result point (containing the intersection location) if they do intersect or if they are coincident at one point two result points (for the overlapping part of the line segments) if the two lines are coincident I realize that this question is just asked so you could post your answer. You should mark it as the accepted answer. It wouldn't hurt to use less confrontational language in the question as well FWIW. @tfinniga: I didn't realize it was confrontational until I rewrote it and made it sound like a puzzle instead of a demand. My goal wasn't to make other people do the work for me but rather to prove that no other implementation even _worked_. (If you can prove me wrong and find a really good solution (that's on SO right now) that works flawlessly I would gladly give you 100 rep). Thanks I think that's much better. A bullet-proof implementation for this common need is valuable and the rephrased question is much more pleasant. This is really very simple. If you have two lines you can find two equations in the form of y = mx + b. For example: y = 2x + 5 y = x - 3 So the two lines intersect when y1 = y2 at the same x coordinate so... 2x + 5 = x - 3 x + 5 = -3 x = -8 When x=-8 y1=y2 and you have found the point of intersection. This should be very trivial to translate into code. If there is no intersection point than the slope m of each line will be equal in which case you do not even need to perform the calculation. This is also subtly wrong: when the points are above and below each other the slope is infinite and all hell breaks lose. When the slopes of each line are equal they can still intersect at one point or at a line segment or even not overlap at all.  Sounds like you have your solution which is great. I have some suggestions for improving it. The method has a major usability problem in that it is very confusing to understand (1) what the parameters going in mean and (2) what the results coming out mean. Both are little puzzles that you have to figure out if you want to use the method. I would be more inclined to use the type system to make it much more clear what this method does. I'd start by defining a type -- perhaps a struct particularly if it was going to be immutable -- called LineSegment. A LineSegment consists of two PointF structs representing the end point. Second I would define an abstract base type ""Locus"" and derived types EmptyLocus PointLocus LineSegmentLocus and perhaps UnionLocus if you need to represent the locus that is the union of two or more loci. An empty locus is just a singleton a point locus is just a single point and so on. Now your method signature becomes much more clear: static Locus Intersect(LineSegment l1 LineSegment l2) This method takes two line segments and computes the locus of points that is their intersection -- either empty a single point or a line segment. Note that you can then generalize this method. Computing the intersection of a line segment with a line segment is tricky but computing the intersection of a line segment with a point or a point with a point or anything with the empty locus is easy. And it's not hard to extend intersection to arbitrary unions of loci. Therefore you could actually write: static Locus Intersect(Locus l1 Locus l2) And hey now it becomes clear that Intersect could be an extension method on locus: static Locus Intersect(this Locus l1 Locus l2) Add an implicit conversion from PointF to PointLocus and LineSegment to LineSegmentLocus and you can say things like var point = new PointF(whatever); var lineseg = new LineSegment(somepoint someotherpoint); var intersection = lineseg.Intersect(point); if (intersection is EmptyLocus) ... Using the type system well can massively improve the readability of a program. Thanks for the recommendations and extensions. This is a great method Eric I was previously using enums combined with other objects to provide a result. This is elegant and far superior. Thank you.  @Jared Great question and great answer. The problem can be simplified by representing the position of a point along a line as a function of a single parameter as explained on Joseph O' Rourke's CGA FAQ here. Let r be a parameter to indicate P's location along the line containing AB with the following meaning:  r=0 P = A r=1 P = B r<0 P is on the backward extension of AB r>1 P is on the forward extension of AB 0<r<1 P is interior to AB Thinking along those lines for any point C(cxcy) we calculate r as follows: double deltax = bx - ax; double deltay = by - ay; double l2 = deltax * deltax + deltay * deltay; double r = (ay - cy) * (ay - by) - (ax - cx) * (bx - ax) / l2; This should make it easier to calculate the overlapping segment. Note that we avoid taking square roots because only the square of the length is required. One plus for the link reference. It was useful to me   // port of this JavaScript code with some changes: // http://www.kevlindev.com/gui/math/intersection/Intersection.js // found here: // http://stackoverflow.com/questions/563198/how-do-you-detect-where-two-line-segments-intersect/563240#563240 public class Intersector { static double MyEpsilon = 0.00001; private static float[] OverlapIntervals(float ub1 float ub2) { float l = Math.Min(ub1 ub2); float r = Math.Max(ub1 ub2); float A = Math.Max(0 l); float B = Math.Min(1 r); if (A > B) // no intersection return new float[] { }; else if (A == B) return new float[] { A }; else // if (A < B) return new float[] { A B }; } // IMPORTANT: a1 and a2 cannot be the same e.g. a1--a2 is a true segment not a point // b1/b2 may be the same (b1--b2 is a point) private static PointF[] OneD_Intersection(PointF a1 PointF a2 PointF b1 PointF b2) { //float ua1 = 0.0f; // by definition //float ua2 = 1.0f; // by definition float ub1 ub2; float denomx = a2.X - a1.X; float denomy = a2.Y - a1.Y; if (Math.Abs(denomx) > Math.Abs(denomy)) { ub1 = (b1.X - a1.X) / denomx; ub2 = (b2.X - a1.X) / denomx; } else { ub1 = (b1.Y - a1.Y) / denomy; ub2 = (b2.Y - a1.Y) / denomy; } List<PointF> ret = new List<PointF>(); float[] interval = OverlapIntervals(ub1 ub2); foreach (float f in interval) { float x = a2.X * f + a1.X * (1.0f - f); float y = a2.Y * f + a1.Y * (1.0f - f); PointF p = new PointF(x y); ret.Add(p); } return ret.ToArray(); } private static bool PointOnLine(PointF p PointF a1 PointF a2) { float dummyU = 0.0f; double d = DistFromSeg(p a1 a2 MyEpsilon ref dummyU); return d < MyEpsilon; } private static double DistFromSeg(PointF p PointF q0 PointF q1 double radius ref float u) { // formula here: //http://mathworld.wolfram.com/Point-LineDistance2-Dimensional.html // where x0y0 = p // x1y1 = q0 // x2y2 = q1 double dx21 = q1.X - q0.X; double dy21 = q1.Y - q0.Y; double dx10 = q0.X - p.X; double dy10 = q0.Y - p.Y; double segLength = Math.Sqrt(dx21 * dx21 + dy21 * dy21); if (segLength < MyEpsilon) throw new Exception(""Expected line segment not point.""); double num = Math.Abs(dx21 * dy10 - dx10 * dy21); double d = num / segLength; return d; } // this is the general case. Really really general public static PointF[] Intersection(PointF a1 PointF a2 PointF b1 PointF b2) { if (a1.Equals(a2) && b1.Equals(b2)) { // both ""segments"" are points return either point if (a1.Equals(b1)) return new PointF[] { a1 }; else // both ""segments"" are different points return empty set return new PointF[] { }; } else if (b1.Equals(b2)) // b is a point a is a segment { if (PointOnLine(b1 a1 a2)) return new PointF[] { b1 }; else return new PointF[] { }; } else if (a1.Equals(a2)) // a is a point b is a segment { if (PointOnLine(a1 b1 b2)) return new PointF[] { a1 }; else return new PointF[] { }; } // at this point we know both a and b are actual segments float ua_t = (b2.X - b1.X) * (a1.Y - b1.Y) - (b2.Y - b1.Y) * (a1.X - b1.X); float ub_t = (a2.X - a1.X) * (a1.Y - b1.Y) - (a2.Y - a1.Y) * (a1.X - b1.X); float u_b = (b2.Y - b1.Y) * (a2.X - a1.X) - (b2.X - b1.X) * (a2.Y - a1.Y); // Infinite lines intersect somewhere if (!(-MyEpsilon < u_b && u_b < MyEpsilon)) // e.g. u_b != 0.0 { float ua = ua_t / u_b; float ub = ub_t / u_b; if (0.0f <= ua && ua <= 1.0f && 0.0f <= ub && ub <= 1.0f) { // Intersection return new PointF[] { new PointF(a1.X + ua * (a2.X - a1.X) a1.Y + ua * (a2.Y - a1.Y)) }; } else { // No Intersection return new PointF[] { }; } } else // lines (not just segments) are parallel or the same line { // Coincident // find the common overlapping section of the lines // first find the distance (squared) from one point (a1) to each point if ((-MyEpsilon < ua_t && ua_t < MyEpsilon) || (-MyEpsilon < ub_t && ub_t < MyEpsilon)) { if (a1.Equals(a2)) // danger! return OneD_Intersection(b1 b2 a1 a2); else // safe return OneD_Intersection(a1 a2 b1 b2); } else { // Parallel return new PointF[] { }; } } } } Here is the test code:  public class IntersectTest { public static void PrintPoints(PointF[] pf) { if (pf == null || pf.Length < 1) System.Console.WriteLine(""Doesn't intersect""); else if (pf.Length == 1) { System.Console.WriteLine(pf[0]); } else if (pf.Length == 2) { System.Console.WriteLine(pf[0] + "" -- "" + pf[1]); } } public static void TestIntersect(PointF a1 PointF a2 PointF b1 PointF b2) { System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""Does "" + a1 + "" -- "" + a2); System.Console.WriteLine(""intersect "" + b1 + "" -- "" + b2 + "" and if so where?""); System.Console.WriteLine(""""); PointF[] result = Intersect.Intersection(a1 a2 b1 b2); PrintPoints(result); } public static void Main() { System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""line segments intersect""); TestIntersect(new PointF(0 0) new PointF(100 100) new PointF(100 0) new PointF(0 100)); TestIntersect(new PointF(5 17) new PointF(100 100) new PointF(100 29) new PointF(8 100)); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""""); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""just touching points and lines cross""); TestIntersect(new PointF(0 0) new PointF(25 25) new PointF(25 25) new PointF(100 75)); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""""); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""parallel""); TestIntersect(new PointF(0 0) new PointF(0 100) new PointF(100 0) new PointF(100 100)); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""""); System.Console.WriteLine(""----""); System.Console.WriteLine(""lines cross but segments don't intersect""); TestIntersect(new PointF(50 50) new PointF(100 100) new PointF(0 25) new PointF(25 0)); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""""); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""coincident but do not overlap!""); TestIntersect(new PointF(0 0) new PointF(25 25) new PointF(75 75) new PointF(100 100)); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""""); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""touching points and coincident!""); TestIntersect(new PointF(0 0) new PointF(25 25) new PointF(25 25) new PointF(100 100)); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""""); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""overlap/coincident""); TestIntersect(new PointF(0 0) new PointF(75 75) new PointF(25 25) new PointF(100 100)); TestIntersect(new PointF(0 0) new PointF(100 100) new PointF(0 0) new PointF(100 100)); System.Console.WriteLine(""----------------------------------------------------------""); System.Console.WriteLine(""""); while (!System.Console.KeyAvailable) { } } } and here is the output:  ---------------------------------------------------------- line segments intersect ---------------------------------------------------------- Does {X=0 Y=0} -- {X=100 Y=100} intersect {X=100 Y=0} -- {X=0 Y=100} and if so where? {X=50 Y=50} ---------------------------------------------------------- Does {X=5 Y=17} -- {X=100 Y=100} intersect {X=100 Y=29} -- {X=8 Y=100} and if so where? {X=56.85001 Y=62.30054} ---------------------------------------------------------- ---------------------------------------------------------- just touching points and lines cross ---------------------------------------------------------- Does {X=0 Y=0} -- {X=25 Y=25} intersect {X=25 Y=25} -- {X=100 Y=75} and if so where? {X=25 Y=25} ---------------------------------------------------------- ---------------------------------------------------------- parallel ---------------------------------------------------------- Does {X=0 Y=0} -- {X=0 Y=100} intersect {X=100 Y=0} -- {X=100 Y=100} and if so where? Doesn't intersect ---------------------------------------------------------- ---- lines cross but segments don't intersect ---------------------------------------------------------- Does {X=50 Y=50} -- {X=100 Y=100} intersect {X=0 Y=25} -- {X=25 Y=0} and if so where? Doesn't intersect ---------------------------------------------------------- ---------------------------------------------------------- coincident but do not overlap! ---------------------------------------------------------- Does {X=0 Y=0} -- {X=25 Y=25} intersect {X=75 Y=75} -- {X=100 Y=100} and if so where? Doesn't intersect ---------------------------------------------------------- ---------------------------------------------------------- touching points and coincident! ---------------------------------------------------------- Does {X=0 Y=0} -- {X=25 Y=25} intersect {X=25 Y=25} -- {X=100 Y=100} and if so where? {X=25 Y=25} ---------------------------------------------------------- ---------------------------------------------------------- overlap/coincident ---------------------------------------------------------- Does {X=0 Y=0} -- {X=75 Y=75} intersect {X=25 Y=25} -- {X=100 Y=100} and if so where? {X=25 Y=25} -- {X=75 Y=75} ---------------------------------------------------------- Does {X=0 Y=0} -- {X=100 Y=100} intersect {X=0 Y=0} -- {X=100 Y=100} and if so where? {X=0 Y=0} -- {X=100 Y=100} ---------------------------------------------------------- I downvoted only because I feel that providing a complete code sample here goes against the spirit of the site. The OP does not seem to want to learn they just want their assignment done by someone else. errr... I didn't notice that you posted the question as well =P. I have removed the downvote. ...or not I am being told that my 10 minute old post is too old to change. I have upvoted another of your answers to make up for it. Sorry. :) Thanks for ""taking it back"". You will notice that I also marked the question as Community Wiki to avoid people thinking I was rep farming etc. That is an interesting question though... is posting a working code example against the spirit of the site? Perhaps I should post it somewhere else (blog etc.) and link to it? The point is that a lot of the answers to other similar questions have fatal flaws in them which is really... against the spirit of the site too. Thanks for the attempted explanation. Perhaps I should have just posted this on a blog somewhere when I finished. Sorry... Also since it's community Wiki neither of us have lost any rep! You know I read the original question which while well written was simply asking for ""teh codez"" :). I just did not realize that you wrote it specifically because you had to find the answer yourself. Hi great and very helpfull but there is still one bug. In pointOfLine the distance calculated check that the point is on the line not in the segment. Il line segment is (00)->(100) and the point is (15 0) then dist to segment is 0 and pointOfLine answers true"
676,A,"How can I tell if a closed path contains a given point? In Android I have a Path object which I happen to know defines a closed path and I need to figure out if a given point is contained within the path. What I was hoping for was something along the lines of path.contains(int x int y) but that doesn't seem to exist. The specific reason I'm looking for this is because I have a collection of shapes on screen defined as paths and I want to figure out which one the user clicked on. If there is a better way to be approaching this such as using different UI elements rather than doing it ""the hard way"" myself I'm open to suggestions. I'm open to writing an algorithm myself if I have to but that means different research I guess. The android.graphics.Path class doesn't have such a method. The Canvas class does have a clipping region that can be set to a path there is no way to test it against a point. You might try Canvas.quickReject testing against a single point rectangle (or a 1x1 Rect). I don't know if that would really check against the path or just the enclosing rectangle though. The Region class clearly only keeps track of the containing rectangle. You might consider drawing each of your regions into an 8-bit alpha layer Bitmap with each Path filled in it's own 'color' value (make sure anti-aliasing is turned off in your Paint). This creates kind of a mask for each path filled with an index to the path that filled it. Then you could just use the pixel value as an index into your list of paths. Bitmap lookup = Bitmap.createBitmap(width height Bitmap.Config.ALPHA_8); //do this so that regions outside any path have a default //path index of 255 lookup.eraseColor(0xFF000000); Canvas canvas = new Canvas(lookup); Paint paint = new Paint(); //these are defaults you only need them if reusing a Paint paint.setAntiAlias(false); paint.setStyle(Paint.Style.FILL); for(int i=0;i<paths.size();i++) { paint.setColor(i<<24); // use only alpha value for color 0xXX000000 canvas.drawPath(paths.get(i) paint); } Then look up points int pathIndex = lookup.getPixel(x y); pathIndex >>>= 24; Be sure to check for 255 (no path) if there are unfilled points. Ah okay I like it. That extra memory wasn't doing anything useful anyway! One problem I had though was that the ALPHA_8 wouldn't ever give me anything other than just 0 back using getPixel. I had to just give in and use an ARGB_8888. I've found almost no documentation regarding the ALPHA_8 format and what it's limitations are but it sure didn't work for me here. Thanks Brian. Exercise that memory! The whole Skia Android 2D framework is under-documented. Shame about the 4x memory requirement but at least the Android screens are pretty small. @TomSeago Hi I got the same problem with you I have to use ARGB_8888 as well nothing return if using ALPHA_8!  Here is what I did and it seems to work: RectF rectF = new RectF(); path.computeBounds(rectF true); region = new Region(); region.setPath(path new Region((int) rectF.left (int) rectF.top (int) rectF.right (int) rectF.bottom)); Now you can use the region.contians(xy) method. Point point = new Point(); mapView.getProjection().toPixels(geoPoint point); if (region.contains(point.x point.y)) { // Within the path. } ** Update on 6/7/2010 ** The region.setPath method will cause my app to crash (no warning message) if the rectF is too large. Here is my solution: // Get the screen rect. If this intersects with the path's rect // then lets display this zone. The rectF will become the // intersection of the two rects. This will decrease the size therefor no more crashes. Rect drawableRect = new Rect(); mapView.getDrawingRect(drawableRect); if (rectF.intersects(drawableRect.left drawableRect.top drawableRect.right drawableRect.bottom)) { // ... Display Zone. } why do I get region.getBounds() as 0000 while using the code above? It does not work on my project. Where do I make mistake? Unfortunately if path is not a regular Rect point touched is seen in path also if it's not in. A simple example is a triangle. Bound of a Rectangle-Triangle is a Rectangle! So if you for ex. tap above hypotenuse point touched is still in bound !! Maybe a better solution can be this: http://stackoverflow.com/questions/7044838/finding-points-contained-in-a-path-in-android Its work but sometimes region take point that are outside of closed path  WebKit's SkiaUtils has a C++ work-around for Randy Findley's bug: bool SkPathContainsPoint(SkPath* originalPath const FloatPoint& point SkPath::FillType ft) { SkRegion rgn; SkRegion clip; SkPath::FillType originalFillType = originalPath->getFillType(); const SkPath* path = originalPath; SkPath scaledPath; int scale = 1; SkRect bounds = originalPath->getBounds(); // We can immediately return false if the point is outside the bounding rect if (!bounds.contains(SkFloatToScalar(point.x()) SkFloatToScalar(point.y()))) return false; originalPath->setFillType(ft); // Skia has trouble with coordinates close to the max signed 16-bit values // If we have those we need to scale. // // TODO: remove this code once Skia is patched to work properly with large // values const SkScalar kMaxCoordinate = SkIntToScalar(1<<15); SkScalar biggestCoord = std::max(std::max(std::max(bounds.fRight bounds.fBottom) -bounds.fLeft) -bounds.fTop); if (biggestCoord > kMaxCoordinate) { scale = SkScalarCeil(SkScalarDiv(biggestCoord kMaxCoordinate)); SkMatrix m; m.setScale(SkScalarInvert(SkIntToScalar(scale)) SkScalarInvert(SkIntToScalar(scale))); originalPath->transform(m &scaledPath); path = &scaledPath; } int x = static_cast<int>(floorf(point.x() / scale)); int y = static_cast<int>(floorf(point.y() / scale)); clip.setRect(x y x + 1 y + 1); bool contains = rgn.setPath(*path clip); originalPath->setFillType(originalFillType); return contains; }  I know I'm a bit late to the party but I would solve this problem by thinking about it like determining whether or not a point is in a polygon. http://en.wikipedia.org/wiki/Point_in_polygon The math computes more slowly when you're looking at Bezier splines instead of line segments but drawing a ray from the point still works."
677,A,"How to change current Plot Window Size (in R) For example. Assume I do: dev.new(width=5 height=4) plot(1:20) And now I wish to do plot(1:40) But I want a bigger window for it. I would guess that the way to do it would be (assuming I don't want to open a new window) to do plot(1:40 width=10 height=4) Which of course doesn't work. The only solution I see to it would be to turn off the window and start a new one. (Which will end my plotting history) Is there a better way ? Thanks. Tal-from your example (increase in width by 2x) it seems you want to be able to substantially increase the plot area. If it's a smaller increase in plot area you want then you can move the four margins back e.g. par(mar=c(3.0 3.0 1.5 1.5)) You want to resize the current window? Once the window is opened it ""belongs"" to the window manager. I am not aware of any call that allows you to resize and already-opened window. You could cheat and simulate in code the 'mouse activates windows and enlarges' but it strikes me as having a poor cost/benefit ratio. Hello Doug Shane and Firk - Thank you for answering. My situation is that I am to give a lecture on R. And in that lecture I intend to move between: par(mfrow = c(11)) to par(mfrow = c(12)) back and forth. Which will damage the image proportions and will force me to resize the window. The only solution I found for doing this in the code was by closing and opening a new window but that removes my ability to store the history of the plots. I hope my question was clearer now. Best Tal help(dev.new) and help(dev.set) Here is a my solution to this: resize.win <- function(Width=6 Height=6) { # works for windows dev.off(); # dev.new(width=6 height=6) windows(record=TRUE width=Width height=Height) } resize.win(55) plot(rnorm(100)) resize.win(1010) plot(rnorm(100)) That would be 'works only for Windows'. No other system has a function `windows` as Brian Ripley tried to explain to you. Hi Dirk Thanks for mentioning this (also notice I wrote it in the code). But I guess this is something too... Best Tal What about quartz() - works in OSX ;)?  Some workaround could be rather than using dev.new() R function use this function which should work across platform :  dev.new <- function(width = 7 height = 7) { platform <- sessionInfo()$platform if (grepl(""linux""platform)) { x11(width=width height=height) } else if (grepl(""pc""platform)) { windows(width=width height=height) } else if (grepl(""apple"" platform)) { quartz(width=width height=height) } }"
678,A,How do I use texture-mapping in a simple ray tracer? I am attempting to add features to a ray tracer in C++. Namely I am trying to add texture mapping to the spheres. For simplicity I am using an array to store the texture data. I obtained the texture data by using a hex editor and copying the correct byte values into an array in my code. This was just for my testing purposes. When the values of this array correspond to an image that is simply red it appears to work close to what is expected except there is no shading. The bottom right of the image shows what a correct sphere should look like. This sphere's colour using one set colour not a texture map. Another problem is that when the texture map is of something other than just one colour pixels it turns white. My test image is a picture of water and when it maps it shows only one ring of bluish pixels surrounding the white colour. When this is done it simply appears as this: Here are a few code snippets:  Color getColor(const Object *objectconst Ray *ray float *t) { if (object->materialType == TEXTDIF || object->materialType == TEXTMATTE) { float distance = *t; Point pnt = ray->origin + ray->direction * distance; Point oc = object->center; Vector ve = Point(oc.xoc.yoc.z+1) - oc; Normalize(&ve); Vector vn = Point(oc.xoc.y+1oc.z) - oc; Normalize(&vn); Vector vp = pnt - oc; Normalize(&vp); double phi = acos(-vn.dot(vp)); float v = phi / M_PI; float u; float num1 = (float)acos(vp.dot(ve)); float num = (num1 /(float) sin(phi)); float theta = num /(float) (2 * M_PI); if (theta < 0 || theta == NAN) {theta = 0;} if (vn.cross(ve).dot(vp) > 0) { u = theta; } else { u = 1 - theta; } int x = (u * IMAGE_WIDTH) -1; int y = (v * IMAGE_WIDTH) -1; int p = (y * IMAGE_WIDTH + x)*3; return Color(TEXT_DATA[p+2]TEXT_DATA[p+1]TEXT_DATA[p]); } else { return object->color; } }; I call the colour code here in Trace: if (object->materialType == MATTE) return getColor(object ray &t); Ray shadowRay; int isInShadow = 0; shadowRay.origin.x = pHit.x + nHit.x * bias; shadowRay.origin.y = pHit.y + nHit.y * bias; shadowRay.origin.z = pHit.z + nHit.z * bias; shadowRay.direction = light->object->center - pHit; float len = shadowRay.direction.length(); Normalize(&shadowRay.direction); float LdotN = shadowRay.direction.dot(nHit); if (LdotN < 0) return 0; Color lightColor = light->object->color; for (int k = 0; k < numObjects; k++) { if (Intersect(objects[k] &shadowRay &t) && !objects[k]->isLight) { if (objects[k]->materialType == GLASS) lightColor *= getColor(objects[k] &shadowRay &t); // attenuate light color by glass color else isInShadow = 1; break; } } lightColor *= 1.f/(len*len); return (isInShadow) ? 0 : getColor(object &shadowRay &t) * lightColor * LdotN; } I left out the rest of the code as to not bog down the post but it can be seen here. Any help is greatly appreciated. The only portion not included in the code is where I define the texture data which as I said is simply taken straight from a bitmap file of the above image. Thanks. It could be that the texture is just washed out because the light is so bright and so close. Notice how in the solid red case there doesn't seem to be any gradation around the sphere. The red looks like it's saturated. Your uv mapping looks right but there could be a mistake there. I'd add some assert statements to make sure u and v and really between 0 and 1 and that the p index into your TEXT_DATA array is also within range. Then I am not sure if I set the sphere to simply display color and no texture the gradations appear fine. I will look into adding assert statements and also see what happens when I reduce the brightness of the light. That was part of the issue. The light was too bright and it was washing out the sphere but the sphere still is only showing a single solid colour any ideas? http://dl.dropbox.com/u/367232/image3.jpg Instead of rendering into an `int` which clips try using `float` or `double` for your rendering buffer (and all computation). Then examine the result and convert the buffer to int's by dividing the intensity each pixel by the brightest (max intensity) pixel of the whole image.  If you're debugging your textures you should use a constant material whose color is determined only by the texture and not the lights. That way you can make sure you are correctly mapping your texture to your primitive and filtering it properly before doing any lighting on it. Then you know that part isn't the problem.
679,A,.NET test for bitonal TIFF image without trapping exception? I am developing a document database application that handles TIFF images. When annotating images in order for the user-interface to present only black and white as choices for bitonal (Format1bppIndexed) images versus multiple colors for images with a higher color depth I need to test for whether or not a TIFF is bitonal. Here is my function: private bool IsBitonal(string filePath) { bool isBitonal = false; try { Bitmap bitmap = new Bitmap(sourceFilePath); Graphics graphics = Graphics.FromImage(bitmap); } catch (Exception ex) { isBitonal = true; } return isBitonal; } It works but it is not graceful. The indexed pixel format exception that is thrown is described here: http://msdn.microsoft.com/en-us/library/system.drawing.graphics.fromimage.aspx. Trapping an exception for normal program flow is poor practice. Also it is conceivable that a different exception could be thrown. Is there a better way to implement an IsBitonal function that does not rely on trapping an exception? I tried searching the web for information and I found examples of how to load a TIFF image and avoid the exception by converting to a different format but I couldn't find any examples of a simple test for whether or not a TIFF image is bitonal. Thanks. Can you just check the PixelFormat Property of your bitmap? private bool IsBitonal(string sourceFilePath) { Bitmap bitmap = new Bitmap(sourceFilePath); return (bitmap.PixelFormat == PixelFormat.Format1bppIndexed) } If that doesn't work You should be able to look at the PropertyItems Collection in the Bitmap class and directly read the TIFF tags from the image. See the TIFF spec here: http://www.awaresystems.be/imaging/tiff/tifftags/baseline.html You could also look at the dimensions and Resolution of the image and contrast it with the filesize to get and idea of how many bits per pixel there are. (This probably wouldn't be very exact given compression and Metadata in the image). Checking the bitmap.PixelFormat property works. Thanks for the info.
680,A,"Generate 2D cross-section polygon from 3D mesh I'm writing a game which uses 3D models to draw a scene (top-down orthographic projection) but a 2D physics engine to calculate response to collisions etc. I have a few 3D assets for which I'd like to be able to automatically generate a hitbox by 'slicing' the 3D mesh with the X-Y plane and creating a polygon from the resultant edges. Google is failing me on this one (and not much helpful material on SO either). Suggestions? The meshes I'm dealing with will be simplified versions of the displayed models which are connected closed non-convex and have zero genus. Considering your description might it also be acceptable to project the 3D mesh onto a 2D plane? The projection part is easy and reduces the question to ""creating a polygon from a bunch of overlapping triangles"" which may be easier to solve especially if your projection is convex. Maybe you can tell us more about your mesh. Is it convex? Is it connected? Is it closed? Does it have zero genus? How is it represented in memory? The meshes are not convex but they will be connected and closed and have zero genus. @Thomas could you provide any more detail about the ""create a poly from overlapping triangles"" method? (or should I start a new question?) I think a new question would be appropriate. Post the link here once you've opened it! http://stackoverflow.com/questions/2817017/merge-overlapping-triangles-into-a-polygon You can do this with abit of geometry by finding all of the polygons which intersect with the plane and then finding the exact segment of the intersection. these segments are the lines of the 2D polygon you're looking for. How do I order the segments? segments that are adjacent originate from two triangles that share the same edge.  Since your meshes are not convex the resulting cross-section may be disconnected so actually consist of multiple polygons. This means that every triangle must be checked so you'll need at least O(n) operations for n triangles. Here's one way to do it: T <- the set of all triangles P <- {} while T is not empty: t <- some element from T remove t from T if t intersects the plane: l <- the line segment that is the intersection between t and the plane p <- [l] s <- l.start while l.end is not s: t <- the triangle neighbouring t on the edge that generated l.end remove t from T l <- the line segment that is the intersection between t and the plane append l to p add p to P This will run in O(n) time for n triangles provided that your triangles have pointers to their three neighbours and that T supports constant-time removals (e.g. a hash set). As with all geometric algorithms the devil is in the details. Think carefully about cases where a triangle's vertex is exactly in the plane for example. That's why I asked how your mesh is represented in memory. You could preprocess the mesh linking up edges if they share two vertices. You can also loop over all triangles first generating the line segments and keep pointers back to the original triangles. Then loop over all line segments and link them up. That will save you the preprocessing phase but might be slower if you do it many times. Thanks now I just need to think a bit about how to determine which triangle is neighbouring which other triangle on which edge..."
681,A,How to change the coordinate of a point that is inside a GraphicsPath? Is there anyway to change the coordinates of some of the points within a GraphicsPath object while leaving the other points where they are? The GraphicsPath object that gets passed into my method will contain a mixture of polygons and lines. My method would want to look something like: void UpdateGraphicsPath(GraphicsPath gPath RectangleF regionToBeChanged PointF delta) { // Find the points in gPath that are inside regionToBeChanged // and move them by delta. // gPath.PathPoints[i].X += delta.X; // Compiles but doesn't work } GraphicsPath.PathPoints seems to be readonly so does GraphicsPath.PathData.Points. So I am wondering if this is even possible. Perhaps generating a new GraphicsPath object with an updated set of points? How can I know if a point is part of a line or a polygon? If anyone has any suggestions then I would be grateful. Thanks for your suggestions Hans here is my implementation of your suggestion to use the GraphicsPath(PointF[] byte[]) constructor: GraphicsPath UpdateGraphicsPath(GraphicsPath gP RectangleF rF PointF delta) { // Find the points in gP that are inside rF and move them by delta. PointF[] updatePoints = gP.PathData.Points; byte[] updateTypes = gP.PathData.Types; for (int i = 0; i < gP.PointCount; i++) { if (rF.Contains(updatePoints[i])) { updatePoints[i].X += delta.X; updatePoints[i].Y += delta.Y; } } return new GraphicsPath(updatePoints updateTypes); } Seems to be working fine. Thanks for the help.
682,A,"How to set a Transparent Background of JPanel I need to know if a JPanel`s bacground can be set to Transparent? My frame is has two Jpanels Image Panel and Feature Panel Feature Panel is overlapping Image Panel the Image Panel is working as a background and it is loading image from a remote Url now I want to draw shaps on Feature Panel  but now Image Panel cannot be seen due to Feature Panel's background color. I need to make Feature Panel background transparent while still drawing its shapes and i want Image Panel to be visible since it is doing tiling and cache function of images. I need to seperate the image drawing and shape drawing thats why I`m using two jPanels! is there anyway the overlapping Jpanel have a transparent background? thanks JPanel when transparent PNG image is loaded and set `JPanel.setOpaque(false);` it will use the image transparent method else it will show not transparent picture. In my particular case it was easier to do this:  panel.setOpaque(true); panel.setBackground(new Color(0000)): // any color with alpha 0 (in this case the color is black violating a contract is ... violating a contract and as such ... wrong -1 what you say is true but for the desired purpose of design set any color with alpha 0 will to de job besides he din't specified if he need to have access to the background panel components as I understood the panel which is on the lower level is just to set a background image. that's invalid - with opaque to true the panel guarantees filling filling its background completely (which it doesn't with a transparent background)  Calling setOpaque(false) on the upper JPanel should work. From your comment it sounds like Swing painting may be broken somewhere - First - you probably wanted to override paintComponent() rather than paint() in whatever component you have paint() overridden in. Second - when you do override paintComponent() you'll first want to call super.paintComponent() first to do all the default Swing painting stuff (of which honoring setOpaque() is one). Example - import java.awt.Color; import java.awt.Graphics; import javax.swing.JFrame; import javax.swing.JPanel; public class TwoPanels { public static void main(String[] args) { JPanel p = new JPanel(); // setting layout to null so we can make panels overlap p.setLayout(null); CirclePanel topPanel = new CirclePanel(); // drawing should be in blue topPanel.setForeground(Color.blue); // background should be black except it's not opaque so // background will not be drawn topPanel.setBackground(Color.black); // set opaque to false - background not drawn topPanel.setOpaque(false); topPanel.setBounds(50 50 100 100); // add topPanel - components paint in order added // so add topPanel first p.add(topPanel); CirclePanel bottomPanel = new CirclePanel(); // drawing in green bottomPanel.setForeground(Color.green); // background in cyan bottomPanel.setBackground(Color.cyan); // and it will show this time because opaque is true bottomPanel.setOpaque(true); bottomPanel.setBounds(30 30 100 100); // add bottomPanel last... p.add(bottomPanel); // frame handling code... JFrame f = new JFrame(""Two Panels""); f.setContentPane(p); f.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); f.setSize(300 300); f.setLocationRelativeTo(null); f.setVisible(true); } // Panel with a circle drawn on it. private static class CirclePanel extends JPanel { // This is Swing so override paint*Component* - not paint protected void paintComponent(Graphics g) { // call super.paintComponent to get default Swing // painting behavior (opaque honored etc.) super.paintComponent(g); int x = 10; int y = 10; int width = getWidth() - 20; int height = getHeight() - 20; g.drawArc(x y width height 0 360); } } } hey thanks for your response. I got it working but the overlapping JPanel is expereincing a lot of Flickering. The overlapped panel does a fade in on new image load. Both panels are being painted through seperate threads. Now the overlapped thread draws xml response. Xml response loads and draws before the image is loaded in the overlapped Jpanel. During the fadeIn function of overlapped JPanel the already drawn overlapping Japenl starts flickering and looks very ugly. just for your info. I`m trying to write a rich client for a web-mapping application that shows satellite image and features Ok I think i spoke too soon now the problem is upside down. The contents of the overlapping Jpanel are erased when the Jpanel which is below redraws. Its some kind of a round robin. I draw the overlapping Jpanel without background but when as soon as the overlapped jpanel redraws the overlapping jpanel looses its contents hmmmm..now what. Post some code representative of what you're doing... I'll post a short example too. I did  but it did not workshould I call this function in the constructor or in the paint() method? Alright thanks Nate that worked. before calling super.paint() I set Opaque to false and then before drawing my shape I set Opaque to true. thanks buddy @Imran - In the constructor or in the paint() method of what? The panel itself? see updated answer... You shouldn't have to set it on and off... basically all `setOpaque()` handles is whether the ""background"" should be painted or not - you can just call it on the panel right after you create it and it will stay set. If you have an overridden `paintComponent()` on that panel you can paint on the component (and be sure to call super.paintComponent() so default Swing painting can occur) and anything you draw will be treated as ""foreground"" and will show up without calling `setOpaque(true)`. The threading part - are you using SwingUtilities.invokeLater() SwingWorker or anything similar to handle updating only in the EDT? How often are you updating - the ""flickery"" description sounds like you're trying to update the component faster than it can really repaint so it's repainting ""as available"" and not at a set rate. Or it could be many other things... threading issues doing too much processing on the EDT etc. thanks this worked for me. Much less complicated compared to the accepted answer.  Alternatively consider The Glass Pane discussed in the article How to Use Root Panes. You could draw your ""Feature"" content in the glass pane's paintComponent() method. Addendum: Working with the GlassPaneDemo I added an image: /* Set up the content pane where the ""main GUI"" lives. */ frame.add(changeButton BorderLayout.SOUTH); frame.add(new JLabel(new ImageIcon(""img.jpg"")) BorderLayout.CENTER); and altered the glass pane's paintComponent() method: protected void paintComponent(Graphics g) { if (point != null) { Graphics2D g2d = (Graphics2D) g; g2d.setRenderingHint( RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON); g2d.setComposite(AlphaComposite.getInstance( AlphaComposite.SRC_OVER 0.3f)); g2d.setColor(Color.yellow); g2d.fillOval(point.x point.y 120 60); } } Using a glasspane won't work (correctly) without setOpaque() (as described in my post) working.   public void paintComponent (Graphics g) { ((Graphics2D) g).setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_OVER0.0f)); // draw transparent background super.paintComponent(g); ((Graphics2D) g).setComposite(AlphaComposite.getInstance(AlphaComposite.SRC_OVER1.0f)); // turn on opacity g.setColor(Color.RED); g.fillRect(20 20 500 300); } I have tried to do it this way but it is very flickery  (Feature Panel).setOpaque(false); Hope this helps."
683,A,"Find most readable colour of text that is drawn on a coloured surface I'm not sure how to ask this but here goes. I draw a filled coloured rectangle on screen. The colour is in form of RGB I then want to draw text on top of the rectangle however the colour of the text has to be such that it provides the best contrast meaning it's readable. Example: If I draw a black rectangle the obvious colour for text would be white. What I tried right now is this. I pass this function the colour of the rectangle and it returns an inverted colour that I then use for my text. It works but it's not the best way. Any suggestions? // eg. usage: Color textColor = GetInverseLuminance(rectColor); private Color GetInverseLuminance(Color color) { int greyscale = (int)(255 - ((color.R * 0.30f) + (color.G * 0.59f) + (color.B * 0.11f))); return Color.FromArgb(greyscale greyscale greyscale); } why grey? either black or white would be best. white on dark colors black on light colors. just see if luminance is above a threshold and pick one or the other (you don't need the .net c# or asp.net tags by the way)  Visit: www.visibone.com Where due consideration is given to those of us who cannot see colors at all!  One simple approach that is guaranteed to give a significantly different color is to toggle the top bit of each component of the RGB triple. Color inverse(Color c) { return new Color(c.R ^ 0x80 c.G ^ 0x80 c.B ^ 0x80); } If the original color was #1AF283 the ""inverse"" will be #9A7203. The contrast will be significant. I make no guarantees about the aesthetics. Update 2009/4/3: I experimented with this and other schemes. Results at my blog. Another way to express it for each element is (c.R + 128) % 256. You move halfway across the colorspace. Or if c.R was < 128 you add 128 significantly brightening while if c.R was >= 128 you subtract 128 making it noticeably darker. This is awesome works very well. What does flipping the last bit actually achieve? Like what does mean and why does it work? @Gautam: Check out this explanation on Wikipedia: http://en.wikipedia.org/wiki/Complementary_color Forgot to add I used the method you posted but then turn the colour into a greyscale and it works very well in terms of contrast  Your function won't work if you supply it with RGB(127127127) because it will return the exact same colour. (modifying your function to return either black or white would slightly improve things) The best way to always have things readable is to have white text with black around it or the other way around. It's oftenly achieved by first drawing black text at (x-1y-1)(x+1y-1)(x+1y-1)(x+1x+1) and then white text at (xy). Alternatively you could first draw a semi-transparent black block and then non-transparent white text over it. That ensures that there will always be a certain amount of contrast between your background and your text.  There's already an SO answer which is close: How to find good looking font color if background color is known? It has some code examples as well that might work for your specific problem.  You need to study some color theory. A program called ""Color Wheel Pro"" is fun to play around with and will give you the general idea. Essentially you're looking for complimentary colors for a given color. That said I think you will find that while color theory helps you still need a human eye to fine tune. if you simply complement color and keep luminance a human eye won't pick edges so well (edges are the job of rods color is done by cones). for readability you need maximum luminance contrast => black or white  The most readable color is going to be either white or black. The most 'soothing' color will be something that is not white nor black it will be a color that lightly contrasts your background color. There is no way to programmatically do this because it is subjective. You will not find the most readable color for everyone because everyone sees things differently.  Some tips about color particularly concerning foreground and background juxtaposition such as with text. The human eye is essentially a simple lens and therefore can only effectively focus on one color at a time. The lenses used in most modern cameras work around this problem by using multiple lenses of different refractive indexes (chromatic lenses) so that all colors are in focus at one time but the human eye is not that advanced. For that reason your users should only have to focus on one color at a time to read the text. This means that either the foreground is in color or the background but never both. This leads to a condition typically called vibration in which the eye rapidly shifts focus between foreground and background colors trying to resolve the shape but it never resolves the shape is never in focus and it leads to eyestrain."
684,A,"Draw and manipulate shapes at run time What's the best way to draw shapes interactively at run time using Delphi? I need to be able to select drag and resize the shapes. This will be used to mark up existing images and documents. This looks like a good starting point but I'm wondering if there's a more complete library (preferably free) available that will save some time. Update: If you're going with a custom solution from scratch I've seen another example on Delphi Central that might be an even better starting point. Not sure if you've moved on now with this Bruce but if you haven't it might be worth looking at TMS Components Diagram Studio - it's certainly cheap and looks quite powerful from the demo. Thanks for the link. I like TMS.  I will recommend you read some links on my site. Are explained and all the source code is available; You can see and get some usefull for you. Plugin system in Delphi - Part 2 Not directly what you need is a plugin system for Delphi. But all the samples are based on a drawing tool that uses Shapes (Creating selecting resizing). You can review the code and extract what you need. Sample manipulating of ""Maps and Figures"" Sample of how to create select and move components at runtime (in this case with TImage). - Select shapes visually: Shows different ways to select shapes visually. The web is in Spanish but you can generate an authomatic translation on the web itself. Anyway the code is commented. Regards. Excuse-me for my bad english. Very nice looking components. Since I can not read spanish I could not find the download link. Can you post the source-download link here? Overkill for this project but I've bookmarked it for future use. thanks. Hello Warren. I have commented that the page has a small assitant for translate it to English (at Right). PLUGING SYSTEM PART 2 ==> (source) neftali.Clubdelphi.com/articulos/Prototipo2.zip and (binarys) neftali.Clubdelphi.com/articulos/Prototipo2_exe.zip SAMPLE ""MAPS AND FIGURES"" ==> neftali.Clubdelphi.com/ejemplos_files/demo_restaurante.zip SELECT SHAPES VISUALLY ==> neftali.Clubdelphi.com/ejemplos_files/… Excuse-me for tha bad format. Regards.  Please check here: TCAD -2d graphics component for delphi http://www.codeidea.com wish can help you. Link only answers are not welcome to Stack Overflow.  I would use Flex Graphics (commercial $499 for one developer with sources $1500 for site-license with source code). When I bought it it was a lot less than that. So I guess I wouldn't pay that now. It's a lightweight 'drawing/cad' package. But as I already own it I could import a page from the original document as an image perhaps rendered in PNG or WMF and then mark it up with lines etc. You could think of it as a light ""cad"" package. It has most primitive shapes and you can easily create your own new objects or shapes in Delphi classes that could be ""smart shapes"" like the ones in Visio. http://www.flex-graphics.com/ Another commercial component set that I have heard only good things about is TRichView. They have a TRichViewEdit that looks like you could emulate a document markup environment easily with it. Interesting but WAY too expensive for the problem at hand. On the bright side it's a little less expensive. $499 per developer (with source)  One freeware option would be TssControlSizer. Just change the ""control"" property to the control you want to manipulate resize/move. I wound up going with a custom solution and this was useful."
685,A,How might I convert a Rectangle to a RectangleF? What is the easiest way to convert a Rectangle to a RectangleF in .NET? Edit: This sounds trivial and it is but I was trying to save some typing. The best I could come up with: RectangleF rdest(rsrc.Location rsrc.Size); // C++/CLI ...or... RectangleF rdest = new RectangleF(rsrc.Location rsrc.Size) // C# There's an implicit converter so you can simply do: RectangleF rectanglef = rectangle; http://msdn.microsoft.com/en-us/library/system.drawing.rectanglef.op_implicit.aspx In fact you already knew that: it's easily missed but your code is using two such implicit casts - you're using Point and Size where there should be PointF and SizeF. :D This is great!! Thanks. +1 great answer!
686,A,"Overall Title for Plotting Window If I create a plotting window in R with m rows and n columns how can I give the ""overall"" graphic a main title? For example I might have three scatterplots showing the relationship between GPA and SAT score for 3 different schools. How could I give one master title to all three plots such as ""SAT score vs. GPA for 3 schools in CA""? The most obvious methods that come to my mind are to use either Lattice or ggplot2. Here's an example using lattice:  library(lattice) depthgroup<-equal.count(quakes$depth number=3 overlap=0) magnitude<-equal.count(quakes$mag number=2 overlap=0) xyplot(lat ~ long | depthgroup*magnitude data=quakes main=""Fiji Earthquakes"" ylab=""latitude"" xlab=""longitude"" pch=""."" scales=list(x=list(alternating=c(111))) between=list(y=1) par.strip.text=list(cex=0.7) par.settings=list(axis.text=list(cex=0.7))) In lattice you would change the main= parameter. The above example was lifted from here. I don't have a good ggplot2 example but there are a metricasston of examples with ggpolot2 over at the learn r blog. One option might be this example where they use ggplot2 and opts (title = ""RSS and NINO3.4 Temperature Anomalies \nand SATO Index Trends Since 1980"") But you would have to have all three graphs created in gg2plot naturally. I think you should be fine with either lattice or ggplot2. oh that's a good point dalloliogm. I didn't know that so I will remove that from my answer. Thanks! BTW a metricasston is exactly 1.42 imperialasstons. how much is a metricasston? :) What do you mean when you say that ggplot2 has not support for secondary axis labels? You can customize them when you set the scale e.g. scale_y_continous('y axis label'). WONDERFUL!!!! Thanks!!!!  Using the traditional graphics system here are two ways: (1) par(mfrow=c(22)) for( i in 1:4 ) plot(1:10) mtext(""Title""side=3outer=TRUEpadj=3) (2) par(mfrow=c(22)) for( i in 1:4 ) plot(1:10) par(mfrow=c(11)mar=rep(04)oma=rep(04)) plot.window(0:10:1) text(.5.98""Title"") This is a great answer too and doesn't require ggplot2 or lattice. Wonderful I was just looking for that - thanks!"
687,A,"Coordinates in distorted grid I have a grid in a 2D system like the one in the before image where all points ABCDA'B'C'D' are given (meaning I know the respective x- and y-coordinates). I need to calculate the x- and y-coordinates of A(new) B(new) C(new) and D(new) when the grid is distorted (so that A' is moved to A'(new) B' is moved to B'(new) C' is moved to C'(new) and D' is moved to D'(new)). The distortion happens in a way in which the lines of the grid are each divided into sub-lines of equal length (meaning for example that AB is divided into 5 parts of the equal length |AB|/5 and A(new)B(new) is divided into 5 parts of the equal length |A(new)B(new)|/5). The distortion is done with the DistortImage class of the Sandy 3D Flash engine. (My practical task is to distort an image using this class where the handles are not positioned at the corners of the image like in this demo but somewhere within it). I can't give you the full answer but I am pretty sure you will find it in ""Fundamentals of Texture Mapping and Image Warping"" by Paul S. Heckbert: http://www.cs.cmu.edu/~ph/texfund/texfund.pdf (in the appendix it contains source code for all kinds of mappings) Thanks for the link! Great resource! Thanks for the link. It's a great and very useful read.  This looks like a linear transformation to me. We know this because any line in the original graph is also a line in the transformed graph (this is not always true of the demo you gave but will be true if you follow the directions you gave in your question and not allow concave vertices). I do believe that AS3 has built-in support for transformation matrix manipulations. If not you can implement them yourself. [ x' ] = [ a b ] x [ x ] [ y' ] [ c d ] [ y ] These are matrices. The first is a vector (your final point) and that equals a certain 2x2 matrix multiplied by a vector (your original point) If you're not familiar with matrix multiplication this can be simplified out to be: x' = a*x + b*y y' = c*x + b*y Any linear transformation can be represented by that two-by-two matrix of a b c and d. Pick numbers for each of them and you get a linear transformation. So how do you find the values of a b c and d that will give you yours? For four unknowns you need four equations. If you look at the equations above you'll see that one point (""vector"") and its transformation will give you two equations. So...we need two points. As you'll see later it'll be useful to pick points in the un-transformed form (m0) and (0n) (ie one along your x axis and the other along your y axis). In your example these are easy to find! They are B and D (if A or C is your origin)! I'll be using a bit different notation: ""primes"" for transformed versions of points. B => B' B_x => B'_x B_y => B'_y If you know the before and the after coordinates of B and D you can find your transformation matrix abcd. Setting up your equations: B'_x = a * B_x + b * B_y B'_y = c * B_x + d * B_y D'_x = a * D_x + b * D_y D'_y = c * D_x + d * D_y Now let's say that B is your x-axis point in form (B_x0). Say D is your y-axis point in form (0D_y). If this is opposite switch them. Here we assume that your origin is A = (00) like in most Flash apps. Setting B_y = 0 and D_x = 0 we get: B'_x = a * B_x + b * 0 B'_y = c * B_x + d * 0 D'_x = a * 0 + b * D_y D'_y = c * 0 + d * D_y Using the powers of algebra we find: a = B'_x / B_x c = B'_y / B_x b = D'_x / D_y d = D'_y / D_y So in short: If you know original points: (the vertices on the original x and y axis) M = (M_x 0) N = (0  N_x) and their transformed/distorted points M' = (M'_x M'_y) N' = (N'_x N'_y) (so that M => M' and N => N') then calculate and store these four variables: a = M'_x / M_x b = N'_x / N_y c = M'_y / M_x d = N'_y / N_y and finally: (x y) => ( a*x + b*y  c*x + d*y ) edit: Okay I ran through a few of your arbitrary-corner-handle transformation and realized that I jumped to conclusions when I assumed that your transformation was linear. The equations above will only be linear under two conditions: Your new grid is the shape of some rotated parallelogram (and you original is a square 'normal' grid) The position of your (00) origin point does not change. Now I'm not sure what degrees of freedom you are allowing your handles to be positioned by but perhaps you can restrain them so that your gridspace always follows the form of a rotated parallelogram. It's not a linear transformation. The grid's are visibly ""distorted"" with a perspective transformation which is non-linear. @Eamon - See edit :( This is why I would like to see an answer for this question which is better than mine. I think any the non linear aspects can be addressed by switching to a higher order matrix equation. I'm not sure what form should be used but IIRC 3D work is often (e.g. in openGL) done with some kind of 4x4 matrix. OTOH given that you only have 3 reference points (and thus can fit any problem with 4 operations a X&Y scaling a rotation and a skew all possible with a 2x2) I think you won't need it @BCS - correct - perspective transforms are often handled using 4x4 matrices and 4D co-ordinate systems. The fourth ordinate is usually zero since it's only there to match the transform matrices. The inverse transform is done using the inverse matrix with care taken over the non-commutativity of matrix multiplication. IIRC there's an easy way to calculate inverse matrices using Gaussian elimination with ""augmented"" matrices. At least it seemed very easy at the time - I don't remember the details now.  Do you want help from algorithmic point of view (Transformations etc) or are you looking for help on how to achieve transformations specifically using Sandy 3D flash engine? yes. But I do not see a comment link anywhere near the question I need an algorithmic solution. It's not about Sandy (I'm just using their distortion class). This isn't an answer - you should really be asking this in the comments section of the original question.  You speak of a 2D system yet the distortion you're applying is a 3D distortion (albeit projected back down into 2d of course) - is that intentional? In general I'm not actually sure it's possibly to compute A'new's new location without more information about the perspective projection being using than just ABCD's new locations. After all perspective projection exaggerates nearby distances and compresses distant ones - and the degree to which it does this depends on the distance from the focal point (equivalently the FOV). It'd be easiest in any case to reuse whatever needed to be computed for the projection in the first place - if you can get the original transformation matrix and perspective transform you can straightforwardly apply those to A' B' and D' as well.  Are you sure that you want to be getting into all of this for handles? in the examples the handles are there to act as reference points on the movieclip for drawing they are not related in any way to the image or the distortion apon it. if you are wanting to just have the handlers inside the image then you would just need to offset the point calculation value (topRightCorner = new Point(pointb.x +10 point.y - 10) for example) if you are really wanting to get into calculating exact points then you can try doing a call to localToGlobal(new Point(this.x this.y)) from a handler to work out where it is (and therefore the translation that has been applied to it). otherwise you are going to have to deal with transformation matricies and work out exactly how the class calculates its triangles. for a very good guide to transformation matricies see here: senoular.com good luck Thanks for the reply. I think that the handles are firmly connected to the image as they always at the same relative position of it: Let's say ABCD is an image which is masked with A'B'C'D'. If you interpret A'newB'newC'newD'new as a projection of A'B'C'D' (= it's viewed from another view point and masks the exact same content of ABCD) I'm absolutely sure that AnewBnewCnewDnew is well defined. As I'm dealing with this masking problem simply shifting the handles or localToGlobal does not help. Afaik transformation matrices don't allow the required kind of distortions... `localToGlobal` takes a `Point` as params not two coordinates. corrected. Thanks  Let (uv) represent ""texture"" coordinates. Your handles stay at the same texture coordinates regardless of grid distortions. So in texture space A=(00) B=(10) C=(11) D=(01) A'=(auav) B'=(bubv) ... We can convert from texture space to pixel space. P(uv) = A*(1-u)*(1-v) + B*u*(1-v) + C*u*v + D*(1-u)*v (uv are texture coordinates and A B C D refer to pixel coordinates) So if your handle A' is defined to have texture coordinate (.35.15) then the position of A' in pixel space is given by P(.35.15). Now say the user drags handle A'. We need to solve for the new location of A. BCD remain fixed. It's just simple algebra. A' = A*(1-au)*(1-av) + B*au*(1-av) + C*au*av + D*(1-au)*av A' - B*au*(1-av) - C*au*av - D*(1-au)*av = A*(1-au)*(1-av) A = (A' - B*au*(1-av) - C*au*av - D*(1-au)*av) / ((1-au)*(1-av)) Not too bad. The same process gets formulas for the other handles. Here is my C# code. It gets called when any of the 8 ""Thumbs"" get dragged:  double au = .35 av = .15; // texture coordinates of A' double bu = .8 bv = .2; // texture coordinates of B' double cu = .8 cv = .6; // texture coordinates of C' double du = .2 dv = .9; // texture coordinates of D' // if we're dragging A' (AA) then move A etc. if (sender == ThumbAA) A = (AA - B*au*(1-av) - C*au*av - D*(1-au)*av) / ((1-au)*(1-av)); if (sender == ThumbBB) B = (BB - A*(1-bu)*(1-bv) - C*bu*bv - D*(1-bu)*bv) / (bu*(1-bv)); if (sender == ThumbCC) C = (CC - A*(1-cu)*(1-cv) - B*cu*(1-cv) - D*(1-cu)*cv) / (cu*cv); if (sender == ThumbDD) D = (DD - A*(1-du)*(1-dv) - B*du*(1-dv) - C*du*dv) / ((1-du)*dv); // update position of A' B' C' D' AA = A*(1-au)*(1-av) + B*au*(1-av) + C*au*av + D*(1-au)*av; BB = A*(1-bu)*(1-bv) + B*bu*(1-bv) + C*bu*bv + D*(1-bu)*bv; CC = A*(1-cu)*(1-cv) + B*cu*(1-cv) + C*cu*cv + D*(1-cu)*cv; DD = A*(1-du)*(1-dv) + B*du*(1-dv) + C*du*dv + D*(1-du)*dv; video of my demo app: http://screencast.com/t/NDU2ZWRj This doesn't answer your exact question about repositioning 4 handles at the same time. Is that what you really want? Also let me know if your handles' texture coordinates aren't predetermined. You can convert from pixel to texture coordinates but it is more complicated. This might well solve my problem. I'll check it in detail shortly. Would you mind posting the demo app somewhere? http://rapidshare.com/files/403571153/WarpGrid.exe.html (10 downloads allowed) This solves my problem. Thank you very much!  You could probably figure something out but I doubt that you or your users are going to like the result. To see why select a point in the interior of the image. Say you want to move it upwards by a bit. You can do that by moving any one of the four corners (try it in the demo). That means if you wanted to put a handle there and move it upwards the four corners would move in some combination. Most likely the corners would not do what you want. For example all four of them could move up by the same amount resulting in a pure translation and no warp. Or a single corner could move but maybe not the one you expected. There are an infinity of solutions and most likely the one you choose is not the one you (or somebody else) is expecting. The distortion method being used in the demo by the way is ""bilinear interpolation"". Having the handles at the corners is probably the best way to control it. I think there's a defined solution: Let's say ABCD is an image which is masked with A'B'C'D'. If you interpret A'newB'newC'newD'new as a projection of A'B'C'D' (= it's viewed from another view point and masks the exact same content of ABCD) I'm absolutely sure that AnewBnewCnewDnew is well defined. @Carsten the distortion you're using is not a perspective transformation - it's a bilinear interpolation. Neverthless your point is taken that if you somehow restrict the possible families of transformations and how ABCD should react to an internal hangle being moved you can cut down the number of possibilities. the sample image Carsten included certainly are perspective projected."
688,A,"Drawing a grid in javascript ( game of life for example ) Essentially I had this idea in my head for a sort of evolution simulator not exactly like Conways Game of Life but the one part where they do match is that they will both be based on a square grid. Now personally I like working in HTML+Javascript+ for simple apps since it allows fast UI creation and if you're not doing something computationally heavy then JS in a browser is a decent platform. The problem I'm trying to solve right now involves drawing and updating the grid. I might be missing something but it seems like there is no easy AND computationally light way of doing this for an 80x40 grid. The easy way would be to generate a div with absolute position and a specific background color for any square that is NOT empty. However that can become very slow with anything more than 60-70 colored squares. I'm definitely willing to switch to a different language if the situation calls for it but first I just want to know I'm not stupidly missing out on an easy way to do this with HTML+JS. Answer should include either one of the following: a) A reasonable way to draw and update a 80x40 grid ( where the squares change color and ""move"" ) in HTML+JS b) Another language that can do this reasonably fast. I would prefer to avoid having to spend a few days learning DirectDraw or something of the sort. Have you tried using a table? For a small grid (< 100x100) use a table and give each cell an ID for fast access. For bigger grids you should consider using a canvas object or embedding an Java or Flash applet.  <canvas> seems to be the right way to do this. A library like Raphael will help you avoid cross-platform issues. Another options is Processing.js but it does not work in IE.  Why not build the grid as an HTML Table? After all this is what you want? Give each cell a computed id and create some javascript functions to update them. Shoudlnt be a problem at all. You could look at the new canvas tag in HTML 5 but from what you've said I dont think you need it. I haven't tried the computed table yet but I figured that having to process 80x40 would be more work than processing the divs for only the colored squares. My browser slowed down on generating my test grid of only half the squares being colored so I don't know whether it can handle that many table cells. Never mind. Tried using a table and for some reason even though there's more HTML being generated it worked faster. Weird."
689,A,How to draw 3D mathematical models in .NET? What 3D engine would best suitable to do this effectively? I recently faced a problem of presenting the output of simple genetic algorithm that looks for extremes of 2 argument function f(x1x2) . I would like to be able to use x1 as x x2 as y and f as z and to draw points in 3d space that I could rotate. ( I'm currently drawing this on bitmap using color as the 'z axis'.) Where should I start? UPDATE: I found a directory of 3D engines for C# but there are a lot to choose from... Could you advise if any one of them would be best for my problem? UPDATE 2: Thanks to Cameron's suggestion a 3D options in WPF seem to be suiting my needs. I will give it a try for sure. I am reposting his links here: * WPF 3D Tutorial * CodeProject: WPF 3D Primer * CodeProject: WPF 3D : Part 1 of n There is tao framework if you would like to do tings from scratch like me. It is a opengl framework(with more) for C#. taoframework link  If you prefer coding OpenGL directly (instead of using an engine) check out OpenTK or the Tao Framework. Both use the same codebase but OpenTK follows .Net conventions more closely and is easier to use.  If you can use WPF at all then you can do some pretty easy simple stuff right away. Here's some links to get started: WPF 3D Tutorial CodeProject: WPF 3D Primer CodeProject: WPF 3D : Part 1 of n Thanks Cameron very interesting!  wpf + managed dx.  I use OpenInventor for scientific visualisation. It may be (in some ways) a relic of the old SGI days but it is still being supported and works well. Regarding graphing and other scientific visualisation look at MeshViz and other extensions from Mercury: Mercury (formerly TGS) OpenInventor MeshViz extension It has charting vector visualisation etc. It's quite comprehensive. It's not free but they do trial licenses so you can determine if it suits your needs. oops - forgot to explicitly mention that there are .Net bindings available from Mercury that work quite well.
690,A,Associating System.Drawing.Graphics with a device context? Is there a way of attaching a System.Drawing.Graphics class to a device context (HDC) for another window which was retrieved via GetDC API function? It is easier than using GDI+ directly. From MSDN: Graphics.FromHdc Method Creates a new Graphics from the specified handle to a device context.
691,A,"Programming a user interface for a small device I'm looking for ideas/elements of a user interface for a device I'm making. Full description (and video of development setup) here. In short: It's essentially a direction finder so I'm starting off thinking ""Compass"" but wondering what other design patterns would fit There isn't much interface to a compass - what kinds of input other than physically turning the device should I consider? I don't like deep menus. These are targetted at kids so what gaming patterns and interfaces should I consider for options? I'm assuming that children aren't invested heavily in computer patterns but don't know what patterns they are invested in. This device is limited memory - so the interface has to be dynamically drawn each refresh there is no video buffer. In some versions I'll have color (160x128 and 320x240) and others I'll have 15 level grayscale (100x160). I'd like the interface to be general enough and vector-ish enough that one can use any of the devices as easily as the others. What ideas do you have? Where should I look for such interface and design patterns? Where should I look for low level graphics programming (generating vector graphics on the fly I expect but perhaps I'm wrong to think of it this way)? P.S. I'm trying to make it into the next phase of the design competition and could really use your help - I need people to vote for my design. I'm hoping for 100 votes total... http://mypic32.com/web/guest/contestantsprofiles?profileID=50331 I’d certainly try to look at some GPS something basic like the Garmin eTrex Legend. It’s cheap it’s grayscale it’s probably also memory constrained as Your device will be and I am sure You will find a lot of UI ideas there. I would also consider looking at the iPhone. It is quite a different device but lot of the UI concepts there are well thought-out and could be used even on a simple grayscale device. You can get the iPhone SDK with the iPhone simulator for free.  You're summary page indicates that you're getting GPS coordinates from two devices... so a nice feature for children would be to show a dot for mom and a dot at display center for the child. Draw a line between these. Draw an arrow showing the child's most recent direction of travel to GPS resolution. Tell the child to press the ""FIND MOM"" button line up the arrow on the line and go that way going around things as needed (and the arrow and line will always keep them up to date on which way they would like to be walking if possible). Make the dots line and arrow big and friendly. Change the colors or make things flash when you're within the minimum GPS resolution or generally within shouting distance. This way if the child ends up on the other side of a high barrier he or she knows shouting is an option. Interesting project idea. I think that moving the thing and having the ""on"" button is enough of an interface if you have good enough motion detection sensors in the unit. It should probably auto-poweroff after an interval. Whether you generate vector graphics or not is not the main thing... you should probably generate coordinates for the dots (which are also the endpoints of a line) and the arrow and arrowhead segments. All these can be drawn on a raster display quickly using only integer math using the old Bresenham line and circle algorithms. Links to Wikipedia: http://en.wikipedia.org/wiki/Midpoint_circle_algorithm http://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm Good ideas I'll look into that... Here's the Besenham Line Algorithm: http://en.wikipedia.org/wiki/Bresenham_algorithm For actual display I'd build a display list and every refresh rip it to the display (a display list is a list of instructions of what to draw). This is how the game Asteroids worked (more or less).  I think it would be really cool to have it shaped like a Pokemon. I'm just saying... That's not a bad idea. Ideally the design will be flexible enough that any number of shapes could be made."
692,A,"Why does my jscrollpane result in odd paint calls in java swing? I've got a routine that paints an insanely large graph inside a scroll pane. It's too big to paint before adding to the scrollpane - the menory requirements would be several gigs. Because of the size of the graph I'm rendering the chart within the paint method of the scrollpane's child. Which works out well however I'm noticing that every time a scrollbar is moved my paint routine is called twice - once with a clipping rect equal to the uncovered area scrolled to and a second time with the clipping rect equal to the viewport's dimensions. For example if my viewport is 245x195 & I scroll down by 3 pixels my paint routine gets called with g.getClipBounds() set as follows: java.awt.Rectangle[x=0y=195width=245height=3] java.awt.Rectangle[x=0y=3width=245height=195] ... because I render within the paint routine this is causing flicker (I do my calcs as fast as I can but there is a wee bit of a delay I guess). Questions: Does anyone know how to prevent the 2nd paint call? This is plain-jane JScrollPane stuff I'm doing here - I have a component I add it to the scrollpane I add the scrollpane to a parent component. You can see this behavior even in the first image scrolling demo @ the swing tutorial. If the answer to #1 is 'nope': can anyone think of a good way to deal with this ? Should I paint to some sort of image buffer track recent paint calls & copy the image where possible ? I cannot imagine this being much faster than re-rendering but any insight appreciated :-) I've ran into this issue in the .NET world. Double buffering should solve your problem. If you're rendering directly onto a surface that is shown on the screen you have no control over when the ""showing"" actually happens. What typically happens is: you start rendering the not-yet-finished image is being displayed on the screen you finish rendering and then that is finally shown on the screen. If you begin your rendering logic by clearing to a background color then this will appear like a flash. Double buffering prevents this because it's always displaying from a completed render. The worst that could happen is slight 'tearing' but that's only noticeable in quickly changing animations. Even if you only want to render part of a gigantic image you can still use this technique. Simply render what you need onto an off-screen surface (which is the size of the visible portion you want). And then when you're done draw the entire image onto your display surface in one fell swoop. Oh no worries. I actually didn't understand that what you were drawing was that huge until your comment. The only thing to do to make it quicker is to improve your drawing logic but there's always going to be some delay. Good luck. my only problem is that I cannot buffer the entire image - it's too huge. Which means I need to keep buffered sections as they're drawn check for coverage when I'm called to paint() & sew them together as needed. I guess I'll give it a shot but I'll hold off to see if anyone else has any ideas Well double buffering can still come into play. Simply render what you need to on an off-screen surface (the size of the visible portion). And then when you're done draw the entire image onto your display surface in one fell swoop. actually this works. Thanks :-) Scrolling is a wee bit slower but the flicker is gone. Sorry I didn't get it @ first - my stupidity was no match for your perseverance ;-)"
693,A,Rss feed for game programmer? I was browsing this thread which has good recommendation but a bit too general for me. So if anyone has a collection of nice game programming feedsplease share them. :) (both general and specific topics are welcome) I used http://www.gamedev.net/ in college a lot especially the NeHe Tutorials  GameDevKicks.com might become interesting over time - if used more: http://www.gamedevkicks.com/  AIGameDev.com: http://feeds.aigamedev.com/AiGameDev  Here are two I've used DirectX forum feed and Summary of interesting resources
694,A,Java graphic library for multicoloured text I would like to know the recommended library or procedure for dealing with multi-coloured text within Java. My current usage of java.awt.Graphics while function appears to be a bit more complex than necessary. The main issue involves the frequent change of colour creating a new java.awt.Colour() object whenever a new colour is needed (and it's usually not one of the predefined values.) I already keep track of the previously used rgb value but it's possible that the colour can potentially change to unique values for each character I draw: java.awt.Color colorRender = new java.awt.Color(rgb); g.setColor(colorRender); I also ran a profiler on my code and identified a secondary bottleneck in an extreme case. I suspect that it may be the method used for drawing a single character but there may be overhead in determining said character: char[] c = new char[1]; // Created once for many uses /* ... */ g.drawChars(charRender 0 1 x y); I have looked at the BufferedImage class - while it's great for pixel-level graphics it doesn't directly support drawing characters. Not a real answer but if you think/measure that creating lots of Color objects is a performance bottleneck you can replace the calls to new Color(rgb) by your own factory method that caches already created colors. (I'm assuming the Colors class is immutable - it looks like it is) So add a ColorsFactory class with a (static) method getColor(rgb) method that caches colors that have already been created. You could simply put all colors in a map rgb -> Color(rgb) and keep them around forever or you could try and create cache that removes colors that are not used frequently (lots of libraries to do that) -- depends a bit on the way your program is used. http://en.wikipedia.org/wiki/Flyweight_pattern ah - it has a name forgot about that :-) The library is being used in this fashion: http://en.wikipedia.org/wiki/Roguelike  Update for the readers: The actual problem was caused by a frequently called method throwing a runtime exception and using that exception to return a default value. I changed the code of this method to avoid the exception ahead of time and should it still occur lets the calling function(s) handle them. Since the default value isn't that useful I added code to skip past rendering most of those characters.  Try the open source part of the JIDE components. This includes a class called StyledLabel which is a Swing JLabel that supports multicolored fonts.  I found this: How to add colored text to the document.  I'm assuming you're rendering text to an arbitrary component (via paintComponent()) rather than trying to modify the color of text in a JTextPane JLabel or other pre-existing widget. If this is the case you should look into using an AttributedString along with TextAttribute. These allow you to assign different styles (color font etc.) to various ranges of characters within a string and then render the whole string using Graphics.drawString(...). This way the underlying graphics subsystem will handle any necessary changes to the graphics state during rendering making your code much more readable and probably faster. Here is an example usage. Of course as other have mentioned you should also be caching your Color objects rather than recreating them over and over.
695,A,"Why is my glutWireCube not placed in origin? I have the following OpenGL code in the display function: glLoadIdentity(); gluLookAt(eyex eyey eyez atx aty atz upx upy upz); // called as: gluLookAt(20 5 5 -20 5 5 0 1 0); axis(); glutWireCube (1.); glFlush (); axis() draws lines from (000) to (1000) (0100) and (0100) plus a line from (100) to (130). My reshape function contains the following: glViewport (0 0 (GLsizei) w (GLsizei) h); glMatrixMode (GL_PROJECTION); glLoadIdentity (); gluPerspective(45.0 (GLsizei) w / (GLsizei) h 1.0 100.0); glMatrixMode (GL_MODELVIEW); This image shows the result of running the program with 1. as the argument to glutWireCube: As you can see the cube isn't centered around (000) as the documentation says it should be: The cube is centered at the modeling coordinates origin (...) (source) If I run the program with 5. as the argument the cube is displaced even further: Why is that and how do I place the cubes around (000)? FURTHER INFORMATION It doesn't matter if I switch the order of axis() and glutWireCube. Surrounding axis() with glPushMatrix() and glPopMatrix() doesn't fix it either. SOLUTION I modified gluPerspective to start looking further away from the camera and now the Z-buffering works properly so it is clear that the cubes are placed around the origin. It might be right I think it's hard to determine due to the perspective ... But I guess it isn't from staring a bit more at it. To quickly rule out that axis() isn't modifying the model view matrix surround the call with matrix push/pops: glPushMatrix(); axis(); glPopMatrix(); Things to investigate/check: Is this the entire window? It seems odd that the view is down in one corner. Does it help if you add an increasing rotation before the rendering? That can make it easier to determine the perspective by giving more clues. You can also try moving the ""camera"" around by changing the arguments to gluLookAt() dynamically. > Is this the entire window? It seems odd that the view is down in one corner. Yes it's the entire window. I'm forced to do a front-perspective with a eye-position at (2055). Unfortunately that doesn't change the result.  Are you sure axis does not mess with the view matrix ? What happens if you call it after the drawing of the cube ? Edit to add: Actually... Looking at the picture closer it looks like it might be centered at the origin. The center of the cube seems to align exactly with the intersection of the 3 axes. The only thing that looks suspicious is that the red line does not write over the white edge. do you have Z-buffering properly set up ? I think I have Z-buffering enabled with `glEnable(GL_DEPTH_TEST)` and `glDepthMask(GL_TRUE)`. The result is the same if the lines are present or not. Actually you are right. If I view the image with `glOrtho` instead of `gluLookAt` and `gluPerspective` the cubes are centered around the origin. How would I enable z-buffering correctly? I increased `znear` in `gluPerspective` and now the Z-buffering works. Now I just have to figure out how to see more of the image :) how about reducing zfar then ? the ratio of the 2 is what really affects your Z-buffer precision Unfortunately that doesn't change the result either. :("
696,A,"How do I remove color (colour) profile information from images using .NET? I have a tool which manipulates images at runtime as part of my web app. This generally works fine but with the release of Firefox 3.5 we're seeing some colour problems. I believe this is because Firefox 3.5 now supports embedded ICC colour profiles where no other browsers do. In order to achieve consistency of display I'd like to programatically remove any ICC colour profile in my .NET code. Can anyone point me in the right direction? Thanks - Chris This method may work (I have not tested it) although it may be overkill: public void StripBitmap(string path) { Bitmap originalBitmap = (Bitmap)Bitmap.FromFile(path); Bitmap strippedBitmap = new Bitmap(originalBitmap.Width originalBitmap.Height); using (Graphics g = Graphics.FromImage(strippedBitmap)) { g.DrawImage(originalBitmap 0 0); } System.Drawing.Imaging.ImageFormat fmt = originalBitmap.RawFormat; originalBitmap.Dispose(); System.IO.File.Delete(path); strippedBitmap.Save(path fmt); strippedBitmap.Dispose(); } The Bitmap class in GDI+ does not appear to support color profiles but if it does support them I don't think they would be carried over by the DrawImage operation in the above sample. Thanks for the reply but my code is doing this sort of thing already. Unfortunately I don't know enough about colour profiles to know where the problem is. For all I know it might be a *lack* of colour profile that's causing FireFox 3.5 to render the colours differently?! @Chris: that sounds plausible. Can you post some pictures or something that show what the color problems are exactly? By the way I think most people know that ""color"" == ""colour"". We're just saving up our excess vowels to be shipped to the Balkans. :) Yeah - I know everyone knows the difference in the spelling - just making it easier for the search engines!  On further investigation it appears that IE was taking notice of some Gamma correction information and FireFox 3.5 was taking notice of an embedded ICC colour profile. All of this information appears to have been added by the .NET framework's PNG implementation by default. It is possible to remove this information in .NET - I've blogged about it here."
697,A,How to render a gradient in memory using GDI(+) I am trying to render an Image object in memory with the dimensions 1x16. This image is used as a tiled background. The gradient itself should have 3 colors in a non-linear fashion. Pixel 1 to 6: Gradient Color 1 to Color 2 Pixel 7 to 16: Gradient Color 3 to Color 4 I just found out myself how to do it. I was expecting an answer like this:  Bitmap bmp = new Bitmap(1 16); Graphics g = Graphics.FromImage(bmp); System.Drawing.Drawing2D.LinearGradientBrush b1 = new System.Drawing.Drawing2D.LinearGradientBrush( new Rectangle(0 0 1 6) Color1 Color2 System.Drawing.Drawing2D.LinearGradientMode.Vertical); System.Drawing.Drawing2D.LinearGradientBrush b2 = new System.Drawing.Drawing2D.LinearGradientBrush( new Rectangle(0 7 1 16) Color3 Color4 System.Drawing.Drawing2D.LinearGradientMode.Vertical); g.FillRectangle(b1 new Rectangle(0 0 1 6)); g.FillRectangle(b2 new Rectangle(0 7 1 16)); g.Dispose(); The Bitmap bmp has now the 2 gradient.  You could use the GradientFill function. For a custom solution see if this article can help. thanks for the hint. I was more looking for a managed GDI (preferable) C# solution.
698,A,"How to load an image server-side in ASP.NET? I'm trying to load an image that is in the root dir of my project: Dim b As Bitmap = New Bitmap(""img.bmp"") but it doesn't seem to find the file. I've tried various combinations like ~img.gif /img.gif \img.gif ~/img.gif etc but none seems to work. How to access the ""current directory on server"" in ASP.NET? Thanks Have you tried: Dim b As Bitmap = New Bitmap(HttpContext.Current.Server.MapPath(""~/img.bmp"")) thats about right it worked thanks"
699,A,"Problem with a volumetric fog in OpenGL Good day. I am trying to make a volumetric fog in OpenGL using glFogCoordfEXT. Why does a fog affect to all object of my scene even if they're not in fog's volume? And these objects become evenly gray as a fog itself. Here is a pic Code: void CFog::init() { glEnable(GL_FOG); glFogi(GL_FOG_MODE GL_LINEAR); glFogfv(GL_FOG_COLOR this->color); glFogf(GL_FOG_START 0.0f); glFogf(GL_FOG_END 1.0f); glHint(GL_FOG_HINT GL_NICEST); glFogi(GL_FOG_COORDINATE_SOURCE_EXT GL_FOG_COORDINATE_EXT); } void CFog::draw() { glBlendFunc(GL_SRC_ALPHA GL_SRC_ALPHA); glEnable(GL_BLEND); glPushMatrix(); glTranslatef(this->coords[0] this->coords[1] this->coords[2]); if(this->angle[0] != 0.0f) glRotatef(this->angle[0] 1.0f 0.0f 0.0f); if(this->angle[1] != 0.0f) glRotatef(this->angle[1] 0.0f 1.0f 0.0f); if(this->angle[2] != 0.0f) glRotatef(this->angle[2] 0.0f 0.0f 1.0f); glScalef(this->size this->size this->size); GLfloat one = 1.0f; GLfloat zero = 0.0f; glColor4f(0.0 0.0 0.0 0.5); glBegin(GL_QUADS); // Back Wall glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glEnd(); GLenum err; if((err = glGetError()) != GL_NO_ERROR) { char * str = (char *)glGetString(err); } glBegin(GL_QUADS); // Floor glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f-2.5f 15.0f); glEnd(); glBegin(GL_QUADS); // Roof glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f 2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f 2.5f 15.0f); glEnd(); glBegin(GL_QUADS); // Right Wall glFogCoordfEXT( zero); glVertex3f( 2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f( 2.5f 2.5f 15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f( 2.5f-2.5f-15.0f); glEnd(); glBegin(GL_QUADS); // Left Wall glFogCoordfEXT( zero); glVertex3f(-2.5f-2.5f 15.0f); glFogCoordfEXT( zero); glVertex3f(-2.5f 2.5f 15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f 2.5f-15.0f); glFogCoordfEXT( one); glVertex3f(-2.5f-2.5f-15.0f); glEnd(); glPopMatrix(); glDisable(GL_BLEND); //glDisable(GL_FOG); } Maybe there's no lighting in the scene? If all light sources are disabled all objects are going to appear in a flat ambient color. Check to see what happens if you disable fog altogether. Does this still happen? No lighting is there if I comment fog draw and init in my scene render functon all is fine  You seem to not be clear on how GL_fog_coord_EXT works. You're saying that an object is ""outside the fog volume"" but OpenGL does not have any notion of a fog volume. At any point either Fog is completely off or it's on in which case the fog equation will be applied with a fog coefficient that depends both on the fog mode (LINEAR in your case) and the fog coordinate. Regarding the fog coordinate. when using  glFogi(GL_FOG_COORDINATE_SOURCE_EXT GL_FOG_COORDINATE_EXT); You're telling OpenGL that every time you'll provide a vertex you'll also provide which fog coordinate to use through glFogCoordfEXT So what does it mean in your case ? Assuming you're not calling glFogCoordfEXT in your teapot drawing code you'll end up with the value of your last call to glFogCoordfEXT which looks like a glFogCoordf(one). So everything drawn in that case will be fully in fog which is what you observe. Now I'm not sure exactly what you're trying to achieve so I don't know how to help you solve the issue exactly. However if the goal is to use your quads to mimic fog simply turn fog off when drawing the scene and turn it on only when drawing the cube (I'm pretty sure it won't look like nice fog though). Thank you Bahbar!! this: ""you'll end up with the value of your last call to glFogCoordfEXT which looks like a glFogCoordf(one). "" helped! I just changed a drawing order and everything became ok:))"
700,A,C# - How to initialize gradient panel demo? I found a web page that describes how to create a gradient panel in a WinForms application. Does anyone know how to initialize this panel demonstration? There is source code but no comments or examples how the code can be used. You build the project to get a DLL (to add as a reference) or embed the code in your project. On your forms designer you should see a new item in your toolbox drag it onto your form. It has properties for colour etc. Is this any help? Thanks - way too obvious! lol I was a little curious tbh but we are all allowed to forget the obvious once in a while :)
701,A,Border in DrawRectangle Well I'm coding the OnPaint event for my own control and it is very nescessary for me to make it pixel-accurate. I've got a little problem with borders of rectangles. See picture: These two rectangles were drawn with the same location and size parameters but using different size of the pen. See what happend? When border became larger it has eaten the free space before the rectangle (on the left). I wonder if there is some kind of property which makes border be drawn inside of the rectangle so that the distance to rectangle will always be the same. Thanks. You can do this by specifying PenAlignment Pen pen = new Pen(Color.Black 2); pen.Alignment = PenAlignment.Inset; //<-- this g.DrawRectangle(pen rect); This should be the accepted answer Agreed this is the correct answer to the original question  I guess not... but you may move the drawing position half the pen size to the bottom right  If you want the outer bounds of the rectangle to be constrained in all directions you will need to recalculate it in relation to the pen width: private void DrawRectangle(Graphics g Rectangle rect float penWidth) { using (Pen pen = new Pen(SystemColors.ControlDark penWidth)) { float shrinkAmount = pen.Width / 2; g.DrawRectangle( pen rect.X + shrinkAmount // move half a pen-width to the right rect.Y + shrinkAmount // move half a pen-width to the down rect.Width - penWidth // shrink width with one pen-width rect.Height - penWidth); // shrink height with one pen-width } }  This isn't a direct answer to the question but you might want to consider using the ControlPaint.DrawBorder method. You can specify the border style colour and various other properties. I also believe it handles adjusting the margins for you. Though it does draw the border right way for some readon this function uses considerably more processor time than DrawRectangle. @undsoft: Yeah I wouldn't be too surprised. I'm not sure how it works behind the scenes but it's certainly a lot more complicated than DrawRectangle given that's it's capable of drawing 3D and other style borders among other things.
702,A,"How can I draw a WPF polygon with openings? I have a collection of custom ""PlanarMember"" objects exposing Boundary and Openings properties. The Boundary property is a custom ""Polygon"" class exposing vertices and segments etc. The Openings is an IEnumerable basically describing openings in the larger polygon. I can guarantee that the openings do not overlap/intersect the outer boundary is non-self intersecting and closed the openings are fully contained by the outer boundary and other reasonable limitations. I want to create a WPF binding to allow me to throw these objects into a ItemsControl collection with a custom ItemsPanel of Canvas that basically means I need to be able to convert my custom PlanarMember object into a WPF object that can be placed on a Canvas and look like a solid filled polygon with openings. This library is used in other contexts so using a WPF class instead of the original PlanarMember is out of the question. It would be trivial to just represent the outer boundary as a WPF Polygon but it's the openings that are throwing me. Any thoughts? If it helps some examples of what I'd be trying to represent include a wall with window and door openings a floor slab with penetrations etc. I also can't just draw solid filled shapes on the larger shape to represent openings because the objects behind need to show through the holes. After experimenting with this for a while I've come up with a solution that satisfies my needs. I'll post it here for posterity. Hopefully it saves someone else the 6 hours or so I've spent on this issue. Couple classes I use that are defined elsewhere: PlanarElement: Simply describes a 2d ""thing"" with a Boundary property exposing a Polygon and an Openings property exposing an IEnumerable list of Polygon objects Polygon: 2d planar geometric class exposing a property of type IEnumerable of type Point Point: 3d geometric point Here is the solution: 1) Here is the ItemsControl to which my PlanarMembers will be bound <ItemsControl> <ItemsControl.Resources> <DataTemplate DataType=""{x:Type o:PlanarMember}""> <Path Stroke=""Black"" StrokeThickness=""1"" Data=""{Binding Converter={StaticResource PlanarMemberConverter}}""> <Path.Fill> <SolidColorBrush Opacity=""0.5"" Color=""LightGray"" /> </Path.Fill> </Path> </DataTemplate> </ItemsControl.Resources> <ItemsControl.ItemsPanel> <ItemsPanelTemplate> <Canvas RenderTransform=""{StaticResource CurrentDisplayTransform}""/> </ItemsPanelTemplate> </ItemsControl.ItemsPanel> </ItemsControl> 2) Notice the use of PlanarMemberConverter in the Path's Data setting created here: <this:PlanarMemberConverter x:Key=""PlanarMemberConverter"" /> 3) The actual IValueConverter derived class is defined here: [ValueConversion(typeof(PlanarMember) typeof(Geometry))] class PlanarMemberConverter : IValueConverter { #region IValueConverter Members public object Convert(object value Type targetType object parameter System.Globalization.CultureInfo culture) { if (targetType != typeof(Geometry)) return null; var planarMember = value as PlanarMember; var collection = new PathFigureCollection(planarMember.Openings.Count() + 1) { new PathFigure( planarMember.Boundary.Vertices.First().AsPoint() ToSegments(planarMember.Boundary.Vertices) true){IsFilled = true} }; foreach (var opening in planarMember.Openings) { collection.Add(new PathFigure( opening.Vertices.First().AsPoint() ToSegments(opening.Vertices) true) { IsFilled = true }); } return new PathGeometry(collection) {FillRule = FillRule.EvenOdd}; } public object ConvertBack(object value Type targetType object parameter System.Globalization.CultureInfo culture) { throw new NotImplementedException(); } #endregion public IEnumerable<PathSegment> ToSegments(IEnumerable<Point> points) { return from point in points select (PathSegment)new LineSegment(point.AsPoint() true); } }"
703,A,"c# graphics creation I have some code that is used to programmatically create a Document to send to the printer. It goes something like this: private void pd_PrintPage(object sender System.Drawing.Printing.PrintPageEventArgs ev) { ev.Graphics.DrawImage(pictureBox1.Image 50 100); string drawToday=""Date : ""+strToday; string drawPolicyNo=""Policy # : "" + strPolicyNo; string drawUser=""User : "" + strUser; Font drawFont=new Font(""Arial""30); SolidBrush drawBrush=new SolidBrush(Color.Black); PointF drawPointToday=new Point(50400); PointF drawPointPolicyNo=new Point(50450); PointF drawPointUser=new Point(50500); ev.Graphics.DrawString(drawTodaydrawFontdrawBrushdrawPointToday); ev.Graphics.DrawString(drawPolicyNodrawFontdrawBrushdrawPointPolicyNo); ev.Graphics.DrawString(drawUserdrawFontdrawBrushdrawPointUser); } Its effective code but now I need to do the same procedure but instead write it to an image file so that it can be sent to a browser and printed from there. It should be relatively simple to reuse this code but I am getting hung up unfortunately on what drawing surface to use in replacement of the PrintPageEventArgument. Thanks Edit: THanks I get that I just need another Graphics object but the Graphics object by itself does not have a public constructor so what I am looking for is a suggestion on what object I need to substatiate to be able to create a Graphics object to draw on. I thought perhaps bitmap? Bitmaps are of course pixel based instead of point based so I was not sure that this is the best medium to use. You can get a Graphics object from many different places such images and controls. You will probably want to look into the DpiX DpiY PageScale and PageUnit (and perhaps a couple of others that I don't recall right now) properties in order to get graphics objects from different sources to behave in a similar manner. Use the debugger to investigate the values of the graphics object in your PrintPage event to get a good starting point. To write your graphics object to an image file you can do something like this: public void SaveImage(Graphics surface) { Bitmap bmp = new Bitmap(50 100 surface); bmp.Save(""filename.png"" ImageFormat.Png); } You can choose other formats from the ImageFormat class such as JPG BMP etc.  If I understand you correctly you could simply break out the code from your event handler into another method accepting a Graphics object as parameter: private void Print(Graphics g) { g.DrawImage(pictureBox1.Image 50 100); string drawToday=""Date : ""+strToday; string drawPolicyNo=""Policy # : "" + strPolicyNo; string drawUser=""User : "" + strUser; Font drawFont=new Font(""Arial""30); SolidBrush drawBrush=new SolidBrush(Color.Black); PointF drawPointToday=new Point(50400); PointF drawPointPolicyNo=new Point(50450); PointF drawPointUser=new Point(50500); g.DrawString(drawTodaydrawFontdrawBrushdrawPointToday); g.DrawString(drawPolicyNodrawFontdrawBrushdrawPointPolicyNo); g.DrawString(drawUserdrawFontdrawBrushdrawPointUser); } And then call that method from your event handler: private void pd_PrintPage(object sender System.Drawing.Printing.PrintPageEventArgs ev) { Print(ev.Graphics); } Then you can reuse the same method for printing the output to any other Graphics instance: using (Bitmap img = new Bitmap(width height)) using (Graphics g = Graphics.FromImage(img)) { Print(g); img.Save(fileName); } Thanks Fredrik kinda what I figured. Bitmap would be the way to go on this. You are welcome. Also note heavyd's answer on image formats in case you want more control over format color depth and such."
704,A,Sprite array list never works! Need help understanding how to create arrays of class instances in JAVA // Particle Stuct Instance private Sprite[] mParticles = new Sprite[10]; /// Particle emitter Properties private Context mContext; private int mPositionX mPositionY mWidth mHeight mNumParticles; private Rect srcRect dstRect; /*** Constructor ! ***/ public ParticleEmitter(Context c Sprite spriteImage int num_particles) { super(c); mContext = c; Sprite[] Particles = new Sprite[10]; Particles[0] = new Sprite(mContext R.drawable.icon); // mParticles = spriteImage; //mParticles[num_particles].InitAttributes(c R.drawable.icon); // Allocate Particles instances and copy into mParticle member //mParticles = new Sprite[num_particles]; // Sprite sprite1 = new Sprite(mContext R.drawable.icon); // Sprite sprite2 = new Sprite(mContext R.drawable.icon); // mParticles[0] = spriteImage; // mParticles[1] = sprite2; /* for(int i = 0; i < num_particles; i++) { mParticles[i].InitAttributes(mContext R.drawable.icon); mParticles[i].setXPosition(i); } */ // mParticles[0].InitAttributes(mContext R.drawable.icon); // nullify our positioning attributes mPositionX = mPositionY = 0; } Your initialization seems to be okay what problems do you need help with? When you say new Sprite[10] is creates an array of ten Sprite references which are all equal to null by default. It does not create any new Sprite objects. After this you'll likely want to create a new Sprite object to stick in each of these ten places. For example: Sprite[] rgSprite = new Sprite[10]; for (int i = 0; i < rgSprite.length; i++) { rgSprite[i] = new Sprite(mContext R.drawable.icon); } Thanks for the responses guys! Gunslingers you were indeed correct. However I ended up using Java generics like List and ArrayList to manage my instances ( which worked brilliantly for my needs ). Example: // Particle Instance Array stucture using Generics private List mParticles = new ArrayList(10); ... Then I would just iterate through the list instantiating them with new. Thanks for the help.
705,A,JOGL Double Buffering What is eligible way to implement double buffering in JOGL (Java OpenGL)? I am trying to do that by the following code: ... /** Creating canvas. */ GLCapabilities capabilities = new GLCapabilities(); capabilities.setDoubleBuffered(true); GLCanvas canvas = new GLCanvas(capabilities); ... /** Function display(…) which draws a white Rectangle on a black background. */ public void display(GLAutoDrawable drawable) { drawable.swapBuffers(); gl = drawable.getGL(); gl.glClear(GL.GL_COLOR_BUFFER_BIT | GL.GL_DEPTH_BUFFER_BIT); gl.glClearColor(0.0f 0.0f 0.0f 0.0f); gl.glColor3f(1.0f 1.0f 1.0f); gl.glBegin(GL.GL_POLYGON); gl.glVertex2f(-0.5f -0.5f); gl.glVertex2f(-0.5f 0.5f); gl.glVertex2f(0.5f 0.5f); gl.glVertex2f(0.5f -0.5f); gl.glEnd(); } ... /** Other functions are empty. */ Questions: — When I'm resizing the window I usually get flickering. As I see it I have a mistake in my double buffering implementation. — I have doubt where I must place function swapBuffers — before or after (as many sources says) the drawing? As you noticed I use function swapBuffers (drawable.swapBuffers()) before drawing a rectangle. Otherwise I'm getting a noise after resize. So what is an appropriate way to do that? Including or omitting the line capabilities.setDoubleBuffered(true) does not make any effect. If you use a GLCanvas autoSwapBuffer mode is set to true by default you should not have to call swapBuffers() manually. Your flickering has nothing to do with double buffering rather set sun.awt.noerasebackground to true.  Here's an example of double-buffered animation using JOGL: http://www.java-tips.org/other-api-tips/jogl/how-to-implement-a-simple-double-buffered-animation-with-mouse-e.html Try instead of calling swapBuffers() at the end of display(...) call: gl.glFlush(); It's been a while since I've done anything with JOGL; hope this helps. You need both. glFlush flushes the polygon pipeline drawing everything to the video buffer. But you still need to swap buffers to make the result visible on the screen.  If JOGL is like the C/C++ version: RMorrisey and the sample code is incorrect in stating the use of glFlush. The swapBuffers function must go at the end of the drawing. To confirm this: have the shapes do animation very quickly and watch for tearing. If you get tearing then you are doing a single draw if you don't then you are using double buffering. If you get tearing it may just be that the buffer swapping is not synchronized with the screen refresh. Double buffering is designed to intentionally avoid that. Rather directly draw onto the screen the results are cached in the background and then the new memory segment is shown on the screen. that sounds like something that the Java version would do. You should be double bufferring anyways What Steven says is true... but only if you do what finnw says. In JOGL this is accomplished by the (default: on) AutoSwapBufferMode in GLDrawable (in case the OP manually shut it off elsewhere in code).
706,A,"GDI: Dynamical Multiple Graphics in a page? I'm quite new to drawing shapes graphics bitmaps etc. I googled for a few daysbut still havent got a real clue what to do so please help me: I want to draw a floorplan with certain objects(represented as circles) moving on it. When I click on a object it needs to show something. So far I ve been able to draw some circles on a graphic and been able to move the dots by clearing the graphic every time. Ofcourse this isnt a real solution since I cant keep track of the different objects on the floorplan (which i need for my clickevent and movings). I hope I explained my problem ok here. This is the (stripped version of the) sourcecode that gets called every second: (dev (of type Device) is the object i want to draw)  Graphics gfx = FloorplanTabPage.CreateGraphics(); gfx.Clear(Color.White); foreach (Device dev in _deviceList) { Pen myPen = new Pen(Color.Black) { Width = 10 }; if(dev.InRoom != null) { myPen.Color = Color.DarkOrchid; int x = dev.InRoom.XPos + (dev.InRoom.Width / 2) - 5; int y = (dev.InRoom.YPos + (dev.InRoom.Height / 2) - 5; if (dev.ToRoom != null) { x = (x + (dev.ToRoom.XPos + (dev.ToRoom.Width / 2)) / 2; y = (y + (dev.ToRoom.YPos + (dev.ToRoom.Height / 2)) / 2; } gfx.DrawEllipse(myPen x y 10 10); gfx.DrawString(dev.Name new Font(""Arial"" 10) Brushes.Purple x y - 15); } } Why aren't you using WPF? ok check my updated answer for an article link Remember this community is all about voting and accepting answers so if you think my answer is useful please vote and/or accept. I'll upvote your question anyway. I tried to vote up but I need Reputation Points for that I did mark your post as ""answered"" thanks a lot its working as it should now I think the easiest solution is to use WPF for this. If you do not want to use WPF then I suggest writing your own custom winforms control for each object you want on your floorplan drawing a circle when the control paints itself. This will give you full control of how the control looks and behaves. UPDATE: there are plenty of tutorials on this for example http://www.codeproject.com/KB/miscctrl/ScrollingTextControlArtic.aspx I would like to use WPF (already tried) but got the idea that it would things only more complicated since it started to bother me with Threading issues almost instantly. Any tutorial or examplecode I can look into is welcome. As far as I found out there are like 10000 ways te do something similar but thats only more confusing to a WPF newbie like myself Thanks a lot at least I now know what to look for. I'll post my solution as soon as i have it I think this is gonna work out well for me! thanks for helping out the newbie :)"
707,A,"Threaded image-scaling sub still bogs down UI I created an image-viewing control for my boss that incorporates panning zooming via the mouse wheel and drawing a box to zoom to. The control needs to support very large image files (i.e. several thousand pixels on each side). It all works but whenever the code is scaling the image the control UI becomes unresponsive. My boss had me use threading to set the scaling code apart from the UI. The scaling code is now definitely on a separate thread but the UI is still bogged down while scaling code is running! Can anyone please help me with this?? Below is the Scaling code. Let me know if this isn't enough to help me and I'll post whatever code you need! UPDATE: Here's the control code in its entirety. link text how are you invoking this method? your problem is not in this function but in the way you are attempting to run it asynchronously. Please post/describe the calling context. Ski please try the code from the latest update in my answer. Also you are passing both the ScaleImageArguments and the img arguments ByVal. Those should be passed ByRef. Although that is not the reason the UI is thread is blocking during the execution of the ScaleImage. It appears that there is some global lock within the GDI+ API. For test I created two threads based on following function  static void test_thread() { Bitmap bmp = new Bitmap( 4000 4000 ); Graphics g = Graphics.FromImage( bmp ); Brush b = Brushes.Red ; for ( ; ; ) { g.FillRectangle( b 0 0 bmp.Width bmp.Height ); } } If the infinite loop has ben left empty the CPU usage was over 90% so they they utilized both cores of my CPU. With the FillRectangle present the usage was slightly bellow 50% indicating that only one thread can run it at time. So it is possible that any GDI+ call you do from the GUI thread while the scaling is in progress will block until the scale completes. You are correct! I tried replacing any code in the threaded function that uses Graphics with a long-running empty for loop and the threading works as intended--tho my control now shows an empty box without the Graphics code. Now the question is how can I bypass that GDI+ blocking? Am I going to have to write my own scaling function? You might try to use the old GDI using the pinvoke mechanism. The problem is that you will not have control about quality of the scaling and there might be other limitations. Other than that I believe that you will have to write your own scaler or find existing one.  I suspect that since the scaling is using all the CPU that the UI is bogging. Both threads are probably at the same priority. Try lowering the priority of the scaling thread to allow the UI to be responsive. Values for Thread Priority: Above Normal -> Gives thread higher priority Below Normal ->Gives thread lower priority Normal -> Gives thread normal priority Lowest -> Gives thread lowest priority Highest -> Gives thread highest priority so you would probably use: Thread1.Priority=System.Threading.ThreadPriority.BelowNormal I somehow doubt that's the case. I think most likely the invocation is the problem. In the sub that creates and starts the subroutine above I already set the priority to BelowNormal. It makes no difference if I set the priority to Lowest. Ski please provide details about how you're invoking this method asynchronously. That way we'll be able to help! @Miky D: done! see above.  How is your ImageScaled() diplaying the image back to the UI? The control has a Dictionary of images called ScaledImageHash which uses the scale of the image (a Double) as the key. The control's Paint sub then paints the image in ScaledImageHash that corresponds to the current image scale. The Dictionary allows me to 1) Cache previously scaled images for future use and more importantly 2) Gives me the ability to preemptively scale the image based on the potential future scale when the user zooms with the mouse wheel. This works but every time I zoom the image the control preemptively scales the image again and the UI bogs down. Could the paint be bogged down waiting? You say you have a dictionary which is fine but what is the UI displaying while WAITING for the scaling to happen? Does it wait in the paint thread until the scaling is done or does it just return and NOT paint until the scaling is finished? If it's the first even if done in another thread it's still ""serial"" and thus threading gets you nothing. The second requires your callback to then paint it. @Kevin you may be right. I thought he was using the ImageScaled event to know when the operation has completed - although your scenario would explain why his threading is not yielding the expected results. I just updated with the paint sub. CurrentScaledImage = ScaledImageHash(CurrentImageScale). As you can see if the current scaled image is not found the paint sub just doesn't try to paint it. The ImageScaled event at the end of the ScaleImage sub calls Invalidate.  Try using this instead. Private Sub Worker(object o) Dim args as ScaleImageArguments args = CType(o ScaleImageArguments) ScaleImage(args) End Sub Private Sub RunScaleImageAsync(ByVal img As Image ByVal scale As Double) System.Threading.ThreadPool.QueueUserWorkItem(Worker _ New ScaleImageArguments(img.Clone scale)) End Sub ALTERNATIVE - Using the Async pattern. Private Delegate Sub ScaleImageDelegate(ByRef arg As ScaleImageArguments) Private Sub BeginScaleImage(ByRef img As Image ByVal scale As Double) Dim d As ScaleImageDelegate d = New ScaleImageDelegate(AddressOf ScaleImage) d.BeginInvoke(New ScaleImageArguments(img.Clone scale) _ New AsyncCallback(AddressOf EndScaleImage) d) End Sub Private Sub EndScaleImage(ar As IAsyncResult) Dim d As ScaleImageDelegate d = CType(ar.AsyncState ScaleImageDelegate) d.EndInvoke(ar) End Sub Then just call BeginScaleImage to run it asynchronously. EDIT - Please see corrections above. The ar argument on the EndScaleImage should be declared ByRef and also the img param of the BeginScaleImage. There is no reason why they should be passed ByVal!! Just tried it unfortunately it didn't make a difference. Have you tried the second option too? I tried the second option. With img as ByVal in BeginScaleImage there was no difference. With img as ByRef the code gets caught in an endless loop while the image is loading. Then the problem is most likely as @Kevin suggested in a comment on a different answer with the way you're ""waiting"" for the scaled image to be ready. I've looked at the Paint event handler but it's not very clear where those variables are coming from - CurrentScaleImage and the mouse_down mouse_end.. I don't think that's the problem though I won't say it's impossible. Should I just post the entire control code for you to loot at? I think that would be helpful. There is definitely something that has escaped us so far but unfortunately I have to get going for the day. If you don't get the answer until tomorrow I'll be happy to take a look at the code. It's an interesting problem!"
708,A,Does the iPhone 3G/3Gs camera put metadata in its images? (and how do you get it?) I've been trying to figure how whether the iPhone (either 3G or 3Gs) camera puts metadata into it's images. Anecdotally it appears that it does (e.g. I've seen images posted on the web that included a bunch of metadata) but I can't find reference to it anywhere in the SDK documentation. So....does anyone have a definitive answer? Also if there is metadata how do I get at it? Isn't it just normal EXIF metadata? IIRC the API hides that data out of privacy concerns. The data is in the images but you can't get to it using the Apple API. How would I get at that data then (for example if I wanted to parse it myself)? I think UIImage.CGImage only gives you content data not the entire thing.  There is metadata. Check out the iphone-exif project which provides you a means to get/set the EXIF tags. As they note UIImage will strip out the metadata. iphone-exif works around this. It requires you to use the UIImageJPEGRepresentation() function to feed the NSData into a specialized scanner class which they provide.
709,A,Creating SVGs using Python I'm building a set of SVG files that include an unfortunate number of hardcoded values (they must print with some elements sized in mm while others must be scaled as a percent and most of the values are defined relative to each other). Rather than managing those numbers by hand (heaven forbid I want to change something) I thought I might use my trusty hammer python for the task. SVG 1.1 doesn't natively support any kind of variable scheme that would let me do what I want and I'm not interested in introducing javascript or unstable w3c draft specs into the mix. One obvious solution is to use string formatting to read parse and replace variables in my SVG file. This seems like a bad idea for a larger document but has the advantage of being simple and portable. My second though was to investigate the available python->svg libraries. Unfortunately it seems that the few options tend to be either too new (pySVG still has an unstable interface) too old (not updated since 2005) or abandoned. I haven't looked closely but my sense is that the charting applications are not flexible enough to generate my documents. The third option I came across was that of using some other drawing tool (cairo for instance) that can be convinced to put out svg. This has the (potential) disadvantage of not natively supporting the absolute element sizes that are so important to me but might include the ability to output PDF which would be convenient. I've already done the googling so I'm looking for input from people who have used any of the methods mentioned or who might know of some other approach. Long-term stability of whatever solution is chosen is important to me (it was the original reason for the hand-coding instead of just using illustrator). At this point I'm leaning towards the first solution so recommendations on best practices for using python to parse and replace variables in XML files are welcome. Since SVG is XML maybe you could use XSLT to transform a source XML file containing your variables to SVG. In your XSLT style sheet you would have templates corresponding to various elements of your SVG illustration that change their output depending on the values found in the source XML file. Or you could use a template SVG as the source and transform it into the final one with the values passed as parameters to the XSLT processor. You's either use XSLT directly or via Python if you need some logic that's easier to perform in a traditional language. So that would be a variation of your first solution using a solution adapted to dealing with XML and providing powerful tools to help you such as XPath. I've done stuff in the past using XSLT and came away with a really nasty taste in my mouth. The parsers aren't standardized as far as features go and simple things like date transformations involved half a page of cruft. I'd like to stay away from them but might go have another look if there's not a better option. Though really thinking more about it maybe my task is simple enough that XSLT is the right way to go. It would be nice to keep the more complex languages out of the equation. Out of curiosity I just had a look at the Python bindings for libxslt (it's also been a while since I did something in XSLT and now I'm using Python a lot). The second example extfunc.py shows how to define a Python function you can call from the XSLT transformation. That could come in handy for example in case you'd ever need to make date transformation ;-) http://xmlsoft.org/XSLT/python.html If you're going down the XSLT path you might want to look at lxml the more modern and pythonic bindings for libxml2 and libxslt. http://codespeak.net/lxml/xpathxslt.html#xslt  A markup based templating engine such as genshi might be useful. It would let you do most of the authoring using a SVG tool and do the customization in the template. I'd definitely prefer it to XSLT. That's a good point. XSLT still makes me shudder. And for SVG documents Genshi should allow me to develop with test values and then replace them as is convenient. I'd be able to keep the templates open in my browser as I work.
710,A,How to rotate an object in Java 3D? I have a Cone I drew in Java 3D with the following code: Cone cone = new Cone(2f 3f); Transform3D t3d = new Transform3D(); TransformGroup coneTransform = new TransformGroup(t3d); coneTransform.setCapability(TransformGroup.ALLOW_TRANSFORM_WRITE); t3d.setTranslation(new Vector3f(0f0f0f); coneTransform.setTransform(t3d); coneTransform.addChild(cone); this.addChild(coneTransform); Suppose I have the cone sitting at point (111) and I want the tip of the cone to point down an imaginary line running through (000) and (111)... how can I do this? Here's an example of what I've been trying: Transform3D t3d = new Transform3D(); Vector3f direction = new Vector3f(121); final double angleX = direction.angle(new Vector3f(100)); final double angleY = direction.angle(new Vector3f(010)); final double angleZ = direction.angle(new Vector3f(001)); t3d.rotX(angleX); t3d.rotY(angleY); t3d.rotZ(angleZ); t3d.setTranslation(direction); coneTransform.setTransform(t3d); Thanks in advance for all help! This is not a java3D specific answer. In general a matrix can be built such that there are 4 vectors that describe it. 1) A side (or lateral) vector 2) An up vector 3) A direction vector 4) A position Each row of a 4x4 matrix. Thus for a simple identity matrix we have the following matrix (I'll define a column major matrix for a row major matrix all you need to do is swap the matrix indices around such that row 2 col 3 becomes row 3 col 2 throughout the matrix). 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 in this the first column is the side vector. The second column the up vector. The third the direction and the fourth the position. Logically we can see that the vector (1 0 0 0) points along the x axis (and thus is the side vector). The vector (0 1 0 0) points along the y axis (and thus is the up vector). The third (0 0 1 0) points along the Z-axis (and thus is the direction vector). The fourth (0 0 0 1) indicates that the objects does not move at all. Now lets say we wanted to face along the X-axis. Obviously that would mean we have a vector of (1 0 0 0 ) for our direction vector. Up would still be (0 1 0 0) and position still 0 0 0 1. So what would our side vector be? Well logically it would point along the z-axis. But which way? Well hold your fingers such that one finger points forward one to the side and one up. Now rotate so that the forward finger is facing the same direction as the side pointing finger. Which way is the side pointing finger pointing now? The opposite direction to the original direction pointing finger. Thus the matrix is  0 0 1 0 0 1 0 0 -1 0 0 0 0 0 0 1 At this point things seemingly get a little more complicated. It is simple enough to take an arbitrary position and an arbitrary point to look at (I'll call them vPos and vFocus). It is easy enough to form a vector from vPos to vFocus by subtracting vPos from vFocus (vFocus.x - vPos.x vFocus.y - vPos.y vFocus.z - vPos.z vFocus.w - vPos.w ). Bear in mind all positions should be defined with a '1' in the w position where all directions should have a '0'. This is automatically taken care of when you do the subtraction above as the 1 in both ws will cancel out and leave 0. Anyway we now have a vector pointing from the position towards vFocus we'll call it vDir. Unfortunately it has the length of the difference between vPos and vFocus. However if we divide the vDir vector by its length (vDir.x / length vDir.y / length vDir.z / length vDir.w / length) then we normalise it and we have a direction with a total length of 1. At this ponit we now have our 3rd and 4th columns of our matrix. Now lets assuem up is still (0 1 0 0) or vUp. We can assume that the crossproduct of the direction and vUp will produce a vector that is perpendicular (and also of unit length) to the plane formed by vDir and vUp. This gives us our side vector or vLat. Now .. we did kind of assume the up vector so its not strictly correct. We can now calculate it exactly by taking the cross product of vLat and vDir and we have all 4 vectors. The final matrix is thus defined as follows vLat.x vUp.x vDir.x vPos.x vLat.y vUp.y vDir.y vPos.y vLat.z vUp.z vDir.z vPos.z vLat.w vUp.w vDir.w vPos.w This isn't strictly the full answer as you will get problems as you look towards a point near to your (0 1 0 0) vector but that should work for most cases.  I'm just learning Java 3D myself at the moment and from my current knowledge the rotation methods set the transform to a rotation about that axis only. Therefore if you wish to perform rotations about multiple axes then you will need to use a second Transform3D. ie: Transform3D rotation = new Transform3D(); Transform3D temp = new Transform3D(); rotation.rotX(Math.PI/2); temp.rotZ(Math.PI/2); rotation.mul(temp); // multiply the 2 transformation matrices together. As for the reason for Math.PI this is because it uses radians instead of degrees where Math.PI is equivalent to 180 degrees. Finding the angle between your current orientation and your intended orientation isn't too hard - you could use Vector3fs with the angle() method. A Vector would be set up with the initial orientation and another in the intended. However this doesn't tell you in which axes the angle lies. Doing so would require examination of the vectors to see which segments are set. [of course there may be something that I am currently unaware of in the API] I went with using quaternions for rotations. Rotating around x and y just kept giving me unreliable issues. Apparently gimble lock is not 100% avoidable with them. Quaternions avoid this. Ah... so I need a separate Transform3D for each axis I want to rotate on? I was trying to use the same instance of Transform3D and doing the rotX() rotY() rotZ() on it. I'll give that a try.  you can give your Transform3D a rotation matrix. you can get a rotation matrix using Rotation matrix calculator online: http://toolserver.org/~dschwen/tools/rotationmatrix.html here's my example:  Matrix3f mat = new Matrix3f(0.492403876506104f 0.586824088833465f -0.642787609686539f 0.413175911166535f 0.492403876506104f 0.766044443118978f 0.766044443118978f -0.642787609686539f 0f); Transform3D trans = new Transform3D(); trans.set(mat);  I finally figured out what I wanted to do by using Quaternions which I learned about here: http://www.cs.uic.edu/~jbell/Courses/Eng591_F1999/outline_2.html Here's my solution. Creating the cone:  private void attachCone(float size) { Cone cone = new Cone(size size* 2); // The group for rotation arrowheadRotationGroup = new TransformGroup(); arrowheadRotationGroup. setCapability(TransformGroup.ALLOW_TRANSFORM_WRITE); arrowheadRotationGroup.addChild(cone); // The group for positioning the cone arrowheadPositionGroup = new TransformGroup(); arrowheadPositionGroup. setCapability(TransformGroup.ALLOW_TRANSFORM_WRITE); arrowheadPositionGroup.addChild(arrowheadRotationGroup); super.addChild(arrowheadPositionGroup); } Now when I want to rotate the cone to point in a certain direction specified as the vector from the point (000) to (direction.x direction.y direction.z) I use: private final Vector3f yAxis = new Vector3f(0f 1f 0f); private Vector3f direction; private void rotateCone() { // Get the normalized axis perpendicular to the direction Vector3f axis = new Vector3f(); axis.cross(yAxis direction); axis.normalize(); // When the intended direction is a point on the yAxis rotate on x if (Float.isNaN(axis.x) && Float.isNaN(axis.y) && Float.isNaN(axis.z)) { axis.x = 1f; axis.y = 0f; axis.z = 0f; } // Compute the quaternion transformations final float angleX = yAxis.angle(direction); final float a = axis.x * (float) Math.sin(angleX / 2f); final float b = axis.y * (float) Math.sin(angleX / 2f); final float c = axis.z * (float) Math.sin(angleX / 2f); final float d = (float) Math.cos(angleX / 2f); Transform3D t3d = new Transform3D(); Quat4f quat = new Quat4f(a b c d); t3d.set(quat); arrowheadRotationGroup.setTransform(t3d); Transform3D translateToTarget = new Transform3D(); translateToTarget.setTranslation(this.direction); arrowheadPositionGroup.setTransform(translateToTarget); }  I think this should do it: coneTransform.rotX(Math.PI / 4); coneTransform.rotY(Math.PI / 4); Thanks. t3d.rotX(Math.PI / 2); and similarly for Y Z will rotate the cone. Could you describe how to calculate the angle to rotate X and Y? For instance if I have a line drawn from (000) to (111) how to calculate the amount to rotate by? Rotating around x and y just kept giving me unreliable issues. I eventually discovered a way you can see in the answer I posted. The rotation functions take radian values as input. There is 2*PI one full rotation or PI/2 degrees between perpendicular lines (This means that I was wrong and you need to rotate PI/4 only). In general you never need to rotate along all three angles as you can describe any rotations by rotating along two axes. Haha-- I *just* realized I don't need all 3 rotations. I'm working it out on paper now how to compute the angles I need. Thanks for your help.
711,A,Extracting rgb color components from integer value if there is an integer value eg. 86 then how can i extraact the rgb components from this integer value....? I am working on Visual C++ 2008 express edition. Thanks.. How are the RGB components encoded in this integer in the first place? let me explain what i have done uptil now... i have read the file content into a byte array so now the byte array has values from 0 - 255 now i have to display these array values in a picturebox control so i used Bitmap^ bmp; bmp->SetPixel(ijColor::FromArgb( buf[count] ); //buf is the byte array but this code does not display anything so i found out that maybe if i extract the rgb components and give them as argument in Color::FromArbg(int redint greenint blue) then it might work.. eg. if i have value 86 then how can i get rgb components from it..? Usually (and I repeat usually since you don't specify much in your question) a color is packed in a 4-bytes integer with RGBA components. What you need to do it so mask and shift for example: int color = 0xRRGGBBAA; u8 red = (color & 0xFF000000) >> 24; u8 green = (color & 0x00FF0000) >> 16; u8 blue = (color & 0x0000FF00) >> 8; This assumes the kind of encoding I specified but can me modified according to yours. EDIT: in your example you spoke about a 0-255 value. It is not clear if compnents are 2bit sized (4 intensity values per component). In that case the approach still remains the same but you will have just few colors: u8 color = 86; // so you take 2 bits and multiply by 64 to possibly have intensities: 0 64 128 192 u8 red = ((color & 0xC0) >> 6) * 64; u8 green = ((color & 0x30) >> 4) * 64; u8 blue = ((color & 0x0C) >> 2) * 64; EDIT2: Maybe your colors are indexed with palette in that case you should have an array that stores the palette itself and the byte you read from the file should be the index of a color stored somewhere else. Thanks a ton...!  Normally you want to create a color from three components using the RGB macro. Assuming the value you have is in the same format you can pull it back apart into the individual pieces with GetRValue GetGValue and GetBValue.
712,A,Calculate a bezier spline to get from point to point I have 2 points in XY + Rotation and I need to calculate a bezier spline (a collection of quadratic beziers) that connects these 2 points smoothly. (see pic) The point represents a unit in a game which can only rotate slowly. So to get from point A to B it has to take a long path. The attached picture shows quite an exaggeratedly curvy path but you get the idea. What formulas can I use to calculate such a bezier spline? Just saw that I misunderstood your question. Couldn't you use a single cubic hermite splines instead since you have a start and end point and two directions (tangents)? Are there any additional constraints? To calculate the start and end tangents just use the start and end direction and scale them with the distance between the start and end points (and additionally some other constant factor like 0.5 depending on how curvy you want the path to be).: p0 = startpoint; p1 = endpoint; float scale = distance(p0 p1); m0 = Vec2(cos(startangle) sin(startangle)) * scale; m1 = Vec2(cos(endangle) sin(endangle)) * scale; I use this system to interpolate camera paths in a game I'm working on and it works great.  I don't remember where I got it but I have been using this: Vector2 CalculateBezierPoint(float t Vector2 p0 Vector2 p1 Vector2 p2 Vector2 p3) { float u = 1 - t; float tt = t*t; float uu = u*u; float uuu = uu * u; float ttt = tt * t; Vector2 p = uuu * p0; //first term p += 3 * uu * t * p1; //second term p += 3 * u * tt * p2; //third term p += ttt * p3; //fourth term return p; } where t is ratio along path between start and end points.  As you probably know there are infinite solutions even if we assume 2 control points. I found Smoothing Algorithm Using Bézier Curves which answers your question (see equations of Bx(t) and By(t) in the beginning): Bx(t) = (1-t)3 P1x + 3 (1-t)2 t  P2x + 3 (1-t) t2 P3x + t3 P4x By(t) = (1-t)3 P1y + 3 (1-t)2 t  P2y + 3 (1-t) t2 P3y + t3 P4y P1 and P4 are your endpoints and P2 and P3 are the control points which you're free to choose along the desired angles. If your control point is at a distance r along an angle θ from point (x y) the coordinates of the point are: x' = x - r sin(θ) y' = y - r cos(θ) (according to the co-ordinate system you've used—I think I got the signs right). The only free parameter is r which you can choose as you want. You probably want to use r = α dist(P1 P4) with α < 1.
713,A,"iPhone Rendering Question I'm new to iPhone/Objective-C development. I ""jumped the gun"" and started reading and implementing some chapters from O'Reilly's iPhone Development. I was following the ebook's code exactly and my code was generating the following error: CGContextSetFillColorWithColor: invalid context CGContextFillRects: invalid context CGContextSetFillColorWithColor: invalid context CGContextGetShouldSmoothFonts: invalid context However when I downloaded the sample code for the same chapter the code is different. Book Code: - (void) Render { CGContextRef g = UIGraphicsGetCurrentContext(); //fill background with gray CGContextSetFillColorWithColor(g [UIColor grayColor].CGColor); CGContextFillRect(g CGRectMake(0 0 self.frame.size.width self.frame.size.height)); //draw text in black. CGContextSetFillColorWithColor(g [UIColor blackColor].CGColor); [@""O'Reilly Rules!"" drawAtPoint:CGPointMake(10.0 20.0) withFont:[UIFont systemFontOfSize:[UIFont systemFontSize]]]; } Actual Project Code from the website (works): - (void) Render { [self setNeedsDisplay]; //this sets up a deferred call to drawRect. } - (void)drawRect:(CGRect)rect { CGContextRef g = UIGraphicsGetCurrentContext(); //fill background with gray CGContextSetFillColorWithColor(g [UIColor grayColor].CGColor); CGContextFillRect(g CGRectMake(0 0 self.frame.size.width self.frame.size.height)); //draw text in black. CGContextSetFillColorWithColor(g [UIColor blackColor].CGColor); [@""O'Reilly Rules!"" drawAtPoint:CGPointMake(10.0 20.0) withFont:[UIFont systemFontOfSize:[UIFont systemFontSize]]]; } What is it about these lines of code that make the app render correctly? - (void) Render { [self setNeedsDisplay]; //this sets up a deferred call to drawRect. } - (void)drawRect:(CGRect)rect { Thanks in advance for helping out a newbie! When you draw things to the screen you need to have a graphics context (which I'm sure You've read all about) but this is only provided (more or less) when Cocoa calls -drawRect: so your options are basically only call render in -drawRect: which doesn't make much sense from the look of what you're trying to do or have -render tell the system that you want to draw your view which will cause the system to (eventually) create a graphics context and call your -drawRect: where the actual drawing code needs to be. Otherwise there will be no graphics context to draw to which I believe is what your errors are saying. (note: system above means as much UIKit as it does OS) Makes sense... thank you Jared!"
714,A,"What services have you used for design work for your web site programming? As a programmer I don't have the best design skills. I often find the need to have a single graphic or even an entire web site designed. What sites have you used to get the graphics you needed and would you use them again? You can contract a designer using Elance. You can find designers (and developers) at excellent rates. The site provides both portfolios and reviews for designers. Check out the designer's portfolio to get a sense of their style. When working with designers be very explicit about what you want in order to keep the amount of rework to a minimum.  You could try this fairly new site called 99 designs it lets you run a 'contest' (with a prize amount you set) people submit their entries and you choose and pay for the one you like most. I'm curious about your experiences here. Have you personally used them and would you use them again? I used them for our company logo and StackOverflow used them for their logo. For me it was a great experience. If you launch a contest be sure to give feedback on every submission. It will encourage more submissions. I'd definitely use them again.  At work we contract out to a firm we've used numerous times in the past (the designer is very good and has been our go to for all our recent design work on our various sites). When I'm looking for something non-work related I turn to sites that have been mentioned above (Open Source Web Design and Open Web Design are my favorites). I've also seen friends use TemplateMonster for good Flash components or designs. I also try to keep in touch with design blogs and such to try to expand my horizons a little so I have a better understanding of what good design looks like. So while I may not be able to create the designs I at least have some sense of what to look for in a ""good design.""  I've used TemplateMonster in the past as well as Open Source Web Design. Another is Open Web Design. I would definitely consider 99 designs though I have never used them. Btw you're not alone thinking you're not a good designer. Most programmers are terrible designers. The worst offenders tend to be the ones that think they're good designers because they've learned all the buttons in photoshop. If you can you should consider hiring a web designer to build you a design. For an open-source project you may be able to find someone willing to contribute a design which lets them bolster their own portfolio."
715,A,Visual Python - Visualize graphs relating to a movement I'm working with visual python on a project where I need to simulate a physical movement. I'd like to present in a different window than the one the actual 3D sim is running two graphs both related to the movement: How the velocity and angular velocity progress over time. How the movement and rotation progress over time. All these vars are refreshed once per cycle (inside a while(true)) How can I accomplish this? Thank you for your time! I've accomplished what I wanted with gdisplay. Reference: here  If you're looking to do plots of live data you can use matplotlib. Here's a live plotting example: http://eli.thegreenplace.net/files/prog_code/wx_mpl_dynamic_graph.py.txt | Screenshot It uses wx but matplotlib has bindings for QT or GTK. Alternatively you could use the rpy2 interface to hook into R which has an excellent plotting engine and may be slightly faster for large amounts of data.
716,A,Where is the Shape located on Canvas? Say I put a Shape on a Canvas like this: Canvas.SetLeft(myShape 50); Canvas.SetTop(myShape 30); canvas.Children.Add(myShape); After that myShape was drag & dropped on this Canvas. How can I determine the current myShape coordinates ? For example before the drag & drop I would like to get (5030). Thanks ! var left = Canvas.GetLeft(myShape); var top = Canvas.GetTop(myShape); Thanks a lot !! You might want to **accept** Aviad's answer. Thanks doesn't feed his kids. ;-) Nice althought accepting the answer doesn't feed his kids either. My bad :-)
717,A,"Fast Perspective transform in Java Advanced Imaging API For the needs of my program I have created a facility to distort an image and place it on a map (my program is a map based program). I wrote my own mechanism for placing and distorting the image using three points that are placed on the image three points that are placed on the map and then what I simply do it create an AffineTransform that transforms the first triangle to the second in effect placing the image exactly where I want it and transformed to fit what the user wanted. The problem is that with AffineTransforms you can only perform the most basic of transformations. You can translate rotate scale and skew your image. This is enough for most cases but I recently wanted to implement a 4 point transform similar to Photoshop's free transform. I did some research and found JAI's Perspective transform which with the help of a WarpPerspective I can achieve what I want. I initially had one issue with having the resulting background of the transformed image transparent but i found a solution to that as well. This is the code i have: package com.hampton.utils; import java.awt.Dimension; import java.awt.RenderingHints; import java.awt.Transparency; import java.awt.color.ColorSpace; import java.awt.image.ColorModel; import java.awt.image.DataBuffer; import java.awt.image.SampleModel; import java.awt.image.renderable.ParameterBlock; import javax.media.jai.ImageLayout; import javax.media.jai.Interpolation; import javax.media.jai.JAI; import javax.media.jai.PerspectiveTransform; import javax.media.jai.PlanarImage; import javax.media.jai.RasterFactory; import javax.media.jai.RenderedOp; import javax.media.jai.WarpPerspective; import javax.media.jai.operator.CompositeDescriptor; /** * @author Savvas Dalkitsis */ public class PerspectiveTransformation { public static PlanarImage getPrespective(PlanarImage img) { int numBands = img.getSampleModel().getNumBands(); ParameterBlock pb = new ParameterBlock(); pb.add(new Float(img.getWidth())).add(new Float(img.getHeight())); pb.add(new Byte[]{new Byte((byte)0xFF)}); RenderedOp alpha = JAI.create(""constant"" pb); pb = new ParameterBlock(); pb.addSource(img).addSource(img); pb.add(alpha).add(alpha).add(Boolean.FALSE); pb.add(CompositeDescriptor.DESTINATION_ALPHA_LAST); SampleModel sm = RasterFactory.createComponentSampleModel(img.getSampleModel() DataBuffer.TYPE_BYTE img.getTileWidth() img.getTileHeight() numBands + 1); ColorSpace cs = ColorSpace.getInstance(numBands == 1 ? ColorSpace.CS_GRAY : ColorSpace.CS_sRGB); ColorModel cm = RasterFactory.createComponentColorModel(DataBuffer.TYPE_BYTE cs true false Transparency.BITMASK); ImageLayout il = new ImageLayout(); il.setSampleModel(sm).setColorModel(cm); RenderingHints rh = new RenderingHints(JAI.KEY_IMAGE_LAYOUT il); RenderedOp srca = JAI.create(""composite"" pb rh); Dimension d = new Dimension(img.getWidth()img.getHeight()); PerspectiveTransform p = PerspectiveTransform.getQuadToQuad(0 0 d.width 0 d.width d.height 0 d.height 100 0 300 0 d.width d.height 0 d.height); WarpPerspective wp = new WarpPerspective(p); pb = (new ParameterBlock()).addSource(srca); pb.add(wp); pb.add(Interpolation.getInstance(Interpolation.INTERP_BICUBIC)); PlanarImage i = (PlanarImage)JAI.create(""warp""pb); return i; } } It works (I have hard coded the transformation for the sake of simplicity later on i will incorporate it with my transformation system only this time i will use 4 points instead of 3). The problem i have is that so far with my old method all i had to do is store an AffineTransform (that maps the image to the map) and the simply do a g2.drawImage(imgtransform) and all was good. It was really fast and it worked like a charm. My problem now is that creating the transformed PlanarImage takes a lot of time (2-3 seconds) and thus is is not useful for real-time use. I know i can simply save the transformed image and then draw that which will be really fast but the reason i want this is that in my 3 point transformation mechanism i provide real time view of the resulting transformation. When the user drags each of the points he immediately sees the resulting image. My question then is is there a fast way to use WarpPerspectives in Graphics2D? What I am looking for is something similar to Graphics2D draw(ImageAffineTransform) method. Or if you have a third party image transformation library that performs really fast that would be welcome as well. Try switching off the bicubic interpolation - that should save you some time but it would produce lower quality results. Before 3D acceleration became widespread games used smart tricks for drawing images in perspective. Wikipedia has a great article explaining this in detail. I am not aware of a Java-based library utilizing those concepts but you could implement the affine-mapping version quite easily - just split the original image into several triangles and map each triangle using a different affine transform. If you want really smooth correct anti-aliased preview I would suggest using a 3D library like Java3D o JOGL. i thought of the triangle based approach but if you think about it a while you will see that it is not correct...You are assuming that the two triangles can be transformed independently from each other while this is not the case... I found a better approach by using the Filters library from Java Image Editor...It is slightly faster but requires me to create an extra buffered image each step. It is faster that this but a memory hog... @SavvasDalkitsis Using > 100 triangles should give visually OK results."
718,A,"How are Swing components internally created laid out repainted notified of events ...? I wonder if there's a good documentation (or a (viewable) ebook) about the lifecycle of Swing components. Is ""lifecycle"" the correct term anyway? I hope to find answers to question such as: How when in which order painting methods are called? How when which events are called by whom? What is the exact sequence of method calls for component creation? From time to time I encounter strange behavior of my apps for example: ComponentListener's resize event is called before setVisible(true) (so that root pane has negative dimensions!) Some components are laid out correctly only after resizing the JFrame by hand Changing a super class from JPanel to JLayeredPane causes my class to be laid out differently inside an other container. And lot of other strange things... I had the same question long ago. I can't believe how hard is to find a good resource about this topic in the internet. Fortunately I've found this link and now I have it in my bookmark with golden tag. :) A Swing Architecture Overview Once you have a good grasp of how they work conceptually you will be able to fix most of the problems you mention. I hope it helps."
719,A,"Rotating an Image in Silverlight without cropping I am currently working on a simple Silverlight app that will allow people to upload an image crop resize and rotate it and then load it via a webservice to a CMS. Cropping and resizing is done however rotation is causing some problems. The image gets cropped and is off centre after the rotation. WriteableBitmap wb = new WriteableBitmap(destWidth destHeight); RotateTransform rt = new RotateTransform(); rt.Angle = 90; rt.CenterX = width/2; rt.CenterY = height/2; //Draw to the Writeable Bitmap Image tempImage2 = new Image(); tempImage2.Width = width; tempImage2.Height = height; tempImage2.Source = rawImage; wb.Render(tempImage2rt); wb.Invalidate(); rawImage = wb; message.Text = ""h:"" + rawImage.PixelHeight.ToString(); message.Text += "":w:"" + rawImage.PixelWidth.ToString(); //Finally set the Image back MyImage.Source = wb; MyImage.Width = destWidth; MyImage.Height = destHeight; The code above only needs to rotate by 90° at this time so I'm just setting destWidth and destHeight to the height and width of the original image. Many thanks to those above.. they helped a lot. I include here a simple example which includes the additional transform necessary to move the rotated image back to the top left corner of the result.  int width = currentImage.PixelWidth; int height = currentImage.PixelHeight; int full = Math.Max(width height); Image tempImage2 = new Image(); tempImage2.Width = full; tempImage2.Height = full; tempImage2.Source = currentImage; // New bitmap has swapped width/height WriteableBitmap wb1 = new WriteableBitmap(heightwidth); TransformGroup transformGroup = new TransformGroup(); // Rotate around centre RotateTransform rotate = new RotateTransform(); rotate.Angle = 90; rotate.CenterX = full/2; rotate.CenterY = full/2; transformGroup.Children.Add(rotate); // and transform back to top left corner of new image TranslateTransform translate = new TranslateTransform(); translate.X = -(full - height) / 2; translate.Y = -(full - width) / 2; transformGroup.Children.Add(translate); wb1.Render(tempImage2 transformGroup); wb1.Invalidate(); Thanks for posting this Chris. Super helpful.  It looks like your target image is the same size as your source image. If you want to rotate over 90 degrees your width and height should be exchanged: WriteableBitmap wb = new WriteableBitmap(destHeight destWidth); Also if you rotate about the centre of the original image part of it will end up outside the boundaries. You could either include some translation transforms or simply rotate the image about a different point: rt.CenterX = rt.CenterY = Math.Min(width / 2 height / 2); Try it with a piece of rectangular paper to see why that makes sense. If I could give you more reputation I would! Been banging my head against this problem too long. Should've just grabbed a bit of A4 from the start!  If the image isn't square you will get cropping. I know this won't give you exactly the right result you'll need to crop it afterwards but it will create a bitmap big enough in each direction to take the rotated image.  //Draw to the Writeable Bitmap Image tempImage2 = new Image(); tempImage2.Width = Math.Max(width height); tempImage2.Height = Math.Max(width height); tempImage2.Source = rawImage;"
720,A,Generating a beveled edge for a 2D polygon I'm trying to programmatically generate beveled edges for geometric polygons. For example given an array of 4 vertices defining a square I want to generate something like this. But computing the vertices of the inner shape is baffling me. Simply creating a copy of the original shape and then scaling it down will not produce the desired result most of the time. My algorithm so far involves analyzing adjacent edges (triples of vertices; e.g. the bottom-left top-left and top-right vertices of a square). From there I need to find the angle between them and then create a vertex somewhere along that angle depending on how deep I want the bevel to be. And because I don't have much of a math background that's where I'm stuck. How do I find that center angle? Or is there a much simpler way of attacking this problem? Will you always have 4 vertices or do you need a solution that will work for an arbitrary number? In what way does simply scaling it down not work? @Skywalker I need a general solution that will work for any convex or concave polygon. @mathmike Consider a C-shaped polygon. A scaled-down version will not fit nicely inside the original. Certainly the bevel will not be uniform. The general algorithm is pretty complex. The operation you're looking for is known as offsetting the polygon; if you search around for that you might find some pointers/papers etc. If you're working in or near C++ you could try CGAL. This seems to be the answer I was looking for. However it is more complicated than I had hoped. I'm going to have to forgo bevels for now.  Let say your point is p1 and points creating adjacent edges are points p2 and p3. Then take a vector from p1 to p2 and p1 to p3. like - v1 = p2 - p1 v2 = p3 - p1 Find the angle between v1 and v2 and generate your point. You can find angle using this.  I would do something like this: for each side make a copy and push it 'inward' the desired width of the bevel. ('inward' being along the normal vector of the side). Once you've done this find the intersection points between the new copies (and the copies of whichever sides they previously intersected) and use those as the vertices for your inner shape. For the intersections you'll need consider true lines (rather than segments) since sides in concave regions will need to grow. This will break horribly if you try to use it on a shape with regions less then twice the width of your bevel size but should be fine otherwise. (I'm sure you could add something to handle those cases but that's another discussion) Alternately if you wanted the bevel width to be relative to vertices you could also just push those 'inward' using the same principle. Estimate the vertice's normal angle by averaging the normals of the side it connects.
721,A,Set bufferedimage to be a color in Java I need to create a rectangular BufferedImage with specified color as a background draw some pattern on the background and save it to file. My problem comes from how to create the backgroundI am using nexted loop: BufferedImage b_img = ... for every row for every column setRGB(rgb); But it's very slow when the image is large. How to set the color in a more efficient way? Probably something like: BufferedImage image = new BufferedImage(...); Graphics2D g2d = image.createGraphics(); g2d.setColor(...); g2d.fillRect(...);  Get the graphics object for the image set the current paint to the desired colour then call fillRect(00widthheight). BufferedImage b_img = ... Graphics2D graphics = b_img.createGraphics(); graphics.setPaint ( new Color ( r g b ) ); graphics.fillRect ( 0 0 b_img.getWidth() b_img.getHeight() );
722,A,"Capturing print output as vector format (PDFSVGEMFetc.) BACKGROUND I am using a commercial application on windows that creates a drawing This application allows only two output options: (1) save as a bitmap file and (2) print to a printer the bitmap is useless for my purposes - I want the vectors Looking at the print output (I sent to the Windows XPS print driver) it seems clear based on the amount of zooming I can do without loss of detail that the underlying vectors are being send to the print driver Once I get the vectors I will be writing some code to transform them for some other use. MY QUESTION Whart are my options for geting the vectors from the print? (am open to both commercial and open source) OPTIONS I HAVE THOUGHT OF SO FAR Take the bitmap and use a program like VectorMagick to. I have tried this approach. It does not produce the fidelity I seek even when the original bitmap is large. Practically speaking I believe that using any tracing approach will not give me the quality vectors I need. Print to the Adobe PDF driver. This technically works. I have Adobe CS4 so I can print to it save the resulting PDF and then import the PDF into Illustrator and then export as some other vector format. The problem with this approach is money/licensing. I own a personal copy of Adobe CS4 - so this is fine for me. But I need to capture the vectors at work for business purposes - and no I'm not going to install my personal copy of CS4 at work. Is there a ""print driver"" that captures the print output directly into a vector format? I have seen some commercial ones via google. If you've used them I would like to hear about your experience with this technique. I could write my own and in that case do you have links to any existing code that I can start with. http://sourceforge.net/projects/emfprinter/  My favorite is the open source (GPL) PDFCreator  If this is an ongoing solution you need then you might need to buy something or build your own. If it's a onetime affair you might look to use an 'older' Lexmark PCL printer driver. I'd recommend something like the T610. If you download the PCL driver and install it you can modify the defaults and change the Graphics option from XL or Autoselect to GL/2. This will force the driver to output GL/2 output which is vector (GL/2 is a plotter language). This might do the trick for you. Other printer drivers may have the abiltiy to force GL/2 (vs. Raster) but I'm not sure. I use to work for Lexmark and have used this before for a similar requirement. Ensure you use the Lexmark 'Custom' driver as I don't think the Microsoft-based one support this feature. ...pausing while I investigate a few things............I'm back... Another option is to find another GL/2 driver or build you own...I just took a few minutes to search the web and came up with a few other options that might work. Build you own: I've built drivers (minidrivers) using the Windows Driver Development Kit (DDK) it's quite simple to construct basic drivers. Looks like there is a setting you can set to enable GL/2 output: Enabling HP-GL/2 Vector Graphics Support (PCL-5e) in the GPD Alternate drivers: Depending on the OS you are on there is probably a 'generic' GL/2 driver built in. I believe XP has a Hewlett-Packard HP-GL/2 Plotter. You might need to check the license (as with the Lexmark solution) but it might work for you and as it's part of the OS there shouldn't be concern about using it. It's probably written and copyrighted to Microsoft Keep in mind you will have to do some work to convert GL/2 to whatever output you want but it should be a matter of an simple translator to convert each set of commands. There may be tools out there to help. Here is a quick link to Lexmark GL/2 reference which might be enough to get you going check out the GL/2 information under the PCL section: Lexmark Technical Reference Guide Postscript: The last option I have is to use a generic Postscript driver. Postscript should output the vector images as vector graphics in the Postscript but my knowledge of this is limited at best. Output: If you need the output to route to file you can set the port to FILE: which requries user intervention or install something like Redmon (or connect with me and I'll send you our port monitor that allows for automatic output to file). Hope this helps in some way."
723,A,How do I draw double height text using Graphics.DrawString? I am trying to emulate a POS printer with System.Drawing and one of the functions I need is to draw text at double height. Any idea how I can do this using .Net's Graphics class? Do I need to draw the text twice as large and condense it or draw normal size and then stretch? Both seem like messy options but is there an alternative? Look at the transformation matrix on the Graphics object - you can control horizontal and vertical scaling independently.  Use a ScaleTransform and only scale up the y.
724,A,"BlackBerry - Develope Cropit Like Application i want to develop cropit like application i just want to know how to increase or decrease the size of the rectangle over the image (multitouch events) which define the image portion to be cropped. Thanks alot Use onTouchEvent look for DOWN and MOVE events to correctly draw selection. To make simulator multitouch: Alt+T and then use left mouse button for finger 1 and right mouse button for finger 2. Code sample: class Scr extends MainScreen { XYPoint mLeftUp = null; XYPoint mRightBottom = null; XYPoint mCoursor = null; // Bitmap bitmap = Bitmap.getBitmapResource(""wallpaper_bold.jpg""); Bitmap bitmap = Bitmap.getBitmapResource(""wallpaper_storm.jpg""); BitmapField mBitmapField = new BitmapField(bitmap); public Scr() { add(mBitmapField); mCoursor = new XYPoint(bitmap.getWidth() >> 1 bitmap.getHeight() >> 1); } protected void makeMenu(Menu menu int instance) { super.makeMenu(menu instance); if (mLeftUp != null && mRightBottom != null) { menu.add(new MenuItem(""crop"" 0 0) { public void run() { XYRect rect = null; if (mLeftUp.x > mRightBottom.x && mLeftUp.y > mRightBottom.y) { rect = new XYRect(mRightBottom mLeftUp); } else { rect = new XYRect(mLeftUp mRightBottom); } Bitmap crop = cropImage(bitmap rect.x rect.y rect.width rect.height); mBitmapField.setBitmap(crop); mCoursor = new XYPoint(crop.getWidth() >> 1 crop .getHeight() >> 1); mLeftUp = null; mRightBottom = null; invalidate(); } }); menu.add(new MenuItem(""reset"" 0 0) { public void run() { mBitmapField.setBitmap(bitmap); mCoursor = new XYPoint(bitmap.getWidth() >> 1 bitmap .getHeight() >> 1); invalidate(); } }); } } protected void paint(Graphics graphics) { super.paint(graphics); if (mCoursor != null) { graphics.setColor(Color.RED); graphics.drawLine(mCoursor.x - 4 mCoursor.y - 4 mCoursor.x + 4 mCoursor.y + 4); graphics.drawLine(mCoursor.x + 4 mCoursor.y - 4 mCoursor.x - 4 mCoursor.y + 4); } if (mLeftUp != null && mRightBottom != null) { graphics.setColor(Color.RED); graphics.drawPoint(mLeftUp.x mLeftUp.y); graphics.drawPoint(mRightBottom.x mRightBottom.y); graphics.drawPoint(mLeftUp.x mRightBottom.y); graphics.drawPoint(mRightBottom.x mLeftUp.y); graphics.setColor(Color.WHITESMOKE); XYRect redRect = null; if (mLeftUp.x > mRightBottom.x && mLeftUp.y > mRightBottom.y) { redRect = new XYRect(mRightBottom mLeftUp); } else { redRect = new XYRect(mLeftUp mRightBottom); } graphics.drawRect(redRect.x redRect.y redRect.width redRect.height); } } // comment block for Bold protected boolean touchEvent(TouchEvent message) { int x1 = message.getX(1); int y1 = message.getY(1); int x2 = message.getX(2); int y2 = message.getY(2); switch (message.getEvent()) { case TouchEvent.DOWN: if (x1 != -1) { mLeftUp = new XYPoint(x1 y1); } else if (x2 != -1) { mRightBottom = new XYPoint(x2 y2); } break; case TouchEvent.MOVE: if (x1 != -1) { mLeftUp = new XYPoint(x1 y1); } if (x2 != -1) { mRightBottom = new XYPoint(x2 y2); } break; case TouchEvent.UNCLICK: mLeftUp = null; mRightBottom = null; default: break; } invalidate(); return true; } protected boolean navigationMovement(int dx int dy int status int time) { moveCoursor(dx dy); return true; } private void moveCoursor(int dx int dy) { mCoursor.translate(dx dy); if (mLeftUp != null) { mRightBottom = new XYPoint(mCoursor); } invalidate(); } protected boolean navigationUnclick(int status int time) { clickCoursor(); return true; } private void clickCoursor() { if (mLeftUp != null && mLeftUp.equals(mCoursor)) { mLeftUp = null; mRightBottom = null; } else { mLeftUp = new XYPoint(mCoursor); } invalidate(); } public static Bitmap cropImage(Bitmap image int x int y int width int height) { Bitmap result = new Bitmap(width height); Graphics g = Graphics.create(result); g.drawBitmap(0 0 width height image x y); return result; } } Hey Thanks man this is the code which i wanted.. thanks alot You're welcome :)"
725,A,How to program a full-screen mode in Java? I'd like my application to have a full-screen mode. What is the easiest way to do this do I need a third party library for this or is there something in the JDK that already offers this? Use this code: JFrame frame = new JFrame(); // set properties frame.setSize(Toolkit.getDefaultToolkit().getScreenSize()); frame.setUndecorated(true); frame.setVisible(true); Make sure setUndecorated() comes before setVisible() or it won't work.  It really depends on what you're using to display your interface i.e. AWT/Spring or OpenGL etc. Java has a full screen exclusive mode API - see this tutorial from Sun.  JFrame setUndecorated(true) method  Try the Full-Screen Exclusive Mode API. It was introduced in the JDK in release 1.4. Some of the features include: Full-Screen Exclusive Mode - allows you to suspend the windowing system so that drawing can be done directly to the screen. Display Mode - composed of the size (width and height of the monitor in pixels) bit depth (number of bits per pixel) and refresh rate (how frequently the monitor updates itself). Passive vs. Active Rendering - painting while on the main event loop using the paint method is passive whereas rendering in your own thread is active. Double Buffering and Page Flipping - Smoother drawing means better perceived performance and a much better user experience. BufferStrategy and BufferCapabilities - classes that allow you to draw to surfaces and components without having to know the number of buffers used or the technique used to display them and help you determine the capabilities of your graphics device. There are several full-screen exclusive mode examples in the linked tutorial. Perfect. Thanks. thanks for the really good explained answer.. :) a vote up for it...  I've done this using JOGL when having a full screen OpenGL user interface for a game. It's quite easy. I believe that the capability was added to Java with version 5 as well but it's so long ago that I've forgotten how to do it (edit: see answer above for how).
726,A,"WPF / XAML - Can text be auto sized? For a fixed size wrappable text area is there any way to make the font size as large as possible based on the amount of text? For example if you have a 500x500 area with the text ""Hello"" the font size would be really big. But if you have a paragraph of text the font size would be smaller to fit into the area. I have looked at Viewbox but can't see that it could work with wrappable text. ANY xaml or code that could do this would help (doesn't have to be a specific control). What you're asking is more complex than it sounds but I'll give you an idea: <DockPanel x:Name=""LayoutRoot""> <TextBox x:Name=""text"" Text=""this is some text and some more text I don't see any problems..."" DockPanel.Dock=""Top"" TextChanged=""text_TextChanged""/> <TextBlock DockPanel.Dock=""Top"" Text=""{Binding ElementName=tb Path=FontSize}""/> <Border Name=""bd"" BorderBrush=""Black"" BorderThickness=""1""> <TextBlock Name=""tb"" Text=""{Binding ElementName=text Path=Text}"" TextWrapping=""Wrap""/> </Border> </DockPanel> And in code behind: public MainWindow() { this.InitializeComponent(); RecalcFontSize(); tb.SizeChanged += new SizeChangedEventHandler(tb_SizeChanged); } void tb_SizeChanged(object sender SizeChangedEventArgs e) { RecalcFontSize(); } private void RecalcFontSize() { if (tb == null) return; Size constraint = new Size(tb.ActualWidth tb.ActualHeight); tb.Measure(constraint); while (tb.DesiredSize.Height < tb.ActualHeight) { tb.FontSize += 1; tb.Measure(constraint); } tb.FontSize -= 1; } private void text_TextChanged(object sender TextChangedEventArgs e) { RecalcFontSize(); } Try it drag it around change the text... With this last revision it seems to work perfectly for all cases - Thanks. It looks like working with the height only was the key. I'm now going to work on a custom control so it can easily be added to Xaml without changing the Window code behind. I have marked your answer correct. I'll be interested in getting that control when you're done with it if there's no commercial considerations that is :) It works for (200200) unfortunately it does not work for (440330). If you try that constraint you'll notice the bottom line is clipped off. I have a screen shot and sample app if that helps. Any ideas? I'll reproduce it and update the post. Try now. Mind you this is just a lead there's probably a better way to reach the optimal font size without incrementing it 1 by 1... Not at all - I'll try to get it published with binary and source code to share for anyone who might find it use full - Still doesn't work - add this text to the sample above ""problematic stuff"". When narrowing the window the word ""problematic"" breaks into separate letters rather than downsizing the text. WPF cannot be tamed!"
727,A,"How do you resize a Bitmap under .NET CF 2.0 I have a Bitmap that I want to enlarge programatically to ~1.5x or 2x to its original size. Is there an easy way to do that under .NET CF 2.0? One ""normal"" way would be to create a new Bitmap of the desired size create a Graphics for it and then draw the old image onto it with Graphics.DrawImage(Point Rectangle). Are any of those calls not available on the Compact Framework? EDIT: Here's a short but complete app which works on the desktop: using System; using System.Drawing; class Test { static void Main() { using (Image original = Image.FromFile(""original.jpg"")) using (Bitmap bigger = new Bitmap(original.Width * 2 original.Height * 2 original.PixelFormat)) using (Graphics g = Graphics.FromImage(bigger)) { g.DrawImage(original new Rectangle(Point.Empty bigger.Size)); bigger.Save(""bigger.jpg""); } } } Even though this works there may well be better ways of doing it in terms of interpolation etc. If it works on the Compact Framework it would at least give you a starting point. Beat me to it because I looked up if they were available on the Compact Framework =) I tried but MSDN wasn't terribly clear on the matter... at least not the offline version I was using. I wonder what InterpolationMode it uses when you just construct a Bitmap like that and ask it to scale it. Again MSDN isn't clear... @Mr. Lame - thanks will remove that option :) Works but requires to fix few errors concerning CF compatibility: 1) Bitmap.PixelFormat property does not exist 2) Rectangle has only the constructor Rectangle(xywidthheight) 3) g.DrawImage does not have the simple override. You need to call this: g.DrawImage(original new Rectangle(0 0 bigger.Width bigger.Height) new Rectangle(0 0 original.Width original.Height) GraphicsUnit.Pixel);  The CF has access to the standard Graphics and Bitmap objects like the full framework. Get the original image into a Bitmap Create a new Bitmap of the desired size Associate a Graphics object with the NEW Bitmap Call g.DrawImage() with the old image and the overload to specify width/height Dispose of things Versions: .NET Compact Framework Supported in: 3.5 2.0 1.0"
728,A,Rendering different triangle types and triangle fans using vertex buffer objects? (OpenGL) About half of my meshes are using triangles another half using triangle fans. I'd like to offload these into a vertex buffer object but I'm not quite sure how to do this. The triangle fans all have a different number of vertices... for example one might have 5 and another 7. VBO's are fairly straight forward using plain triangles however I'm not sure how to use them with triangle fans or with different triangle types. I'm fairly sure I need an index buffer but I'm not quite sure what I need to do this. I know how many vertices make up each fan during run time... I'm thinking I can use that to call something like glArrayElement Any help here would be much appreciated! VBOs and index buffers are an orthogonal things. If you're not using index buffers yet maybe it is wiser to move one step at a time. So... regarding your question. If you put all your triangle fans in a vbo the only thing you need to draw them is to setup your vbo and pass the index in it for your fan start  glBindBuffer(GL_VERTEX_BUFFER buffer); glVertexPointer(3 GL_FLOAT 0 NULL); // 3 floats per vertex for each i in fans glDrawArrays(GL_TRIANGLE_FAN indef_of_first_vertex_for_fan[i] fan_vertex_count[i]) Edit: I'd have to say that you're probably better off transforming your fans to a regular triangle set and use glDrawArrays(GL_TRIANGLES) for all your triangles. A call per primitive is rarely efficient. Well if you have 200K triangle fans then you have that many calls **per frame**. Think about the cost compared to some load-time (or better off-line process time) that's required to transform the fans to triangles. sun proposed an extension because this was too much of an overhead *10 years ago*. The overhead/intersting work ratio only increased since. http://www.opengl.org/registry/specs/SUN/triangle_list.txt To be complete I should add that [primitive restart](http://www.opengl.org/registry/specs/NV/primitive_restart.txt) is an extension that you could be using as well (but requires indexing of your primitives which you should look at anyways). [glMultiDrawArrays](http://www.opengl.org/sdk/docs/man/xhtml/glMultiDrawArrays.xml) is yet another attempt at reducing the overhead but is rarely actually optimized in the drivers. Bahbar Thanks for the tip! I had read about transforming the fans into triangles... However I might have 200000 triangle fan primitives. Granted I'd only need to do this once so in your opinion is it cheaper to transform them and then do glDrawArrays or should I just do the call for each prim? Thanks Good stuff thanks man. It is so hard to find good examples/documentation of this kind of stuff
729,A,"Slow C++ DirectX 2D Game I'm new to C++ and DirectX I come from XNA. I have developed a game like Fly The Copter. What i've done is created a class named Wall. While the game is running I draw all the walls. In XNA I stored the walls in a ArrayList and in C++ I've used vector. In XNA the game just runs fast and in C++ really slow. Here's the C++ code: void GameScreen::Update() { //Update Walls int len = walls.size(); for(int i = wallsPassed; i < len; i++) { walls.at(i).Update(); if (walls.at(i).pos.x <= -40) wallsPassed += 2; } } void GameScreen::Draw() { //Draw Walls int len = walls.size(); for(int i = wallsPassed; i < len; i++) { if (walls.at(i).pos.x < 1280) walls.at(i).Draw(); else break; } } In the Update method I decrease the X value by 4. In the Draw method I call sprite->Draw (Direct3DXSprite). That the only codes that runs in the game loop. I know this is a bad code if you have an idea to improve it please help. Thanks and sorry about my english. it might be useful to know how much slower. we talking half speed or orders of magnitude? The code here looks reasonably smart. The problem most likely lies in the rendering pipeline and optimization flags. I would say that you didn't turn-on optimization flags. The lookups into your list of walls are unlikely to be the source of your slowdown. The cost of drawing objects in 3D will typically be the limiting factor. The important parts are your draw code the flags you used to create the DirectX device and the flags you use to create your textures. My stab in the dark... check that you initialize the device as HAL (hardware 3d) rather than REF (software 3d). Also how many sprites are you drawing? Each draw call has a fair amount of overhead. If you make more than couple-hundred per frame that will be your limiting factor.  You can aid batching of sprite draw calls. Presumably Your draw call calls your only instance of ID3DXSprite::Draw with the relevant parameters. You can get much improved performance by doing a call to ID3DXSprite::Begin (with the D3DXSPRITE_SORT_TEXTURE flag set) and then calling ID3DXSprite::End when you've done all your rendering. ID3DXSprite will then sort all your sprite calls by texture to decrease the number of texture switches and batch the relevant calls together. This will improve performance massively. Its difficult to say more however without seeing the internals of your Update and Draw calls. The above is only a guess ...  To draw every single wall with a different draw call is a bad idea. Try to batch the data into a single vertex buffer/index buffer and send them into a single draw. That's a more sane idea. Anyway for getting an idea of WHY it goes slowly try with some CPU and GPU (PerfHud Intel GPA etc...) to know first of all WHAT's the bottleneck (if the CPU or the GPU). And then you can fight to alleviate the problem.  How fast is 'fast'? How slow is'really slow'? How many sprites are you drawing? How big is each one as an image file and in pixels drawn on-screen? How does performance scale (in XNA/C++) as you change the number of sprites drawn? What difference do you get if you draw without updating or vice versa  Maybe you just have forgotten to turn on release mode :) I had some problems with it in the past - I thought my code was very slow because of debug mode. If it's not it you can have a problem with rendering part or with huge count of objects. The code you provided looks good...  Try to narrow the cause of the performance problem (also termed profiling). I would try drawing only one object while continue updating all the objects. If its suddenly faster then its a DirectX drawing problem. Otherwise try drawing all the objects but updating only one wall. If its faster then your update() function may be too expensive.  Have you tried multiple buffers (a.k.a. Double Buffering) for the bitmaps? The typical scenario is to draw in one buffer then while the first buffer is copied to the screen draw in a second buffer. Another technique is to have a huge ""logical"" screen in memory. The portion draw in the physical display is a viewport or view into a small area in the logical screen. Moving the background (or screen) just requires a copy on the part of the graphics processor.  Try replacing all occurrences of at() with the [] operator. For example:  walls[i].Draw(); and then turn on all optimisations. Both [] and at() are function calls - to get the maximum performance you need to make sure that they are inlined which is what upping the optimisation level will do. You can also do some minimal caching of a wall object - for example:  for(int i = wallsPassed; i < len; i++) { Wall & w = walls[i]; w.Update(); if (w.pos.x <= -40) wallsPassed += 2; } @Neil Butterworth my compiler is Visual C++ @Adir Sorry I don't use VC++ any more - but optimisation control should be available via your project options dialog(s). It should be noted that `[]` in XNA (.net) will do the same bounds checking as `.at()` so I highly doubt that's the issue... I've seen inlining get a 3x speed increase on vector access you will need it. Replacing at to [] changed dramatically the performance Thanks! BTW what are the optimization flags? Thanks again :) @Adir Depends on your compiler - for GCC use -02 for starters. If your 3d app changed speed measurably after making this change then something else must have changed at the same time. I guarantee this was not the limiting factor in your code. What's the main difference between `[]` and `at()`? EDIT: Oh just found the answer: `at()` checks the boundaries. There is no real difference the [] operated is overloaded so its just shorthand. Additionally I downvoted because this kind of optimization () vs [] is trivial. There is a difference - at() does range checking which takes time and is pointless if you don't need it. And the effect of inlining operator[] (or at() come to that) can be very marked. aramadia: as dreamlax correctly added `at()` is checked and `[]` isn't. Hence the optimisation may be far from trivial since you're avoiding a conditional on every array access. Upvoting. But the effect of one or two inlined `[]` operations is going to be negligible compared to the actual drawing. My bad then. However I still maintain its a negligible performance hit. Ill remove the downvote. How did this stupid micro-optimisation tip get picked as the best answer? If you think your 3D card is going to be stalled from rendering millions of pixels while the CPU does a bound-checked array-lookup you're mad. By your logic any Java/C# app would be slow. I'd love if we could get the accepted answer on a negative vote-count ;)"
730,A,"Dynamic updates of objects in a Silverlight application? Maybe I am just over thinking this and need to write some more prototype code but I wanted to get some of your thoughts: Basically the application is going to be displaying up to several hundred custom visual objects in an X/Y ""grid"". The position and attributes of some of these objects may change between updates; the application will periodically retrieve updates via Web Service calls made approximately every minute. My question is what is the best way to process these updates on the client? I have considered either looking up each object and updating it directly or constructing a ""tree"" of data objects that will be updated in a background thread - once all the updates have been applied in the background the displayed objects would be recreated based upon data in the ""tree"". Can anyone offer any suggestions one way or the other? Are there any Silverlight design patterns that may be a good fit? I would use a data layer and bind to the properties in the XAML--ala m-v-c model-- and base your objects from INotifyPropertyChanged then the UI will update 'em automatically when your data updates. Just make sure to compare the new value to the existing value int SomeNumber { get { return this.m_someInt; } set { if ( value != this.m_someInt ) { this.m_someInt = value; NotifyChange(""SomeNumber""); } } NotifyChange( string propName ) { ..... } Thanks for the tip I am going to go look into this..."
731,A,Drawing a dot grid I'm new to graphics programming. I'm trying to create a program that allows you to draw directed graphs. For a start I have managed to draw a set of rectangles (representing the nodes) and have made pan and zoom capabilities by overriding the paint method in Java. This all seems to work reasonably well while there aren't too many nodes. My problem is when it comes to trying to draw a dot grid. I used a simple bit of test code at first that overlayed a dot grid using two nested for loops: int iPanX = (int) panX; int iPanY = (int) panY; int a = this.figure.getWidth() - iPanX; int b = this.figure.getHeight() - (int) iPanY; for (int i = -iPanX; i < a; i += 10) { for (int j = -iPanY; j < b; j += 10) { g.drawLine(i j i j); } } This allows me to pan the grid but not zoom. However the performance when panning is terrible! I've done a lot of searching but I feel that I must be missing something obvious because I can't find anything on the subject. Any help or pointers would be greatly appreciated. --Stephen I think re-drawing all your dots every time the mouse moves is going to give you performance problems. Perhaps you should look into taking a snapshot of the view as a bitmap and panning that around redrawing the view 'properly' when the user releases the mouse button?  Use a BufferedImage for the dot grid. Initialize it once and later only paint the image instead of drawing the grid over and over. private init(){ BufferedImage image = new BufferedImage(width height BufferedImage.TYPE_INT_ARGB); Graphics g = image.getGraphics(); // then draw your grid into g } public void paint(Graphics g) { g.drawImage(image 0 0 null); // then draw the graphs } And zooming is easily achieved using this: g.drawImage(image 0 0 null); // so you paint the grid at a 1:1 resolution Graphics2D g2 = (Graphics2D) g; g2.scale(zoom zoom); // then draw the rest into g2 instead of g Drawing into the zoomed Graphics will lead to proportionally larger line width etc. Thanks performance has increased massively! There are a few quirks that I'll have to iron out but overall it is much improved. The solution you offer for zooming isn't ideal as this increases the size of the grid which I would prefer to keep as single pixels. Do I update the grid img on a zoom to accomodate? yeah that occurred to me afterwards. It depends on how often you expect the zoom to change. I think you should update the grid image with every zoom change. And in paint(g) you could draw the grid at a 1:1 zoom and then set the scaling factor for the rest of the objects to paint. I think i'm going to update that in my answer.
732,A,Double buffering in BlackBerry? Do we need to do double buffering in BlackBerry while rendering our paint code or does BlackBerry handle that? If we need to do that ourselves how do we do that? Just like Richard said Although there is a need to perform explicit double buffering in Mobile Information Device Profile (MIDP) applications the BlackBerry user interface (UI) implementation is inherently double-buffered. This means there is no need to perform your own double buffering of UI images. Essentially all drawing functions in net.rim.device.api.ui.Graphics objects are drawn to an off-screen bitmap by the system. When you call for a repaint of the screen it draws this off-screen bitmap instead of sequentially painting the screen. This same drawing paradigm is true for all fields and field managers within the Blackberry UI. BlackBerry KB - How do I perform double buffering using the BlackBerry UI?
733,A,"Dynamically set the height of a graphic in SQL Server 2008 Reporting Services HI I have placed a graphic inside my report in SQL Server Reporting Services 2008. My only problem is that the natural behavior of the graphic is to have fixed width and heights no matter how many items you have inside your graphic. Sometimes I have as few as 5 to 8 items in my Y axis but sometimes I have some 20 or 25. What happens is that because of the fixed height the graphics with few items have too much vertical space and large bars and the ones with many items are automatically compressed into a space that is small for them. So what I need is to dinamically set the height of the graphic based on how many items are in the Y axis of the graphic. The X axis does not expand so I don`t need to set it dynamically. How would you set the height of the graphic dynamically? I probably have to set a function for the graphic height... Any help will be appreciated. Thanks. by graphic do you mean chart? Assuming you mean chart then the function for chart height would look something like this: =countdistinct(fields!yValue.value ""Dataset"")*HeightOfOneYvalue+HeightofXAxis I have done this from memory so it may not be exactly correct. Hey thank you I will try this and some variations in case it does not work exactly like this. Thank you! Hi I am sorry to insist but... Do you have an example to show me? Yes I mean a chart (better than saying a ""graphic""). First doubt is... where do I set the height of the chart. Aparently there is no such setting. I was trying to do that in the Axis properties >> set axis scale and properties >> Maximum. But I don`t think that is the place to change the chart height... I appreciate any help. Thank you very much. You need to use the DynamicHeight property on the chart. It will override the height property once set. It is a string so you will need to put & "" cm"" or & "" in"". Hey thank you ""DynamicHeight"" is the keyword to the answer. I hadn`t realized I could add expressions for each chart parameter and set them dynamically. In case anybody needs here is a link to a complete explanation: http://blogs.msdn.com/robertbruckner/archive/2008/10/27/charts-with-dynamic-size-based-on-categories-or-data.aspx"
734,A,how to force Java print graphics in 300dpi as I googled for problem somehow java printing API is crippled with limitation that all pictures sent to printer must be printed in 72dpi resolution. We are using jasper report to print documents and no matter how big barcode we draw barcode reader won't scan it.. any similar experiences? How to solve this issue? We are using jasper reports as well in conjunction with barcode4j (instead of the included barbecue). We are not experiencing the problems you have. Maybe you could attach a scanned image of the printer output. Is the generated PDF crappy as well? What is the exported format? I do not handle all the details right now; we are not generating PDF (although it is also posible) - from JasperViewer generated page is being sent to laser printer directly. We need 2D barcode (PDF-417) but our scanner would not scan normal EAN13 barcode as well as before mentioned. You need to specify the printer resolution by means of attribute PrinterResolution. You should also know the source resolution so it is correctly converted such as: PrinterResolution pr = new PrinterResolution(300 300 PrinterResolution.DPI);
735,A,"Are Bitmap.LockBits and Graphics.FromImage combinable in C# Can you combine the methods of Bitmap.LockBits and Graphics.FromImage or in other words if I have a bitmap ""bmp"" and I want to edit the bitmap with a Graphics-object g are the changes visible in the byte-array of the BitmapData.Scan0: Bitmap bmp = new Bitmap(200200); Graphics g = Graphics.FromImage(bmp); bmp.LockBits(new Rectangle(00200200) ImageLockMode.ReadOnlyPixelFormat.Format32bppArgb); byte* pixelData = (byte*) (void*) bmd.Scan0; g.FillRectangle(Brushes.Rednew Rectangle(005050)); can I see the changes in PixelData after I filled a red rectangle? Yes should be able to combine operations if the operations don't use the same type of locking meaning that you should pass a compatible ImageLockMode parameter to your LockBits method."
736,A,"Automatic tracking algorithm I'm trying to write a simple tracking routine to track some points on a movie. Essentially I have a series of 100-frames-long movies showing some bright spots on dark background. I have ~100-150 spots per frame and they move over the course of the movie. I would like to track them so I'm looking for some efficient (but possibly not overkilling to implement) routine to do that. A few more infos: the spots are a few (es. 5x5) pixels in size the movement are not big. A spot generally does not move more than 5-10 pixels from its original position. The movements are generally smooth. the ""shape"" of these spots is generally fixed they don't grow or shrink BUT they become less bright as the movie progresses. the spots don't move in a particular direction. They can move right and then left and then right again the user will select a region around each spot and then this region will be tracked so I do not need to automatically find the points. As the videos are b/w I though I should rely on brigthness. For instance I thought I could move around the region and calculate the correlation of the region's area in the previous frame with that in the various positions in the next frame. I understand that this is a quite naïve solution but do you think it may work? Does anyone know specific algorithms that do this? It doesn't need to be superfast as long as it is accurate I'm happy. Thank you nico Are the shapes unique? If not you may have issues with accuracy especially if the shapes are within 10 pixels of one another. @Anon: you make a very good point. I guess you can say the spots are all different from each other but on very small things like that you understand that the differences cannot be so huge. I already have a ""manual tracking"" routine that allows the user to manually move the region over time so in the case of these *difficult* regions the user may have to do it by hand that is not a problem. I just don't want to have to manually track all the 150 regions for each image... I would suggest the Pearson's product. Having a model (which could be any template image) you can measure the correlation of the template with any section of the frame. The result is a probability factor which determine the correlation of the samples with the template one. It is especially applicable to 2D cases. It has the advantage to be independent from the sample absolute value since the result is dependent on the covariance related with the mean of the samples. Once you detect an high probability you can track the successive frames in the neightboor of the original position and select the best correlation factor. However the size and the rotation of the template matter but this is not the case as I can understand. You can customize the detection with any shape since the template image could represent any configuration. Here is a single pass algorithm implementation  that I've used and works correctly. Thanks everyone for the answers they were all useful. At the end I used this solution so I chose this as the accepted answer. I still have a little bit of tweaking to do but overall it seems to work fairly well (still some manual adjustment is required but that's OK). Thank you Luca. I will try to implement something like this. I'll let you know if it works  Sounds like a job for Blob detection to me. +1. Sounds like it :-) +1 Agreed. It sound like an adaptive mirror control project I worked on. I found the original positions of the 'dots' with blob analysis. Then to operate at 1000/frames per second I had a custom algorithm to look for small changes in position of the dots based on a simple 'centroid' of the mass of 'white'.  Simple is good. I'd start doing something like:  1) over a small rectangle that surrounds a spot: 2) apply a weighted average of all the pixel coordinates in the area 3) call the averaged X and Y values the objects position 4) while scanning these pixels do something to approximate the bounding box size 5) repeat next frame with a slightly enlarged bounding box so you don't clip spot that moves The weight for the average should go to zero for pixels below some threshold. Number 4 can be as simple as tracking the min/max position of anything brighter than the same threshold. This will of course have issues with spots that overlap or cross paths. But for some reason I keep thinking you're tracking stars with some unknown camera motion in which case this should be fine. I'm tracking fluorescence microscopy images but your idea applies anyway!  Predator (TLD Algorithm) by Zdenek Kalal is also good for Automatic Tracking See this: http://www.youtube.com/watch?v=1GhNXHCQGsM and http://bit.ly/shahab-vision  This has got to be a well reasearched topic and I suspect there won't be any 100% accurate solution. Some links which might be of use: Learning patterns of activity using real-time tracking. A paper by two guys from MIT. Kalman Filter. Especially the Computer Vision part. Motion Tracker. A student project which also has code and sample videos I believe. Of course this might be overkill for you but hope it helps giving you other leads. Yes you're right it is very well researched maybe even too much as I could find tons of papers about it. The problem is that too much information can often be only confusing if you don't know where to look and where to start. The links you provided seem to be an interesting starting point I think I will have something to read over the weekend... The Kalman filter in particular seems like a good thing to use.  I'm afraid that blob tracking is not simple not if you want to do it well. Start with blob detection as genpfault says. Now you have spots on every frame and you need to link them up. If the blobs are moving independently you can use some sort of correspondence algorithm to link them up. See for instance http://server.cs.ucf.edu/~vision/papers/01359751.pdf. Now you may have collisions. You can use mixture of gaussians to try to separate them give up and let the tracks cross use any other before-and-after information to resolve the collisions (e.g. if A and B collide and A is brighter before and will be brighter after you can keep track of A; if A and B move along predictable trajectories you can use that also). Or you can collaborate with a lab that does this sort of stuff all the time."
737,A,"Center text output from Graphics.DrawString() I'm using the .NETCF (Windows Mobile) Graphics class and the DrawString() method to render a single character to the screen. The problem is that I can't seem to get it centered properly. No matter what I set for the Y coordinate of the location of the string render it always comes out lower than that and the larger the text size the greater the Y offset. For example at text size 12 the offset is about 4 but at 32 the offset is about 10. I want the character to vertically take up most of the rectangle it's being drawn in and be centered horizontally. Here's my basic code. this is referencing the user control it's being drawn in. Graphics g = this.CreateGraphics(); float padx = ((float)this.Size.Width) * (0.05F); float pady = ((float)this.Size.Height) * (0.05F); float width = ((float)this.Size.Width) - 2 * padx; float height = ((float)this.Size.Height) - 2 * pady; float emSize = height; g.DrawString(letter new Font(FontFamily.GenericSansSerif emSize FontStyle.Regular) new SolidBrush(Color.Black) padx pady); Yes I know there is the label control that I could use instead and set the centering with that but I actually do need to do this manually with the Graphics class. I'd like to add another vote for the StringFormat object. You can use this simply to specify ""centre centre"" and the text will be drawn centrally in the rectangle or points provided: StringFormat format = new StringFormat(); format.LineAlignment = StringAlignment.Center; format.Alignment = StringAlignment.Center; However there is one issue with this in CF. If you use Center for both values then it turns TextWrapping off. No idea why this happens it appears to be a bug with the CF.  Here's some code. This assumes you are doing this on a form or a UserControl. Graphics g = this.CreateGraphics(); SizeF size = g.MeasureString(""string to measure""); int nLeft = Convert.ToInt32((this.ClientRectangle.Width / 2) - (size.Width / 2)); int nTop = Convert.ToInt32((this.ClientRectangle.Height / 2) - (size.Height / 2)); From your post it sounds like the ClientRectangle part (as in you're not using it) is what's giving you difficulty.  You can use an instance of the string formatter object passed in to the drawstring method to centre the text. see http://msdn.microsoft.com/en-us/library/21kdfbzs.aspx and http://msdn.microsoft.com/en-us/library/system.drawing.stringformat.aspx  To draw a centered text: TextRenderer.DrawText(g ""my text"" Font Bounds ForeColor BackColor TextFormatFlags.HorizontalCenter | TextFormatFlags.VerticalCenter | TextFormatFlags.GlyphOverhangPadding); Determining optimal font size to fill an area is a bit more difficult. One working soultion I found is trial-and-error: start with a big font then repeatedly measure the string and shrink the font until it fits. Font FindBestFitFont(Graphics g String text Font font Size proposedSize TextFormatFlags flags) { // Compute actual size shrink if needed while (true) { Size size = TextRenderer.MeasureText(g text font proposedSize flags); // It fits back out if ( size.Height <= proposedSize.Height && size.Width <= proposedSize.Width) { return font; } // Try a smaller font (90% of old size) Font oldFont = font; font = new Font(font.FontFamily (float)(font.Size * .9)); oldFont.Dispose(); } } You'd use this as: Font bestFitFont = FindBestFitFont(g text someBigFont sizeToFitIn flags); // Then do your drawing using the bestFitFont // Don't forget to dispose the font (if/when needed)  To align a text use the following: StringFormat sf = new StringFormat(); sf.LineAlignment = StringAlignment.Center; sf.Alignment = StringAlignment.Center; e.Graphics.DrawString(""My String"" this.Font Brushes.Black ClientRectangle sf); Please note that the text here is aligned in the given bounds. In this sample this is the ClientRectangle.  Through a combination of the suggestions I got I came up with this:  private void DrawLetter() { Graphics g = this.CreateGraphics(); float width = ((float)this.ClientRectangle.Width); float height = ((float)this.ClientRectangle.Width); float emSize = height; Font font = new Font(FontFamily.GenericSansSerif emSize FontStyle.Regular); font = FindBestFitFont(g letter.ToString() font this.ClientRectangle.Size); SizeF size = g.MeasureString(letter.ToString() font); g.DrawString(letter font new SolidBrush(Color.Black) (width-size.Width)/2 0); } private Font FindBestFitFont(Graphics g String text Font font Size proposedSize) { // Compute actual size shrink if needed while (true) { SizeF size = g.MeasureString(text font); // It fits back out if (size.Height <= proposedSize.Height && size.Width <= proposedSize.Width) { return font; } // Try a smaller font (90% of old size) Font oldFont = font; font = new Font(font.Name (float)(font.Size * .9) font.Style); oldFont.Dispose(); } } So far this works flawlessly. The only thing I would change is to move the FindBestFitFont() call to the OnResize() event so that I'm not calling it every time I draw a letter. It only needs to be called when the control size changes. I just included it in the function for completeness."
738,A,"Mobile phone application design guidelines Any documents or articles that contain design guidelines for creating mobile phone applications? How applications should be built for ideal user experiences. User Interface & Keypad Graphics Languages Links from specific manufacturers are also welcome. How about guidelines based on personal experience? Help the user avoid laborious text-entry (this applies to all phones except my beloved Samsung i760 and all other phones with a full physical keyboard) Keep the screens as sparse and simple as possible - avoid the urge to cram as many controls and as much text and graphics onto a form as will fit Don't write applications like ""Shake the Baby"" Remember to test your application outside in full sunlight - mobile apps are a lot like vampires Avoid device- and manufacturer-specific code whenever possible +1 for Avoid device- and manufacturer-specific code whenever possible. :)  Some application design wisdom: behaviour is consistent or predictable feedback is provided the user's memory is not strained dialogue is task-oriented  Microsoft's design guidelines for Windows Mobile applications is fairly okay and makes sense for most mobile applications in general: Design Guidelines  high contrast (sunlight issue that MusiGenesis mentioned) I tend to prefer 100% widths on everything to accommodate for multiple widths For larger projects I'll incorporate WURFL to at least get the device's screen dimensions and adjust necessary pieces like font size and or fixed-width graphics"
739,A,Rotating coordinates around an axis I'm representing a shape as a set of coordinates in 3D I'm trying to rotate the whole object around an axis (In this case the Z axis but I'd like to rotate around all three once I get it working). I've written some code to do this using a rotation matrix:  //Coord is a 3D vector of floats //pos is a coordinate //angles is a 3d vector each component is the angle of rotation around the component axis //in radians Coord<float> Polymers::rotateByMatrix(Coord<float> pos const Coord<float> &angles) { float xrot = angles[0]; float yrot = angles[1]; float zrot = angles[2]; //z axis rotation pos[0] = (cosf(zrot) * pos[0] - (sinf(zrot) * pos[1])); pos[1] = (sinf(zrot) * pos[0] + cosf(zrot) * pos[1]); return pos; } The image below shows the object I'm trying to rotate (looking down the Z axis) before the rotation is attempted each small sphere indicates one of the coordinates I'm trying to rotate The rotation is performed for the object by the following code:  //loop over each coordinate in the object for (int k=start; k<finish; ++k) { Coord<float> pos = mp[k-start]; //move object away from origin to test rotation around origin pos += Coord<float>(5.05.05.0); pos = rotateByMatrix(pos rots); //wrap particle position //these bits of code just wrap the coordinates around if the are //outside of the volume and write the results to the positions //array and so shouldn't affect the rotation. for (int l=0; l<3; ++l) { //wrap to ensure torroidal space if (pos[l] < origin[l]) pos[l] += dims[l]; if (pos[l] >= (origin[l] + dims[l])) pos[l] -= dims[l]; parts->m_hPos[k * 4 + l] = pos[l]; } } The problem is that when I perform the rotation in this way with the angles parameter set to (0.00.01.0) it works (sort of) but the object gets deformed like so: which is not what I want. Can anyone tell me what I'm doing wrong and how I can rotate the entire object around the axis without deforming it? Thanks nodlams Your rotation code looks right. Try it without your wrap code? I just tried it without the wrap code unfortunately it didn't make a difference. I am not sure..but in the second statement in the rotate function (i.e. pos[1]= ...) you are using the updated value of pos[0]. Is that intended? Where you do your rotation in rotateByMatrix you compute the new pos[0] but then feed that into the next line for computing the new pos[1]. So the pos[0] you're using to compute the new pos[1] is not the input but the output. Store the result in a temp var and return that. Coord<float> tmp; tmp[0] = (cosf(zrot) * pos[0] - (sinf(zrot) * pos[1])); tmp[1] = (sinf(zrot) * pos[0] + cosf(zrot) * pos[1]); return tmp; Also pass the pos into the function as a const reference. const Coord<float> &pos Plus you should compute the sin and cos values once store them in temporaries and reuse them. Ah yeh that works I new I must have made some stupid mistake somewhere. Thanks for your help!
740,A,ActionScript lineStyle Thickness to fill a circle I'm trying to build a circle using lines. Each line starts in the center of the circle and is as long as the circle's radius. Using a loop along with sine and cosign waves I can build the circle using the sine and cosign to mark the coordinates of the lineTo parameter. My problem is with the line thickness parameter of lineStyle. I would like the ends of the lines to match up perfectly no matter how big the circumference of the circle but i can't figure out a proper method for the line thickness. //this is what makes sense to me but it still creates some gaps lineThickness = 1 + (((nRadius * 2) * Math.PI) - 360) / 359; for(var i:int = 0; i < 360; i++) { // Convert the degree to radians. nRadians = i * (Math.PI / 180); // Calculate the coordinate in which the line should be drawn to. nX = nRadius * Math.cos(nRadians); nY = nRadius * Math.sin(nRadians); // Create and drawn the line. graphics.lineStyle(lineThickness 0 1 false LineScaleMode.NORMAL CapsStyle.NONE); graphics.moveTo(0 0); graphics.lineTo(nX nY); } To make the ends of the lines meet up at the circles circumference without any gaps I need to widen the lines to fill in the space that's remaining. What makes sense to me but doesn't work is to subtract the 360 from the circumference then divide that number by the amount of empty slots between the lines (which is 359) and adding that number the the thickness of 1. What's concerning me is that the lineStyle thickness parameter is a Number but seems to take only values between 0 and 255 so I'm not sure if a floating point number like 1.354 is a valid thickness. i think i understand what you are trying to achieve but i don't really understand *why*? (i'm just curious) it's so i can access the colour value of each line I'd suggest drawing them as wedges instead of lines copy and paste this into a new FLA to see what I mean: var nRadians : Number; var nRadius : Number = 100; var nX : Number; var nY : Number; var previousX : Number = nRadius; var previousY : Number = 0; //this is what makes sense to me but it still creates some gaps var lineThickness : Number = 1 + ( ( ( nRadius * 2 ) * Math.PI ) - 360 ) / 359; for( var i : int = 0; i < 360; i++ ) { // Convert the degree to radians. nRadians = i * ( Math.PI / 180 ); // Calculate the coordinate in which the line should be drawn to. nX = nRadius * Math.cos( nRadians ); nY = nRadius * Math.sin( nRadians ); // Create and drawn the line. graphics.beginFill( Math.random() * 0xFFFFFF ); graphics.moveTo( 0 0 ); graphics.lineTo( previousX previousY ); graphics.lineTo( nX nY ); graphics.lineTo( 0 0 ); graphics.endFill(); previousX = nX; previousY = nY; }
741,A,"C# Windows form Control to Image? I am trying to create an image of a Panel and save it to a folder. The problem is the panel has scrollbars and the image generated is only for the visible portion of the Panel. The code I use is something like Panel.DrawToImage. Could there be any help out here to save the entire Panel as a picture and not just the visible portion? Perhaps you could clone the control to another (hidden) form at the desired size (such that the scroll bars aren't displayed) and call your DrawToImage() function that way?  Consider temporarily increasing the panel's size calling p.Layout() then p.DrawToImage() The Control is bigger than the display size of the screen anyways. So the scrollbars are just impossible to get rid of!!  I think the WM_PRINT message is going to be helpful. Here's a sample that almost works. (It still prints the scrollbars themselves and the background is lost on the ""out of scroll"" portions.) Perhaps you can play with this and get it working or someone with more WinForms experience can take this to the next level? Declare the following in your class: [DllImport(""user32.dll"")] public static extern int SendMessage(IntPtr hWnd int Msg int wParam int lParam); private const int WM_PRINT = 791; /// <summary> /// The WM_PRINT drawing options /// </summary> [Flags] private enum DrawingOptions { /// <summary> /// Draws the window only if it is visible. /// </summary> PRF_CHECKVISIBLE = 1 /// <summary> /// Draws the nonclient area of the window. /// </summary> PRF_NONCLIENT = 2 /// <summary> /// Draws the client area of the window. /// </summary> PRF_CLIENT = 4 /// <summary> /// Erases the background before drawing the window. /// </summary> PRF_ERASEBKGND = 8 /// <summary> /// Draws all visible children windows. /// </summary> PRF_CHILDREN = 16 /// <summary> /// Draws all owned windows. /// </summary> PRF_OWNED = 32 } Then to paint the control to a bitmap: using (Bitmap screenshot = new Bitmap(this.panel1.DisplayRectangle.Width this.panel1.DisplayRectangle.Height)) using (Graphics g = Graphics.FromImage(screenshot)) { try { SendMessage(this.panel1.Handle WM_PRINT g.GetHdc().ToInt32() (int)(DrawingOptions.PRF_CHILDREN | DrawingOptions.PRF_CLIENT | DrawingOptions.PRF_NONCLIENT | DrawingOptions.PRF_OWNED)); } finally { g.ReleaseHdc(); } screenshot.Save(""temp.bmp""); } EDIT: Here's an alternate paint strategy that might get you what you're looking for. I'm making some assumptions but perhaps it will work. It paints a dummy background on the Bitmap first and also removes the scrollbars: using (Bitmap screenshot = new Bitmap(this.panel1.DisplayRectangle.Width this.panel1.DisplayRectangle.Height)) using (Graphics g = Graphics.FromImage(screenshot)) { g.FillRectangle(SystemBrushes.Control 0 0 screenshot.Width screenshot.Height); try { SendMessage(this.panel1.Handle WM_PRINT g.GetHdc().ToInt32() (int)(DrawingOptions.PRF_CHILDREN | DrawingOptions.PRF_CLIENT | DrawingOptions.PRF_OWNED)); } finally { g.ReleaseHdc(); } screenshot.Save(""temp.bmp""); }  you need to make your panel AutoZize=True and AutoScroll=False than put it into another container and make the container AutoScroll=true.  No happy answers here DrawToBitmap can only draw what would be visible to the user. Tricks like changing the Location and Size of the panel beforehand usually quickly run out of gas due to the restriction on the control size imposed by the parent. The form itself can never get larger than the screen now you'll also depend on the resolution of the video adapter on the target machine. One ugly hack is to run DrawToBitmap multiple times changing the Panel's AutoScrollPosition property between each call. You'd have to stitch the resulting images together. Paying special attention to the last one of course since it won't be as large as the other ones. Once you start considering code like this you really ought to think about PrintDocument or a report generator instead. One benefit to that is that the printout will look a heckofalot cleaner. Printed screen-shots are invariably ugly due to the huge difference in video and printer resolution. +1 for PrintDocument Hey guys Just an update on this.... A work around was to create the same control in memory as well and define custom height and width and then call the DrawToBitmap function!! Was a sweet success!! Hope this helps others!! Thanks!! That seems the only way at the moment!! A bit of work though!! Thanks again!!  Hmmm... It sounds like (as you did not say so in the original question) but would you have a picturebox acting as a container inside the panel's container i.e. nested containers? Have you not considered doing this:  // Assume pic is the type of PictureBox and the image property is assigned PictureBox pic; // And that the picturebox is embedded in the Panel variable p. p.Controls.Add(pic); // ... // So why not do this? pic.Image.Save(...); The Save method of the Image class has 5 overloads so choose one at your convenience. Hope this helps Best regards Tom. I think he wants to ""screenshot"" a container control on his form. @hometoast: Oh...Now I understand...ummm...how about Alt+PrtScrn and paste it into mspaint? Or some other Screen grabber tool? In short the answer is no as the screen capture what's there regardless of whether the image is clipped or not!  I'd create a bitmap with the correct size (and clear it with the right color or paint the panels' background image to it) and then loop over all child controls and .DrawToImage each one of them to the correct place."
742,A,Tool to draw transparent icon for iPhone app Is there a handy tool to draw transparent icons for an iPhone app? I know PhotoShop can do this but I prefer a simple one. Which OS do you use? Mac OS X This isn't a programming question but one about design software so it belongs on Super User. If you are looking for free image editors I recommand you to follow this link. Link to free image editors question ( Super User ) It was an question asked by me & there were many good answers for mac image editors. you might choose any of them.  Pixelmator is simple quite cheap and popular on the Mac. I find it too simple and prefer Gimp which is nowhere near as simple to use but is very powerful and free. Now It comes in price. :( 30$ . @sugar: AFAIK it has always had a price tag.
743,A,"how to draw lines and shapes on picture box on mouse move event in c#? i am trying to draw a rectangle when i press mouse left button and drag. i want to show the size of rectangle change according to move of mouse. how is it possible. i tried to paint on picture box but the rectangle draw when i release the mouse button. i want to show it during drag? it just like we click on our windows desktop and and darg the mouse we see a rectangle growing with mouse move i have to do this any other option? What code do you have now? ok thanx i have find its solution You can do this with Windows XP just not Windows 7. If I am understanding your inquiry correctly you have to hold down the ""Start"" button key on your keyboard while dragging. You have to let go of the key on the keyboard before letting go of your mouse. Otherwise your art will be deleted. Pull up a window (i.e. Microsoft Word) and shrink the window. You can drag this around and use it as an ""eraser"" if you will. I hope that this is what you are looking for.  if you want to use the paint method then you should handle all repaint event and other complexities of GDI+ . but the easy way is to create a panel with borders on mouse click and when the mouse is moving change panel properties and with mouse release you will have the panel and your rectangle and no need to worry about repaint and ... .  Have a look at Drawing Graphics in C# Scribble Sample: Visual C# MDI Drawing Application DrawTools"
744,A,Flipping a gui Component in the x-z plane I would like to know the respective graphics transformations ..in creating a flipping effect of a ui component about the x-z plane. It needs to be done only using 2d since the swing toolkit only supports 2d affine transformations. http://www.verysimple.com/flex/flipcard/ .... is an example of the effect to be achieved . Your question isn't very clear (at least to me). Why would you expect to be able to do a 3D transformation (which is what your use of the z coordinate implies) using only 2D transformations? Could you give a concrete example of how the transformation should work? Also x-z plane isn't very clear. They tend to be named differently here and there. Which plane is what? Not a true 3-D flipping but the effect looks very similar if you just do 2-D scaling like this Render the front image. Scale X from 1 to 0 anchored at the middle. Render the back image. Scale X from 0 to 1 anchored at the middle. To simulate a constant angular speed the scaling factor can be calculated like this double scale = Math.cos(i*Math.PI/(2.0*steps)); The i is step number and steps is the total number of steps need to simulate a 90 degree rotation. You can also introduce some shear transformation to simulate the perspective of a true 3-D rotation but the effect is not that noticeable for a fast flipping.
745,A,"Ball to Ball Collision - Detection and Handling With the help of the Stack Overflow community I've written a pretty basic-but fun physics simulator. You click and drag the mouse to launch a ball. It will bounce around and eventually stop on the ""floor"". My next big feature I want to add in is ball to ball collision. The ball's movement is broken up into a x and y speed vector. I have gravity (small reduction of the y vector each step) I have friction (small reduction of both vectors each collision with a wall). The balls honestly move around in a surprisingly realistic way. I guess my question has two parts: What is the best method to detect ball to ball collision? Do I just have an O(n^2) loop that iterates over each ball and checks every other ball to see if it's radius overlaps? What equations do I use to handle the ball to ball collisions? Physics 101 How does it effect the two balls speed x/y vectors? What is the resulting direction the two balls head off in? How do I apply this to each ball? Handling the collision detection of the ""walls"" and the resulting vector changes were easy but I see more complications with ball-ball collisions. With walls I simply had to take the negative of the appropriate x or y vector and off it would go in the correct direction. With balls I don't think it is that way. Some quick clarifications: for simplicity I'm ok with a perfectly elastic collision for now also all my balls have the same mass right now but I might change that in the future. In case anyone is interested in playing with the simulator I have made so far I've uploaded the source here (EDIT: Check the updated source below). Edit: Resources I have found useful 2d Ball physics with vectors: 2-Dimensional Collisions Without Trigonometry.pdf 2d Ball collision detection example: Adding Collision Detection Success! I have the ball collision detection and response working great! Relevant code: Collision Detection: for (int i = 0; i < ballCount; i++) { for (int j = i + 1; j < ballCount; j++) { if (balls[i].colliding(balls[j])) { balls[i].resolveCollision(balls[j]); } } } This will check for collisions between every ball but skip redundant checks (if you have to check if ball 1 collides with ball 2 then you don't need to check if ball 2 collides with ball 1. Also it skips checking for collisions with itself). Then in my ball class I have my colliding() and resolveCollision() methods: public boolean colliding(Ball ball) { float xd = position.getX() - ball.position.getX(); float yd = position.getY() - ball.position.getY(); float sumRadius = getRadius() + ball.getRadius(); float sqrRadius = sumRadius * sumRadius; float distSqr = (xd * xd) + (yd * yd); if (distSqr <= sqrRadius) { return true; } return false; } public void resolveCollision(Ball ball) { // get the mtd Vector2d delta = (position.subtract(ball.position)); float d = delta.getLength(); // minimum translation distance to push balls apart after intersecting Vector2d mtd = delta.multiply(((getRadius() + ball.getRadius())-d)/d); // resolve intersection -- // inverse mass quantities float im1 = 1 / getMass(); float im2 = 1 / ball.getMass(); // push-pull them apart based off their mass position = position.add(mtd.multiply(im1 / (im1 + im2))); ball.position = ball.position.subtract(mtd.multiply(im2 / (im1 + im2))); // impact speed Vector2d v = (this.velocity.subtract(ball.velocity)); float vn = v.dot(mtd.normalize()); // sphere intersecting but moving away from each other already if (vn > 0.0f) return; // collision impulse float i = (-(1.0f + Constants.restitution) * vn) / (im1 + im2); Vector2d impulse = mtd.multiply(i); // change in momentum this.velocity = this.velocity.add(impulse.multiply(im1)); ball.velocity = ball.velocity.subtract(impulse.multiply(im2)); } Source Code: Complete source for ball to ball collider. Binary: Compiled binary in case you just want to try bouncing some balls around. If anyone has some suggestions for how to improve this basic physics simulator let me know! One thing I have yet to add is angular momentum so the balls will roll more realistically. Any other suggestions? Leave a comment! I don't think this algorithm is good enough because if your balls are moving too fast (ex: faster then 2*radius per frame one ball can pass through another ball without any collisions. Here is a link to the last version of BallBounce I worked on: http://dl.dropbox.com/u/638285/ballbounce.rar @Simulcal could you upload your source code again (all the filedropper.com links appear to be broken). Also could you put up the pdf file you got from [geocities.com/vobarian/2dcollisions/2dcollisions.pdf] as geocities has gone offline recently @To all who contributed: Can you please shed some light to transform this engine to 3D. How this great engine can also work in Java3D. Line `Vector2d impulse = mtd.multiply(i);` should be i * the normalized mtd vector. Something like: `Vector2d impulse = mtd.normalize().multiply(i);` I implemented this code in JavaScript using the HTML Canvas element and it produced wonderful simulations at 60 frames per second. I started the simulation off with a collection of a dozen balls at random positions and velocities. I found that at higher velocities a glancing collision between a small ball and a much larger one caused the small ball to appear to STICK to the edge of the larger ball and moved up to around 90 degrees around the larger ball before separating. (I wonder if anyone else observed this behavior.) Some logging of the calculations showed that the Minimum Translation Distance in these cases was not large enough to prevent the same balls from colliding in the very next time step. I did some experimenting and found that I could solve this problem by scaling up the MTD based on the relative velocities: dot_velocity = ball_1.velocity.dot(ball_2.velocity); mtd_factor = 1. + 0.5 * Math.abs(dot_velocity * Math.sin(collision_angle)); mtd.multplyScalar(mtd_factor); I verified that before and after this fix the total kinetic energy was conserved for every collision. The 0.5 value in the mtd_factor was the approximately the minumum value found to always cause the balls to separate after a collision. Although this fix introduces a small amount of error in the exact physics of the system the tradeoff is that now very fast balls can be simulated in a browser without decreasing the time step size.  You have two easy ways to do this. Jay has covered the accurate way of checking from the center of the ball. The easier way is to use a rectangle bounding box set the size of your box to be 80% the size of the ball and you'll simulate collision pretty well. Add a method to your ball class: public Rectangle getBoundingRect() { int ballHeight = (int)Ball.Height * 0.80f; int ballWidth = (int)Ball.Width * 0.80f; int x = Ball.X - ballWidth / 2; int y = Ball.Y - ballHeight / 2; return new Rectangle(xyballHeightballWidth); } Then in your loop: // Checks every ball against every other ball. // For best results split it into quadrants like Ryan suggested. // I didn't do that for simplicity here. for (int i = 0; i < balls.count; i++) { Rectangle r1 = balls[i].getBoundingRect(); for (int k = 0; k < balls.count; k++) { if (balls[i] != balls[k]) { Rectangle r2 = balls[k].getBoundingRect(); if (r1.Intersects(r2)) { // balls[i] collided with balls[k] } } } } This would make the balls go into each other 20% on horizontal and vertical collisions. Might as well use circular bounding boxes as the efficiency difference negligible. Also `(x-width)/2` should be `x-width/2`. Good call on the precedence typo. You'll find that most 2d games use rectangular bounding boxes on non rectangular shapes because it is fast and the user almost never notices anyway. You could do rectangular bounding box then if it has a hit check the circular bounding box. @Jonathan Holland your inner loop should be for(int k = i + 1; ...) This will get rid of all the redundant checks. (ie checking with collision of self and checking collision ball1 with ball2 then ball2 with ball1). Actually a square bounding box is likely to be _worse_ performance-wise than a circular bounding box (assuming you've optimized the square root away)  I see it hinted here and there but you could also do a faster calculation first like compare the bounding boxes for overlap and THEN do a radius-based overlap if that first test passes. The addition/difference math is much faster for a bounding box than all the trig for the radius and most times the bounding box test will dismiss the possibility of a collision. But if you then re-test with trig you're getting the accurate results that you're seeking. Yes it's two tests but it will be faster overall. You don't need trig. `bool is_overlapping(int x1 int y1 int r1 int x2 int y2 int r2) { return (x2-x1)*(x2-x1)+(y2-y1)*(y2-y1)<(r1+r2)*(r1+r2); }`  One thing I see here to optimize. While I do agree that the balls hit when the distance is the sum of their radii one should never actually calculate this distance! Rather calculate it's square and work with it that way. There's no reason for that expensive square root operation. Also once you have found a collision you have to continue to evaluate collisions until no more remain. The problem is that the first one might cause others that have to be resolved before you get an accurate picture. Consider what happens if the ball hits a ball at the edge? The second ball hits the edge and immediately rebounds into the first ball. If you bang into a pile of balls in the corner you could have quite a few collisions that have to be resolved before you can iterate the next cycle. As for the O(n^2) all you can do is minimize the cost of rejecting ones that miss: 1) A ball that is not moving can't hit anything. If there are a reasonable number of balls lying around on the floor this could save a lot of tests. (Note that you must still check if something hit the stationary ball.) 2) Something that might be worth doing: Divide the screen into a number of zones but the lines should be fuzzy--balls at the edge of a zone are listed as being in all the relevant (could be 4) zones. I would use a 4x4 grid store the zones as bits. If an AND of the zones of two balls zones returns zero end of test. 3) As I mentioned don't do the square root. Thank you for the information on the square root tip. Didn't know about its expensive nature when compared to the square. Another optimization would be to find balls which are nowhere near any other balls. This would work reliably only if the velocities of the balls are constrained. I disagree on looking for isolated balls. That's just as expensive as detecting the collision. To improve things you need something that's less than O(n) for the ball in question. Excellent post.  A good way of reducing the number of collision checks is to split the screen into different sections. You then only compare each ball to the balls in the same section.  Well years ago I made the program like you presented here. There is one hidden problem (or many depends on point of view): If the speed of the ball is too high you can miss the collision. And also almost in 100% cases your new speeds will be wrong. Well not speeds but positions. You have to calculate new speeds precisely in the correct place. Otherwise you just shift balls on some small ""error"" amount which is available from the previous discrete step. The solution is obvious: you have to split the timestep so that first you shift to correct place then collide then shift for the rest of the time you have. +1 insightful. He's indeed likely to need that insight next. [Here](http://www.a-coding.com/2010/10/predictive-collision-detection.html) is a way of doing the collision detection without this problem. If shift positions on `timeframelength*speed/2` then positions would be statistically fixed. @Nakilon: no it helps only in some cases but generally it is possible to miss the collision. And the probability to miss the collision increases with the size of the timeframelength. By the way it seems that Aleph demonstrated the correct solution (I just skimmed it though). @avp I was not about *If the speed of the ball is too high you can miss the collision.* but about *your new positions will be wrong*. Because of collision is being detected a bit later than they really collided if substract `timeframelength*speed/2` from that position accuracy will increase twice.  To detect whether two balls collide just check whether the distance between their centers is less than two times the radius. To do a perfectly elastic collision between the balls you only need to worry about the component of the velocity that is in the direction of the collision. The other component (tangent to the collision) will stay the same for both balls. You can get the collision components by creating a unit vector pointing in the direction from one ball to the other then taking the dot product with the velocity vectors of the balls. You can then plug these components into a 1D perfectly elastic collision equation. Wikipedia has a pretty good summary of the whole process. For balls of any mass the new velocities can be calculated using the equations (where v1 v2 are velocities after the collision and u1 u2 are from before): If the balls have the same mass then the velocities are simply switched. Here's some code I wrote which does something similar: void Simulation::collide(Storage::Iterator a Storage::Iterator b) { // Check whether there actually was a collision if (a == b) return; Vector collision = a.position() - b.position(); double distance = collision.length(); if (distance == 0.0) { // hack to avoid div by zero collision = Vector(1.0 0.0); distance = 1.0; } if (distance > 1.0) return; // Get the components of the velocity vectors which are parallel to the collision. // The perpendicular component remains the same for both fish collision = collision / distance; double aci = a.velocity().dot(collision); double bci = b.velocity().dot(collision); // Solve for the new velocities using the 1-dimensional elastic collision equations. // Turns out it's really simple when the masses are the same. double acf = bci; double bcf = aci; // Replace the collision velocity components with the new ones a.velocity() += (acf - aci) * collision; b.velocity() += (bcf - bci) * collision; } As for efficiency Ryan Fox is right you should consider dividing up the region into sections then doing collision detection within each section. Keep in mind that balls can collide with other balls on the boundaries of a section so this may make your code much more complicated. Efficiency probably won't matter until you have several hundred balls though. For bonus points you can run each section on a different core or split up the processing of collisions within each section. Lets say masses of the two balls aren't equal. How does that effect the vector change between balls? It's been a while since grade 12 but I think they get a ratio of the momentum corresponding to the ratio of the masses. Edited to include equations from Wikipedia @Jay just to point out.. that one equation image you added is for a 1 dimensional collision not 2 dimensional. @simucal. not true... u and v are vectors in that equation. That is they have x y (and z) components. @Simucal you are right they are for the one dimensional case. For more dimensions just use the components of the velocity that are in line with the collision (aci bci in code). The other components are orthogonal to the collision and will not change so you don't need to worry about them.  As a clarification to the suggestion by Ryan Fox to split the screen into regions and only checking for collisions within regions... e.g. split the play area up into a grid of squares (which will will arbitrarily say are of 1 unit length per side) and check for collisions within each grid square. That's absolutely the correct solution. The only problem with it (as another poster pointed out) is that collisions across boundaries are a problem. The solution to this is to overlay a second grid at a 0.5 unit vertical and horizontal offset to the first one. Then any collisions that would be across boundaries in the first grid (and hence not detected) will be within grid squares in the second grid. As long as you keep track of the collisions you've already handled (as there is likely to be some overlap) you don't have to worry about handling edge cases. All collisions will be within a grid square on one of the grids. +1 for a more accurate solution and to counter the cowardly drive-by downvoter thats a good idea. I did this once and i checked the current cell and all neighboring cells but your method is more efficient. Another way I just thought of is to check the current cell and then check to see if it intersects with the current cells boundaries and if so check objects in THAT neighboring cell.  I found an excellent page with information on collision detection and response in 2D. http://www.metanetsoftware.com/technique.html They try to explain how it's done from an academic point of view. They start with the simple object-to-object collision detection and move on to collision response and how to scale it up. Edit: Updated link +1 good find Mizard.  This KineticModel is an implementation of the cited approach in Java.  You should use space partitioning to solve this problem. Read up on Binary Space Partitioning and Quadtrees Thanks for the information I appreciate it. Instead of space partitioning wouldn't a [sweep and prune](http://en.wikipedia.org/wiki/Sweep_and_prune) algorithm work better? the balls are moving fast so any partitioning will have to be updated frequently incurring overhead. A sweep and prune could find all colliding pairs in O(n log n) without any transient data structures. [Here is a good tutorial for the basics](http://jitter-physics.com/wordpress/?tag=sweep-and-prune)"
746,A,"Can we output a picture in c? Hi can we output a .jpg image or .gif image in c? I mean can we print a picture as output with the help of a c program? Aslo can we write a script in C language for html pages as can be written in javascript? Can the browsers operate on it? if not possible is there any plugin for any of the browsers? any example code or links please? If you want to do any server-side stuff like CGI you'll need to also run Apache on your PC. See http://httpd.apache.org/docs/1.3/windows.html – Adam Pierce (5 mins ago) Learning Javascript or PHP could also be a good idea. C is not an ideal language for web programming. – Adam Pierce (4 mins ago) That's a lot of questions! You can generate JPEG and PNG files from C using libjpeg and libpng respectively. (Yes I'm dodging your GIF question on purpose. You don't need to do that; PNG is well-supported by mainstream browsers now and should be preferred. :-P) Browsers generally only support JavaScript for client-side scripting. Using anything else will only destroy the portability of your webpages. Yes like you say you can use plugins but some people are averse to installing plugins and others are behind corporate policies that don't allow such things. thank u.I was just curious to know whether ther is any provision in c to do that? then how to do that? i need some example code. please find me one.once again thank u for ur response?  To save jpeg files you will need libjpeg. I am sure that there is a similar library for GIFs too. C is not a scripting language. You cannot write scripts in C like in Javascript. You can write server-side code in with CGI if you dare but this is really not a good idea. There are a couple of better tools to write server side components including C#/ASP.NET Java Python Perl Ruby (with Rails) or PHP. Even Coldfusion is better for webpages than C. C is not an interpreted language. There is no browser that going to run your C code and I doubt there ever will be one for millions of reasons. Also there is no plugin for that. This is something really unfeasable. C is a great language for a lot of stuff but the web is one of the areas where it's rarely used (except if you want to write a browser or a http server in C. But even for those tasks C++ is usually better.) If you want client-side scripting in webpages use Javascript. Some browsers will interpret VBScript too but Javascript is the preferred browser scripting language.  You can generate a web page from a C program by using the Common Gateway Interface (CGI). The C program is compiled and runs on the server this is different from Javascript which runs on the browser. You can also generate images via CGI too. Just set the content type appropriately eg. image/jpeg for a JPEG image. You can use libjpeg to generate the actual image. Here is an example C program to generate a page with a JPEG image: #include <stdio.h> main() { char *pageTitle = ""Look a JPEG!""; char *urlImage = ""/myimage.jpeg""; // Send HTTP header. printf(""Content-type: text/html\r\n\r\n""); // Send the generated HTML. printf(""<html><head><title>%s</title></head>\r\n"" ""<body>\r\n"" ""<h1>%s</h1>\r\n"" ""<img src=\""%s\"">\r\n"" ""</body></html>\r\n"" pageTitle pageTitle urlImage); }"
747,A,How can I extract all default skins / styles / icons etc. from my Flex app? I'm developing a Flex application. Since I'm not a very talented graphical designer I leave all stylings etc. at their defaults. Once I finish the app I would like to give it to a graphical designer together with all the fonts icons styles etc. that Flex has put into my app so she can swap them out and make my app pretty and theme it to my CI. Also I'd like to use this approach as the foundation for interchangeable themes at a later stage. There's two parts to my question really: a) how do I get all the assets that are used in my app (not simply all that the Flex framework has bundled) extracted and b) how do I figure out which asset is used in which place so the designer doesn't have to guess and reverse engineer. There's a third part (how do I repackage the assets to make them swappable at run / compile time) but I think I know how that works roughly. I don't believe you can do that. You will have to set up skinning yourself. The default graphics used in the components are built into the flex framework and generated at runtime using the drawing api instead of embedding graphics. Your best bet is going to be to use css and a graphics framework that supports skinning like degrafa ( http://www.degrafa.org/ ). If you want to check out the code that generates the base skin for flex 3.x you can have a look in the svn repo here - http://opensource.adobe.com/svn/opensource/flex/sdk/trunk/frameworks/projects/halo/
748,A,How do I get the current size of a matrix stack in OpenGL? How do I get the current size of a matrix stack (GL_MODELVIEW GL_PROJECTION GL_TEXTURE) in OpenGL? I want this so that I can do some error checking to ensure that in certain parts of the code I can check that the matrix stacks have been left in the original condition. Try:  GLint depth; glGetIntegerv (GL_MODELVIEW_STACK_DEPTH &depth); The enums for the other stacks are:  GL_MODELVIEW_STACK_DEPTH GL_PROJECTION_STACK_DEPTH GL_TEXTURE_STACK_DEPTH If you use multi-texturing you have more than one texture matrix stack to query. To do so set the current texture-unit via glActiveTexture();.
749,A,"What kind of language should I design for a particle engine scriptable engine? I was wondering which kind of expressiveness fits a language used to generate particle effects.. Supposing to want an engine as flexible as possible what kind of 'features' should it have? (in addition to trivial ones like color position velocity acceleration) SO everybody's urging you not to reinvent the wheel and that's a great idea I have a soft spot for Python (which would allow your scripting users to also install and use plenty of other useful math libs &c) but LUA's no doubt even easier to integrate (and other scripting languages such as Ruby would also no doubt be just fine). Not writing your own ad-hoc scripting language is excellent advice. But reading your question suggests to me that your issue is more about -- what attributes of the objects of my engine (and what objects -- particles sure but what else besides) should I expose to whatever scripting language I ember (or say via MS COM or .NET to whatever scripting or non-scripting language my users prefer)? Specific properties of each particle such as those you list are no doubt worthwhile. Do you have anything else in your engine besides point-like particles such as say surfaces and other non-pointlike entities off which particles might bounce? What about ""forces"" of attraction or repulsion? Might your particles have angular momentum / spin? A great idea would be to make your particles' properties ""expando"" to use a popular term (other object models express the same idea differently) -- depending on the app using your engine other programmers may decide to add to particles whatever properties they need... maybe mass say -- maybe electric charge -- maybe cost in eurocents for all you know or care;-). This makes life easier for your users compared to just offering a ""particle ID"" (which you should anyway of course;-) for them to use in hash tables or the like to keep track of the specific attributes they care about! Allowing them to add ""methods"" and ""triggers"" (methods that you call automatically if and when certain conditions hold e.g. two particles get closer than a certain distance) would be awesome but maybe a bit harder. Don't forget BTW to allow a good method to ""snapshot"" the current state of the particle system (INCLUDING user-added expando properties) to a named stream or file and restore from such a snapshot -- that's absolutely crucial in many uses. Going beyond specific particles (and possibly other objects such as surfaces if you have them) you should probably have a ""global environment"" with its own properties (including expando ones) and ideally methods and triggers too. E.g. a force field acting on all particles depending on their position (and maybe their charge mass etc...!-)... Hope some of these ideas strike you as interesting -- hard for me to tell with little idea of your intended field of application!-)  Don't try and design a new language just for your application instead embed another well-established language in there. Take a look at the mess it has caused for other applications trying to implement their own scripting language (mIRC is a good example). It will mean users will have to learn another language just to script your application. Also if you design your own language it will probably end up not as useful as other languages. Don't try to reinvent the wheel. You might want to look at Lua as it is light-weight popular well-established and is designed to be used by games (users of it include EA Blizzard Garry's Mod etc.) and has a very minimal core library (it is designed to be a modular language).  Just embed Lua. It's a great language design excellent performance widely used by game developers and small enough that you can master it in a few days. Then you can get on with your game design."
750,A,Source code for Xiaolin Wu's line algorithm in C? I'm looking for a nice and efficient implementation of Xiaolin Wu's anti-aliased line drawing algorithm in C does anyone have this code they could share with me? Thanks Wikipedia has pseudo code. Google has many examples like this one or this one. And your question reminded me this nice article on antialiasing. EDIT: It's time to discover Hugo Helias's website if you don't know it already. thanks the second link was exactly what i wanted. I searched on google myself but for some reason couldn't get these links thanks! :)
751,A,"Silverlight DataGrid Redraw Issue I have a Silverlight DataGrid that contains a single template column which displays a user control. The user control has a progress bar to represent processing and when the processing is complete an animation hides the progress bar and shows a finished label. There are two instances in which the datagrid seems not to redraw itself: First when a user does a lot of scrolling then the datagrid will start by redrawing the animation when the usercontrol comes into view and will finally reach a point where it doesn't redraw anything. Second there is a click event in the usercontrol that displays a popup. When the popup is closed then again the usercontrol fails to redraw itself properly. Any ideas as to why this behavior occurs and ways around it? You have to understand that a DataGrid does not have one instance of your control for every row in the grid. The data source could have millions of rows while the grid doesn't need more instances of your control than will fit on the screen. This means that when you scroll for instance it doesn't reveal instances of your control previously invisible; it just assigns the currently-visible controls whatever values are needed to make the correct display for the current scroll position. I don't know if there is a workaround.  The previous answer is correct. I have found that sometimes you have to properly handle the Loading_Row and Unloading_Row events if you want any of your controls to behave properly. In my cases having the controls properties actually bound to an object to work best. If you try to rely on animations and visibility and the like you will see strange behavior but if you bind the progressbar to an object that is maintaining the progress for each item behind the scenes then every time that row is shown it will rebind and redraw. Have you had strange behavior when users click inside the grid as well? Clicking seems to also invoke a ""repaint"" which can force a redraw of animation too."
752,A,Draw a semicircle / half circle in WPF / C# I need to draw a semicircle / half circle in WPF. Any idea how to do that? Thanks for any hint! ArcSegment would be a good place to start. And here is a good example of how to use it in code. thank you very much for that useful answer and links! bummer link is now broken. Might have been this one http://blogs.vertigo.com/personal/ralph/Blog/Lists/Posts/Post.aspx?ID=5
753,A,"How can I create a hardware-accelerated image with Java2D? I'm trying to create a fast image generator that does lots of 2d transformations and shape rendering so I'm trying to use a BufferedImage and then acquire the Graphics2D object to perform all my drawing. My main concern now is to make is really fast so I'm creating a BufferedImage like this: GraphicsEnvironment ge = GraphicsEnvironment.getLocalGraphicsEnvironment(); GraphicsConfiguration gc = ge.getDefaultScreenDevice().getDefaultConfiguration(); BufferedImage bImage = gc.createCompatibleImage(width height Transparency.TRANSLUCENT); Graphics2D graphics = bImage.createGraphics(); However if I do: System.out.println(bImage.getCapabilities(gc).isAccelerated()); The output is always false even if I start the JVM with -Dsun.java2d.opengl=True which prints the line: OpenGL pipeline enabled for default config on screen 0 I'm doing a BufferedImage because in the end I want to save it to a PNG file with ImageIO.write(bImage ""PNG"" file); I can create a VolatileImage which will say that it is accelerated but ImageIO doesn't like it when trying to save saying that that image cannot be casted to RenderedImage. Any ideas on how to get an accelerated image that can be stored to disk? Also I don't want to create a VolatileImage and the copy to a BufferedImage to save since my images are really big and I'll run into out of memory issues... I'm using Java6 with the Sun JDK What version of Java are you using? Can I suggest that you implement the code and then time it once the code is in place? This way you work out if there is enough slowdown to warrant trying to accelerate it. Try adding this to your command line: -Dsun.java2d.accthreshold=0 EDIT I did some reading on this after you said the above didn't work. Try changing the True on the OpenGL parameter to be all lowercase. -Dsun.java2d.opengl=true Adding that parameter unfortunately does not change anything. Being lowercase has no impact. From the documentation an uppercase True enabled the OpenGL pipeline with debug information e.g. it prints: OpenGL pipeline enabled for default config on screen 0 with lowercase it is the same behaviour but without the debug information being printed out. I just went and Googled some docs about this and that is what they said. I guess lesson learned for trying to help on something you don't know.  If speed really matters for you you should consider using graphic hardware directly. I could suggest you CUDA: a C library for using massively parallel NVIDA GPUs. Take a look to JCUDA for using it from JAVA (http://www.jcuda.org/) CUDA is very popular in the parellel processing world but at the price of buying a NVIDIA card.  Here is how it works: The way you create the image is correct. But Java does not accelerate the image right away. There is a threshold of render calls that need to be directed at your image after which the image will be accelerated. But if you set the following -Dsun.java2d.accthreshold=0 your image should be accelerated the moment you create it. You might still have to use the image somehow for it to become accelerated. For example i have the following code piece that is used to copy a loaded image (loading through ImageIO will give you an image that can not be accelerated) into my new BufferedImage. Graphics2D g2d = bufferedImage.createGraphics(); g2d.drawImage(image00null); g2d.dispose(); Before this code segment the BufferedImage is not accelerated afterwards it is. But beware if you manipulate the image data directly (i.e. not through Java-2d) your image will no longer be cached in VRAM and you have to copy it over to a new Managed Image in order to regain your acceleration.  From my investigation and reading the Sun/Oracle 2D tutorial this is what I've found: To create an accelerated image one can either use the volatile image which is created as a pbuffer. There are advantages and disadvantages VolatileImages are fast until you try to get the raster pixel data. At that moment they become a normal bufferedImage. Also they are volatile and there is no guarantee that it will be available in the end of your rendering. To do this you execute: GraphicsEnvironment ge = GraphicsEnvironment.getLocalGraphicsEnvironment(); GraphicsConfiguration gc = ge.getDefaultScreenDevice().getDefaultConfiguration(); VolatileImage vImage = gc.createCompatibleVolatileImage(width height Transparency.TRANSLUCENT); This will get you a accelerated image however it is not possible to use the ImageIO to write it directly to say a PNG file. In order to use a normal BufferedImage you need to do something like: GraphicsEnvironment ge = GraphicsEnvironment.getLocalGraphicsEnvironment(); GraphicsConfiguration gc = ge.getDefaultScreenDevice().getDefaultConfiguration(); BufferedImage img = gc.createCompatibleImage(width height Transparency.TRANSLUCENT); img.setAccelerationPriority(1); This creates a BufferedImage which is not accelerated but you give the hint to the JVM that is should by passing 1 in setAccelerationPriority. This you can read on the 2D trail of the Java Tutorial."
754,A,"Ensure text remains always readable in Silverlight We have a Silverlight application that shows text over video. Both the text and video can be considered variables. Sometimes we might have a dark video sometimes a bright video sometimes a video that has sections of both. Think of credits at the end of a movie. We want to ensure the end user can always read the text being show over the video. The text is always an overlay on top of the video. The simple solution is to two show the text twice once in white and once in black with a small offset. This almost works but actually looks a little rough and takes away from the user experience. Ideally we would have the text with slight semitransparent glow around the edges. So if the text were white there would be a black glow right around the edges. Is there a way to do this? Or is there an equal or better work-around? Sounds similar to the problem of ensuring subtitles are always readable in films/TV. The most robust but not necessarily most elegant solution is to have a coloured background rectangle for the text which is either opaque or has a low transparency value - often grey or black with good contrasting foreground colour. You're right that would work. That would take a lot of way from the user experience for this application. We'd probably leave as is if that ends up being only option.  I've done this with the DropShadow pixel shader effect in Silverlight 3. It works nicely but since the pixel shaders aren't executed on the hardware it can have a pretty heavy impact on the performance of the application. If you wanted to get ambitious you could write your own pixel shader. Silverlight 3 supports HLSL Shaders.  You could try displaying it with a contrasting outline rather than just a ""drop shadow"" like you get if you display it once with a small offset. To do this display it four times in one color then a fifth time with a contrasting color centered over the four previous copies. The four first ones should be offset one pixel up right down and left of the center. The net effect should be an outline. Of course perhaps this too looks ""rough"" since it's computer-generated and thus not perfect with respect to issues like kerning spacing between characters and so on. But it's quick to try at least. In general automatically finding good contrasting colors when the background is video sounds a bit difficult. In the worst case the video contains text just like the one you want to display. The correct solution in that case is hard to imagine. Thanks the 4 direction shadow has the roughness as well. I think it need either the glow effect or an anti aliasing correction. Does anyone know if Silverlight 3.0 bitmap pixel shaders could do either of these?"
755,A,"Drawing an image in Java slow as hell on a netbook In follow-up to my previous questions (especially this one : http://stackoverflow.com/questions/2684123/java-volatileimage-slower-than-bufferedimage) i have noticed that simply drawing an Image (it doesn't matter if it's buffered or volatile since the computer has no accelerated memory* and tests shows it's doesn't change anything) tends to be very long. (*) System.out.println(GraphicsEnvironment.getLocalGraphicsEnvironment() .getDefaultScreenDevice().getAvailableAcceleratedMemory()); --> 0 How long ? For a 500x400 image about 0.04 seconds. This is only drawing the image on the backbuffer (obtained via buffer strategy). Now considering that world of warcraft runs on that netbook (tough it is quite laggy) and that online java games seems to have no problem whatsoever this is quite thought provoking. I'm quite certain I didn't miss something obvious I've searched extensively the web but nothing will do. So do any of you java whiz have an idea of what obscure problem might be causing this (or maybe it is normal tough I doubt it) ? PS : As I'm writing this I realized this might be cause by my Linux installation (archlinux) tough I have the correct Intel driver. But my computer normally has ""Integrated Intel Graphics Media Accelerator 950"" which would mean it should have accelerated video memory somehow. Any ideas about this side of things ? @ Tom Hawtin : Yup || @ camickr : I draw on a Canvas which is not swing. Also in a game you obviously want to disable the auto swing refresh anyway. As for the SSCCE the point is moot because basically you create the simplest of simplest win and compatible image (using GraphicsDevice and stuff) then draw it and it's still slow. For comparison it runs about 50 times faster on a top-notch Windows computer and about 10 times faster on another netbook (with Intel Graphics Media Accelerator 3150). The other netbook - what OS does it run? See my edited answer. If you edit your question you can use the code formatting around your code which would be helpful. Created a ""compatible"" buffer? Why don't you post your SSCCE: http://sscce.org. Then maybe people will test it on other environments. We have no idea what you actual code is like and if you are doing anything wrong. I always questions why people use a BufferStrategy. Swing is double buffered and it reduces the complexity of the coding. Today I learned that `hell` is a unit of time measuring 0.04 seconds. I will commence usage of this term when I wish to denote small amounts of time. I'm also running Arch Linux and noticed my games going slow sometimes especially when using alpha transparencies with my images. It turns out that even Windows not only Linux sometimes turns off hardware acceleration by default. I looked for a solution to the problem and found this: http://web.archive.org/web/20120926022918/http://www.systemparadox.co.uk/node/29 Enabling OpenGL considerably sped up my framerates and I assume if you ran your tests again you'd get faster draws. I can't really confirm as the test configuration in question was trashed some time ago. But I'll accept this in hopes it can help others.  I don't know much about java graphics but if I were in your shoes I would assume that the measurement means nothing without a comparison value which it sounds like you might have but are not sharing. Add this information to your question along with the specs of the comparison system (is it a desktop? does it have a dedicated video card? does it run windows or linux?). Concerning your measurement that it's 10 times faster on another netbook does that other notebook run Windows or is that one also Linux? Linux has historically had very mediocre graphics drivers - they just don't run nearly as well as the Windows equivalents. In fact for a long time the only drivers you could get were not written by ATI/nVidia/etc. but rather by hobbyists. It would not surprise me at all if a Linux machine ran a graphical program ten times slower than a similar machine running Windows. This was the situation as I understood it about five years ago. I doubt it's changed much. @Chris : See my answer to camickr above for comparison values. || @Tom : My own calculations told me the same :) || @Chris : I looked it up some time ago but there seems to be no magic trick of the sort I seek there. Actually I'm now about 99% the fault is in the OS camp. I have opened a thread on the archlinux forums with no success so far. We can do some calculations. 500x400/0.04 is 5000000 pixel/second. That is going to be a read and a write of presumably 32 bits each. I'd be disappointed if a twenty year old (non-PC) desktop couldn't manage that. What about looking at existing games which use complex Java2D operations? I'd recommend the stuff on http://goldenstudios.or.id/products/games/index.php as perhaps a benchmark :) Ubuntu Netbook Edition 9.10 (Karmic Koala) I think. Btw Intel makes quality open source driver which support graphical acceleration. I've looked around and found no one who had comparable problems - it definitely seems to be my system because glxgears (a linux 3D benchmark) gives me 60fps which is really low (people with my netbook usually report about 300-400). I have found one or two things to try but I highly doubt they'll work :s"
756,A,"Monochrome BitMap Library I am trying to create a piece of software that can be used to create VERY large (10000x10000) sized bitmaps. All I need is something that can work in monochrome since the required output is a matrix containing details of black and white pixels in the bitmap. The closest thing I can think of is a font editor but the size is a problem. Is there any library out there that I can use to create the software or will I have to write the whole thing from the start? Edited on May 25: OK so I've been searching around and I have found that using the GtkTree Widget is a good way to create grids. Has anybody tried that with the large sizes that I require? And if so can it be made to look like a drawing surface rather than a Spreadsheet like view? What operations do you need to perform on the ""image""? Do you need to display it? Do you need to print it? What platform(s) does it need to run on? 10000 x 10000 is really not that large. Medical imaging typically uses that order of resolution. Have you confirmed that some of the common graphics libraries really don't work? Why don't you use bitmap objects like gdk pixmaps if you use GTK? 10000 x 10000 pixels with a depth of 1 (monochrome) is 100000000 bits which is 12500000 bytes around 12 megabytes. Not that large. Agreed the images out of a good desktop scanner are *much* bigger."
757,A,"How to make translucent sprites in pygame I've just started working with pygame and I'm trying to make a semi-transparent sprite and the sprite's source file is a non-transparent bitmap file loaded from the disk. I don't want to edit the source image if I can help it. I'm sure there's a way to do this with pygame code but Google is of no help to me. I may have not been clear in my original question but I think I figured it out on my own. What I was looking for turns out to be Surface's set_alpha() method so all I had to do was make sure that translucent images were on their own surface. Here's an example with my stripped down code: import pygame os.path from pygame.locals import * class TranslucentSprite(pygame.sprite.Sprite): def __init__(self): pygame.sprite.Sprite.__init__(self TranslucentSprite.container) self.image = pygame.image.load(os.path.join('data' 'image.bmp')) self.image = self.image.convert() self.image.set_colorkey(-1 RLEACCEL) self.rect = self.image.get_rect() self.rect.center = (320240) def main(): pygame.init() screen = pygame.display.set_mode((640480)) background = pygame.Surface(screen.get_size()) background = background.convert() background.fill((250250250)) clock = pygame.time.Clock() transgroups = pygame.sprite.Group() TranslucentSprite.container = transgroups """"""Here's the Translucency Code"""""" transsurface = pygame.display.set_mode(screen.get_size()) transsurface = transsurface.convert(screen) transsurface.fill((2550255)) transsurface.set_colorkey((2550255)) transsurface.set_alpha(50) TranslucentSprite() while 1: clock.tick(60) for event in pygame.event.get(): if event.type == QUIT: return elif event.type == KEYDOWN and event.key == K_ESCAPE: return transgroups.draw(transsurface) screen.blit(background(00)) screen.blit(transsurface(00)) pygame.display.flip() if __name__ == '__main__' : main() Is this the best technique? It seems to be the most simple and straightforward.  you might consider switching to using png images where you can do any kind of transparency you want directly in the image.  If your image has a solid color background that you want it to became transparent you can set it as color_key value and pygame will make it transparent when blitting the image. eg: color = image.get_at((00)) #we get the color of the upper-left corner pixel image.set_colorkey(color)  After loading the image you will need to enable an alpha channel on the Surface. that will look a little like this: background = pygame.Display.set_mode() myimage = pygame.image.load(""path/to/image.bmp"").convert_alpha(background) This will load the image and immediately convert it to a pixel format suitable for alpha blending onto the display surface. You could use some other surface if you need to blit to some other off screen buffer in another format. You can set per-pixel alpha simply enough say you have a function which takes a 3-tuple for rgb color value and returns some desired 4tuple of rgba color+alpha you could alter the surface per pixel: def set_alphas(color): if color == (2552550): # magenta means clear return (0000) if color == (0255255): # cyan means shadow return (000128) rgb = color return (rgb255) # otherwise use the solid color from the image. for row in range(myimage.get_height()): for col in range(myimageget_width()): myimage.set_at((row col) set_alphas(myimage.get_at((row col))[:3])) There are other more useful ways to do this but this gives you the idea I hope."
758,A,"DirectX capabilities on different graphics cards I'm writing a Direct3D application using DirectX 9. Although it works on my PC I need to make it work on a wide range of systems. I need to know what capabilities I can expect to see on other systems. Is there a list of the DirectX capabilities that graphics cards support? I've found one site which I'll post as an answer but it's a bit out of date. Edit #1: Of course I will test for all the capabilities before I use them. But there are two different approaches to a missing capability: either workaround it or just fail to start. I need to know how many people will be affected before I decide which approach to take. Edit #2: By ""capabilities"" I mean the values in D3DCAPS9 returned by IDirect3D9::GetDeviceCaps. These can (and do) differ amongst different graphics cards. There's an excel spreadsheet of capabilities of all major graphics cards in DXSDK. Find it in the SDK's Samples\C++\Direct3D\ConfigSystem\CardCaps.xls. It also gives the lowest common denominator that all cards can support. The newer SDKs come with the spreadsheet of newer cards while the older versions of DXSDK come with the spreadsheet of older cards. It seems MS is keeping updating it. You can choose based on your needs. Thankyou that's perfect.  DirectX 9 was last updated (DirectX 9.0C) in 2004 and that was a fairly minor update. The original release was in 2002. IMO it's not worth even trying to work around missing DirectX 9 capabilities -- anybody whose card isn't capable of DirectX 9 is long-since accustomed to only ancient games working at all. Maybe I didn't explain very well... I assume there is a DirectX 9 driver installed the question is about what features exist in the hardware. This is because DirectX only exposes features that actually exist and (unlike DirectX 10) there's no minimum feature set that all cards are guaranteed to support. I've edited the question to try to clarify it...  Chris Dragon's Direct3D and OpenGL Device Capabilities Database is extremely well-done but a bit out of date."
759,A,"How would you implement a perfect line-of-sight algorithm? Disclaimer: I'm not actually trying to make one I'm just curious as to how it could be done. When I say ""Most Accurate"" I include the basics wall distance light levels and the more complicated Dust in Atmosphere rain sleet snow clouds vegetation smoke fire If I were to want to program this what resources should I look into and what things should I watch out for? Also are there any relevant books on the theory behind line of sight including all these variables? It's about an algorithm. It certainly can be programmed... You should start by reading some physics books I think. It's a very broad question though. Entire books have been written on particle systems alone. This *is* programming related. It's graphics programming. Someone should edit it and clarify. @ Michael Donohue: I think this is a question about how to design a ""realistic"" AI for computer controlled characters in a game - but I could be wrong... Any books in particular? certainly it has potential relevance in the realm of programming; could be for a game might be for GPS or missile guidance possibly other less-obvious applications @Michael are you saying the study of algorithms is not programming related? That's very strange because there are a large number of programming related books which concentrate on algorithms. And I would believe that Computer Science theory is also programming related. So where is this question not programming related? Even if it doesn't directly involve code it can impact coding implementations and further questions. I do have to say though I believe this belongs on CW because it asks for ""good"" books which is subjective. What's an algorithm? There is no ""one algorithm"" for these since the inputs are not well defined. If you treat Dust-In-Atmosphere as a constant value then there is an algorithm that can take it into account but the fact is that dust levels will vary from point to point and thus the algorithm you want needs to be aware of how your dust-data is structured. The most used algorithm in todays ray-tracers is just incremental ray-marching which is by definition not correct but it does approximate the Ultimate Answer to a fair degree. Even if you managed to incorporate all these properties into a single master-algorithm you'd still have to somehow deal with how different people perceive the same setting. Some people are near-sighted some far-sighted. Then there's the colour-blind. Not to mention that Dust-In-Atmosphere levels also affect tear-glands which in turn affects visibility. And then there's the whole dichotomy between what people are actually seeying and what they think they are seeying... There are far too many variables here to aim for a unified solution. Treat your environment as a voxelated space and shoot your rays through it. I suspect that's the only solution you'll be able to complete within a single lifetime...  The obvious question is do you really want the most accurate and why? I've worked on games that depended on line of sight and you really need to think clearly about what kind of line of sight you want. First can the AI see any part of your body? Or are you talking about ""eye to eye"" LOS? Second if the player's camera view is not his avatar's eye view the player will not perceive your highly accurate LOS as highly accurate. At which point inaccuracies are fine. I'm not trying to dissuade you but remember that player experience is #1 and that might mean not having the best LOS. A good friend of mine has done the AI for a long=-running series of popular console games. He often tells a story about how the AIs are most interesting (and fun) in the first game because they stumble into you rather than see you from afar. Now he has great LOS and spends his time trying to dumb them down to make them as fun as they were in the first game. So why are you doing this? Does the game need it? Or do you just want the challenge? The challenge was a big part of it the thought came to me when I was looking into Weather forecasting models and wondered how hard it would be to create a line of sight algorithm to work with them.  I personally don't know too much about this topic but a quick couple of Google searches turns up some formal papers that contain some very relevant information: http://www.tecgraf.puc-rio.br/publications/artigo_1999_efficient_lineofsight_algorithms.pdf - Provides a detailed description of two different methods of efficiently performing an LOS calculation along with issues involved http://www.agc.army.mil/operations/programs/LOS/LOS%20Compendium.doc - This one aims to maintain ""a current list of unique LOS algorithms""; it has a section listing quite a few and describing them in detail with a focus on military applications. Hope this helps!  Typically one represents the world as a set of volumes of space held in some kind of space partitioning data structure then intersects the ray representing your ""line of sight"" with that structure to find the set of objects it hits; these are then walked in order from ray origin to determine the overall result. Reflective objects cause further rays to be fired opaque objects stop the walk and semitransparent objects partially contribute to the result. You might like to read up on ray tracing; there is a great body of literature on the subject and well-understood ways of solving what are basically the same problems you list exist. most ray tracing resources that I've seen have focused soley on what I listed as the basics and I couldn't find anything related to any of the advanced Just think of all those things as semitransparent objects. So smoke is a volume of space that contributes an amount of opacity proportional to the length of the section of ray that crosses it etc. For actual raytracing you'd also need to read up on subjects like procedural noise generation but for LOS you're just interested in opacity/reflectiveness right?"
760,A,"Drawing ""point-like"" shapes in OpenGL indifferent to zoom I'm working with Qt and QWt3D Plotting tools and extending them to provide some 3-D and 2-D plotting functionality that I need so I'm learning some OpenGL in the process. I am currently able to plot points using OpenGL but only as circles (or ""squares"" by turning anti-aliasing off). These points act the way I like - i.e. they don't change size as I zoom in although their x/y/z locations move appropriately as I zoom pan etc. What I'd like to be able to do is plot points using a myriad of shapes (^<>*. etc.). From what I understand of OpenGL (which isn't very much) this is not trivial to accomplish because OpenGL treats everything as a ""real"" 3-D object so zooming in on any openGL shape but a ""point"" changes the object's projected size. After doing some reading I think there are (at least) 2 possible solutions to this problem: Use OpenGL textures. This doesn't seem to difficult but I believe that the texture images will get larger and smaller as I zoom in - is that correct? Use OpenGL polygons lines etc. and draw *'s triangles or whatever. But here again I run into the same problem - how do I prevent OpenGL from re-sizing the ""points"" as I zoom? Is the solution to simply bite the bullet and re-draw the whole data set each time the user zooms or pans to make sure that the points stay the same size? Is there some way to just tell openGL to not re-calculate an object's size? Sorry if this is in the OpenGL doc somewhere - I could not find it. The scene is re-drawn every time the user zooms or pans anyway so you might as well re-calculate the size. You suggested using a textured poly or using polygons directly which sound like good ideas to me. It sounds like you want the plot points to remain in the correct position in the graph as the camera moves but you want to prevent them from changing size when the user zooms. To do this just resize the plot point polygons so the ratio between the polygon's size and the distance to the camera remains constant. If you've got a lot of plot points computing the distance to the camera might get expensive because of the square-root involved but a lookup table would probably solve that. In addition to resizing you'll want to keep the plot points facing the camera so billboarding is your solution there. An alternative is to project each of the 3D plot point locations to find out their 2D screen coordinates. Then simply render the polygons at those screen coordinates. No re-scaling necessary. However gluProject is quite slow so I'd be very surprised if this wasn't orders of magnitude slower than simply rescaling the plot point polygons like I first suggested. Good luck!  There's no easy way to do what you want to do. You'll have to dynamically resize the primitives you're drawing depending on the camera's current zoom. You can use a technique known as billboarding to make sure that your objects always face the camera.  What you want is called a ""point sprite."" OpenGL1.4 supports these through the ARB_point_sprite extension. Try this tutorial http://www.ploksoftware.org/ExNihilo/pages/Tutorialpointsprite.htm and see if it's what you're looking for."
761,A,"In MATLAB how do I plot to an image and save the result without displaying it? This question kind of starts where this question ends up. MATLAB has a powerful and flexible image display system which lets you use the imshow and plot commands to display complex images and then save the result. For example: im = imread('image.tif'); f = figure imshow(im 'Border' 'tight'); rectangle('Position' [100 100 10 10]); print(f '-r80' '-dtiff' 'image2.tif'); This works great. The problem is that if you are doing a lot of image processing it starts to be real drag to show every image you create - you mostly want to just save them. I know I could start directly writing to an image and then saving the result. But using plot/rectangle/imshow is so easy so I'm hoping there is a command that can let me call plot imshow etc not display the results and then save what would have been displayed. Anyone know any quick solutions for this? Alternatively a quick way to put a spline onto a bitmap might work... When you create the figure you set the Visibile property to Off. f = figure('visible''off') Which in your case would be im = imread('image.tif'); f = figure('visible''off') imshow(im 'Border' 'tight'); rectangle('Position' [100 100 10 10]); print(f '-r80' '-dtiff' 'image2.tif'); And if you want to view it again you can do set(f'visible''on')  I'm expanding on Bessi's solution here a bit. I've found that it's very helpful to know how to have the image take up the whole figure and to be able to tightly control the output image size. % prevent the figure window from appearing at all f = figure('visible''off'); % alternative way of hiding an existing figure set(f 'visible''off'); % can use the GCF function instead % If you start getting odd error messages or blank images % add in a DRAWNOW call. Sometimes it helps fix rendering % bugs especially in long-running scripts on Linux. %drawnow; % optional: have the axes take up the whole figure subplot('position' [0 0 1 1]); % show the image and rectangle im = imread('peppers.png'); imshow(im 'border''tight'); rectangle('Position' [100 100 10 10]); % Save the image controlling exactly the output % image size (in this case making it equal to % the input's). [HWD] = size(im); dpi = 100; set(f 'paperposition' [0 0 W/dpi H/dpi]); set(f 'papersize' [W/dpi H/dpi]); print(f sprintf('-r%d'dpi) '-dtiff' 'image2.tif'); If you'd like to render the figure to a matrix type ""help @avifile/addframe"" then extract the subfunction called ""getFrameForFigure"". It's a Mathworks-supplied function that uses some (currently) undocumented ways of extracting data from figure.  Here is a completely different answer: If you want an image file out why not just save the image instead of the entire figure? im = magic(10) imwrite(im/max(im(:))'magic.jpg') Then prove that it worked. imshow('magic.jpg') This can be done for indexed and RGB also for different output formats.  The simple answer to your question is given by Bessi and Mr Fooz: set the 'Visible' setting for the figure to 'off'. Although it's very easy to use commands like IMSHOW and PRINT to generate figures I'll summarize why I think it's not necessarily the best option: As illustrated by Mr Fooz's answer there are many other factors that come into play when trying to save figures as images. The type of output you get is going to be dependent on many figure and axes settings thus increasing the likelihood that you will not get the output you want. This could be especially problematic if you have your figures set to be invisible since you won't notice some discrepancy that could be caused by a change in a default setting for the figure or axes. In short your output becomes highly sensitive to a number of settings that you would then have to add to your code to control your output as Mr Fooz's example shows. Even if you're not viewing the figures as they are made you're still probably making MATLAB do more work than is really necessary. Graphics objects are still created even if they are not rendered. If speed is a concern generating images from figures doesn't seem like the ideal solution. My suggestion is to actually modify the image data directly and save it using IMWRITE. It may not be as easy as using IMSHOW and other plotting solutions but I think it is more efficient and gives more robust and consistent results that are not as sensitive to various plot settings. For the example you give I believe the alternative code for creating a black rectangle would look something like this: im = imread('image.tif'); [rcd] = size(im); x0 = 100; y0 = 100; w = 10; h = 10; x = [x0:x0+w x0*ones(1h+1) x0:x0+w (x0+w)*ones(1h+1)]; y = [y0*ones(1w+1) y0:y0+h (y0+h)*ones(1w+1) y0:y0+h]; index = sub2ind([r c]yx); im(index) = 0; im(index+r*c) = 0; im(index+2*r*c) = 0; imwrite(im'image2.tif');"
762,A,"Sun Raster images: Why 1 byte row padding when width is odd? This may be waaay to specific for SO but there seems to be a dearth of info on the sun raster standard. (Even JWZ is frustrated by this!) Intro: The Sun raster standard says that rows of pixels have padding at the end such that the number of bits in a row is a factor of 16 (i.e. an even number of bytes). For example if you had a 7-pixel-wide 24-bit image a row would normally take 7 * 3 = 21 bytes but sun raster would pad it to 22 bytes so the number of bits is divisible by 16. The code below achieves this for 24-bit images of arbitrary width: row_byte_length = num_cols * 3; row_byte_length += width_in_bytes % 2; Here's my question: both Imagemagick and Gimp follow this rule for 24-bit images but for 32-bit images it does something weird that I don't understand. Since the bit depth gives 4-byte pixels any image width would take an even number of bytes per row which always complies with the ""16-bit alignment"" rule. But when they compute the row length they add an extra byte for images with odd widths making the row length odd (i.e. the number of bits for the row is not divisible by 16). The code below describes what they're doing for 32-bit images: row_byte_length = num_cols * 4 + num_cols % 2; Adding one appears to go against the ""16-bit alignment"" rule as specified by the sun format and is done with no apparent purpose. However I'm sure if Gimp and Imagemagick do it this way I must be misreading the sun raster spec. Are there any Sun raster experts out there who know why this is done? Edit My mistake Gimp only outputs up to 24 bit Sun raster. Looks like this is only an Imagemagick issue so probably a bug. I'm labeling this for closure; better to discuss on the ImageMagick forums. I'd say the image loading code in Gimp and ImageMagick has a bug. Simple as that. Keep in mind that the SUN-Raster format isn't that widely used. It's very possible that you're one of the first who actually tried to use this format found out that it doesn't work as expected and not ignored it. If the spec. sais something along the lines: Regardless of width the stored scanlines are rounded up to multiples of 16 bits then there isn't much room for interpretation. Turned out to be an ImageMagick bug. I was mistaken Gimp's implementation is compliant with the spec.  It seems like a mistake to me too. I even wonder if Sun even supports 32-bits RAS files. Basically a 32-bits image would most likely add an alpha channel as ""fourth"" colour to support transparency. But like many image file formats it's a bit old and others have made adjustments to the format to support 32-bits images. So I think that whomever added the 32-bits support just implemented it wrong and ever since we have to live with that decision. Compare it with the referer misspelling that has become part of the HTTP standard. :-) A mistake that has become part of a new standard."
763,A,"VB6 Game Development I am developing a game in VB6 (plz don't ask me why :) ). The storyboard is ready and a rough implementation is underway. I am following a ""pure-software-rendering"" approach. (i.e. no DirectX no openGL etc.) Amongst many others the following ""serious"" problems exist: 2D alpha transparency reqd. to implement overlays. Parallax implementation to give depth-of-field illusion. Capturing mouse-scroll events globally (as in FPS-es; mapping them to changing weapon). Async sound play with absolute ""near-zero-lag"". Any ideas anyone. Please suggest any well documented library/ocx or sample-code. Plz do suggest solutions with good performance and as little overhead as possible. Also anyone who has developed any games and would be open to sharing her/his code would be highly appreciated. (any well-acknowledged VB games whose source-code i can study??) UPDATE: Here is a screen shot of GearHead Garage. This picture ought to describe what i was attempting in words above... :) You want fries with that? ;) @Robusto Sorry if i sounded too rude in asking for help. But anything U could suggest in solving these issues would be of great help... Thank you. I didn't think that was rude -- I thought that response was because you are asking for the world. Won't threading be somewhat of a concern? @ RobS I don't think (-read hope with my fingers crossed) so. The game is a single-player offline game with no NPCs An alternative to fixing these issues would be to use XNA with VB.NET to minimize the amount of code you would have to rewrite. For audio playback I have used http://www.fmod.org/ in the past. This and other libraries like BASS are only free for non-commercial use. I also suggest avoiding the built-in multimedia playback object.  Have a look at DxIce : http://gamedev.digiapp.com/  EGL25 by Erkan Sanli is a fast open source VB 6 renderer that can render rotate animate etc. complex solid shapes made of thousands of polygons. Just Windows API calls – no DirectX no OpenGL. VBMigration.com chose EGL25 as a high-quality open-source VB6 project (to demonstrate their VB6 to VB.Net upgrade tool). Despite that and despite my opinion that VB6 is often criticised too harshly I can't help thinking there must be better options for game development in 2010? I never heard about EGL25 before. +1 Thanks for sharing. how do i create the objects i.e. models for EGL25?? I have no idea you'll have to download it and find out.  You may want to check out the Game Programming Wiki -- it used to be ""Lucky's VB Game Site"" (and we're talking a LONG time ago) but all of the content (VB5/6 centric) moved to the Wiki with the addition of other languages. It appears that much of the legacy VB6 content is still available on the site. Thanks. Been following the site for a while now... Good for starters-->intermediate.  I think you will find no well-acknowledged written games in VB6 for precisely the reasons you state above. It was not designed to be a high performance language. For that you NEED to use the graphics libraries (DirectX OpenGL) you said you didn't want to use unless you want to BitBLT everything yourself using API calls which is probably not going to get what you need. VB6 is interpreted outdated and I'd be surprised if it runs on Windows 7. I think you need to seriously re-evaluate the methodology here. @MarkJ - are you sure it is ""fully supported"" by Microsoft? Or are you thinking of VB.Net? Everything I've read indicates that official VB6 support ended back in 2005. Also VB6 can be interpreted or native-compiled according to developer choice so it's not a ""is/is-not"" so much as a ""maybe/maybe-not"". In my earlier attempts at game-dev in i had using wav and mid files embedded in the res & was able to get almost zero-lag audio effects. Also i have used bitblt and a crude parallax implemenatation in code while scrolling along one direction. Bitblitting is fine as a last resort for me. I am trying to save the time and effort and of meddling in the low-level code if i can. I hav no idea to implement Capturing mouse-scroll events globally though... thanks for your concern @MarkJ You were right VB6 can be compiled to native code but it does compile to P-Code by default. http://msdn.microsoft.com/en-us/library/aa240840(VS.60).aspx It also looks like MS will maintain an ""it just works"" philosophy on Win 7 but will not be officially supported. http://msdn.microsoft.com/en-us/vbrun/ms788708.aspx @Jeremy If you read a bit further down: ""The core Visual Basic 6.0 runtime will be supported for the full lifetime of... Windows 7"". Like it says in this Stackoverflow answer from Feb 09 by well erm cough blush me :) http://stackoverflow.com/questions/447007/will-windows-7-support-the-vb6-runtime/592740#592740 @GalacticCowboy yes I am sure VB6 is supported see the other comments which link to the official Microsoft statement. VB6 is not interpreted it is compiled to native code. It runs on Windows 7 - in fact it is fully supported by Microsoft on Windows 7. Still I won't give you -1 because choosing VB6 to develop a 3D game is pretty odd and I wouldn't do it myself. But people have done odd things - see my answer for an open source VB6 3D rendering engine http://stackoverflow.com/questions/2690656/vb6-game-development/2692088#2692088"
764,A,Write text along a curve in Java I was wondering if it were possible to fit text along the curve of a circle. It would be great if there were a way to accomplish this in Java2D. Draw text along a curve with just awt and swing libraries.
765,A,How do I stop a System::Windows::Forms::UserControl from erasing it's background? I have a C++/CLI System::Windows::Forms::UserControl derived control which should only redraw (a small portion of) itself as new data is fed into it. For some reason though the OnPaint mechanism is being called even when there's nothing to cause it external to the app. Here's a snippet: void Spectrogram::OnPaint(System::Windows::Forms::PaintEventArgs ^e) { // Overidden to stop the background being painted(?) } void Spectrogram::AddNewFFTData( float* data int fataWidth ) { Graphics^ gfx = CreateGraphics(); //now do some drawing gfx->Dispose(); } So I call the add data method to add some new data which should in theory write over the previous entry (which clears some highlighting) and write the new entry. Back in the day I used to develop MFC/OpenGL apps and one of the first things I'd do would be to override the OnEraseBackground method. As far as I can see though there's no obvious way of stopping it being erased. What have I missed? You may be looking for Control.OnPaintBackground(). I've had to override that to do nothing for a custom control I wrote to bring a legacy MFC control into a Winforms project. Otherwise it would paint the background on top of the MFC control's paint job. Essentially in the .cpp: void MyControl::OnPaintBackground(System::Windows::Forms::PaintEventArgs ^pevent) { // Do nothing we don't want to paint over the native control. // You may want to do something a little fancier for DesignMode // if you use the winforms designer though. } On the prototype: protected: void virtual OnPaintBackground(System::Windows::Forms::PaintEventArgs ^pevent) override; What rectangle is being passed in to you via the event args? Is the entire control being invalidated or just a portion of it? +1: That's excellent - exactly what I was looking for. One more thing to remember though; make sure you've disabled double-buffering or the buffers will occasionally swap and erase everything. Oh and it was the entire control being invalidated. Awesome I'm glad that helped. :) I was just wondering what was being invalidated because that can be a clue as to what was causing the invalidation.  Maybe it's a statement like this in the Form's constructor: //do own background painting this.SetStyle(ControlStyles.Opaque true); I think that prevents OnPaintBackground being invoked at all: so you don't need to override OnPaintBackground and instead you can erase the background (or not) yourself in your OnPaint. +1: I just tested and you're right setting that stops the background being erased - I wish I could accept both answers! +1: Cool didn't know about that one. :)  I did some stuff with the OnPaint lately (C# if that matters) and I noticed it literally is drawn when a area of the control is revealed. A better solution is to draw on a cached Bitmap and draw it to the control every time dotNet asks for it.
766,A,"C# Drawing Strange Behaviour the following code is fairly straight forward - it fills a design surface with randomnly selected pixels - nothing special (ignore the XXXXXXX's in the 2nd method for now). private void PaintBackground() { Random rnd = new Random(); Bitmap b = new Bitmap(this.Width this.Height); for (int vertical = 0; vertical < this.Height; vertical++) { for (int horizontal = 0; horizontal < this.Width; horizontal++) { Color randomColour = GetRandomColor(rnd); b.SetPixel(horizontal vertical randomColour); } } Graphics g = this.CreateGraphics(); g.DrawImage(b new Point(0 0)); } public Color GetRandomColor(Random rnd) { XXXXXXXXXXXXXXXX byte r = Convert.ToByte(rnd.Next(0 255)); byte g = Convert.ToByte(rnd.Next(0 255)); byte b = Convert.ToByte(rnd.Next(0 255)); return Color.FromArgb(255 r g b); } The question i have is this... if you replace the XXXXXXXXX with ""Random rnd = new Random();"" the test pattern completely changes into horizontal bars of the same colour and is therefore not random. Come someone explain to me why this is? As far as I can tell the only difference in the second attempt is that the GetRandomColour method creates and uses a new instance of the Random class but I don't see how that makes horizontal bars.. Random when created have a default seed zero. Recreating it in that function will always give the same number. Create it in constructor and than reuse to get different random numbers. Wrong! The seed is based on something like tickcount else you would end up with a real crappy random class. You right i was wrong. I remember that when i was implementing a Particle swarm optimizer and had almost same problem. But since sequences were almost identical for each particle i concluded that it must be same. Thanks for the info.  Randoms aren't really random. They're ""Psuedo-random"". All you're really doing (from the machine standpoint) is generating the same random at the origination point over and over again. What you really need to do is either pass the constructor a ""seed"" or have a higher scoped random that you can call the Next() method. so in attempt #2 - by creating a new instance and asking it for next(0 255) it will be the same return value everytime? that seems hard to believe.. ? Yes because they all have the same start point. And the randomness algorithm isn't random at all. Like the article says it's ""psuedo-random"". It has a definite algorithm so given the same start point same seed you'll achieve the same result every time.  Your application runs so fast that the seed the PRNG is being initialized with stays the same throughout the entire loop. Thus it is not truly random hence the name Pseudo Random Number Generator. Wouldnt there be slight variations in the horizontal bars though? they're almost perfectly aligned Nope the RNG is initialized with the same seed so it will generate the same results. Your vision just sees this as lines.  From MSDN: The random number generation starts from a seed value. If the same seed is used repeatedly the same series of numbers is generated. One way to produce different sequences is to make the seed value time-dependent thereby producing a different series with each new instance of Random. By default the parameterless constructor of the Random class uses the system clock to generate its seed value while its parameterized constructor can take an Int32 value based on the number of ticks in the current time. However because the clock has finite resolution using the parameterless constructor to create different Random objects in close succession creates random number generators that produce identical sequences of random numbers. The following example illustrates that two Random objects that are instantiated in close succession generate an identical series of random numbers. So given the same seed the Random instance will produce the same sequence of numbers. And in your example due to the finite resolution of the system clock the Random instances were created using the same tick count as seed resulting in the same sequence. The consecutive calls to GetRandomColor() are executed within one time slice of the system clock. To test this try slowing the method down with Thread.Sleep(1). You should see different colors being generated. I was too slow deleted my post and gave you +1  See this article. (Kidding)"
767,A,Problem in generating the border of a rectangle in Java? I am using java.awt.geom.Rectangle2D.Double class to generate a rectangle. I want to generate a rectangle which is filled with a color (say green) and have a border (outline). Now the problem is if I call g2.draw(new Rectangle2D.Double(....)); // g2 is an instance of Graphics2D then it doesn't fill the rectangle and when I call g2.fill(new Rectangle2D.Double(....)); // g2 is an instance of Graphics2D then id doesn't generate border. Dan and Samuel are both right. It's logical too. `fill` fills the entire rectangular area including the area occupied by the border you just drew. Time to pull your foot out of the line of fire :) To do this render the rectangle twice first the fill and then the border (draw). Rectangle2D rect = new Rectangle2D.Double(...); g2.setColor(Color.white); g2.fill(rect); g2.setColor(Color.black); g2.draw(rect);  How about doing both? Draw the filled rectangle first and then draw the outline one over the top.
768,A,"Beginner's guide to 3D graphics programming What are the best guides / tutorials / books / websites for someone with minimal experience (or none) in the world of 3D graphics programming? I realize that the fundamentals of 3D graphics and mathematics apply across platform specific 3D library implementations such as OpenGL DirectX WPF etc.. Therefore it would be useful if answers would explain if they focus on a specific library implementation on the fundamentals or maybe both. Rationale for for asking this question: With Windows Presentation Foundation (WPF) 3D on the scene it's realistic for many programmers to now seriously consider using 3D for their applications where this would have been almost impossible even a few years ago. I'm sure there are many programmers out there like me who find the leap from 2D to 3D a very big one. A good handle on the math behind things can be useful. This tutorial is a good place to start.  I recommend that you implement a simple software based 3d rendering engine. Simple stuff like line quads lighting etc. You will learn a whole lot more about 3d programming in general and it will give you a good prescriptive on 3d graphics and it's limitations. This should get you started: http://www.devmaster.net/articles/software-rendering/part1.php Brilliant...was looking for something just like this +1  One book I'd definitely recommend is Computer Graphics by Foley and Van Dam.  I chose the easy route. I am using an available engine - www.3dgamestudio.com - so I can focus on the actual game. I feel that reinventing the wheel takes too much time.  One site I have been recommended previously is GameDev. It is full of articles and tutorials for 3D game development. Looks useful thanks.  What do you want to learn to do? build a graphics / game engine? or USE a graphics or game engine? .. This should really have been a comment. Interested in USING a graphics engine. We already have DirectX OpenGL WPF etc no need to reinvent the wheel.  Petzold's 3D Programming for Windows is an obvious start if you are doing WPF. For Opengl the book is available free online  there are also tutorials at NeHe although OpenGL does require that you understand the details of transforms to really do more than cut and paste +1 I actually own a copy. It is great for the detail but it might be a bit advanced in certain areas for beginners. But for a beginners guide - the details of shaders and VBOs are probably unecessary. The free OpenGL ""redbook"" is several revision behind the current edition. While many of the basics are the same there are lots of bits in OpenGL 3.x worth learning.  Here is a good hands-on tutorial for getting started quickly with a little bit of mathematical theory included: http://www.kindohm.com/technical/wpf3dtutorial.htm  I thought I'd start by providing this resource I found during my own research: The Twelve days of WPF 3D by Eric Sink. It is a series of articles focusing on WPF from the beginner to intermediate level. It focuses on getting practical things done with WPF 3D rather then fundamental 3D math etc but is great for answering some common questions most WPF 3D programmers eventually come across."
769,A,"Convert Quaternion rotation to rotation matrix? Basically given a quaterion (qx qy qz qw)... How can i convert that to an OpenGL rotation matrix? I'm also interested in which matrix row is ""Up"" ""Right"" ""Forward"" etc... I have a camera rotation in quaternion that I need in vectors... One way to do it which is pretty easy to visualize is to apply the rotation specified by your quaternion to the basis vectors (100) (010) and (001). The rotated values give the basis vectors in the rotated system relative to the original system. Use these vectors to form the rows of the rotation matrix. The resulting matrix and its transpose represent the forward and inverse transformations between the original system and the rotated system. I'm not familiar with the conventions used by OpenGL so maybe someone else can answer that part of your question... Yeah I forgot I could do this... I'll give it a go Mathematically correct but computationally more expensive to do it like this. I would test it because this version uses instructions that some hardware (I am specifically thinking of GPU shaders) might be specifically optimized for so while probable I would not be so sure it is more computationally expensive...  using glm you can simply use a casting operator. so to convert from a matrix4 to quaternion simply write glm::mat4_cast(quaternion_name)  You might not have to deal with a rotation matrix at all. Here is a way that appears to be faster than converting to a matrix and multiplying a vector with it:  // move vector to camera position co (before or after rotation depending on the goal) v -= co; // rotate vector v by quaternion q; see info [1] vec3 t = 2 * cross(q.xyz v); v = v + q.w * t + cross(q.xyz t); [1] http://mollyrocket.com/forums/viewtopic.php?t=833&sid=3a84e00a70ccb046cfc87ac39881a3d0  The following code is based on a quaternion (qw qx qy qz) where the order is based on the Boost quaternions: boost::math::quaternion<float> quaternion; float qw = quaternion.R_component_1(); float qx = quaternion.R_component_2(); float qy = quaternion.R_component_3(); float qz = quaternion.R_component_4(); First you have to normalize the quaternion: const float n = 1.0f/sqrt(qx*qx+qy*qy+qz*qz+qw*qw); qx *= n; qy *= n; qz *= n; qw *= n; Then you can create your matrix: Matrix<float 4>( 1.0f - 2.0f*qy*qy - 2.0f*qz*qz 2.0f*qx*qy - 2.0f*qz*qw 2.0f*qx*qz + 2.0f*qy*qw 0.0f 2.0f*qx*qy + 2.0f*qz*qw 1.0f - 2.0f*qx*qx - 2.0f*qz*qz 2.0f*qy*qz - 2.0f*qx*qw 0.0f 2.0f*qx*qz - 2.0f*qy*qw 2.0f*qy*qz + 2.0f*qx*qw 1.0f - 2.0f*qx*qx - 2.0f*qy*qy 0.0f 0.0f 0.0f 0.0f 1.0f); Depending on your matrix class you might have to transpose it before passing it to OpenGL. fyi this looks like a row-major interpretation for anyone looking at this. It should work-as is when passing to OpenGL since column-major and row-major result in same linear array. However if using this with a column-major library to multiply with another matrix you could get issues depending on how your library works. Nice. So boost can do pretty much everything is what I'm finding."
770,A,"Sort Four Points in Clockwise Order Four 2D points in an array. I need to sort them in clockwise order. I think it can be done with just one swap operation but I have not been able to put this down formally. Edit: The four points are a convex polygon in my case. Edit: The four points are the vertices of a convex polygon. They need not be in order. Do you mean clockwise around the origin? Not necessarily. Or it could be clockwise around bondary center. To me clockwise order for a convex polygon means that you only make right hand turns when traversing the points. The origin should not matter. If we are talking about a clockwise polygon check my edit regarding decomosing to two triangles and swapping on that basis. This is a way simpler proposition. if AB crosses CD swap BC elif AD crosses BC swap CD if area (ABC) > 0 swap BD (I mean area(ABC) > 0 when A->B->C is counter-clockwise). Let p*x + q*y + r = 0 be the straight line that joins A and B. Then AB crosses CD if p*Cx + q*Cy + r and p*Dx + q*Dy + r have different sign i.e. their product is negative. The first 'if/elif' brings the four points in clockwise or counterclockwise order. (Since your polygon is convex the only other 'crossing' alternative is 'AC crosses BD' which means that the four points are already sorted.) The last 'if' inverts orientation whenever it is counter-clockwise. In order words 3 1 2 is already in the correct order here. Ok I'm wrong but not for the reason you mentioned. My idea was this: sorting A312 (only a relabeling of ADBC) in order to give A123 is the same as sorting 312 in ascending order. This is true in a sense but irrelevant here because in our setting I can move also A. Anyway why all this fuss about swaps? Swapping is indeed a fast operation and I would prefer to compute 2 areas and do 2 swaps rather than computing 3 areas in order to choose the only right swap that will do the magic. p.s. One swap is not sufficient to bring the four points in clockwise order. This would be the same as pretending to sort three numbers in ascending order with only one swap (3 1 2 what do you do?). Wrong. In the numbers example: 1 2 and 3 have fixed final positions. In the 4 points clockwise case we can have ABCD BCDA CDAB and DABC.  Wedge’s Answer is correct. To implement it easily I think the same way as smacl: you need to find the boundary’s center and translate your points to that center. Like this: centerPonintX = Min(x) + ( (Max(x) – Min(x)) / 2 ) centerPonintY = Min(y) + ( (Max(y) – Min(y)) / 2 ) Then decrease centerPointX and centerPointY from each poin in order to translate it to boundary’s origin. Finally apply Wedge’s solution with just one twist: Get the Absolute value of arctan(x/y) for every instance (worked for me that way). -1 because abs(arctan(x/y)) is wrong.  The following case would fail in you previous example A D B C By your previous algorithm you would swap BC to get A D C B which is not in clockwise order  var arr = [{x:3y:3}{x:4y:1}{x:0y:2}{x:5y:2}{x:1y:1}]; var reference = {x:2y:2}; arr.sort(function(ab) { var aTanA = Math.atan2((a.y - reference.y)(a.x - reference.x)); var aTanB = Math.atan2((b.y - reference.y)(b.x - reference.x)); if (aTanA < aTanB) return -1; else if (aTanB < aTanA) return 1; return 0; }); console.log(arr); Where reference point lies inside the polygon. More info at this site  Oliver is right. This code (community wikified) generates and sorts all possible combinations of an array of 4 points. #include <cstdio> #include <algorithm> struct PointF { float x; float y; }; // Returns the z-component of the cross product of a and b inline double CrossProductZ(const PointF &a const PointF &b) { return a.x * b.y - a.y * b.x; } // Orientation is positive if abc is counterclockwise negative if clockwise. // (It is actually twice the area of triangle abc calculated using the // Shoelace formula: http://en.wikipedia.org/wiki/Shoelace_formula .) inline double Orientation(const PointF &a const PointF &b const PointF &c) { return CrossProductZ(a b) + CrossProductZ(b c) + CrossProductZ(c a); } void Sort4PointsClockwise(PointF points[4]){ PointF& a = points[0]; PointF& b = points[1]; PointF& c = points[2]; PointF& d = points[3]; if (Orientation(a b c) < 0.0) { // Triangle abc is already clockwise. Where does d fit? if (Orientation(a c d) < 0.0) { return; // Cool! } else if (Orientation(a b d) < 0.0) { std::swap(d c); } else { std::swap(a d); } } else if (Orientation(a c d) < 0.0) { // Triangle abc is counterclockwise i.e. acb is clockwise. // Also acd is clockwise. if (Orientation(a b d) < 0.0) { std::swap(b c); } else { std::swap(a b); } } else { // Triangle abc is counterclockwise and acd is counterclockwise. // Therefore abcd is counterclockwise. std::swap(a c); } } void PrintPoints(const char *caption const PointF points[4]){ printf(""%s: (%f%f)(%f%f)(%f%f)(%f%f)\n"" caption points[0].x points[0].y points[1].x points[1].y points[2].x points[2].y points[3].x points[3].y); } int main(){ PointF points[] = { {5.0f 20.0f} {5.0f 5.0f} {20.0f 20.0f} {20.0f 5.0f} }; for(int i = 0; i < 4; i++){ for(int j = 0; j < 4; j++){ if(j == i) continue; for(int k = 0; k < 4; k++){ if(j == k || i == k) continue; for(int l = 0; l < 4; l++){ if(j == l || i == l || k == l) continue; PointF sample[4]; sample[0] = points[i]; sample[1] = points[j]; sample[2] = points[k]; sample[3] = points[l]; PrintPoints(""input: "" sample); Sort4PointsClockwise(sample); PrintPoints(""output: "" sample); printf(""\n""); } } } } return 0; } Separating out the signed area calculation as a function would improve readability. If performance is important then you can pull out all the expressions like (a.x*c.y - c.x*a.y) - you end up using them all twice.  I believe you're right that a single swap can ensure that a polygon represented by four points in the plane is convex. The questions that remain to be answered are: Is this set of four points a convex polygon? If no then which two points need to be swapped? Which direction is clockwise? Upon further reflection I think that the only answer to the second question above is ""the middle two"". -1 since your answer to the second question is wrong unless you assume the points are either in clockwise or anticlockwise order (consider the example I give below).  Calculate the area from the coordinates with the shoelace formula (devoided of the absolute value such that the area can be positive or negative) for every points permutations. The maximal area values seems to correspond to direct simple quadrilaterals:Simple direct quadrilaterals found with the shoelace formula  if we assume that point x is bigger than point y if the angle it has with the point (00) is bigger then we can implement this this way in c#  class Point : IComparable<Point> { public int X { set; get; } public int Y { set; get; } public double Angle { get { return Math.Atan2(X Y); } } #region IComparable<Point> Members public int CompareTo(Point other) { return this.Angle.CompareTo(other.Angle); } #endregion public static List<Point> Sort(List<Point> points) { return points.Sort(); } } Note that you calculate the same angle for (1 1) and (-1 -1). You want Math.Atan2(x y) that's right I fixed it now  You should take a look at the Graham's Scan. Of course you will need to adapt it since it finds to points counter-clockwise. p.s: This might be overkill for 4 points but if the number of points increase it could be interesting  If you want to take a more mathematical perspective we can consider the permutations of 4 points In our case there are 4 permutations that are in clockwise order A B C D B C D A C D A B D A B C All other possible permutations can be converted to one of these forms with 0 or 1 swaps. (I will only consider permutations starting with A as it is symmetrical) A B C D - done A B D C - swap C and D A C B D - swap B and C A C D B - swap A and B A D B C - swap A and D A D C B - swap B and D Thus only one swap is ever needed - but it may take some work to identify which. By looking at the first three points and checking the sign of the signed area of ABC we can determine whether they are clockwise or not. If they are clockwise then we are in case 1 2 or 5 to distinguish between these cases we have to check two more triangles - if ACD is clockwise then we can narrow this down to case 1 otherwise we must be in case 2 or 5. To pick between cases 2 and 5 we can test ABD We can check for the case of ABC anti-clockwise similarly. In the worst case we have to test 3 triangles. If your points are not convex you would find the inside point sort the rest and then add it in any edge. Note that if the quad is convex then 4 points no longer uniquely determine the quad there are 3 equally valid quads.  I have one further improvement to add to my previous answer remember - these are the cases we can be in. A B C D A B D C A C B D A C D B A D B C A D C B If ABC is anticlockwise (has negative signed area) then we are in cases 3 4 6. If we swap B & C in this case then we are left with the following possibilities: A B C D A B D C A B C D A B D C A D B C A D B C Next we can check for ABD and swap B & D if it is anticlockwise (cases 5 6) A B C D A B D C A B C D A B D C A B D C A B D C Finally we need to check ACD and swap C & D if ACD is anticlockwise. Now we know our points are all in order. This method is not as efficient as my previous method - this requires 3 checks every time and more than one swap; but the code would be a lot simpler.  A couple of thoughts worth considering here; Clockwise is only meaningful with respect to an origin. I would tend to think of the origin as the centre of gravity of a set of points. e.g. Clockwise relative to a point at the mean position of the four points rather than the possibly very distant origin. If you have four points abcd there exists multiple clockwise orderings of those points around your origin. For example if (abcd) formed a clockwise ordering so would (bcda) (cdab) and (dabc) Do your four points already form a polygon? If so it is a matter of checking and reversing the winding rather than sorting the points e.g. abcd becomes dcba. If not I would sort based on the join bearing between each point and the origin as per Wedges response. Edit: regarding your comments on which points to swap; In the case of a triangle (abc) we can say that it is clockwise if the third point c is on the right hand side of the line ab. I use the following side function to determine this based on the point's coordinates; int side(double x1double y1double x2double y2double pxdouble py) { double dx1dx2dy1dy2; double o; dx1 = x2 - x1; dy1 = y2 - y1; dx2 = px - x1; dy2 = py - y1; o = (dx1*dy2)-(dy1*dx2); if (o > 0.0) return(LEFT_SIDE); if (o < 0.0) return(RIGHT_SIDE); return(COLINEAR); } If I have a four point convex polygon (abcd) I can consider this as two triangles (abc) and (cda). If (abc) is counter clockwise I change the winding (abcd) to (adcb) to change the winding of the polygon as a whole to clockwise. I strongly suggest drawing this with a few sample points to see what I'm talking about. Note you have a lot of exceptional cases to deal with such as concave polygons colinear points coincident points etc... This only works if the two triangles you split it into do not intersect. See the second diagram in my post - ABC and CDA intersect each other and so making them clockwise does not fix the whole problem I consider (abcd) to be an ordered set of vertices or cycle such that the sides of the polygon are abbccd and da. I think this is reasonable as the opening question relates to a convex polygon which implies an ordering of the vertices. I meant the points are vertices of a convex polygon. I did not mean that they are in order.  How about this? // Take signed area of ABC. // If negative // Swap B and C. // Otherwise // Take signed area of ACD. // If negative swap C and D. Ideas? Swap opposite verts not adjacent ones. But otherwise yes. PS: You only need to do the second test if the signed area of ABC is zero (or within some epsilon of it). This doesn't work for some arrangements see my answer  If someone is interested here is my quick and dirty solution to a similar problem. My problem was to have my rectangle corners ordered in the following order: top-left > top-right > bottom-right > bottom-left Basically it is clockwise order starting from the top-left corner. The idea for the algorithm is: Order the corners by rows and then order corner-pairs by cols. // top-left = 0; top-right = 1; // right-bottom = 2; left-bottom = 3; List<Point> orderRectCorners(List<Point> corners) { if(corners.size() == 4) { ordCorners = orderPointsByRows(corners); if(ordCorners.get(0).x > ordCorners.get(1).x) { // swap points Point tmp = ordCorners.get(0); ordCorners.set(0 ordCorners.get(1)); ordCorners.set(1 tmp); } if(ordCorners.get(2).x < ordCorners.get(3).x) { // swap points Point tmp = ordCorners.get(2); ordCorners.set(2 ordCorners.get(3)); ordCorners.set(3 tmp); } return ordCorners; } return empty list or something; } List<Point> orderPointsByRows(List<Point> points) { Collections.sort(points new Comparator<Point>() { public int compare(Point p1 Point p2) { if (p1.y < p2.y) return -1; if (p1.y > p2.y) return 1; return 0; } }); return points; }  if( (p2.x-p1.x)*(p3.y-p1.y) > (p3.x-p1.x)*(p2.y-p1.y) ) swap( &p1 &p3 ); The '>' might be facing the wrong way but you get the idea.  Work it out the long way then optimize it. A more specific problem would be to sort the coordinates by decreasing angle relative to the positive x axis. This angle in radians will be given by this function: x>0 AND y >= 0 angle = arctan(y/x) AND y < 0 angle = arctan(y/x) + 2*pi x==0 AND y >= 0 angle = 0 AND y < 0 angle = 3*pi/2 x<0 angle = arctan(y/x) + pi Then of course it's just a matter of sorting the coordinates by angle. Note that arctan(w) > arctan(z) if and only if x > z so you can optimize a function which compares angles to each other pretty easily. Sorting such that the angle is monotonically decreasing over a window (or such that it increases at most once) is a bit different. In lieu of a an extensive proof I'll mention that I verified that a single swap operation will sort 4 2D points in clockwise order. Determining which swap operation is necessary is the trick of course. Second Wedge on both counts. ""In lieu of a an extensive proof I'll mention that I verified that a single swap operation will sort 4 2D points in clockwise order. Determining which swap operation is necessary is the trick of course."" How did you verify that? @Vulcan by inspection there are only so many possible ways to order 4 values I listed all of those ways and then determined how to sort them in the desired way for every case only one swap operation was needed. You can do this yourself write all the ways to order the numbers 1234. This arranges the points to be clockwise around the origin not in a clockwise quadrilateral. Plus arctan is (relatively) slow. note that in .NET Math.Atan2(x y) calculates your angle without all that mess. @Oliver well the real goal is to create an optimized solution which doesn't use atan at all."
771,A,"JPanel Layout Image Cutoff I am adding images to a JPanel but the images are getting cut off. I was originally trying BorderLayout but that only worked for one image and adding others added image cut-off. So I switched to other layouts and the best and closest I could get was BoxLayout however that adds a very large cut-off which is not acceptable either. So basically; How can I add images (from a custom JComponent) to a custom JPanel without bad effects such as the one present in the code. Custom JPanel: import java.awt.Color; import java.awt.Graphics; import java.awt.Graphics2D; import java.awt.event.ActionEvent; import java.awt.event.ActionListener; import java.awt.event.MouseEvent; import java.awt.event.MouseListener; import javax.swing.BoxLayout; import javax.swing.JPanel; import javax.swing.Timer; public class GraphicsPanel extends JPanel implements MouseListener { private Entity test; private Timer timer; private long startTime = 0; private int numFrames = 0; private float fps = 0.0f; GraphicsPanel() { test = new Entity(""test.png""); Thread t1 = new Thread(test); t1.start(); Entity ent2 = new Entity(""images.jpg""); ent2.setX(150); ent2.setY(150); Thread t2 = new Thread(ent2); t2.start(); Entity ent3 = new Entity(""test.png""); ent3.setX(0); ent3.setY(150); Thread t3 = new Thread(ent3); t3.start(); //ESSENTIAL COMMENT ANY OF THESE and you will see the problem immediately //You can use ANY image to reproduce the problem setLayout(new BoxLayout(this BoxLayout.X_AXIS)); add(test); add(ent2); add(ent3); //GAMELOOP timer = new Timer(30 new Gameloop(this)); timer.start(); addMouseListener(this); } @Override public void paintComponent(Graphics g) { super.paintComponent(g); Graphics2D g2 = (Graphics2D) g.create(); g2.setClip(0 0 getWidth() getHeight()); g2.setColor(Color.BLACK); g2.drawString(""FPS: "" + fps 1 15); } public void getFPS() { ++numFrames; if (startTime == 0) { startTime = System.currentTimeMillis(); } else { long currentTime = System.currentTimeMillis(); long delta = (currentTime - startTime); if (delta > 1000) { fps = (numFrames * 1000) / delta; numFrames = 0; startTime = currentTime; } } } public void mouseClicked(MouseEvent e) {} public void mousePressed(MouseEvent e) {} public void mouseReleased(MouseEvent e) {} public void mouseEntered(MouseEvent e) { } public void mouseExited(MouseEvent e) { } class Gameloop implements ActionListener { private GraphicsPanel gp; Gameloop(GraphicsPanel gp) { this.gp = gp; } public void actionPerformed(ActionEvent e) { try { gp.getFPS(); gp.repaint(); } catch (Exception ez) { } } } } Main class: import java.awt.EventQueue; import javax.swing.JFrame; public class MainWindow { public static void main(String[] args) { new MainWindow(); } private JFrame frame; private GraphicsPanel gp = new GraphicsPanel(); MainWindow() { EventQueue.invokeLater(new Runnable() { public void run() { frame = new JFrame(""Graphics Practice""); frame.setSize(680 420); frame.setVisible(true); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.add(gp); } }); } } Custom JComponent import java.awt.Color; import java.awt.Dimension; import java.awt.Graphics; import java.awt.Graphics2D; import java.awt.image.BufferedImage; import javax.imageio.ImageIO; import javax.swing.JComponent; public class Entity extends JComponent implements Runnable { private BufferedImage bImg; private int x = 0; private int y = 0; private int entityWidth entityHeight; private String filename; Entity(String filename) { this.filename = filename; } public void run() { bImg = loadBImage(filename); entityWidth = bImg.getWidth(); entityHeight = bImg.getHeight(); setPreferredSize(new Dimension(entityWidth entityHeight)); } @Override public void paintComponent(Graphics g) { super.paintComponent(g); Graphics2D g2d = (Graphics2D) g.create(); g2d.drawImage(bImg x y null); g2d.dispose(); } public BufferedImage loadBImage(String filename) { try { bImg = ImageIO.read(getClass().getResource(filename)); } catch (Exception e) { } return bImg; } public int getEntityWidth() { return entityWidth; } public int getEntityHeight() { return entityHeight; } public int getX() { return x; } public int getY() { return y; } public void setX(int x) { this.x = x; } public void setY(int y) { this.y = y; } } I could easily fix the problem by making entity load a bunch of components but I feel that this is probably most likely not a best practice. @camickr is likely right about why your existing approach isn't getting the results you want. As an alternative you might consider using JInternalFrame within a JDesktopPane. In this way your images would be documents that could be individually moved resized and scrolled. The article How to Use Internal Frames gives an idea of how such an implementation might look. This example shows a simple approach to staggering the frames and selecting them from a menu.  One thing I notice is that your preferred size is calculated incorrectly. You set the preferred size to be the size of the image. The problem is you paint the image at (x y). So the preferred size needs to take that into account. Otherwise I don't understand the question and running the code doesn't help since I don't know the size of your images whether they should be large small same size etc.. What is happening is when I add an object to GraphicsPanel the layout does not work correctly. For example the images compete for the custom JPanel space. Basically adding components is not as intuitive and easy as it should be with a regular jpanel. Also how do I take into count the x y position when setting a preferred size? All components are the same. You use the same code to a regular component or a custom component to a panel. This assumes you created the component correctly and set its properties. correctly. Normally when you add an icon to a label the image is painted at location (0 0). If in your custom component you are drawing the image at (x y) then the preferred size of the component increases by the x y amounts."
772,A,"OpenGL: scale then translate? and how? Hey all I've got some 2D geometry. I want to take some bounding rect around my geometry and then render a smaller version of it somewhere else on the plane. Here's more or less the code I have to do scaling and translation: // source and dest are arbitrary rectangles. float scaleX = dest.width / source.width; float scaleY = dest.height / source.height; float translateX = dest.x - source.x; float translateY = dest.y - source.y; glScalef(scaleX scaleY 0.0); glTranslatef(translateX translateY 0.0); // Draw geometry in question with its normal verts. This works exactly as expected for a given dimension when the dest origin is 0. But if the origin for say x is nonzero the result is still scaled correctly but looks like (?) it's translated to something near zero on that axis anyways-- turns out it's not exactly the same as if dest.x were zero. Can someone point out something obvious I'm missing? Thanks! FINAL UPDATE Per Bahbar's and Marcus's answers below I did some more experimentation and solved this. Adam Bowen's comment was the tip off. I was missing two critical facts: I needed to be scaling around the center of the geometry I cared about. I needed to apply the transforms in the opposite order of the intuition (for me). The first is kind of obvious in retrospect. But for the latter for other good programmers/bad mathematicians like me: Turns out my intuition was operating in what the Red Book calls a ""Grand Fixed Coordinate System"" in which there is an absolute plane and your geometry moves around on that plane using transforms. This is OK but given the nature of the math behind stacking multiple transforms into one matrix it's the opposite of how things really work (see answers below or Red Book for more). Basically the transforms are ""applied"" in ""reverse order"" to how they appear in code. Here's the final working solution: // source and dest are arbitrary rectangles. float scaleX = dest.width / source.width; float scaleY = dest.height / source.height; Point sourceCenter = centerPointOfRect(source); Point destCenter = centerPointOfRect(dest); glTranslatef(destCenter.x destCenter.y 0.0); glScalef(scaleX scaleY 0.0); glTranslatef(sourceCenter.x * -1.0 sourceCenter.y * -1.0 0.0); // Draw geometry in question with its normal verts. In OpenGL matrices you specify are multiplied to the right of the existing matrix and the vertex is on the far right of the expression. Thus the last operation you specify are in the coordinate system of the geometry itself. (The first is usually the view transform i.e. inverse of your camera's to-world transform.) Bahbar makes a good point that you need to consider the center point for scaling. (or the pivot point for rotations.) Usually you translate there rotate/scale then translate back. (or in general apply basis transform the operation then the inverse). This is called Change of Basis which you might want to read up on. Anyway to get some intuition about how it works try with some simple values (zero etc) then alter them slightly (perhaps an animation) and see what happens with the output. Then it's much easier to see what your transforms are actually doing to your geometry. Update That the order is ""reversed"" w.r.t. intuition is rather common among beginner OpenGL-coders. I've been tutoring a computer graphics course and many react in a similar manner. It becomes easier to think about how OpenGL does it if you consider the use of pushmatrix/popmatrix while rendering a tree (scene-graph) of transforms and geometries. Then the current order-of-things becomes rather natural and the opposite would make it rather difficult to get anything useful done. In short change the order to glTranslate(dest); glScale(scale); glTranslate(source); Adam: This doesn't seem to work as you suggest although I'm intrigued-- can you elaborate on why the ordering would go like that and perhaps offer a more explicit idea on how that would look relative to the existing code? Perhaps I'm just misinterpreting your pseudocode. Thanks! Nevermind: the ""doesn't work"" was a fatfingered value in this case. After fixing that we're in business. See updated question for more. Thanks!  I haven't played with OpenGL ES just a bit with OpenGL. It sounds like you want to transform from a different position as opposed to the origin not sure but can you try to do the transforms and draws that bit within glPushMatrix() and glPopMatrix() ? e.g. // source and dest are arbitrary rectangles. float scaleX = dest.width / source.width; float scaleY = dest.height / source.height; float translateX = dest.x - source.x; float translateY = dest.y - source.y; glPushMatrix(); glScalef(scaleX scaleY 0.0); glTranslatef(translateX translateY 0.0); // Draw geometry in question with its normal verts. //as if it were drawn from 00 glPopMatrix(); Here's a simple Processing sketch I wrote to illustrate the point: import processing.opengl.*; import javax.media.opengl.*; void setup() { size(500 400 OPENGL); } void draw() { background(255); PGraphicsOpenGL pgl = (PGraphicsOpenGL) g; GL gl = pgl.beginGL(); gl.glPushMatrix(); //transform the 'pivot' gl.glTranslatef(1001000); gl.glScalef(101010); //draw something from the 'pivot' gl.glColor3f(0 0.77 0); drawTriangle(gl); gl.glPopMatrix(); //matrix poped we're back to orginin(000) continue as normal gl.glColor3f(0.77 0 0); drawTriangle(gl); pgl.endGL(); } void drawTriangle(GL gl){ gl.glBegin(GL.GL_TRIANGLES); gl.glVertex2i(10 0); gl.glVertex2i(0 20); gl.glVertex2i(20 20); gl.glEnd(); } Here is an image of the sketch running the same green triangle is drawn with translation and scale applied then the red one outsie the push/pop 'block' so it is not affected by the transform: HTH George Thanks George but the push/pop isn't actually the issue; I'm already doing a push/pop outside the code that I made the snippet of. Thanks for illustrating with pictures though! A good visualization. ok so you want to grab a snapshot of a rectangular region and redraw that flat area as a smaller 'thumbnail' somewhere else on the screen ?  Scale just like Rotate operates from the origin. so if you scale by half an object that spans the segment [10:20] (on axis X e.g.) you get [5:10]. The object therefore was scaled and moved closer to the origin. Exactly what you observed. This is why you apply Scale first in general (because objects tend to be defined around 0). So if you want to scale an object around point Center you can translate the object from Center to the origin scale there and translate back. Side note if you translate first and then scale then your scale is applied to the previous translation which is why you probably had issues with this method. Thank you for the reply. Can you spell out for a matrix-math dimwit what operations I would do to do the appropriate translate-to-origin then scale then translate-back? I attempted stacking these three things in succession (translate by negative of source origin then scale by scale factors then translate by dest origin) in three calls before drawing the geometry but got no visible results. Ah perhaps I need to do the translations relative to the center of the rect rather than the origin. Will try this. Nope. No dice with: translate-by-negative-center-of-source then scale then translate-by-positive-center-of-dest. See update to question. Thanks."
773,A,"Point in Polygon aka hit test What's your best most elegant 2D ""point inside polygon"" or Polygon.contains(p:Point) algorithm? Edit: There may be different answers for floats vs integers. Primary goal is speed. You forgot to tell us about your perceptions on the question of right or left handedness - which can also be interpreted as ""inside"" vs ""outside"" -- RT Yes I realize now the question leaves many details unspecified but at this point I'm sorta interested in seeing the variety of responses. A 90 sided polygon is called a enneacontagon and a 10000 sided polygon is called a myriagon. ""Most elegant"" is out of the target since I have had trouble with finding a ""work at all""-algorithm. I must figure it out myself : http://stackoverflow.com/questions/14818567/point-in-polygon-algorithm-giving-wrong-results-sometimes/18190354#18190354 Eric Haines's survey of approaches Heres a point in polygon test in C that isn't using ray-casting. And it can work for overlapping areas (self intersections) see the use_holes argument. /* math lib (defined below) */ static float dot_v2v2(const float a[2] const float b[2]); static float angle_signed_v2v2(const float v1[2] const float v2[2]); static void copy_v2_v2(float r[2] const float a[2]); /* intersection function */ bool isect_point_poly_v2(const float pt[2] const float verts[][2] const unsigned int nr const bool use_holes) { /* we do the angle rule define that all added angles should be about zero or (2 * PI) */ float angletot = 0.0; float fp1[2] fp2[2]; unsigned int i; const float *p1 *p2; p1 = verts[nr - 1]; /* first vector */ fp1[0] = p1[0] - pt[0]; fp1[1] = p1[1] - pt[1]; for (i = 0; i < nr; i++) { p2 = verts[i]; /* second vector */ fp2[0] = p2[0] - pt[0]; fp2[1] = p2[1] - pt[1]; /* dot and angle and cross */ angletot += angle_signed_v2v2(fp1 fp2); /* circulate */ copy_v2_v2(fp1 fp2); p1 = p2; } angletot = fabsf(angletot); if (use_holes) { const float nested = floorf((angletot / (float)(M_PI * 2.0)) + 0.00001f); angletot -= nested * (float)(M_PI * 2.0); return (angletot > 4.0f) != ((int)nested % 2); } else { return (angletot > 4.0f); } } /* math lib */ static float dot_v2v2(const float a[2] const float b[2]) { return a[0] * b[0] + a[1] * b[1]; } static float angle_signed_v2v2(const float v1[2] const float v2[2]) { const float perp_dot = (v1[1] * v2[0]) - (v1[0] * v2[1]); return atan2f(perp_dot dot_v2v2(v1 v2)); } static void copy_v2_v2(float r[2] const float a[2]) { r[0] = a[0]; r[1] = a[1]; } Note: this is one of the less optimal methods since it includes a lot of calls to atan2f but it may be of interest to developers reading this thread (in my tests its ~23x slower then using the line intersection method).  David Segond's answer is pretty much the standard general answer and Richard T's is the most common optimization though therre are some others. Other strong optimizations are based on less general solutions. For example if you are going to check the same polygon with lots of points triangulating the polygon can speed things up hugely as there are a number of very fast TIN searching algorithms. Another is if the polygon and points are on a limited plane at low resolution say a screen display you can paint the polygon onto a memory mapped display buffer in a given colour and check the color of a given pixel to see if it lies in the polygons. Like many optimizations these are based on specific rather than general cases and yield beneifits based on amortized time rather than single usage. Working in this field i found Joeseph O'Rourkes 'Computation Geometry in C' ISBN 0-521-44034-3 to be a great help.  I too thought 360 summing only worked for convex polygons but this isn't true. This site has a nice diagram showing exactly this and a good explanation on hit testing: Gamasutra - Crashing into the New Year: Collision Detection  I realize this is old but here is a ray casting algorithm implemented in Cocoa in case anyone is interested. Not sure it is the most efficient way to do things but it may help someone out. - (BOOL)shape:(NSBezierPath *)path containsPoint:(NSPoint)point { NSBezierPath *currentPath = [path bezierPathByFlatteningPath]; BOOL result; float aggregateX = 0; //I use these to calculate the centroid of the shape float aggregateY = 0; NSPoint firstPoint[1]; [currentPath elementAtIndex:0 associatedPoints:firstPoint]; float olderX = firstPoint[0].x; float olderY = firstPoint[0].y; NSPoint interPoint; int noOfIntersections = 0; for (int n = 0; n < [currentPath elementCount]; n++) { NSPoint points[1]; [currentPath elementAtIndex:n associatedPoints:points]; aggregateX += points[0].x; aggregateY += points[0].y; } for (int n = 0; n < [currentPath elementCount]; n++) { NSPoint points[1]; [currentPath elementAtIndex:n associatedPoints:points]; //line equations in Ax + By = C form float _A_FOO = (aggregateY/[currentPath elementCount]) - point.y; float _B_FOO = point.x - (aggregateX/[currentPath elementCount]); float _C_FOO = (_A_FOO * point.x) + (_B_FOO * point.y); float _A_BAR = olderY - points[0].y; float _B_BAR = points[0].x - olderX; float _C_BAR = (_A_BAR * olderX) + (_B_BAR * olderY); float det = (_A_FOO * _B_BAR) - (_A_BAR * _B_FOO); if (det != 0) { //intersection points with the edges float xIntersectionPoint = ((_B_BAR * _C_FOO) - (_B_FOO * _C_BAR)) / det; float yIntersectionPoint = ((_A_FOO * _C_BAR) - (_A_BAR * _C_FOO)) / det; interPoint = NSMakePoint(xIntersectionPoint yIntersectionPoint); if (olderX <= points[0].x) { //doesn't matter in which direction the ray goes so I send it right-ward. if ((interPoint.x >= olderX && interPoint.x <= points[0].x) && (interPoint.x > point.x)) { noOfIntersections++; } } else { if ((interPoint.x >= points[0].x && interPoint.x <= olderX) && (interPoint.x > point.x)) { noOfIntersections++; } } } olderX = points[0].x; olderY = points[0].y; } if (noOfIntersections % 2 == 0) { result = FALSE; } else { result = TRUE; } return result; } Note that if you are really doing it in Cocoa then you can use the [NSBezierPath containsPoint:] method.  Compute the oriented sum of angles between the point p and each of the polygon apices. If the total oriented angle is 360 degrees the point is inside. If the total is 0 the point is outside. I like this method better because it is more robust and less dependent on numerical precision. Methods that compute evenness of number of intersections are limited because you can 'hit' an apex during the computation of the number of intersections. EDIT: By The Way this method works with concave and convex polygons. EDIT: I recently found a whole Wikipedia article on the topic. works only for convex polygons. Nope this is not true. This works regardless of the convexity of the polygon. mathematically elegant but takes a bit of trig making it painfully slow. @DarenW: Only one acos per vertex; on the other hand this algorithm should be the easiest to implement in SIMD because it has absolutely no branching. This doesn't work at all. Suppose the polygon is a triangle and the point is far below it (vertical distance much larger than both horizontal distance and the size of the triangle). Then the angle between p and all tree apices is aprox 90 deg and the sum is aprox 270 deg. This is not a pathological example it's just easy to visualize. If the point is inside the sum is indeed 360 deg but if it is outside the sum might coincidentally be 360 deg @emilio if the point is far away from the triangle I don't see how the angle formed by the point and two apices of the triangle will be 90 degrees. First use bounding box check to solve ""point is far"" cases. For trig you could use pregenerated tables. @DarenW: According to [wikipedia](http://en.wikipedia.org/wiki/Point_in_polygon#Winding_number_algorithm) it doesn't have to be slow. You can simply check the quadrants instead of calculating the angles.  Obj-C version of nirg's answer with sample method for testing points. Nirg's answer worked well for me. - (BOOL)isPointInPolygon:(NSArray *)vertices point:(CGPoint)test { NSUInteger nvert = [vertices count]; NSInteger i j c = 0; CGPoint verti vertj; for (i = 0 j = nvert-1; i < nvert; j = i++) { verti = [(NSValue *)[vertices objectAtIndex:i] CGPointValue]; vertj = [(NSValue *)[vertices objectAtIndex:j] CGPointValue]; if (( (verti.y > test.y) != (vertj.y > test.y) ) && ( test.x < ( vertj.x - verti.x ) * ( test.y - verti.y ) / ( vertj.y - verti.y ) + verti.x) ) c = !c; } return (c ? YES : NO); } - (void)testPoint { NSArray *polygonVertices = [NSArray arrayWithObjects: [NSValue valueWithCGPoint:CGPointMake(13.5 41.5)] [NSValue valueWithCGPoint:CGPointMake(42.5 56.5)] [NSValue valueWithCGPoint:CGPointMake(39.5 69.5)] [NSValue valueWithCGPoint:CGPointMake(42.5 84.5)] [NSValue valueWithCGPoint:CGPointMake(13.5 100.0)] [NSValue valueWithCGPoint:CGPointMake(6.0 70.5)] nil ]; CGPoint tappedPoint = CGPointMake(23.0 70.0); if ([self isPointInPolygon:polygonVertices point:tappedPoint]) { NSLog(@""YES""); } else { NSLog(@""NO""); } } Of course in Objective-C `CGPathContainsPoint()` is your friend. @ZevEisenberg but that would be too easy! Thanks for the note. I'll dig up that project at some point to see why I used a custom solution. I likely didn't know about `CGPathContainsPoint()`  Here is a JavaScript variant of the answer by M. Katz based on Nirg's approach: function pointIsInPoly(p polygon) { var isInside = false; var minX = polygon[0].x maxX = polygon[0].x; var minY = polygon[0].y maxY = polygon[0].y; for (var n = 1; n < polygon.length; n++) { var q = polygon[n]; minX = Math.min(q.x minX); maxX = Math.max(q.x maxX); minY = Math.min(q.y minY); maxY = Math.max(q.y maxY); } if (p.x < minX || p.x > maxX || p.y < minY || p.y > maxY) { return false; } var i = 0 j = polygon.length - 1; for (i j; i < polygon.length; j = i++) { if ( (polygon[i].y > p.y) != (polygon[j].y > p.y) && p.x < (polygon[j].x - polygon[i].x) * (p.y - polygon[i].y) / (polygon[j].y - polygon[i].y) + polygon[i].x ) { isInside = !isInside; } } return isInside; }  Here is a C# version of the answer given by nirg which comes from this RPI professor. Note that use of the code from that RPI source requires attribution. A bounding box check has been added at the top.  public bool IsPointInPolygon( Point p Point[] polygon ) { double minX = polygon[ 0 ].X; double maxX = polygon[ 0 ].X; double minY = polygon[ 0 ].Y; double maxY = polygon[ 0 ].Y; for ( int i = 1 ; i < polygon.Length ; i++ ) { Point q = polygon[ i ]; minX = Math.Min( q.X minX ); maxX = Math.Max( q.X maxX ); minY = Math.Min( q.Y minY ); maxY = Math.Max( q.Y maxY ); } if ( p.X < minX || p.X > maxX || p.Y < minY || p.Y > maxY ) { return false; } // http://www.ecse.rpi.edu/Homepages/wrf/Research/Short_Notes/pnpoly.html bool inside = false; for ( int i = 0 j = polygon.Length - 1 ; i < polygon.Length ; j = i++ ) { if ( ( polygon[ i ].Y > p.Y ) != ( polygon[ j ].Y > p.Y ) && p.X < ( polygon[ j ].X - polygon[ i ].X ) * ( p.Y - polygon[ i ].Y ) / ( polygon[ j ].Y - polygon[ i ].Y ) + polygon[ i ].X ) { inside = !inside; } } return inside; } Works great thanks I converted to JavaScript. http://stackoverflow.com/questions/217578/point-in-polygon-aka-hit-test/17490923#17490923 Thanks for this saved me a heap of time. Also very handy that you included the prior bounding box check. Kudos... Awesome! Worked on my ios project! Thanks!  Java Version: public class Geocode { private float latitude; private float longitude; public Geocode() { } public Geocode(float latitude float longitude) { this.latitude = latitude; this.longitude = longitude; } public float getLatitude() { return latitude; } public void setLatitude(float latitude) { this.latitude = latitude; } public float getLongitude() { return longitude; } public void setLongitude(float longitude) { this.longitude = longitude; } } public class GeoPolygon { private ArrayList<Geocode> points; public GeoPolygon() { this.points = new ArrayList<Geocode>(); } public GeoPolygon(ArrayList<Geocode> points) { this.points = points; } public GeoPolygon add(Geocode geo) { points.add(geo); return this; } public boolean inside(Geocode geo) { int i j; boolean c = false; for (i = 0 j = points.size() - 1; i < points.size(); j = i++) { if (((points.get(i).getLongitude() > geo.getLongitude()) != (points.get(j).getLongitude() > geo.getLongitude())) && (geo.getLatitude() < (points.get(j).getLatitude() - points.get(i).getLatitude()) * (geo.getLongitude() - points.get(i).getLongitude()) / (points.get(j).getLongitude() - points.get(i).getLongitude()) + points.get(i).getLatitude())) c = !c; } return c; } }  For graphics I'd never go with Integers. Many OSes use Integers for painting UI (pixels are ints after all) but Mac OS X for example uses float for everything (it talks of points. A point can translate to one pixel but depending on monitor resolution it might translate to anything else but a pixel and if resolution is very high of a monitor in theory a pixel can translate to half a point so 1.5/1.5 can be a different pixel than 1/1 and 2/2). However I never noticed that Mac UIs are significantly slower than other UIs. After all 3D (OpenGL or Direct3D) also works with floats all of the time and modern graphics libraries might very often take advantage of high power GPUs of a system without you even noticing it (as a GPU can also speed up 2D painting not just 3D; 2D is only a subset of 3D consider 2D to be 3D where the Z coordinates are always 0 for example). Now you said speed is your main concern okay let's go for speed. Before you run any sophisticated algorithm first do a simple test. Create an axis aligned bounding box around your polygon. This is very easy fast and can already safe you tons of CPU time. How does that work? Iterate over all points of the polygon (every point being X/Y) and find the min/max values of X and Y. E.g. you have the points (9/1) (4/3) (2/7) (8/2) (3/6). Xmin is 2 Xmax is 9 Ymin is 1 and Ymax is 7. Now you know that no point within your polygon can ever have a x value smaller than 2 and greater than 9 no point can have a y value smaller than 1 and greater than 7. This way you can quickly exclude many points not being within your polygon: // p is your point p.x is the x coord p.y is the y coord if (p.x < Xmin || p.x > Xmax || p.y < Ymin || p.y > Ymax) { // Definitely not within the polygon! } This is the first test to run on any point. If this test already excludes the point from the polygon (even though it's a very coarse test!) then there is no use of running any other tests. As you can see this test is ultra fast. If this test won't exclude the point it is within the axis aligned bounding box but that does not mean it is within the polygon; it only means it might be within the polygon. What we need next is a more sophisticated test to check if the point is really within the polygon or just within the bounding box. There are a couple of ways how this can be calculated. It also makes a huge difference is the polygon can have holes or whether its solid. Here are examples of solid ones (one convex one concave): And here's one with a hole: The green one has a hole in the middle! The easiest way is to use ray casting since it can handle all the polygons shown above correctly no special handling is necessary and it still provides good speed. The idea of the algorithm is pretty simple: Draw a virtual ray from anywhere outside the polygon to your point and count how often it hits any side of the polygon. If the number of hits is even it's outside of the polygon if it's odd it's inside. The winding number algorithm is more accurate for points being very very very close to a polygon line; ray casting may fail here because of float precision and rounding issues however winding number is much slower and if a point is accidentally detected to be outside of the polygon if it's so close to a polygon line that your eye can't even tell if it's inside or outside is it really a problem? I don't think so so let's keep things simple. You still have the bounding box of above remember? Okay now let's say your point is p again (p.x/p.y). Your ray might go from (Xmin - e/p.y) to (p.x/p.y) or (p.x/p.y) to (Xmax + e/p.y) or (p.x/Ymin - e) to (p.x/p.y) or (p.x/p.y) to (p.x/Ymax + e) It won't matter. Choose whatever sounds best to you. It's only important that the ray starts definitely outside of the polygon and stops at the point. So what is e anyway? Well e (actually epsilon) gives the bounding box some padding. As I said ray tracing fails if we start too close to a polygon line. Since the bounding box might equal the polygon (if the polygon is actually an axis aligned rectangle the bounding box is equal to the polygon itself! And the ray would start directly on the polygon side). How big should you choose e? Not too big. It depends on the coordinate system scale you use for drawing. You could select e to be 1.0 however if your polygons have coordinates much smaller than 1.0 selecting e to be 0.001 might be large enough. You could select e to be always 1% of the polygon size e.g. when having a ray along the x axis you could calculate e like this: e = ((Xmax - Xmin) / 100) Now that we have the ray with its start and end coordinates the problem shifts from ""is the point within the polygon"" to ""how often intersects the ray a polygon side"". Therefor we can't just work with the polygon points as before (for the bounding box) now we need the actual sides. A side is always defined by two points. side 1: (X1/Y1)-(X2/Y2) side 2: (X2/Y2)-(X3/Y3) side 3: (X3/Y3)-(X4/Y4) : You need to test the ray against all sides. Consider the ray to be a vector and every side to be a vector. The ray has to hit each side exactly once or never at all. It can't hit the same side twice (two lines in 2D space will always intersect exactly once unless they are parallel in which case they never intersect. However since vectors have a limited length two vectors might not be parallel and still never intersect). // Test the ray against all sides int intersections = 0; for (side = 0; side < numberOfSides; side++) { // Test if current side intersects with ray. // If yes intersections++; } if ((intersections & 1) == 1) { // Inside of polygon } else { // Outside of polygon } So far so well but how do you test if two vectors intersect? Here's some C code (not tested) that should do the trick: #define NO 0 #define YES 1 #define COLLINEAR 2 int areIntersecting( float v1x1 float v1y1 float v1x2 float v1y2 float v2x1 float v2y1 float v2x2 float v2y2 ) { float d1 d2; float a1 a2 b1 b2 c1 c2; // Convert vector 1 to a line (line 1) of infinite length. // We want the line in linear equation standard form: A*x + B*y + C = 0 // See: http://en.wikipedia.org/wiki/Linear_equation a1 = v1y2 - v1y1; b1 = v1x1 - v1x2; c1 = (v1x2 * v1y1) - (v1x1 * v1y2); // Every point (xy) that solves the equation above is on the line // every point that does not solve it is either above or below the line. // We insert (x1y1) and (x2y2) of vector 2 into the equation above. d1 = (a1 * v2x1) + (b1 * v2y1) + c1; d2 = (a1 * v2x2) + (b1 * v2y2) + c1; // If d1 and d2 both have the same sign they are both on the same side of // our line 1 and in that case no intersection is possible. Careful 0 is // a special case that's why we don't test "">="" and ""<="" but ""<"" and "">"". if (d1 > 0 && d2 > 0) return NO; if (d1 < 0 && d2 < 0) return NO; // We repeat everything above for vector 2. // We start by calculating line 2 in linear equation standard form. a2 = v2y2 - v2y1; b2 = v2x1 - v2x2; c2 = (v2x2 * v2y1) - (v2x1 * v2y2); // Calulate d1 and d2 again this time using points of vector 1 d1 = (a2 * v1x1) + (b2 * v1y1) + c2; d2 = (a2 * v1x2) + (b2 * v1y2) + c2; // Again if both have the same sign (and neither one is 0) // no intersection is possible. if (d1 > 0 && d2 > 0) return NO; if (d1 < 0 && d2 < 0) return NO; // If we get here only three possibilities are left. Either the two // vectors intersect in exactly one point or they are collinear // (they both lie both on the same infinite line) in which case they // may intersect in an infinite number of points or not at all. if ((a1 * b2) - (a2 * b1) == 0.0f) return COLLINEAR; // If they are not collinear they must intersect in exactly one point. return YES; } The input values are the two endpoints of vector 1 (x and y) and vector 2 (x and y). So you have 2 vectors 4 points 8 coordinates. YES and NO are clear. YES increases intersections NO does nothing. What about COLLINEAR? It means both vectors lie on the same infinite line depending on position and length they don't intersect at all or they intersect in an endless number of points. I'm not absolutely sure how to handle this case I would not count it as intersection either way. Well this case is rather rare in practice anyway because of floating point rounding errors; better code would probably not test for == 0.0f but instead for something like < epsilon where epsilon is a rather small number (so smaller rounding mistakes won't lead to wrong results). Last but not least: If you may use 3D hardware to solve the problem forget about anything above. There is a much faster and much easier way. Just let the GPU do all the work for you. Create a painting surface that is off screen (so you can paint into it without it appearing anywhere on the screen). Fill it completely with the color black. Now let OpenGL or Direct3D paint your polygon (or even all of your polygons if you just want to test if the point is within any of them but you don't care for which one) into this drawing surface and fill the polygon(s) with a different color e.g. white. To check if a point is within the polygon get the color of this point from the drawing surface. This is just a O(1) memory fetch. If it's white it's inside if it's black it's outside. Easy isn't it? This method will pay off if you have very little polygons (e.g. 50-100) but a damn lot of points to test (> 1000) in which case this method is much speedier than anything else. It will only be a problem if your drawing surface must be huge because your polygons are. If your drawing surface needs to be 100 MB or more to make the polygons fit this method might become very slow (despite the fact that it wastes tons of memory). +1 Fantastic answer. On using the hardware to do it I've read in other places that it can be slow because you have to get data back from the graphics card. But I do like the principle of taking load off the CPU a lot. Does anyone have any good references for how this might be done in OpenGL? If I'm not mistaken the ray casting algorithm you describe only works for convex polygons unless you consider a concave section to be a ""hole"" in the polygon +1 because this is so simple! The main problem is if your polygon and test point line up on a grid (not uncommon) then you have to deal with ""duplicitous"" intersections for example straight through a polygon point! (yielding a parity of two instead of one). Gets into this weird area: http://stackoverflow.com/questions/2255842/detecting-coincident-subset-of-two-coincident-line-segments . Computer Graphics is full of these special cases: simple in theory hairy in practice. This is one of the best answers I've seen on this site. It's not only accurate but is also simple to understand. Thanks a ton @RMorrisey: Why do you think so? I fail to see how it would fail for a concave polygon. The ray may leave and re-enter the polygon multiple times when the polygon is concave but in the end the hit counter will be odd if the point is within and even if it is outside also for concave polygons. The 'Fast Winding Number Algorithm' described at http://softsurfer.com/Archive/algorithm_0103/algorithm_0103.htm works pretty fast... @AlexAndro: a1 should be a b1 should be b and h2 should be hd; yet I saw there were some more tiny flaws in the code. Anyway I replaced the broken sample code by a less highly optimized therefor much easier to understand version that is a bit less accurate (mathematically seen) still it does the trick for the intersection test my answer needs (at least for most projects this code should do the trick). @Mecki Could you tell me please what is `a1 b1 and h2` They appear in code but they are not defined anywhere. Created my very first point in polygon method from this thing. Thanks a lot mate! Java implementation: https://github.com/sromku/polygon-contains-point Just wondering how to handle the ray passing over corners of the polygon depending on the angle it could count two lines making the point outside the polygon instead of inside. I have only ONE polygon which is guaranteed to be smaller than the screen of the iPhone/iPad. Perhaps I should use the GPU approach? @NicolasMiari For a single polygon I guess the GPU approach will be slower since for raw pixel access to a drawing buffer the whole buffer must first be copied in memory which is a pretty slow operation on iOS devices; unless you want to test thousands of points to be inside or outside your polygon in which case the faster test after the copy may pay off in the end. I ended up using vectors. A bit tough to debug bit pays off. Your usage of / to separate x and y coordinates is confusing it reads as x divided by y. It's much more clear to use x y (i.e. x comma y) Overall a useful answer. Just used the ""hardware"" method to implement this using js on a canvas. Worked a charm. So mush easier to understand than all this maths stuff ;) Plus no gpu used as im not using hardware acceleration. And on top of that its really easy to debug as i can just unhide the second black and white canvas. Genius. Relevant Wikipedia link: http://en.wikipedia.org/wiki/Point_in_polygon. One of the pictures is from this article. Would 3 tests of a point VS an OBB using SAT be faster than using this method by combing all 3 OOB's as concave polygon? @paulm: I don't know. Maybe. I'd implement both and benchmark them. This will not only tell you which one is faster but also if one of them is significant faster or not.  The trivial solution would be to divide the polygon to triangles and hit test the triangles as explained here If your polygon is CONVEX there might be a better approach though. Look at the polygon as a collection of infinite lines. Each line dividing space into two. for every point it's easy to say if its on the one side or the other side of the line. If a point is on the same side of all lines then it is inside the polygon. very fast and can be applied to more general shapes. back around 1990 we had ""curvigons"" where some sides were circular arcs. By analyzing those sides into circular wedges and a pair of triangles joining the wedge to the origin (polygon centroid) hit testing was fast and easy. got any references on these curvigons? I would love a ref for the curvigons too.  Really like the solution posted by Nirg and edited by bobobobo. I just made it javascript friendly and a little more legible for my use: function insidePoly(poly pointx pointy) { var i j; var inside = false; for (i = 0 j = poly.length - 1; i < poly.length; j = i++) { if(((poly[i].y > pointy) != (poly[j].y > pointy)) && (pointx < (poly[j].x-poly[i].x) * (pointy-poly[i].y) / (poly[j].y-poly[i].y) + poly[i].x) ) inside = !inside; } return inside; } Wow that is nice and short  I did some work on this back when I was a researcher under Michael Stonebraker - you know the professor who came up with Ingres PostgreSQL etc. We realized that the fastest way was to first do a bounding box because it's SUPER fast. If it's outside the bounding box it's outside. Otherwise you do the harder work... If you want a great algorithm look to the open source project PostgreSQL source code for the geo work... I want to point out we never got any insight into right vs left handedness (also expressible as an ""inside"" vs ""outside"" problem... UPDATE BKB's link provided a good number of reasonable algorithms. I was working on Earth Science problems and therefore needed a solution that works in latitude/longitude and it has the peculiar problem of handedness - is the area inside the smaller area or the bigger area? The answer is that the ""direction"" of the verticies matters - it's either left-handed or right handed and in this way you can indicate either area as ""inside"" any given polygon. As such my work used solution three enumerated on that page. In addition my work used separate functions for ""on the line"" tests. ...Since someone asked: we figured out that bounding box tests were best when the number of verticies went beyond some number - do a very quick test before doing the longer test if necessary... A bounding box is created by simply taking the largest x smallest x largest y and smallest y and putting them together to make four points of a box... Another tip for those that follow: we did all our more sophisticated and ""light-dimming"" computing in a grid space all in positive points on a plane and then re-projected back into ""real"" longitude/latitude thus avoiding possible errors of wrapping around when one crossed line 180 of longitude and when handling polar regions. Worked great! What if I don't happen to have the bounding box? :) You can easily create a bounding box - it's just the four points which use the greatest and least x and greatest and least y. More soon.  What about the Alciatore & Miranda algorithm? it is general and only uses multiplication: http://www.engr.colostate.edu/~dga/dga/papers/point_in_polygon.pdf To me this is one of the best!  .Net port:  static void Main(string[] args) { Console.Write(""Hola""); List<double> vertx = new List<double>(); List<double> verty = new List<double>(); int i j c = 0; vertx.Add(1); vertx.Add(2); vertx.Add(1); vertx.Add(4); vertx.Add(4); vertx.Add(1); verty.Add(1); verty.Add(2); verty.Add(4); verty.Add(4); verty.Add(1); verty.Add(1); int nvert = 6; //Vértices del poligono double testx = 2; double testy = 5; for (i = 0 j = nvert - 1; i < nvert; j = i++) { if (((verty[i] > testy) != (verty[j] > testy)) && (testx < (vertx[j] - vertx[i]) * (testy - verty[i]) / (verty[j] - verty[i]) + vertx[i])) c = 1; } }  C# version of nirg's answer is here: I'll just share the code. It may save someone some time. public static bool IsPointInPolygon(IList<Point> polygon Point testPoint) { bool result = false; int j = polygon.Count() - 1; for (int i = 0; i < polygon.Count(); i++) { if (polygon[i].Y < testPoint.Y && polygon[j].Y >= testPoint.Y || polygon[j].Y < testPoint.Y && polygon[i].Y >= testPoint.Y) { if (polygon[i].X + (testPoint.Y - polygon[i].Y) / (polygon[j].Y - polygon[i].Y) * (polygon[j].X - polygon[i].X) < testPoint.X) { result = !result; } } j = i; } return result; } this works in most of the cases but it is wrong and doesnt work properly always ! use the solution from M Katz which is correct  I think the following piece of code is the best solution (taken from here): int pnpoly(int nvert float *vertx float *verty float testx float testy) { int i j c = 0; for (i = 0 j = nvert-1; i < nvert; j = i++) { if ( ((verty[i]>testy) != (verty[j]>testy)) && (testx < (vertx[j]-vertx[i]) * (testy-verty[i]) / (verty[j]-verty[i]) + vertx[i]) ) c = !c; } return c; } Arguments nvert: Number of vertices in the polygon. Whether to repeat the first vertex at the end has been discussed in the article referred above. vertx verty: Arrays containing the x- and y-coordinates of the polygon's vertices. testx testy: X- and y-coordinate of the test point. It's both short and efficient and works both for convex and concave polygons. As suggested before you should check the bounding rectangle first and treat polygon holes separately. The idea behind this is pretty simple. The author describes it as follows: I run a semi-infinite ray horizontally (increasing x fixed y) out from the test point and count how many edges it crosses. At each crossing the ray switches between inside and outside. This is called the Jordan curve theorem. nice concise answer! thanks for the executive summary ;-) Totally underrated answer. It is 7 lines of code. I don't quite understand it but it works. This is so incredibly helpful can't believe it's not rated higher. +1 this is good too Question. What exactly are the variables that I pass it? What do they represent? @CarlosP thank you. @tekknolagi nvert: Number of vertices in the polygon. vertx verty Arrays containing the x- and y-coordinates of the polygon's vertices. testx testy X- and y-coordinate of the test point. Don't you have to watch out for divide by zero if verty[j]==verty[i]? the post links to a page which discusses further details including the divide-by-zero issue. @Mick It checks that `verty[i]` and `verty[j]` are either side of `testy` so they are never equal. These 7 lines of code have saved my life +1 I'm still trying to understand this but it works!! There seems to be a bug when the polygon is concave. That is perfect! Thank you so much! This actually works. I created a [github gist](https://gist.github.com/superwills/5808630) to test it (GLUT). Any body can re-factor this method to accept parameters like this `check([testpointxtestpointy][p1xp1yp2xp2y....])`? How can we find Number of vertices in the polygon? does this use the MacMartin test as described in the artice bobobo cited? I can hardly read it it's so condensed and good looking. It was the fastest by far (70% faster than non-macmartin ray casting) when dealing with large polygons so I'm trying to figure out if this covers that This code is not robust and I would not recommend using it. Here is a link giving some detailed analysis: http://www-ma2.upc.es/geoc/Schirra-pointPolygon.pdf  The Eric Haines article cited by bobobobo is really excellent. Particularly interesting are the tables comparing performance of the algorithms; the angle summation method is really bad compared to the others. Also interesting is that optimisations like using a lookup grid to further subdivide the polygon into ""in"" and ""out"" sectors can make the test incredibly fast even on polygons with > 1000 sides. Anyway it's early days but my vote goes to the ""crossings"" method which is pretty much what Mecki describes I think. However I found it most succintly described and codified by David Bourke. I love that there is no real trigonometry required and it works for convex and concave and it performs reasonably well as the number of sides increases. By the way here's one of the performance tables from the Eric Haines' article for interest testing on random polygons.  number of edges per polygon 3 4 10 100 1000 MacMartin 2.9 3.2 5.9 50.6 485 Crossings 3.1 3.4 6.8 60.0 624 Triangle Fan+edge sort 1.1 1.8 6.5 77.6 787 Triangle Fan 1.2 2.1 7.3 85.4 865 Barycentric 2.1 3.8 13.8 160.7 1665 Angle Summation 56.2 70.4 153.6 1403.8 14693 Grid (100x100) 1.5 1.5 1.6 2.1 9.8 Grid (20x20) 1.7 1.7 1.9 5.7 42.2 Bins (100) 1.8 1.9 2.7 15.1 117 Bins (20) 2.1 2.2 3.7 26.3 278  There is nothing more beutiful than an inductive definition of a problem. For the sake of completeness here you have a version in prolog which might also clarify the thoughs behind ray casting: Based on the simulation of simplicity algorithm in http://www.ecse.rpi.edu/Homepages/wrf/Research/Short_Notes/pnpoly.html Some helper predicates: exor(AB):- \+AB;A\+B. in_range(CoordinateCACB) :- exor((CA>Coordinate)(CB>Coordinate)). inside(false). inside(_[_|[]]). inside(X:Y [X1:Y1X2:Y2|R]) :- in_range(YY1Y2) X > ( ((X2-X1)*(Y-Y1))/(Y2-Y1) + X1)toggle_ray inside(X:Y [X2:Y2|R]); inside(X:Y [X2:Y2|R]). get_line(__[]). get_line([XA:YAXB:YB][X1:Y1X2:Y2|R]):- [XA:YAXB:YB]=[X1:Y1X2:Y2]; get_line([XA:YAXB:YB][X2:Y2|R]). The equation of a line given 2 points A and B (Line(AB)) is:  (YB-YA) Y - YA = ------- * (X - XA) (XB-YB) It is important that the direction of rotation for the line is setted to clock-wise for boundaries and anti-clock-wise for holes. We are going to check whether the point (XY) i.e the tested point is at the left half-plane of our line (it is a matter of taste it could also be the right side but also the direction of boundaries lines has to be changed in that case) this is to project the ray from the point to the right (or left) and acknowledge the intersection with the line. We have chosen to project the ray in the horizontal direction (again it is a matter of taste it could also be done in vertical with similar restrictions) so we have:  (XB-XA) X < ------- * (Y - YA) + XA (YB-YA) Now we need to know if the point is at the left (or right) side of the line segment only not the entire plane so we need to restrict the search only to this segment but this is easy since to be inside the segment only one point in the line can be higher than Y in the vertical axis. As this is a stronger restriction it needs to be the first to check so we take first only those lines meeting this requirement and then check its possition. By the Jordan Curve theorem any ray projected to a polygon must intersect at an even number of lines. So we are done we will throw the ray to the right and then everytime it intersects a line toggle its state. However in our implementation we are goint to check the lenght of the bag of solutions meeting the given restrictions and decide the innership upon it. for each line in the polygon this have to be done. is_left_half_plane(_[][]_). is_left_half_plane(X:Y[XA:YAXB:YB] [[X1:Y1X2:Y2]|R] Test) :- [XA:YA XB:YB] = [X1:Y1 X2:Y2] call(Test X  (((XB - XA) * (Y - YA)) / (YB - YA) + XA)); is_left_half_plane(X:Y [XA:YA XB:YB] R Test). in_y_range_at_poly(Y[XA:YAXB:YB]Polygon) :- get_line([XA:YAXB:YB]Polygon) in_range(YYAYB). all_in_range(CoordinatePolygonLines) :- aggregate(bag(Line) in_y_range_at_poly(CoordinateLinePolygon) Lines). traverses_ray(X:Y Lines Count) :- aggregate(bag(Line) is_left_half_plane(X:Y Line Lines <) IntersectingLines) length(IntersectingLines Count). % This is the entry point predicate inside_poly(X:YPolygonAnswer) :- all_in_range(YPolygonLines) traverses_ray(X:Y Lines Count) (1 is mod(Count2)->Answer=inside;Answer=outside)."
774,A,"Making a game in C++ using parallel processing I wanted to ""emulate"" a popular flash game Chrontron in C++ and needed some help getting started. (NOTE: Not for release just practicing for myself)  Basics: Player has a time machine. On each iteration of using the time machine a parallel state is created co-existing with a previous state. One of the states must complete all the objectives of the level before ending the stage. In addition all the stages must be able to end the stage normally without causing a state paradox (wherein they should have been able to finish the stage normally but due to the interactions of another state were not). So that sort of explains how the game works. You should play it a bit to really understand what my problem is. I'm thinking a good way to solve this would be to use linked lists to store each state which will probably either be a hash map based on time or a linked list that iterates based on time. I'm still unsure. ACTUAL QUESTION: Now that I have some rough specs I need some help deciding on which data structures to use for this and why. Also I want to know what Graphics API/Layer I should use to do this: SDL OpenGL or DirectX (my current choice is SDL). And how would I go about implementing parallel states? With parallel threads? EDIT (To clarify more): OS -- Windows (since this is a hobby project may do this in Linux later) Graphics -- 2D Language -- C++ (must be C++ -- this is practice for a course next semester) Q-Unanswered: SDL : OpenGL : Direct X Q-Answered: Avoid Parallel Processing Q-Answered: Use STL to implement time-step actions.  So far from what people have said I should: 1. Use STL to store actions. 2. Iterate through actions based on time-step. 3. Forget parallel processing -- period. (But I'd still like some pointers as to how it could be used and in what cases it should be used since this is for practice). Appending to the question I've mostly used C# PHP and Java before so I wouldn't describe myself as a hotshot programmer. What C++ specific knowledge would help make this project easier for me? (ie. Vectors?) I have played this game before. I don't necessarily think parallel processing is the way to go. You have shared objects in the game (levers boxes elevators etc) that will need to be shared between processes possibly with every delta thereby reducing the effectiveness of the parallelism. I would personally just keep a list of actions then for each subsequent iteration start interleaving them together. For example if the list is in the format of <[iteration.action]> then the 3rd time thru would execute actions 1.1 2.1 3.1 1.2 2.2 3.3 etc. And I would store these actions with the time as the key as in an associative array? As long as you have a fixed time frame between steps a simple list should do. No problems with using a key but it just seems simpler.  This sounds very similar to Braid. You really don't want parallel processing for this - parallel programming is hard and for something like this performance should not be an issue. Since the game state vector will grow very quickly (probably on the order of several kilobytes per second depending on the frame rate and how much data you store) you don't want a linked list which has a lot of overhead in terms of space (and can introduce big performance penalties due to cache misses if it is laid out poorly). For each parallel timeline you want a vector data structure. You can store each parallel timeline in a linked list. Each timeline knows at what time it began. To run the game you iterate through all active timelines and perform one frame's worth of actions from each of them in lockstep. No need for parallel processing.  Unless you're desperate to use C++ for your own education you should definitely look at XNA for your game & graphics framework (it uses C#). It's completely free it does a lot of things for you and soon you'll be able to sell your game on Xbox Live. To answer your main question nothing that you can already do in Flash would ever need to use more than one thread. Just store a list of positions in an array and loop through with a different offset for each robot.  After briefly glossing over the description I think you have the right idea I would have a state object that holds the state data and place this into a linked list...I don't think you need parallel threads... as far as the graphics API I have only used opengl and can say that it is pretty powerful and has a good C / C++ API opengl would also be more cross platform as you can use the messa library on *Nix computers.  A very interesting game idea. I think you are right that parrellel computing would be benefical to this design but no more then any other high resource program. The question is a bit ambigous. I see that you are going to write this in C++ but what OS are you coding it for? Do you intend on it being cross platform and what kind of graphics would you like ie 3D 2D high end web based. So basically we need a lot more information.  Parallel processing isn't the answer. You should simply ""record"" the players actions then play them back for the ""previous actions"" So you create a vector (singly linked list) of vectors that holds the actions. Simply store the frame number that the action was taken (or the delta) and complete that action on the ""dummy bot"" that represents the player during that particular instance. You simply loop through the states and trigger them one after another. You get a side effect of easily ""breaking"" the game when a state paradox happens simply because the next action fails.  What you should do is first to read and understand the ""fixed time-step"" game loop (Here's a good explanation: http://www.gaffer.org/game-physics/fix-your-timestep). Then what you do is to keep a list of list of pairs of frame counter and action. STL example: std::list<std::list<std::pair<unsigned long Action> > > state; Or maybe a vector of lists of pairs. To create the state for every action (player interaction) you store the frame number and what action is performed most likely you'd get the best results if action simply was ""key pressed"" or ""key released"": state.back().push_back(std::make_pair(currentFrame VK_LEFT | KEY_PRESSED)); To play back the previous states you'd have to reset the frame counter every time the player activates the time machine and then iterate through the state list for each previous state and see if any matches the current frame. If there is perform the action for that state. To optimize you could keep a list of iterators to where you are in each previous state-list. Here's some pseudo-code for that: typedef std::list<std::pair<unsigned long Action> > StateList; std::list<StateList::iterator> stateIteratorList; // foreach(it in stateIteratorList) { if(it->first == currentFrame) { performAction(it->second); ++it; } } I hope you get the idea... Separate threads would simply complicate the matter greatly this way you get the same result every time which you cannot guarantee by using separate threads (can't really see how that would be implemented) or a non-fixed time-step game loop. When it comes to graphics API I'd go with SDL as it's probably the easiest thing to get you started. You can always use OpenGL from SDL later on if you want to go 3D. Thanks for the great detail on this one. This actually looks like a good solution for this. Any recommendations on the graphics engine to use to go along with it? I'd go with SDL as it's probably the easiest thing to get you started. You can always use OpenGL from SDL later if you want to go 3D. OGRE is a great 2D73D game engine for C++"
775,A,"Given a start and end point and a distance calculate a point along a line Looking for the quickest way to calculate a point that lies on a line a given distance away from the end point of the line: void calculate_line_point(int x1 int y1 int x2 int y2 int distance int *px int *py) { //calculate a point on the line x1-y1 to x2-y2 that is distance from x2-y2 *px = ??? *py = ??? } Thanks for the responses no this is not homework just some hacking out of my normal area of expertise. This is the function suggested below. It's not close to working. If I calculate points every 5 degrees on the upper right 90 degree portion of a circle as starting points and call the function below with the center of the circle as x2y2 with a distance of 4 the end points are totally wrong. They lie below and to the right of the center and the length is as long as the center point. Anyone have any suggestions? void calculate_line_point(int x1 int y1 int x2 int y2 int distance) { //calculate a point on the line x1-y1 to x2-y2 that is distance from x2-y2 double vx = x2 - x1; // x vector double vy = y2 - y1; // y vector double mag = sqrt(vx*vx + vy*vy); // length vx /= mag; vy /= mag; // calculate the new vector which is x2y2 + vxvy * (mag + distance). px = (int) ( (double) x2 + vx * (mag + (double)distance) ); py = (int) ( (double) y2 + vy * (mag + (double)distance) ); } I've found this solution on stackoverflow but don't understand it completely can anyone clarify? What Lucas said. Also you probably read my post while I had a typo. If x1y1 is the origin you want x1y1 + vxvy * (mag + distance) not x2y2. That is starting from the origin you want to travel the distance _to x2y2_ plus the additional distance using the direction from x1y1 to x2y2. Though I think you might want to rephrase your question. What exactly do you want to do? The question as it is now seems more like an intermediate problem. Maybe you should use floats/doubles because you will get rounding errors. This might be a concern. Is this homework? To err on the safe side: Create a vector describing the direction of the line. Normalize the vector Your desired point is start_point + distance * vector He wanted the point which is ""distance"" from the end point not the start point. Or maybe she wanted the point which is ""distance"" from the end point not the start point. And no it is not homework.  I think this belongs on MathOverflow but I'll answer since this is your first post. First you calculate the vector from x1y1 to x2y2: float vx = x2 - x1; float vy = y2 - y1; Then calculate the length: float mag = sqrt(vx*vx + vy*vy); Normalize the vector to unit length: vx /= mag; vy /= mag; Finally calculate the new vector which is x2y2 + vxvy * (mag + distance). *px = (int)((float)x1 + vx * (mag + distance)); *py = (int)((float)y1 + vy * (mag + distance)); You can omit some of the calculations multiplying with distance / mag instead. unfortunately mathoverflow is too snobbish to entertain this kind of question; it does belong here. That's unfortunate. Sorry if it's too simple guys totally out of my normal area of competence. Thanks for the help. Good work. I was a professor once upon a time and the first biggest lesson I learned was that what I thought was simple was not simple to others. I like SO as a means of helping each other along. Whatever the merits of MathOverflow are the task of writing the correct **code** would be actually outside the core competences of people at MathOverflow. I'm more active on MathOverflow now (I have about 3k rep there) but I'll continue to answer math questions on SO as well. The sites complement each other well in my opinion. +1 @Mads Elvheim - thanks. You saved my day! :D it is a shame that the guy did not accept your answer! I think this could be much simpler: *px = (int)((float)x2 + vx * distance)); *py = (int)((float)y2 + vy * distance));"
776,A,"solving origin of a vectors I have two endpoints (xaya) and (xbyb) of two vectors respectively a and b originating from a same point (xo yo). Also I know that |a|=|b|+s where s is a constant. I tried to compute the origin (xo yo) but seem to fail at some point. How to solve this? so you know values of |a| and |b|? only the difference The origin will be somewhere on a hyperbolic surface. The points a and b are the foci.  In the general case there isn't a unique solution. You need another constraint.  Essentially you have two line segments and you know the position of one end for each and their length difference. This easily results in an infinite amount of points where the ends could meet and therefore doesn't uniquely identify your ""origin"". Thanks for the tip. Seems that there's plenty of mathematical solutions when looking for position-related math. Especially TDOA based locationing math are essentially the same. Usually locationing is working the other way around but it's reciprocalso I got it now. Solution became quite obvious with the additional constraint. (See e.g. Wikipedia: multilateration) So I guess it does not help if I knew range of |a| and |b| either (Vladimir: I know only the difference of |a| and |b| which is the s). Would i get an aswer to the problem if i had a third endpoint say (xcyc) and knew also that (like for b) |a|=|c|+v where v would be a constant too? Also assume |a|>0 This sounds an awful lot like trying to figure out the position of something by measuring the time difference between beacon signals that are sent from your three points at the same moment. Have you tried googling for positioning-related things?"
777,A,WPF Architecture and Direct3D graphics acceleration After reading the wikipedia article on WPF architecture I am a bit confused with the benefits that WPF will offer me. (wikipedia is not a good research reference but i found it useful). I have some questions 1) WPF uses d3d surfaces to render. However the scenegraph is rendered into the d3d surface by the media integrated layer which runs on the CPU. Is this true ? 2) I just found out by asking a question here that bitmaps dont use native resources. Does this mean that if i use alot of images the MIL will copy each when rendering rather than storing the bitmaps on the video card as a texture ? 3) The article mentions that WPF uses the painters algorithm which is back to front. Thats painfully slow. Is there any rational why WPF omits using Z-buffering and rendering front to back ? I am guessing its because the simplest way to handle transparency but it seems weak. The reason i ask is that i am thinking it wont be wise for me to put hundreds of buttons on a screen even though my colleagues are saying its directx accelerated. I dont quite believe that whole directx accelerated bit about WPF. I used to work on video games and my memory of writing d3d and opengl code tells me to be cautious. For questions #1 and #3 you might want to check out this section of the SDK that discusses the Visual class and how it's rendering instructions are exchanged between the higher level framework and the media integration layer (MIL). It also discusses why the painters algorithm is used. For #2 no that is most definitely not the case. The bitmap data will be moved to the hardware and cached there. Thanks. The architecture link was very useful in answering some of my questions :)  I tested that I wrote two programs that show 1000 buttons on screen one in WinForms and one in WPF both worked just fine. I then pushed that up to 10000 buttons at that point the WPF app took a few seconds to start but run just fine the WinForms app didn't start. Win32 itself (and WinForms) isn't built for applications with hundreds of controls (believe me I wrote such an app) at some point it just stops working WPF on the other hand keeps working even if it slows down a bit at some point. So if you do need to put a lot of controls on screen WPF is your best bet (unless you want to roll your own UI framework - and you think you can do better than the entire MS perf team). Also WPF has many advantages other than graphics acceleration: richer graphics drawing model that is easier to work with animations 3d and my personal favorite - amazing data-binding. This will let you develop richer UIs faster - and I think that will make a much bigger difference than the painting algorithm used. BTW if you need to put hundreds of buttons on the screen this is likely to be a bad user experience and you may want to reconsider your UI design Alas my application shows a view of a switchboard which has a lot of buttons to make the operator feel comfortable. The UI is so cluttered but they like it that way. The new version I am working on will have the ability to float away buttons which are not useful to reduce the clutter. I doubt i will roll out my own framework WPF will do just fine :)
778,A,"C# How to draw in front of objects? How do you draw a string that is supposed to be on top of everything else? Right now in my panel i have some user controls in Panel1.Controls. So i added this to the Paint method: Graphics g = Panel1.CreateGraphics(); g.DrawString(""BUSTED"" new Font(""Arial"" 20f) new SolidBrush(Color.Black) new PointF(50 50)); The problem is that the text is printed behind the user controls so it can't be seen. (If i change the position of the text out in the open it's displayed correctly). Any ideas? Which version of .NET are you using? I am using .Net 3.5 Isn't this a .NET issue and not a C# issue? C# is the language. .NET is the Framework. what about overriding OnPaint in Panel and writing your drawing code AFTER the base paint? class DrawingPanel : Panel { protected override void OnPaint(PaintEventArgs e) { base.OnPaint(e); // Your drawing code here } }  Try adding panel that is over all your controls then drawing in it. The panel must be set with transparency. Here's more info : http://stackoverflow.com/questions/282838/drawing-on-top-of-controls-inside-a-panel-c-winforms That won't work pre .NET 3.0 since the only way to make a transparent panel before that was using window styles and not painting the panel basically. I tried creating a transparent panel: public class TransparentPanel : Panel { public TransparentPanel() { } protected override CreateParams CreateParams { get { CreateParams createParams = base.CreateParams; createParams.ExStyle |= 0x00000020; // WS_EX_TRANSPARENT return createParams; } } protected override void OnPaintBackground(PaintEventArgs e) { } } But everything is just blurry when i apply it. I tried your code and I don't get the blur. I tried this: (This is inside the Paint method of pnlCards (Panel)) TransparentPanel mPanel = new TransparentPanel(); mPanel.Left = pnlCards.Left; mPanel.Top = pnlCards.Top; mPanel.Width = pnlCards.Width; mPanel.Height = pnlCards.Height; g = mPanel.CreateGraphics(); g.DrawString(""BUSTED"" new Font(""Arial"" 20f) new SolidBrush(Color.Black) new PointF(50 50)); this.Controls.Add(mPanel); and i got this result: http://www.pafo.net/error.jpg i want something like: http://www.pafo.net/correct.jpg  Im not sure it will work but you could try SendToBack on all user controls inside the panel then maybe the text will show on top. Alternately `BringToFront` on the control you want in front.  All windows forms controls are HWND based meaning they each have their own window handle and the z-order/parent-child of the controls determines what clips what. The panel paint event is painting onto the panel control but the panel controls output is clipped by all it's child controls. Transparency does not work correctly in windows forms. If you set something to have a transparent background it will typically wind up having the form background show through and not it's immediate parents. I have worked around this in various ways. Do all the compositing yourself by rendering everything to a bitmap and then displaying that. This only works with static content. Create a windowless rendering model that renders everything like in #1 also handles mouse interaction with its constituent elements. Create a new top level layered window (WS_EX_LAYERED) that overlays the control and renders the desired content. This prohibits interaction with the underlying control unless steps are taken to pass messages through. I believe this is the method Visual Studio 2008 uses for the windows form designer surface."
779,A,"Java - drawing many images with Graphics.drawImage() and 2-screen buffer strategy distorts and cuts images I am using a loop to invoke double buffering painting. This together with overriding my only Panel's repaint method is designed to pass complete control of repaint to my loop and only render when it necessary (i.e. some change was made in the GUI). This is my rendering routine:  Log.write(""renderer painting""); setNeedsRendering(false); Graphics g = frame.getBufferStrategy().getDrawGraphics(); g.setFont(font); g.setColor(Color.BLACK); g.fillRect(0 0 window.getWidth()window.getHeight()); if(frame != null) window.paint(g); g.dispose(); frame.getBufferStrategy().show(); As you can see it is pretty standard. I get the grpahics object from the buffer strategy (initialized to 2) make it all black and pass it to the paint method of my ""window"" object. After window is done using the graphics object I dispose of it and invoke show on the buffer strategy to display the contents of the virtual buffer. It is important to note that window passes the graphics object to many other children components the populate the window and each one in turn uses the same instance of the graphics object to draw something onto the screen: text shapes or images. My problem begins to show when the system is running and a large image is rendered. The image appears to be cut into seveeal pieces and drawn again and again (3-4 times) with different offsets inside of where the image is supposed to be rendered. See my attached images: This is the original image: This is what I get: Note that in the second picture I am rendering shapes over the picture - these are always at the correct position. Any idea why this is happening? If I save the image to file as it is in memory right before the call to g.drawImage(...) it is identical to the original. Uh you are using Swing ? Normally Swing automatically renders the image you can't switch it off. The repaint() method is out of bounds because Swing has a very complicated rendering routine due to method compatibility for AWT widgets and several optimizations inclusive drawing only when necessary ! If you want to use the High-Speed Drawing API you use a component with a BufferStrategy like JFrame and Window use setIgnoreRepaint(false); to switch off Swing rendering set up a drawing loop and paint the content itself. Or you can use JOGL for OpenGL rendering. The method you are using seems completely at odds with correct Java2D usage. Here the correct use: public final class FastDraw extends JFrame { private static final transient double NANO = 1.0e-9; private BufferStrategy bs; private BufferedImage frontImg; private BufferedImage backImg; private int PIC_WIDTH PIC_HEIGHT; private Timer timer; public FastDraw() { timer = new Timer(true); JMenu menu = new JMenu(""Dummy""); menu.add(new JMenuItem(""Display me !"")); menu.add(new JMenuItem(""Display me too !"")); JMenuBar menuBar = new JMenuBar(); menuBar.add(menu); setJMenuBar(menuBar); setIgnoreRepaint(true); setVisible(true); addWindowListener(new WindowAdapter() { public void windowClosing(WindowEvent evt) { super.windowClosing(evt); timer.cancel(); dispose(); System.exit(0); } }); try { backImg = javax.imageio.ImageIO.read(new File(""MyView"")); frontImg = javax.imageio.ImageIO.read(new File(""MyView"")); } catch (IOException e) { System.out.println(e.getMessage()); } PIC_WIDTH = backImg.getWidth(); PIC_HEIGHT = backImg.getHeight(); setSize(PIC_WIDTH PIC_HEIGHT); createBufferStrategy(1); // Double buffering bs = getBufferStrategy(); timer.schedule(new Drawer()020); } public static void main(String[] args) { new FastDraw(); } private class Drawer extends TimerTask { private VolatileImage img; private int count = 0; private double time = 0; public void run() { long begin = System.nanoTime(); Graphics2D g = (Graphics2D) bs.getDrawGraphics(); GraphicsConfiguration gc = g.getDeviceConfiguration(); if (img == null) img = gc.createCompatibleVolatileImage(PIC_WIDTH PIC_HEIGHT); Graphics2D g2 = img.createGraphics(); // Zeichenschleife do { int valStatus = img.validate(gc); if (valStatus == VolatileImage.IMAGE_OK) g2.drawImage(backImg00null); else { g.drawImage(frontImg 0 0 null); } // volatile image is ready g.drawImage(img050null); bs.show(); } while (img.contentsLost()); time = NANO*(System.nanoTime()-begin); count++; if (count % 100 == 0) System.out.println(1.0/time); } } Thanks for your response. I don't understand the part whre you check the volatile image's status. If it is OK you draw to g2 but if not you draw to g? Why? And I guess that frontImg and backImg are two different images you're using to debug this code? Also why are you repeating the drawing over and over while the contants of the volatile image are lost? Even though I implemented your suggestion the problem persists."
780,A,"Merge overlapping triangles into a polygon I've got a bunch of overlapping triangles from a 3D model projected into a 2D plane. I need to merge each island of touching triangles into a closed non-convex polygon. The resultant polygons shouldn't have any holes in them (since the source data doesn't). Many of the source triangles share (floating point identical) edges with other triangles in the source data. What's the easiest way to do this? Performance isn't particularly important since this will be done at design time. See also ""Union of complex polygons"": http://stackoverflow.com/questions/2667748/union-of-complex-polygons Try gpc or the General Polygon Clipper Library. unfortunately this will be used in a commercial game; not sure how UManchester would feel about that. gpc has a commercial license as well. Moreover gpc lists other similar libraries at http://www.cs.man.ac.uk/~toby/alan/software/#Links . Perhaps one of those has a more suitable license.  Imagine the projection onto a plane as a ""view"" of the model (i.e. the direction of projection is the line of sight and the projection is what you see). In that case the borders of the polygons you want to compute correspond to the silhouette of the model. The silhouette in turn is a set of edges in the model. For each edge in the silhouette the adjacent faces will have normals that either point away from the plane or toward the plane. You can check this be taking the dot product of the face normal with the plane normal -- look for edges whose adjacent face normals have dot products of opposite signs with the projection direction. Once you have found all the silhouette edges you can join them together into the boundaries of the desired polygons. Generally you can find more about silhouette detection and extraction by googling terms like mesh silouette finding detection. Maybe a good place to start is here. This isn't quite adequate -- for one I have several model parts whose projections overlap. @nomagon good point. Also are your models closed? I.e. do they have an inside and an outside like a sphere or a torus? Or are they just a general polygon soup?  I've also found this[1] approach which I will be trying next. [1] http://stackoverflow.com/questions/1014293/2d-outline-algorithm-for-projected-3d-mesh"
781,A,"Combined re-scaling and color reduction of an image in Java? Given a rectangular input image I'd like to create an output image of size 40x40 pixels using a maximum of 10 colors. Hence the two operatons needed are re-scaling and color reduction. The following ImageMagick command does the trick: convert input.png -scale 40x40 -colors 10 output.png How would you achieve the corresponding result in Java? Shelling out to ImageMagick is not an option :-) Look up Java Advanced Imaging (JAI) at jai.dev.java.net  Something like this would work using JAI:  // now resize the image ParameterBlock pb = new ParameterBlock(); pb.addSource(image); // The source image pb.add(wScale); // The xScale pb.add(hScale); // The yScale pb.add(0.0F); // The x translation pb.add(0.0F); // The y translation RenderingHints hints = new RenderingHints(RenderingHints.KEY_RENDERING RenderingHints.VALUE_RENDER_QUALITY); RenderedOp resizedImage = JAI.create(""SubsampleAverage"" pb hints); // lastly write the newly-resized image to an // output stream in a specific encoding try { FileOutputStream fos = new FileOutputStream(new File(filename)); JAI.create(""encode"" resizedImage fos getImageType(filename) null); ParameterBlock ParameterBlock pb = new ParameterBlock(); ColorModel cm = new ComponentColorModel(ColorSpace.getInstance(ColorSpace.TYPE_YCbCr) new int[] {8} false false Transparency.OPAQUE DataBuffer.TYPE_BYTE); pb.add(cm); RenderedOp imgycc = JAI.create(""ColorConvert"" pb); } catch (FileNotFoundException e) { }"
782,A,"Newell's Method to Calculate Plane Equation of Concave Polygon - Improvements? 3 points are needed to define a plane. Newell's method is known to fail if the 3 points are chosen around a concave corner - the normal of the resulting plane will point in the direction opposite to the expected one. Are there any improvements to Newell's method that help in choosing a valid starting point? Or is there an alternative algorithm that doesn't have this issue? Since your only interest in the plane is whether a given point is ""deeper"" or ""closer"" than it I suppose you expect the normal of the plane to point towards the viewpoint (or away from it depending on your convention). So just calculate the dot-product of the normal and the vector from the viewpoint to one of the three points and look at its sign; if it's positive when you generally expect negative (or vice-versa) then reverse the normal.  Just came across this after some Googling for Newell's Plane algorithm. I'm also interested in using the algorithm but my reading of various literature leaves me thinking that it is the ""three point"" method that fails on the concave corner case while Newell's Method will work correctly. Here is the relevant passage: This technique first suggested by Newell (Sutherland et al. 1974) works for concave polygons and polygons containing collinear vertices as well as for nonplanar polygons e.g. polygons resulting from perturbed vertex locations... Newell's method may seem inefficient for planar polygons since it uses all the vertices of a polygon when in fact only three points are needed to define a plane. It should be noted though that for arbitrary planar polygons these three points must be chosen very carefully: Three points uniquely define a plane if and only if they are not collinear; and if the three points are chosen around a ""concave"" corner the normal of the resulting plane will point in the direction opposite to the expected one. Checking for the properties would reduce the efficiency of the three-point method as well as making its coding rather inelegant. A good strategy may be that of using the three-point method for polygons that are already known to be planar and strictly convex (no collinear vertices) and using Newell's method for the rest. Source: Filippo Tampieri. “Newell's Method for Computing the Plane Equation of a Polygon”. In Graphics Gems III Academic Press 1992 pp. 231–232."
783,A,"How to force Mac window to foreground? How can I programmatically force a mac window to be the front window? I have the window handle and want to ensure that my window is displayed above all other windows. I can use both Carbon & Cocoa for this. Are you talking about a window in your own program? Or do you want to deal with other app's windows? Raymond Chen wrote about this very problem when he asked the important question: ""What if two programs did this?"" Once you realize that you'll understand that never can _force_ this only _request_ this. If you can (32 bit only) use kOverlayWindowClass: WindowRef carbon_window = NULL; CreateNewWindow( kOverlayWindowClass  ...  &carbon_window ); // if you need cocoa: NSWindow *cocoa_window = [[NSWindow alloc] initWithWindowRef:carbon_window]; Otherwise create an NSWindow and set the window level to kCGOverlayWindowLevel. Note that this will be above the dashboard as well. If you want to be below the dashbord use kCGUtilityWindowLevel.  For Cocoa you can set the window level using: [window setLevel:NSFloatingWindowLevel]; A floating window will display above all other regular windows even if your app isn't active. If you want to make your app active you can use: [NSApp activateIgnoringOtherApps:YES]; and [window makeKeyAndOrderFront:nil];"
784,A,"As relates to texture atlases what is a ""quad""? It is my understanding that a texture atlas is basically a single texture that contains many smaller textures and that they are useful for making games or animations faster because they allow you to access many animation frames by loading a single file rather than files for each and every frame. So in discussions of texture atlases I see the term ""quad"" mentioned everywhere - Is a quad simply the x y width and height of an individual texture from a texture atlas or am I missing something? Quadrilateral - not necessarily a rectangle. So it's four xy coordinates that define a four-sided polygon? Doh! http://en.wikipedia.org/wiki/Quadrilateral"
785,A,"Blackberry - Small Up Down Arrow Button i want to create 2 buttons for blackberry which look like this... and the second one inverted of the above i wanted to do this without using images (for efficiency) and the buttons should appear only when there is focus on them and dis sapper when focus goes off.. ArrowButtonField as a Field extention: class ArrowButtonField extends Field { public static final int TYPE_UP = 0; public static final int TYPE_DOWN = 1; private int mBackgroundColor = Color.WHITE; private int mFillColor = Color.CRIMSON; private int mWidth = 20; private int mHeight = 12; private int mArrowType = TYPE_UP; public ArrowButtonField(int bgColor int fillColor int arrowType) { super(FOCUSABLE); setMargin(0 0 0 0); setPadding(0 0 0 0); mArrowType = arrowType; mBackgroundColor = bgColor; mFillColor = fillColor; } // cancel theme border and background style protected void applyTheme(Graphics arg0 boolean arg1) { } protected boolean navigationUnclick(int status int time) { fieldChangeNotify(0); return true; } protected void onUnfocus() { invalidate(); super.onUnfocus(); } protected void paint(Graphics graphics) { graphics.clear(); graphics.setColor(mBackgroundColor); graphics.fillRect(0 0 mWidth mHeight); if (isFocus()) { graphics.setColor(mFillColor); int xc = 10; int y1 = 0 y2 = 0 x2 = xc - 9 x1 = xc + 9; switch (mArrowType) { case TYPE_DOWN: y1 = 11; y2 = 1; break; case TYPE_UP: y1 = 1; y2 = 11; break; default: break; } int[] xPts = new int[] { x1 x2 xc }; int[] yPts = new int[] { y1 y1 y2 }; graphics.drawFilledPath(xPts yPts null null); } } public int getPreferredWidth() { return mWidth; } public int getPreferredHeight() { return mHeight; } protected void layout(int width int height) { setExtent(mWidth mHeight); } } Classes for Up and Down Arrow: class UpArrowButtonField extends ArrowButtonField { public UpArrowButtonField(int backgroundColor int fillColor) { super(backgroundColor fillColor TYPE_UP); } } class DownArrowButtonField extends ArrowButtonField { public DownArrowButtonField(int backgroundColor int fillColor) { super(backgroundColor fillColor TYPE_DOWN); } } Sample of use: class Scr extends MainScreen implements FieldChangeListener { UpArrowButtonField arrowUp; DownArrowButtonField arrowDown; public Scr() { arrowUp = new UpArrowButtonField(Color.WHITE Color.RED); arrowUp.setChangeListener(this); add(arrowUp); arrowDown = new DownArrowButtonField(Color.WHITE Color.RED); arrowDown.setChangeListener(this); add(arrowDown); } public void fieldChanged(Field field int context) { if (field == arrowUp) { Dialog.inform(""UP""); } else if (field == arrowDown) { Dialog.inform(""DOWN""); } } }  You can create a custom Field object then implement a custom paint() method that draws the geometric shapes you require. Take a look at the Graphics class - your field's paint method is passed the Graphics object and you can make use of that to draw filled rectangles polygons etc."
786,A,"concept question about dll My boss asks me to create a dll file using C++. The dll file needs to do the following: create a blank area in Window create some simple shapes (for an example a rectangle) on the blank area control the locations of the shapes in the blank area I am new to C++ so please correct me if my understand is incorrect Dll is a binary file does it allow to call other libraries to create the blank area? ""Window 2"" ???? http://en.wikipedia.org/wiki/Dynamic-link_library the DLL should be called from some executable and the dll can also call other dll's functions. While creating a dll you need to create an executable to test the dll and you can use other dll by dynamically loading or using its .lib in the project."
787,A,"Load and save bitmaps using dotnet This may be simple one but 5 mins of Googling didn't give me the answer. How do you save and load bitmaps using .Net librabries? I have an Image object and I need to save it to disk in some format (preferably png) and load back in later. A C# example would be great. Here's a really simple example. Top of code file using System.Drawing; In code Image test = new Bitmap(""picture.bmp""); test.Save(""picture.png"" System.Drawing.Imaging.ImageFormat.Png); Remember to give write permissions to the ASPNET user for the folder where the image is to be saved. Make sure you have write permissions for ASP.NET on the folder you want to save. `Bitmap` is in the `System.Drawing` assembly. OK added it into the answer.  Hiya use the Image.Save() method. A better explanation and code sample than I could provide can be found here: MSDN  About 10 seconds of google lead me to this example for the save method you can dig around a bit more for the others."
788,A,"How-to draw a circle on a changing background There is a timer on the form and in its Tick event I've:  this.BackColor = ColorTranslator.FromHtml(""#"" + ColorCodesBack[_index]); CurrentColor = ColorTranslator.FromHtml(""#"" + ColorCodesFore[_index]); SolidBrush sb = new SolidBrush(CurrentColor); g.FillEllipse(sb this.Width/2 -200 this.Height/2 - 200 200 200); g.DrawImage(b 150 150); The problem is just background changes on every tick and I don't see a Circle on the form. What you should do is put your code in the forms Paint event. That will cause your code to redraw ever time the form has to repaint. Like running your mouse over the form or moving the form. Also where are you declaring your graphic object? Because the only way it will be drawn on your form is if you do: Graphics g = this.CreateGraphics(); If you use the paint event you won't even need a timer object. Thanks I figured already and you're right I should put this code into Paint event.  You should combine this code with the same tick event that is changing the backgroud. Otherwise you effectively have a race condition as to which ""tick"" will be fired first. Combining them will solve that problem. They're already in same Tick. The first line is changing the background.  The suggestion to update the background and the foreground at the same time is solid. If you cannot control this perhaps you could add a transparent control to your window on top of the background and draw onto the transparent control? That way you would always be above in the Z-Order. This would also reduce your need to redraw regularly. Well I tried to draw the circle on a Panel. But it didn't work.. You've a code sample to draw a circle on a transparent control?"
789,A,c#: Rotating Text within a custom control I am attempting to rotate some text within a label. I have a cusom label which allows me to have control over the text rendering process. protected override void OnPaint ( PaintEventArgs pe ) { Graphics g = pe.Graphics; g.RotateTransform( angle ); g.drawString( text ); g.ResetTransform(); } The problem I am having is that the rotation appears to occur aroundthe origin of the control i.e. co-ordinates (00). Is there a method to allow the text to rotate about the center of the control rather than the oragin? I am aware of the function 'g.RotateTransform( )' so one possible solution would be to rotate the text and then translate it to the centre of the control. If this is the only way to manage the job is there a generic manner which I can caluclate the transfor to ensure that the text is in the centre of the control? Thanks A rotation around an arbitrary point is usually a translation of that point to the origin then a rotation and a translation back again. The problem would probably be to determine the dimensions of the text after rotation to move it back accordingly.
790,A,How do you best make complicated resizable graphics in Silverlight? In WPF it is very easy to make resizable graphics/artwork. You simply create a DrawingBrush and use it as the fill for a Rectangle or a Shape. Or you can simply use a VisualBrush that refers to Canvas with Shapes/Paths on it. In Silverlight though there isn't a DrawingBrush and there isn't a VisualBrush. So how does one best create graphics in Silverlight that are resizable? I've played around with using Path and setting the Stretch property to Uniform ... and even taking a couple paths and combining them into a compound path ... but there are limits to this approach (e.g. the fill doesn't always fill where you want it to if the paths overlap). I've also simply put a Viewbox around a Canvas that has Shapes/Paths on it. How do you do it? And what do you think the best practice is? Hmm. The more I play with this the more I like simply putting a Viewbox around something. I guess that does come at the cost of another reference (i.e. to the Silverlight Toolkit) ... since the Silverlight Viewbox currently lives in the toolkit ... but you are already probably referencing that anyways ... and eventually Viewbox will get rolled into the Silverlight SDK proper. I still miss DrawingBrush and VisualBrush though and wish Silverlight had them. Problem is someone else will miss some other feature and yet another person will miss something else entirely. We might wish that SL was completely compatible with WPF but then we'd still want it all to fit in a 5MB download be entirely sandboxed so it can't be mis-used and oh yes run across multple platforms. Ha! True very true.
791,A,Organizing Master Graphics Files For A Web Application (Photoshop / Gimp) Some of us programmers have to deal with graphic files every once and awhile... Making quick fixes to existing graphics Throwing together a quick sprite as a prototype Create an icon because the designer is too busy I don't think anybody should ever be editing web graphics (jpg / gif / png) directly. They should edit the master file and export over top of the web graphics. Right now all the psd files in our system are either on the designers or various other peoples drives and if a programmer ever needs to make a proper change they can't without high overhead. How should I approach solving this impediment? Save the *.psd / *.xcf files right along side the web graphics *.jpg *.png *.gif so they're easy to find. Either train the Graphic design in source control or have somebody responsible for integration. I tend to like this because it make everything very discoverable and you can just have your web publishing system skip *.pdf *.xcf. However I can also see a coder not liking 2mb psd files crufting up their repository and having to download that on svn update. Create a separate source control repository or network share for master graphics files. Continue as is... Any other suggestions? I'd go for option 2. As you say the overhead of large PSDs in your source repository is probably too high for comfort! Have you looked into scripting Photoshop export so that the 'Change PSD file' -> 'Updated graphics on website' workflow is as compact and easy as possible? That'll make the whole enterprise really attractive for everyone IMO; especially if its set to run as a build step on commit. EDIT: Photoshop seems to support Javascript as a scripting language so getting a satisfying result should be fairly straightforward http://morris-photographics.com/photoshop/tutorials/scripting1.html  I always treat the PSD's used on the projects I work on as source files and the jpegs and gifs that are exported from those PSD's as binaries. Same goes for Word files that turn into PDF's. My trunk normally has two subfolders: trunk /source /deploy Everything that doesn't go up onto a server belongs in source folder and everything else belongs in deploy. If I create a database export that serves as a backup then I store it in a sub-folder in the source folder (since it doesn't go onto the server). There are purists that claim that binary files do not belong in version control. I say that is BS. A PSD is a source file for your jpegs and gifs in the same way a .java file is the source to a .class file. I say tread it as such. +1 (But as if you use word files to make pdfs... use LaTeX!)
792,A,Any good library for generating graphs for the iPhone? Possible Duplicate: Is there a good charting library for iPhone? I need a charting library that could work on the Iphone standalone (no Internet). I only need bars & pie. Is for a bussines-oriented app. If not exist then how integrate a JS one? This is a duplicate of http://stackoverflow.com/questions/769749/is-there-a-good-charting-library-for-iphone and http://stackoverflow.com/questions/263472/cocoa-graphing-plotting-framework-that-works-on-iphoneos MM... I see.. Then what about embed a JS library? You can have a look at DSBarChart which does exactly what you are looking for. You are posting a lot of ads for your products/website lately. @AndrewBarber Oops. Sorry. Just wanted to share something that I created. Posting ads wasn't my intention. Apologies. Will not repeat this. Apologies again.  Since this was asked an excellent open source framework has come available: http://code.google.com/p/core-plot Why was this up-voted? It has the same answer as the existing answer.  There are no mature graph libraries for the iPhone (yet). You may want to keep an eye on the Core Plot project which is developing a Mac OS X and iPhone plotting library that makes use of Core Animation on both targets. It's currently not ready for production use. In particular bar and pie charts are not implemented yet. Core Plot is the subject of WWDC 2009's scientific coding project however so expect it to make significant progress during and after WWDC. In terms of embedding a JavaScript charting library you can always use a WebKit view to host a web page containing whatever JavaScript you want. You will likely be interested in this sample code showing how to call into JS from Objective-C and visa versa.
793,A,Load NSImage into QPixmap or QImage I have an NSImage pointer from a platform SDK and I need to load it into Qt's QImage class. To make things easier I can create a QImage from a CGImageRef by using QPixmap as an intermediate format like this: CGImageRef myImage = // ... get a CGImageRef somehow. QImage img = QPixmap::fromMacCGImageRef(myImage).toImage(); However I cannot find a way to convert from an NSImage to a CGImageRef. Several other people have had the same problem but I have yet to find a solution. There is the CGImageForProposedRect method but I can't seem to get it to work. I'm currently trying this (img is my NSImage ptr): CGImageRef ir = [img CGImageFirProposedRect:0:0:0]; Any ideas? // Sample to create 16x16 QPixmap with alpha channel using Cocoa const int width = 16; const int height = 16; NSBitmapImageRep * bmp = [[NSBitmapImageRep alloc] initWithBitmapDataPlanes:NULL pixelsWide:width pixelsHigh:height bitsPerSample:8 samplesPerPixel:4 hasAlpha:YES isPlanar:NO colorSpaceName:NSDeviceRGBColorSpace bitmapFormat:NSAlphaFirstBitmapFormat bytesPerRow:0 bitsPerPixel:0 ]; [NSGraphicsContext saveGraphicsState]; [NSGraphicsContext setCurrentContext:[NSGraphicsContext graphicsContextWithBitmapImageRep:bmp]]; // assume NSImage nsimage [nsimage drawInRect:NSMakeRect(00widthheight) fromRect:NSZeroRect operation: NSCompositeSourceOver fraction: 1]; [NSGraphicsContext restoreGraphicsState]; QPixmap qpixmap = QPixmap::fromMacCGImageRef([bmp CGImage]); [bmp release];  NSImage is a high level image wrapper that might contain more than one image (thumbnails different resolutions vector representations ...) and does a lot of caching magic. A CGImage on the other hand is one plain bitmap image. Since NSImage is a so much richer object there’s no easy way of converting in between the two. To get a CGImageRef from an NSImage you have some options: Manually select an NSBitmapImageRep from the NSImage (using [img representations]) and get the CGImage from that. Setup a graphics context (CGBitmapContextCreate) draw the image into that and create a CGImage from this context. Use the new Snow Leopard API to create the CGImage directly from the NSImage: [img CGImageForProposedRect:NULL context:nil hints:nil] That works perfectly thanks!
794,A,What exactly is a surface in OpenGL ES or Direct3D? I did not find a good definition of the concept of a drawing surface. What properties are associated with a surface? In the context of pure OpenGL there is no surface since OpenGL has no notion of window system specific things. In OpenGL ES though you have the EGL API which introduces the concept of a drawing surface without defining it properly. What is your concise definition of a drawing surface? In the Direct3D world broadly speaking a surface is some 2D image data. A texture is something that can be sampled and used in a shader. Typically textures are 'made of' surfaces; for example each mip-map of a 2D texture is a surface and each face of a cube map is a surface.  Basically a surface is something that you can render to. It's a kind of device context but potentially smarter since surfaces may know how to display themselves or do other useful things. EGL has three surface types: Window Surface: a window. Pixmap Surface: an image. Pbuffer Surface: a pixel buffer. This forum post may be helpful.  In Direct3D a hardware surface is typically -- but not always -- a section of hardware memory in the DirectDraw surface format. This is the same format used by DDS image files and basically consists of a header and then image data in one of several image formats that are specified in the header section. The usual properties are width height pixel format and maybe a few misc things like stereo (which may not actually be supported of course). It's basically not much more than a generic term for an image.
795,A,"Why does a SolidBrush result in a patterned image? I'm trying to dynamically generate a gif image of a specified size and color in an HttpHandler. For some reason the image is being generated with a dithered color pattern rather than the solid color I expected the SolidBrush to create. The following code is the simplest I could find that exposes the problem. private void GenerateSquareImage(String filename String color Int32 width) { Bitmap img = new Bitmap(width width); Graphics g = Graphics.FromImage(img); SolidBrush brush = new SolidBrush(ColorTranslator.FromHtml(""#"" + color)); g.FillRectangle(brush 0 0 width width); img.Save(filename System.Drawing.Imaging.ImageFormat.Gif); img.Dispose(); brush.Dispose(); g.Dispose(); } Here is a sample of the results I'm getting when passing ""415976"" in the color parameter: (The block on the left is an HTML SPAN with background-color set to #415976. The block on the right is my graphic where you can see the dithered colors.) The result is the same if I use this code: g.Clear(ColorTranslator.FromHtml(""#"" + color)); in place of the g.FillRectangle call in the method above. UPDATE: By default GDI+ saves GIFs with the web safe palette if any programmatic changes are made to the Bitmap. This is what was causing my non safe palette color to be dithered. I was able to modify the code referenced in Marco's post below (http://msdn.microsoft.com/en-us/library/aa479306.aspx) to modify the palette on my freshly created image so that my color was respected. GIF files are limited to 256 colors as far as I know so it resorts to dithering. PNG is probably the format you need. Or you can look here for a solution using a complex quantizer to build a custom palette (I don't take responsibility for the code in the link never tried myself). This other link from Microsoft has an in-depth analysis of the problem. Hope this helps. See my update to the original question. This answer eventually pointed me to the solution.  IIRC GIF's are limited to a specific pallete on .NET. There are workarounds but they were above my understanding."
796,A,"How do I bounce a point off of a line? I'm working on writing a Pong game and I've run across a problem. I'm trying to figure out how to bounce a point off of a line. The best method I can figure out to do this with is Calculate the current and future position of the Ball. Line Segment: {Ball.location Ball.location + Ball.direction} (Ball.location and Ball.direction use a custom vector/coordinate class) Calculate if the generated line segment intersects with any of the walls or paddles. ??? Don't know how to do this yet (Will ask in a separate question) At the first intersection found Bounce the ball off of the line Create a triangle formed with a = Ball's current position b = Intersection point of the line. c = Closest point to the Ball's current position on the line. Find the angle that the ball hits the line angle = cos(distance(b c) / distance(a b)) Find the angle to rotate the ball's direction (90 - angle)*2 Rotate Ball's direction and move it to it's new position ignoring distance traveled to hit the line for now doesn't need to be exactly on the line Else if there is no intersection Move the ball to it's new position. Is this an acceptable method or am I missing something? In your game loop calculate the new ball position as if there were no obsticals. Then check to see if the new position intersects with or is PAST the obstical edge. If either condition is met then calculate a new ball position by bouncing it off the obstical. A method that calculates a point given a starting point angle and distance would come in handy: Public Function TranslatePoint(ByVal tStartPoint As Point _ ByVal tAngle as Decimal ByVal tDistance As Decimal) As Point I forgot to mention that I'm doing this in C++ with OpenGL. It was an example.  If you want the authentic Pong feel then computing the new vector is much easier than doing real reflections. Pong just reverses the sign of either dx or dy. If the ball hits a horizontal barrier then negate dy. Vertical then negate dx. That would make things alot easier. I was trying to make a more flexible model but I'm not really doing anything fancy. I might just go for this. I would mark this as accepted as well if I could. I implemented this instead of going for a generic formula which saved me enough time to implement some shiny lighting which probably helped my grade more than anything else I was trying to do.  You just need to check if the center of the ball is within its radius of the paddle to tell whether or not its time to bounce. There was an older question asked that has several answers on calculating bounce angle. You are forgetting bouncing off one of the side walls or going out of bounds. It's the same calculation. If the ball gets within its radius of an obstacle its direction should change. Just check to see if it goes beyond a certain X coordinate to tell if it's out of bounds. What do you mean by ""within it's radius"". I rejected a bounce when near method because it would be possible for the ball to travel beyond the nearness value in a single step. (There is a command to accelerate or freeze the ball for x steps) If it's possible for the ball to travel beyond the nearness value in a single step then your update rate is too slow. I'll look at doing this method then. I'll get back to you on how well it works :) I implemented this method and it worked quite well. Sorry for taking so long to get back to you on this. Cool thanks for the feedback. Do you have any code short enough to post? :)  Here is what I would do: 1) assume your pong sides are at the lines x=a and x=b 2) let your balls position be (x0 y0) and traveling on the line Ax+By=C 3) from this you can calculate easily (x1 y1) the point where that line intersects the wall 4) you can get the equation of the line after the bounce by knowing that (x0 y1+y0) is the reflection after the bounce. (The ball is y1-y0 below the intersection point and hence will be y1+y0 above the intersection point.) 5) given the 2 equations you can plot your points (assuming you know the velocity).  Bouncing is a problem from the physics domain not the graphics domain. Therefore OpenGL offers no real support for it. If you look at the hardware you'll notice that there have been seperate physics cards similar to videocards although the most recent trends are to integrate them.  You may be interested in the N Tutorials which discuss collision detection and response (including bounce and friction) used in the game ""N"". The tutorials may go deeper than you particularly need but they're well-written and have interactive examples -- and simple bounce is one of the first topics."
797,A,"BlackBerry - Convert EncodedImage to byte [] I am using below code where i don't want to use JPEGEncodedImage.encode because it increases the size. So I need to directly convert from EncodedImage to byte array. FileConnection fc= (FileConnection)Connector.open(name); is=fc.openInputStream(); byte[] ReimgData = IOUtilities.streamToBytes(is); EncodedImage encode_image = EncodedImage.createEncodedImage(ReimgData 0 (int)fc.fileSize()); encode_image = sizeImage(encode_image (int)maxWidth(int)maxHeight); JPEGEncodedImage encoder=JPEGEncodedImage.encode(encode_image.getBitmap()50); ReimgData=encoder.getData(); is.read(ReimgData); HttpMultipartRequest( content[0] content[1] content[2] params ""image""txtfile.getText() ""image/jpeg"" ReimgData ); Try EncodedImage.getData(): public final byte[] getData() Returns the encoded data of this image. Returns: A byte array containing the encoded data for this image. Since: JDE 3.7.0 This is right. thanks for solution. You're welcome!"
798,A,"Occlusion algorithms collection The occlusion algorithm is necessary in CAD and game industry. And they are different in the two industries I think. My questions are: What kind of occlusion algorithms are applied respectively in the two indurstries? and what is the difference? I am working on CAD software development and the occlusion algorithm we have adopted is - set the object identifier as its color (a integer) and then render the scene at last read the pixel to find out the visible objects. The performance is not so good so I want to get some good ideas here. Thanks. After read the anwsers I want to clarify that the occlusion algorithms here means ""occlusion culling"" - find out visible surface or entities before send them into the pipeline. With google I have found a algorithm at gamasutra. Any other good ideas or findings? Thanks. It sounds like you are using an item buffer: http://stackoverflow.com/questions/498601/what-is-the-best-approach-to-compute-efficiently-the-first-intersection-between-a/570861#570861 That's not a bad algorithm but will have some issues at the visual edges. The term you should search on is hidden surface removal. Realtime rendering usually takes advantage of one simple method of hidden surface removal: backface culling. Each poly will have a ""surface normal"" point that is pre-calculate at a set distance from the surface. By checking the angle of the surface normal with respect to the camera you'd know that the surface is facing away and therefore does not need to be rendered. Here's some interactive flash-based demos and explanations. This process is necessary but not sufficient. Any concave object will still need to have the surfaces rendered in the correct order and if one object appears in front of another then simple backface culling doesn't help. I named backface culling because Z-buffering was already mentioned. If I wrote any more then I'd just be mostly reiterating the Wikipedia page I linked.  Hardware pixel Z-Buffering is by far the simplest technique however in high-density object scenes you can still be trying to render the same pixel multiple times which may become a performance problem in some situations. - You certainly need to make sure that you're not mapping and texturing thousands of objects that just aren't visible. I'm currently thinking about this issue in one of my projects I've found this stimulated a few ideas: http://www.cs.tau.ac.il/~dcor/Graphics/adv-slides/short-image-based-culling.pdf  In games occlusion is done behind the scene using one of two 3D libraries: DirectX or OpenGL. To get into specifics occlusion is done using a Z buffer. Each point has a Z component points that are closer occlude points that are further away. The occlusion algorithm is usually done in hardware by a dedicated 3D graphics processing chip that implements DirectX or OpenGL functions. A game program using DirectX or OpenGL will draw objects in 3D space and have the OpenGL/DirectX library render the scene taking into account projection and occlusion.  It stuck me that most of the answers so far only discuss image-order occlusion. I'm not entirely sure for CAD but in games occlusion starts at a much higher level using BSP trees oct trees and/or portal rendering to quickly determine the objects that appear within the viewing frustum. Yes; but apart from pixel based occlusion D3D nor OpenGL do scene graph ""magic"". Clipping and culling isn't enough to justify sending all the polygons known to the rendering device since clipping scales somewhat linearly. +1. DirectX/OpenGL already does a decent job of drawing just the things you see (via a Z-buffer). If you wish to limit the amount of stuff you're rendering to get a better performance you have to look into one of these algorithms."
799,A,LPD3DXFONT DrawText using DT_CALCRECT? How do I use DT_CALCRECT to determine my rectangle bottom and right coords? e.g I have this rect: RECT textPos; textPos.left = 100; textPos.right = 100; What do I do next to calculate the rect and draw the text? Mmm you just make a call to DrawText with the DT_CALCRECT parameter set and the pointer to your original rectangle. It will modify the rectangle extending the bottom and right values. Then you make another call to DrawText with your updated rectangle and whatever DT_ parameter needed. http://msdn.microsoft.com/en-us/library/ms901121.aspx
800,A,Drawing a Rotated Rectangle I realize this might be more of a math problem. To draw the lines for my rectangles I need to solve for their corners. I have a rectangle center at (xy) With a defined Width and Height. To find the blue points on a non rotated rectangle on top (angle = 0) It is UL = (x-Width/2)(y+height/2) UR = (x+Width/2)(y+height/2) LR = (x+Width/2)(y-height/2) LL = (x-Width/2)(y-height/2) How do I find the points if the angle isn't 0? Thanks in advance. Update: although I have (00) in my picture as the center point most likely the center point won't be at that location. You have n't defined correctly even the corner points when Angle is 0 Please refer my solution I've just fixed them. If 'theta' is the anti-clockwise angle of rotation then the rotation matrix is: | cos(theta) -sin(theta) | | sin(theta) cos(theta) | i.e. x' = x.cos(theta) - y.sin(theta) y' = x.sin(theta) + y.cos(theta) There's examples of other transformations at http://en.wikipedia.org/wiki/Transformation_matrix  Rotation matrix (this is becoming a FAQ)  One of the easiest ways to do this is to take the location of the point before rotation and then apply a coordinate transform. Since it's centred on (00) this is simply a case of using: x' = x cos(theta) - y sin(theta) y' = y cos(theta) + x sin(theta)  See 2D Rotation. q = initial angle f = angle of rotation. x = r cos q y = r sin q x' = r cos ( q + f ) = r cos q cos f - r sin q sin f y' = r sin ( q + w ) = r sin q cos f + r cos q sin f hence: x' = x cos f - y sin f y' = y cos f + x sin f  First transform the centre point to 00 X' = X-x Y' = Y-y Then rotate for an angle of A X'' = (X-x) * cos A - (Y-y) * sin A Y'' = (Y-y) * cos A + (X-x) * sin A Again transform back the centre point to xy X''' = (X-x) * cos A - (Y-y) * sin A + x Y''' = (Y-y) * cos A + (X-x) * sin A + y Hence compute for all 4 points of (XY) with following transformation X''' = (X-x) * cos A - (Y-y) * sin A + x Y''' = (Y-y) * cos A + (X-x) * sin A + y where x y are the centre points of rectangle and XY are the corner points You have n't defined correctly even the corner points when Angle is 0 as I have given in the comments. After substituting you will get UL = x + ( Width / 2 ) * cos A - ( Height / 2 ) * sin A  y + ( Height / 2 ) * cos A + ( Width / 2 ) * sin A UR = x - ( Width / 2 ) * cos A - ( Height / 2 ) * sin A  y + ( Height / 2 ) * cos A - ( Width / 2 ) * sin A BL = x + ( Width / 2 ) * cos A + ( Height / 2 ) * sin A  y - ( Height / 2 ) * cos A + ( Width / 2 ) * sin A BR = x - ( Width / 2 ) * cos A + ( Height / 2 ) * sin A  y - ( Height / 2 ) * cos A - ( Width / 2 ) * sin A I think this suits your solution. Thanks that worked for me. Perfect! thanks for sharing. hi @lakshmanaraj  could u pls tell me here what is X value and x value
801,A,C#: Creating a graphical screensaver I am thinking about creating a screen saver. I have the whole thing its graphics and the back-end kind of ready in my head. But I am not quite sure how to get the graphics out in code. What I would like is a slide show of images with a bit of movement (kind of like the slide show in media center) and some floating text and shapes on top. The shapes somehow translucent. I currently have a very simple static slideshow made in WinForms. Just a simple application that goes fullscreen and displays some images and pretends to fade them in and out in a hackish kind of way. But it is not very well made and the performance is not very good. For example to prevent lag I fade in a black square on top of the image instead of fading in the actual image. Silly perhaps but it kind of worked :p Anyways I would like to do a better job. But not sure where to start. Is WPF a good solution for this? Or should I look into DirectX or OpenGL? Is this something that could be handled well with XNA or is that too game spesific? A few weeks ago I wrote a two-part article describing how to create a Windows screen saver with GDI+. I am not displaying a slide show in my screen saver but instead I am randomly drawing shapes. I did however explain the fundamentals of creating a screen saver for Windows which should be of some help if you have never created a screen saver before. Create a Screen Saver Using C# – Part 1 Create a Screen Saver Using C# – Part 2 Awesome. Will have to read that some day soon then. Thanks for letting me know =)  Actually XNA works pretty well. For example: this is an (advanced) example of what can be made with XNA. The community is quite helpful and XNA has great potential.  Recently I finished with my first WPF (I wanted to see how it can be done with WPF) screen-saver. You can check-it out on YouTube. Try to see HD-version. Though I never tried XNA I'm really pleased with WPF so far. Easy and flexible. But I guess you probably wouldn't get an XNA-performance (or am I wrong here?). You can google for GDI+ or WPF ScreenSaver-templates to start with. nice work with those screen savers did you open source this project? Emm no. I put it at some on-line-stores for 5$ :). Though there is a direct link (to get a .scr) at the YouTube page for free. Would it be possible to have a look at the source code for those screen savers?  I guess XNA works well. There's a sample screensaver in C# Express by the way.  WPF is not a bad idea. It takes advantage of DirectX and hardware acceleration for its animations and effects. You will get better performance if you write this kind of stuff natively (against directx or opengl) but the cost of writing it will be much higher. It's quite possible you will not need that edge anyway. Have a look at hanselman's baby smash (which is a full screen wpf app with animations) to get a grasp of what you can do with wpf. Note: I did write a slide show kind of thingy in WPF way back the key to getting this to work smoothly is loading up the images in a background thread and freezing it.  If you want to go with just GDI and GDI+ I wrote some info here about how to speed them up when rendering images and drawing them to screen. There is also fully functional screen saver source code at the above link (which I wrote myself after digging for some of the more obscure screen saver details) in case that helps.
802,A,Generate vector EMF/WMF (Windows Metafile) clipboard content from Qt 4.5 We are moving a large codebase from GDI to QPainter. One thing we used to get on Windows easily was the EMF clipboard format which enabled customers to manipulate their pasted output in other programs (like Office) in a vector format. It's easy for us to produce bitmaps from Qt into EMF but that really doesn't give us back the editability of the old format. We can also make mime data from Postscript which in some programs will retain vector format but won't allow editability in Office for example. Since Trolltech/Nokia abandoned the GDI rendering path is there any decent way to produce vector EMF output from QPainter? Options include 3rd-party libraries postprocessing other output formats (which we haven't seen usable results from) phony printer drivers (which introduce Windows-version specific issues). It has to work on WinXP -> 7. Has anybody else solved this? Please note: we know we could write our own graphics backend. The question is can we do anything short of that. Also note that there are solutions that work for Qt3 on the Qt mailing lists but they only work because they still used GDI then. I have a PyQt QPaintEngine for rendering to EMF here (GPL): http://svn.gna.org/viewcvs/veusz/trunk/document/emf_export.py?view=markup It also uses the PyEMF library. It would be hard to convert this to C++ if necessary.  I just came across this possible solution: EmfEngine. I haven't used it so YMMV.  Well now I see that this has been asked and answered here before. The accepted answer there is that it can't be done easily. If anyone has any better news I'd be happy to hear it.
803,A,Writing Color Calibration Data to a TIFF or PNG file My custom homebrew photography processing software running on 64 bit Linux/GNU writes out PNG and TIFF files. These are to be sent to a quality printing shop to be made into fine art. Working with interior designers - it's important to get the colors just right! The print shops usually have no trouble with TIFF and PNGs made from commercial software such as Photoshop. Even though i have the TIFF 6.0 specs PNG specs and other info in hand it is not clear how to include color calibration data or implement color management system on linux. My files are often rejected as faulty without sufficient error reports to make fixes. This has been a nasty problem for a while for many. Even my contacts at the Hollywood postproduction studios are struggling with this issue. One studio even wanted to hire me to take care of their color calibration thinking i was the expert - but no i am just as blind and lost as everyone! Does anyone know of good code examples detailed technical information or have any other enlightenment? Or time to switch to pure Apple? There is a consultant called Charles Poynton who specialises in this area. I work for one of the post production studios you mention (albeit in london not hollywood) and have seen him speak on the subject a couple of times. His website contains a lot of the material he presents and you might find something of use there. He also has a book called Digital Video and HDTV Algorithms and Interfaces which is not as heavy as the title might suggest! While these resources might not answer your question directly it might provide a spring board to other solutions. More specifically which libraries are you using to write the png and tif files - you mention they are homebrew but how custom are they exactly? Postprocessing the images in an image manipulation program (such as ImageMagick or dcraw) might allow you to inject this information into the header more successfully. Sorry I don't have any specific answers but maybe something that will point you a bit further in the right direction...  As a GNU/Linux user you’ll want to consider DispcalGUI – http://dispcalgui.hoech.net/ – a GNOME-based GUI that centralizes color management ICC profile management and (crucially for your case) device calibration. It can talk to well-known pro- and mid-level hardware e.g i1 X-Rite Spyder etc. But before you get into that – you say you are generating your files to spec; are you validating your output using a test suite specific to the format in question? If not here are three to get you started: imagetestsuite supports the well-known formats: https://code.google.com/p/imagetestsuite/w/list?can=1&q= The Luminous* test suite is a JIRA plugin if that’s your thing: https://marketplace.atlassian.com/plugins/com.luminouslead.plugin.jira.testsuite.LuminousTestSuite FLOSS Decoder implementations often have one you can use i.e. OpenJPEG – https://code.google.com/p/openjpeg/wiki/TestSuiteDocumentation But even barring all of those it seems like your problem is with embedded ICC data – which is two specs in one. First there’s the host image-file format and they all handle embedding differently (meaning the ICC data will likely look totally different when embedded in a TIFF than say a JPEG or WebP file). Second there is the ICC spec itself. It is documented here: http://color.org/v4spec.xalter – and you may also want to look at the source for the aforementioned dispcalGUI which includes a very legible and hackable ICC profile class in Python: http://sourceforge.net/p/dispcalgui/code/HEAD/tree/trunk/dispcalGUI/ICCProfile.py Full disclosure: I have contributed to that very ICC profile class to which I just linked in that last ¶ That’s the basics (many of which you have no doubt covered)... beyond that if you post more information about what exactly is going wrong I’d be interested to look it over. Good luck with it either way. * NB. This project is unrelated to the long-standing photography website “the Luminous Landscape”  Take a look at LittleCMS http://www.littlecms.com/ This page has the code for applying it to TIFF http://www.littlecms.com/newutils.htm The basic thing you need to know is that Color profile data is something you need to store in the meta-data of the file itself.
804,A,"Rotation & OpenGL Matrices I have a class that holds a 4x4 matrix for scaling and translations. How would I implement rotation methods to this class? And should I implement the rotation as a separate matrix? Answering the second half of the question a single 4x4 matrix is perfectly capable of holding a scaling a translation and a rotation. So unless you've put special limitations on what sort of 4x4 matrices you can handle a single 4x4 is a fine for what you want. As for rotation about an arbitrary vector (as you are asking in comments) look at the ""Rotation about an arbitrary vector"" section in the Wikipedia article yabcok links to. You will want to extend that to a 4x4 matrix by padding it out with zeros except for the 44 (scaling) position which should be one. Then use matrix multiplication with your scaling/translation 4x4 to generate a new 4x4 matrix.  This page has quite a bit of useful information: http://knol.google.com/k/matrices-for-3d-applications-translation-rotation  You can multiply Your current matrix with a rotation matrix. Take a look at http://en.wikipedia.org/wiki/Rotation_matrix ah so I would only generate a rotation matrix and multiply with current matrix rather than storing 2? How would I generate a rotation matrix though given a vector xyz to rotate in 3 dimensions? @Nowell What do you mean? Depending on your structure create a matrix for rotation and multiply it by your source matrix. Or you can apply the multiplication directly on your matrix. Im basically trying to recreate the functionality of glRotate Wait!!! Unfortunately the wikipedia page cited doesn't talk about the standard 4x4 matrix that handles 3D scaling+transformations. Wait!!!! It does. See ""Nested dimensions"". Touche. :) But it doesn't still doesn't talk about homogeneous coordinates. yeah. my answer is not very comprehensive anyway :)  You want to make sure you find a reference which talks about the right kind of matrix that's used for computer graphics (namely 3D homogeneous coordinates using a 4x4 transformation matrix for rotation/translation/skewing). See a computer graphics ""bible"" such as Foley and Van Dam (pg. 213) or one of these: The Mathematics of the 3D Rotation Matrix Mathematics of 3D Graphics MSDN 3D graphics tutorial SIGGRAPH article about 3D rotation other page from CProgramming.com  There's a site which I use every time when I need to look up the details of a 3D transformation called http://www.euclideanspace.com. The particular page on matrix rotations can be found here. Edit: Rotation around a given axis look at the axis & angle representation. This page also links to a description on how to translate one representation to another. If you need to rotate around mutiple axes simply multiply the corresponding matrices. hmm that tells me how to rotate in x y or z axis or to rotate a vector by an angle but not how to rotate x y and z azis all at once given a vector"
805,A,Why does wgluseFontBitmaps consume too much memory on some computers? I'm creating a game in OpenGL which loads the entire Arial Unicode MS font when it loads. The program uses on avg. 10 megs of memory on my computer (op sys is WinXP SP2) and runs without problems but when I move the program to my laptop (with Vista) the wglUseFontBitmaps hangs and allocates memory fluently and never returns. This problem occured recently and I have no idea why and never had such problem before. Why does wglUseFontBitmaps do this and how to fix it? update: I tried an older version and it runs but eats 400megs of memory (so it is not a new problem) How many glyph display lists are you trying to generate with wglUseFontBitmaps()? Can you show us your invocation? Perhaps Vista is trying to do all 60000-some-odd glyphs in one go and XP is doing some sort of on-demand construction? I've had good luck with FreeType2 and MS Arial Unicode though it does take some time to get up to speed with the API. This tutorial can be C++-ized to great effect. I loaded just the entire font... #define UNICODEFONTSIZE 65535 ... SelectObject(hdchfontArialUnicodeStuff); wglUseFontBitmapsW(hdc 0 UNICODEFONTSIZEListBase); ... Please note this does not cause any problems on XP. I have heard that Vista has some OpenGL issues. This may be one of them. For XP 10MB Mem Usage or Mem Usage + VM Size? overall mem usage Do the machines have the same DPI? Does the machines have different ClearType settings?
806,A,What is maximium resolution of a html page? I have few question in this regard When you create an internet page does the program automatically create 75pdi? Could we create 300DPI page could this be able communicate on internet ? What is maximum DPI resolution you can get on a Web page? DPI is a print term. The monitor display term is PPI (pixels per inch). HTML doesn’t have a resolution. You rather mean images that are referenced in HTML. Web displays graphics at 72dpi. If you make an image that is 300dpi it's going to look much larger on the screen than was intended. This simply isn't the case.  DPI be it 72 or 300 is only relevant when going to an output device like a printer talking about DPI for web graphics is meaningless. On the web all images are shown 1::1. A pixel of data in the image is a pixel of data on the screen. You can use any DPI you want for a web image. It makes no difference in how it will be displayed. BUT - if you are working towards the web you can no longer measure things in inches centimeters of picas. You need to start working with all dimensions in pixels. If you are viewing your graphics in Photoshop make sure your view is set to 100%. Then you'll be seeing the same thing that will be displayed in the browser. Everyones browser is different so a conservative estimate for a static page design is that your page content should be about 900 pixels or so wide. (People are used to scrolling down so your page height can be whatever you want).  Unless the entire web page is just an image file web pages don't specify a resolution like that. HTML defines the layout and contents of the page the video and printer drivers determine the resolution it is displayed or printed in. Meaningless question see #1. See #2.  To answer your questions (I'm presuming you're talking about images on a web page rather than the web page itself which is created in HTML etc.) You should create the image at 72dpi. Most programs with 'Save for web' functionality should convert the image to 72dpi but you may need to do this yourself. You could put a 300dpi image on the web and it should display correctly in pretty much all browsers (and should print at the correspondingly higher resolution) but this is a bad idea as it'll be much slower to load/render will consume bandwidth etc. As such I'd really recommend sticking to 72dpi. If you want a high resolution version of an image link to the raw image file or create a (resolution independent PDF or SVG etc.) As above there's no maximum (although the web site's visitors machines will eventually grind to a halt attempting to decode an 'n' DPI image).
807,A,"Ball to Ball Collision - Gains significant velocity upon collision I implemented the code from the question ""Ball to Ball Collision - Detection and Handling"" in Objective-C. However whenever the balls collide at an angle their velocity increases dramatically. All of the vector math is done using cocos2d-iphone with the header CGPointExtension.h. What is the cause of this undesired acceleration? The following is an example of increase in speed: Input: mass == 12.56637 velocity.x == 1.73199439 velocity.y == -10.5695238 ball.mass == 12.56637 ball.velocity.x == 6.04341078 ball.velocity.y == 14.2686739 Output: mass == 12.56637 velocity.x == 110.004326 velocity.y == -10.5695238 ball.mass == 12.56637 ball.velocity.x == -102.22892 ball.velocity.y == -72.4030228 #import ""CGPointExtension.h"" #define RESTITUTION_CONSTANT (0.75) //elasticity of the system - (void) resolveCollision:(Ball*) ball { // get the mtd (minimum translation distance) CGPoint delta = ccpSub(position ball.position); float d = ccpLength(delta); // minimum translation distance to push balls apart after intersecting CGPoint mtd = ccpMult(delta (((radius + ball.radius)-d)/d)); // resolve intersection -- // inverse mass quantities float im1 = 1 / [self mass]; float im2 = 1 / [ball mass]; // push-pull them apart based off their mass position = ccpAdd(position ccpMult(mtd (im1 / (im1 + im2)))); ball.position = ccpSub(ball.position ccpMult(mtd (im2 / (im1 + im2)))); // impact speed CGPoint v = ccpSub(velocity ball.velocity); float vn = ccpDot(vccpNormalize(mtd)); // sphere intersecting but moving away from each other already if (vn > 0.0f) return; // collision impulse float i = (-(1.0f + RESTITUTION_CONSTANT) * vn) / ([self mass] + [ball mass]); CGPoint impulse = ccpMult(mtd i); // change in momentum velocity = ccpAdd(velocity ccpMult(impulse im1)); ball.velocity = ccpSub(ball.velocity ccpMult(impulse im2)); } @jpinto3912 - don't you mean conservation of momentum? @Cade Roux - it still is not working... using the real mass instead of the inverse simply results in the balls pushing around each other instead of bouncing off at an absurd speed. Here is the google code link to my final version of this code that I wrote. I added a couple special divide-by-zero conditions when balls are directly on top of eachother. http://code.google.com/p/simucal-projects/source/browse/ballbounce/trunk/Ball.java I'm not sure. Why don't you check the formulas in the degenerate case. Two identical balls (mass and radius) one ball stationary COR = 1 ball centers in line with trajectory of the second moving ball. You should see the moving ball stop dead and the second ball start moving with the velocity of the original moving ball a la Newton's cradle. Your physics are the issue here: you're setting up a collision that does not conserve kinetic energy since the Coefficient of Restitution is 1.75 (over 1). Hence this is not a programing related error. Could you post the final version of your code that works? Having reviewed the original code and the comments by the original poster the code seems the same so if the original is a correct implementation I would suspect a bad vector library or some kind of uninitialized variable. Why are you adding 1.0 to the coefficient of restitution? From: http://en.wikipedia.org/wiki/Coefficient_of_restitution The COR is generally a number in the range [01]. Qualitatively 1 represents a perfectly elastic collision while 0 represents a perfectly inelastic collision. A COR greater than one is theoretically possible representing a collision that generates kinetic energy such as land mines being thrown together and exploding. Another problem is this: / (im1 + im2) You're dividing by the sum of the reciprocals of the masses to get the impulse along the vector of contact - you probably should be dividing by the sum of the masses themselves. This is magnifying your impulse (""that's what she said""). nice catch 1.0 -> 1.0f The problem is the same with 1.0f What is the difference between 1.0 and 1.0f anyway? My question is why is your effective coefficient of restitution 1.75 - i.e. you are adding energy into the system at time of collision which would obviously be converted into kinetic energy in the balls. The difference between 1.0f and 1.0 is that you are specifying 1.0f to be 1.0 of data type float (as opposed to whatever the compiler defaults to - presumably its options are either float or double). I don't think that's material to the problem. According to most style guidelines instead of #define you should use a const float. I removed the addition of 1.0 to the COR and the acceleration remains. The difference between 1.0 and 1.75 would not increase the velocity from 1.73 pixels/refresh to 110 pixels/refresh as it does above. Some other aspect is causing this effect. See the update to my answer regardng the impulse scaling  I'm the one who wrote the original ball bounce code you referenced. If you download and try out that code you can see it works fine. The following code is correct (the way you originally had it): // collision impulse float i = (-(1.0f + RESTITUTION_CONSTANT) * vn) / (im1 + im2); CGPoint impulse = ccpMult(mtd i); This is very common physics code and you can see it nearly exactly implemented like this in the following examples: Find collision response of two objects - GameDev 3D Pong Collision Response Another ball to ball collision written in Java This is correct and it ~isn't~ creating a CoR over 1.0 like others have suggested. This is calculating the relative impulse vector based off mass and Coefficient of Restitution. Ignoring friction a simple 1d example is as follows: J = -Vr(1+e) / {1/m1 + 1/m2} Where e is your CoR Vr is your normalized velocity and J is a scalar value of the impulse velocity. If you plan on doing anything more advanced than this I suggest you use one of the many physics libraries already out there. When I used the code above it was fine for a few balls but when I ramped it up to several hundred it started to choke. I've used the Box2D physics engine and its solver could handle more balls and it is much more accurate. Anyway I looked over your code and at first glance it looks fine (it is a pretty faithful translation). It is probably a small and subtle error of a wrong value being passed in or a vector math problem. I don't know anything concerning iPhone development but I would suggest setting a breakpoint at the top of that method and monitoring each steps resulting value and finding where the blow-up is. Ensure that the MTD is calculated correctly the impact velocities etc etc until you see where the large increase is getting introduced. Report back with the values of each step in that method and we'll see what we have.  In this line: CGPoint impulse = ccpMult(mtd i); mtd needs to have been normalised. The error happened because in the original code mtd was normalized in a previous line but not in your code. You can fix it by doing something like this: CGPoint impulse = ccpMult(ccpNormalize(mtd) i);"
808,A,"Get Font Glyphs as Vectors manipulate and product SVG or Bitmap I have an application that needs to apply some transformations to text (including non-affine transformations). Does anyone know of a toolkit (or group of tools) that would let me bring in a True Type or Postscript Font get the glyphs as outlines then apply transformations to the outlines and render it as a bitmap or svg file ? Flash won't do non-affine transformations so it is out. Illustrator has a function which converts text to outlines but Illustrator scripting is very unstable so I can't really use it for this. Thanks for any help. While Batik is a good answer it will convert only TrueType fonts to SVG Fonts. If you have OpenType fonts and they more and more dominate now you can use FontForge to get an SVG Font for these. Another advantage of using OpenType is that the produced SVG uses not only quadratic but also cubic Beziers in the path which is more effective.  You could use the Batik TTF to SVG Font converter. The SVG font format uses the same path data format that the SVG <path> element uses. For example this is the output from converting Gentium Basic Regular using the above tool. With the right coordinate system you can just grab out the path data transform it however you like and then draw it with a <path>. Note though that the glyph coordinate system in SVG fonts is actually inverted with the (00) at the bottom left corner of the glyph cell box compared to the regular SVG canvas which has (00) at the top left corner. So don't forget to flip the glyph e.g. by putting transform=""scale(1-1)"" on the <path> you use to render the glyph. Once you have the SVG document that renders the glyphs as shapes you can convert it to a bitmap using your favourite tool. (Batik can do that too.) Thanks I'll look into this"
809,A,"Smooth movement of icon displayed on a Panel I'm coding an app where I display a System.Drawing.Icon object on a System.Windows.Forms.Panel using a code that goes something like so: Graphics g = _panel.CreateGraphics(); g.DrawIcon(this.NodeIcon _rectangle); I have code to move the icon around using drag-n-drop. My problem is that when the user move the icon around it is anything but smooth. The icon looks distorted until the user stops moving the icon. I have tried to find information on this around the net but I can't get it to be smooth. I have little previous experience of this particular kind of coding (using graphics) so I am a rookie to this. If any kind soul could help me with some hints it would be much appreciated. Thanks in advance! I have no idea as well it seems while dragging the redrawing algo is taking place and not very efficient. I take it you've had similar problems? @Freddy: Not really that's my hunch :P When I read ""drag and drop"" that has very specific connotations but as I read your question I think you are referring to moving a graphic element around a panel not ""inserting"" the icon into a control within the panel or moving it outside some control to the panel surface itself. Perhaps you can clarify exactly what you are doing how you implemented ""hit detection"" on the icon. Also : is just dragging a picturebox with an icon rendered in it an option using the standard over-rides for MouseDown MouseUp and MouseMove ? Hope these comments are helpful best In general you shouldn't be using the Graphics object returned from CreateGraphics for anything but text measurement (Graphics.MeasureString). Rendering should happen in OnPaint. Hit detection is performed in the MouseDown event just calculating if the MousePosition.X/Y is inside the icons boundaries. I have not tried using the PictureBox for this. I will try it. I need to read up on OnPaint and experiment more. Thank you all for your comments. When I've cracked I will let you know. I believe what you're trying to do is to redraw your control on the MouseMove event handler. And looks like your problem is the flicker when redrawing the panel. First what you can try to do is set true to DoubleBuffered property of your panel. Doing this you would set the panel to redraw its surface using a secondary buffer to reduce or prevent flicker. This property is protected so you would need to create a new panel descendant: public class TestPanel : Panel { public TestPanel() { DoubleBuffered = true; } } as an alternative you can set the DoubleBuffered property for your panel through reflection hope this helps regards Thank you for your comment. I will try this as well as the other tips from the other comments. When I've cracked the problem I will let you know. Thanks! I finally had the time to test these tips and the DoubleBuffered did what I needed. Thanks again!"
810,A,Spline B-Spline and NURBS C++ library Does anyone know of a library or set of classes for splines - specifically b-splines and NURBS (optional). A fast efficient b-spline library would be so useful for me at the moment. This library may also be promising: http://libnurbs.sourceforge.net  Also ITK has a class for bspline itkBSplineScatteredDataPointSetToImageFilter see example at http://www.itk.org/Wiki/ITK/Examples/WishList/PointSet/BSplineScatteredDataPointSetToImageFilter ( documentation at http://www.itk.org/Doxygen/html/classitk_1_1BSplineScatteredDataPointSetToImageFilter.html http://www.itk.org/Doxygen/html/classitk_1_1BSplineScatteredDataPointSetToImageFilter.html )  1.) For B Splines - You should check Numerical Recipes in C (there is book for that and it is also available online for reference) 2.) Also check: sourceforge.net/projects/einspline/ & this -AD The numerical recipes in C code used to fall under a fairly draconian license. Not sure if this is still the case but but be advised that it may be illegal to use it in open-source projects (I'm not an expert on this though) Numerical Recipes doesn't have anything on B-Splines.  I know I'm answering months after this question was asked but for others who might be searching for a similar answer I'll point out openNURBS. OpenNURBS also happens to be the library used in the modeling package Rhinoceros. It's a very complete library and it's worth consideration. Understood. Just hoped to make the answers more complete. :) Thanks man! If I hadn't marked an answer already OpenNurbs would probably deserve it. I think openNURBS takes care only of saving to/opening from the .3dm format; it does not actually perform geometrical operations. For that as is mentioned in the link Rhino SDK is needed which is proprietary tech.  Eigen Spline is based on the famous (fast) C++ template library for linear algebra.
811,A,Haskell pixel drawing library linux I wish to draw individual pixels on a screen in a window or something for real-time display in haskell. I'm just getting started with haskell (but not functional programming and not graphics) so I'm trying to create some rudimentary graphics things with it. I have tried to use SDL but the following code gives me a blank screen: import Prelude import Graphics.UI.SDL as SDL createColor screen r g b = SDL.mapRGB (SDL.surfaceGetPixelFormat screen) r g b drawGrad screen = SDL.setColors screen [SDL.Color x y 255 | x <- [0..255] y <- [0..255]] 800 main = do SDL.init [InitEverything] setVideoMode 256 256 32 [] screen <- getVideoSurface drawGrad screen SDL.flip screen quitHandler quitHandler :: IO () quitHandler = do e <- waitEvent case e of Quit -> return () otherwise -> quitHandler I don't see how to do this. I think SDL wants you to write pixels directly to the surface; but the return type of `SDL.surfaceGetPixels` isn't exposed. If it were you could use `withForeignPtr` I think. @Jason Orendorff - This is not true actually the return type is a Ptr to an empty data declaration (Which is legal) this forces you to use castPtr which makes sense because the actual type of the pointer will depend on the surface's format (bit depth pixel format etc). Look at my answer below to see how to do it. It looks to me like you aren't actually drawing anything there. Are you sure setColors does what you think it does? If memory serves me it's for setting the color palette for an 8-bit surface whereas you're explicitly setting a 32-bit video mode. As an aside if you've done graphics programming before I'm sure you know the difference but for the benefit of others reading this: 8-bit video modes store pixel colors as an 8-bit index into a table of 256 colors chosen from a much larger set of potential colors; in higher color depths the palette table is dispensed with and each pixel color is stored directly as a packed RGB triple typically of 8 bits each (32-bit pixels either have an 8-bit alpha channel as well or just 8 bits of padding for better memory alignment I can't recall). Either way if I'm guessing correctly what you were attempting to do try adding this to your program: import Ix -- Given coordinates create a 1x1 rectangle containing that pixel getPix x y = SDL.Rect (fromIntegral x) (fromIntegral y) 1 1 -- Draw a pixel on a surface with color determined by position drawPixel surf (x y) = do let pixRect = Just (getPix x y) pixColor <- createColor surf x y 255 SDL.fillRect surf pixRect pixColor -- Apply drawPixel to each coordinate on the screen drawGrad screen = mapM_ (drawPixel screen) $ range ((00)(255255)) Warning: That's almost certainly horrible and inefficient code and it has been tested only in a Win32 environment. I am not a qualified Haskell programmer and my advice should not be taken as indicative of correct use of the language. For external use only. Do not taunt the Happy Fun IO Monad. Either way I think your problem lies with use of SDL not Haskell. Graphics.UI.SDL is pretty raw little more than wrapping the C API in appropriate Haskell types. If you've not used SDL before or used it only with slightly more elaborate bindings (e.g. PyGame) you may want to consider looking for SDL examples in C as reference material. I guess this is good enough while I'm learning even if its slow I can figure out how to do it faster later. Check my answer to see how to do it the correct efficent way.  I know this question is about a year old but there are misleading answers in here you can do pixel manipulation with Haskell SDL bindings check my code for example of how to do it's a port of LazyFoo's SDL tutorial 31
812,A,.NET Graphics on Windows 7: White corners around images? I've been working on some custom graphics controls and I found this weird problem with windows 7 rendering my button controls. I've used Photoshop to delete the pixels in the background all around the button image I'm using then saved it as a GIF and imported it into VS to use as the background image of my button. When windows XP renders it it is fine but when windows 7 renders it all 4 corners have an odd white border around them. You can barely see them in this pic but they are much more apparent when looking at them on the client PC's. Is there something wrong with the way I am transfering the image? should I not use a gif? is there something wrong with the way I am displaying it on the button? What can I do about it? I'd use the PNG format for starters and see if that solves anything. GIF was a bad choice it can only render images with 256 colors. You need all the colors you can get to make the anti-aliasing work properly. Use PNG. You will also need to make sure that the background color of the container is the same as the one you used in Photoshop the anti-aliasing pixels will otherwise have the wrong colors. And you cannot stretch the image that will also stretch the anti-aliasing pixels ruining the effect. ty exactly what I needed. I agree with nobugz
813,A,"How to write directly to linux framebuffer? How to write directly to linux framebuffer? Are you looking to write a device driver? If so check out this HowTo guide http://www.linux-fbdev.org/HOWTO/index.html Also read the file Documentation/fb/framebuffer.txt (and the adjoining docs for specific drivers) in the linux kernel tree.  Basically you open /dev/fb0 do some ioctls on it then mmap it. Then you just write to the mmap'd area in your process.  look at FBIOPUT_VSCREENINFO ioctl and mmap (I have the code but not at this pc sorry) edit: this should get you started  //open file descriptor and get info inf fdScreen = open( ""devicename"" O_RDWR ); fb_var_screeninfo varInfo; ioctl( fdScreen FBIOGET_VSCREENINFO &varInfo ); //set resolution/dpi/color depth/.. in varInfo then write it back ioctl( fdScreen FBIOPUT_VSCREENINFO &varInfo ); //get writable screen memory; unsigned short here for 16bit color unsigned short* display = mmap( 0 nScreenSize PROT_READ | PROT_WRITE MAP_SHARED fdScreen 0 );"
814,A,"Android: How to overlay-a-bitmap/draw-over a bitmap? I have two questions actually: Is it better to draw an image on a bitmap or create a bitmap as resource and then draw it over a bitmap? Performance wise... which one is better? If I want to draw something transparent over a bitmap how would I go about doing it? If I want to overlay one transparent bitmap over another how would I do it? Sorry for the long list but in the interest of learning I would like to explore both the approaches... I cant believe no-one has answered this yet! A rare occurrence on SO! 1 Question doesn't quite make sense to me. But ill give it a stab. If you're asking about direct drawing to a canvas (polygons shading text etc...) VS loading a bitmap and blitting it onto the canvas that would depend on the complexity of your drawing. As the drawing gets more complex the CPU time required will increase accordingly. However blitting a bitmap onto a canvas will always be a constant time which is proportional to the size of the bitmap. 2 Without knowing what ""something"" is how can i show you how to do it? You should be able to figure out #2 from the answer for #3. 3 Assumptions: bmp1 is the bigger of the two you want them both overlaid from the top left corner.  private Bitmap overlay(Bitmap bmp1 Bitmap bmp2) { Bitmap bmOverlay = Bitmap.createBitmap(bmp1.getWidth() bmp1.getHeight() bmp1.getConfig()); Canvas canvas = new Canvas(bmOverlay); canvas.drawBitmap(bmp1 new Matrix() null); canvas.drawBitmap(bmp2 new Matrix() null); return bmOverlay; } Not working for me.  If the purpose is to obtain a bitmap this is very simple: Canvas canvas = new Canvas(); canvas.setBitmap(image); canvas.drawBitmap(image2 new Matrix() null); In the end image will contain the overlap of image and image2.  You can do something like this: public void putOverlay(Bitmap bitmap Bitmap overlay) { Canvas canvas = new Canvas(bitmap); Paint paint = new Paint(Paint.FILTER_BITMAP_FLAG); canvas.drawBitmap(overlay 0 0 paint); } The idea is very simple: Once you associate a bitmap with a canvas you can call any of the canvas' methods to draw over the bitmap. This will work for bitmaps that have transparency. A bitmap will have transparency if it has an alpha channel. Look at Bitmap.Config. You'd probably want to use ARGB_8888. Important: Look at this Android sample for the different ways you can perform drawing. It will help you a lot. Performance wise (memory-wise to be exact) Bitmaps are the best objects to use since they simply wrap a native bitmap. An ImageView is a subclass of View and a BitmapDrawable holds a Bitmap inside but it holds many other things as well. But this is an over-simplification. You can suggest a performance-specific scenario for a precise answer."
815,A,Why is drawRoundRectComplex() not documented in ActionScript? In studying actionscript 3's graphics class I've come across the undocumented drawRoundRectComplex() method. It's a variant of drawRoundRect() but with 8 parameters the final four being the diameter of each corner (x y width height top left top right bottom left bottom right). //example var sp:Sprite = new Sprite(); sp.graphics.lineStyle(1 0x000000); sp.graphics.drawRoundRectComplex(0 0 200 150 110 50 0 10); addChild(sp); this seems to be a pretty useful method so i'm just curious if anyone knows of any reasons why adobe chose not to document it? I believe it's documented in the mx.utils.GraphicUtils class. Link. is mx.utils.GraphicUtils a legacy class? if so would that make drawRoundRectComplex old also? is there something newer? Formally I think everything in the mx.* package is Flex or at least used to be. Offhand I suspect that the undocumented part is that Graphics implements what used to be a flex-only routine? there it is. strange that it doesn't show up in blue in frame or actionscripts for flash cs4. i guess it's under-documented rather than undocumented.
816,A,C# GDI Drawing2D help What GDI methods can I use to draw the blue shape shown in the image below? The center must be transparent. There are a number of ways but you'll probably want to use the following: FillRectangle FillPolygon DrawLine since it looks like your shape can be reduced to a rectangle and two polygons and then outlined by a few lines. Here is a really simple and hard-coded example of what i was thinking: Private Sub Control_Paint(ByVal sender As Object ByVal e As PaintEventArgs) _ Handles MyBase.Paint Dim g As Graphics = e.Graphics g.FillRectangle(Brushes.Aqua New Rectangle(10 10 10 90)) g.FillPolygon(Brushes.Aqua New Point() { _ New Point(10 10) _ New Point(20 10) _ New Point(40 50) _ New Point(30 50)}) g.FillPolygon(Brushes.Aqua New Point() { _ New Point(10 100) _ New Point(20 100) _ New Point(40 50) _ New Point(30 50)}) g.DrawLine(Pens.Black New Point(10 10) New Point(10 100)) g.DrawLine(Pens.Black New Point(10 100) New Point(20 100)) g.DrawLine(Pens.Black New Point(20 100) New Point(40 50)) g.DrawLine(Pens.Black New Point(40 50) New Point(20 10)) g.DrawLine(Pens.Black New Point(20 10) New Point(10 10)) ... I think a rectangle and a polygon will work i'll give it a shot. @Kevin: added some sample drawing code to expand on what i was imagining. Do note that it is hard-coded and does not bother outlining the inner triangle. Not enough time today. ;-) the inside borders are a bit more tricky.  Wouldn't it just be easier to draw it using a bitmap? That's what they're for anyway :).  Im assuming GDI+ here aka System.Drawing namespace. The best thing to do is to look at System.Drawing.Drawing2d.GraphicsPath class : http://msdn.microsoft.com/en-us/library/system.drawing.drawing2d.graphicspath.aspx You need to make sure you close the path to get the hollow effect.
817,A,"How can I render reflections in OpenGL ES on the iPhone without a stencil buffer? I'm looking for an alternative technique for rendering reflections in OpenGL ES on the iPhone. Usually I would do this by using the stencil buffer to mark where the reflection can be seen (the reflective surface) and then render the reversed image only in those pixels. Thus when the reflected object moves off the surface its reflection is no longer seen. However since the iPhone's implementation doesn't support the stencil buffer I can't determine how to hide the portions of the reflection that fall outside of the surface. To clarify the issue isn't rendering the reflections themselves but hiding them when they wouldn't be visible. Any ideas? Render the reflected scene first; copy out to a texture using glCopyTexImage2D; clear the framebuffer; draw the scene proper applying the copied texture to the reflective surface.  I don't have an answer for reflections but here's how I'm doing shadows without the stencil buffer perhaps it will give you an idea: I perform basic front-face/back-face determination of the mesh from the point of view of the light source. I then get a list of all edges that connect a front triangle to a back triangle. I treat this edge list as a line ""loop"". I project the vertices of this loop along the object-light ray until it intersects the ground. These intersection points are then used to calculate a 2D polygon on the same plane as the ground. I then use a tesselation algorithm to turn that poly into triangles. (This works fine as long as your lights sources or objects don't move too often.) Once I have the triangles I render them with a slight offset such that the depth buffer will allow the shadow to pass. Alternatively you can use a decaling algorithm such as the one in the Red Book."
818,A,"Literature and tutorials for writing a ray tracer I am interested in finding recommendations on books on writing a raytracer simple and clear implementations of ray tracing that can be seen on the web and online resources on introductory ray tracing. Ideally the approach would be incremental and tutorial in style and explain both the programming techniques and underyling mathematics starting from the basics. As said above the best book you can possibly get is Physically based Rendering by Matt Pharr (check out www.pbrt.org). Explains a lot of algorithms in great detail including advanced stuff like photon mapping. Moreover it includes a fully working ray-tracer so you can take a look at it. It also covers the math basics so if you don't want to buy many books I'd definitely recommend to take a look at this one. It's much better than the classic books on this subject as they tend to explain only the theory not so much how to really implement it. For basics any math book will do it or you could try ""Realtime collision detection"" which also explains lots of intersection routines (which you will need in ray-tracing). If you really want to start at the basics you should try ""Computer Graphics: Principles and Practice"" it's dated (some parts are really nonsense now) but it explains the basics pretty well. If you want a more recent book try ""Fundamentals of Computer Graphics"" which contains the same just not that detailed (should be good enough to get you started thou). Last but not least the wikipedia page on Raytracing is actually pretty good and should give you some starting points. Take a look at the external links section.  A few years ago someone challenged me to do a Delphi port of a tiny ray-tracer (less than 200 lines of C code). i ported it to Delphi perhaps one day i'll re-port to C#.  If you are looking for a single good book that brings you from nothing to working code that can produce images I started with Andrew Glassner's An Introduction to Ray Tracing. I can't get to Amazon right now but here is the relevant link. Coincidentally this is actually the book and problem domain that introduced me to object-oriented design. Boy that was a while ago.... From there I would recommend moving on to Pete Shirley's book as the Wikipedia bibliography seems to imply. Actually an even better suggestion is to take his ray tracing class. It worked for me!  This user has some interesting tutorials on ray tracing these tutorials use C# I believe: http://www.codeproject.com/script/Articles/MemberArticles.aspx?amid=3589667 I'm sure there will be some better tutorials but this might be of interest.  This is a tool that may be useful for understanding and visualizing the general ideas of Raytracing: Raytracing Simulator It is a simulator that I built for the Graphics course that I teach. Instead of rendering a 3D scene into a 2D image it renders a 2D scene into a 1D image which allows the visualization of the whole algorithm at once letting you modify the parameters of the scene in real-time.  Have you seen povray? IMHO it is a very good starting point to understand ray-tracing (http://www.povray.org/)  ""Ray Tracing from the Ground Up"" by Kevin Suffern for getting started. ""Physically Based Rendering"" by Matt Pharr and Greg Humphreys for a more advanced take on it when you've gotten comfortable with the basics. And if you get really into it the Ray Tracing News archives are a good read. Also a lot of ray tracing enthusiast hang out at the ompf.org forum.  The best one I have found is: http://www.devmaster.net/articles/raytracing_series/part1.php This tutorial does tend to move a bit fast but it covers lots of aspects of raytracing.  The advanced rendering class I took in college had the best designed projects I've ever seen. With the project helps and the lecture notes all you need is on the website. The basic idea is that it's really easy to make very subtle bugs in a raytracer especially when you get into things like refraction. If you're just randomly creating a bunch of spheres in space it's pretty hard to verify correctness or diagnose errors. So there's a parser that's easy to incorporate into your own code and a bunch of diagnostic scene files. The first file is the easiest to get working (eye at origin looking down one axis a single sphere) and if you can render them all correctly you are pretty much guaranteed to have a properly working raytracer. Additionally the parser uses the RIB format which is Pixar's standard scene file format and can be exported from pretty much any modeler. It also explores distributed raytracing and path tracing which can give you some really nice images.  Ok I haven't found any perfect answers myself that step through from the very basics; I have found a lot of simple ray tracers and pointers to voluminous textbooks and academic references however. I haven't tried these ray tracers but they look interesting and simple. http://www.barakcohen.co.il/2008/10/ray-tracing-simple-java-open-source.html http://blogs.msdn.com/lukeh/archive/2007/04/03/a-ray-tracer-in-c-3-0.aspx http://www.ffconsultancy.com/languages/ray_tracer/ From the ACM Cross Roads student magazine http://www.acm.org/crossroads/xrds3-4/raytracing.html This one in C++ is at least short; simplicity was lost for speed http://ompf.org/ray/sphereflake/ A PhD Thesis on some aspects of ray tracing is here A series of articles on DevMaster is here The OMPF forum must read series of posts can be found here It sounds like the book ""An Introduction to Ray Tracing"" is what I need ;-) and the authors home page is here for more graphics related programming. Ray tracing from the ground up includes downloadable ray tracer source code too. MiniLight is a minimal global illumination renderer. SmallPT is a global illumination renderer in 99 lines of C++ with a variant using single precision float on CPU and GPU in OpenCL. Thanks to the other posters for the pointers The Glassner book (intro to ray tracing) is the one I used."
819,A,"Need C# function to convert grayscale TIFF to black & white (monochrome/1BPP) TIFF I need a C# function that will take a Byte[] of an 8 bit grayscale TIFF and return a Byte[] of a 1 bit (black & white) TIFF. I'm fairly new to working with TIFFs but the general idea is that we need to convert them from grayscale or color to black and white/monochrome/binary image format. We receive the images via a WCF as a Byte[] then we need to make this conversion to black & white in order to send them to a component which does further processing. We do not plan at this point to ever save them as files. For reference in our test client this is how we create the Byte[]:  FileStream fs = new FileStream(""test1.tif"" FileMode.Open FileAccess.Read); this.image = new byte[fs.Length]; fs.Read(this.image 0 System.Convert.ToInt32(fs.Length)); fs.Close(); --------update--------- I think there may be more than 1 good answer here but we ended up using the code from the CodeProject site with the following method added to overload the convert function to accept Byte[] as well as bitmap: public static Byte[] ConvertToBitonal(Byte[] original) { Bitmap bm = new Bitmap(new System.IO.MemoryStream(original false)); bm = ConvertToBitonal(bm); System.IO.MemoryStream s = new System.IO.MemoryStream(); bm.Save(s System.Drawing.Imaging.ImageFormat.Tiff); return s.ToArray(); } This is not the way you want to do this. To make this work you need to parse the entire TIFF. TIFF is a non-trivial file format. Use a toolkit that can read and write TIFF files. I'm looking for a c# function that would parse the entire TIFF as you mention. So far I've seen a few in VB and am currently reviewing one in c#. We'd like the source rather than a sealed component. might want to check out 'Craigs Utility Library' I believe he has that functionality in place. Craig's Utility Library This is a cool library however it seems to focus on bitmaps rather than tifs.  First you would need to know how an XY pixel location maps to an index value in you array. This will depend upon how your Byte[] was constructed. You need to know the details of your image format - for example what is the stride? I don't see 8 bit grayscale TIFF in the PixelFormat enumeration. If it was there it would tell you what you need to know. Then iterate through each pixel and look at its color value. You need to decide on a threshold value - if the color of the pixel is above the threshold make the new color white; otherwise make it black. If you want to simulate grayscale shading with 1BPP you could look at more advanced techniques such as dithering.  My company's product dotImage will do this. Given an image you can convert from multi-bit to single bit using several methods including simple threshold global threshold local threshold adaptive threshold dithering (ordered and Floyd Steinberg) and dynamic threshold. The right choice depends on the type of the input image (document image graph). The typical code looks like this: AtalaImage image = new AtalaImage(""path-to-tiff"" null); ImageCommand threshold = SomeFactoryToConstructAThresholdCommand(); AtalaImage finalImage = threshold.Apply(image).Image; SomeFactoryToConstructAThresholdCommand() is a method that will return a new command that will process the image. It could be as simple as return new DynamicThresholdCommand(); or return new GlobalThresholdCommand(); And generally speaking if you're looking to convert an entire multi-page tiff to black and white you would do something like this: // open a sequence of images FileSystemImageSource source = new FileSystemImageSource(""path-to-tiff"" true); using (FileStream outstm = new FileStream(""outputpath"" FileMode.Create)) { // make an encoder and a threshold command TiffEncoder encoder = new TiffEncoder(TiffCompression.Auto true); // dynamic is good for documents -- needs the DocumentImaging SDK ImageCommand threshold = new DynamicThreshold(); while (source.HasMoreImages()) { // get next image AtalaImage image = source.AcquireNext(); AtalaImage final = threshold.Apply(image).Image; try { encoder.Save(outstm final null); } finally { // free memory from current image final.Dispose(); // release the source image back to the image source source.Release(image); } } } I can't upvote this but I'm giving you a virtual +1 for being a code pimp. he be big pimpin  @neodymium has a good answer but GetPixel/SetPixel will kill performance. Bob Powell has a great method here: http://www.bobpowell.net/onebit.htm C#:  private Bitmap convertTo1bpp(Bitmap img) { BitmapData bmdo = img.LockBits(new Rectangle(0 0 img.Width img.Height) ImageLockMode.ReadOnly img.PixelFormat); // and the new 1bpp bitmap Bitmap bm = new Bitmap(img.Width img.Height PixelFormat.Format1bppIndexed); BitmapData bmdn = bm.LockBits(new Rectangle(0 0 bm.Width bm.Height) ImageLockMode.ReadWrite PixelFormat.Format1bppIndexed); // scan through the pixels Y by X for(int y = 0; y < img.Height; y++) { for(int x = 0; x < img.Width; x++) { // generate the address of the colour pixel int index = y * bmdo.Stride + x * 4; // check its brightness if(Color.FromArgb(Marshal.ReadByte(bmdo.Scan0 index + 2) Marshal.ReadByte(bmdo.Scan0 index + 1) Marshal.ReadByte(bmdo.Scan0 index)).GetBrightness() > 0.5F) { setIndexedPixel(x y bmdn true); // set it if its bright. } } } // tidy up bm.UnlockBits(bmdn); img.UnlockBits(bmdo); return bm; } private void setIndexedPixel(int x int y BitmapData bmd bool pixel) { int index = y * bmd.Stride + (x >> 3); byte p = Marshal.ReadByte(bmd.Scan0 index); byte mask = (byte)(0x80 >> (x & 0x7)); if (pixel) { p |= mask; } else { p &= (byte)(mask ^ 0xFF); } Marshal.WriteByte(bmd.Scan0 index p); } @NagaMensch if you already have a Byte[] you don't even need to read the source bitmap via a BitmapData instance. Just use the Byte array - just make sure you know what the stride is. This is helpful but the code is in VB rather than c#. +1 beat me by 1 minute :) This method is using simple threshold to do the conversion. That's funny I didn't even notice it was in VB! Here it is in C#... OK that looks interesting. How would I get my Byte[] into the Bitmap object to send to this function? @NagaMensch - I'm not sure you will be able to. According to http://msdn.microsoft.com/en-us/library/system.drawing.imaging.pixelformat.aspx 8 bit grayscale is not a supported PixelFormat.  There is an article on CodeProject here that describes what you need. This solution appears to have worked! I added a function to enable it to accept Byte[] as well as bitmap.  Something like this might work I haven't tested it. (Should be easy to C# it.)  Dim bmpGrayscale As Bitmap = Bitmap.FromFile(""Grayscale.tif"") Dim bmpMonochrome As New Bitmap(bmpGrayscale.Width bmpgrayscale.Height Imaging.PixelFormat.Format1bppIndexed) Using gfxMonochrome As Graphics = Graphics.FromImage(bmpMonochrome) gfxMonochrome.Clear(Color.White) End Using For y As Integer = 0 To bmpGrayscale.Height - 1 For x As Integer = 0 To bmpGrayscale.Width - 1 If bmpGrayscale.GetPixel(x y) <> Color.White Then bmpMonochrome.SetPixel(x y Color.Black) End If Next Next bmpMonochrome.Save(""Monochrome.tif"") This might be a better way still: Using bmpGrayscale As Bitmap = Bitmap.FromFile(""Grayscale.tif"") Using bmpMonochrome As New Bitmap(bmpGrayscale.Width bmpgrayscale.Height Imaging.PixelFormat.Format1bppIndexed) Using gfxMonochrome As Graphics = Graphics.FromImage(bmpMonochrome) gfxMonochrome.CompositingQuality = Drawing2D.CompositingQuality.HighQuality gfxMonochrome.SmoothingMode = Drawing2D.SmoothingMode.HighQuality gfxMonochrome.DrawImage(bmpGrayscale new Rectangle(0 0 bmpMonochrome.Width bmpMonochrome.Height) End Using bmpMonochrome.Save(""Monochrome.tif"") End Using End Using I believe the term you are looking for is ""resampling"". This will turn any non-white pixel black so all shades of gray will become black - probably not what he wants. @plinth - Yup. But you have to admit that it would be pretty darn easy to apply a threshold. :)"
820,A,"How would you allow a user to interact with graphical objects via the mouse? The application that I'm working on is going to be used to create charts of data contained in a database. Right now objects on the chart are manipulated using a ""control panel"" - essentially a list of objects and a PropertyGrid to edit values. The users would also like to be able to interact with the objects using mouse interactions - things like grabbing the corner of the chart and dragging to expand/contract it clicking on a number and getting a text box to edit it or right clicking on something to get a menu of possible interactions. The chart is being drawn with GDI+ on a metafile (a requirement) which is then drawn on a user-drawn form. I'm not really sure how to implement this. I've had a couple ideas: Create some custom controls that get overlaid on the chart graphic. Each control could be associated with a specific object or property of an object on a chart and would update those values depending on how the user interacted with it. Just keep track of where objects are located and when a user does something with the mouse run through the list and figure out which object is supposed to be at the mouse location and go from there. I'm interested in how you guys would implement this and would really appreciate some suggestions. Thanks! +1 I am also interested in this. I would like to see links to further information or good sample applications. The latter - for instance if it's a pie chart you're going to have to do most the hit testing work anyway to deal with irregular shaped controls. +1 for doing hit testing yourself this is definitely the way to go. But cemkalyoncu is right that if you kept a bounding box in addition to the actual coordinates of the object then you could quickly reject objects and narrow your search scope.  The former - if you have it overlaid on a form then all the hit testing will be done for you by the forms framework. You simply need to create some controls and then implement event handlers for them. The above also describes any windows forms (or probably MPF for that matter) program :) Creating your own list of objects etc. is tantamount to re-implementing the windows form framework or at least a substantial part of it. You don't want to reinvent the wheel especially since you have wheels already in your application.  A good method for hit detection: have another off-screen image. draw every clickable object on this image with a unique color. You have to disable anti-aliasing. When user clicks get the color at that point from the off-screen image and determine the object. If you have a list of objects you can use object index as color. This method will handle hit detection of irregular shaped objects but will be a bit slower. PS. Using controls will be slower than this. Interesting. This will be more memory intensive than recalculating the shape if you have only a few large shapes but if you have many irregularly-shaped small objects it may just prove to be a win. Thinking of memory you can surely use palleted image to reduce memory usage to a factor of 4 unless you have more than 256 objects in the workspace. This is pretty well how object selection is done in OpenGL Yep adapted from there :)  Easiest: have a list of objects and their bounding boxes. When mouse event is made check the list for which object is clicked."
821,A,"Can't draw on Internet Explorer I have the following code which draws a square on the main window of another process: private void DrawOnWindow(string processname) { IntPtr winhandle = GetWindowHandle(processname); Graphics grph = Graphics.FromHwnd(winhandle); grph.DrawRectangle(Pens.Red 10 10 100 100); } private IntPtr GetWindowHandle(string windowName) { IntPtr windowHandle = IntPtr.Zero; Process[] processes = Process.GetProcessesByName(windowName); if (processes.Length > 0) { for (int i = 0; i < processes.Length; i++) { if (processes[i].MainWindowHandle.ToInt32() > 0) { windowHandle = processes[i].MainWindowHandle; break; } } } return windowHandle; } This works when I pass ""firefox"" to the DrawOnWindow method but it does not work when I pass ""iexplore"" and I don't understand why. What do I need to do to get this to work on internet explorer? (IE8) IE8 has one top-level process which contains the main window and draws the frame address bar and other controls in it. t also has several child processes which own one or more tabs and their corresponding windows. Your code will return the window handle for the top-level window of the last Iexplore process. However if that process happens to be a child window there are two problems: it does not have a top-level window; nothing guarantees you that the current visible tab belongs to this process.\ You need to modify your code and special-case it for IE to accommodate for the specifics of the IE process/window hierarchy. Btw if you decide to draw on Chrome you will have similar problems as they also do similar tricks with multiple processes and window hierarchy that spans across these processes.  With IE what you're drawing on is the parent window and IE8 has the WS_CLIPCHILDREN window style set. What you need to do is to descent the window hierarchy and figure out exactly which child you want to render on and then go to town on that window instead. WS_CLIPCHILDREN is not the problem with IE8. How do I traverse the IE8 window hierarchy? To traverse the hierarchy you need to use the EnumChildWindows() GDI function (or FindWindowEx() etc). You may want to use Spy++ first to look at the IE8 window and compare the handle to the value you're getting to determine if the problem is what I'm describing or what @Franci Penov is describing."
822,A,Looking for a Java Graphics alternative Any time I embark on a project that requires some rendering of primitive shapes and lines I usually turn to Java because it's just so easy. For my latest project I decided I might like to learn another API similar to but not Java Graphics2D. I would preferably like something that will work with C++ on Linux. Does anybody have any good recommendations for me? Thanks! Cairo graphics is a 2D library that's cross-platform. It is written in C though a C++ wrapper exists (cairomm). It's under a LGPL license.  Anti-Grain geometry gives high quality 2D rendering from path and font primitives is a good example of idiomatic use of templates in C++ and looks fantastic. It has more documentation on the algorithms than on the API so be prepared to look at the examples for how to use it. It requires some OS specific code to take the in-memory bitmap and blit it onto the screen. The other disadvantage is that when you next look at Java 2D or GDI+ applications you'll think Ewww as they're so badly rendered. That does look great. Thanks!  I suspect that you aren't goning to use raw X11 for windows and input so my suggestion would depend on the GUI toolkit you plan to use. Qt has its own painting engine. You can paint directly on to windows or widgets or you can paint onto a QPicture which allows you to both display print and save the result easily. For more complex scenes you can turn to QGraphicsScene. With gtk it's more common to use cairo already mentioned by Jeff Foster Even better in Qt if you use a QPainter on a QGLWidget it can benefit from your graphics card's anti-aliasing/multi-sampling capabilities (if enabled) and it can look stunning. Unless you multi-sample 768 times it won't be as good antialiasing as pixel-coverage based antialiasing which is what anti-grain uses
823,A,BlackBerry - Cropping image i want to crop a part of Image for that i am using following code:  int x=20; int y=50; int [] rgbdata=new int[(0+width-x+height-y)* (image.getWidth())]; image.getARGB(rgbdata 0 image.getWidth() x y width height); cropedImage=new Bitmap(image.getWidth()image.getWidth()); cropedImage.setARGB(rgbdata 0image.getWidth() 8080 width height); x an y are the position from where the cropping will be done in the rectangular form. but it is not working can any one help me out please. any sample code will work for me. its urgent. thanks in advance you can do this using graphics: public Bitmap cropImage(Bitmap image int x int y int width int height) { Bitmap result = new Bitmap(width height); Graphics g = new Graphics(result); g.drawBitmap(0 0 width height image x y); return result; }
824,A,"How can I ""best fit"" an arbitrary cairo (pycairo) path? It seems like given the information in stroke_extents() and the translate(x y) and scale(x y) functions I should be able to take any arbitrary cairo (I'm using pycairo) path and ""best fit"" it. In other words center it and expand it to fill the available space. Before drawing the path I have scaled the canvas such that the origin is the lower left corner up is y+ right is x+ and the height and width are both 1. Given these conditions this code seems to correctly scale the path: # cr is the canvas extents = cr.stroke_extents() x_size = abs(extents[0]) + abs(extents[2]) y_size = abs(extents[1]) + abs(extents[3]) cr.scale(1.0 / x_size 1.0 / y_size) I cannot for the life of me figure out the translating though. Is there a simpler approach? How can I ""best fit"" a cairo path on its canvas? Please ask for clarification if anything is unclear in this question. I have found a solution that I like (at least for my purposes). Just create a new surface and paint the old surface on to the new one.  As for the scale only I have done a similar thing to adjust an image inside a box with a ""best-fit"" approach. As about scale here is the code: available_width = 800 available_height = 600 path_width = 500 figure_height = 700 # formulas width_ratio = float(available_width)/path_width height_ratio = float(available_height)/figure_height scale = min(height_ratio width_ratio) # result new_path_width = path_width*scale new_figure_height = figure_height*scale print new_path_width new_figure_height The image gets drawn aligned to the origin (top left in my case) so perhaps a similar thing should be done to translate the path. Also this best fit is intended to preserve aspect ratio. If you want to stretch the figure use each of the ratios instead of the 'scale' variable. Hope I have helped"
825,A,What can Pygame do in terms of graphics that wxPython can't? I want to develop a very simple 2D game in Python. Pygame is the most popular library for game development in Python but I'm already quite familiar with wxPython and feel comfortable using it. I've even written a Tetris clone in it and it was pretty smooth. I wonder what does Pygame offer in terms of graphics (leaving sound aside for a moment) that wxPython can't do ? Is it somehow simpler/faster to do graphics in Pygame than in wxPython ? Is it even more cross-platform ? It looks like I'm missing something here but I don't know what. Well in theory there is nothing you can do with Pygame that you can't with wxPython. The point is not what but how. In my opinion it's easier to write a game with PyGame becasue: It's faster. Pygame is based on SDL which is a C library specifically designed for games it has been developed with speed in mind. When you develop games you need speed. Is a game library not a general purpose canvas It has classes and functions useful for sprites transformations input handling drawing collision detection. It also implements algorithms and techniques often used in games like dirty rectangles page flipping etc. There are thousands of games and examples made with it. It will be easier for you to discover how to do any trick. There are a lot of libraries with effects and utilities you could reuse. You want an isometric game there is a library you want a physics engine there is a library you what some cool visual effect there is a library. PyWeek. :) This is to make the development of your game even funnier! For some very simple games like Tetris the difference won't be too much but if you want to develop a fairly complex game believe me you will want something like PyGame.  wxPython is based on wxWidgets which is a GUI-oriented toolkit. It has the advantage of using the styles and decorations provided by the system it runs on and thus it is very easy to write portable applications that integrate nicely into the look and feel of whatever you're running. You want a checkbox? Use wxCheckBox and wxPython will handle looks and interaction. pyGame on the other hand is oriented towards game development and thus brings you closer to the hardware in ways wxPython doesn't (and doesn't need to since it calls the OS for drawing most of its controls). pyGame has lots of game related stuff like collision detection fine-grained control of surfaces and layers or flipping display buffers at a time of your choosing. That said graphics-wise you can probably always find a way to do what you want with both toolkits. However when speed counts or you wish to implement graphically more taxing game ideas than Tetris you're probably better off with pyGame. If you want to use lots of GUI elements and don't need the fancy graphics and sound functions you're better off with wxPython. Portability is not an issue. Both are available for the big three (Linux OSX Windows). It's more a question of what kind of special capabilities you need really. You might want to consider displaying pyGame-graphics in a wxPython-Window. Try this link: http://wiki.wxpython.org/IntegratingPyGame Yeah I've looked at that before but it doesn't look very comprehensive. The descriptions are somewhat reserved and doubtful which is also what I found in other places. And what if I want to use both ? I.e. have a graphic-full game and be able to control / configure it with standard GUI widgets ? As of this comment I've had good success mixing the two. I've just had to be a bit careful but everything running smoothly without crashes
826,A,"OpenGL 2D Texture Mapping problem I am relatively new to OpenGL and I am having some issues when I am rendering an image as a texture for a QUAD which is as the same size of the image. Here is my code. I would be very grateful if someone helps me to solve this problem. The image appears way smaller and is squished. (BTW the image dimensions are 500x375). glGenTextures( 1 &S_GLator_InputFrameTextureIDSu ); glBindTexture(GL_TEXTURE_2D S_GLator_InputFrameTextureIDSu); glTexParameteri(GL_TEXTURE_2D GL_TEXTURE_WRAP_S GL_CLAMP); glTexParameteri (GL_TEXTURE_2D GL_TEXTURE_WRAP_T GL_CLAMP); glTexParameteri(GL_TEXTURE_2DGL_TEXTURE_MAG_FILTERGL_LINEAR); glTexParameteri(GL_TEXTURE_2DGL_TEXTURE_MIN_FILTERGL_LINEAR); glTexImage2D( GL_TEXTURE_2D 0 4 S_GLator_EffectCommonData.mRenderBufferWidthSu S_GLator_EffectCommonData.mRenderBufferHeightSu 0 GL_RGBA GL_UNSIGNED_BYTE dataP); glBindTexture(GL_TEXTURE_2D S_GLator_InputFrameTextureIDSu); glTexSubImage2D(GL_TEXTURE_2D 0 0 0 S_GLator_EffectCommonData.mRenderBufferWidthSu S_GLator_EffectCommonData.mRenderBufferHeightSu GL_RGBA GL_UNSIGNED_BYTE bufferP); //set the matrix modes glMatrixMode( GL_PROJECTION ); glLoadIdentity(); //gluPerspective( 45.0 (GLdouble)widthL / heightL 0.1 100.0 ); glOrtho (0 1 0 1 -1 1); // Set up the frame-buffer object just like a window. glViewport( 0 0 widthL heightL ); glDisable(GL_DEPTH_TEST); glClearColor( 0.0f 0.0f 0.0f 0.0f ); glClear( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT ); glMatrixMode( GL_MODELVIEW ); glLoadIdentity(); glBindTexture( GL_TEXTURE_2D S_GLator_InputFrameTextureIDSu ); //Render the geometry to the frame-buffer object glBegin(GL_QUADS); //input frame glColor4f(1.f1.f1.f1.f); glTexCoord2f(0.0f0.0f); glVertex3f(0.f 0.f 0.0f); glTexCoord2f(1.0f0.0f); glVertex3f(1.f 0.f0.0f); glTexCoord2f(1.0f1.f); glVertex3f(1.f 1.f0.0f); glTexCoord2f(0.0f1.f); glVertex3f(0.f 1.f0.0f); glEnd(); a picture of what you observe would help. I think I see you issue here. glViewport defines the viewport size not the window size. So when you create your window (with glut sdl or wgl or whatever) you have to specify the size. To get a 1:1 mapping of the glViewport to this window those two need to match. and I'm assuming your fbo is non-square? The one thing that's making this a tad hard to figure out is that you've got a different vertical and horizontal pixel size. You're texture is non-square but yet you've coded the above as if it was.....now there's nothing wrong with this approach but id does make it a bit hard to figure out. What happens if instead of using 0-1 as your coordinates you re-write it to use 0-width. btw...0-1 should read ""zero to one"" not ""zero minus one"" Thank you for the reply. If am rendering it to an FBO so there is no window here. Even if put the view port sizes in the ""glOrtho (0 1 0 1 -1 1);"" I get the same problem. Also on an interesting note I can draw the QUAD perfectly without any size issues but I am having problem with the textures. Hope that might help a little bit more to understand the problem.  First your image dimensions must be powers of two not just arbitrary 500x375. Second the internalformat (third) argument to glTexImage2D should be a symbolic constant describing the internal format layout not just ""4"" as it's deprecated since OpenGL 1.2. Third your dataP and bufferP storage layout must correspond to the current GL_UNPACK_ALIGNMENT. Fourth your equilateral quad on which you're trying to put the texture on is essentially square and thus your non-square texture will appear squashed or stretched with such a texture coordinates. Hey Thanks for the info. I did end up using Rectangular Textures later on but I will make the other corrections you suggested.  Ok somebody replied me yesterday to change the texture coordinates by dividing the image dimensions with the nearest power of two number and that worked! I don't know why I dont see that answer today but anyways thank you all for taking time to help me out. I think this just masked your problem. Using rectangular non-power-of-two textures on square surfaces with some kind of ""magic math"" would suddenly pop out in future."
827,A,"Minimize Polygon Vertices What is a good algorithm for reducing the number of vertices in a polygon without changing the way it looks very much? Input: A polygon represented as a list of points with way too many verticies: raw input from the mouse for example. Output: A polygon with much fewer verticies that still looks a lot like the original: something usable for collision detection for example (not necessarily convex). Edit: The solution to this would be similar to finding a multi-segmented line of best fit on a graph. It's called Segmented Least Squares in my algorithms book. Edit2: The Douglas Peucker Algorithm is what I really want. Wow! This algorithm just ROCKS! :D There's a lot of material out there. Just google for things like ""mesh reduction"" ""mesh simplification"" ""mesh optimization"" etc. Most of these mesh algorithms expect many triangles. I'm just trying to reduce the vertices in a single polygon. If your polygon isn't already represented by a mesh of triangles can't you convert it to a mesh and use any of the mentioned algorithms?  Edit: Oh look Simplifying Polygons You mentioned collision detection. You could go really simple and calculate a bounding convex hull around it. If you cared about the concave areas you can calculate a concave hull by taking the centroid of your polygon and choosing a point to start. From the starting point rotate around the centroid finding each vertex you want to keep and assigning that as the next vertex in the bounding hull. The complexity of the algorithm would come in how you determined which vertices to keep but I'm sure you thought of that already. You can throw all your vertices into buckets based on their location relative to the centroid. When a bucket gets more than an arbitrary number of vertices full you can split it. Then take the mean of the vertices in that bucket as the vertex to use in your bounding hull. Or forget the buckets and when you're moving around the centroid only choose a point if its more than a given distance from the last point. Actually you could probably just use all the vertices in your polygon as ""cloud of points"" and calculate the concave hull around that. I'll look for an algorithm link. Worst case on this would be a completely convex polygon. Another alternative is to start with a bounding rectangle. For each vertex on the rectangle find the distance from the point to the polygon. For the farthest vertex split it into two more vertices and move them in some. Repeat until some proportion of either vertices or area is met. I'd have to think about the details of this one a little more. If you care about the polygon actually looking similar even in the case of a self-intersecting polygon then another approach would be required but it doesn't sound like thats necessary since you asked about collision detection. This post has some details about the convex hull part. I can use the algorithms at cgal.org to deconstruct my polygon into convex components later if needed for collision detection. Sorry for the red herring."
828,A,Simpliest ancestor for a clickable hand-drawn component? I am creating a component that will be a large plus or a large minus. I don't want to use a bitmap because even I can draw this using the Graphics class but the component must be clickable (the Shape class is not). It will be part of an item renderer so I want it to be as light-weight as possible. UIComponent does not seem to sent CLICK messages. Thanks for any ideas How about SpriteAsset? I don't recommend you to hand-draw the component it may damage your screen. If you look here: UIComponent Docs You will see that UIComponent has InteractiveObject in its inheritance path. InteractiveObject is the class that adds mouse event functionality.  I would suggest creating a Sprite object and drawing the minus and plus arrows to its graphics object. You'll then have to addEventListener(MouseEvent.CLICK someFunction); in its constructor or wherever else you'll need it. You may also want to set cacheAsBitmap to true at that point so that it's not redrawn every frame. EDIT: Per @jeremynealbrown you have to use the SpriteAsset class if you are working in Flex apparently. Very similar but another 2 levels of abstraction are added. Sprite is the highest class that you can access both the graphics object and the MouseEvent.CLICK event. What about MovieClips? A Sprite is essentially the same thing as a MovieClip but without the timeline. A Sprite is higher in the heirarchy than a MovieClip so it will be more lightweight. MovieClip extends Sprite :) I guess it's how you look at it higher-lower True I generally look at the base class as being at the top and everything else comes down from it. Not sure why... lol and I think of it the other way around! Neither will work with the Flex framework you will need to use SpriteAsset or MovieClipAsset.  UIComponent will actually dispatch click events. However if there is no content drawn to the graphics the UIComponent will have no area that can be clicked. If the plus or minus icon you draw is too small to reliably catch mouse activities then draw a fully-transparent rectangle to increase the hit area. Exactly what I was going to reply :)
829,A,"PCL and 1200 DPI Is it possible to send the PCL Set resolution command with 1200 dpi (and higher) as parameter? I've been looking at the spec sheets for HP PCL and it tells me that the only valid values for it are 75 100 150 200 300 600. I am trying to print to a non-HP machine claiming to support HP PCL5 and it prints fine for 300 and 600 dpi but when I try printing at 1200 dpi it prints bigger (2x the number of lines horizontally and vertically). Seems to be using 600 dpi still.. Thanks Edward I will assume you're using Windows based on the driver comments. You might want to try bypassing the driver altogether using Raw printing. Here's a Microsoft article that shows how to do it in C# and searching for raw printing should turn up more if that's not adequate. The complete PCL 5 manual is available as a PDF from HP as well. Thanks for answering.. I actually already have a way to send the raw data to the printer.. My problem was that the printer wasn't recognizing the 1200 dpi command when using PCL5 and I was wondering if I was doing it wrong.. Thanks anyway :) I've switched to PCLXL and it works :)  On most printers resolution can be set by either PJL or PCL. In PCL resolution can be set this way: <esc>*t1200R A printer that truely supports 1200dpi will honour this setting. In PJL the device might support something like this: @PJL SET RESOLUTION=1200 What I typically do in a situation like this is to install the proper PCL driver (5 not 6/XL) for the device on a Windows system set all the settings in the driver and print something to file (use something simple like some text in Notepad). You should see one or both of the above settings. Another option is to then just swich the resolution back and compare the files to see what's changed. Thanks for the reply.. I have set them both in PJL and PCL using those commands above. It seems the printer doesn't honor the 1200dpi setting.. The marketing material for the printer says it supports true 2400x2400 dpi printing so I don't know what's wrong.. I already know about the print to file trick but I guess I forgot thanks for remindng me :) I just tried printing but I couldn't find the PCL drivers for it.. Only had PCL6 drivers. Does this mean I have to use PCL6 for this? :S PCL6 is a compiled stream of data as such you won't be able to make sense of what's in it. You can try though maybe they have embedded the resolution command in the PJL of th PCL6 driver. I surprised there is no PCL5 driver for it. What's the device? Sometimes (like in the case of Lexmark) they only make 1 PCL driver and you select XL (PCL6) or Raster (PCL5) or GL/2 (PCL5) under the graphics tab. That defines what it outputs Sorry I didn't realize you had commented on this again.. Had gotten busy cause I had to be pulled away to work on another project.. The device in question is a Xerox DocuCentre II C5400. So I checked the driver again there isn't an option to select XL Raster or GL/2.. It just prints in PCLXL. One weird thing though I printed in Black/High resolution mode and the PJL command in the file lists the SET RESOLUTION command as 1200. However if I check the advanced options in the driver there is a combo box to set the resolution but it only has the following values Auto 600 300 and 200. However the marketing material for it states that it has true 2400x2400 printing.. Is this relevant at all? I'm not able to manually set the resolution to 1200 however by choosing ""High Resolution"" under the Image Quality combo box under the image options tab it IS printing at 1200 (or so the PJL commands tell me..)"
830,A,"How to deal with animating in-between states when the model is discrete The data model in my program has a number of discrete states but I want to animate the transition between these states. While the animation is going on what the user sees on the screen is disconnected from what the underlying data is like. Once the animation is complete they match up again. For example let's say we have a simple game where Snuffles the bunny hops around on a 2D grid. The model of Snuffles contains integer x/y coordinates. When the player tells Snuffles to hop north his y-coordinate is decremented by one immediately. However on the screen Snuffles should at that point still be in his old location. Then frame by frame Snuffles proceeds to hop over to his new location until he is shown in the location his model states. So usually when we draw Snuffles we can just look up his coordinates in his model. But when he's hopping that coordinate is wrong. If there is only ever one thing moving on the screen I can just about get away with freezing the entire game state and not allowing the user to do anything until Snuffles has finished hopping. But what if there is more than one bunny on the screen? It gets worse if elements interact merge or split. If Snuffles magically merges with a hat to become a potato at which point does the data model delete the bunny and the hat and add the potato? If it does so immediately the view instantly loses access to information about Snuffles and the potato which it still needs to draw the animation of the magical merger. I have come across this problem multiple times while implementing animated GUIs especially games and have found no satisfactory solution. Unsatisfactory ones include: Do the changes immediately but then suspend any further changes in the model until the animation has resolved. Makes things unresponsive and doesn't work if more than one thing can move or things interact in complex ways. Merge the model and the view - Snuffles gains floating-point coordinates and probably a z-coordinate to indicate how far up he is. The model's rules become massively more complex as a result as the model can no longer make concise statements like ""you cannot hop north if there is a wall at (x y - 1)"". Any change to the rules takes much longer and development slows to a crawl. Keep what amounts to a duplicate of the data in the view. SnufflesModel has integer coordinates but SnufflesSprite has floating-point ones. End up duplicating some of the model rules in the view and having to keep them in sync. Spend lots of time debugging to make sure that SnufflesModel and SnufflesSprite don't de-sync under some rare circumstance. My best bet at the moment is option 3 but it hardly strikes me as elegant. Thoughts? You would need a stronger model to account for the time component of changes: Each sprite needs to maintain a queue of animation actions it is supposed to carry out. Adding animations to the queue should be a zero-time action (game time). Queued animations proceed on a frame-by-frame basis when they get a tick from the animation clock. Queuing lets you separate the model from the graphics subsystem and the animations. Each animation in the queue carries with it a model action to perform when the animation is complete. Some languages make that easier e.g. with anonymous functions in C# or JavaScript. In other languages you could use a callback instead. The model action lets you specify how the model would change when the animation completes. Sprites can carry high resolution coordinates (e.g. floating point) while the model stays with integer ones. Sprites don't need to know anything about the game rules though -- the queued animation-completion model-actions deal with that. Model entities should be able to account for a transitional states: appearing disappearing and moving. This lets you avoid checking the rules for objects in transition. To implement Snuffle's quest you could then: User asked to move Snuffle north? -(1) check that the rules allow the move (2) queue a move animation on Snuffle's sprite coupled with a landing model action. Put the bunny's model into a transitional moving state. Snuffle's landing model action brings the bunny's model back to a normal state and checks the landing spot and the rules. Finding a hat it queues a potato appearance animation (the ""merger"") and two disappearance animations for the bunny and hat. The model actions for the disappearance animations delete the bunny and hat when complete."
831,A,"Python - Best library for drawing So I'm looking for a pretty basic library in python where I can create a window and then draw lines and basic shapes on it. Nothing too complex just nice and simple. I figure there's lots of libraries out there that can do this but I don't know any of them. Can anyone give me some recommendations? EDIT: Note that there are several answers describing different GUI toolkits with code snippets. Take a look at the various answers for a brief look into the various toolkits. I think Stack Overflow needs to change their ""Q&A format"" rules. Clearly this was a useful post as many have pointed out with +1 and yet it was closed as ""not constructive"". He asked for recommendations and he got them. Q&A. Boom. (I'm not the ""Code Monkey"" that closed it btw) Yea I'm not really sure why this was closed. It was helpful and continues to be (it's views and votes go up all the time). Though despised by the GMs of this site this type of closed question has become one of the site's most useful features. A hated offspring helping thousands of people like Cinderella. I have created a proposal on Area 51 for a new site specifically to be a home for questions like this. Head over and make it happen! http://area51.stackexchange.com/proposals/66606/code-recommendations?referrer=S-_R40wBFZh6TlZ3Al4_Sw2 The ""turtle"" module from python's standard library provides turtle graphics similar to LOGO (but with python syntax of course) in an automagically generated TK window. I certainly wouldn't say that this is the ""best"" way to draw graphics with python (too many inherent limitations) but it is extremely easy to get started with and may be all you need if your task is simple enough. from turtle import * setup (width=200 height=200 startx=0 starty=0) speed (""fastest"") # important! turtle is intolerably slow otherwise tracer (False) # This too: rendering the 'turtle' wastes time for i in range(200): forward(i) right(90.5) done() EDIT: There is also a new and improved module for turtle grapics called xturtle.py. I haven't tried it yet but it may be worth a look. EDIT: The extended xturtle module mentioned above seems to have been rolled into the standard library (as ""turtle"") from Python version 2.6 onwards. :D that 0.5 addition to 90 to make that awesome pattern just opened up a closet of million ideas to do stuff with turtle xD thanks a lot for this code! never thought of making curves with floating point angles mfw almost nobody mentions the standard and default way of accomplishing this task with a module that literally comes with python itself. .-. thank you for this.  Pyglet will do what you want and runs on OS X Windows and Linux. An example that draws a red line: import pyglet window = pyglet.window.Window(resizable=True) @window.event def on_draw(): window.clear() pyglet.gl.glColor4f(1.0001.0) pyglet.graphics.draw(2 pyglet.gl.GL_LINES ('v2i' (10 15 30 35)) ) pyglet.app.run() See more examples and documentation here. pyglet rocks it's so simple to get going with full 3D What a nice lib! Their own helloworld fails with AttributeError: 'module' object has no attribute 'window'. Totally broken with current Macs. Sadness and failure fries. The OS X situation is rapidly improving.  Try the TK canvas that comes with TkInter. TkInter comes bundled as standard with the base Python system so you don't have to go hunting down any third-party libraries. It's also pretty easy to learn and very portable as it runs on pretty much every platform supported by Python. A little ferreting around on the web gives this code sample from Fredrick Lundh's web site. from Tkinter import * master = Tk() w = Canvas(master width=200 height=100) w.pack() w.create_line(0 0 200 100) w.create_line(0 100 200 0 fill=""red"" dash=(4 4)) w.create_rectangle(50 25 150 75 fill=""blue"") mainloop() +1 for using standard module Googled ""python drawing"" or something like that. This is exactly what I was looking for thanks! This is great. :) +1. Also note that in Python 3+ the module name to import is now lowercase `tkinter`. Note that drawing pixels on the canvas is impossible directly and you have to use PhotoImage like so: http://stackoverflow.com/a/12287117/2036036  And here's a sample with PyQT taken from This tutorial at www.zetcode.com. QT is a cross-platform GUI toolkit with python bindings available from www.riverbankcomputing.com. It runs on Unix/Linux Windows OSX and there is also a version for smart phones. #!/usr/bin/python # colors.py import sys random from PyQt4 import QtGui QtCore class Colors(QtGui.QWidget): def __init__(self parent=None): QtGui.QWidget.__init__(self parent) self.setGeometry(300 300 350 280) self.setWindowTitle('Colors') def paintEvent(self event): paint = QtGui.QPainter() paint.begin(self) color = QtGui.QColor(0 0 0) color.setNamedColor('#d4d4d4') paint.setPen(color) paint.setBrush(QtGui.QColor(255 0 0 80)) paint.drawRect(10 15 90 60) paint.setBrush(QtGui.QColor(255 0 0 160)) paint.drawRect(130 15 90 60) paint.setBrush(QtGui.QColor(255 0 0 255)) paint.drawRect(250 15 90 60) paint.setBrush(QtGui.QColor(10 163 2 55)) paint.drawRect(10 105 90 60) paint.setBrush(QtGui.QColor(160 100 0 255)) paint.drawRect(130 105 90 60) paint.setBrush(QtGui.QColor(60 100 60 255)) paint.drawRect(250 105 90 60) paint.setBrush(QtGui.QColor(50 50 50 255)) paint.drawRect(10 195 90 60) paint.setBrush(QtGui.QColor(50 150 50 255)) paint.drawRect(130 195 90 60) paint.setBrush(QtGui.QColor(223 135 19 255)) paint.drawRect(250 195 90 60) paint.end() app = QtGui.QApplication(sys.argv) dt = Colors() dt.show() app.exec_()  I'd definitely recommend pygame. Here is some code that does what you want: import sys #import and init pygame import pygame pygame.init() #create the screen window = pygame.display.set_mode((640 480)) #draw a line - see http://www.pygame.org/docs/ref/draw.html for more pygame.draw.line(window (255 255 255) (0 0) (30 50)) #draw it to the screen pygame.display.flip() #input handling (somewhat boilerplate code): while True: for event in pygame.event.get(): if event.type == pygame.QUIT: sys.exit(0) else: print event +1 for the code sample. +1 for suggesting it simply because pygame is an awesome library with tons of easy to use examples and functions. Thanks - I would have never thought of pygame for simple graphics - certainly simpler than opengl. If you intend to publish your figures you'll want to use library that supports anti-aliased drawing. In pygame you can replace draw.line calls with draw.aaline to accomplish this. Another hack would be to draw figures on 4 X bigger canvas and then scale it down on intended size. Macports users beware py-game has dependencies: [port-alldeps](http://code.activestate.com/recipes/576868) -> py-game* -> python24* libsdl* libsdl_mixer* libsdl_image* libsdl_ttf* smpeg* py-numeric* libsdl* -> xorg-libXext* xorg-libXrandr* xrender perl5* -> perl5.8* ... +1. Also note that the event handling loop in SDL and pygame can be resource intensive if used the way it is in this code sample. To rectify this problem add something like `pygame.time.wait(10)` to give up processor time to other processes. Pygame is terrible. It's slow and limited. I believe the line ""if event.type == QUIT:"" should read ""if event.type == pygame.QUIT"". Also there needs to be an ""import sys"" line in there somewhere before sys is used. @Bryce: fixed! Thanks for the heads up. Pyglet >>>> Pygame. Though it's easier to use. So if you want to get something done really quickly use pygame. If you are into a serious project go for Pyglet which is nicely designed and implemented. @phooky: Sorry to hear that! What about the arc drawing was so bad? Note you can also draw (non-anti-aliased) polygons which might work better for some things.. Huge -1 for the drawing primitives in pygame. The arc drawing primitive in particular is a mess. I started a project with pygame based on this answer and had to rip it out later.  wxPython is pretty easy to get up to speed with. The tutorial is fairly good and it is cross-platform so your code will run on OSX Linux and WinXP with little or no changes to it. Here is a basic program to draw a circle in a window from this site. # draw lines a rounded-rectangle and a circle on a wx.PaintDC() surface # tested with Python24 and wxPython26 vegaseat 06mar2007 # Works with Python2.5 on OSX bcl 28Nov2008 import wx class MyFrame(wx.Frame): """"""a frame with a panel"""""" def __init__(self parent=None id=-1 title=None): wx.Frame.__init__(self parent id title) self.panel = wx.Panel(self size=(350 200)) self.panel.Bind(wx.EVT_PAINT self.on_paint) self.Fit() def on_paint(self event): # establish the painting surface dc = wx.PaintDC(self.panel) dc.SetPen(wx.Pen('blue' 4)) # draw a blue line (thickness = 4) dc.DrawLine(50 20 300 20) dc.SetPen(wx.Pen('red' 1)) # draw a red rounded-rectangle rect = wx.Rect(50 50 100 100) dc.DrawRoundedRectangleRect(rect 8) # draw a red circle with yellow fill dc.SetBrush(wx.Brush('yellow')) x = 250 y = 100 r = 50 dc.DrawCircle(x y r) # test it ... app = wx.PySimpleApp() frame1 = MyFrame(title='rounded-rectangle & circle') frame1.Center() frame1.Show() app.MainLoop() +1 Code samples showing off using the various systems is a ++good idea. -1 for recommending a 20 Mb large framework just for doing some basic drawing in a overcomplicated way. I agree wx is powerful and very useful in most cases. This is obviously not one of those cases. Thanks for explaining the -1. Maybe you don't realize that wxPython is included by default on OSX and many Linux distributions. (Accepted) PyGame is not exactly small neither.  A little digging and here's the scribble demo from the pygtk tutorial. Gtk started life on Unix/Linux but now has been ported to Windows and OSX. It has extensive support for theming - this theming support is baked into the system and is used to provide 'themes' that are actually wrappers for native widgets on the host O/S. Arguably GTK was the system that really popularised theming support in GUI toolkits. This allows applications with a reasonably native look and feel on non-X11 systems. This example is the scribble demo. It is perhaps a little long-winded so add a comment pointing me to something a bit more terse and and I'll substitute that. #!/usr/bin/env python # example scribblesimple.py # GTK - The GIMP Toolkit # Copyright (C) 1995-1997 Peter Mattis Spencer Kimball and Josh MacDonald # Copyright (C) 2001-2004 John Finlay # # This library is free software; you can redistribute it and/or # modify it under the terms of the GNU Library General Public # License as published by the Free Software Foundation; either # version 2 of the License or (at your option) any later version. # # This library is distributed in the hope that it will be useful # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU # Library General Public License for more details. # # You should have received a copy of the GNU Library General Public # License along with this library; if not write to the # Free Software Foundation Inc. 59 Temple Place - Suite 330 # Boston MA 02111-1307 USA. import pygtk pygtk.require('2.0') import gtk # Backing pixmap for drawing area pixmap = None # Create a new backing pixmap of the appropriate size def configure_event(widget event): global pixmap x y width height = widget.get_allocation() pixmap = gtk.gdk.Pixmap(widget.window width height) pixmap.draw_rectangle(widget.get_style().white_gc True 0 0 width height) return True # Redraw the screen from the backing pixmap def expose_event(widget event): x  y width height = event.area widget.window.draw_drawable(widget.get_style().fg_gc[gtk.STATE_NORMAL] pixmap x y x y width height) return False # Draw a rectangle on the screen def draw_brush(widget x y): rect = (int(x-5) int(y-5) 10 10) pixmap.draw_rectangle(widget.get_style().black_gc True rect[0] rect[1] rect[2] rect[3]) widget.queue_draw_area(rect[0] rect[1] rect[2] rect[3]) def button_press_event(widget event): if event.button == 1 and pixmap != None: draw_brush(widget event.x event.y) return True def motion_notify_event(widget event): if event.is_hint: x y state = event.window.get_pointer() else: x = event.x y = event.y state = event.state if state & gtk.gdk.BUTTON1_MASK and pixmap != None: draw_brush(widget x y) return True def main(): window = gtk.Window(gtk.WINDOW_TOPLEVEL) window.set_name (""Test Input"") vbox = gtk.VBox(False 0) window.add(vbox) vbox.show() window.connect(""destroy"" lambda w: gtk.main_quit()) # Create the drawing area drawing_area = gtk.DrawingArea() drawing_area.set_size_request(200 200) vbox.pack_start(drawing_area True True 0) drawing_area.show() # Signals used to handle backing pixmap drawing_area.connect(""expose_event"" expose_event) drawing_area.connect(""configure_event"" configure_event) # Event signals drawing_area.connect(""motion_notify_event"" motion_notify_event) drawing_area.connect(""button_press_event"" button_press_event) drawing_area.set_events(gtk.gdk.EXPOSURE_MASK | gtk.gdk.LEAVE_NOTIFY_MASK | gtk.gdk.BUTTON_PRESS_MASK | gtk.gdk.POINTER_MOTION_MASK | gtk.gdk.POINTER_MOTION_HINT_MASK) # .. And a quit button button = gtk.Button(""Quit"") vbox.pack_start(button False False 0) button.connect_object(""clicked"" lambda w: w.destroy() window) button.show() window.show() gtk.main() return 0 if __name__ == ""__main__"": main()  For the matter of completeness I contribute also a PyCairo example. import cairo as C from math import pi width height = 400 250 output = ""circle.png"" surf = C.ImageSurface(C.FORMAT_RGB24widthheight) ctx = C.Context(surf) # fill everyting with white ctx.new_path() ctx.set_source_rgb(0.90.90.9) ctx.rectangle(00widthheight) ctx.fill() # fill current path # display text in the center ctx.set_source_rgb(000) # black txt = ""Hello Stack Overflow!"" ctx.select_font_face(""Ubuntu"" C.FONT_SLANT_NORMAL C.FONT_WEIGHT_NORMAL) ctx.set_font_size(18) x_off y_off tw th = ctx.text_extents(txt)[:4] ctx.move_to(width/2-x_off-tw/2height/2-y_off-th/2) ctx.show_text(txt) # draw a circle in the center ctx.new_path() ctx.set_source_rgb(00.20.8) # blue ctx.arc(width/2height/2tw*0.602*pi) ctx.stroke() # stroke current path # save to PNG surf.write_to_png(output) And the result: Cairo can produce high quality graphics but as you see the programming style is extremely stateful and imperative and you need to refer to Cairo C Reference all the time and guess how its API has been translated in Python. Docstrings in PyCairo are lacking so it is not much fun to explore the library interactively from ipython. There is also CairoCFFI package which is mostly comptable with PyCairo but is better documented (there are docstrings). See also PyCairo Tutorial for more examples. +1 just for including a screenshot! If you have any trouble installing PyCairo or you want better documentation check out the nearly identical [cairocffi](http://pythonhosted.org/cairocffi/) @BenjaminGolder Thanks for the link. Docstrings alone are a reason enough to switch for me. Can you draw directly to a visible buffer (i.e. displayed in a window)? @EvgeniSergeev I suppose it can. ImageSurface should render to memory buffers. See http://pythonhosted.org/cairocffi/api.html#imagesurface (cairocffi reference) and http://cairographics.org/manual/cairo-Image-Surfaces.html (C reference). Though I'd use toolkit facilities to draw in a GUI window (GTK Qt wx etc).  In the past I have used PyGTK PyCairo (used PyGTK to create the window but cairo to draw) PyGame and http://www.pyglet.org/. They are all worth a look. Personally I'd probably use PyCairo with PyGTK but it's a little effort compared to NXC's suggestion. EDIT: Nowadays I'd probably use PyQt."
832,A,""".OFF"" descriptions I'd like to make a scene that uses meshes and primitives (such as spheres cylinders boxes etc). I was wondering if there are any recommendations with regard to where I can go to find .off files that describes complex meshes such as mountains rocks trees animals etc. There are plenty of sites that share/sell models like turbosquid. You can use Blender to import many 3d formats(3dsobjlwomaetc.) and export your mesh(es) to .off format. .obj is highlighted but you can see DEC Object File Format (*.off) on the list."
833,A,"How do I get access to an image of the desktop buffer in .NET? I am making a very simple app in C# with Visual Studio so I'm using that Forms package. I need to get access to an image of everything below it so that I can manipulate the image. How can I do this? It doesn't have to be real time very much since I'll probably be polling it not more than say 10fps. very interesting question You can use Interop to get a hook to the ""Desktop Window"" if that's what you mean and then you can use that to get your screenshot...this link might help: Getting the Desktop Window from .NET Another option (you said WinForms right) is to create a placeholder Bitmap and use the Graphics.CopyFromScreen method: int screenWidth = 1024; int screenHeight = 768; Bitmap holder = new Bitmap(screenWidth screenHeight); Graphics graphics = Graphics.FromImage(holder); graphics.CopyFromScreen(0000new Size(screenWidth screenHeight) CopyPixelOperation.SourceCopy); Link broke but it looks like there's a new link: http://www.peterprovost.org/blog/2003/08/05/Getting-the-Desktop-Window-from-NET/ @RobertB Thanks updated the link  You could use Graphics.CopyFromScreen but you will need to hide your window before you call it or otherwise it will appear in the image."
834,A,How to get camera up vector from roll pitch and yaw? I need to get an up vector for a camera (to get the right look) from a roll pitch and yaw angles (in degrees). I've been trying different things for a couple hours and have had no luck :(. Any help here would be appreciated! Yep Blaenk that is correct... I just needed some help with the maths :D Since you tagged it with all of the major graphics libraries I take it it's safe to assume that you mean how to do it in general. This may not directly answer your question although it may still help. I have a free open-source project for XNA that creates a debug terminal that overlays your game while it is running. You can use this for looking up values invoking methods or whatever. So if you have a transformation matrix and you wanted to extract various parts of it while the game is running you can do that. The project can be found here: http://www.protohacks.net/xna%5Fdebug%5Fterminal I don't have much expertise in the kind of math you are using but hopefully Shoosh's post helps on that. Maybe the debug terminal can help you when trying out his idea or in any other future problems you encounter. hah i can't wait to play with this later. It looks like a tool ive wanted for a while ... ive been having to write a console in my projects and output data there ... very very clunky.  Roll Pitch and Yaw define a rotation in 3 axis. from these angles you can construct a 3x3 transformation matrix which express this rotation (see here how) After you have this matrix you take your regular up vector say (010) if 'up' is the Y axis and multiply it with the matrix. What you'll get is the transformed up vector. Edit- Applying the transformation to (010) is the same thing as taking the middle row. The 3 rows of the matrix make up an orthogonal base of the rotated system. Mind you that a 3D graphic API uses 4x4 matrices. So to make a 4x4 matrix out of the 3x3 rotation matrix you need to add a '1' at M[3][3] (the corner) and zeros at the rest like so: r r r 0 r r r 0 r r r 0 0 0 0 1 Shoosh what if I don't apply the transformation to the (010)? Can I just pull out one of the rows/columns from the rotation matrix as my up? Thanks Yes instead of doing the vector-matrix transformation you can simplify the equation by leaving out the zero-sums/terms. Also vectors can be column-major or row-major. Make sure you extract the axis correctly.
835,A,How do I draw simple graphics in C#? I just want to draw simple 2D objects like circle line square etc in C#. How do I do that? Back in the Turbo C++ days I remember initializing some graphics library for doing the same. Do I need to do something similar in .NET? Is it any different for 3D objects? Will things like DirectX make this any easier? Any links to tutorials or samples much appreciated. Look for Managed Direct3D graphics API in .NET Source  Here's a simple code sample that will get you started (assumes you have a PictureBox named pictureBox1): Bitmap bmp = new Bitmap(pictureBox1.Width pictureBox1.Height); using (Graphics g = Graphics.FromImage(bmp)) { g.DrawLine(new Pen(Color.Red) 0 0 10 10); } pictureBox1.Image = bmp; The graphics object has a bunch of other drawing methods and Intellisense will show you how to call them.  Check out the System.Drawing namespace: http://msdn.microsoft.com/en-us/library/system.drawing.aspx  Look at the System.Drawing Namespace  As others have said check out System.Drawing. (I'm only repeating that for completeness.) System.Drawing exposes the GDI+ Windows drawing library to your application. A good tutorial to get you jump-started with System.Drawing and GDI+ can be found at C# Corner. Some important items to note: Many GDI+ objects implement the IDisposable interface and therefore should be wrapped in using blocks. Be sure you follow the appropriate disposal conventions; failing to dispose GDI+ objects can result in really nasty side effects for your app. (GDI+ objects in .NET correspond to their underlying Windows API equivalents.) APIs such as DirectX are extremely complex and for good reason. They're designed not for simple shapes but rather for complex highly-performant and highly-interactive multimedia applications. (In other words games typically.) You can access DirectX through the Managed DirectX interfaces but again it's probably overkill for your direct purposes. If you are interested in an easier way to work with DirectX XNA is the way to go. However this is very much a gaming-specific library and again is likely to be overkill.  GDI+ using System.Drawing  Read about GDI GDI+ System.Drawing namespace for example here. DirectX is not something you would use to draw simple shapes rather render complicated 3D stuff also using DX Api under C# is a bit trickier (although not that hard).  You need to use GDI+. How you do it depends slightly on what you want to draw on. You can draw on a control or a form or you can draw on an image object. Either way you need a System.Drawing.Graphics object which I believe is located in System.Drawing.dll. You can instantiate a new Bitmap class and call Graphics.FromImage(myImage) and then draw using the methods on the Graphics object you just created. If you want to draw on a form or control just override the OnPaint method and look for the Graphics property on the EventArgs class. More information on System.Drawing namespace here: http://msdn.microsoft.com/en-us/library/system.drawing.aspx You can't instantiate a new Image but you can instantiate a new Bitmap. Ah yes thanks for the clarification. I knew that but forgot.
836,A,"Combining graphics with a user interface in Java Hey guys I've got a nice text based java program that I'd like to add a GUI to. I have dabbled with Netbeans and shouldn't have too much of a problem getting the components in place. However the program will have to dynamically update images within the interface window and I'm not sure what the best way to go about doing it is. Are there any common practices for moving images (PNG's JPEG's...) around in the interface? Are there any good resources for getting very basic information on this? Thanks A little more information might be helpful. If you want to re-lay-out the images have a look at the Swing LayoutManager class. You can put the image in an ImageIcon the ImageIcon in a JLabel and the layout manager can position and re-position the JLabels on a JPanel. If you want images to be draggable convention is to put the JLabel into another container component (as labels do not usually support mouse or tab events). You can look those classes up in the Swing tutorial. There are also plenty of additional frameworks for making Swing work better for you -- some listed here Alternatively JavaFX also provides a quick way for doing Java GUIs that can be quicker and easier to write (if you don't mind learning a new scripting language). For example it makes adding animation (the images swooshing into place) easier.  Do you mean having images fly around on top of buttons and stuff? Or just within a specific region? If it's within a specific region and you don't need to move buttons and other GUI items around then the simplest way to do it would be to create a Canvas (or a swing component and overload the paint function) and then use the Graphics2D API to draw stuff. The basic outline would be: public class MyClass extends JComponent{ Image myPicture; int x y; //location to draw the picture. ... public void paint(Graphics g) { Graphics2D gx = (Graphics2D)g; //cast to graphics 2D g.drawImage(myPicturexythis); } ... } Then just change the values of x and y to move the image. yeah just add a paint function. Make sure the function signature is ""public void paint(Graphics g)"" and your function should run. The graphics will not overlap any of the GUI elements. I'm just a little overwhelmed by all the terminology and am not sure what the best way to do things is... I just want to keep it simple. I have the netbeans code but there is no paint() method in the code that it has generated. Shall I just add in the method myself and handle the updating of the graphics withing it?  Java has a nice and powerful API for drawing graphics and images. Have a look at the 2D Graphics tutorial from Sun's set of Java tutorials. If you want to do animation you'll find the timing framework useful. A nice book about making great-looking animated GUIs in Java is Filthy Rich Clients by Chet Haase and Romain Guy."
837,A,"Java 2D Shading / Filling I have created a ""blob"" from Bezier curves (screenshot below) and would now like to shade it in such a way that it appears pseudo-3D with darker shading on all ""left"" edges and lighter on all ""right"" edges and perhaps pure white ""light spots"" on the surface itself. For example: I'd be interested in how to achieve the shading used in this video. Can anyone recommend a good way to achieve this? I am guessing that standard Graphics2D.fill and setPaint methods may not be sophisticated enough. Also can anyone recommend some good resources (preferrably free / online) for learning more on this? EDIT Some additional information: To achieve the flat fill effect below I'm creating an Area object and am adding the individual Ellipse2D Shapes to it using add(new Area(ellipse)) and then finally adding the central polygon area to avoid leaving a white space in the middle. I used a custom RadialGradientPaint in this kineic model to get a pseudo-3D effect. I believe a more general implementation is available in Java 6. Thanks. Looks like Java 6's RadialGradientPaint only shades in a circular fashion but perhaps I can implement my own using this code as a starting point. I was thinking you might shade the largest inscribed circle of each ellipse using alpha to ""feather"" the edges.  The IPhone apps have access to OpenGL-ES which allows significant latitude in shading and rendering what is basically a coloured iso-surface with emissive lighting. Java2d will definitely not be sophisticated enough unless you are willing to write a whole software 3d library for it. Mixing 2d and 3d is also possible. Thanks for the advice. I'm still fairly new to graphics so will probably steer clear of 3D graphics / Open GL for the moment. Maybe I can cheat by defining a Stroke that applies shading to the edges of the blob. Not sure if I could change the shading colour depending on the current direction of the path being stroked though."
838,A,"List of alternatives for functions missing in OpenGLES There are number of functions that exist in OpenGL but not in OpenGLES 1.1 (for iPhone). Is there a list or resource that lists some alternative functions that can be used in OpenGLES 1.1? For example: gluOrtho2D glPolygonMode glVertex3f etc See: http://stackoverflow.com/questions/272970/whats-in-and-out-of-opengl-es-porting-from-opengl from the man page for gluOrtho2D DESCRIPTION gluOrtho2D sets up a two-dimensional orthographic viewing region. This is equivalent to calling glOrtho with near=-1 and far=1. Instead of using glVertex3f you must use Vertex Arrays see link  Numerous convenience functions have been stripped in the OpenGLES standard for simplicity as well as inefficient depreciated functions. There where mainly to make commonly used features easier or provide optimized implementations of common commands. The two examples you have given gluOrtho2D is just a wrapper for glOrtho and glPolygonMode can be achived with glTriangle's with a bit of pre-processing. Im not sure of a list perse but if a function doesnt exist most probably the man page will tell you what similar functions it acts as a wrapper for or the alternatives you can use.  The ""OpenGL ES 1.1.12 Difference Specification"" (http://www.khronos.org/registry/gles/specs/1.1/es_cm_spec_1.1.12.pdf) lists the differences between OpenGL ES 1.X and OpenGL 1.5. The iPhone uses OpenGL ES 1.1 I would also recommend you make a list of the OpenGL functions you call and check the ES documentation to see if they are wholly/partially supported.  You should be able to substitute glOrtho for gluOrtho2D. The only extra thing you have to do is set your near and far clipping planes. It looks as though glPolygonMode is not part of the OpenGLES spec because only filled triangles are supported. See here."
839,A,"Starting Graphics & Games Programming (Java and maybe C++) I've always had an interest in creating my own games and now at university I have the opportunity to create some 2D and 3D games using Java and C++ for those who are that way inclined. I've never really programmed a game before let alone graphics so I'm completely new to the area. After a quick trip to the library today I came across very little information on starting 2D game development or even graphics programming in Java or C++. I can program in Java at a reasonable level but I have never touched C++. Can someone recommend any good books on starting Graphics Programming in Java or C++? I have no C++ experience but I've programmed in C and Java and feel reasonably comfortable in both. Should I take the jump and dive into C++ when important marks are at stake? I hear a lot of good things about C++ as far as Games Programming goes so I'm unsure as to what is the best option for me. I'm looking to write a 2D game in my own time for a bit of fun before I start getting into the heavy work at university. Can anyone recommend some good resources for those interested in writing their own games using OpenGL and Java/C++. For those who have followed my other questions here I am terrible at Maths and hold very little knowledge of even the foundations. Is this going to be a serious problem for me? If I were to need Math knowledge what do you recommend I brush up on? I apologise if my questions seem a bit vague as I'm a complete newbie when it comes to a lot of these topics. Any help you could provide would be much appreciated. EDIT: I've already checked out the question titled ""game programming"" but have found it to not really cater for my specific questions. Both. I've always had an interest in Graphics Programming but now that I have a class in it I'm forced to take action. As far as I am aware at university we'll be using Java and OpenGL (my university is a Java School so we only get support for that) but I am interested in learning C++ as well. Are you doing graphics at uni or just in your spare time? If you're doing it at uni then what do they use? C++ OpenGL and (usually) GLUT seems to be the standard language&APIs I've observed. It's probably useful to follow that path & knowing basics will give you more time to pick up the bonus marks Similar question here that you might find useful: [http://stackoverflow.com/questions/124322/game-programming#124427](http://stackoverflow.com/questions/124322/game-programming#124427) Game programming especially where graphics are concerned involves a fair amount of math. You'll at least want to have a basic familiarity with vectors and matrices specifically with regard to representing rotation translation and scaling transformations. To that end I recommend Geometric Tools for Computer Graphics. A course in linear algebra wouldn't hurt as well although that's probably already in your curriculum. As far as game programming in Java I recommend taking a look at jMonkeyEngine an open source game engine with all sorts of fun example code to get you started. It was originally based on the engine presented in the book 3D Game Engine Design (same author as Geometric Tools above) which is another good source of information about game programming in 3D. There's also C++ code and papers on various topics in 3D graphics at that book's official site.  I am going through a similar process to you at the moment. I found the following site helpful it has a great tutorial (space invaders in Java): http://www.cokeandcode.com/node/6 You can get the source code and mess around with it. You will probably benefit from an IDE if you don't already have a favorite you could try Netbeans (netbeans.org) which is free and I think it's pretty good. As for books this one is OK (it's java-centric): http://www.amazon.com/Killer-Game-Programming-Andrew-Davison/dp/0596007302 I personally decided to use Java for games (for now) because I am very comfortable with Java and far less so with C++. But you will find that most people use C++ for commercial games. Actually I started with pygame (python games framework) which is also nice to get started especially if you know python. That tutorial is great! I've just downloaded the source run through with it and it all looks a lot easier than I had anticipated. Thanks for the answer!  I am currently doing exactly the same thing as you are now at university but I have nearly finished studying that part of my course! How bizzare!?! Mite be able to send you some extra lecture material aswell if it helps? On top of books I think you should check out these sites: NeHe Productions Video Tutorials Rock Opengl Programming Guide: The Official Guide to Learning Opengl  Version 2.1 Addison Wesley 2007 Here are the books that I would also check out: Hearn & Baker Computer Graphics with OpenGL Prentice-Hall 2003. I'd also seriously consider using C++ as your programming language. From what I have been taught and what I have been reading around C/C++ language branch are the most used by the game industry for example I heard STEAM who made the half life series use C++ and some others. Here is a free C++ book on the internet: Thinking in C++ 2nd ed. Volume 1 ©2000 by Bruce Eckel Here is what I learnt from: C++ in 24 Hours Sams Teach Yourself  I know that you specifically asked for Java or C++ but I have found that PyGame (based on SDL) has been a really good primer for me on basic game techniques. They have a couple of comprehensive tutorials and examples and python is a very clear and easy to pick up language. I would also suggest the Xbox 360 option as well if you already have one. That is what I will be moving onto next so that I can learn all the 3D stuff that PyGame is not so great at. As for math you could probably get by without real skills for simple 2D games. If you intend on having any kind of physics simulation then are going to have to actaully learn some real math. Even basic 2D games like riding a motorcycle over jumps involve a lot of physics math to make it work/fun  I would recommend starting with C++ and SFML. C++ has the largest available library of existing code and every example you find will probably be C/C++ oriented. SFML is a windowing and graphics library (comparable to SDL if you've heard of that) done using object-oriented design that fits into C++ better. It'll allow you to get up and running with an OpenGL window right off the bat and the documentation is pretty good as well.  Here are a few books that I used when writing OpenGL code in C++: Fundamentals of Computer Graphics OpenGL SuperBible OpenGL Red Book OpenGL Primer You might check out the MIT OpenCourse on computer graphics too. It might help supplement your development. Best of luck!  Honnestly go buy a xbox 360 and download the free XNA SDK package to get started. Update your maths skills and modify some indie games in their sample kit. Build a small game invite some friends around for some beers and have FUN! Your going to learn a lot faster than touching C/C++/Java and also too your going to have fun well doing it. Interesting answer. At university I am told to use Java or another language if we're feeling adventurous but the Xbox 360 idea does sound like it'll be good fun when the coursework is out of the way and if I decide to keep making games. Thanks for answering! The reason why i recommend the Xbox 360 approach is because its fast has fast turn around times and is just fun! At this stage you do not want to be bogged down with the problems with large scale C++ development. This is were most young people dont get past. Why is turn around time so important? In our game development studio the turn around time from compile to run is about 5-10 minutes depending on the content. So if you make a mistake and your new its going to take you 5 minutes to find out. This is why turn around time is important at the start.  I have this book: Beginning C++ Through Game Programming. It might seem trivial at first but it teaches you a-lot as you go through it. There is no GUI based programming in this book just the console. Which is good to an extent if you want to see how an entire ""story"" of a game can come together. You can also check out Gamedev.net they have a vast amount of resources and articles to get you started. Good luck. :) Thank you for the link to the book. I'm not very confident in my programming abilities as of yet and this book seems to be perfect for my needs. I'll be sure to check it out very soon.  Something to help you get into OpenGL is GLUT - the OpenGL Utility Toolkit. It takes care of a lot of the lower-level setup that's a pain to deal with if you're starting an OpenGL project from scratch. It makes it easier to jump right in and start putting stuff on the screen. GLUT hasn't been updated for awhile but you can also try FreeGLUT which is a newer open source replacement of GLUT and is included by several linux distributions out of the box.  Short answer: Don't write in C++ !!! You'll spend more time futzing with the language than actually learning about games physics collisions etc. Get a copy of Python and PyGame. It's easy to get started but you'll actually learn heaps. Having learnt Java you'll be amazed how much simpler Python is for getting the same things done. Once you're comfortable with your skill set then look at Panda3D. It's what's used at Pixar / Disney. If you choose to get your hands dirty with C++ at this stage then diving into Panda3D is going to be good. All the major studios use Python as does Google. If you end up specialising in C++ you'll become an engine-room programmer. Very necessary but not as glamorous. Oh you mentioned grades etc. Unless you need to take a course in C++ programming you'd be wasting precious time and energy on it. I don't know that any major studios use Python for much game code other than EVE Online which uses it on its massive server backend. Certainly for any console platform I've found Python to be unacceptably memory-bulky."
840,A,"How to add graphics on top of a multiscaleimage? I want to add graphics to a multiscaleimage. I use the project that Deep Zoom Composer auto generated when creating a multiscaleimage. Displaying the multiscaleimage from VS2008 works. I've tried this and added the graphics to the maincanvas without any results: <Grid> <Canvas Name=""maincanvas""/> <MultiScaleImage/> </Grid> Edit: When the user zooms in on the multiscaleimage the user can place geometrically figures on the image. E.g. place a rectangle next to a person in the image which will act as a speech balloon. I know this can be done on the image before the image is scaled but I would like to do it dynamically. can u be more speciific.. u wanted to add a graphic item on top of the selected image... or all the loaded images?? By following the tutorial at link text and then using the following code made it possible to draw on top of the multiscaleimage. <Grid x:Name=""LayoutRoot""> <MultiScaleImage x:Name=""deepZoomObject"" Source=""source/dzc_output.xml""/> <Canvas> <Line X1=""10"" Y1=""100"" X2=""80"" Y2=""30"" Stroke=""Red"" StrokeThickness=""20"" Canvas.Top=""-16"" Canvas.Left=""238""></Line> </Canvas> </Grid>  In order to draw the grpahics over the particular image first u need to find out the co-odrinates of the multiscale image..Am not sure abt reading co-ordinates of all the images in mutliscale canvass image... but there is a way you can do it on user selected image... project silverlights wilfred posted an interesting article abt picking the selected multiscale item and its co-ordinates.. And there is a dedicated forum for deepzoom.. its very active one.. you can try that too.."
841,A,"CSG operations on implicit surfaces with marching cubes I render isosurfaces with marching cubes (or perhaps marching squares as this is 2D) and I want to do set operations like set difference intersection and union. I thought this was easy to implement by simply choosing between two vertex scalars from two different implicit surfaces but it is not. For my initial testing I tried with two spheres circles and the set operation difference. i.e A - B. One circle is moving and the other one is stationary. Here's the approach I tried when picking vertex scalars and when classifying corner vertices as inside or outside. The code is written in C++. OpenGL is used for rendering but that's not important. Normal rendering without any CSG operations does give the expected result.  void march(const vec2& cmin //min x and y for the grid cell const vec2& cmax //max x and y for the grid cell std::vector<vec2>& tri float iso float (*cmp1)(const vec2&) //distance from stationary circle float (*cmp2)(const vec2&) //distance from moving circle ) { unsigned int squareindex = 0; float scalar[4]; vec2 verts[8]; /* initial setup of the grid cell */ verts[0] = vec2(cmax.x cmax.y); verts[2] = vec2(cmin.x cmax.y); verts[4] = vec2(cmin.x cmin.y); verts[6] = vec2(cmax.x cmin.y); float s1s2; /********************************** ********For-loop of interest****** *******Set difference between **** *******two implicit surfaces****** **********************************/ for(int i=0j=0; i<4; ++i j+=2){ s1 = cmp1(verts[j]); s2 = cmp2(verts[j]); if((s1 < iso)){ //if inside circle1 if((s2 < iso)){ //if inside circle2 scalar[i] = s2; //then set the scalar to the moving circle } else { scalar[i] = s1; //only inside circle1 squareindex |= (1<<i); //mark as inside } } else { scalar[i] = s1; //inside neither circle } } if(squareindex == 0) return; /* Usual interpolation between edge points to compute the new intersection points */ verts[1] = mix(iso verts[0] verts[2] scalar[0] scalar[1]); verts[3] = mix(iso verts[2] verts[4] scalar[1] scalar[2]); verts[5] = mix(iso verts[4] verts[6] scalar[2] scalar[3]); verts[7] = mix(iso verts[6] verts[0] scalar[3] scalar[0]); for(int i=0; i<10; ++i){ //10 = maxmimum 3 triangles + one end token int index = triTable[squareindex][i]; //look up our indices for triangulation if(index == -1) break; tri.push_back(verts[index]); } } This gives me weird jaggies: It looks like the CSG operation is done without interpolation. It just ""discards"" the whole triangle. Do I need to interpolate in some other way or combine the vertex scalar values? I'd love some help with this. A full testcase can be downloaded HERE EDIT: Basically my implementation of marching squares works fine. It is my scalar field which is broken and I wonder what the correct way would look like. Preferably I'm looking for a general approach to implement the three set operations I discussed above for the usual primitives (circle rectangle/square plane) EDIT 2: Here are some new images after implementing the answerer's whitepaper: 1.Difference 2.Intersection 3.Union EDIT 3: I implemented this in 3D too with proper shading/lighting: 1.Difference between a greater sphere and a smaller sphere 2.Difference between a greater sphere and a smaller sphere in the center clipped by two planes on both sides and then union with a sphere in the center. 3.Union between two cylinders. This is not how you mix the scalar fields. Your scalars say one thing but your flags whether you are inside or not say another. First merge the fields then render as if you were doing a single compound object: for(int i=0j=0; i<4; ++i j+=2){ s1 = cmp1(verts[j]); s2 = cmp2(verts[j]); s = max(s1 iso-s2); // This is the secret sauce if(s < iso) { // inside circle1 but not inside circle2 squareindex |= (1<<i); } scalar[i] = s; } This article might be helpful: Combining CSG modeling with soft blending using Lipschitz-based implicit surfaces. You're welcome! Good luck with your project implicit surfaces are fun! Hm weird. This does actually work but the edge get kind of weird. I'm going to investigate if this is a precision problem. It doesn't show up for normal circles though. Here's a screenshot of a 100x100 grid: http://www.mechcore.net/images/gfx/csgbug3.png Right it is a precision issue. Bigger circles works great but increasing the grid tesselation does not (I'm using floats). Great answer and great paper. My deepest thanks to you sir."
842,A,"Optimizing random-access bilinear sampling I'm working on an old-school ""image warp"" filter. Essentially I have a 2D array of pixels (ignoring for the present the question of whether they are color grayscale float RGBA etc.) and another 2D array of vectors (with floating-point components) with the image being at least as large as the vector array. In pseudo code I want to do this: FOR EACH PIXEL (xy) vec = vectors[xy] // Get vector val = get(img x + vec.x y + vec.y) // Get input at <xy> + vec output[xy] = val // Write to output The catch is that get() needs to bilinearly sample the input image because the vectors can refer to subpixel coordinates. But unlike bilinear sampling in say texture mapping where we can work the interpolation math into a loop so it's all just adds here the reads are from random locations. So get()'s definition looks something like this: FUNCTION get(inxy) ix = floor(x); iy = floor(y) // Integer upper-left coordinates xf = x - ix; yf = y - iy // Fractional parts a = in[ixiy]; b = in[iy+1iy] // Four bordering pixel values b = in[ixiy+1]; d = in[ix+1iy+1] ab = lerp(abxf) // Interpolate cd = lerp(cdxf) RETURN lerp(abcdyf) and lerp() is simply FUNCTION lerp(abx) RETURN (1-x)*a + x*b Assuming that neither the input image nor the vector array is known in advance what kind of high-level optimizations are possible? (Note: ""Use the GPU"" is cheating.) One thing I can think of is rearranging the interpolation math in get() so that we can cache the pixel reads and intermediate calculations for a given (ixiy). That way if successive accesses are to the same subpixel quad we can avoid some work. If the vector array is known in advance then we can rearrange it so that the coordinates passed to get() tend to be more local. This might help with cache locality as well but at the expense of having the writes to output be all over the place. But then we can't do fancy things like scale the vectors on the fly or even move the warp effect from its original precalculated location. The only other possibility would be to use fixed-point vector components perhaps with very limited fractional parts. E.g. if the vectors only have 2-bit fractional components then there are only 16 subpixel regions that could be accessed. We could precompute the weights for these and avoid much of the interpolation math altogether but with a hit to quality. Any other ideas? I want to accumulate a few different methods before I implement them and see which is the best. If someone could point me to source code of a fast implementation that would be great. Interesting problem. Your problem definition has basically enforced unpredictable access to in[xy] - since any vector might be provided. Assuming the vector image tends to refer to local pixels the very first optimisation would be to ensure that you traverse the memory in a suitable order to make the most of cache locality. This might mean scanning 32*32 blocks in the ""for each pixel"" loop so that in[xy] hits the same pixels as often as possible in a short time. Most likely the performance of your algorithm is going to be bound by two things How fast you can load vectors[xy] and in[xy] from main memory How long it takes to do the multiplications and sums There are SSE instructions that can multiply several elements together at a time and then add them together (multiply and accumulate). What you should do is compute af = (1 - xf) * ( 1 - yf ) bf = ( xf) * ( 1 - yf ) cf = (1 - xf) * ( yf ) df = ( xf) * ( yf ) and then compute a *= af b *= bf c *= cf d *= cf return (a + b + c + d) There is a good chance that both of these steps could be accomplished with a surprisingly small number of SSE instructions (depending on your pixel representation). I think that caching intermediate values is very unlikely to be useful - it seems extremely unlikely that >1% of the vector requests will point to exactly the same place and caching stuff will cost you much more in memory bandwidth than it will save. If you use the prefetch instructions on your cpu to prefetch in[vectors[x+1 y]] as you process vectors[xy] you might improve memory performance there is no way the CPU will be able to predict your random walk around memory otherwise. The final way to improve the performance of your algorithm is to handle chunks of input pixels at once i.e x[0..4] x[5..8] - this lets you unroll the inner maths loops. However you are so likely to be memory bound that this won't help."
843,A,"Displaying the axis values for a mouseover event in a chart image I return a chart image (with axis x and y) to the browser created my matplotlit. Now I want to show the axis values when there is mouseover event. When I did something like this recently I returned a PNG from matplotlib to the browser. I then applied an image-map to the PNG using the pixel values that correspond to the points plotted. I used an onmouseover event to pop up a 'tooltip' (actually just an absolute positioned div) that contained meta data about the point. This article provided the foundation for my efforts but I remember there were certain hiccups in his implementation (mostly due to changes in the matplotlib api). If this question is still lingering around on Monday I'll update this answer with specific code from my implementation (I don't have access to my work machines at the moment). EDIT: As Promised Sample Code import matplotlib.pyplot as plt dpi = 96 fig = plt.figure(figsize=(88)dpi=dpi) imgWidth = fig.get_figwidth() * dpi ## this is the actual pixel size of the plot imgHeight = fig.get_figheight() * dpi ## this is the actual pixel size my_lines = [] my_lines.append(plt.plot(XsYsmarker='o')[0]) # add a line object to plot mapHTML = '<MAP name=""curveMap"">' for lineObj in my_lines: ## convert the points to pixel locations for pop-ups lineObj._transform_path() path affine = lineObj._transformed_path.get_transformed_points_and_affine() path = affine.transform_path(path) for realpixel in zip(lineObj.get_xydata()path.vertices): mapHTML+='''<AREA shape=\""circle\"" coords=\""%i%i5\"" href=\""javascript: void(0);\"" onmouseout=\""outFly();\"" onmouseover=\""javascript: popFly\(event\\'%s\\'%i%i\)\"" />''' % (pixel[0]imgHeight-pixel[1]lineNamereal[0]real[1]) mapHTML += '</MAP>' fig.savefig(file(plot_file""w"")format='png'dpi=dpi) plt.close(fig) plotHTML = '''<img src=""/getPlot?uniq=%f"" width=""%i"" height=""%i"" ismap usemap=""#curveMap"" onload=""imageLoadCallback();"" id=""curPlot"" />''' % (time.time()imgWidthimgHeight) return ""({'plotHTML':'%s''mapHTML':'%s'})"" % (plotHTMLmapHTML) You'll see that I'm writing the image to a png file and then returning JSON. I use javascript to update a DIV with the new img and image map.  Here's a solution using jQuery: $('#myChart').mousemove(function(e){ var x = e.pageX - this.offsetLeft; var y = e.pageY - this.offsetTop; // Do something with x and y; }); See http://docs.jquery.com/Tutorials:Mouse_Position to learn more."
844,A,"How do I create a Bezier curve to represent a smoothed polyline? I have a polyline that approximates a curve and I want to draw it as a smooth curve in PostScript which supports Bezier curve drawing. To do that I need to generate two control points between each pair of polyline points. (I can't get the original curve from the source just the polyline.) I've had some success using cardinal splines from this description but the parameters there are different from Wikipedia and GDIPlus.DrawCurve both of which refer to tension. MS has no details and Wikipedia has incomplete details (cardinal spline ignores x values?). What are the formulas for the control points based on tension? You can do this with a two step process: first do a cubic b-spline and then get the cubic Bezier curves from this. The trick with fitting the Bezier curves is matching the derivatives at the segment end points and this is essentially what the spline does. If you can get at the representation of the spline it should have the points for the end points and the control point for each segment. Otherwise you could determine the slopes at each segment end point and the control point will be the intersection of the extrapolated lines.  See this link http://www.ibiblio.org/e-notes/Splines/Cardinal.htm which provides simple formulas which can be used to calculate Bezier control points for a multi-segment smooth curve. The equations are really simple but for those who don't want to repeat the calculations I'm providing my results: Let Pi (i=1..n) be the polyline points. First learn how to calculate the derivatives on Pi: P1' = (P2 - P1) / a Pi' = (Pi+1 - Pi-1) / a (for i=2..n-1) Pn' = (Pn - Pn-1) / a where ""a"" is a coeffecient (which probably means ""tension"" you mentioned) for instance a=2. Then for each segment i (i=1..n-1) from Pi to Pi+1 Bezier control points B1i and B2i would be: B1i = Pi + Pi'/3 B2i = Pi+1 - Pi+1'/3 I was hoping someone knew definitively but by inspecting the DrawPath results I think tension there is 2/a in the terms used in your answer."
845,A,"Can Jung graphics appear in the same place every time? I'm using JUNG ( http://jung.sourceforge.net/index.html ) to draw graphics in java. The software is great but I have a small question. How can I be sure that the displayed graph is each time the same (no changes is architecture or position)? To be more specific: the graph model (data to be represented) doesn't change but its representation changes each time I hit the ""View graph"" button :) [some vertices are in other places for example: sometimes in the upper part of the window sometimes in the lower part ] Thank you Iulian A StaticLayout lets you specify a vertex to Point2D transformer. This will allow you to control where the vertices are placed and should do what you want to do. You should use the following constructor: public StaticLayout(Graph<VE> graph org.apache.commons.collections15.Transformer<VPoint2D> initializer) You'll need to implement your own transformer that takes in a vertex and returns the location where the vertex should appear. An example of its in use: package test; import java.awt.Dimension; import java.awt.geom.Point2D; import java.io.IOException; import javax.swing.JFrame; import org.apache.commons.collections15.Transformer; import edu.uci.ics.jung.algorithms.layout.StaticLayout; import edu.uci.ics.jung.graph.Graph; import edu.uci.ics.jung.graph.SparseMultigraph; import edu.uci.ics.jung.visualization.VisualizationViewer; /** * Jung example - vertices appearing in same location * * @author Kah */ public class StaticLocation { /** * @param args * @throws IOException */ public static void main(String[] args) throws IOException { // Setup the example graph. Graph<Integer String> basis = new SparseMultigraph<Integer String>(); basis.addVertex(Integer.valueOf(0)); basis.addVertex(Integer.valueOf(1)); basis.addVertex(Integer.valueOf(2)); basis.addEdge(""Edge 1"" Integer.valueOf(0) Integer.valueOf(1)); basis.addEdge(""Edge 2"" Integer.valueOf(0) Integer.valueOf(2)); basis.addEdge(""Edge 3"" Integer.valueOf(1) Integer.valueOf(2)); Transformer<Integer Point2D> locationTransformer = new Transformer<Integer Point2D>() { @Override public Point2D transform(Integer vertex) { int value = (vertex.intValue() * 40) + 20; return new Point2D.Double((double) value (double) value); } }; StaticLayout<Integer String> layout = new StaticLayout<Integer String>( basis locationTransformer); layout.setSize(new Dimension(250 250)); VisualizationViewer<Integer String> vv = new VisualizationViewer<Integer String>( layout); vv.setPreferredSize(new Dimension(250 250)); JFrame frame = new JFrame(""Simple Graph View 2""); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.getContentPane().add(vv); vv.setOpaque(false); frame.pack(); frame.setVisible(true); } } Added 20 Feb 2010: An alternative is to use a PersistentLayoutImpl to save the locations of the vertices to a file. However you need to also somehow persist the graph to get which vertices and vertices were on there (this needs to persisted separately). There are number of classes for persisting the graph in edu.uci.ics.jung.io. This is an example that uses just PersistentLayoutImpl: package test; import java.awt.Dimension; import java.io.File; import java.io.FileWriter; import java.io.IOException; import javax.swing.JFrame; import org.apache.commons.collections15.Transformer; import edu.uci.ics.jung.algorithms.layout.Layout; import edu.uci.ics.jung.algorithms.layout.SpringLayout2; import edu.uci.ics.jung.graph.Graph; import edu.uci.ics.jung.graph.SparseMultigraph; import edu.uci.ics.jung.io.GraphMLReader; import edu.uci.ics.jung.io.GraphMLWriter; import edu.uci.ics.jung.visualization.VisualizationViewer; import edu.uci.ics.jung.visualization.decorators.ToStringLabeller; import edu.uci.ics.jung.visualization.layout.PersistentLayoutImpl; /** * Jung example - vertices appearing in same location * * @author Kah */ public class PersistentVertices { /** * @param args * @throws IOException */ public static void main(String[] args) throws IOException { // Setup the example graph. try { VisualizationViewer<Integer String> vv = new VisualizationViewer<Integer String>( getLayout()); vv.setPreferredSize(new Dimension(250 250)); JFrame frame = new JFrame(""Simple Graph View 2""); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.getContentPane().add(vv); vv.setOpaque(false); frame.pack(); frame.setVisible(true); } catch (Exception e) { e.printStackTrace(); } } private static Layout<Integer String> getLayout() throws IOException ClassNotFoundException { Graph<Integer String> graph = new SparseMultigraph<Integer String>(); File source = new File(""C:\\layout.dat""); SpringLayout2<Integer String> backing = new SpringLayout2<Integer String>( graph); PersistentLayoutImpl<Integer String> layout = new PersistentLayoutImpl<Integer String>( backing); layout.setSize(new Dimension(250 250)); // Note that you also need to put the vertices and edges back before // restoring. graph.addVertex(Integer.valueOf(0)); graph.addVertex(Integer.valueOf(1)); graph.addVertex(Integer.valueOf(2)); graph.addEdge(""Edge 1"" Integer.valueOf(0) Integer.valueOf(1)); graph.addEdge(""Edge 2"" Integer.valueOf(0) Integer.valueOf(2)); graph.addEdge(""Edge 3"" Integer.valueOf(1) Integer.valueOf(2)); if (source.exists()) { layout.restore(source.getAbsolutePath()); } else { layout.persist(source.getAbsolutePath()); } return layout; } } Note that the example does not persist the vertices and edges yet as I haven't had the time to figure out how use the classes in edu.uci.ics.jung.io yet. I was hoping to avoid that but that's life. I really think this is the best choice. Thank you Klarth. This means that I have to reposition all the points by myself? I have some complex graph architecture that needs to be visualized and I'm not sure if I can ""tell"" the right position of each vertex. No you don't have to reposition the points by yourself. You can use a PersistentLayout to use another layout and then save it. Please see my revised answer (I added something new). From your first part of the answer i have a question how can i get the exact xy coordinates so that i can save them in a file and later when reinitialize the graph i can set their position based upon ur first example? Is it possible?"
846,A,Compiling OpenGL SOIL on Mac OS X How would I link in or compile SOIL (http://lonesock.net/soil.html) into my C++ OpenGL project on Mac OS X? @Adam Luchjenbroers: Actually you can just run make -f makefile and it will work (at least with my current version of make) You can pass any file to make with the -f flag  On newer versions of Mac OS X such as Leopard you'll have to edit the make file and add '-arch 1386 -arch x86_64' to the CXX macro of the Makefile. After compiling you'll also have to link in the CoreFoundation.framework in your project. So your final build command might look something like gcc -Wall -lSOIL -framework OpenGL -framework GLUT -framework CoreFoundation Is this when you compile SOIL or your program? Tried both but still get [file was built for archive which is not the architecture being linked].  There's a makefile in the zip that you could try using (projects/makefile). You'll want to rename makefile to __M__akefile (capital M) then just run make in the projects/makefile directory. You'll also need to create the folder for it to put the compiled objects into From a command line prompt cd <path to unpacked SOIL archive> cd projects/makefile cp makefile Makefile mkdir obj make This builds fine on Linux and should work on OS X provided you have a C compiler installed.
847,A,Is there an easy way to convert an image to grayscale with .NET I need to convert all the images in a folder to greyscale (I believe their gif files if that matters). Any suggestions? I have .NET 3.5 (sp1) check http://msdn.microsoft.com/en-us/library/system.windows.media.imaging.formatconvertedbitmap.aspx Maybe you can do like this guy. Or maybe like so Use the second one...you do need to use weighted values for the colors rather than an even average... the human eye does not take in all colors equally. Erm they are the same. @nobugz they are not the same. The second example shows two different ways to do it. They both use the ordinary 0.3/0.59/0.11-rule to transform RGB into luminance/intensity.  There are different approaches: Average the red green and blue values. Simple but the result probably won't look right unless you weight the values first. Convert the image to Hue Lightness and Saturation values and take the lightness. You can read images using the Image class. There's a method to read the image from file.
848,A,"C/D line drawing package I find my self in need of a line drawing package. I need to pop a window and draw lines and points. Text would be nice but I can live without it. Most importantly I need something that is dirt simple to get running. I don't have time to dink around with libs (if I did have time I'd be willing but I'm already way behind as it is). I'd prefer a D language solution (Windows XP D1.0 Phobos) but I might be able to use anything with C linkage and source. I'd also be able to use an out of process solution as in: generate input file call program. Any ideas? QD. It was made for this. Just import qd link with SDL.lib (SDL_ttf if you want text) then screen(width height) to set up line(x1 y1 x2 y2) to draw a line pset(x1 y1) to draw a point print(x1 y1 Bottom|Right ""text"") to print text. cls to reset flip to update the screen. events() to handle events. Append  rgb(r g b) to any of the above commands to change the line color Fill(rgb(r g b)) to change the fill color. For examples see test*.d Good luck! Where can I get SDL.lib? Check here: http://www.libsdl.org/download-1.2.php  You can use SDL to pop a window and SDL_gfxPrimitves.h from SDL_gfx to draw the lines (it can also render basic text and shapes). It doesn't take much time to set up and is portable. #include <SDL/SDL.h> #include <SDL/SDL_gfxPrimitives.h> main() { SDL_Surface *screen = NULL; if ( SDL_Init(SDL_INIT_VIDEO) < 0 ) exit(EXIT_FAILURE); atexit(SDL_Quit); screen = SDL_SetVideoMode(500 500 32 SDL_SWSURFACE|SDL_ANYFORMAT); if ( screen == NULL ) exit(EXIT_FAILURE); lineColor(screen 50 50 200 200 0xff0000ff); SDL_Flip(screen); sleep(5); }  wxwidgets is among the more featureful and more widely ported GUI toolkits. The toolkit is natively C but there are bindings for plenty of other languages. I don't know if D is among them.  Another alternative is to use Cairo. It has a very easy to learn API is quite powerful and can write PNG PS PDF and SVG out of the box. It also supports drawing to GDI X and Quartz windows. There is an old D binding for cairo (written by some talent-less hack) which might still work. If nothing else it'll demonstrate how to link and use cairo in D.  If you want an out-of-process solution for getting something up and running quickly it's hard to beat generating PostScript and launching a PostScript viewer. The great advantage of this trick is that you generate something you don't like the way it looks you can edit it by hand until it looks better. Then you go back and edit the generator. So your prototyping cycle is very quick. You could also generate SVG and launch a web-browser."
849,A,Java2d: Set gradient for a lines I am having multiple points in a plane and some hundreds of lines pass through those points. Some points can have more lines passing through them than other points. I want to show some kind of more gradient or brightness associated with lines crowded together. Is this possible to do in java2d. Please refer to this : http://ft.ornl.gov/doku/_media/ft/projects/paraxis.jpg Thank you. That picture looks like just a bunch of lines drawn with a low alpha value - in other words using a nearly transparent color. When you have a lot of lines close together they'll overlap and the color will get brighter. So assuming the picture shows the kind of effect you're looking for just use one of the Color constructors with four arguments and specify a low alpha value for the last one (maybe 0.1 as a float or 20-30 as an int) then go crazy drawing your lines ;-) If that doesn't do what you want maybe I misunderstood your question...
850,A,"What is the most elegant way to cast a variable you just received as an argument? More specifically what's the most elegant way to cast the Graphics object in a paint() call up to a Graphics2D object? public void paint(Graphics g) { // How do I convert/cast/etc the g variable to a Graphics2D object? } Or am I missing the point here? Is there a better way to handle this in general? Its not directly relevant but in Scala they recognised this ""unpleasantness"" - and encourage you to use pattern matching to deal with the correct class instead of a cast: g match { case Graphics2D(g2d) => //code goes here ! case _ => Nothing //or throw exception etc } They actively discourage casts (there is no special cast syntax only a method). Not entirely relevant to you but thought its worth noting that others are troubled by what troubles you ;)  Graphics2D g2 = Graphics2D.class.cast(g); Why would you do that?  Personally I like Graphics2D g2 = g as Graphics2D; if (g2 == null) //do stuff But as said before no reason to make it anymore complicated than it is :P Oh it's java .. My bad. I was sleepy :O Can you use 'as' in java? I didn't think so.  Graphics2D g2 = (Graphics2D)g; There is no need to be fancy about it. You will always receive a G2D in the paint method. I even like to call the Graphics parameter g2 and then cast to a Graphics2D named g to make using it simpler. You might want to use g as the Graphics2D variable as it is going to be used throughout the code. You could also have a general component and forward to a renderer using an interface that uses Graphics2D instead of Graphics for parameters. So you always receive one? Why are the arguments in the declaration ""Graphics g"" and there is no ""Graphics2D g"" alternative to override? Internally is a Graphics2D object being cast to a Graphics object for backward compatibility and then you're able to restore it to a Graphics2D if you need to? You are being given a Graphics object for some historical reason that I have grown to not care about. I just always do the cast if I need G2D methods.  As far as I know you can't do much in this situation. You can validate that you actually got a Graphics2D object before casting. But as far as I know you can get anything else anyway."
851,A,"What is the minimum boilerplate code for a OpenGL 2D View? What's the minimum boilerplate code required to setup an OpenGL view (with the necessary projectionscamera angles etc) for drawing a 2D game? For example the minimum required to do Quartz 2D drawing in a custom view (and say load a background image) is the following: #import <Cocoa/Cocoa.h> @interface MyView : NSView { } @end = = = #import ""MyView.h"" @implementation MyView - (id)initWithFrame:(NSRect)frame { self = [super initWithFrame:frame]; return self; } - (void)drawRect:(NSRect)rect { CGContextRef myContext = [[NSGraphicsContext currentContext] graphicsPort]; CGRect frame = CGRectMake(bounds.origin.x bounds.origin.y bounds.size.width bounds.size.height); CFBundleRef mainBundle = CFBundleGetMainBundle(); CFURLRef url = CFBundleCopyResourceURL(mainBundle CFSTR(""background"") CFSTR(""png"") NULL); CGDataProviderRef provider = CGDataProviderCreateWithURL (url); CGImageRef image = CGImageCreateWithPNGDataProvider (provider NULL true kCGRenderingIntentDefault); CGDataProviderRelease (provider); CGContextDrawImage (myContext frame image); CGImageRelease (image); //rest of drawing code here... } @end Would anything in the boilerplate code be different for Open GS ES on the iPhone as opposed to using Open GL on a Mac? Oh and please not recommendations for game libraries cocos2D et al. I want an answer based on OpenGL alone. Would anything in the boilerplate code be different for [OpenGL] ES on the iPhone as opposed to using Open GL on a Mac? Yes. On a Mac you'd use NSOpenGLView which is part of Application Kit. The iPhone doesn't have AppKit. I can't say more for sure because I'm not an iPhone developer but I would guess that you'll use EAGL on the iPhone.  The easiest way to setup an OpenGL application on the iPhone is to create a ""OpenGL ES Application"" through Xcode. It generates the boilerplate source code you'll need to get started. Here is the boilerplate source I am using for an OpenGL iPhone game: @implementation EAGLView @synthesize context; // You must implement this method + (Class)layerClass { return [CAEAGLLayer class]; } //The GL view is stored in the nib file. When it's unarchived it's sent -initWithCoder: - (id)initWithCoder:(NSCoder*)coder { if ((self = [super initWithCoder:coder])) { // Get the layer CAEAGLLayer *eaglLayer = (CAEAGLLayer *)self.layer; eaglLayer.opaque = YES; eaglLayer.drawableProperties = [NSDictionary dictionaryWithObjectsAndKeys: [NSNumber numberWithBool:NO] kEAGLDrawablePropertyRetainedBacking kEAGLColorFormatRGBA8 kEAGLDrawablePropertyColorFormat nil]; context = [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES1]; if (!context || ![EAGLContext setCurrentContext:context]) { [self release]; return nil; } } return self; } - (void)drawView { [EAGLContext setCurrentContext:context]; glBindFramebufferOES(GL_FRAMEBUFFER_OES viewFramebuffer); glViewport(0 0 ScreenWidth ScreenHeight); glMatrixMode(GL_PROJECTION); glLoadIdentity(); // Setup the coordinate system to use 00 as the lower left corner // and 320480 as the upper right corner of the screen (in portrait mode). glOrthof(0.0f ScreenWidth 0.0f ScreenHeight -1.0f 1.0f); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); // Here is where you draw everything in your world. DrawWorld(); glBindRenderbufferOES(GL_RENDERBUFFER_OES viewRenderbuffer); [context presentRenderbuffer:GL_RENDERBUFFER_OES]; } - (void)layoutSubviews { [EAGLContext setCurrentContext:context]; [self destroyFramebuffer]; [self createFramebuffer]; [self drawView]; } - (BOOL)createFramebuffer { glGenFramebuffersOES(1 &viewFramebuffer); glGenRenderbuffersOES(1 &viewRenderbuffer); glBindFramebufferOES(GL_FRAMEBUFFER_OES viewFramebuffer); glBindRenderbufferOES(GL_RENDERBUFFER_OES viewRenderbuffer); [context renderbufferStorage:GL_RENDERBUFFER_OES fromDrawable:(CAEAGLLayer*)self.layer]; glFramebufferRenderbufferOES(GL_FRAMEBUFFER_OES GL_COLOR_ATTACHMENT0_OES GL_RENDERBUFFER_OES viewRenderbuffer); glGetRenderbufferParameterivOES(GL_RENDERBUFFER_OES GL_RENDERBUFFER_WIDTH_OES &ScreenWidth); glGetRenderbufferParameterivOES(GL_RENDERBUFFER_OES GL_RENDERBUFFER_HEIGHT_OES &ScreenHeight); if(glCheckFramebufferStatusOES(GL_FRAMEBUFFER_OES) != GL_FRAMEBUFFER_COMPLETE_OES) { NSLog(@""failed to make complete framebuffer object %x"" glCheckFramebufferStatusOES(GL_FRAMEBUFFER_OES)); return NO; } return YES; } - (void)destroyFramebuffer { glDeleteFramebuffersOES(1 &viewFramebuffer); viewFramebuffer = 0; glDeleteRenderbuffersOES(1 &viewRenderbuffer); viewRenderbuffer = 0; } - (void)dealloc { if ([EAGLContext currentContext] == context) { [EAGLContext setCurrentContext:nil]; } [context release]; [super dealloc]; } @end As an alternative this article provides a nice walkthrough in creating an OpenGL ES application on the iPhone."
852,A,"Flex: Why is line obscured by canvas' background I want MyCanvas to draw lines on itself but they seem to be drawn behind the background. What's going on and how should I do this? Main.mxml <?xml version=""1.0"" encoding=""utf-8""?> <mx:Application xmlns:mx=""http://www.adobe.com/2006/mxml"" xmlns:my=""*""> <my:MyCanvas width=""300"" height=""300"" id=""myCanvas""></my:MyCanvas> <mx:Button label=""Draw"" click=""myCanvas.Draw();""></mx:Button> </mx:Application> MyCanvas.as package { import mx.containers.Canvas; public class MyCanvas extends Canvas { public function MyCanvas() { this.setStyle(""backgroundColor"" ""white""); } public function Draw():void { graphics.lineStyle(1); graphics.moveTo( -10 -10); graphics.lineTo(width + 10 height + 10); } } } Thanks. The Graphics object of the DisplayObject itself is always positioned at the bottom of the DisplayList. In this case it is possible that an overlapping child is added by the superclass perhaps when you apply the white background (not sure I'm not Flex expert). However it may be safer to try to add a specifically dedicated child to draw into. This way you can have more control over its visibility : package { import mx.containers.Canvas; public class MyCanvas extends Canvas { private var shape:Shape; public function MyCanvas() { this.setStyle(""backgroundColor"" ""white""); addChild(shape = new Shape()); } public function Draw():void { shape.graphics.lineStyle(1); shape.graphics.moveTo( -10 -10); shape.graphics.lineTo(width + 10 height + 10); } } } Thanks. Line ""addChild(shape = new Shape());"" needs to be changed to ""rawChildren.addChild(shape = new Shape());"" It should be noted that best practices for the flex3 sdk == adding children in the createChildren() method. override protected function createChildren() let the inherited classes do their thing by 'supering' first (99% of the time) then addChild. Wow! Awesome thanks!"
853,A,"Geometrical Arc to Bezier Curve When drawing an Arc in 2D using a Bezier Curve approximation how does one calculate the two control points given that you have a center point of a circle a start and end angle and a radius? Is it correct to assume that you're trying to approximate a circular arc? Raphael 2.1.0 has support for Arc->Cubic (path2curve-function) and after fixing a bug in S and T path normalization it seems to work now. I updated *the Random Path Generator* so that it generates only arcs so it's easy test all possible path combinations: http://jsbin.com/oqojan/53/ Test and if some path fails I'd be happy to get report. EDIT: Just realized that this is 3 years old thread...  There's Mathematica code at Wolfram MathWorld: Bézier Curve Approximation of an Arc which should get you started. See also: Drawing a circle with Bézier Curves Approximation of Circle Using Cubic Bezier Curve all of the links are not available ... A working link explaining the same thing would be http://pomax.github.io/bezierinfo/#circles_cubic  I've had success with this general solution for any elliptical arc as a cubic Bezier curve. It even includes the start and end angles in the formulation so there's no extra rotation needed (which would be a problem for a non-circular ellipse).  This isn't easily explained in a StackOverflow post particularly since proving it to you will involve a number of detailed steps. However what you're describing is a common question and there's a number of thorough explanations. See here and here; I like #2 very much and have used it before. typo near the end of #2: ""x3=x1"" should be ""x3=x0"". I have a slightly more generic explanation over at http://pomax.github.io/bezierinfo/#circles_cubic which covers the second link's conclusion while explaining what the coordinates would be for any arc length rather than for arcs with angles up to a quarter circle."
854,A,"Does anyone know of a good tutorial for the basics of 3d graphics programming (from scratch not Direct3d or OpenGL) I'm looking for some material on how homogeneous coordinates perspectives and projections work in 3d graphics on a basic level. An approach using programming would be stellar. I've been searching around and my searches are convoluted with OpenGL Direct3d and material more concerned with the mathematical proofs than the actual application. Does anyone know of a place where I could find this information (online access preferred)? Mm I'm assuming you have a good reason for it but it's like asking ""Does anyone know of a good tutorial for the basics of Windows programming (from scratch in assembler not C/MFC/something-a-little-higher-levelled)"". I've found a lot of great 3D graphics programming information on Charles Petzold's book blog. The 3D code is mostly in XAML so it is simple enough to understands the basics without too much overhead.  I'm reading the following two books together right now to learn WPF's 3D graphics model and the underlying math at the same time. Both books are outstanding in my opinion though it may not be what you're looking for: 3D Math Primer for Graphics and Game Development 3D Programming for Windows (this is a WPF 3d book though the title doesn't reflect it)  You probably have to look for pre-OpenGL textbooks like Foley and van Dam's Computer Graphics: Principles and Practice in C (2nd Edition). In particular Ch 11 Representing Curves and Surfaces and Ch 15 Visible-Surface Determination would be relevant but earlier material on how to draw lines and shapes would also be useful if you are truly doing everything from scratch. Something as simple as drawing a line is non-trivial if you think about it.  A good start would be the OpenGL Programming Guide (Red Book). You can read it online (albeit an older edition) and is full of examples (actual working code)."
855,A,Need a way to scale a font to fit a rectangle I just wrote some code to scale a font to fit within (the length of) a rectangle. It starts at 18 width and iterates down until it fits. This seems horribly inefficient but I can't find a non-looping way to do it. This line is for labels in a game grid that scales so I can't see a work-around solution (wrapping cutting off and extending past the rectangle are all unacceptable). It's actually pretty quick I'm doing this for hundreds of rectangles and it's fast enough to just slow it down a touch. If nobody comes up with anything better I'll just load the starting guess from a table (so that it's much closer than 18) and use this--except for the lag it works great. public Font scaleFont(String text Rectangle rect Graphics g Font pFont) { float nextTry=18.0f; Font font=pFont; while(x > 4) { font=g.getFont().deriveFont(nextTry); FontMetrics fm=g.getFontMetrics(font); int width=fm.stringWidth(text); if(width <= rect.width) return font; nextTry*=.9; } return font; } Huh. Turns out Netbeans isn't too fond of using `try` as an identifier... Change all width variables to float instead of int for better result. public static Font scaleFontToFit(String text int width Graphics g Font pFont) { float fontSize = pFont.getSize(); float fWidth = g.getFontMetrics(pFont).stringWidth(text); if(fWidth <= width) return pFont; fontSize = ((float)width / fWidth) * fontSize; return pFont.deriveFont(fontSize); }  You can improve the efficiency using a binary search pattern - high/low with some granularity - either 1 0.5 or 0.25 points. For example guess at 18 too high? Move to 9 too Low? 13.5 too low? 15.75 too high? 14!  A different obvious way would be to have the text pre-drawn on a bitmap and then shrink the bitmap to fit the rectangle; but because of hand-crafted font design and hinting etc. finding the right font size produces the best-looking (although perhaps not the quickest) result.  private Font scaleFont ( String text Rectangle rect Graphics gc ) { final float fMinimumFont = 0.1f; float fMaximumFont = 1000f; /* Use Point2d.Float to hold ( font width of font in pixels ) pairs. */ Point2D.Float lowerPoint = new Point2D.Float ( fMinimumFont getWidthInPixelsOfString ( text fMinimumFont gc ) ); Point2D.Float upperPoint = new Point2D.Float ( fMaximumFont getWidthInPixelsOfString ( text fMaximumFont gc ) ); Point2D.Float midPoint = new Point2D.Float (); for ( int i = 0; i < 50; i++ ) { float middleFont = ( lowerPoint.x + upperPoint.x ) / 2; midPoint.setLocation ( middleFont getWidthInPixelsOfString ( text middleFont gc ) ); if ( midPoint.y >= rect.getWidth () * .95 && midPoint.y <= rect.getWidth () ) break; else if ( midPoint.y < rect.getWidth () ) lowerPoint.setLocation ( midPoint ); else if ( midPoint.y > rect.getWidth () ) upperPoint.setLocation ( midPoint ); } fMaximumFont = midPoint.x; Font font = gc.getFont ().deriveFont ( fMaximumFont ); /* Now use Point2d.Float to hold ( font height of font in pixels ) pairs. */ lowerPoint.setLocation ( fMinimumFont getHeightInPixelsOfString ( text fMinimumFont gc ) ); upperPoint.setLocation ( fMaximumFont getHeightInPixelsOfString ( text fMaximumFont gc ) ); if ( upperPoint.y < rect.getHeight () ) return font; for ( int i = 0; i < 50; i++ ) { float middleFont = ( lowerPoint.x + upperPoint.x ) / 2; midPoint.setLocation ( middleFont getHeightInPixelsOfString ( text middleFont gc ) ); if ( midPoint.y >= rect.getHeight () * .95 && midPoint.y <= rect.getHeight () ) break; else if ( midPoint.y < rect.getHeight () ) lowerPoint.setLocation ( midPoint ); else if ( midPoint.y > rect.getHeight () ) upperPoint.setLocation ( midPoint ); } fMaximumFont = midPoint.x; font = gc.getFont ().deriveFont ( fMaximumFont ); return font; } private float getWidthInPixelsOfString ( String str float fontSize Graphics gc ) { Font font = gc.getFont ().deriveFont ( fontSize ); return getWidthInPixelsOfString ( str font gc ); } private float getWidthInPixelsOfString ( String str Font font Graphics gc ) { FontMetrics fm = gc.getFontMetrics ( font ); int nWidthInPixelsOfCurrentFont = fm.stringWidth ( str ); return (float) nWidthInPixelsOfCurrentFont; } private float getHeightInPixelsOfString ( String string float fontSize Graphics gc ) { Font font = gc.getFont ().deriveFont ( fontSize ); return getHeightInPixelsOfString ( string font gc ); } private float getHeightInPixelsOfString ( String string Font font Graphics gc ) { FontMetrics metrics = gc.getFontMetrics ( font ); int nHeightInPixelsOfCurrentFont = (int) metrics.getStringBounds ( string gc ).getHeight () - metrics.getDescent () - metrics.getLeading (); return (float) nHeightInPixelsOfCurrentFont * .75f; }  You could use interpolation search: public static Font scaleFont(String text Rectangle rect Graphics g Font pFont) { float min=0.1f; float max=72f; float size=18.0f; Font font=pFont; while(max - min <= 0.1) { font = g.getFont().deriveFont(size); FontMetrics fm = g.getFontMetrics(font); int width = fm.stringWidth(text); if (width == rect.width) { return font; } else { if (width < rect.width) { min = size; } else { max = size; } size = Math.min(max Math.max(min size * (float)rect.width / (float)width)); } } return font; }  semi-psuedo code: public Font scaleFont(String text Rectangle rect Graphics g Font pFont) { float fontSize = 20.0f; Font font = pFont; font = g.getFont().deriveFont(fontSize); int width = g.getFontMetrics(font).stringWidth(text); fontSize = (rect.width / width ) * fontSize; return g.getFont().deriveFont(fontSize); } i'm not sure why you pass in pFont as it's not used... Good answer. If I'm understanding your code it reminds me of a Newton-Raphson approximation. Maybe do that twice instead of once to be more accurate as there may be some rounding error. Yeah I thought about the fact that it mightn't scale like that. I figured that if you wanted it to be the size of a rectangle without it being within a bounding box then this would be fine. Otherwise your testing would show if it's good enough and you could then add a short iteration loop to fix it. Good catch with the pFont--threw that in at the last second after scooping this code out of another method and didn't give it much thought. This looks tempting! I'm not totally sure I'd get the right size (not sure font sizes grow linearly) but if nothing else it will make an AWESOME first guess.
856,A,What to use when text comments just wont do? I'm writing a graphics application that's maths and geometry intensive. Frequently I find that although I comment my code as best as I can after sometime of working on some other part of the app the previous comments become are hard to understand when I revisit it (usually for debugging). I'm starting to think that there are cases where text comments just won't do and diagrams are required -- lots of them. I suspect this problem is not new and I would like to know how what techniques other experienced developers/teams use. I'm getting to the point many of my methods probably require at least 2-pages of notes and diagrams to describe. For example one method that I debugged yesterday handles five different ways by which a line can intersect a triangle edge. I've commented the method as best as I can but I'm worried that my well-crafted comments won't make sense in 4 months when I have to debug the method. Thanks Olumide It is not uncommon to include a readme file or other type of help file with your program. For example those other types could be PDFs that include diagrams. You can annotate your code to refer to the included help files.  If the algorithm is described somewhere on the Web include a URL in the comments to supplement the comments in the code. And there's always ASCII art!  In such cases I refer to an external document (pdf most likely) which describes the problem with graphics. I most likely use the <see> tag in the comment section (.NET world). See http://msdn.microsoft.com/en-us/library/acd0tfbe.aspx This is exactly what I do too. works exceedingly well for pointing to the diagrams or design notes.
857,A,"PDF renderer for iPhone app Mayb do you know a good open-source pdf renderer for objective-c? :) Why not use UIWebView which is capable of displaying PDF? Have a look at GhostScript Not sure if this can be used on the iphone though and you may need to check the licensing for redistribution.  If you're looking to draw into a PDF take a look at the section on PDF Document Creation Viewing and Transforming in the Quartz 2D Guide.  Core Graphics provides a lot of native PDF-rendering functionality. If you just want to provide a view that lets the user pan and zoom around a PDF use a UIWebView; all you have to do is give it the URL to a PDF inside your bundle as in NSString *resourcePath = [[NSBundle mainBundle] pathForResource:@""my_file"" ofType:@""pdf""]; NSURL *url = [NSURL URLWithString:resourcePath]; [webView loadRequest:[NSURLRequest requestWithURL:url]]; That should not be NSString * url it should be NSURL * url. Thanks David; revised."
858,A,AffineTransform linear interpolation Given 2 java AffineTransform items how can I interpolate between them. I need the image on screen to slowly move from the position/rotation/scale with one matrix applied to the other. Preferably this should be reasonably efficient since it's running every time a game draws. My current (really hacky) solution is to getTranslate() from both matrices lerp between them and then create a new matrix (This doesn't work fully since there is no equivalent for rotation) Get the affine matrixes of each transform via getMatrix(). Step through the interpolation of one matrix to the other creating a new transform via AffineTransform(float[] matrix) at each step. You could also decompose the two transformations into the same translate/rotate/scale/shear sequence and morph each subtransformation separately. The subtransformations would be combined in each step to form the current AffineTransformation -- resulting in a less puzzling visual transformation. Are you getting 4 or 6 values in the getMatrix() method? If 6 are the last 3 elements of both matricies {0 0 1}? How many values are you giving to the AffineTransform constructor? Hey I accepted this but after implemented it noticed something odd. When the camera is lerping from one matrix to the other it scales everything to be smaller. This actually looks cool so it's not a problem but any ideas why it does that? Because your interpolated transform is likely no longer affine...
859,A,Register level programming of GMA 950 hardware I'm trying to write a basic driver for the GMA 950 hardware. I've been looking for a datasheet or some programming guide but cannot find anything. I've also looked at the Linux and FreeBSD source but they are quite large and will take time to understand. The GMA 950 is associated with an Intel 945 Express chipset. Does anyone know of good documentation that I can write hardware register level code to for the 950 and/or 945? Thanks FM I'm not sure if this is exactly what your looking for as it looks like it is just for i965 but the Intel Linux Graphics Documentation might help. Excellent! Perhaps the 965 is a superset of the 945? I'll do some poking around with the registers defined in this document and see what I find.
860,A,How can I find the intersecting point of three planes? I'm trying to build render the raw data from a Quake 3 .map file with Java. The .map format stores brush (shape) information as a series of planes but I'd like to convert them to points of the form (xyz). The math involved is a bit beyond me at this point so does anyone have any advice on how I can do this? I'm fine with using external libraries if need be but I'd rather use my own code if possible. Some data to play with: Dimensions: 64*64*64 Position: All sides are equidistant from the origin (000) Shape: Cube ( 64 64 64 ) ( 64 -64 64 ) ( -64 64 64 ) ( 64 64 64 ) ( -64 64 64 ) ( 64 64 -64 ) ( 64 64 64 ) ( 64 64 -64 ) ( 64 -64 64 ) ( -64 -64 -64 ) ( 64 -64 -64 ) ( -64 64 -64 ) ( -64 -64 -64 ) ( -64 -64 64 ) ( 64 -64 -64 ) ( -64 -64 -64 ) ( -64 64 -64 ) ( -64 -64 64 ) Edit: This data is showing a cube which has 6 sides. Each of these six sides can be stored as a plane with three coordinates to show the position and orientation. Each row of the above data shows one plane and all 6 of the rows make the 6 planes that make up a cube. This image helps illustrate my point: Three Points Defining a Plane Points p1 p2 and p3 are what I'm giving per row in the above data. The Quake 3 engine has to sort out what parts of the planes to keep at compile time but I'm not interested in that at the moment. If you would like more information feel free to ask! Can you please explain how your data represents three planes and not 18 vectors? Of course. I've edited my question. I go to Wolfram Mathworld whenever I have questions like this. For this problem try this page: Plane-Plane Intersection Equation 8 on that page gives the intersection of three planes. To use it you first need to find unit normals for the planes. This is easy: given three points a b and c on the plane (that's what you've got right?) take the cross product of (a-b) and (a-c) to get a normal then divide it by its own magnitude to get a unit normal. That helped! Thanks a lot!
861,A,Plotting a Decimal valued Points in Java I am Creating a Graph Plotting Software in java. As a part of process if the equation is of form of y = Sin(x) * Cos(x) then in that case the value of y is a decimal valued points. But in Java we are allowed to plot only integer points. Is there any way to plot points in decimal format using any package ??? The Another strategy that can be used is to convert the decimal value into corresponding integer value. But that algorithm won't give me a smooth curve ... Thanx in Advance... What package are you currently using to do the plotting. There is *nothing* in java that would make integers-only a hard and fast rule of plotting but maybe there's a framework that has that constraint. No one can help unless you indicate what tool you're using. Why are you only allowed to plot integer points? It is possible to plot points at non integer positions. You may use Point2D.Double or Point2D.Float. See http://java.sun.com/docs/books/tutorial/2d/geometry/primitives.html for further details.  I suppose the integers x y for a plot are the pixels at which they should go. If you can't get a nice curve by scaling your values you should probably think of creating a larger image in memory ( perhaps twice the y scale) and paint on that en then scaling that image back to your GUI using getScaledInstanceSampleModel.getScaledInstance on your buffered image with the hint SCALE_SMOOTH. Though what Pierre said is possibly better.  If your using the Graphics class to draw your picture use a XXXX2D class rather than XXXX class with a Graphics2D. For example: Graphics2D g=(Graphics2D)g1; g.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON); g.draw(new Line2D.Double(1.0001.2278.980983.098)); (...)
862,A,How to validate image file format in C# Does anyone know the script to validate what the file format is for a given image. Currently i am populating an image object looking at it's height width and resolution. I don't see any specific properties off of this object that explains the file format. I would like to check for jpg AI PSD High Jes Jpg Bitmap and Tiff. here is my current script:  protected bool IsValidImage(HttpPostedFileBase file string fileName) { //verify that the image is no more than 648 wide and 648 pixels tall Image imgPhoto = Image.FromStream(file.InputStream); if (imgPhoto.Width > 648) return false; if (imgPhoto.Height > 648) return false; if (imgPhoto.HorizontalResolution != 72 || imgPhoto.VerticalResolution != 72) return false; return true; } Thanks in advance Well if you've got the filename why not just check the extension? The Image class won't be able to handle AI or PSD... @Thomas - I believe if you have the AI or PSD codecs installed then Image will be able to tell you by using the ImageFormat GUID. What about: bool isJpeg = imgPhoto.RawFormat.Equals(Imaging.ImageFormat.Jpeg);  You can visit Wotsit to find out the magic bytes used as a marker in the beginning of the file. Click on the 'Graphics File' to see the list of file formats.. CodeKaizen's answer is pretty good...+1 from me...did not know about that...Thanks! :)  Use Image.RawFormat. The result is an instance of the ImageFormat class which can be compared against the static properties of ImageFormat. See http://msdn.microsoft.com/en-us/library/system.drawing.imaging.imageformat.aspx for more details. How can you tell from the ImageFormat GUID that the image type is PSD or AI? BTW thank you for your answer. It was right on. Just need to test the PSD and AI formats. @Billy - have you considered using Windows Imaging Components (WIC). I'm more positive that you can use codecs to decode a file format in that case and WIC is server-friendly whereas GDI is not. No i am not extremely familar with GDI or imaging objects in the first place. Don't have to deal with them much. Will check it out though. Thank you for your quick responses.  public bool validateImage(byte[] bytes) { try { Stream stream = new MemoryStream(bytes); using(Image img = Image.FromStream(stream)) { if (img.RawFormat.Equals(ImageFormat.Bmp) || img.RawFormat.Equals(ImageFormat.Gif) || img.RawFormat.Equals(ImageFormat.Jpeg) || img.RawFormat.Equals(ImageFormat.Png)) return true; } return false; } catch { return false; } }
863,A,"Fonts in R plots What graphics devices let me use system fonts for text within charts? The base graphics system only has a small amount of documentation around the par(family=...) options. Ideally I'd like to be able to use any font I can browse through a tool like xfontsel on Linux or the equivalent utilities on other platforms. My current solution is to plot out as PDF and then use a 3rd party program to replace the fonts from within the PDF. This is not ideal. I was curious about this as well so after a little digging I found this blog post on the topic: http://www.jameskeirstead.ca/general/changing-the-fonts-in-r-plots/ It's not as easy as selecting it from a font browsing tool but it seems to work for any font you have on your system. I hope that helps.  I just worked on this issue this morning. I found that you can get a list of fonts available to the pdf() command like this: > names(pdfFonts()) [1] ""serif"" ""sans"" ""mono"" [4] ""AvantGarde"" ""Bookman"" ""Courier"" [7] ""Helvetica"" ""Helvetica-Narrow"" ""NewCenturySchoolbook"" [10] ""Palatino"" ""Times"" ""URWGothic"" ... etc ... So I then went about my business with this: > pdf(file=""plot.pdf""family=""Palatino"" pointsize=16 width=16height=10)  You can use system fonts with cairo_pdf. On Ubuntu (and many other types of Linux I guess) the family argument takes any font name you see in fc-list. Alternatively you can use the extrafont package. This will allow you to use any system font with the regular pdf device."
864,A,Font in 'GraphicsPath.AddString' is smaller than usual font For some reason if I add a string to GraphicsPath using AddString the font is going to be smaller than it looks like in the Font Dialog.  SizeF sz = g.MeasureString(Text new Font(Font.FontFamily (int)(Font.Size - (Font.Size / 7)) Font.Style) new PointF(0 0) StringFormat.GenericDefault); this.Size = new Size((int)sz.Width (int)sz.Height); //These are not the same fontpath.AddString(this.Text this.Font.FontFamily(int)this.Font.Style this.Font.Size new Point(0 0)StringFormat.GenericDefault); Does anyone know why it might be doing that? Assuming your Font.Size's unit is Point you should convert the size that you passed to AddString to emSize (The height of the em square box that bounds the character). float emSize = graphics.DpiY * font.Size / 72;
865,A,"Writing BMP image in pure c/c++ without other libraries In my algorithm i need create information output. I must to write boolean matrix in bmp file. It must be monocromic image where pixel is white if matrix on such element is true. Main problem is bmp header and how to write this. You can check http://qdbmp.sourceforge.net/ for implementation details :). Maybe of use to visitors Googling similar concepts is my almost related question and answer here: http://stackoverflow.com/questions/17918978/plot-an-array-into-bitmap-in-c-c-for-thermal-printer Note that the lines are saved from down to up and not the other way around. Additionally the scanlines must have a byte-length of multiples of four you should insert fill bytes at the end of the lines to ensure this. I undestand nothing that you wrote. Don't worry you will once you've actually tried to implement this. @Ben it's easy to miss the multiple-of-four requirement in testing since most real world images are already a multiple of 4 wide. @mark: A monochrome image has to be a multiple of 32 since the alignment requirement is in bytes not pixels.  Here is a C++ variant of the code that works for me. Note I had to change the size computation to account for the line padding. // mimeType = ""image/bmp""; unsigned char file[14] = { 'B''M' // magic 0000 // size in bytes 00 // app data 00 // app data 40+14000 // start of data offset }; unsigned char info[40] = { 40000 // info hd size 0000 // width 0000 // heigth 10 // number color planes 240 // bits per pixel 0000 // compression is none 0000 // image bits size 0x130x0B00 // horz resoluition in pixel / m 0x130x0B00 // vert resolutions (0x03C3 = 96 dpi 0x0B13 = 72 dpi) 0000 // #colors in pallete 0000 // #important colors }; int w=waterfallWidth; int h=waterfallHeight; int padSize = (4-w%4)%4; int sizeData = w*h*3 + h*padSize; int sizeAll = sizeData + sizeof(file) + sizeof(info); file[ 2] = (unsigned char)( sizeAll ); file[ 3] = (unsigned char)( sizeAll>> 8); file[ 4] = (unsigned char)( sizeAll>>16); file[ 5] = (unsigned char)( sizeAll>>24); info[ 4] = (unsigned char)( w ); info[ 5] = (unsigned char)( w>> 8); info[ 6] = (unsigned char)( w>>16); info[ 7] = (unsigned char)( w>>24); info[ 8] = (unsigned char)( h ); info[ 9] = (unsigned char)( h>> 8); info[10] = (unsigned char)( h>>16); info[11] = (unsigned char)( h>>24); info[24] = (unsigned char)( sizeData ); info[25] = (unsigned char)( sizeData>> 8); info[26] = (unsigned char)( sizeData>>16); info[27] = (unsigned char)( sizeData>>24); stream.write( (char*)file sizeof(file) ); stream.write( (char*)info sizeof(info) ); unsigned char pad[3] = {000}; for ( int y=0; y<h; y++ ) { for ( int x=0; x<w; x++ ) { long red = lround( 255.0 * waterfall[x][y] ); if ( red < 0 ) red=0; if ( red > 255 ) red=255; long green = red; long blue = red; unsigned char pixel[3]; pixel[0] = blue; pixel[1] = green; pixel[2] = red; stream.write( (char*)pixel 3 ); } stream.write( (char*)pad padSize ); } The padsize does not seem right; I believe it should be: `int padSize = (4 - 3 * w % 4) % 4;` what is waterfall[][] here?  Without the use of any other library you can look at the BMP file format. I've implemented it in the past and it can be done without too much work. Bitmap-File Structures Each bitmap file contains a bitmap-file header a bitmap-information header a color table and an array of bytes that defines the bitmap bits. The file has the following form: BITMAPFILEHEADER bmfh; BITMAPINFOHEADER bmih; RGBQUAD aColors[]; BYTE aBitmapBits[]; ... see the file format for more details no it isn't. Some hardware support both and the OS can determine which endianess to use. Thanks. It seems too good description. Remember to write everything out little-endian if your processor isn't x86. I have x86 thanks.  See if this works for you... In this code i had 3 2-dimensional arrays called redgreen and blue. Each one was of size [width][height] and each element corresponded to a pixel - I hope this makes sense! FILE *f; unsigned char *img = NULL; int filesize = 54 + 3*w*h; //w is your image width h is image height both int if( img ) free( img ); img = (unsigned char *)malloc(3*w*h); memset(img0sizeof(img)); for(int i=0; i<w; i++) { for(int j=0; j<h; j++) { x=i; y=(yres-1)-j; r = red[i][j]*255; g = green[i][j]*255; b = blue[i][j]*255; if (r > 255) r=255; if (g > 255) g=255; if (b > 255) b=255; img[(x+y*w)*3+2] = (unsigned char)(r); img[(x+y*w)*3+1] = (unsigned char)(g); img[(x+y*w)*3+0] = (unsigned char)(b); } } unsigned char bmpfileheader[14] = {'B''M' 0000 00 00 54000}; unsigned char bmpinfoheader[40] = {40000 0000 0000 10 240}; unsigned char bmppad[3] = {000}; bmpfileheader[ 2] = (unsigned char)(filesize ); bmpfileheader[ 3] = (unsigned char)(filesize>> 8); bmpfileheader[ 4] = (unsigned char)(filesize>>16); bmpfileheader[ 5] = (unsigned char)(filesize>>24); bmpinfoheader[ 4] = (unsigned char)( w ); bmpinfoheader[ 5] = (unsigned char)( w>> 8); bmpinfoheader[ 6] = (unsigned char)( w>>16); bmpinfoheader[ 7] = (unsigned char)( w>>24); bmpinfoheader[ 8] = (unsigned char)( h ); bmpinfoheader[ 9] = (unsigned char)( h>> 8); bmpinfoheader[10] = (unsigned char)( h>>16); bmpinfoheader[11] = (unsigned char)( h>>24); f = fopen(""img.bmp""""wb""); fwrite(bmpfileheader114f); fwrite(bmpinfoheader140f); for(i=0; i<h; i++) { fwrite(img+(w*(h-i-1)*3)3wf); fwrite(bmppad1(4-(w*3)%4)%4f); } fclose(f); Hmm comments wouldn't hurt. True but the whole code is self explanatory. And it works. Just put it inside `main()` function and initialize all variables. And Done! ;) Can anyone tell what is `yres` in the code ?? I think this code doesn't take into account the padding in the BITMAPFILEHEADER.bfSize so while loading you have to add the padding to the bfSize if you use it to calculate array for bitmap data. I meant ""calculate array size"". I think yres should be replaced with h"
866,A,.NET's equivalent of Java's MemoryImageSource I found a wonderful open source Java program that I'm translating into C#. The built-in translator in Visual Studio got me started and I've now spent about a month translating the rest manually line by line. I've completed over 15000 lines of translation and the only thing that remains is trying to figure out how to convert their MemoryImageSource stuff into C#/.NET. What's the .NET equivalent way of implementing this stuff? Is there a native .NET library already? There's nothing about the Java library proper that I care for. There is just a program that happens to be be written in Java that I really dig. Just curious what's the java library that impressed you so much? `MemoryImageSource` itself isn't used much nowadays since the more versatile `BufferedImage` was introduced in Java 1.2. Look at the System.Drawing namespace. This wraps the standard windows GDI+ native code. System.Drawing.BitMap is probably what you want.  The standard .NET System.Drawing.Bitmap class in the System.Drawing.dll assembly comes close to what I know of the MemoryImageSource (i.e. an image represented as an array of pixels that you can manipulate). For better performance you can use the ImageTraverser class from CodeProject which accessses the array through unmanaged pointers.
867,A,Graphics library used by Windows Vista Freecell and Solitaire What graphics library is used to create the graphics in the Solitaire and Freecell games included with Windows Vista (e.g. XNA GDI WPF)? A good answer would include the name of the library and evidence. I looked at solitaire.exe with dependency walker and it shows many calls to gdi32.dll and gdiplus.dll but also a call to Direct3DCreate9 in d3d9.dll. It uses Direct3D 9 via the C++ COM interfaces. You have all the evidence you need in the call to Direct3DCreate9; that is all that the game requires to get an IDirect3D9* interface at which point it can create a device interface etc.
868,A,"'D3DRS_SEPARATEDESTALPHAENABLE' : undeclared identifier - even though it's mentioned in the DirectX comments? In d3d9types.h in the _D3DRENDERSTATETYPE struct the last 3 types are: D3DRS_SRCBLENDALPHA = 207 /* SRC blend factor for the alpha channel when D3DRS_SEPARATEDESTALPHAENABLE is TRUE */ D3DRS_DESTBLENDALPHA = 208 /* DST blend factor for the alpha channel when D3DRS_SEPARATEDESTALPHAENABLE is TRUE */ D3DRS_BLENDOPALPHA = 209 /* Blending operation for the alpha channel when D3DRS_SEPARATEDESTALPHAENABLE is TRUE */ Notice it mentions that these will be used if 'D3DRS_SEPARATEDESTALPHAENABLE is TRUE' however there is no D3DRS_SEPARATEDESTALPHAENABLE in the struct whatsoever. The closest thing seems to be: ""D3DRS_SEPARATEALPHABLENDENABLE"" but I'm not at all sure if this is the same thing. So i was just wondering what should be set to true for those last three renderstates to actually work (if anything?) I strongly think it's D3DRS_SEPARATEALPHABLENDENABLE but would like someone to please confirm? Yep D3DRS_SEPARATEALPHABLENDENABLE. Looks like a typo in the comments. From the DXSDK: D3DRS_SRCBLENDALPHA One member of the D3DBLEND enumerated type. This value is ignored unless D3DRS_SEPARATEALPHABLENDENABLE is true. The default value is D3DBLEND_ONE. D3DRS_DESTBLENDALPHA One member of the D3DBLEND enumerated type. This value is ignored unless D3DRS_SEPARATEALPHABLENDENABLE is true. The default value is D3DBLEND_ZERO. D3DRS_BLENDOPALPHA Value used to select the arithmetic operation applied to separate alpha blending when the render state D3DRS_SEPARATEALPHABLENDENABLE is set to TRUE. Valid values are defined by the D3DBLENDOP enumerated type. The default value is D3DBLENDOP_ADD. If the D3DPMISCCAPS_BLENDOP device capability is not supported then D3DBLENDOP_ADD is performed. See D3DPMISCCAPS."
869,A,"Generate a polygon from line I want to draw a line with thickness in j2me. This can easily be achieved in desktop java by setting Pen width as thickness value. However in j2me Pen class does not support width. My idea is to generate a polygon from the line I have that resembles the line with thickness i want to draw. In picture on the left is what I have a line with points. On the right is what I want a polygon that when filled a line with thickness. Could anyone know how to generate a polygon from line? I didn't get you question.What you exactly want??? I want to convert Point[] that make up the line in picture (left) into Point[] that make up polygon in the picture (right). That polygon can actually be a line with width when draw filled. For j2me that don't support drawing lines with a width (or weight). Do you want a line with a specific width? I am not sure how do we call it as a width or a weight. Sorry for confused question. I want a line with specific thickness as a polygon. Dude the thing you want is very easy to do but it is kinda hard to explain (should be done on paper). You just need to find perpendiculars at the points of line. I.e. at ends of line segments. Using a point-length perpendicular you can easily calulcate offsets for points to be used in polygon segments. Just get a sheet of paper pencil and ruler and see how you would do it and try to understand math behind it. Ah if you're preprocessing that should make your life easier. Here's a little code I whipped up using Graphics2D (using J2SE). I don't like that the output includes extra interior segments but when it's filled it looks pretty good. import java.awt.BasicStroke; import java.awt.Shape; import java.awt.geom.Path2D; import java.awt.geom.PathIterator; public class StrokePath { public static void main(String[] args) { // set line width to 6 use bevel for line joins BasicStroke bs = new BasicStroke(6.0f BasicStroke.CAP_SQUARE BasicStroke.JOIN_BEVEL); // create a path for the input Path2D p = new Path2D.Float(); p.moveTo(50.0 50.0); p.lineTo(65.0 100.0); p.lineTo(70.0 60.0); p.lineTo(120.0 65.0); p.lineTo(40.0 200.0); // create outline of wide lines by stroking the path with the stroke Shape s = bs.createStrokedShape(p); // output each of the segments of the stroked path for the output polygon PathIterator pi = s.getPathIterator(null); while (!pi.isDone()) { pi.next(); double[] coords = new double[6]; int type = pi.currentSegment(coords); switch (type) { case PathIterator.SEG_LINETO: System.out.println(String.format(""SEG_LINETO %f%f"" coords[0] coords[1])); break; case PathIterator.SEG_CLOSE: System.out.println(""SEG_CLOSE""); break; case PathIterator.SEG_MOVETO: System.out.println(String.format(""SEG_MOVETO %f%f"" coords[0] coords[1])); break; default: System.out.println(""*** More complicated than LINETO... Maybe should use FlatteningPathIterator? ***""); break; } } } } Here are my results after rendering these coordinates: Glad we got it! This is great. You knew exactly what I wanted. thanks ;)  This will draw a line: Graphics g = this.CreateGraphics(); Pen pen = new Pen(Color.Black 2); //black Width 2 pen.DashStyle = System.Drawing.Drawing2D.DashStyle.Solid; g.DrawLine(pen point1 point2); //point1 and point2 are instances of Point class So i guess if you have an array of points you could do something like for(int i=0;i<myPoints.Length-1;i++) g.DrawLine(penmyPoints[i]myPoints[i+1]); question is not very clear... hope this helps Yes @Ram it is as you said and as @m0s' code. But in j2me Pen doesn't support second width argument making me unable to use @m0s' code. This perfectly draw the intended result in C# I knew. I would like to do the same thing in j2me which does not support Pen width. So I'm trying to create a polygon out of a line to draw a line with thickness. @mOs: I think this will help but it will draw line only above/ below the original line. I guess VOX need the thick line equally on both the sides of the original route. So I guess if he want a line of thickness 10 he want thickness of 5 above original line and thickness of 5 below original line. @VOX now I see your problem... unfortunately I have no idea about j2me programming... however here is a suggestion... I am sure you can draw triangles in j2me... So index the points in zigzag manner then go over them in a loop and draw triangles for each next 3 points performance might be horrible but it will do the trick. Thanks for the trick m0s however those little things that run j2me have horrible performance. With your trick I could probably have to wait a long time to finish rendering. I'm drawing a map. Even if I found a way to generate polygons I'll have to pre-generate them on desktop. So any ideas?  Unfortunately the general algorithm for this is pretty complicated and if you're drawing a map you probably need the general algorithm... The TinyLine library looks promising if you can spend the money see for example: http://www.tinyline.com/2d/download/guide/gstate.html#joinStyle Chances are this will help you with other things you'll want to do in drawing your map too. I haven't used the library (or done any J2ME programming frankly) but if it does what it claims it seems likely to be worth the money. TinyLine is too expensive to effort at the moment and it costs a lot of time to draw a map with that. My idea is to preprocess these lines into polygons and save them. Then the running j2me app just need to read those saved polygons without any processing and draw directly on screen.  I guess I'll draw parallel lines beside the line and connect them as a polygon. Than fill. To draw a parallel line http://stackoverflow.com/questions/2825412/draw-a-parallel-line. It seems to me that if you essentially turn each segment of the line into a rectangle of the same length and whatever thickness/width you're aiming for that you will have ugly corners. Exactly I'm now having ugly corners. Any pointer to round those corners?"
870,A,"c# - clear surface when resizing I'm trying to build my own custom control for a windows forms application in C#.Net. Currently I paint some rectangles and other graphic elements using the paint event. When I now resize the app form to fit the desktop size all elements are repainted (which is exactly the behaviour I need) but the old one's are shown in the background. Here's what I'm doing by now: Pen penDefaultBorder = new Pen(Color.Wheat 1); int margin = 5; private void CustomControl_Paint(object sender PaintEventArgs e) { CustomControl calendar = (CustomControl)sender; Graphics graphics = e.Graphics; graphics.Clear(Color.WhiteSmoke); graphics.DrawRectangle(penDefaultBorder margin margin calendar.Width - margin * 2 calendar.Height - margin * 2); //... } Neither the graphics.Clear nor adding a graphics.FillRectangle(...) will hide the old rectangle from the surface. Ideas? Thank you all. Have you tried .Invalidate() to cause the form to redraw? This will work; it's a bit of a shotgun approach but it will work. I still prefer to render the buffer only when changes are required and copy the regions requested in the OnPaint. Hmm - this was easy ;-) Thanks for your help now the resizing works as expected.  Paint events usually don't request an update for the entire canvas just the area specified in the PaintEventArgs. I'm guessing what's happening is that only the newly-exposed regions of the canvas are being passed in the PaintEventArgs. This one of the reasons that you shouldn't do any rendering in the Paint event. You should render to an offscreen bitmap - a buffer - and copy from that buffer to the control's canvas in the Paint event. Searching for ""double buffering"" here or on Google will give you many examples of the technique. Thanks for your hint - I found some interesting articles concerning double buffering. I'll have a look at those."
871,A,"What pixel format does X server use? What pixel format (RGBA ARBG BGRA) does the X server use? If any specific format at all. Update: I'm specifically looking for information about the color component order and bit patterns. You mean the framebuffer format? Or all the supported pixmap formats? Whichever it is it depends on your graphics hardware driver and configuration. To see all the formats supported on your X server (they're called ""visuals"" in X lingo) try the xdpyinfo utility. You will probably see lots of separate but identically looking visuals. They differ in additional data associated with each pixel besides the RGB values (alpha depth stencil multisample etc.). This can be shown by the glxinfo utility. That all being said the most popular format on contemporary PC hardware is the windowsish 8-bit per channel 32-bit per pixel BGRX or BGRA (X stands for unused). Note that this is byte order not ""logical word"" order which on a little-endian architecture like x86 would be the opposite (XRGB ARGB).  Use xdpyinfo to find information about an X server.  That's a difficult question to answer as you don't say explicitly what kind of a graphical interface you are running on since X is the bare-bones X driver behind the GUI such as GNOME KDE WindowMaker fluxbox NextStep to name a few I know for certain that KDE's interface uses compiz to communicate with the X driver interface to draw on the screen and perform eye-candy movements...but the resolution of X is still the same regardless..8 bits 16 bits 24 bits 32 bits pixel format depending on your screen resolution and how X is configured. To clue you in on the graphics display look for /etc/X11/xorg.conf or similar to find out .... you can usually hit 'Ctrl' 'Alt' '+' to increase resolution or use a '-' to decrease resolution (bear in mind that you need to hold down the Ctrl+Alt and hit either the + or - key to achieve the resolution where warranted...) What is wrong with this answer? The SO did not specify graphics card/resolution etc...we're not mind readers...."
872,A,"Is it possible to break axis labels into 2 lines in base graphics? I am trying to have the x-axis labels to be split into two lines. I would also like the labels to be rotated 45 degrees. How can I do this? What I have so far: N <- 10 dnow <- data.frame(x=1:N y=runif(N) labels=paste(""This is observation ""1:N)) with(dnow plot(xy xaxt=""n"" xlab="""")) atn <- seq(1N3) axis(1 at=atn labels=labels[atn]) What do you mean 2 lines? Do you mean you want ""This is\n observation ...""? @chis_dubois That's the first part of the answer! Thanks! This is what I cooked up (before my ggplot2 days) using base graphics: ## data N <- 10 dnow <- data.frame(x=1:N y=runif(N) labels=paste(""This is \nobservation ""1:N)) ## make margins wide par(mfrow=c(11) mar=c(101064)) ## plot without axix labels or ticks with(dnow plot(xy xaxt=""n"" xlab="""")) ## the positions we ant to plot atn <- seq(1N3) ## the label for these positions lab <- dnow$labels[atn] ## plot the axis but do not plot labels axis(1 at=atn labels=FALSE) ## plot labels text(atn ## x position par(""usr"")[3]-.05 ## position of the low axis srt=45 ## angle labels=lab ##labels xpd=TRUE ## allows plotting outside the region pos=2) ## par(""usr"")[3] I think this is also a important contribution. With `ggplot2` probably this is not required. But it is good to have too. Also a better and elegant solution with `grid` is given in the R Graphics book by Paul Murrel.  Here's one possibility with the ggplot2 package. N <- 10 labs <- factor(1:Nlabels=paste(""This is \n observation""1:N)) dnow <- data.frame(x=1:N y=runif(N) labels=labs) qplot(labelsydata=dnow) + opts(axis.text.x=theme_text(angle=-45hjust=0)) I'm looking forward to seeing the base package examples too! @chris - I would like to accept your question (all the cool kids use ggplot2 these days). But could you fix the x-axis before hand (ggplot orders character vectors alphabetically so you have to convert it to a factor.) Thanks! Good suggestion. I switched it to be a factor but the right hand side is still being cut off. I'll try and fix that. this also works for data labels - if you drop a \n into them it will break lines!"
873,A,"Looking for MsPaint alternative with alpha support In our codebase I write many things that for testing depends on the image Im loading have alpha. Usually I make all my own testimages/programmers-art in MsPaint. But every time I need an image with alpha I have to go and disturb one of our artists. (Or start the loading of GIMP and go for a coffee break.) So Im looking for alternatives as quick and simple as MsPaint but which has alpha support. I tried GIMP but the time it takes to load that behemoth makes it not an option. Is there no software in the middle grounds between mspaint and gimp? From the looks of my googling and searching through wikipedia most raster graphics editors aim to be a Photoshop alternative. Paint.NET It's basically exactly what you're asking for. Not hardcore as Photoshop more advanced than Paint. Built on the .NET Framework so may only run on Windows (might work with Mono I don't know) but as you've said you use Paint I'm assuming you're on Windows anyway. I'd upvote this twice if I could. :) +1 for Paint.NET (and if I were 38 seconds quicker those upvotes would be mine!!! :) Hehe yes just a little too slow :) Enjoy your reputation Mr Shutler. I'll get you next time. *shakes fist* +1 fast finger typing :-)  Paint.Net is a simple and easy alternative that gives you a little more features than MSPaint without having to load big heavy monsters (or read 500-page manuals). As far as I can tell it supports Alpha quite nicely as well as many other features a non-artist might need for their graphics work. http://www.paint.net/ Seems like Paint.Net is the way to go. Found it on wikipedia but deemed to much photoshop-ish. Im downloading it now and to compensate for your ""lost"" upvotes Ill accept your answer. ;) Looks like they sold the URL paint.net and moved to [http://www.getpaint.net/](http://www.getpaint.net/) with download page [here](http://www.dotpdn.com/downloads/pdn.html)."
874,A,"Generic solution for resolution of image in SVG I have an xml file which I need to visualize in a pdf file. I use xslt to do transformations and an svg image in the pdf. The svg will finally draw some information and also display a tiff image for which the path and the resolution is in the xml file. The problem is that the image can have different resolutions which I need to fit into the same boxed area. I do have the resolution and I have it working for two different resolutions but a third resolution doesn't work. So what I need is a more generic solution. At this moent I have the following code to draw the tiff image in the svg: <!-- Draw the tiff image --> <svg:g transform=""translate({$svg_image_width_first} {$svg_image_height_first})""> <svg:g transform=""rotate({$tiff_rotation})""> <svg:g transform=""scale({$scale_x} 1)""> <svg:image xlink:href=""{$xml_bitmap_path}"" x=""{$tiff_height_offset}"" y=""{$tiff_width_offset}"" width=""{$svg_image_height}"" height=""{$svg_image_width}""> <svg:title>Front Image</svg:title> </svg:image> </svg:g> </svg:g> </svg:g> I figured out that I need to do something with the scaleing so I leave the y scaling constant and alter the x scaling accordingly: <xsl:variable name=""scale_x"" select=""$horizontal_resolution div $vertical_resolution""/> Due to the scaling the view of the svg changes and I need to alter the tiff_height_offset variable to be able to cope with the changes in the scale: <!-- After rotation place of the image should be corrected--> <xsl:variable name=""tiff_height_offset""> <xsl:choose> <xsl:when test=""$tiff_rotation = '-90'""> <xsl:value-of select=""0 - (($svg_image_height + ($svg_image_height * (1 div $scale_x )))div $scale_x)""/> </xsl:when> <xsl:otherwise> <xsl:value-of select=""0""/> </xsl:otherwise> </xsl:choose> </xsl:variable> So these pieces of code seem to work for some resolutions but not for all. I hope someone can give me a solution or hint for how to make this generic for all kinds of resolutions. After some extensive deeper look into the problem I figured out a solution. First I figured out that also the rotation changes the vie too so the width becomes the height etc. So I don't need to scale in the x direction but in the y direction: <xsl:variable name=""scale_y"" select=""$horizontal_resolution div $vertical_resolution""/> And I don't need to alter the height but the width according the scale: <xsl:variable name=""tiff_width_offset""> <xsl:choose> <xsl:when test=""$tiff_rotation = '90'""> <xsl:value-of select=""(0 - $svg_image_width) * $scale_y"" /> </xsl:when> <xsl:otherwise> <xsl:value-of select=""0""/> </xsl:otherwise> </xsl:choose> </xsl:variable> So the drawing of the image now looks like this: <svg:g transform=""translate({$svg_image_width_first} {$svg_image_height_first})""> <svg:g transform=""rotate({$tiff_rotation})""> <svg:image xlink:href=""{$xml_bitmap_path}"" x=""{$tiff_height_offset}"" y=""{$tiff_width_offset}"" width=""{$svg_image_height}"" height=""{$svg_image_width * $scale_y}"" transform=""scale(1 {1 div $scale_y})"" > <svg:title>Front Image</svg:title> </svg:image> </svg:g> </svg:g> Now I can understand all calculations and I tested it with several resolutions."
875,A,How Scanline based 2d rendering engines works? Will you please provide me a reference to help me understand how scanline based rendering engines works? I want to implement a 2D rendering engine which can support region-based clipping basic shape drawing and filling with anti aliasing and basic transformations (Perspective Rotation Scaling). I need algorithms which give priority to performance rather than quality because I want to implement it for embedded systems with no fpu. I mean No GPU and No FPU just ARM926EJS RISC Processor with 200mhz and 32MB SDram running at 100Mhz. Do you mean GPU (Graphics Processing Unit) or FPU (Floating-Point Unit)? I'm probably showing my age but I still love my copy of Foley Feiner van Dam and Hughes (The White Book). Jim Blinn had a great column that's available as a book called Jim Blinn's Corner: A Trip Down the Graphics Pipeline. Both of these are quited dated now and aside from the principles of 3D geometry they're not very useful for programming today's powerful pixel pushers. OTOH they're probably just perfect for an embedded environment with no GPU or FPU! Hi mtnygard I bought (Foley Feiner van Dam and Hughes) Book. Thank for suggation.  Here is a good series of articles by Chris Hecker that covers software rasterization: http://chrishecker.com/Miscellaneous_Technical_Articles And here is a site that talks about and includes code for a software rasterizer. It was written for a system that does not have an FPU (the GP2X) and includes source for a fixed point math library. http://www.trenki.net Well Links are preety good for 3d rendering But I want to implement a 2D renderer. Will work on 3d after I implement 2D.  I'm not sure about the rest but I can help you with fast scaling and 2D rotation for ARM (written in assembly language). Check out a demo: http://www.modaco.com/content/smartphone-software-games/291993/bbgfx-2d-graphics-library-beta/ L.B. Bitbank I can't find assembly optimized functions in the ZIP file Will you please check and tell me where they are? Thanks. Sorry Sunny but the source code is not free. The point of passing you the link is to allow you to run the demo code and see if it is what you are looking for. I don't work with embedded linux. contact me directly to continue this discussion. Hi Bitbank I am running embedded-linux. So I can not test .exe file. Have you executable for linux? Thanks Sunny. The link is a post on Modaco about a 2D graphics engine I'm working on. It includes fast ARM assembly language to rotate 2D bitmaps in real time. Here is the link to the demo application (Windows Mobile) and sample code: www.bitbanksoftware.com/private/bbgfx_demo.zip Hi LB I don't found Rotation and scaling code in that link. It is blank. I am eager to know that how can you rotate any bitmap.
876,A,"Which parts of Graphics Pipelines are done using CPU & GPU? Which parts of pipelines are done using CPU and which are done using GPU? Reading Wikipedia on Graphics Pipeline maybe my question does not precisely represent what I am asking. Referring to this question which ""steps"" are done in CPU and which are done in GPU? Edit: My question is more into which parts of logical high level steps needed to display terrain+3D models [from files] are using CPU/GPU instead of which functions. Possible duplicate: http://stackoverflow.com/questions/2713417/which-opengl-functions-are-not-gpu-accelerated @xavier Thanks! That's a great thread. The answer depends on what GPU (since they differ in HW capabilities specially in integrated ones) but on modern non-integrated GPUs the entire pipeline is implemented in HW. This includes culling z-sort and occlusion. Integrated chipsets such as Intel's have less HW features for example they usually do tranformation & lighting using software. modern integrated chipsets can do T&L (aka vertex processing) in hardware too. Please don't downvote valid answers.  It depends a lot on the system configuration at least the CPU has to issue the commands to the GPU (send vertices lists upload textures etc.) and the GPU usually does all the geometric transformations texture mappings lighting etc. But in many cases part of the work that the GPU should be doing might be done by the CPU if the hardware doesn't support certain features. If you're using a 3D engine it might also do some pre-optimizations on the CPU to alleviate the number of triangles that the GPU has to process like discarding beforehand parts of the geometry that are known to be hidden (by using BSP's like IdTech engines or Portals like Unreal Engine for example). Things like moving and rotating the meshes of the models (e.g. the walking animations) are usually done in the CPU but nowadays I guess the trend would be to HW accelerate them too. Regarding the wikipedia link all the steps mentioned there are usually done in the GPU but is up to the software running in the CPU to decide what polygons are going to be sent for processing (even if they are discarded by the GPU later on because they're hidden or out of the viewing frustrum).  Which parts of pipelines are done using CPU and which are done using GPU? As far as I know... It depends entirely on the hardware driver and opengl implementation. Everything you can do in OpenGL works but you can't be 100% sure where processing happens - it is conveniently hidden from you (it is actually one of OpenGL benefits). There are full-software OpenGL emulators for example - mesa3d (non-certified). Put such opengl32.dll (if you're on windows) into your program folder and everything will be running on CPU. Typically rasterization (last stage of pipeline - alpha-blend putting pixel ztest) is done in hardware. Get old videocard and it is possible that the rest of functionality will run on CPU. Vertex transformations vertex manipulations and even standard lighting can be done on CPU (and they were done on CPU for a long time) - depending on videocard. However I must admit that that videocards without hardware TL (transform and lighting) support are quite old. AFAIK transformation and lighting is always performed on CPU if card only supports full hardware acceleration of DirectX 7 (windows-platform specific). There may be platform dependent way to get videocard capabilities in openGL though.. Side note: DirectX gives you more information about card capabilities and a ways to control what happens where (i.e. you can choose software T&L even if videocard supports hardware acceleration - in some cases this may be necessarry). This comes with penalty - DirectX is less flexible than OpenGL harder to use harder to experiment with it is available on less platforms than OpenGL and worrying about which feature is supported and which isn't takes development time."
877,A,"How can I turn on (change color) of a single pixel by coordinates in WPF I have seen similar questions on here but haven't found an answer. I'm taking a computer graphics course in college and we are taught different algorithms that are used to display shapes. My assignment is to choose any development platform and implement these algorithms. Since I have experience developing in WPF I want to use it for this assignment. But I can't seem to find how to give the coordinates of a pixel and change its color. I know school-related questions aren't so popular here on stackoverflow but I don't feel that asking this question is cheating on my homework in any way. Thank you! You can use a Shape object like Line or Rectangle in XAML or in code. For example using Line in XAML you could use <Line X1=""10"" Y1=""10"" X2=""11"" Y2=""11"" Stroke=""Black"" StrokeThickness=""1"" /> X1 is the begin x coordinate. X2 is the end x coordinate. Y1 is the begin y coordinate. Y2 is the end y coordinate.  Try taking a look at the WriteableBitmap class. WPF doesn't let you deal directly with pixels but the WriteableBitmap will allow you to set pixels on a bitmap and then render it.  You've got three options: Add a 1 pixel sized rectangle to a Canvas (the canvas is how you do co-ordinate position in WPF) Do some custom painting in a WriteableBitmap (examples are at that page) Do some custom painting in the CompositionTarget.Rendering event and ""opening"" a renderer like so: using (DrawingContext context = visual.RenderOpen()) { context.DrawRectangle(Brushes.Red null new Rect(5511)); } The code snippet isn't clear about what type `visual` is. That got me stuck trying to Google more info about this because the base `Visual` class doesn't have a `RenderOpen` method. It looks like it has to be of type `DrawingVisual`."
878,A,"winforms newbie trying to get into wpf looking for resources I am brand new in the world of Wpf and am thinking it's time to get into it for my next project. I'm a hardcore Winforms guy. Many of my projects perform tons of custom drawing - for example drawing a virtual baseball strike zone and then drawing icons to represent pitches thrown by a pitcher or seen by a batter. The pitches are interactive - the user can select one or multiple pitches (click shift-click rectangle drag) and then play video of the selected pitches. I also have applications where I draw custom objects and then allow the user to drag them around and place them on a blank canvas. I am trying to learn how one would do these types of things in the Wpf world. I got my first ""hello world"" graphic program working today where I overrode ArrangeOverride and drew some lines on a blank window. But I've been reading about UIElement classes and Adorners and want to make sure I'm doing things ""the right way"". Should all my pitches be their own UIElements for example? I'm wondering if someone can guide me to some sample code or a book or article to get me started. If I could see ""the right way"" to create a custom drawn object (of any complexity a simple rectangle would be fine) have the user select it (highlighting it somehow to show that it's selected) and then have the user drag it around the Wpf window I would be well on my way. offtopic: that baseball stuff sounds like it was fun! I also come from a WinForms background and the best book I've found is ""Windows Presentation Foundation Unleashed"" by Adam Nathan. Amazon: http://www.amazon.com/Windows-Presentation-Foundation-Unleashed-WPF/dp/0672328917 It's a very XAML focused book. I feel is very important for a WinForms guy because you have to get out of the mindset that code controls your UI. XAML is kind in the WPF world and you need to get into the habbit of starting in XAML vs. code. great suggestion - b/c I'm lost as to how XAML can help in these all-custom-graphics type user-interfaces. As I see it XAML is the declarative description of your window - button here listbox there. But the UI of my app isn't known till runtime - pull these 30 pitches out of the DB and draw them. This is a really good book just finished reading it. Covers everything in a real world way unlike many microsoft books."
879,A,"how to set a footer at top layer of the app in android? i want set my footer at the top layer of my app. it should not have any shakes and moves while the activity navigation or showing up the keyboard. it should always settled in the bottom of the screen. how to do that? any ideas plz. You can do that. I tried that and its working.. the keyboard is hiding my footer.. Here I am pasting my code snippet..  <RelativeLayout android:layout_width=""fill_parent"" android:layout_height=""fill_parent"" xmlns:android=""http://schemas.android.com/apk/res/android""> <LinearLayout android:layout_width=""fill_parent"" android:orientation=""vertical"" android:layout_below=""@+id/top_layout"" android:id=""@+id/control_layout"" android:layout_height=""fill_parent"" android:layout_centerVertical=""true"" android:layout_centerHorizontal=""true"" android:background=""@drawable/bg"" android:fadingEdge=""horizontal|vertical"" android:fitsSystemWindows=""true""> <EditText android:layout_width=""wrap_content"" android:textStyle=""normal"" android:typeface=""serif"" android:imeOptions=""actionDone|flagNoEnterAction"" android:textSize=""14sp"" android:text=""Enter TExt here"" android:isScrollContainer=""true"" android:scrollHorizontally=""false"" android:layout_height=""fill_parent"" android:layout_marginBottom=""45dip"" android:scrollbarStyle=""insideInset"" android:id=""@+id/edit_text"" android:background=""@android:color/transparent"" android:gravity=""top"" android:paddingBottom=""1dip"" android:paddingLeft=""2dip"" android:paddingRight=""1dip"" android:paddingTop=""1dip"" android:visibility=""visible""></EditText> </LinearLayout> <LinearLayout android:layout_width=""fill_parent"" android:orientation=""vertical"" android:id=""@+id/main_layout"" android:gravity=""bottom"" android:isScrollContainer=""true"" android:layout_height=""wrap_content"" android:layout_alignParentBottom=""true""> <LinearLayout android:id=""@+id/bottom_panel"" android:layout_height=""wrap_content"" android:orientation=""horizontal"" android:layout_width=""fill_parent"" android:background=""@drawable/bottom_panel_bg""> <Button android:layout_width=""wrap_content"" android:text=""Explore"" android:background=""@drawable/explore"" android:layout_gravity=""center_vertical"" android:id=""@+id/explore_text"" android:textColor=""#FFFFFF"" android:gravity=""bottom"" android:paddingRight=""1dip"" android:layout_height=""fill_parent"" android:textColorHighlight=""#FFFFA6"" android:typeface=""serif"" android:layout_marginLeft=""5dip""></Button> <Button android:layout_width=""wrap_content"" android:id=""@+id/search_text"" android:background=""@drawable/btn_search"" android:gravity=""bottom"" android:text=""Search"" android:textColor=""#FFFFFF"" android:layout_height=""fill_parent"" android:typeface=""serif"" android:layout_marginLeft=""11dip""></Button> <Button android:layout_width=""wrap_content"" android:id=""@+id/fav_text"" android:text=""Favorite"" android:background=""@drawable/favorites_text"" android:gravity=""bottom"" android:textColor=""#FFFFFF"" android:layout_height=""fill_parent"" android:typeface=""serif"" android:layout_marginLeft=""10dip""></Button> <Button android:layout_width=""wrap_content"" android:text=""My Data"" android:id=""@+id/my_text"" android:background=""@drawable/my_texts"" android:gravity=""bottom"" android:textColor=""#FFFFFF"" android:layout_height=""fill_parent"" android:typeface=""serif"" android:layout_marginLeft=""11dip""></Button> <Button android:id=""@+id/about_us"" android:background=""@drawable/about_us"" android:gravity=""bottom"" android:textColor=""#FFFFFF"" android:layout_gravity=""center_horizontal"" android:text=""About"" android:layout_height=""fill_parent"" android:textColorHighlight=""#FFFFA6"" android:layout_width=""wrap_content"" android:typeface=""serif"" android:layout_marginLeft=""11dip""></Button> </LinearLayout> </LinearLayout> Check whether it help you or not  Set your root layout node to be a RelativeLayout. Wrap your main application content to be a LinearLayout as normal and wrap your footer content in a layout with attribute android:layout_alignParentBottom=""true"" set. its not working....  it should not have any shakes and moves while the activity navigation or showing up the keyboard. it should always settled in the bottom of the screen. That is not possible sorry. i saw a university of nebraska app for iphone. they use a footer always at the bottom. why not in android we can't do that??? i wanna develop my app like that? i think my English is a problem to u. so can u see that free app in iphone. and give me some suggestions plz... ""why not in android we can't do that?"" The soft keyboard is more important than your footer. Hence either the soft keyboard will appear over top of your footer or it will cause your footer to slide up the screen depending on your settings (see http://developer.android.com/resources/articles/on-screen-inputs.html). okay. why we cant do that while activity navigation??? If you disable inter-activity animations for your application you *may* get the effect you want. I have not tried this and so I do not know if it will meet your needs. See http://developer.android.com/reference/android/app/Activity.html#overridePendingTransition%28int%20int%29 thanks a lot for your support and encouragement Mark.. i think the overridePendingTransition() is used to set an animation between activity navigation. we can't do anything for the layouts Correct but if layout of Activity A has your footer and the layout of Activity B has your footer and if there is no animation between them the visual effect *might* be what you want. Again I have no tried this and do not know if it will meet your needs. okay. i shall try it and let u know.. yes it works. but without animation its not good looking. thanks a lot Mark."
880,A,"Equidistant points across Bezier curves Currently I'm attempting to make multiple beziers have equidistant points. I'm currently using cubic interpolation to find the points but because the way beziers work some areas are more dense than others and proving gross for texture mapping because of the variable distance. Is there a way to find points on a bezier by distance rather than by percentage? Furthermore is it possible to extend this to multiple connected curves? This is called ""arc length"" parameterization. I wrote a paper about this several years ago: http://www.saccade.com/writing/graphics/RE-PARAM.PDF The idea is to pre-compute a ""parameterization"" curve and evaluate the curve through that. Haven't read the paper fully yet. But I'd like to ask if there is better way to define curves which wouldn't need to be ""converted"" first. E.g. do you know if I'd use NURBS to define all paths/curves would it support faster equidistant arc length parametrization? Or some other way perhaps? Edit: By faster I mean using CPU or GPU. Using NURBs won't help the fundamental problem is the same. The end of the paper shows a method of composing the re-parameterization curve with the original. This produces a new curve with arc-length parameterization but the order if the curve is higher so it's slower to evaluate.  distance between P0 and P3 (in cubic form) yes but I think you knew that is straight forward. Distance on a curve is just arc length: where: (see the rest) Probably you'd have t0 = 0 t1 = 1.0 and dz(t) = 0 (2d plane). This is how you find the arc length given the parameter but finding equidistant points requires the inverse of this function. Getting from one to the other is not trivial. @Christian Romo: how did you do it? I mean you can just use binary search but that would be horribly slow (for what I'm trying to do anyway)."
881,A,Direct3D: What does the stream number do in device->SetStreamSource? Where else is the stream number used other than in these two places: GetStreamSource and SetStreamSource? Using multiple streams allows you to combine together vertex component data from different sources. This can be useful when you have different rendering methods each of which requires different sets of vertex components. Instead of always sending the entire set of data you can separate it into streams and only use the ones you need. See this chapter from GPU Gems 2 for an example and sample code. It can also be useful for effects such as morphing. When calling CreateVertexDeclaration you specify the stream number in the D3DVERTEXELEMENT9 elements to determine which stream each vertex component comes from.
882,A,How to implement high speed animation? I'm trying to write an application (winforms) that can demonstrate how two oscillating colors will result in a third color. For this I need to be able to switch between two colors very fast (at >50 fps). I'm really hoping to do this in managed code. Right now I'm drawing two small rectangular bitmaps with solid colors on top of each other. Using GDI+ DrawImage with two in-memory bitmaps in a doublebuffering enabled control doesn't cut it and results in flickering/tearing at high speeds. A timer connected to a slider triggers the switching. Is this a sensible approach? Will GDI and BitBLT be better? Does WPF perform better? What about DirectX or other technologies? I would really appreciate feedback TIA! I have never had good luck with GDI to do high speed graphics so I used DirectX but MS has dropped support for Managed DirectX so you may need to do this in unmanaged C++. Just write your controller in C# then have a very thin layer of managed C++ that just calls to the unmanaged C++ DLL that has DirectX support. You will need to get exclusive control of the computer so that no other application can really use the cpu otherwise you will find that your framerate can dropped or at least no be very consistent. If you use an older version of DirectX such as DirectX 9.0c that may still have support for .NET and I used that to get a framerate for a music program of about 70 frames/second. I haven't written C++ in years (since college actually) but if I can get away with a minimum of C++ code that would be great. Unfortunately if you used a newer version of DirectX you would be doing a lot more C++ so just look for the version I mentioned. Late feedback but anyway: This ended up as the solution due to some tearing problems I couldn't find a fast solution for with SDL. Found the DirectX 9.0c version created a winform control which did the graphics magic. To avoid the additional DirectX installation I just bundled the MDX DLLs needed in the app solution. Thanks.  This will work with GDI but you won't be able to control flicker so it's kind of out of the question. Direct X may be a lot of extra fluff just for showing two non-flickering images. Perhaps SDL will work well enough? It's cross platform and you can literally code this effect in less than 30 lines of code. http://cs-sdl.sourceforge.net/index.php/SimpleExample I had forgotten about SDL. Definitely going to test this since there is a .NET library available. This might just be the solution.  Flicker should be avoidable with a double-buffered approach (and by this I don't mean just setting the rendering control's DoubleBuffered property to True - ironically this will have no effect on flicker). Tearing can be dealt with via DirectX but only if you synchronize your frame rate with your monitor's refresh rate. This may not be possible especially if you need to achieve a specific frame rate (and it doesn't happen to be your monitor's refresh rate). I don't think WPF gets around the fundamental tearing problem (but I could be wrong). Thanks for your feedback.
883,A,"Developing applications expected to run over RDP; any tips? Supposing I was developing a fairly graphically intensive application (C++ or C# graphics API undecided) for which most of the usage will be by remote users over RDP (either terminal server sessions or remote access to a single-user machine). It's obvious that non-essential ""eye-candy"" effects and animations should be avoided. My questions are: What should I be careful to do/avoid doing to make most efficient use of the RDP protocol ? (e.g I have an idea RDP can remote some graphics drawing primitives straight to the client... but is that only for GDI ? Does using double-buffering break such remoting and force a bitmap mode ? Does the client-side bitmap cache ""just work"" or does it only cache certain things like fonts and icons ?) Is there any sort of RDP protocol analyser available which will give some insight into what an RDP stream is actually transporting (in particular bitmaps vs drawing primitives) ? (I can imagine adding some instrumentation to the rdesktop source to do this but maybe something exists already). In my experience I'd be careful when it comes to animations - especially fade up/down controls that can seriously kill performance over RDP. Double-buffering might also cause some problems however I personally haven't had to do too much in the way of workarounds for this - the article by Raymond Chen explains the possible pitfalls quite well. Essentially it's a good idea to check in code whether it's running in a remote sessions (RDP Citrix etc). Take a look at: GetSystemMetrics( SM_REMOTESESSION ) - you can then decide at runtime whether to enable or disable certain features.  My idea is that the optimization work made on RDP already cover 90% of the problem you're describing so I would not worry about optimizing for RDP you're already removed the eye-candy stuff you know that the application will be used via RDP so I suppose you'll avoid operations that involves continuous redrawing of form I believe that sould be enough. Our application was never designed with RDP in mind we had the same worries you have when a customer told us that all its client will be used via RDP (Citrix in that specific case) from remote locations but also if we didn't change a single line of code the customer never called with slowlyness problems due to RDP. Remember... Premature optimization is evil."
884,A,"Drawing massive amounts of data in NSView or something else? I have hundreds of thousands of points of time series data that I want to represent to the user. My current solution is to render said data to a PNG with a 3rd party library and then load that PNG into a NSImage and display it in a scroll view. This works great except that: NSImages more than 32k pixels wide don't display properly I want to be able to zoom into the data quickly and easily Reading to and writing from disk is stupid My current attempt is to directly draw NSBezierPaths to a NSView. The view renders beautifully but very very slowly even if I only draw a limited subset of points at a time. And every time I scroll I have to re-draw which is also slow. I'm certain as a relative Cocoa newbie that I'm missing some better ways to do this. What's the ""right"" way to do it? The docs have a section on NSBezierPath performance.  My current attempt is to directly draw NSBezierPaths to a NSView. The view renders beautifully but very very slowly even if I only draw a limited subset of points at a time. And every time I scroll I have to re-draw which is also slow. Before you undertake any drastic solutions try these simpler steps: Clip. Use NSRectClip passing the rectangle that you take as the argument to drawRect:. This is a one-liner. Do simple rectangle testing before filling paths. For each path get its bounds and use NSIntersectsRect to test whether it's inside the rectangle. Do slightly-less-simple rectangle testing before stroking paths. Similar to the previous step except that you need to grow the rectangle (the one you received as your argument) by the line width since half of the stroke will fall outside the path. For each path get the linewidth and negate it (delta = -[path lineWidth] — and yes you must include that minus sign) then pass the result as both arguments to NSInsetRect. Make sure you keep the original rectangle around since different paths may have different linewidths. The idea is quite simply to draw less. Setting the clipping path (using NSRectClip) will reduce the amount of blitting that results from drawing operations. Excluding paths that fall completely outside the draw rectangle will save you the probably-more-expensive clipping for those paths. And of course you should profile at each step to make sure you haven't made things slower. You may want to set up some sort of frame-rate counter. One more thing: Getting the bounds for each path may be expensive. (Again profile.) If so you may want to cache the bounds for each path perhaps using a parallel NSArray of NSValues or by wrapping the path and bounds together in an object of your own devising. Then you only compute the bounds once and merely retrieve it on future drawing runs. Sorry—I always get the order of the words mixed up. It's NSRectClip. I've corrected the post and added a link. Can you give me a pointer to docs on NSClipRect? Google only gives 7 hits for it with this very page being the top hit.  Consider using a framework that handles that sort of stuff efficiently like CorePlot  I'd look at CATiledLayer (Leopard-only) for this. You can draw a massive amount of stuff in the tiled layer and have it display areas as needed. For your case you could set the CATiledLayer as the backing for your NSView draw all your Bezier paths into that layer and then scroll around and even zoom in and out on it. Core Animation layers are handled like OpenGL textures so you should get very good performance from this. The vector drawing is cached in the layer and doesn't redraw as you scroll around like you find on a standard NSView. For an example Bill Dudney has posted some sample code on how to use CATiledLayer to display a massive PDF file.  How dynamic is the data set? If it's reasonably static and you want to ""zoom"" then you should consider consolidating the data in to a scaled subset. Contrived example: if you have 100000 pts (say advancing on the X axis) then clearly in a 1000 pixel image you're only going to be inevitably plotting 1000 pts of that 100000. So you can do that in advanced (1000 pts 10000 pts raw data of 100000pts) and select from the appropriate set based on the ""zoom"" level that you're displaying. As for what value to select when the points overlap you can do min max median average etc.  If you're drawing that many bezier paths OpenGL is probably the way to go. Apple's documentation would be a good place to start. They have sample code for implementing a basic OpenGL rendering context in a cocoa window. This would move the burden of the difficult drawing tasks onto the graphics processor and leave your app free to scroll at will. This page has sample OpenGL code for drawing bezier curves."
885,A,"Should a person new to windowed applications study X GTK+ or what? Let's say the factors for valuing a choice are the library of widgets available the slope of the learning curve and the degree of portability (platforms it works on). As far a language binding goes I'm using C++. Thanks! When you talk of portability do you mean code that needs to be ported between Windows and Linux (in example)? Pretty much yes. I have a desktop with XP and an old laptop with Xubuntu. X and GTK are not the same kind of thing. I suggest getting a generic book on GUIs. I have used Borland's Windows wxWidgets QT and PEG windowing frameworks. In summary there is no standard but GUI systems are Event Driven. Study up on Event Driven programming and that should give you an excellent foundation.  GTK+ it is: runs on most Linux distros and Windows too. Of course there are also Qt and WxWidgets which are cross-platform.  I think there is another issue that you did not mention: where do you want to go with it? do you want to ""just learn it"" or do you have a specific application in mind? if you have an application perhaps you also need to consider: what OS you plan to run it on how is the community supporting it: active? is it difficult to get help? what performance do you need/expect? how long do you plan to support it? (you might decide to run^H^H^Hmove away from C++ in a few years and if so would it make sense to use a different language - ie Java?)  My personal favorite: Qt. It's cross-platform it's very intuitive it's extensively documented there's bindings for many languages (the original is C++) and it's actually fun to develop with it.  Qt is very good. It has quite large library (not UI only). Runs on a lot of platforms.  Unless you're planning to write your own UI toolkit there's really no point in using X directly anymore. Too hard too much work. On Linux you have the two main choices - GTK and Qt. Both work fine. Qt works better as a native C++ toolkit than GTK itself although GTKmm is a decent C++ wrapper for it. GTK tends to be usable from more languages than Qt but that doesn't matter if you're using C++ anyway. Both are cross-platform but GTK feels somewhat alien on other operating systems particularly on Mac OS X. Qt feels completely native on Windows and fairly close on Mac OS X. It also provides a lot of other cross-platform functionality beyond the UI such as threading filesystem access networking and so on. Qt certainly seems to win on the portability front at least. Generally go with something that's popular - there's more chance of finding good examples pre-made applications you can dissect libraries you can use or even just finding help here.  In my opinion the best C++ GUI toolkit is Qt http://qt.nokia.com It's cross-plateform (windows mac os x linux) efficient and has quite a few nice extensions (Qwt Qwt3d QGLViewer ...) On the other hand if you want to learn about GUI programming I would learn quite a few systems including GTK Tk Motif.  I think you can start with wxWidgets. wxWindgets was created for building inter-platform GUIs. Pete asked for a portable solution but it is not clear to me if he meant platform as in different Linux platforms or platform to mean he wants to support both Windows and Linux. ... well only if you want to use q toolkit inspired by the MFCs. I tried it quite a few years ago and unless it changed completely I would never recommend using it  Pure X is quite hardcore these days and not very portable. Basically there are three major toolkits: GTK+ (and C++ wrapper GTKmm) Qt wxWidgets which are pretty comparable so which to choose is a matter of taste. All three run on major three operating systems although GTK+ on Mac and Windows is little bit awkward. Just my two cents from my experience: From easiest and more comfortable to not-so-comfortable-to-work-with - 1. Qt 2. GTK+ 3.wxWidgets. I'd have to vote for Qt too -- it's solid extensive easy portable and it's pretty widely used in industry so it could conceivably be more valuable to potential employers than not.  If you are most familiar with an interested in C GTK+ is a good place to start. If C++ QT is probably a better choice. Your desktop of choice is also a factor. Gnome uses GTK+ KDE uses QT. Raw X programming is much to low-level to start with. Very few programs are written directly against the X API directly. There have always been toolkits layered over it. Some of the older toolkits are Motif (Lesstif) and Athena. Don't try to start with those though they are very old now.  As you requested a widget library for C++ then I would suggest QT which was created for that programming language; GTK is good too but it was created for C (as many of the libraries created for the GNU project which privileges C against C++). Nobody uses X directly while creating an application; the only people who work directly with X is who is creating a new widget library as already reported from other people here."
886,A,Projecting a screen for a camera I'm trying to learn a little more on vectormath through writing a simple ray tracer and I've been doing some reading on it but what I haven't been able to find is how to determine the direction of the primary rays. This sounds like a simple problem and probably is but with my current knowledge I haven't been able to figure it out. I figured that I need a camera (nothing more than a location and a direction as vectors) and from the camera I fire the primary rays onto a screen in front of the camera which represents the final image. What I can't figure out are the corner coordinates of the screen. If I know the screen finding the direction the primary rays is easy. I'm hoping the screen can be figured out using nothing but simple math that doesn't require any rotation matrices. My best guess is this: I have the direction of the camera as a vector this direction is equal to the normal of the plane of the projection screen. So I have the normal of the screen and from there I can easily calculate the center of the screen which is: camera_location + (normal * distance) Where distance is the distance between the screen and the camera. However that's where I get lost and I can't find a way to figure out the corner coordinates of the plane for any arbitrary direction of the camera. Can any of you help me out here? And if my method can't possibly work what does? Welcome to StackOverflow. Nice well thought out question.. good job. Hope you stick around to answer some questions also! +1 Thank you for the information. Personally I'm not too comfortable with matrix transformations and would like to avoid them as much as possible. However from your post I understand that transformations are the only way to do what I want to do? That would too bad since I thought I was pretty close (I got the normal and centerpoint of the screen) and would've liked a pure vector math solution. I'll try to adopt your code but unfortunately I don't know exactly what is going on internally in your objects at some points mainly in the Matrix4d objects. PS: If you want to reply to an answer on StackOverflow as the creator of the question are you supposed to make a comment to that answer or is it OK to create a new 'answer'? You can also edit your question and insert additional comments if appropriate. ok forget about the perspective() function for now. You can also remove all of the code in lookat() for making the viewMatrix object. Strip it down to making the 'eye' 'view' 'right' and 'up' vectors. That said you pretty much can't do ray-tracing without _some_ matrix math. I've shortened the code myself - you should find it easier to understand now. To answer the PS don't make a new answer. Comment on the existing one.  EDIT: Here's some code which is substantially reduced from that originally posted because it omits the creation of a viewMatrix. That was only needed for some some scanline rendering and wasn't used for ray-tracing. The hard work is the lookat() function particularly the creation of the 'up' and 'right' vectors. These have to be perpendicular to each other and also to the vector running between the eye and the center of the picture. Creation of those vectors relies on the cross-product vector function. The actual function for casting rays assumes that the screen viewport runs from -0.5 to +0.5 in the Y direction. That function is pretty simple and just consists of adding the correct proportions of the 'view' 'up' and 'right' vectors together. public class Camera { protected Point3d eye; protected Point3d center; protected Vector3d up; protected Vector3d right; protected Vector3d view; protected double fovy; // half FoV in radians protected double tanf; protected double aspect; public Ray castRay(double x double y) { Vector3d dir = new Vector3d(view); Vector3d t = new Vector3d(); t.scale(tanf * x * aspect right); dir.add(t); t.scale(tanf * y up); dir.add(t); dir.normalize(); return new Ray(eye dir); } /* algorithm taken from gluLookAt */ public void lookAt(Point3d _eye Point3d _center Vector3d _up) { eye = new Point3d(_eye); center = new Point3d(_center); Vector3d u = new Vector3d(_up); Vector3d f = new Vector3d(center); f.sub(eye); f.normalize(); Vector3d s = new Vector3d(); u.normalize(); s.cross(f u); s.normalize(); u.cross(s f); view = new Vector3d(f); right = new Vector3d(s); up = new Vector3d(u); } /* algorithm taken from gluPerspective */ public void setfov(double _fovy double _aspect) { fovy = Math.toRadians(_fovy) / 2.0; tanf = Math.tan(fovy); aspect = _aspect; } } Thanks. I can work with this. I assume that initializing a vector with another vector as the argument simply set the value of the new vector to the values of the argument vector? Yes that's right. These classes are all from the Java 3D API.
887,A,where to download standard word powerpoint excel icons I'm working on a web app that has functionality to export different data -- for instance in excel format. Where do I find the standard icons for Excel Word Powerpoint (and perhaps csv) files I'd like them to be directly downloadable in different sizes (at least 16x16 and 24x24). And preferably in png format (icon format is no good for using in a web app) Try smashing magazine for their icon sets. Thanks alex. do you have a direct link? Famfamfam's Silk icon set has some nice little app icons: www.famfamfam.com/lab/icons/silk/ They're only 16x16 but they're very clean... and there's over 1000 of them so you can get a unified style to the whole app by using them elsewhere. Thanks I completely overlooked the tiny famfamfam icons. Must have looked me blind on them before writing this. I accept this as the answer  Try http://www.iconfinder.net/ iconfinder.net had a reference to one powerpoint document image. It was located in the Quartz icon set. The Quartz icon set could be downloaded from smashing magazine (after a search on 'quartz'). The icon set only contains 64x64 image sizes. Any other suggestions? Did you try looking for 'powerpoint'? I got 24 results/icons after searching for 'powerpoint'. Are you looking for a specific icon ? You can download the Quartz Icon Pack here: http://www.graphicrating.com/wp-content/plugins/download-monitor/download.php?id=1
888,A,"What is the best way to create a realistic highlighter in .NET (using GDI+)? How can i create a realistic highlighter (simulating a real world highlighter pen) in .NET using GDI+? It is meant to be used on a graphics object not on selectable text. Using a transparent brush (with alpha channel) doesn't do the job since everything below the area covered by the brush gets ""fuzzy"" and i would like the ""foreground"" (mostly text) to stay clear (keep it's color). Using a ColorMap for only the background area could work but this would require a lot of code to determine the background area and a certain threshold (i could pick the background form the first pixel or top right pixel or something). A ColorMatrix to colorize an area seems also an option but i see the same problem as the transparent brush solution (i am no expert on ColorMatrices so i might overlook something). I guess i need a dynamic threshold for both the foreground and the background colors but this could reduce the usability of the highlighter. I could live with both a solution that fills a ""selected"" area or a brush like solution. Here's an article that discusses how to do various blending operations in GDI+. Based on my experimentation a Darken or Multiply blend mode would do the trick. http://www.codeproject.com/KB/GDI-plus/KVImageProcess.aspx  I'm thinking that a brush of a given width and height that as it's dragged across an area of the screen grabs the section beneath it and analyzes it setting non-text color to yellow and leaving text color alone then placing the updated section back onto the screen. It should be relatively quick and easy to do this though you do have to hit each pixel (and use some sort of threshold to determine what's text and what isn't as you mentioned).  A yellow highligher lets yellow light pass while blocking the others. In the RGB color model that means you let Red and Green pass while block blue. We can use a sample picture to see the effects of a highlighter (Microsoft's Snipping Tool) on the channels: Original Image: Highlighted: Color channels: So what you need to do is set the blue value to zero wherever you want your highlighter to appear. Unfortunately there is no blending mode image attributes or color matrix that can be used to only draw one color component. The only way to accomplish that in GDI+ is to use LockBits. Thanks! Using `LockBits` to manipulate the RGB values ""under"" the cursor does the trick."
889,A,Programmatically create Photomosaic Goal: Create Photomosaics programmatically using .NET and C#. Main reason I'd like to do this programatically is to be able to record which image and where it goes myself so I can create an animated effect of the images coming together to create the photomosaic. It would be great to just find something existing already that I could use to accomplish this almost right out of the box. Any ideas? I think you can borrow ideas from color quantization. For each photograph calculate it's average color. Since it's small that's how it will look when viewed from a distance. Then split your target image in photograph-sized rectangles and calculate their average colors. Or for better results properly downsize it so that every photograph corresponds to one pixel in the target picture. Then for every pixel of the target picture choose the photograph which has the color value closest to the pixels color. (Closest - as in closest in the 3D RGB cube). You can also apply some dithering for it to look better (Floyd-Steinberg's algorithm is one of the best algorithms out there).  Not really an answer to your question but you should be aware that there is both a patent and trademark associated with Photomosaic. You can find information in the wikipedia article you already referenced.  Photomosaic-generator There's source code available Very good answer thanks!  You can look up metapixel it has code for doing the photomosaic analysis but can also record an output file that lists exactly what source image was used in each location. That should do what you need. Unsure about windows support though.
890,A,"Why do my buttons stay pressed? In my activity I respond to an onClick() by replacing the current view with a new one (setContentView()). For some reason when I do this and then go back to the original view later the button I originally pressed still looks like it is pressed. If you 'refresh' it somehow (e.g. scroll over it with the trackball) then it reverts to the unpressed state. It's as if the button doesn't get the 'touch-up' event. The weird thing is: This only happens on my T-Mobile Pulse (android 1.5). It doesn't happen on the emulator. I've tried calling invalidate()/postInvalidate() on the view when I show it but it doesn't have any effect. tried to redraw the button? Sounds like onClick() executes before the button's View gets set back to its unclicked state and since you remove the view before that happens it maintains its clicked state. You could try something like requesting focus with another item during the onClick(). However calling a setContentView() repeatedly on a screen sounds a little strange. Are you sure you shouldn't be using another Activity? That would fix your button state issue.  My guess is that your ""touch-up"" event will go to your new content view. You can either: Not call setContentView() at all in onClick() using other means of achieving your UI aims (multiple activities ViewFlipper etc.). Not call setContentView() immediately in onClick() but instead post() a Runnable that will call setContentView(). That should put the content view replacement in the event queue after the ""touch-up"" event. Personally I am not a big fan of activities constantly calling setContentView() as I worry about memory leaks and overly-plump activities making state and memory management more difficult. For example let's say you call setContentView() for your original layout (A) then call setContentView() for your new layout (B) then the user rotates the screen. Android's default onSaveInstanceState() will help you with B -- hanging onto EditText contents and the like -- but for A you're on your own. Ah I didn't know about ViewFlipper (or ViewSwitcher!) Looks like exactly what I want. Example here for future readers: http://www.ctctlabs.com/index.php/blog/detail/android_dont_overlook_viewswitcher/"
891,A,"Are there (free) tools or libraries for 3D WPF objects? I wanted to play around with some 3D controls in WPF but was mildly surprised to find that there were no primitive solid controls in WPF - I just wanted to plop a few spheres and cubes in a scene but didn't realize I had to render them using meshes. Surely someone has created libraries of 3d primitives that can be added to WPF 3D scenes by now. Are any of these libraries free? How about modeling tools? I know there are a few free modeling applications (like Blender) but is there a way to export models from these and include them in a WPF application? Thanks! I don't know how well it will play with WPF but you can download XNA direct from Microsoft for free. It gives you some pretty wrappers around the 3D APIs. Also check out Matt's answer to this question. I don't know if waiting for .NET 4.0 is an option for you... ""Dr. WPF"" wrote a CodeProject article about the WPF D3DImage control then blogged about it.  IMHO Do not use Petzold Media 3D for primitives drawing. It works by unusual way for WPF 3D overriding OnDraw() method. It is not a right way. It is a good resource for learning some features no more. Of course there is great free library Helix 3D in codeplex. Any tool for 3D object creation is a good start. Take a look for a post 3D models fatures in WPF Good luck in WPF 3D! p.s. I like to draw 3D primitives by hand also...  I should add something else terribly terribly important that I forgot to mention in my first post -- why are you looking at 3D controls? I looked at the link Thomas posted and they have a 3D animated ""knife switch"" to replace a check box. FOR THE LOVE OF ALL THAT IS PURE AND HOLY do not foist that on a commercial desktop app! If you want to play with it if you want to make childrens' software interesting or something fine great. But I just want to make sure you're not trying to ""add punch"" to your accounting package or something...  Charles Petzold has written a library of basic 3D shapes for WPF that you can find here : http://www.charlespetzold.com/3D/ Admittedly is comes with a weird license but all in all it's rather cheap and you get a free book which I recommend by the way ;)  If you're planning on using Blender you can export the models using the XAML Exporter for Blender There are also exporter for 3D Studio MAX and Maya 3d and more than likely for most other commercial packages as well."
892,A,"Simple Java Display Images I have a GUI written using netbeans with a few simple components in place. I want to be able to draw images (any file type whatever's easiest) alongside the GUI components inside the JFrame. Don't have to resize them just draw them as they are at the x and y location of my choosing. There will be several images being drawn at each update some will become hidden and others displayed. The updating will only occur every 5 seconds or so so it's being fast isn't really an issue. If it would be possible to attach events to the images' being clicked that would be nice but not essential. This is a supremely simple task that I have as yet been unable to get a simple answer to. How do I go about doing this? Thanks package Pokertable; /* * To change this template choose Tools | Templates * and open the template in the editor. */ /* * ClientWindow.java * * Created on Sep 12 2009 9:10:48 PM */ /** * * @author Robert */ public class ClientWindow extends javax.swing.JFrame { /** Creates new form ClientWindow */ public ClientWindow() { initComponents(); } /** This method is called from within the constructor to * initialize the form. * WARNING: Do NOT modify this code. The content of this method is * always regenerated by the Form Editor. */ @SuppressWarnings(""unchecked"") // <editor-fold defaultstate=""collapsed"" desc=""Generated Code""> private void initComponents() { jScrollPane1 = new javax.swing.JScrollPane(); jTextField1 = new javax.swing.JTextField(); jScrollPane2 = new javax.swing.JScrollPane(); jTextArea1 = new javax.swing.JTextArea(); jCheckBox2 = new javax.swing.JCheckBox(); imagePanel1 = new Pokertable.ImagePanel(); setDefaultCloseOperation(javax.swing.WindowConstants.EXIT_ON_CLOSE); setTitle(""Not Logged In""); getContentPane().setLayout(null); jTextField1.addKeyListener(new java.awt.event.KeyAdapter() { public void keyTyped(java.awt.event.KeyEvent evt) { jTextField1KeyTyped(evt); } }); jScrollPane1.setViewportView(jTextField1); getContentPane().add(jScrollPane1); jScrollPane1.setBounds(0 540 170 22); jTextArea1.setColumns(20); jTextArea1.setRows(5); jScrollPane2.setViewportView(jTextArea1); getContentPane().add(jScrollPane2); jScrollPane2.setBounds(0 440 166 96); jCheckBox2.setText(""Sit Out Next Hand""); jCheckBox2.addActionListener(new java.awt.event.ActionListener() { public void actionPerformed(java.awt.event.ActionEvent evt) { jCheckBox2ActionPerformed(evt); } }); getContentPane().add(jCheckBox2); jCheckBox2.setBounds(0 410 113 23); getContentPane().add(imagePanel1); imagePanel1.setBounds(130 130 100 100); pack(); }// </editor-fold> private void jCheckBox2ActionPerformed(java.awt.event.ActionEvent evt) { // TODO add your handling code here: } private void jTextField1KeyTyped(java.awt.event.KeyEvent evt) { // TODO add your handling code here: } /** * @param args the command line arguments */ public static void main(String args[]) { java.awt.EventQueue.invokeLater(new Runnable() { public void run() { new ClientWindow().setVisible(true); } }); } // Variables declaration - do not modify private Pokertable.ImagePanel imagePanel1; private javax.swing.JCheckBox jCheckBox2; private javax.swing.JScrollPane jScrollPane1; private javax.swing.JScrollPane jScrollPane2; private javax.swing.JTextArea jTextArea1; private javax.swing.JTextField jTextField1; // End of variables declaration } Create your custom component and add it to the NetBeans pallete: public class ImagePanel extends JPanel { private Image img; public void setImage(String img) { setImage(new ImageIcon(img).getImage()); } public void setImage(Image img) { int width = this.getWidth(); int height = (int) (((double) img.getHeight(null) / img.getWidth(null)) * width); this.img = img.getScaledInstance(width height Image.SCALE_SMOOTH); } @Override public void paintComponent(Graphics g) { g.drawImage(img 0 0 this); } } I've included some simple scaling you can remove it if you like. A bit of explanation now: extending JPanel makes the component conform to the JavaBean spec. If you want to be able to set the img property via the NetBeans property editor you should define a simpel getter; otherwise you can manually call setImage() and call repaint() which will make the image draw the overridden paintComponent method is called on each repaint of the component and there you can see you draw your image in the boundries of the current JPanel (ImagePanel) I don't have much experience with netbeans so this looks great but I'm not sure how to take advantage of it. How do I add the class as a custom component? How/when do these objects get instantiated? Also the image itself cannot know whether or not it should be made visible and if so where it should be drawn upon each update. That information lies in another class which will be making periodic calls to the graphics to update themselves with information on what is to be displayed and where. Thanks again for the help...this will be a huge step forward. I can't recall now where exactly was the ""add to pallete"" button but look for it it shouldn't be hard to locate. Then you choose your class and add it. You can actually drag the class from the package explorer onto your form and it will appear there. The visibility of the JPanel is controlled in the superclass JPanel so don't worry about that. Just try it. Does it matter how big i make the ImagePanel component relative to the size of the images I want to display? Do i need one of these panels per image displayed? Where in the code above can I call the setImage() method? Also I can't just drag everything into place ahead of time as the images will be displayed at varying x and y coordinates throughout. So how do I go about adding them on the fly at the location of my choosing and how do i remove an existing one from view in a given update cycle? Also several of the images will end up overlapping a large background image. Will these components be able to overlap one another? well you should read some swing tutorial or ask those questions seperately when you stubmle upon something that doesn't happen the way you want"
893,A,Graphics2D: Drawing black on white? I'm sure this is a very stupid question but I can't find the answer I'm not experienced with the Java2D API. I'm trying to create an image and write it to GIF or PNG and I want it to use a black pen on a white background. If I don't set any colors I get white on black. If I use setPaint() (intended for subsequent draw operations) I get the whole canvas repainted with that color. The following sample renders the whole thing black. The sample is in Scala but you get the idea. Feel free to answer in Java!  val bi = new BufferedImage(200 400 BufferedImage.TYPE_BYTE_BINARY ) val g = bi.createGraphics g.setBackground(Color.WHITE) g.setPaint(Color.BLACK) g.draw(new Rectangle(10 10 30 20)) The setBackground method is/was only for use with the clearRect method. Fill the rectangle with the background colour before painting: int width = 200; int height = 400; BufferedImage image = new BufferedImage(width height BufferedImage.TYPE_BYTE_BINARY); Graphics g = image.createGraphics(); g.setColor(Color.WHITE); g.fillRect(0 0 width height); g.setColor(Color.BLACK); //ready for drawing
894,A,"do i need ""Java Advanced Imaging API"" to learn ""image processing"" in Java do i need ""Java Advanced Imaging API"" to learn ""image processing"" in Java? and is there any good link for learning ""image processing"" in java? If your needs go beyond basic operations - I think the best toolkit out there to learn the entire gamut of image processing (from basic to advanced such as object recognition/computer vision) is Open CV. There are plenty of Open CV wrappers for Java.  It depends on what you mean by ""to learn"". If you want to understand the foundations of image processing then no you don't need anything beyond a basic familiarity with the ImageIO class for reading and writing images and the BufferedImage getRGB() and setRGB() methods. If you want to learn how to use existing APIs for image processing you can still get a lot done with the Java2D API but the JAI gives you more file formats and more filters. But in that case I'd say try Python Imaging Library!  I'd say that if you want to learn image processing then you don't need JAI. At university we played around with PGM files learned how to do basic transforms basic filtering etc. PGM is very easy to work with since it's greyscale and you can encode the image using ASCII. This means you can knock up a bit of code to read and write them in no time and then you can start building your own implmentation of image processing algorithms. Check it out here: http://en.wikipedia.org/wiki/Portable_Gray_Map Obviously if you want to do some serious image processing then check out a real API but if you're wanting to play about and learn then this is where I'd start.  No. However there is a good Java program called ""ImageJ"" (google it). It will get you off to a flying start as you can start writing your own plugins for it in Java with minimal effort and complexity. Also there are many plugins and classes contributed for it and freely available.  Consider Processing it is based on Java (some extensions like a color data type). The tutorials provide good introductions to general image processing concepts. ""Processing is an open source programming language and environment for people who want to create images animations and interactions."" http://www.processing.org  If you are looking for learning very basic stuff I'd say you should not start with JAI because this library will help you processing images easily but not learning how to process images -- you won't get to know the really low level stuff like how to manipulate pixel arrays directly. I started by reversing the image cropping the image creating histograms doing histogram equalization various affine transformation (scaling rotating) etc. This lecture might be a good start.. http://kevin.floorsoup.com/scholarship/ip2d/p12-burger.pdf Sorry if you were looking for more sophisticated stuff :)"
895,A,"Calculate the horizon of a curved face? I need to find the 2 points of the visual horizon of a curved face. I have: XYZ of the 4 corner points XYZ of the 2 curved edge bezier points And I need to calculate either: XY of the horizon points XYZ of the horizon points Discussion continued at http://stackoverflow.com/questions/574732/calculate-the-horizon-of-a-curved-face-not-extrema First off you have to convert your 3D beziers to 2D. If I remember right it's sufficient to project the curves just like you project 3D points for rendering. Afterwards you have to find the extrema of the curves. A small HowTo: Convert your bezier-curve from bezier representation to a polyonomial of the form  x(t) = a*t^3 + b*t^2 + c*t + d y(t) = e*t^3 + f*t^2 + g*t + g Here t is your interpolation variable that goes from 0 to 1. a to d are the coefficients for the curve along the x-axis e to g are the coefficients for the curve along the y-axis. Now you build the first derivation of the curve (easy as it's a polynomail). This will give you a quadratic equation. Solve these for the roots and discard all roots that are outside the 0..1 range. Again finding the roots is easy as it's just a quadratic polynomial. You how have a bunch of roots. Plug all these back into the original bezier curve evaluate their position and you get a bunch of points. The extrema - if exist - will be among these points. Now all you have to do is to search for the one with the highest (or lowest - dunno how your coordinate system looks like) y-coordinate. Note that you may not get an extrema at all. This happends if your bezier is for example a straight line. In these cases you may want to include the first and last bezier control point in your extrema search as well. EDIT: You've asked how to turn the bezier into a polynomial. Well you start with the normal bezier curve equation:  x(t) = x0 * (1-t)³ + 3*x1*(1-t)²*t + 3*x2*(1-t)*t² +x3*t³ (x0 to x3 are the x-values of the four control-points of the curve). Then you multiply out all terms one after another and sort them by the powers of t. Unfortunately I don't have my math package running on the computer I'm writing on and I'm to lazy to do it on paper :-) So if anyone has mathlab running could you please edit this answer and add the expanded version? Anyway since you're not really interested in the polynomial but just the derivate of it things are a bit easier. You can get the coefficients directly (here shown for x only): A = 3.0f*(x[1] - x[0]); B = 6.0f*(x[2] - 2.0f*x[1] + x[0]); C = 3.0f*(x[3] - 3.0f*x[2] + 3.0f *x[1] - x[0]); Using these three values (ABC) the polynomial of the first derivate looks like this:  x(t) = A*t^2 + B*t + C Now plug AB and C into a root solver for quadratic polynomials and you're done. For reference I use the solver C-code below: int GetQuadraticRoots (float A float B float C float *roots) { if ((C < -FLT_EPSILON) || (C > FLT_EPSILON)) { float dp; // it is a cubic: p = B*B - 4.0f * C*A; d = 0.5f / C; if (p>=0) { p = (float) sqrt(p); if ((p < -FLT_EPSILON) || (p > FLT_EPSILON)) { // two single roots: roots[0] = (-B + p)*d; roots[1] = (-B - p)*d; return 2; } // one double root: roots[0] = -B*d; return 1; } else { // no roots: return 0; } } // it is linear: if ((B < -FLT_EPSILON) || (B > FLT_EPSILON)) { // one single root: roots[0] = -A/B; return 1; } // it is constant so .. no roots. return 0; } Thank you so much for your great help all I want to know is how to ""Convert a bezier-curve from bezier representation (XY of 3 points) to a polyonomial"" Would this work with a quadratic-bezier's equation as it would return linear derivates? Or should I try converting my quadratic-bezier to a cubic-bezier? Thanks again for your incredible help. Either solution works. I would simply build the first order derivate of your quadratic bezier. That'll give you an equation like a*t+b = 0 which is even easier to solve. I actually understood why your solution won't work. Would you please help me at the re-post of this question: http://stackoverflow.com/questions/574732/calculate-the-horizon-of-a-curved-face-not-extrema"
896,A,How do I visualize audio data? I would like to have something that looks something like this. Two different colors are not nessesary. I already have the audio data (one sample/millisecond) from a stereo wav in two int arrays one each for left and right channel. I have made a few attempts but they don't look anywhere near as clear as this my attempts get to spikey or a compact lump. Any good suggestions? I'm working in c# but psuedocode is ok. Assume we have a function DrawLine(color x1 y1 x2 y2) two int arrays with data right[] and left[] of lenght L data values between 32767 and -32768 If you make any other assumptions just write them down in your answer. for(i = 0; i < L - 1; i++) { // What magic goes here? } This is how it turned out when I applied the solution Han provided. (only one channel) Audacity is open source so you could look at the code. I'd assume something like the following... func getHeight(v) { return abs(v) * 32767 / viewArea.height / 2); samplesPerPixelColumn = samples.len/viewArea.width; for i = 1 to viewArea.width {avgV = Avg(samples[i-i+samplesPerPixelColumn]); colHeight = getHeight(avgV); if avgV >= 0 DrawLine(black i viewArea.height /2 i (viewArea.height / 2) + colHeight) else DrawLine(black i viewArea.height /2 i (viewArea.height / 2) - colHeight); You'll probably need to do some rounding/range handling in there but that should be the gist. I wanted to give you something that might be immediately helpful but not get downvoted if part of it was off. More thoughts: Instead of vertical lines draw diagonals between the previous graph point and next... Also if you zoom in enough you'll have multiple pixels per sample. Why don't you put that into an answer. You'll likely have more than 1 sample for each pixel. For each group of samples mapped to a single pixel you could draw a (vertical) line segment from the minimum value in the sample group to the maximum value. If you zoom in to 1 sample per pixel or less this doesn't work anymore and the 'nice' solution would be to display the sinc interpolated values. Because DrawLine cannot paint a single pixel there is a small problem when the minimum and maximum are the same. In that case you could copy a single pixel image in the desired position as in the code below: double samplesPerPixel = (double)L / _width; double firstSample = 0; int endSample = firstSample + L - 1; for (short pixel = 0; pixel < _width; pixel++) { int lastSample = __min(endSample (int)(firstSample + samplesPerPixel)); double Y = _data[channel][(int)firstSample]; double minY = Y; double maxY = Y; for (int sample = (int)firstSample + 1; sample <= lastSample; sample++) { Y = _data[channel][sample]; minY = __min(Y minY); maxY = __max(Y maxY); } x = pixel + _offsetx; y1 = Value2Pixel(minY); y2 = Value2Pixel(maxY); if (y1 == y2) { g->DrawImageUnscaled(bm x y1); } else { g->DrawLine(pen x y1 x y2); } firstSample += samplesPerPixel; } Note that Value2Pixel scales a sample value to a pixel value (in the y-direction).  You might want to look into the R language for this. I don't have very much experience with it but it's used largely in statistical analysis/visualization scenarios. I would be surprised if they didn't have some smoothing function to get rid of the extremes like you mentioned. And you should have no trouble importing your data into it. Not only can you read flat text files but it's also designed to be easily extensible with C so there is probably some kind of C# interface as well. I doubt that calling an external application to draw my bitmaps would work. I'm updating the bitmap many times a second.
897,A,Unexplained crashs with coregraphic i'm on this bug for a week now and i can't solve it. I have some crash with coregraphic calls it happen randomly (sometimes after 2 mn or just at the start) but often at the same places in the code. I have a class that just wrap a CGContext it have a CGContextRef as member. This Object is re-created each time DrawRect() is called so the CGContextRef is always up-to-date. The draw calls came from the main thread only After looking for this kind of error it appear that it should be object Release related. Here is an example of an error : #0 0x90d8a7a7 in ___forwarding___ #1 0x90d8a8b2 in __forwarding_prep_0___ #2 0x90d0d0b6 in CFRetain #3 0x95e54a5d in CGColorRetain #4 0x95e5491d in CGGStateCreateCopy #5 0x95e5486d in CGGStackSave #6 0x95e54846 in CGContextSaveGState #7 0x00073500 in CAutoContextState::CAutoContextState at Context.cpp:47 the AutoContextSave() class look like this : class CAutoContextState { private: CGContextRef m_Hdc; public: CAutoContextState(const CGContextRef& Hdc) { m_Hdc = Hdc; CGContextSaveGState(m_Hdc); } virtual ~CAutoContextState() { CGContextRestoreGState(m_Hdc); } }; It crash at CGContextSaveGState(m_Hdc). Here is what i see into GDB: * -[Not A Type retain]: message sent to deallocated instance 0x16a148b0. When i type malloc-history on the address i have this : 0: 0x954cf10c in malloc_zone_malloc 1: 0x90d0d201 in _CFRuntimeCreateInstance 2: 0x95e3fe88 in CGTypeCreateInstanceWithAllocator 3: 0x95e44297 in CGTypeCreateInstance 4: 0x95e58f57 in CGColorCreate 5: 0x71fdd in _ZN4Flux4Draw8CContext10DrawStringERKNS_7CStringEPKNS0_5CFontEPKNS0_6CBrushERKNS_5CRectENS0_12tagAlignmentESE_NS0_17tagStringTrimmingEfiPKf at /Volumes/Sources Mac/Flux/Sources/DotFlux/Projects/../Draw/CoreGraphic/Context.cpp:1029 Which point me at this line of code : f32 components[] = {pSolidBrush->GetColor().GetfRed() pSolidBrush->GetColor().GetfGreen() pSolidBrush->GetColor().GetfBlue() pSolidBrush->GetColor().GetfAlpha()}; //{ 1.0 0.0 0.0 0.8 }; CGColorRef TextColor = CGColorCreate(rgbColorSpace components); Point this func : CGColorCreate(); Any help would be appreciated i need to finish this task very soon but i don't know how to resolve this :( Thanks. `CGContextRef` itself is a pointer already it's better to pass them by value. Hello yes i removed the const & already with no joy thanks anyway
898,A,"RGB filters for different forms of color blindness My father is color blind and given that I work in games where the visuals are important I've always wanted to write a filter for screen shots (or even some type of shader) that emulated different forms of color blindness. I've seen plenty of references but never have been able to track down algorithms. Any algorithms would be appreciated. My Google search came up with this one which appears to be exactly what you're looking for. Eight different versions of color blindness are simulated by multiplying each of the RGB values by 3 different percentages and adding them together. http://www.colorjack.com/labs/colormatrix/ (dead link) Working Archived Link (working link) Thank you very much. About six months ago I looked and I looked and never could find that data. The link is dead @gregers I hate it when that happens! Nothing on the Internet is permanent. Luckily the Wayback machine took quite a few snapshots. I won't post a link because I don't know how permanent they are but you can find it easily yourself at http://archive.org/web/web.php @MarkRansom they are quite permanent. Here's an actual link for the poor people who don't want to search. http://web.archive.org/web/20081014161121/http://www.colorjack.com/labs/colormatrix/  The GIMP has a colorblindness filter in View -> Display Filters -> Color Deficient Vision  I came across Color Oracle and thought it might help. Here is the short description: Color Oracle is a colorblindness simulator for Windows Mac and Linux. It takes the guesswork out of designing for color blindness by showing you in real time what people with common color vision impairments will see. perfect thanks. This is exactly what I was looking for: Its a Java application that loads up a tray icon -- with right click select the blindness you want to simulate. The app takes a screenshot of the screen and ""applies"" the color blindness. Great!  Can't help you on algorithms but the following article was quite an eye-opener (excuse the pun): http://critiquewall.com/2007/12/10/blindness. Ah actually http://www.vischeck.com/ is useful. Vischeck is definately a good resource for color-blindness and computing The vischeck stuff is good. I'll have to check out the info stuff. I've looked at their site before but I don't remember the info page. Maybe I just missed it.  The colorjack link has good info but I use http://colorfilter.wickline.org/. It can check a whole page: images CSS colors and all.  Google came up with a number of links perhaps one provides source or algorithm descriptions: http://www.google.com/search?hl=en&q=simulating+color+blindness&aq=f&oq= Edit: Thanks Torlack for pointing out that everybody knows about Google. But using Google effectively requires using the proper search terms and good search terms aren't always obvious. Judging by the returned page titles and a couple of the links this particular search seemed highly relevant. I don't think it was a bad enough answer for a downvote. Pointing out that google exists doesn't exactly help. Ok I took it back."
899,A,Graphics primitive generators I'm looking for something that can generate primitives (e.g. rounded rectangles for dialog boxes etc) so I can load them into a DirectX textured Sprite. Functionality is like SPriG. Its pretty easy and there are 2 ways : 1) If you are using DirectX10+ use Direct2D. 2) Use GDI+ to draw them onto a texture. Both are integrated with the system and do not need for any external library. Just pick and use. As far as Direct2D I don't know all the details but I can assure you can do whatever you want with GDI+. (You generate them once so you don't care about speed) I'd like to use Direct2D yes. But it actually only works on Windows 7. The GDI+ option isn't terribly bad though. As far as I know Direct2D works fine on Vista too if you installed the platform update.
900,A,"How do I fix ""java.lang.OutOfMemoryError at sun.misc.Unsafe.allocateMemory(Native Method)""? I'm making a Java application that uses the Slick library to load images. However on some computers I get this error when trying to run the program: Exception in thread ""main"" java.lang.OutOfMemoryError at sun.misc.Unsafe.allocateMemory(Native Method) at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:99) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288) at org.lwjgl.BufferUtils.createByteBuffer(BufferUtils.java:60) at org.newdawn.slick.opengl.PNGImageData.loadImage(PNGImageData.java:692) at org.newdawn.slick.opengl.CompositeImageData.loadImage(CompositeImageData.java:62) at org.newdawn.slick.opengl.CompositeImageData.loadImage(CompositeImageData.java:43) My VM options are: -Djava.library.path=lib -Xms1024M -Xmx1024M -XX:PermSize=256M -XX:MaxPermSize=256M The program loads a few large images (1024 x 768 resolution) at the beginning. Any help to solve this problem would be greatly appreciated. Found the problem I was trying to load a 6144x6144 PNG into my program. After re-sizing the image to a 256x256 TGA the program loads fine without the error.  The OutOfMemoryError simply indicates that JVM has run out of memory. The first line of the stacktrace isn't really relevant here as it's just ""by coincidence"" exactly there where JVM starts to run out of memory with all Garbage Collecting in vain. There are basically two solutions to this: Give the JVM more memory. Fix memory leaks and/or allocate less memory in the code (i.e. make code more memory efficient don't get hold of memory expensive resources like large byte[] for too long and so on). Point 1 is easy to do but not always the solution if there's apparently a memory leak in the code. Point 2 is best to be nailed down with help of a Java profiler.  @BalusC is mostly correct about the cause and solutions. However it is possible that the immediate cause of the OOMEs is that on some computers the JVM is unable to expand the heap to the size specified by the -XX options. For instance if the amount of memory requested exceeds the remaining available physical memory + swap space the OS will refuse the JVM's request to expand the heap. This might explain why the application works on some machines and not others ... with the same VM options and (I guess) processor architectures and JVM versions. If this is the problem the OP will need to add more physical memory or increase the system's swap space. To expand on @BalusC's second solution the OP may need to change the application so that it does not eagerly load the images at startup. Rather it could load them lazily and use a cache with weak references to ensure that the GC can discard them if memory is tight. However if it is critical that all images are preloaded then the OP has no choice but figure out how to give the JVM a bigger heap; see above.  Depending on the exact scenario it can be a manifestation of a JVM bug present since 2005. If you look at the code in the java.nio.DirectByteBuffer class you will see that it creates a thread to deallocate the requested memory. If you program use lots of instance of this class (indirectly via IOUtils for example) you might get OOME even if you have enough memory. It's just the the thread did not get the chance to free the memory. This is a nasty one. http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6296278"
901,A,Android - ClipDrawable ScaleDrawable how does it work? I'm having quite a problem here and I hope for someone here to able to help me. let's go. Let's say I have quite a big image ( 1500x2000 ) i load it as a drawable fine so far. Now I have a SurfaceView and I want to draw a certain region ( lets say the top-left-most region ) onto a canvas in a non-scaled version. I thought using ClipDrawable would be just the right thing to use but actually I can't get it to do what I want to. It just displays a scaled-down image with the clip applied. So basically my question is: how to draw a non-scaled drawable onto a surface and how to clip that drawable? any help appreciated thank you :-) Alright I'm no using the Modelview transformations provided by the Canvas object. Works fine. Can I see the code for how you got this working? I'm really sorry but I couldn't find it right now.. :-/ You can use these in XML Drawable. Check this doc for more usage for clip and scale label: http://idunnolol.com/android/drawables.html Whats with the link why not link to the official documentation instead of just a screen scrapped lo-fi version?  To clip a drawable simply set the appropriate clip region on the Canvas prior to drawing the drawable. Don't forget to save()/restore() the Canvas! thats the way i'm going now. took some time to figure it out well now it works. thank you.
902,A,Learning Graphical Layout Algorithms During my day-to-day work I tend to come across data that I want to visualize in a custom manner. For example automatically creating a call graph similar to a UML sequence diagram display digraphs or visualizing data from a database (scatter plots 3D contours etc). For graphs I tend to use GraphViz. For UML-like plots and 3D plots I would like to write my own software to run under Linux. I typically program in C++ and prototype in Python. What books have people used to learn these basic graphical algorithms? I've seen some nice posts on force-directed layout and various block-style layout algorithms based upon the Cutting and Packing problems -- these are great starts but I would like a more beginners guide and overview before I jump in. Directed Graph Layout Force directed layout True for scripts that aren't speed limited such as downloading files and parsing them I exclusively use Python. For speed-critical code such as processing tons of database records C++ is normally 6x faster. Why not just write in Python? What does C++ do that Python doesn't? Here are some sources Graphic Layout and Design (Paperback). Active Layout Engine: Algorithms and Applications in Variable Data Printing Great links thank you!
903,A,The Definitive Image Gallery Engine / Plugin Guide I want to get a good list of image gallery engines of all flavours: Stand alone plugins for Wordpress or Rails AJAX no AJAX using simple folders or a database on the server. Please state what is needed (eg MySQL and Django) to run each item if possible. Thanks! [I asked a similar question a while back but had limited responses. Hopefully with more users and a small bounty this will pick up more steam. EDIT - can't attach a bounty for two days. Hold tight.] These are the ones I recall at the moment they are all easy to integrate and they don't require much implementation to use. They all have a good and appealing design. Hope it helps. Cooliris: Runs on flash uses an RSS feed to show the images FancyBox: Jquery Plugin you just need to have create an < a ref... arround the < img src... LightBox: jQuery plugin also easy to use. Photo Slider: jQuery plugin as some thumbnails bellow which you can use to slide through the images SimpleViewer: Nice Design shows thumbnails and images HighSlide JS: Javascript viewer +1 Great sites. I use lightbox2 on my personal site but may have to look at Cooliris and Fancybox a bit more.  If you are looking for a gallery application I recommend the open source project 'Gallery2'.  Lytebox is easy to use and very nice. It's enhanced version of LightBox.  I like Gallery the best of any I've seen. It requires PHP and a database). It can be plugged in to WordPress and other CMSish things  Here is a nice photogallery using silverlight. Slide.Show is another slick Silverlight gallery. There are many gallery modules available for DotNetNuke and an official module. There are also a great many available on Codeplex.  Take a look at SourceForge I get 'no results found' when clicking on that link @epatel seems that SO mangles the URLs. Passed it to TinyURL
904,A,"Flash AS3 - how to set Imported textfield movieclip as unselectable I created a graphic in Flash CS4 that contains text. I embedded the appropriate characters then saved it as a MovieClip into my library. I then exported it to an SWC file. In my AS3 code (using Flex SDK/notepad) I then import the movieclip and assign it some mouse events so I can use it as a button. Unfortunately all the text-in-graphics I import this way have the ""I"" mouse cursor and the text is selectable. This steals the focus from my flash application and is not good! I know when I have a textfield I can: var myButton:TextField = new TextField(); myButton.MouseEnabled = false; But this has no effect when it's a Movieclip I'm importing: var myButton:MovieClip = new MyImportedButtonGraphic(); myButton.MouseEnabled = false; // No effect // Plus some other things I learned: myButton.selectable = false; // also no effect myButton.MouseChildren = false; // No effect What am I doing wrong? In the flash ide select the textField go to the properties panel and uncheck the button that has the characters 'Ab' in it. That stops your text being selectable. Perfect! That did the trick. Serves me right for not learning how to use the Flash IDE. :(  If you are setting the movie clip that holds the text to not be mouse enabled then you need to set both propetries for it mouseEnabled and mouseChildren. mouseEnabled means that that particular movie clip can't get mouse events but doesn't affect the movie clip's children (such as the textfield inside it). mouseChildren means it's children don't register mouse events they just dispatch from the parent. To completely disable it BOTH need to be false.  var myButton:MovieClip = new MyImportedButtonGraphic(); myButton.mouseEnabled = false; myButton.mouseChildren = false; Since the textfield is a child of the movie clip the mouseChildren property is what is going to affect it and you could just set that to false and it would still work. I've added MouseEnabled = false; MouseChildren=false; to my button and the ""I"" editing cursor still appears and I can still highlight text. :( And that's because it's mouseEnabled and mouseChildren (case sensitive). It will disable the clip and all children completely. All you did was create 2 new properties on the movie clip that have no real effect."
905,A,GDI+ How to change Line SmoothingMode? Is it possible to change PowerPacks.LineShape smoothingMode? I tried to use this code(a class that inherits LineShape):  Protected Overrides Sub OnPaint(ByVal e As System.Windows.Forms.PaintEventArgs) Dim g As Graphics = e.Graphics ' no difference when changing the SmoothingMode ' g.SmoothingMode = SmoothingMode.AntiAlias Using pen As New Pen(Color.Blue 3) g.DrawLine(pen X1 Y1 X2 Y2) End Using ' MyBase.OnPaint(e) ' End Sub I always have the same result like this: ======= EDIT updated the test:  Protected Overrides Sub OnPaint(ByVal e As System.Windows.Forms.PaintEventArgs) Dim g As Graphics = e.Graphics Dim oldmode As SmoothingMode = g.SmoothingMode Using pen As New Pen(Color.Blue 3) g.SmoothingMode = SmoothingMode.AntiAlias g.DrawLine(pen X1 Y1 X2 Y2) g.SmoothingMode = SmoothingMode.None g.DrawLine(pen X1 + 50 Y1 X2 + 50 Y2) End Using g.SmoothingMode = oldmode g.Flush() 'MyBase.OnPaint(e)' End Sub Result (don't take in consideration labels and circles): apparently smoothing mode is not taken inconsideration... Are you saying the line does not get anti-aliased? Regardless of the setting? @Finglas: Yes. Exactly. The question was in my development mode: via Remote Desktop Connection on a virtual PC. The RDC does not take in consideration in my case the AntiAlias graphics property. more details: http://stackoverflow.com/questions/2370738/have-you-had-probelms-developping-on-a-virtual-pc Thanks everybody for participating sorry that is wasn't a really .NET problem. It's most likely color depth as RDP on Vista and up will definitely antialias. The difference being Vista supports high color depth.  The SmoothingMode should definitely impact your output Here's some settings I recently used for resizing an image with minimal quality loss: graphics.SmoothingMode = SmoothingMode.HighQuality; graphics.InterpolationMode = InterpolationMode.HighQualityBicubic; graphics.PixelOffsetMode = PixelOffsetMode.HighQuality; The InterpolationMode is probably not relevant for your example but the PixelOffsetMode might be. Let me spin up a quick test app. Update: Here's the quick test app SmoothingMode definitely impacts the lines I draw. private void Form1_Load(object sender EventArgs e) { foreach (var value in Enum.GetValues(typeof(SmoothingMode))) { _ComboBoxSmoothingMode.Items.Add(value); } foreach (var value in Enum.GetValues(typeof(PixelOffsetMode))) { _ComboBoxPixelOffsetMode.Items.Add(value); } _ComboBoxPixelOffsetMode.SelectedIndex = 0; _ComboBoxSmoothingMode.SelectedIndex = 0; } private void _ButtonDraw_Click(object sender EventArgs e) { using (Graphics g = _LabelDrawing.CreateGraphics()) { g.Clear(Color.White); if (_ComboBoxPixelOffsetMode.SelectedItem != null && (PixelOffsetMode)_ComboBoxPixelOffsetMode.SelectedItem != PixelOffsetMode.Invalid) { g.PixelOffsetMode = (PixelOffsetMode)_ComboBoxPixelOffsetMode.SelectedItem; } if (_ComboBoxSmoothingMode.SelectedItem != null && (SmoothingMode)_ComboBoxSmoothingMode.SelectedItem != SmoothingMode.Invalid) { g.SmoothingMode = (SmoothingMode)_ComboBoxSmoothingMode.SelectedItem; } using (Pen pen = new Pen(Color.Blue 3)) { g.DrawLines(pen new[] { new Point(0 0) new Point(25 50) new Point(_LabelDrawing.Width - 25 _LabelDrawing.Height - 50) new Point(_LabelDrawing.Width _LabelDrawing.Height) }); } } } SmoothingMode: AntiAlias None Update: As Morbo pointed out if the Graphics object presented to you in the PaintEventArgs isn't the same Graphics object that will ultimately be used for display then changing the smoothing might not have any effect. Although I would not expect such a drastic difference if that was a Graphics object from a memory Image or something. Wish I could offer more. Maybe if I understood better what the LineShape was giving you and your reasoning for using it over just using one of the Graphics.DrawLine() methods. The reason I question your use of the LineShape is that you are overriding it's OnPaint and drawing your own line. Seems like you could simplify your application and ditch the LineShape but maybe I'm missing something. Update: Ok that makes sense why you are using the LineShape then. Only suggestion I can offer at this point is to override OnPaint in your panel or LineShape try setting the smoothing mode there before calling the base event. Something like: protected override void OnPaint(PaintEventArgs e) { e.Graphichs.SmoothingMode = SmoothingMode.AntiAlias; base.OnPaint(e); } the SmoothingMode does not work ever. I can't ditch `LineShape`. It does HitTest(Clicks) MouseOver and repainting logic. Thanks Cory. Your code in the second update is almost the same I have in my example. I uncommitted MyBase.OnPaint(e) but don't see a difference between lines representation with or without AntiAlias for that LineShapes. thank you Cory. However I draw not simple lines but a overrides PowerPacks.LineShape OnPaint maybe this impacts. Edit. Read Morbo's comment. Will try to change the parent. Finally what I don't understand that what smoothing mode changing have any effect on the line drawing when I override the OnPaint and don't use the base.OnPaint logic at all for my custom line. @serhio: Not sure I understand. Are you saying that if you uncomment `MyBase.OnPaint(e)` the `SmoothingMode` works? @serhio: In your example who's OnPaint are you overriding? The `LineShape` or the `Panel`? (I don't even know that LineShape has an OnPaint) Maybe if you tried switching that? Grapsing at straws here since I don't have a good answer :-) LineShape is a GUI component so it have OnPaint. in my code I inherit the LineShape so I override the `LineShape`'s OnPaint. @serhio: Have you tried inheriting `ShapeContainer` and overriding it's `OnPaint` instead of the individual `LineShape` s? It's another shot in the dark...  From what I can tell in Reflector the PowerPack LineShape component paints using the Graphics object from the container's original Paint event. Changing properties on the Graphics object you're given could affect everything else in the container that paints after your shape. What are your LineShape objects contained within in your sample? (If it's a custom-painted control do you create a Bitmap at some point? How?) If they're inside of a custom control with a low color depth that could be the source of the problem. LineShape draws antialiased by default on my machine. +1: For digging under the hood to find out what the PowerPack LineShape is actually doing. I draw lineShapes on a custom panel. How do I change this panel drawing properties? As you can see I override OnPaint so e.Graphics isn't the parent graphics that are passed to? Also as You can see from my code I don't use base.OnPaint I seem to found the problem see here: http://stackoverflow.com/questions/2370738/have-you-had-probelms-developping-on-a-virtual-pc  What are your display settings set to? At least 24 bit color? I can confirm that deriving a class from LineShape and overriding OnPaint like you showed does in fact affect the line rendering as expected. I also can confirm that both Power Pack 3.0 and the version in Visual Studio 2010 both specifically use AntiAlias in the LineShape.DrawInternal method. I started with a blank .NET 2.0 Windows Forms app. I added the following class which is nearly identical to yours and a form which only contains the ShapeContainer and a diagonal MyLine shape. Imports Microsoft.VisualBasic.PowerPacks Public Class MyLine Inherits LineShape Protected Overrides Sub OnPaint(ByVal e As PaintEventArgs) Dim g As Graphics = e.Graphics 'g.SmoothingMode = SmoothingMode.AntiAlias ' Using pen As New Pen(Color.Blue 3) g.DrawLine(pen X1 Y1 X2 Y2) End Using 'MyBase.OnPaint(e) ' End Sub End Class If I run the project with the above code as-is the line is aliased (jagged). If I uncomment the SmoothingMode setting the line becomes antialiased (smooth). So it definitely should work. I know it seems like a dumb question but have you checked to make sure your code is getting hit in the debugger? Have you tried calling Graphics.Flush() immediately after your DrawLine? Also which version of the PowerPack are you using? Like I said I can see quite clearly in Reflector that LineShape.DrawInternal specifically sets the SmoothingMode to AntiAlias in a try/finally block. It restores the old smoothing mode before returning. But in your example you should never even hit this because you're not calling the base method. I use the latest version of PowerPack - 3.0. I made a test also on a other specially created new project... AntiAlias is working work there... something strange. Holala! I use a remote desktop connection.... and because of this. I seem to found the problem see here: http://stackoverflow.com/questions/2370738/have-you-had-probelms-developping-on-a-virtual-pc Ahh yes Remote Desktop is limited to 16 bit color before Vista. That would do it.
906,A,"GraphViz - How to connect subgraphs? In the DOT language for GraphViz I'm trying to represent a dependency diagram. I need to be able to have nodes inside a container and to be able to make nodes and/or containers dependent on other nodes and/or containers. I'm using subgraph to represent my containers. Node linking works just fine but I can't figure out how to connect subgraphs. Given the program below I need to be able to connect cluster_1 and cluster_2 with an arrow but anything I've tried creates new nodes instead of connecting the clusters: digraph G { graph [fontsize=10 fontname=""Verdana""]; node [shape=record fontsize=10 fontname=""Verdana""]; subgraph cluster_0 { node [style=filled]; ""Item 1"" ""Item 2""; label = ""Container A""; color=blue; } subgraph cluster_1 { node [style=filled]; ""Item 3"" ""Item 4""; label = ""Container B""; color=blue; } subgraph cluster_2 { node [style=filled]; ""Item 5"" ""Item 6""; label = ""Container C""; color=blue; } // Renders fine ""Item 1"" -> ""Item 2""; ""Item 2"" -> ""Item 3""; // Both of these create new nodes cluster_1 -> cluster_2; ""Container A"" -> ""Container C""; } I'm having the same problem yet they have a natural example where subgraphs are acting like nodes http://www.graphviz.org/content/fdpclust. @k102 I do know. Check out that page again; it says you need to use `fdp`. The linked example and the one above both work (the last line in the example here needs to use the subgraph names not the label and it might be nice to include line lengths for the graph); it's a little tight as is). @nlucaroni i wonder if this problem is solved. this example gives me wrong graph: edges connect centers of subgraph. don't you know how to make it to work like in the example? @nlucaroni Using `fdp` v2.28.0 and copy/pasting the source from the example the lines connect to the center of the subgraph not to the edges. If you open the .dot in OmniGraffle they are properly connected while `neato` and `dot` both create superfluous nodes for the cluster. The DOT user manual gives the following example of a graph with clusters with edges between clusters digraph G { compound=true; subgraph cluster0 { a -> b; a -> c; b -> d; c -> d; } subgraph cluster1 { e -> g; e -> f; } b -> f [lhead=cluster1]; d -> e; c -> g [ltail=cluster0lhead=cluster1]; c -> e [ltail=cluster0]; d -> h; } and edges between nodes and clusters. Thanks - that works but it really feels like an ugly hack. I'm **hoping** I don't have a scenario where I have a container with no nodes. In case anyone is interested in this can cause positioning problems if you have labelled links (edges). While the head or the tail of the edge may be hidden beneath a cluster the label is still positioned at the midpoint meaning some edge labels appear to be floating over a cluster instead of being positioned by the edge itself. This answer did help me out now I just have to work out whether I can use neato to lay the clusters out :) @WinstonSmith: Old question but I had a similar problem and solved it with an invisible dummy node per cluster that can be linked to even if the cluster is empty otherwise. `DUMMY_0 [shape=point style=invis]` @DevSolar that's most helpful. Thank you I found my inter-cluster edges to be collapsed to just arrow heads when using clusters that are only vertically connected. I fixed that with [minlen](http://www.graphviz.org/doc/info/attrs.html#d:minlen)=1 on the edges. c -> g [ltail=cluster0lhead=cluster1minlen=1];  Make sure you are using fdp layout for the file. I don't think neato supports clusters. I too have experientially found that the `neato` engine does not support clusters.. I'm not sure if this is a bug or not..  For ease of reference the solution described in HighPerformanceMark's answer applied directly to the original question looks like this: digraph G { graph [fontsize=10 fontname=""Verdana"" compound=true]; node [shape=record fontsize=10 fontname=""Verdana""]; subgraph cluster_0 { node [style=filled]; ""Item 1"" ""Item 2""; label = ""Container A""; color=blue; } subgraph cluster_1 { node [style=filled]; ""Item 3"" ""Item 4""; label = ""Container B""; color=blue; } subgraph cluster_2 { node [style=filled]; ""Item 5"" ""Item 6""; label = ""Container C""; color=blue; } // Edges between nodes render fine ""Item 1"" -> ""Item 2""; ""Item 2"" -> ""Item 3""; // Edges that directly connect one cluster to another ""Item 1"" -> ""Item 3"" [ltail=cluster_0 lhead=cluster_1]; ""Item 1"" -> ""Item 5"" [ltail=cluster_0 lhead=cluster_2]; } and produces output: Note that I changed the edges to reference nodes within the cluster added the ltail and lhead attributes to each edge specifying the cluster name and added the graph-level attribute 'compound=true'. Regarding the worry that one might want to connect a cluster with no nodes inside it my solution has been to always add a node to every cluster rendered with style=plaintext. Use this node to label the cluster (instead of the cluster's built-in ""label"" attribute which should be set to the empty string (in Python label='""""'). This means I'm no longer adding edges that connect clusters directly but it works in my particular situation. Note: 'graph [fontsize=10 fontname=""Verdana"" compound=true];' is essential - if you miss that linking to ltail/lhead does not work. @s.Daniel Thanks for emphasizing that you're absolutely right. @JonathanHartley As per your last paragraph is there any way to center that node right in the middle of the cluster?"
907,A,"Hole in a Polygon I need to create a 3D model of a cube with a circular hole punched at the center of one face passing completely through the cube to the opposite side. I am able to generate the vertices for the faces and for the holes. Four of the faces (untouched by the hole) can be modeled as a single triangle strip. The inside of the hole can also be modeled as a single triangle strip. How do I generate the index buffer for the faces with the holes? Is there a standard algorithm(s) to do this? I am using Direct3D but ideas from elsewhere are welcome. Just a thought - If you're into cheating (as done many times in games) you can always construct a regular cube but have the texture for the two faces you desire with a hole (alpha = 0) you can then either clip it in the shader or blend it (in which case you need to render with Z sort). You get the inside hole by constructing an inner cylinder facing inwards and with no caps. Of course this will only work if the geometry is not important to you.  To generate the index-buffer you want you could do like this. Thinking in 2D with the face in question as a square with vertices (±1 ±1) and the hole as a circle in the middle. You walk along the edge of the circle dividing it into some number of segments. For each vertex you project it onto the surrounding square with (x/My/M) where M is max(abs(x)abs(y)). M is the absolute value of the biggest coordinate so this will scale (xy) so that the biggest coordinate is ±1. This line you also divide into some number of segments. The segments of two succeeding lines you join pairwise as faces. This is an example subdividing the circle into 64 segments and each ray into 8 segments. You can choose the numbers to match your requirements. Here is some Python code that demonstrates this: from math import sin cos pi from itertools import izip def pairs(iterable): """"""Yields the previous and the current item on each iteration. """""" last = None for item in iterable: if last is not None: yield last item last = item def circle(radius subdiv): """"""Yields coordinates of a circle. """""" for angle in xrange(0subdiv+1): x = radius * cos(angle * 2 * pi / subdiv) y = radius * sin(angle * 2 * pi / subdiv) yield x y def line(x0y0x1y1subdiv): """"""Yields coordinates of a line. """""" for t in xrange(subdiv+1): x = (subdiv - t)*x0 + t*x1 y = (subdiv - t)*y0 + t*y1 yield x/subdiv y/subdiv def tesselate_square_with_hole((xy)(wh) radius=0.5 subdiv_circle=64 subdiv_ray=8): """"""Yields quads of a tesselated square with a circluar hole. """""" for (x0y0)(x1y1) in pairs(circle(radiussubdiv_circle)): M0 = max(abs(x0)abs(y0)) xM0 yM0 = x0/M0 y0/M0 M1 = max(abs(x1)abs(y1)) xM1 yM1 = x1/M1 y1/M1 L1 = line(x0y0xM0yM0subdiv_ray) L2 = line(x1y1xM1yM1subdiv_ray) for ((xaya)(xbyb))((xcyc)(xdyd)) in pairs(izip(L1L2)): yield ((x+xa*w/2y+ya*h/2) (x+xb*w/2y+yb*h/2) (x+xc*w/2y+yc*h/2) (x+xd*w/2y+yd*h/2)) import pygame def main(): """"""Simple pygame program that displays the tesselated figure. """""" print('Calculating faces...') faces = list(tesselate_square_with_hole((150150)(200200)0.5648)) print('done') pygame.init() pygame.display.set_mode((300300)) surf = pygame.display.get_surface() running = True while running: need_repaint = False for event in pygame.event.get() or [pygame.event.wait()]: if event.type == pygame.QUIT: running = False elif event.type in (pygame.VIDEOEXPOSE pygame.VIDEORESIZE): need_repaint = True if need_repaint: print('Repaint') surf.fill((255255255)) for papbpcpd in faces: # draw a single quad with corners (papbpdpc) pygame.draw.aalines(surf(000)True(papbpdpc)1) pygame.display.flip() try: main() finally: pygame.quit() MizardX I did not fully understand ""2. For each vertex you project it onto the surrounding square with (x/My/M) where M is max(abs(x)abs(y))."" And how did you generate the image?  I'm imagining 4 triangle fans coming from the 4 corners of the square.  Modern hardware usually can't render concave polygons correctly. Specifically there usually isn't even a way to define a polygon with a hole. You'll need to find a triangulation of the plane around the hole somehow. The best way is probably to create triangles from a vertex of the hole to the nearest vertices of the rectangular face. This will probably create some very thin triangles. if that's not a problem then you're done. if it is then you'll need some mesh fairing/optimization algorithm to create nice looking triangles.  Is alpha blending out of the question? If not just texture the sides with holes using a texture that has transparency in the middle. You have to do more rendering of polygons since you can't take advantage of drawing front-to-back and ignoring covered faces but it might be faster than having a lot tiny triangles. Great idea! But in my case things are a little more complicated and I will have to use a mesh.  You want to look up Tessellation which is the area of math that deals with what MizardX is showing.Folks in 3D Graphcs have to deal with this all the time and there are a variety of tessellation algorithms to take a face with a hole and calculate the triangles needed to render it."
908,A,"How do I display an image within a region defined in a Picturebox? As a follow on to my previous question I have gotten my region but spent the last two hours trying to display tiny pictures wihing that region alone; with the end goal being to be able to arbitarily display any number of images I choose. so far this is my code:  void OnPaintRadar(Object sender PaintEventArgs e) { Graphics g = e.Graphics; Bitmap blip = new Bitmap(tst_Graphics.Properties.Resources.dogtag); Rectangle radar_rect = new Rectangle(myRadarBox.Left + 80 myRadarBox.Left + 7 myRadarBox.Width - 200 myRadarBox.Height + 200); using (Pen drw_pen = new Pen(Color.White 1) ) { using (GraphicsPath path = new GraphicsPath()) { path.AddPie(radar_rect 180 180); path.CloseFigure(); using (Region rdRegion = new Region(path) ) { g.DrawPath(drw_pen path); g.Clip = rdRegion; PictureBox pb = new PictureBox(); pb.Image = (blip); pb.Size = blip.Size; g.DrawImage(blip radar_rect); } } } }// end paint method I have tried the DrawImageUnscaled method also but I either get a my tiny picture blown to fill the pie region or nothing is displayed. This line: pb.Image = (blip); is what's causing the tiny image to appear large. Basically you pulling a tiny bitmap out of resources and then setting the PictureBox's Image property to that bitmap (I'm assuming ""pb"" is a PictureBox on your form). Try commenting out that line and the line below it. No joy.Actually I just thought since I wasn't using the picturebox anyways commenting the lines out wouldn't affect anything since I'm adding the image not the picturebox to the region.  Click here to run a sample application that demonstrates the basics of how to do radar (or one way at least). Note: this application does not do double-buffering or transparency of the tiny image. Source code for the project is here. Thank you very much. I'll have a look at it and get back to you later."
909,A,Transparent bitmap I am having problem here with bitmaps..I want to remove the black background that my bitmap is having. I am creating bitmap bitmap from byte array..and then setting the bitmap in BitMapField..but the image shown has a black background. What language are you using? Use this constructor to create the Bitmap - Bitmap(int type int width int height) where type should be Bitmap.ALPHA_BITDEPTH_8BPP Seems like it's wrong can't use Bitmap.ALPHA_BITDEPTH_8BPP in Bitmap constructor it's for createAlpha()...  Could you post some code? Without some specific code we can't really help you. If you aren't yet you should be using createAlpha from the Bitmap class. Further explanation and help for common problems can be found in the Blackberry Support Forum.
910,A,Navigation graphics overlayed over video Imagine I have a video playing.. Can I have some sort of motion graphics being played 'over' that video.. Like say the moving graphics is on an upper layer than the video which would be the lower layer.. I am comfortable in a C++ and Python so a solution that uses these two will be highly appreciated.. Thank you in advance Rishi.. *Do* you have a video playing? How are you playing it? If not what kind of platform would you like to use? As-is I would say to use a shiny television screen and a mirror. Was the question really answered or not at all? I'm not sure I understand the question correctly but a video file is a sequence of pictures that you can extract (for instance with the opencv library C++ interface) and then you can use it wherever you want. You can play the video on the sides of an opengl 3D cube (available in all opengl tutorials) and other 3D elements around it. Of course you can also displays it in a conventional 2D interface and draw stuff on top of it but for this you need a graphical ui. Is it what you thought or am I completely lost? Yes Nikko you are completely right.. but more so in the second paragraph.. Display the video in a conventional 2D interface.. and then draw stuff on top of it.. And you said for that I'd need a graphical UI. Can you explain a bit more ? You need a graphical interface to display your images (like Qt gtkmm in c++ ...). But with a library like opencv (but there are others) you can extract images from a video file (but also a video device) modify them (like drawing lines add blur draw text) and then send it to your graphical interface. If you think opencv is a bit too complex you can edit your images with a library like ImageMagick (Magick++ for C++) which may be simpler and easy for basic tasks. i dropped the project..
911,A,"What is the most useful type of java.awt.image.BufferedImage for off-screen rendering? I am creating a buffered image that is going to be a snapshot of a JComponent (via paint()) and rendered inside an ImageIcon. There are a large amount of types in the BufferedImage(int width int height int imageType) constructor but which one should I use? I am sure that any of them would work but which ones are better than the others? How should I pick one? And Why? I think the question as it stands now is perfectly constructive. Heck the answer answered the question I came here with! I reworked it to better fit the best answer which points to an API that you should use. See GraphicsConfiguration.createCompatibleImage(int int) for a helper to create a BufferedImage of a ""good"" type among the many available types. How to get your hands on a GraphicsConfiguration instance to make this call? It depends on where your code is executing. See the many methods for getting your hands on a GraphicsConfiguration via methods like getGraphicsConfiguration() or getDeviceConfiguration(). If you only had a code snippet instead of a javadoc link you would so get the answer. and don't forget to fromat it properly by putting in four spaces in the front. be careful iterating over graphics configurations. depending on your hardware some of those methods take several seconds the first time. yes what is X? No check mark with such unsure code snippets. I will have to live forever without the sweet sweet feeling of this particular check mark. Oh what might have been... This is the easiest way `GraphicsEnvironment.getLocalGraphicsEnvironment().getDefaultScreenDevice().getDefaultConfiguration().createCompatibleImage(width height);`"
912,A,Transform pixel height of image to printing size used by graphics object As part of a print procedure of my application I'm trying to print a list of images scaled down to a specified width and placed one below the other. The problem is I can not figure out how to transform the height in pixels of the images to the height in the units used by the graphics object during printing. How do I calculate the imageHeightPrint variable correctly? This code snippet is the part of the image printing loop that scales down the image and calculates it's height and the placement of the next image. Image image = Image.FromStream(imageStream); // Get proportional correct height int imageHeight = image.Height * imageWidth / image.Width; Image imageToPrint = image.GetThumbnailImage(imageWidth imageHeight null IntPtr.Zero); float imageHeightPrint = e.Graphics.DpiY * imageToPrint.Height / imageToPrint.VerticalResolution; e.Graphics.DrawImage(imageToPrint e.MarginBounds.Left yPos); yPos += imageHeightPrint; I found the correct solution my self after dissecting the documentation. This line: float imageHeightPrint = e.Graphics.DpiY * imageToPrint.Height / imageToPrint.VerticalResolution; Should be changed into this: float imageHeightPrint = imageToPrint.Height / imageToPrint.VerticalResolution * 100; The biggest thing I missed was that the height-in-print should be in hundredths of an inch.
913,A,"How to combine repaints in Swing? I am calling repaint a bunch of times from a listeners but the way I designed my paint function is only one repaint is required. I generate a bunch of repaints since it hooked into my mouse motion listener. Is there a way to cancel all pending repaints for a certain component? I can't just start ignoring repaints since some are valid like when you resize the frame or restore it from minimize. Why do I care? Because my paint code is very heavy and I can't do full repaints at a very high FPS. I've hacked something similar together to improve how JFreechart decides to paint when it makes lots of calls to repaint. Essentially I do the following: Create a ScheduledExecutorService as a field in the class Receive first repaint request and and submit it to the executor to run on the EDT in say 50ms getting back the future Receive second request - test to see if the previous repaint has finished (fut.isDone()) and if so to schedule the next repaint; otherwise do nothing. This way you should get at most 20 repaint request per second. I've done similar things for batching up the number of calls to fireDataTableChanged when lots of changes are occuring at the same time.  My understanding is that repaint() simply schedules a repaint by adding a region of the component to the repaint queue. If a repaint has already been requested on a component the new repaint region will just be unioned with the previously requested regions. The repaint is not actually executed until all other events in the event queue are handled. So your additional repaints may not make much of a difference i.e. your painting code will only be executed once. See JComponent.repaint and RepaintManager.addDirtyregion.  Frequent repaint requests are automatically collapsed into one. The best way to optimize this is not to repaint the whole thing but call repaint with coordinates of specific area. This means you repaint only the area which actually changed.  Swing will combine repaints for you: see ""Painting in AWT and Swing"" on Sun's website. If you schedule a number of repaints in rapid succession they'll get combined into a single call to paintImmediately().  I hear what your saying. You can indeed repaint only a section of area. In the ""Performing Custom Painting Examples"" on the Sun website I found a useful example which shows how to draw a dragged rectangle and then only repaint that area when the mouse is moving or released. Here is the relevant section of code... public void mouseDragged(MouseEvent e) { updateSize(e); } public void mouseReleased(MouseEvent e) { updateSize(e); } /* * Update the size of the current rectangle * and call repaint. Because currentRect * always has the same origin translate it * if the width or height is negative. * * For efficiency (though * that isn't an issue for this program) * specify the painting region using arguments * to the repaint() call. * */ void updateSize(MouseEvent e) { int x = e.getX(); int y = e.getY(); currentRect.setSize(x - currentRect.x y - currentRect.y); updateDrawableRect(getWidth() getHeight()); Rectangle totalRepaint = rectToDraw.union(previousRectDrawn); repaint(totalRepaint.x totalRepaint.y totalRepaint.width totalRepaint.height); } This code is subject to copyright (see here for full code and copyright notice) See here for further example listings Truth be told I'm having a similar issue on FPS but that may be due to my currently poor code! I've learnt so much over the past few months that I can now make my code so more efficient. Hopefully I can overcome the FPS issue when more than 2 ""people"" slow down my graphics! Hummmm... I have only implemented the above code for the same section in my code and not others but by all means give it a try!"
914,A,OpenGL Color Interpolation across vertices Right now I have more than 25 vertices that form a model. I want to interpolate color linearly between the first and last vertex. The Problem is when I write the following code glColor3f(1.00.00.0); vertex3f(1.01.01.0); vertex3f(0.91.01.0); . .`<more vertices>; glColor3f(0.00.01.0); vertex3f(0.00.00.0); All the vertices except that last one are red. Now I am wondering if there is a way to interpolate color across these vertices without me having to manually interpolate color (instead natively like how opengl does it automatically) at each vertex since I will be having a lot more number of colors at various vertices. Any help would be extremely appreciated. Thank you! OpenGL will interpolate color within polygons but not across polygons.  OpenGL will interpolate colors of pixels between one vertex and the next but I don't know of any way to get it to automatically interpolate the values for intermediate vertexes. Normally that's not particularly difficult though -- you don't want to write code for each individual vertex anyway so adding the computation is pretty trivial: class pointf { GLfloat x y z; }; std::vector<pointf> spots; // ... GLfloat start_blue = 1.0f; GLfloat end_blue = 0.0f; GLfloat start_green = 0.0f; GLfloat end_green = 0.0f; GLfloat start_red = 0.0f; GLfloat end_red = 1.0f; GLfloat size = spots.size(); glBegin(GL_POLYGON); for (int i=0; i<spots.size(); i++) { GLfloat red = start_red + (end_red-start_red) * i/size; GLfloat green = start_green + (end_green-start_green) * i/size; GLfloat blue = start_blue + (end_blue-start_blue) * i/size; glColor3f(red green blue); glVertex3f(spots[i].x spots[i].y spots[i].z); } glEnd(); One thing though: this does purely linear interpolation per vertex. It does not for example attempt to take into account the distance between one vertex and the next. I'd guess questions of how to do things like this are (at least part of) why OpenGL doesn't attempt this on its own. Edit: for a gradient across a hemisphere I'd try something like this: // Blue light on the left GLfloat blue[] = {0.0f 0.0f 1.0f 1.0f}; GLfloat blue_pos[] = {-1.0f 0.0f -0.3f 0.0f}; // red light on the right GLfloat red[] = {1.0f 0.0f 0.0f 1.0f}; GLfloat red_pos[] = {1.0f 0.0f -0.3f 0.0f}; // turn on lighting: glEnable(GL_LIGHTING); glShadeModel(GL_SMOOTH); // Set up the two lights: glEnable(GL_LIGHT0); glLightfv(GL_LIGHT0 GL_DIFFUSE blue); glLightfv(GL_LIGHT0 GL_POSITION blue_pos); glEnable(GL_LIGHT1); glLightfv(GL_LIGHT1 GL_DIFFUSE red); glLightfv(GL_LIGHT1 GL_POSITION red_pos); // set up the material for our sphere (light neutral gray): GLfloat sph_mat[] = {0.8f 0.8f 0.8f 1.0f}; glMaterialfv(GL_FRONT_AND_BACK GL_AMBIENT_AND_DIFFUSE sph_mat); // Draw a sphere: GLUquadric *q = gluNewQuadric(); gluQuadricOrientation(q GLU_OUTSIDE); gluQuadricDrawStyle(q GLU_FILL); gluQuadricNormals(q GLU_SMOOTH); gluSphere(q 1.0 64 64); For the moment I've done the outside of a sphere but doing a hemisphere isn't drastically different. Thank you for your response. Your answer definitely makes sense. Now the question is can I apply this logic to a hemisphere? I have a huge hemisphere and I want to have various colors at different positions. I am not sure if this logic quite works so do you think I might be better off calculating color at individual vertices? Thank you once again for taking time. @gutsblow: Perhaps you'd be better off telling us a bit more about what you're really trying to accomplish. Right off it sounds like you'd be better off with a sphere in a neutral color with red and blue lights at appropriate positions. Sorry for being vague. Here is what I exactly have in mind a huge sphere/hemisphere that works as a skydome. I can apply textures to it but I want to have a gradient of colors which can be controlled by the position of the colors. Thanks again! Thank you very much. This method works really well. Since I am new to OpenGL I didn't know there is a direct function to create a sphere that makes my life a lot easier. Thank you!
915,A,"Overlapping System.Drawing.Graphics objects I have a WinForms control on which I want to display two things: An underlying image painstakingly loaded bit-by-bit from an external input device; and A series of DrawLine calls that create a visual pattern over that image. Thing #1 for our purposes doesn't change and I'd rather not have to redraw it. Thing #2 has to be redrawn relatively quickly as it rotates when the user turns another control. In my fantasy I want to put each Thing in its own Graphics object give #2 a transparent background and simply hit #2 with a rotational transformation to match the user control setting. But I don't see a way to make a Graphics object transparent nor a way to rotate what's already been drawn on one. So I'm probably asking Graphics to do something it wasn't designed for. Here's my question: What's the best way to set this up? Should I attempt to overlap my Graphics objects or is there some completely different and better way to do this that I'm not thinking of? GDI+ isn't retained-mode so you'll need to redraw the entire control on every Paint. So unfortunately you can't just have two ""things"" and apply a rotation to one of them. Your best bet with GDI+ is probably: Store #1 in an Image object. (You can pre-render elements to an Image by using Graphics.FromImage or by drawing on a pre-constructed Bitmap.) Store #2 as a set of coordinate pairs. Then in the Paint handler redraw #1 quickly using Graphics.DrawImage set a rotate transform using Graphics.RotateTransform and draw your lines. You should be able to make this appear smooth using double-buffering (ControlStyles.DoubleBuffer). As for ""completely different"" ways to do this well the ""fantasy"" you're describing is called Windows Presentation Foundation. WPF does have a retained-mode graphics system and might be able to handle the ""rotate one layer while keeping the other constant"" more conveniently. And you can host WPF in WinForms using the ElementHost control. The rough idea would be to use a Grid to overlay a Canvas on an Image add Line objects to the Canvas set the Canvas' RenderTransform to a RotateTransform and bind the RotateTransform's Angle to the other control's value. However this does raise project considerations (target platform learning curve) and also technical ones (initial overhead of loading WPF DLLs interop constraints). Both answers were awesome. I had to pick one. :-(  The Windows painting model is a good match for your requirements. It separates drawing the background (OnPaintBackground) from the foreground (OnPaint). That however doesn't mean that you can only paint the background once and be done with it. Window surface invalidation invokes both. This is above all required to make anti-aliasing effects work properly they can only look good against a known background color. Punt this and draw the Image in the OnPaintBackground() override. You can let Control do this automatically for you by assigning the BackgroundImage property. You'll probably need to set the DoubleBuffer property to true to avoid the flicker you'll see when the background is drawn temporarily wiping out the foreground pixels. Call Invalidate() if you need to update the foreground. To be complete your fantasy is in fact possible. You'd need to overlay the image with a toplevel layered window. That is easy to get with a Form whose TransparencyKey property is set. Here is a sample implementation: using System; using System.Drawing; using System.Windows.Forms; class OverlayedPictureBox : PictureBox { private Form mOverlay; private bool mShown; public event PaintEventHandler PaintOverlay; public OverlayedPictureBox() { mOverlay = new Form(); mOverlay.FormBorderStyle = FormBorderStyle.None; mOverlay.TransparencyKey = mOverlay.BackColor = Color.Magenta; mOverlay.ShowInTaskbar = false; } protected void OnPaintOverlay(PaintEventArgs e) { // NOTE: override this or implement the PaintOverlay event PaintEventHandler handler = PaintOverlay; if (handler != null) handler(this e); } public void RefreshOverlay() { // NOTE: call this to force the overlay to be repainted mOverlay.Invalidate(); } protected override void Dispose(bool disposing) { if (disposing) mOverlay.Dispose(); base.Dispose(disposing); } protected override void OnVisibleChanged(EventArgs e) { if (!mShown && !this.DesignMode) { Control parent = this.Parent; while (!(parent is Form)) parent = parent.Parent; parent.LocationChanged += new EventHandler(parent_LocationChanged); mOverlay.Paint += new PaintEventHandler(mOverlay_Paint); mOverlay.Show(parent); mShown = true; } base.OnVisibleChanged(e); } protected override void OnLocationChanged(EventArgs e) { mOverlay.Location = this.PointToScreen(Point.Empty); base.OnLocationChanged(e); } protected override void OnSizeChanged(EventArgs e) { mOverlay.Size = this.Size; base.OnSizeChanged(e); } void parent_LocationChanged(object sender EventArgs e) { mOverlay.Location = this.PointToScreen(Point.Empty); } private void mOverlay_Paint(object sender PaintEventArgs e) { OnPaintOverlay(e); } } One interesting artifact: minimizing the form and restoring it again looks erm special."
916,A,"Random number generation without using bit operations I'm writing a vertex shader at the moment and I need some random numbers. Vertex shader hardware doesn't have logical/bit operations so I cannot implement any of the standard random number generators. Is it possible to make a random number generator using only standard arithmetic? the randomness doesn't have to be particularly good! Use a linear congruential generator: X_(n+1) = (a * X_n + c) mod m Those aren't that strong but at least they are well known and can have long periods. The Wikipedia page also has good recommendations: The period of a general LCG is at most m and for some choices of a much less than that. The LCG will have a full period if and only if: 1. c and m are relatively prime 2. a - 1 is divisible by all prime factors of m 3. a - 1 is a multiple of 4 if m is a multiple of 4  If you don't mind crappy randomness a classic method is x[n+1] = (x[n] * x[n] + C) mod N where C and N are constants C != 0 and C != -2 and N is prime. This is a typical pseudorandom generator for Pollard Rho factoring. Try C = 1 and N = 8051 those work ok. that seems to be random enough thankyou! Sure. You don't need great statistical quality for graphics.  Believe it or not I used newx = oldx * 5 + 1 (or a slight variation of it) in several videogames. The randomness is horrible--it's more of a scrambled sequence than a random generator. But sometimes that's all you need. If I recall correctly it goes through all numbers before it repeats. It has some terrible characteristics. It doesn't ever give you the same number twice in a row. A few of us did a bunch of tests on variations of it and we used some variations in other games. We used it when there was no good modulo available to us. It's just a shift by two and two adds (or a multiply by 5 and one add). I would never use it nowadays for random numbers--I'd use an LCG--but maybe it would work OK for a shader where speed is crucial and your instruction set may be limited. hehe that's an interesting RNG the most mathematically easy to understand I've seen for sure! I'll have a play with this since as you say speed is crucial.  Vertex shaders sometimes have built-in noise generators for you to use such as cg's noise() function. HLSL does have a noise function which I could use: http://msdn.microsoft.com/en-us/library/bb509629(VS.85).aspx However whenever I have used it in the past it's never worked in fact until recently I believe it was marked as ""not implemented yet""!"
917,A,Collision detection with hardware generated primitives There's a lot of literature on collision detection and I've read at least a big enough portion of it to be fairly familiar with most techniques. However there's something that has eluded me for a while and I figured since StackOverflow provides access to a large group of brilliant minds at once I'd ask here first before digging around in the bookshelf. In this day and age more and more work is being delegated to GPU rather than CPU and in a lot of cases this is a good thing. For example there are geometry shaders to create new geometry or (slightly less new but still quite fascinating) vertex shaders to which you can through a bunch of vertexes at and something elegant will come out of it. What I was considering though as these primitives exists only on the graphics hardware how would you perform reliable collision detection with these primitives? Let's assume I have some kind of extremely simplified mesh which is displaced in a vertex shader (I don't have a concrete problem I'm more playing with the idea and I haven't gotten very deep into geometry shaders yet). What I've considered so far is separate 'rendering' passes from suitable angles with shading more or less turned off and perhaps lower resolution mesh rendering the inside (with faces flipped inward) of my second primitive along with the mesh I want to test against and executing an occlusion query for the mesh. If the mesh is completely occluded there'd be no intersection. This would of course require that my second primitive is convex. Somehow I get the feeling that this kind of test will be extremely expensive as the number of primitives increase (even if a large portion can be culled directly). Does anyone else have another idea or technique? I'm more familiar with opengl and cg than directx but if you have some examples or so in directx I guess I'll be able to figure out the opengl counterparts. All ideas are appreciated so please brainstorm. :) It sounds like Dan Horn's article “Stream Reduction Operations for GPGPU Applications” in GPU Gems 2 is exactly what you want. Like all chapters it's freely available online. Interesting I actually own this book (for a rather long time too...) but for some reason I've overlooked this section.
918,A,RayTracer project in C# When I was at university I took a subject in raytracing (graphics synthesis) and we got a chance to write a 3D ray tracer in C++. That was heaps of fun except I long ago lost my code in the great hard drive crash of '04'....many files were lost... Is there a RayTracer I can play with in C# that isn't written in one line? Something that's written with proper object orientation so I can extend it? Any help is appreciated Heh I also had great fun writing a ray tracer in c++ for a university course - and all the data was also lost in a hard disk crash in '04. Fortunately I had a backup of the actual code *whew* Damn you IBM DeathStar. User Andalmeida over at CodeProject has some nice articles on raytracing in C# which seems to fit the bill.
919,A,"C# GDI Invert Graphics flipped on the X axis I am drawing a series of Points using the Graphics class. I am reading from a Point array. For whatever reason the rendered image is upside down (flipped on the X axis). Is there a simple way to tell the Graphics class to draw ""upside down"" ? Many thanks. ""flipped on the X axis"" and ""upside down"" both mean that the image is mirrored *on* the X axis (*along* the Y axis) oops. removed my comment. don't know why I read it wrong. It sounds not like it's drawing upside down but like it's being given the coordinates upside down. Check that you're expecting coordinates with the right origin. 00 should be by default top-left of your screen. EDIT: You should be able to compensate by changing Y on each point as you draw it to use the formula Y = height - Y.  You should note that the default orientation or direction of graphics in most drawing systems is from top of display to the bottom of the display. To compensate for this you need to know the intended height of your viewing area and then subtract your coordinates from that height. This will reverse the orientation and set your figures relative to the bottom of the viewing area. Let's say you have two points specified as <xy> and a viewport that is 300 (w) x 200 (h). {{0 0} {100 200}} When drawing in the viewing area <x y> will get mapped to <x 200-y>. This gives us the following points. {{0 200} {100 0}} There are also mechanisms to switch the mapping mode to Cartesian style coordinates which will probably match your source data but is usually avoided because it will remap everything else that has already compensated for the default orientation. The SetMapMode() function in Windows can be used for this purpose if this is what you really wish. SetMapMode() @ MSDN Here's the P/Invoke signature of the function. (from pinvoke.net: setmapmode (gdi32) ) [DllImport(""gdi32.dll"")] static extern int SetMapMode(IntPtr hdc int fnMapMode); edit: There's one more way to accomplish this which I personally haven't tried. Graphics.TransformPoints Method @ MSDN This allows you to specify coordinate systems and the Graphics object will do the mapping for you. In your case you may wish to call it with CoordinateSpace.World or CoordinateSpace.Page as the first argument and CoordinateSpace.Device as the second. TransformPoints makes life simple. THANKS!"
920,A,What is wrong with this floodfill algorithm? I have been using a floodfill algorithm of the stack-based non-recursive variety and it seems to work perfectly except for one annoying situation: if use a line to cut an image in half and then floodfill one half it floods the whole image! This only happens however when I do not put 'borders' around the image. If i draw a rectangle that encapsulates the image (i.e put borders on the image) then it works fine. So obviously there is something wrong with the boundary finding aspect of the code but i cannot for the life of me find the problem. Can someone with (hopefully) a keener eye than me spot the problem? it's driving me crazy! (p.s the language is C) /** scanfill algorithm **/ /* the stack */ #define stackSize 16777218 int stack[stackSize]; int stackPointer; static bool pop(int * x int * y int h) { if(stackPointer > 0) { int p = stack[stackPointer]; *x = p / h; *y = p % h; stackPointer--; return true; } else { return false; } } static bool push(int x int y int h) { if(stackPointer < stackSize - 1) { stackPointer++; stack[stackPointer] = h * x + y; return true; } else { return false; } } static void emptyStack() { int x y; while(pop(&x &y 0)); } void scan_fill_do_action(int x int y texture_info * tex VALUE hash_arg sync sync_mode bool primary action_struct * payload) { action_struct cur; rgba old_color; int y1; bool spanLeft spanRight; if(!bound_by_rect(x y 0 0 tex->width - 1 tex->height - 1)) return; draw_prologue(&cur tex 0 0 1024 1024 &hash_arg sync_mode primary &payload); old_color = get_pixel_color(tex x y); if(cmp_color(old_color cur.color)) return; emptyStack(); if(!push(x y tex->width)) return; while(pop(&x &y tex->width)) { y1 = y; while(y1 >= 0 && cmp_color(old_color get_pixel_color(tex x y1))) y1--; y1++; spanLeft = spanRight = false; while(y1 < tex->height && cmp_color(old_color get_pixel_color(tex x y1)) ) { set_pixel_color_with_style(payload tex x y1); if(!spanLeft && x > 0 && cmp_color(old_color get_pixel_color(tex x - 1 y1))) { if(!push(x - 1 y1 tex->width)) return; spanLeft = true; } else if(spanLeft && x > 0 && !cmp_color(old_color get_pixel_color(tex x - 1 y1))) { spanLeft = false; } if(!spanRight && x < tex->width && cmp_color(old_color get_pixel_color(tex x + 1 y1))) { if(!push(x + 1 y1 tex->width)) return; spanRight = true; } else if(spanRight && x < tex->width && !cmp_color(old_color get_pixel_color(tex x + 1 y1))) { spanRight = false; } y1++; } } draw_epilogue(&cur tex primary); } have you solved it? Cause it's my problem now! I did only have a short glance at it but it seems you have a boundary wrap at if(!spanRight && x < tex->width && ... ´and else if(spanRight && x < tex->width && ... The lines should read  if(!spanRight && x < tex->width-1 && ... else if(spanRight && x < tex->width-1 && ... i have tried this already :( and it didn't fix the problem :((
921,A,Java Graphics disable xor mode I know that g.setXORMode(Color c) enables XOR mode drawing. But how to disable this mode after setXORMode call? g.setPaintMode()
922,A,C# Graphics class: get size of drawn content I am writing to Graphics object dynamically and don't know the actual size of final image until output all peaces to the Graphics. So I create a large image and create Graphics object from it:  int iWidth = 600; int iHeight = 2000; bmpImage = new Bitmap(iWidth iHeight); graphics = Graphics.FromImage(bmpImage); graphics.Clear(Color.White); How then I can to find the actual size of written content so I will be able to create a new bitmap with this size and copy the content to it. It is really hard to calculate the content size before drawing it and want to know is it any other solution. I don't understand your question. You create a new empty bitmap of a particular size. You get a graphical surface from that bitmap. You then fill that graphical surface with the color white. There is nothing here that would indicate a size change. The best solution is probably to keep track of the maximum X and Y values that get used as you draw though this will be an entirely manual process. Another option would be to scan full rows and columns of the bitmap (starting from the right and the bottom) until you encounter a non-white pixel but this will be a very inefficient process. int width = 0; int height = 0; for(int x = bmpImage.Width - 1 x >= 0 x++) { bool foundNonWhite = false; width = x + 1; for(int y = 0; y < bmpImage.Height; y++) { if(bmpImage.GetPixel(x y) != Color.White) { foundNonWhite = true; break; } } if(foundNonWhite) break; } for(int y = bmpImage.Height - 1 x >= 0 x++) { bool foundNonWhite = false; height = y + 1; for(int x = 0; x < bmpImage.Width; x++) { if(bmpImage.GetPixel(x y) != Color.White) { foundNonWhite = true; break; } } if(foundNonWhite) break; } Again I don't recommend this as a solution but it will do what you want without your having to keep track of the coordinate space that you actually use. Thanks! Keep tracking of the maximum X and Y values that get used as draw will work for me. I'm assuming that clipping off of the left or the top isn't going to happen? @Jacob: That was my assumption but either pattern could be fairly easily adapted to account for that as well.
923,A,"For ""draggable"" div tags that are NOT nested: JQuery/JavaScript div tag “containment” approach/algorithm? Background: I've created an online circuit design application where .draggable() div tags are containers that contain smaller div containers and so forth. Question: For any particular div tag I need to quickly identify if it contains other div tags (that may in turn contain other div tags). --> Since the div tags are draggable in the DOM they are NOT nested inside each other but I think are absolutely positioned. So I think that a ""hit testing"" approach is the only way to determine containment unless there is some ""secret"" routine built-in somewhere that could help with this. I've searched JQuery and I don't see any built-in routine for this. Does anyone know of an algorithm that's quicker than O(n^2)? Seems like I have to walk the list of div tags in an outer loop (n) and have an inner loop (another n) to compare against all other div tags and do a ""containment test"" (position width height) building a list of contained div tags. That's n-squared. Then I have to build a list of all nested div tags by concatenating contained lists. So the total would be O(n^2)+n. There must be a better way? Is there some critical reason the divs are not nested? I'm using JQuery div.draggable() so that I can drag circuit component around a workspace. I don't believe draggable() physically nests the div tags if I drag one inside another. $.contains(DivContainer LookForThisDiv); example: jQuery.contains(document.documentElement document.body); // true doc: $.contains() EDIT ok its pretty much useless if those divs are not nested (just read that) anyway it's a nice utility function.  I would use jQuery ""droppable"" as well in addition to ""draggable"". This way you can know where you drop something and can relocate the item in the DOM accordingly.. Have a look at jQuery draggable + droppable: how to snap dropped element to dropped-on element (explains how to remove the dropped element from its original place and add it to the drop target)"
924,A,"Changing a matrix from right-handed to left-handed coordinate system I would like to change a 4x4 matrix from a right handed system where: x is left and right y is front and back and z is up and down to a left-handed system where: x is left and right z is front and back and y is up and down. For a vector it's easy just swap the y and z values but how do you do it for a matrix? I may be confused here why can't you just swap the y and z values? And I assume you mean a 4x4x4 matrix since a 4x4 wouldn't have z. Just a 4x4 matrix. Just flipping the y and z translation values doesn't seem to work and a matrix also contains an operation for rotation so I'm asuming the problem is there. I think you'll need to clarify exactly what you want before anybody can answer your question. For a vector `(xyzw)` you've explained that to ""change from right-handed to left-handed"" means that you change it to the vector `(xzyw)` but it is not at all clear what that phrase means for a matrix. For example suppose a matrix takes the vector `(1234)` to the vector `(5678)` then when you ""change from right-handed to left-handed"" should it take `(1234)` to `(5768)` or should it take `(1324)` to `(5768)` or did you mean for it to do something else? Isn't your question formulated incorrect? You're not trying to switch from right-handed > left-handed coordinate system you're just changing which axis that is up. Because i imagine in both instances that the positive axis related to front and back points towards the viewer from origo? I think I understand your problem because I am currently facing a similar one. You start with a world matrix which transforms a vector in a space where Z is up (e.g. a world matrix). Now you have a space where Y is up and you want to know what to do with your old matrix. Try this: There is a given world matrix Matrix world = ... //space where Z is up This Matrix changes the Y and Z components of a Vector Matrix mToggle_YZ = new Matrix( {1 0 0 0} {0 0 1 0} {0 1 0 0} {0 0 0 1}) You are searching for this: //same world transformation in a space where Y is up Matrix world2 = mToggle_YZ * world * mToggle_YZ; The result is the same matrix cmann posted below. But I think this is more understandable as it combines the following calculation: 1) Switch Y and Z 2) Do the old transformation 3) Switch back Z and Y If there is need to mirror an axis you can do this by adding this matrix in your world matrix multiplication: Matrix mMirror_X = new Matrix( {-1 0 0 0} {0 1 0 0} {0 0 1 0} {0 0 0 1}) thanks Gerrit it worked great.  Since this seems like a homework answer; i'll give you a start at a hint: What can you do to make the determinant of the matrix negative? Further (better hint): Since you already know how to do that transformation with individual vectors don't you think you'd be able to do it with the basis vectors that span the transformation the matrix represents? (Remember that a matrix can be viewed as a linear transormation performed on a tuple of unit vectors) Thanks but actually it isn't homework and I don't really know any of the math behind matrices so I don't actually understand any of that. Okay to be more pragmatic and less mathematical start with the fact you can treat the rotation and translation seperately.. (i'm assuming an affine transformation)  Change sin factor to -sin for swaping coordinate spaces between right and left handed  It depends if you transform your points by multiplying the matrix from the left or from the right. If you multiply from the left (e.g: Ax = x' where A is a matrix and x' the transformed point) you just need to swap the second and third column. If you multiply from the right (e.g: xA = x') you need to swap the second and third row. If your points are column vectors then you're in the first scenario.  Let me try to explain it a little better. I need to export a model from Blender in which the z axis faces up into OpenGL where the y axis faces up. For every coordinate (x y z) it's simple; just swap the y and z values: (x z y). Because I have swapped the all the y and z values any matrix that I use also needs to be flipped so that it has the same effect. After a lot of searching I've eventually found a solution at gamedev: If your matrix looks like this: { rx ry rz 0 } { ux uy uz 0 } { lx ly lz 0 } { px py pz 1 } To change it from left to right or right to left flip it like this: { rx rz ry 0 } { lx lz ly 0 } { ux uz uy 0 } { px pz py 1 }"
925,A,"How to move a bitmap object around a screen and have multiple objects using the same bitmap See above. I need to move my bitmaps around a Form or perhaps inside a PictureBox in a form. I have not been able to find any tutorials on this specific subject and even the base GDI+ stuff is a bit confusing. I am looking for a simple and THOROUGHLY explained way on how to do this. I am needing this for a rendering engine for an 8-bit game I am collaborating on. GDI+ may not be the best option. Sprites are generally drawn using ""blitting"". However I've read (perhaps outdated?) claims that GDI+ blit operations are slow because they're not hardware accelerated. If this is 8-bit game rendering maybe you don't care - but maybe you could use SDL.NET. In particular try this tutorial. You should probably follow that first bit of advice (do the hello world first) but this is worth a skim through first to see if it fits. EDIT - actually there's not much tutorial there ATM. The main SDL docs (for the C version) are probably OK but it's still a bit of a pain. Oh well. What I mean is the SDL C docs are probably adequate for working with the SDL.NET library in C#. Not ideal but - depending on how many other things your juggling at the same time - possibly usable. Since the tutorial is a wiki I may expand it a bit myself but I'll have to get back into practice with C# first or I'll just screw things up anyway. So.... C doesn't really work for me and you're right there's not much there."
926,A,3D Graphics Tutorials I want to start writing simple iphone apps to simulate in 3D things like rolling dice playing billiards dropping objects from a certain height throwing a ball etc. Just basic motion simulation in 3D. I've searched and searched for a tutorial that would get me started here without success. A perfect intro tutorial for me would be a project that simulates the rolling of 2 dice in 3D. Can anyone point me in the appropriate direction? Many thanks in advance! Feel free to check out my demo with a rolling billiard ball. Youtube clip The source code is included in the demo zip available here  I just found this site which seems to have a great set of tutorials for opengl es: Opengl es tutproals  THe NeHe site contains alot of tutorials many of them using physics in 3D. This should bootstrap your learning. The examples are for OpenGL and come in many languages (including Objective-C I think). The physics will probably not be that accurate. This site and also this one give some hint about the physics of rolling dice. However speaking to a friend physicist about this he recons it would be simpler to use a statistical approach and then mock the physics because there are a lot of variables at work and there isn't much point simulating an event designed to be random-ish. But then again Im no physicist. I also found this iPhone app which uses dice simulation. Might be some inspiration.  OpenGL ES from the Ground Up is a site dedicated to OpenGL ES on the iPhone. It includes some of the tutorials from the NeHe site ported to the iPhone. I don't know how much of the information will be relevant to motion simulation but it's definitely worth a read for learning OpenGL.
927,A,How to draw paths specified in terms of straight and curved motion I have information on paths I would like to draw. The information consists of a sequence of straight sections and curves. For straight sections I have only the length. For curves I have the radius direction and angle. Basically I have a turtle that can move straight or move in a circular arc from the current position (after which moving straight will be in a different direction). I would like some way to draw these paths with the following conditions: Minimal (preferably no) trigonometry. Ability to center on a canvas and scale to fit any arbitrary size. From what I can tell GDI+ gives me number 2 Cairo gives me number 1 but neither one makes it particularly easy to get both. I'm open to suggestions of how to make GDI+ or Cairo (preferably pycairo) work and I'm also open to any other library (preferably C# or Python). I'm even open to abstract mathematical explanations of how this would be done that I can convert into code. For 2D motion the state is [x y a]. Where the angle a is relative to the positive x-axis. Assuming initial state of [0 0 0]. 2 routines are needed to update the state according to each type of motion. Each path yields a new state so the coordinates can be used to configure the canvas accordingly. The routines should be something like: //by the definition of the state State followLine(State s double d) { State s = new State(); s.x = s0.x + d * cos(s0.a); s.y = s0.y + d * sin(s0.a); s.a = s0.a; return s; } State followCircle(State s0 double radius double arcAngle boolean clockwise) { State s1 = new State(s0); //look at the end point on the arc if(clockwise) { s1.a = s0.a - arcAngle / 2; } else { s1.a = s0.a + arcAngle / 2; } //move to the end point of the arc State s = followLine(s1 2 * radius * sin(arcAngle/ 2)); //fix new angle if(clockwise) { s.a = s0.a - arcAngle; } else { s.a = s0.a + arcAngle; } return s; } This is definitely helpful. I just need to figure out now how to take the state before and after following a circle and convert that into the arguments for some graphics library.
928,A,"What's the best way of working with an artist on a WPF application? I'm about to finish the core components of my application and I'm willing to change gears soon and do a bit of GUI programming. From the beginning I decided to write a WPF-based application for two reasons: a) I want my application to be visually stunning and 100% skineable. b) I completely suck at designing user interfaces and then some more. So the plan is to have a GUI artist involved in the project and let him take over every visual aspect of it. However I've never worked in a WPF project before and have no clue on how to deal with this. I'd like to hear from people with experience in this field who can give some advice. Thanks. Edit: I didn't mention it but yes the artist will be using the MS Expression Blend software. I'm more interested in knowing how to work with him rather than what tools he'll be using. Make sure your project is using the MVVM pattern. This makes it much easier for the UI designer to do their work without affecting other parts of the application. Use Commands rather than Events and expose them to binding in the VM Avoid any code in the View's code behind. When you use commands the only thing that really needs to be there is a line setting the window's DataContext to the ViewModel. Occasionally you'll also have event listeners to listen to ViewModel events to pop new windows. Use Databinding as much as possible it just makes everything work much more smoothly. Technically this is indeed the way to go. If you as developer use these patterns a maximum decoupling of UI and business code will be achieved.  Always create a fully functional GUI first and then pass it over to the artist to ""skin"" it - this way the internal structure of the XAML will be right for all the logic and data binding. If you let the artist design the skin and then add the logic you are very likely to get XAML you can't use directly and you'll be back in the old ""try to make the app look like the pretty picture"" that WPF is supposed to solve. Of course you will have to get all the usability aspects working (arrange elements by user workflow support keyboard navigation etc) before you pass the files to the artist and probably again after you get the results back.  Best way to use Expression Blend for the Designer to create the UI. Then you can hook up the events. The is the goal of MS with Sliverlight and XAML. The Expression Designer tools are for just that. Make sure the design meets the final product everytime becuase the developer and the designer use tools to create the correct XAML everytime. You have to go beyond just using Blend. You also have to name your controls in such a fashion that they can be swapped out/skinned etc. The Code-Behind has to only use these by names. e.g. PART_Thumb in certain .Net controls.  Expression Blend is your answer as Longhorn213 says. The brilliant thing with Blend is that it uses the same project/solution format as Visual Studio but basically focuses on the XAML. That way the designer can play about with the application as much as he/she likes without messing up your code. Your tasks would be much more than 'hooking up the events' though. More likely you would work alternately even simultaneously on the project. You would provide datasources events etc. which he/she would consume and trigger. Perhaps he/she would want some custom controls that you would build. And so on. The VS/Blend combination is a wonderful new world finally enabling real collaboration between developers and designers throughout the project lifecycle. Hurrah!"
929,A,Can Google O3D be used as a Rich Internet Application framework? Although it's pegged as a 3D graphics framework for the browser could Google O3D be used as an RIA framework similar to GWT Flex or Rails? It could but there are no user interface widgets that can draw on the O3D window. This is a classic game development problem - it is really complicated to get common UI libraries to actually render properly over top of a hardware accelerated 3d window. Even more so if you want to render windows in world space attached to objects with proper sorting. Typically games use custom UI engines. O3D does have a high performance Canvas for 2d drawing and it would be quite possible to write a UI library in JS that could draw on that canvas. But... that would be like re-writing Flex in JS. The upside of that would be tight integration between 2d and 3d content - and the ability to have complex UIs both in the UI layer and attached to objects in the 3d scene with render-to-texture tech.  Short answer: Yes. Long answer: More likely it'll just be used for in-browser games. It takes a lot of work to build a decent RIA framework on top of a 3d framework and it's unlikely such a framework (if it's ever released) will be very developer friendly. If you want good a RIA framework look at Silverlight or Flex.
930,A,"magnetism simulation Say I have p nodes on a n by m pixel 2D surface I want the nodes to be attracted to each other such that the further they are apart the strong the attraction. But if the distance between two nodes say d(AB) is less than some threshold say k then they start to repel. Could anyone get me started on some code on how to update the co-ordinates of the nodes over time. I have something a little like the code below which is start to do the attraction but looking for some advice. (P.S. I can not use an existing library to do this). public class node{ float posX; float posY; } public class mySimulator{ ArrayList<node> myNodes = new ArrayList<node>(); // Imagine I add a load of nodes to myNodes myNodes.add(..... // Now image this is the updating routine that is called at every fixed time increment public void updateLocations(){ for(int i =0; i <= myNodes.size(); i++){ for(int i =0; i <= myNodes.size(); i++){ myNodes.get(i).posX = myNodes.get(i).posX + ""some constant""*(myNodes.get(j).posX -myNodes.get(i).posX); myNodes.get(i).posY = myNodes.get(i).posY + ""some constant""*(myNodes.get(j).posY -myNodes.get(i).posY); } } } } } Is this homework? Yu would probably need to solve a few DiffEqs to get there. Simulating it yourself might get imprecise - you have to worry about speed etc. Wait a second - do these nodes have mass? This is important as massless simulations are easier to do. You could borrow ideas from this guy's code http://www.myphysicslab.com/runge_kutta.html Quick reply to say no it's not homework. And they can be treated as having no mass / identical mass if that makes it easier This kinetic model of elastic collisions is completely unrelated to magnetism but the design might give you some ideas on modeling an ensemble of interacting particles. Excellent that is what I was looking for completely off with magnetism so thank you for reading my posts carefully :D  Say I have p nodes on a n by m pixel 2D surface I want the nodes to be attracted to each other such that the further they are apart the strong the attraction. But if the distance between two nodes say d(AB) is less than some threshold say k then they start to repel. You realize of course that this is not how the physics of magnetism work? Could anyone get me started on some code on how to update the co-ordinates of the nodes over time. Nobody will be able to give you code to do this easily because it's actually a difficult problem. You can numerically integrate the ordinary differential equations for each particle over time. Given initial conditions for position velocity and acceleration vectors in 2D you'll take a time step integrate the equations to get the values at the end of the time step update the values by adding the increment and then doing it again. It requires some knowledge of 2D vectors numerical integration ordinary differential equations linear algebra and physics. Do you know anything about those? Even if you ""make up"" your own physical laws governing the interactions between your particles you'll still have to integrate that set of equations. I'd recommend looking at Runge-Kutta for systems of ODEs. ""Numerical Recipes"" has a nice chapter on it even if you go elsewhere for the implementation. ""NR"" is now in its third edition. It's a bit controversial but the prose is very good. Thanks for the reply I'll have a quick look through NR. I know that's not how magnetism works as the topic is a little tricky it was the best title I could think of :D"
931,A,Is there a good integer only line rasterization algorithm? I've been working on building a simple 3d graphic engine and I'm trying to find a good integer based line rasterization algorithm. ( I'm not trying to re-invent the wheel I'm trying to get a deeper understanding of wheels). Are there any line rasterizing algorithms that don't rely on any floating point math? Thanks. Bresenham's line algorithm is integer only. Thanks cletus! This helped a lot.
932,A,Drawing a scatterplot with Cocoa How should I best approach drawing a scatter plot diagram in Cocoa? You will probably want to create a custom subclass of NSView and override the drawRect: method. You can draw primitives using many of NSBezierPath's class methods such as bezierPathWithOvalInRect: and strokeLineFromPoint:toPoint:  I believe Core Plot offers scatter plots.
933,A,"Handling selection of shapes on a board I have a board as a canvas with several shapes drawn on it some of them are triangles circles rectangles but all are contained inside their own bound delimited rectangle. ""The circle will be inside a rectangle"" I put two circles A B on the board where A is over B and has some area colliding. If I click on A area corresponding to the container box but not the actual A circle shape area I wont select the A circle however that would stop me from selecting B since my A container overlaps and is over B one. In an event base framework the child event will go to the parent not siblings I guess. So my choice was to do a check for all shape container which have some area at point x ordered by z index. Then for each container check if the shape inside it collide. It does not seem super efficient but is there any other ways? --------- | -------- | | | -----| | -------- There are tricks you can play if you really need speed. For example: If you are working with a deep pallet you can use the low order bits of the color to tag the objects. Then peeking at the pixel gives you the object or at least lest you quickly drastically cull the list. Even at low bit depth if the objects are all monochrome you can use the whole color If you're in low enough resolution you can keep an array that says what object owns what pixel. At higher res you can do the same but use RLE to keep the size down (also look into quad trees) And so forth... If it's just simple implementation you're after one quick trick is to record the X & Y repaint the screen and note which object paints that pixel. -- MarkusQ  You're handling it about as well as it can be handled - windowing systems generally obey Z order (layers). This will be better in the long run anyway especially if you want to be able to select multiple items by drawing a selection box around them. There are algorithms for finding if rectangles overlap by converting them into 2d representations on both the x and y axis. You can do the same thing and then compare your point to see which objects your point overlaps: http://stackoverflow.com/questions/115426/algorithm-to-detect-intersection-of-two-rectangles Just treat your point selection (or rectangle selection if you draw a bounding box to select multiple items) as another rectangle to be compared as overlapping to the others."
934,A,Graphics in ASP.Net (c#) I need help with my latest asp.net project. It involves graphics. I need to draw a circle and have several lines going from the middle to the edge (radius). This is the part I know how to do. The next part is the part I don’t know how to do and would appreciate some advice. The users need the ability to grab the lines with their mouse and move them. For example if a line is pointing to the 90 degree mark they might want to grab it and move it to the 45 degree mark. This will affect calculations that will display elsewhere on the page. I will consider third party controls but would prefer a solution with coding or open source controls. Thanks in advance. Bob Avallone You could possibly use an svg to do the work for you maybe modifying the work on http://www.maa.org/joma/Volume7/Lane/Developer.html the specific example is: http://www.maa.org/joma/Volume7/Lane/Thales.svg +1 nice solution. I want to thank everyone who replied. This solution by John looks the most promising. I will pursue it and let you all know how it works out. Bob I like this solution but have to mention that IE doesn't support svg. Yes David I found that out. This is why I am now looking for a JavaScript solution.  ASP.net runs on the server - it simply generates HTML (& JavaScript) which is sent to the client (eg a browser) and rendered there. So what you need is not an asp.net solution but indeed a rich client solution. Some of your options are: Javascript Flash Silverlight Java applet I suspect Javascript or Silverlight will be of the most interest to you. Perhaps you want to do a little investigation and come back with an updated question. UPDATE Re: Bob's Comment: A quick google found this which seems pretty cool: Lightweight Visual Thesauras I think I would like to pursue JavaScript as a solution. Can anyone point me to an example that is similar to what I what to do? @Bob see updated answer I've looked at Javascript as a solution and at this point I do not want to pursue that avenue. Silverlight now looks like a better avenue to pursue. The same question applies does anyone know of an example that is similar to what I am trying to do?  If you don't need to be constrained to ASP.Net this problem is much more easily solved via a Forms app. You can net-enable it and distribute it as a ClickOnce app to get the same degree of external maintainability (if that is why you are using ASP.Net). It doesn't have to be asp.net but they are looking for a web solution. I have been asked to convert a Window app. that is working but for which they don't have the source code. Bob
935,A,"Fully transparent windows in Pygame? Is it possible to get a fully transparent window in Pygame (see the desktop through it)? I've found how to create a window without a frame but there doesn't seem to be any obvious way to make it transparent. I'd be willing to tie into system-specific technology/frameworks as long as there are solutions for both Windows and Mac OS X but I'm not sure which direction to be looking. The only topic I was able to find recommended using wxPython which isn't something I can do for this particular project (needs to be Pygame). PyGame uses SDL which does not support transparent windows. Although at least on Linux making it transparent is done by the window manager not the application. True generally the WM does provide transparency effects but applications can also explicitly enable transparency. urxvt is an example of such an application. Sucks that the answer is ""no"". Darned limitations of external libraries! Really urxvt is fake-transparent. It can only blend with your desktop background not the other windows. So the answer is totally NO."
936,A,"OpenGL glGetTexImage2d type parameter? reading the docs i see that the glGetTexImage2d() function has a 'type' parameter. The docs say that the type parameter ""specifies the data type of the pixel data"" and gives some examples of types such as GL_INT GL_BYTE etc. but what does it mean precisely when the image format is GL_RGBA and GL_INT ? is it an int for each channel? or an int for the whole color? and if it's an int for th whole color then isn't that really the same as GL_BYTE ? since there's 4 bytes in an int which makes each channel a byte each The type parameter specifies the effective type of the data inside the buffer you're sending to OpenGL. The aim here is that OpenGL is going to walk in your buffer and want to know how much elements are present ( width * height * internalformat ) and their size & interpretation (type). For instance if you are to provide an array of unsigned ints containing red/green/blue/alpha channels (in this order) you'll need to specify: target: GL_TEXTURE_2D level: 0 (except if you use mipmaps) internalformat: 4 because you have red green blue and alpha width: 640 height: 480 border: 0 internal format: GL_RGBA to tell opengl the order of our channels and what they mean type: GL_UNSIGNED_INT will let opengl know the type of elements inside our array pixels: a pointer to your array so when i specify GL_UNSIGNED_INT then each and every color component (r g b a ) is an int (i.e 32 bits) or just the color as a 'whole' is 32 bits? Yep when specifying GL_UNSIGNED_INT each element (channel) is an unsigned integer.  It's an int per channel. RGBA means each pixel has R G B and A ints (if you set it to int) in the data-array you're giving it. RBGA (if it exists not sure of that) would also mean four ints but ordered differently. RGB would mean just three (no alpha channel)."
937,A,"frames behind pictures? Hey there my question is related to HTML and PURE html. im working on a new design for my front page on a social network called kwick and i want to create a kind of space for my favorite Video at the moment. now my problem is how do i realize sth like this: i take a picture of my ipod like that(just an example its not my ipod) http://www.clein-online.de/schmuck_projekte/files/ipod_nano_schutzhuelle.jpg now i want to create this pic as gif and cut out the display now behind that cutten out area there should be a youtube video be shown in a way that the embed player is NOT shown completly i mean the player is there completly but the picture is a layin on it. does some1 has a idea how to do this? thanks in advance!! What parts of the player do you want to hide? There may be an option to turn these parts off in the embed code editor on youtube's site. maybe the contol and timeline i only need the play button which is in the beginning of the video in the middle of the screen. and i want to have the complete ipod-screen be filled with the video so i mean i would prefer cutting 20 pixels away dann having a 20 pixel border around the vid. Your biggest hurdle will be ensuring that your WMODE = transparent. Setting WMODE to transparent will keep your flash crap from always appearing on top of other content. Then simply position an image with transparency in the correct place over the video. Since you haven't included any information on how precisely you're including your flash file I can't really tell you what method you'll need to use to set the WMODE. Just do some googling and all will be clear.  You don't need to cut the image at all. Make a div that has enough space for the whole picture of the ipod and use CSS to make it the background image (can be inline styles if you can't add your own CSS files). <div style=""background:url(ipod.jpg); width:300px; height:400px;""> <iframe src=""..."" style=""margin: 25px auto 0;"" width: 250px; height: 200px;></iframe> </div> Apply margin width and height to your iframe or embed or object to position it within the div that has the ipod background image. You can set the iframe to the size you need and make it hide any overflow if necessary but I don't think you're going to be able to hide parts of the flash if you don't have control of the embed code."
938,A,Does OpenGL stencil test happen before or after fragment program runs? When I set glStencilFunc( GL_NEVER . . . ) effectively disabling all drawing and then run my [shader-bound] program I get no performance increase over letting the fragment shader run. I thought the stencil test happened before the fragment program. Is that not the case or at least not guaranteed? Replacing the fragment shader with one that simply writes a constant to gl_FragColor does result in a higher FPS. It's actually a bit of both. Per fragment operations should happen after the fragment program as you can see in this OpenGL ES 2.0 pipeline diagram. However many modern graphic cards have an early z test that discards fragments earlier as long as you don't write to the depth in the fragment shader. Here is a paper from AMD/ATI that talks about such tests. I remember reading that the spec allows early tests as long as doing them before the shader produces the same result as doing them after which is why you wouldn't want to modify the depth or discard a fragment in the shader. This thread on OpenGL forums has some interesting discussion about it.  In addition to fragment depth modification there are a few other things that can prevent the depth/stencil test from happening before the fragment shader. If z-writes are enabled then any method of aborting the fragment in the shader will do this such as alpha-test or the discard shader instruction. If the GPU wants to do the stencil/z test in the same operation as the z/stencil write it has to wait until after the fragment shader executes so that it knows the fragment is allowed to write to the z-buffer. This may vary between different cards though. At least it should be easy to tell if it's your current problem.  As you can see here: http://www.opengl.org/wiki/Stencil_Test Stencil test is run after fragmentShader. I understand that it is not good for performance. Is it a answer or comment?  Take a look at the following outline for the DX10 pipeline it says that the stencil test runs before the pixel shader: http://3.bp.blogspot.com/_2YU3pmPHKN4/Sz_0vqlzrBI/AAAAAAAAAcg/CpDXxOB-r3U/s1600-h/D3D10CheatSheet.jpg and the same is true in DX11: http://4.bp.blogspot.com/_2YU3pmPHKN4/S1KhDSPmotI/AAAAAAAAAcw/d38b4oA_DxM/s1600-h/DX11.JPG I don't know if this is mandated in the OpenGL spec but it would be detrimental for an implementation to not do the stencil test before running the fragment program. According to this page: http://www.opengl.org/wiki/Rendering_Pipeline_Overview#Pipeline the stencil and depth tests can run before the fragment shader if the depth is not modified in the fragment shader. However this does not appear to be happening in my case (I am not writing to gl_FragDepth in the fragment shader).
939,A,"Java 2D Drawing Optimal Performance I'm in the process of writing a Java 2D game. I'm using the built-in Java 2D drawing libraries drawing on a Graphics2D I acquire from a BufferStrategy from a Canvas in a JFrame (which is sometimes full-screened). The BufferStrategy is double-buffered. Repainting is done actively via a timer. I'm having some performance issues though especially on Linux. And Java2D has so very many ways of creating graphics buffers and drawing graphics that I just don't know if I'm doing the right thing. I've been experimenting with graphics2d.getDeviceConfiguration().createCompatibleVolatileImage which looks promising but I have no real proof it it's going to be any faster if I switch the drawing code to that. In your experience what is the fastest way to render 2D graphics onto the screen in Java 1.5+? Note that the game is quite far ahead so I don't want to switch to a completely different method of drawing like OpenGL or a game engine. I basically want to know how to get the fastest way of using a Graphics2D object to draw stuff to the screen. Just out of curiosity what is the game about? :D Re: Mario Ortegon: http://www.metalbeetle.com/spaceexploration/ . I didn't put a link into the main since that would just be stealth advertising. Here are some tips off the top of my head. If you were more specific and what you were trying to do I may be able to help more. Sounds like a game but I don't want to assume. Only draw what you need to! Don't blindly call repaint() all of the time try some of the siblings like repaint(Rect) or repaint(xywh). Be very careful with alpha blending as it can be an expensive operation to blending images / primitives. Try to prerender / cache as much as possible. If you find yourself drawing a circle the same way over and over consider drawing in into a BufferedImage and then just draw the BufferedImage. You're sacrificing memory for speed (typical of games / high perf graphics) Consider using OpenGL use JOGL of LWJGL. JOGL is more Java-like whereas LWJGL provides more gaming functionality on top of OpenGL access. OpenGL can draw orders of magnitude (with proper hardware and drivers) than Swing can.  An alternative if you don't want to go pure Java 2D is to use a game library such as GTGE or JGame (search for them on Google) then offer easy access to graphics and also offer double buffering and much simpler drawing commands.  I've been watching this question hoping someone would offer you a better answer than mine. In the meantime I found the following Sun white paper which was written after a beta release of jdk 1.4. There are some interesting recommendations here on fine-tuning including the runtime flags (at the bottom of the article): ""Runtime Flag For Solaris and Linux Starting with the Beta 3 release of the SDK version 1.4 Java 2D stores images in pixmaps by default when DGA is not available whether you are working in a local or remote display environment. You can override this behavior with the pmoffscreen flag: -Dsun.java2d.pmoffscreen=true/false If you set this flag to true offscreen pixmap support is enabled even if DGA is available. If you set this flag to false offscreen pixmap support is disabled. Disabling offscreen pixmap support can solve some rendering problems. ""  make sure you use double buffering draw first to one big buffer in memory that you then flush to screen when all drawing is done. Currently I use myCanvas.createBufferStrategy(2). But I don't know if manually creating a Volatile buffer wouldn't be faster.  I've done some basic drawing applications using Java. I haven't worked on anything too graphic-intensive but I would recommend that you have a good handle on all the 'repaint' invocations. An extra repaint call on a parent container could double the amount of rendering your doing.  I'm having the same issues as you are I think. Check out my post here: http://stackoverflow.com/questions/196890/java2d-performance-issues It shows the reason for the performance degradation and how to fix it. It's not guaranteed to work well on all platforms though. You'll see why in the post.  A synthesis of the answers to this post the answers to Consty's and my own research: What works: Use GraphicsConfiguration.createCompatibleImage to create images compatible with what you're drawing on. This is absolutely essential! Use double-buffered drawing via Canvas.createBufferStrategy. Use -Dsun.java2d.opengl=True where available to speed up drawing. Avoid using transforms for scaling. Instead cache scaled versions of the images you are going to use. Avoid translucent images! Bitmasked images are fine but translucency is very expensive in Java2D. In my tests using these methods I got a speed increase of 10x - 15x making proper Java 2D graphics a possibility.  There's an important one which hasn't been mentioned yet I think: cache images of everything you can. If you have an object that is moving across the screen and it's appearance doesn't change much draw it to an image and then render the image to the screen in a new position each frame. Do that even if the object is a very simple one - you might be surprised at how much time you save. Rendering a bitmap is much faster than rendering primitives. You might also want to look at setting rendering hints explicitly and turning off things like antialiasing if quality considerations permit.  There are couple of things you will need to keep in mind 1) is refreshing this link shows how to use swingtimers which might be a good option for calling repaint. Getting repaint figured out (as the previous posters have said is important so you're not doing too much work). 2) Make sure you're only doing drawing in one thread. Updating UI from mulitple threads can lead to nasty things. 3) Double Buffering. This makes the rendering smoother. this site has some more good info for you"
940,A,"Setting A CGContext Transparent Background I am still struggling with drawing a line with CGContext. I have actually go to line to draw but now I need the background of the Rect to be transparent so the existing background shows thru. Here's my test code: (void)drawRect:(CGRect)rect { CGContextRef context = UIGraphicsGetCurrentContext(); CGContextSetFillColorWithColor(context [UIColor clearColor].CGColor); CGContextSetAlpha(context0.0); CGContextFillRect(context rect); CGContextSetStrokeColorWithColor(context [UIColor whiteColor].CGColor); CGContextSetLineWidth(context 5.0); CGContextMoveToPoint(context 100.00.0); CGContextAddLineToPoint(context100.0 100.0); CGContextStrokePath(context); } Any ideas? After UIGraphicsGetCurrentContext() call CGContextClearRect(contextrect) Edit: Alright got it. Your custom view with the line should have the following: - (id)initWithFrame:(CGRect)frame { if (self = [super initWithFrame:frame]) { [self setBackgroundColor:[UIColor clearColor]]; } return self; } - (void)drawRect:(CGRect)rect { CGContextRef context = UIGraphicsGetCurrentContext(); CGContextClearRect(context rect); CGContextSetStrokeColorWithColor(context [UIColor whiteColor].CGColor); CGContextSetLineWidth(context 5.0); CGContextMoveToPoint(context 100.00.0); CGContextAddLineToPoint(context100.0 100.0); CGContextStrokePath(context); } My test used this as a very basic UIViewController: - (void)viewDidLoad { [super viewDidLoad]; UIImageView *v = [[UIImageView alloc] initWithFrame:self.view.bounds]; [v setBackgroundColor:[UIColor redColor]]; [self.view addSubview:v]; TopView *t = [[TopView alloc] initWithFrame:self.view.bounds]; [self.view addSubview:t]; [v release]; [t release]; } Background is black must be the iphone background. I'm actually trying to draw on top of a UIView. I don't quite follow. Are you sure you added the UIView to the superview? Is this drawRect in the UIView you want to draw on top of or in a different one? Did you set a background color for this UIView accidentally? Thanks for the questions. I would expect that IM added the UIView to it's superview. The draw rectangle is definitely in the view since the line was drawn properly. Yes I have set a background into the UIView it's that background I want to show thru. All I'm trying to do is draw a line on top of an existing view. Frustrated in Seattle. Fantastic it really works. I really appreciate your effort. Clear Rect doesn't do it here.  I have the same problem then I find it is. I overwrite the init Method is -(id)initWithFrame:(CGRect)rect. In this method self.background = [UIColor clearColor]; but i use this view in xib file !!! That will call the init method is -(id)initWithCoder:(NSCoder*)aDecoder; So overwrite all the init Method and setup BackgroundColor will work OK. None of the above worked for me even though it's set in xib just setting it in code works. +1  CGContextClearRect(contextrect) If the provided context is a window or bitmap context Quartz effectively clears the rectangle. For other context types Quartz fills the rectangle in a device-dependent manner. However you should not use this function in contexts other than window or bitmap contexts.  you can create a image context with this code: cacheContext = CGBitmapContextCreate (cacheBitmap size.width size.height 8 bitmapBytesPerRow CGColorSpaceCreateDeviceRGB() kCGImageAlphaPremultipliedLast); CGContextSetRGBFillColor(cacheContext 0 0 0 0); CGContextFillRect(cacheContext (CGRect){CGPointZero size}); the key is kCGImageAlphaPremultipliedLast.  Easy way: - (id)initWithFrame:(CGRect)frame { if ((self = [super initWithFrame:frame])) { self.opaque = NO; } return self; } - (void)drawRect:(CGRect)rect { CGContextRef context = UIGraphicsGetCurrentContext(); CGContextClearRect(context rect); //your code } I am using this for an accessory indicator in a table view cell. self.opaque = NO works while [self setBackgroundColor:[UIColor clearColor]] in the accepted answer does not.  Having trouble understanding the question here but if you're unable to have a ""background"" UIView show through a ""top"" view into which you're drawing one solution is topView.backgroundColor = [UIColor clearColor]; I was having (I think) this same problem and this solved it for me."
941,A,Is drawing through the Canvas class hardware accelerated on Android? Are calls to Canvas.drawPath()/drawArc()/etc hardware accelerated passed to a device native implementation or implemented in Java? Or is OpenGL the only way to achieve hardware accelerated drawing? I am trying to determine if it is feasible to use the Canvas API for realtime animation. currently Canvas is not hardware accelerated. there are hints in the source code which point to the plans of implementing wrapping canvas calls around so HW accelerator but it's not functional as of now.
942,A,how to discover .dlls my application is using i am trying my hand at using the crystal space api in my graphics applications. crystal space website The applications compile fine but i am having hell with the dlls(dynamic link libraries). The compiled application crashes at run time and i suspect its because of not finding the needed dlls. The only solution i currently have is cutting and pasting my application executable to a folder having all necessary libraries.(which is about 300mb). Is there a way that i can find out the dlls my app needs so that i can copy them?(instead of using all dlls) Any help will be appreciated. PS: i am using g++ and codeblocks ide from dr deo what made you suspect there's a DLL problem? usually if you're missing a DLL a clear message box is displayed with the name of the missing dll. If that doesn't happen you can almost be sure it's not a dll problem. it displayed the messageBox about a dll missing once but afterwards it misteriously just crashes with windows generating error reports. as if some invalid pointer is being referenced Try Dependency Walker By Steve Miller http://www.dependencywalker.com  You can use Dependency Walker. Dependency Walker is a free utility that scans any 32-bit or 64-bit Windows module (exe dll ocx sys etc.) and builds a hierarchical tree diagram of all dependent modules. and it points out issues present as well. i tried it and it gave me this error: Warning: At least one delay-load dependency module was not found. Warning: At least one module has an unresolved import due to a missing export function in a delay-load dependent module. Whats this??? It means that one of the dlls that is required by one of the dlls required by your program is not there. That is why your program will not run. You will need to find the dll or remove that dependency somehow. I get those same two errors from DependencyWalker on all of my applications and assemblies for GPSVC.dll and IESHIMS.dll which don't appear to _actually_ be required because my applications still run.
943,A,"How to keep text inside a circle using Cairo? I a drawing a graph using Cairo (pycairo specifically) and I need to know how can I draw text inside a circle without overlapping it by keeping it inside the bounds of the circle. I have this simple code snippet that draws a letter ""a"" inside the circle: ''' Created on May 8 2010 @author: mrios ''' import cairo math WIDTH HEIGHT = 1000 1000 #surface = cairo.PDFSurface (""/Users/mrios/Desktop/exampleplaces.pdf"" WIDTH HEIGHT) surface = cairo.ImageSurface (cairo.FORMAT_ARGB32 WIDTH HEIGHT) ctx = cairo.Context (surface) ctx.scale (WIDTH/1.0 HEIGHT/1.0) # Normalizing the canvas ctx.rectangle(0 0 1 1) # Rectangle(x0 y0 x1 y1) ctx.set_source_rgb(255255255) ctx.fill() ctx.arc(0.5 0.5 .4 0 2*math.pi) ctx.set_source_rgb(000) ctx.set_line_width(0.03) ctx.stroke() ctx.arc(0.5 0.5 .4 0 2*math.pi) ctx.set_source_rgb(000) ctx.set_line_width(0.01) ctx.set_source_rgb(2550255) ctx.fill() ctx.set_source_rgb(000) ctx.select_font_face(""Georgia"" cairo.FONT_SLANT_NORMAL cairo.FONT_WEIGHT_BOLD) ctx.set_font_size(1.0) x_bearing y_bearing width height = ctx.text_extents(""a"")[:4] print ctx.text_extents(""a"")[:4] ctx.move_to(0.5 - width / 2 - x_bearing 0.5 - height / 2 - y_bearing) ctx.show_text(""a"") surface.write_to_png (""/Users/mrios/Desktop/node.png"") # Output to PNG The problem is that my labels have variable amount of characters (with a limit of 20) and I need to set the size of the font dynamically. It must fit inside the circle no matter the size of the circle nor the size of the label. Also every label has one line of text no spaces no line breaks. Any suggestion? P.S. You should use floats (0-1.0) when calling set_source_rgb(). See http://cairographics.org/documentation/pycairo/2/reference/context.html#class-context Since the size of the circle does not matter you should draw them in the opposite order than your code. Print the text on screen Calculate the text boundaries (using text extents) Draw a circle around the text that is just a little bigger from the text.  I had a similar issue where I need to adjust the size of the font to keep the name of my object within the boundaries of rectangles not circles. I used a while loop and kept checking the text extent size of the string decreasing the font size until it fit. Here what I did: (this is using C++ under Kylix a Delphi derivative).  double fontSize = 20.0; bool bFontFits = false; while (bFontFits == false) { m_pCanvas->Font->Size = (int)fontSize; TSize te = m_pCanvas->TextExtent(m_name.c_str()); if (te.cx < (width*0.90)) // Allow a little room on each side { // Calculate the position m_labelOrigin.x = rectX + (width/2.0) - (te.cx/2); m_labelOrigin.y = rectY + (height/2.0) - te.cy/2); m_fontSize = fontSize; bFontFits = true; break; } fontSize -= 1.0; } Of course this doesn't show error checking. If the rectangle (or your circle) is too small you'll have to break out of the loop."
944,A,Is it smart building an OpenGL C# application to replace GDI I have developed a quite large application using MFC. Naturaly I used GDI for drawing CCmdTarget for event routing and the document-view architecture. It was a convenient development path. Now the client is interested in converting this application to .Net. I would prefer (and they too) writing the new product in C#. The application displays and interacts with thousands of graphic objects so I figured going with GDI+ although seems natuaral can cause performance issues So I am thinking of using OpenGL specifically - OpenTK - as the graphics library (it's 2D). I know that OpenGL works differently that these Windows APIs which rely on Invalidation of portion of the screen. OpenGL has a rendering loop which constantly draws to the screen. My question is: Is this an acceptable way to go thinking of: performance - will the users need special graphics cards (hardware?). It is graphics intensive but it's not a high-end game printing and print preview - are these things complex to achienve? multiple selection and context menus Is this library goes well inside windows forms? Direct X and Open GL much faster than GDI+.  You can also use an TAO framework as an alternative to OpenTK.  In my experience developing CAD-like software the benefits of OpenGL and DirectX are fast depth testing smooth rotation and panning lighting and powerful texture capabilities. Obviously there are other benefits but despite what most tutorials would lead you to believe implementing a rendering system using either of these APIs is a significant undertaking and should not be taken lightly. Specifically: If it is a 2D app and you already have it implemented in GDI then switching to GDI+ will be much easier. Additionally on modern hardware 2D GDI or GDI+ can be about as fast as 2D OpenGL or DirectX. And ultimately the end-user probably won't notice the difference especially with double buffered support in GDI+. You do not need (and probably don't want) a continuous rendering loop for your app. In OpenGL and DirectX you can explicitly invalidate the window when your scene changes. If you go with OpenGL or DirectX you will need to consider putting your objects into display lists or vertex arrays (buffers) for fast drawing. This is not difficult but managing objects in this way adds complexity to the system and will most likely significantly change the architecture of your rendering system. Printing in either OpenGL or DirectX can also be tedious. On the one hand you can render to a bitmap and print that out. However for high quality images you may want vectorized images instead which are difficult to produce with either of these rendering frameworks. I would also stay away from writing GUIs in OpenGL or DirectX...unless you're really looking for a challenge ;~) Finally and this is just an annoyance from an install perspective the Managed DirectX run-time library that must be installed on the user's machine is around 100 MB. wow thanks for all the tips & information. I figured printing would be a problem in OpenGL. I didn't even think about DirectX I thought about using OpenGL. But maybe it will be natural to switch to GD+. I figure I need to pay attention to optimizing its performance. thanks  I have no experience with C# but I have once built a layer system for a drawing program that used openGL for rendering. To make this layer I asked openGL for the current framebuffer and converted it to an image to use as a texture under the current canvas. So I guess from there its pretty easy to go to printing and print preview.  I don't think so. Use WPF if you can or DirectX if you can't. I know it might not be fair but if I'm programming on .NET (microsoft) on windows (microsoft) I'd rather use DirectX ... which is also from microsoft. As a side note: don't reinvent the wheel. Recoding user controls in open-gl can be very time consuming if you do make sure you have a good reason. +1 for WPF neutral on DirectX/OpenGL
945,A,Rendering 2D primitives in scrollable view What is the best approach to render a large number of 2D graphical elements (lines text shapes etc.) in a scrollable view on windows using C#? There is a scrollviewcontainer control but not on the toolbox by default. I would do the above approach to a picture box and embed it inside this control. Also you can use SetStyle to enable double buffering to prevent flicker. Another option is to build a class to handle double buffering (draw to a bitmap object and push out the results using CreateGraphics). There are some good examples out there.  You could put a PictureBox (of whatever overall size required) onto a Panel with AutoScroll set to True and then draw everything you need at once onto the PictureBox using a Graphics object. However if the overall size of the drawing surface is extremely large this approach would not be practical (since it would mean having a huge PictureBox and a correspondingly huge Bitmap which could consume a large amount of memory). If this were the case you'd be better off creating your own scrollable user control (horizontal and vertical) and rendering only the visible portion of the overall surface in the control's Paint event. The first approach would be easier and faster to write but might consume too much memory. The second approach would require more work on your part but would minimize memory consumption.
946,A,How do I project lines dynamically on to 3D terrain? I'm working on a game in XNA for Xbox 360. The game has 3D terrain with a collection of static objects that are connected by a graph of links. I want to draw the links connecting the objects as lines projected on to the terrain. I also want to be able to change the colors etc. of links as players move their selection around though I don't need the links to move. However I'm running into issues making this work correctly and efficiently. Some ideas I've had are: 1) Render quads to a separate render target and use the texture as an overlay on top of the terrain. I currently have this working generating the texture only for the area currently visible to the camera to minimize aliasing. However I'm still getting aliasing issues -- the lines look jaggy and the game chugs frequently when moving the camera EDIT: it chugs all the time I just don't have a frame rate counter on Xbox so I only notice it when things move. 2) Bake the lines into a texture ahead of time. This could increase performance but makes the aliasing issue worse. Also it doesn't let me dynamically change the properties of the lines without much munging. 3) Make geometry that matches the shape of the terrain by tessellating the line-quads over the terrain. This option seems like it could help but I'm unsure if I should spend time trying it out if there's an easier way. Is there some magical way to do this that I haven't thought of? Is one of these paths the best when done correctly? I don't have an answer but this makes me think of linear algebra. Perhaps there's some matrix transform you can do? Sorry it may be no help. The same way you do it statically but more often and after the program is running. :-) Your 1) is a fairly good solution. You can reduce the jagginess by filtering -- first make sure to use bilinear sampling when using the overlay. Then try blurring the overlay after drawing it but before using it; if you choose a proper filter it will remove the aliasing. If it's taking too much time to render the overlay try reducing its resolution. Without the antialiasing filter that would just make it jaggier but with a good filter it might even look better. I don't know why the game would chug only when moving the camera. Remember you should have a separate camera for the overlay -- orthogonal and pointing down onto the terrain. The jaggies are pretty bad to the point that I'm not sure any amount of filtering would help. You might be surprised. Anyway another (sorta equivalent) thing you could try is blurring the edges of the lines as you draw them -- have them fade from transparent at the edges to opaque towards the center.  Does XNA have a shadowing library? If so yo could just pretend the lines are shadows.
947,A,Use fov and center pan and tilt values to calculate boundary's pan and tilt? I have the current fov and the center pan and tilt values. How would I calculate what the pan and tilt values are of the edges(i.e. min pan value and max tilt) with this information? The minimum (that is the bottom of the viewport) tilt is simply the current center tilt minus half the FOV possibly bounded at 90 degrees. The maximum can be located similarly. Beware that the tilt values can exceed 90 degrees if the viewport encompasses one of the poles of the projection sphere and you must decide if you are interested in how far onto the opposite hemisphere the viewport extends or if you are only interested in the maximum rendered tilt (+/-90 degrees for any viewport that spans a pole). Minimum and maximum pan are poorly defined as the edges of a rectilinear viewport will not be parallel to the lines of longitude on the projection sphere so the pan extents at the north end of the viewport will be greater than at the south end (in the case of positive tilt).
948,A,"Removing black border around an image I have a few JPG Images. Some of them may have a black border on one or more sides and I'd like to remove them. The black border may not go around the actual image - some may only have the border at the bottom (with the actual image at the top) while some could be centered (which means black borders on two sides but not connected). Worse the images are JPG Compressed so they may not be exactly 000 black anymore. In a Paint Program I would ""simply"" use the Magic Wand tool with a low tolerance but I need to do it in C# on ASP.net and I don't know what the best way of doing this is. Should I ""scan"" each line and then each column (two nested for-loops) to find black areas? Sounds somewhat stupid to do performance and CPU-Load-wise. Or does GDI+ have some magic wand tool build in already? The images are not that big (474x474 pixels maximum) and cached afterwards but I need to keep the server load as low as possible. Any hints what the least stupid way of doing it would be? It seems like for each edge you could do something like this: for each edge: for (i = 0; ; i++) { compute average pixel value along edge row/column + i if (average value > threshold) break; } crop image That approach seems a bit brute-force (two for-loops) but it is surprisingly fast (less than 1 millisecond per picture). Thanks for the idea with the Average Color value. The sad truth with image processing is at least at a lower level you have to iterate over all pixels of interest. Also be a bit careful with this method if users can upload mostly black images. Maybe set an upper limit on the allowed border size."
949,A,"How do you draw like a Crayon? Crayon Physics Deluxe is a commercial game that came out recently. Watch the video on the main link to get an idea of what I'm talking about. It allows you to draw shapes and have them react with proper physics. The goal is to move a ball to a star across the screen using contraptions and shapes you build. While the game is basically a wrapper for the popular Box2D Physics Engine it does have one feature that I'm curious about how it is implemented. Its drawing looks very much like a Crayon. You can see the texture of the crayon and as it draws it varies in thickness and darkness just like an actual crayon drawing would look like. The background texture is freely available here. Close up of crayon drawing - Note the varying darkness What kind of algorithm would be used to render those lines in a way that looks like a Crayon? Is it a simple texture applied with a random thickness and darkness or is there something more going on? I believe the easiest way would simply be to use a texture with random darkness (some gradients maybe) throughout and set size randomly.  There is a paper available called Mimicking Hand Drawn Pencil Lines which covers a bit of what you are after. Although it doesn't present a very detailed view of the algorithm the authors do cover the basics of the steps that they used. This paper includes high level descriptions of how they generated the lines as well as how they generated the textures for the lines and they get results which are similar to what you want.  This article on rendering chart series to look like XKCD comics has an algorithm for perturbing lines which may be relevant. It doesn't cover calculating the texture of a crayon drawn line but it does offer an approach to making a straight line look imperfect in a human-like way. Example output:  You can base darkness on speed. That's just measuring the distance traveled by the cursor between this frame and the last frame (remember Pythagorean theorem) and then when you go to draw that line segment on screen adjust the alpha (opacity) according to the distance you measured.  I remember reading (a long time ago) a short description of an algorithm to do so: for the general form of the line you split the segment in two at a random point and move this point slightly away from it's position (the variation depending on the distance of the point to the extremity). Repeat recursively/randomly. In this way you lines are not ""perfect"" (straight line) for a given segment you can ""overshoot"" a little bit by extending one extremity or the other (or both). In this way you don't have perfect joints. If i remember well the best was to extends the original extremities but you can do this for the sub-segment if you want to visibly split them. draw the lines with pattern/stamp there was also the (already mentioned) possibility to drawn with different starting and ending opacity (to mimic the tendency to release the pen at the end of drawing) You can use a different size for the stamp on the beginning and the end of the line (also to mimic the tendency to release the pen at the end of drawing). For the same effect you can also draw the line twice with a small variation for one of the extremity (be careful with the alpha in this case as the line will be drawn twice) Last for a given line you can do the previous modifications several times (ie draw the line twice with different variations) : human tend to repeat a line if they make some mistakes. Regards  If you blow the image up you can see a repeating stamp-pattern .. there's probably a small assortment that it uses as it moves from a to b - might even rotate them .. The wavering of the line can't be all that difficult to do. Divide into a bunch of random segments pick positions slightly away from the direct route and draw splines.  Here's a paper that uses a lot of math to simulate the deposition of wax on paper using a model of friction. But I think your best bet is to just use a repeating pattern as another reader mentioned and vary the opacity according to pressure. For the imperfect line drawing parts I have a blog entry describing how to do it using bezier curves. The link to your blog post appears to be dead. I updated the link."
950,A,Separating axis test detecting if a rotated rectangle overlap an other flat rectangle I read about intersection rectangles on: http://stackoverflow.com/questions/115426/algorithm-to-detect-intersection-of-two-rectangles But i have trouble to implement it. If R1 (ABCD) is my rotated rectangle and R2(A'B'C'D') the other rectangle with no rotation. The formula extracted in from the link above is:  edge = v(n) - v(n-1) You can get a perpendicular to this by rotating it by 90°. In 2D this is easy as:  rotated.x = -unrotated.y rotated.y = unrotated.x // rotated: your rotated edge // v(n-1) any point from the edge. // testpoint: the point you want to find out which side it's on. side = sign (rotated.x * (testpoint.x - v(n-1).x) + rotated.y * (testpoint.y - v(n-1).y); My rotated edges will be from R1 with AB (xB-xA yB-yA) so rotated x is xB-xA ? BC (xC-xB yC-y1) CD ... AD ... Testpoint will be A' B' C' D' from R2 So i have to check the sign of the result from all points of R2 against the 4 edges from R1. Thats 16 comparisons if intersecting. How do i know if i found a separating edge ? Thanks If for any given edge the signs of any of the dot products testing against that edge don't match then you have an intersection. The sign of the dot product will be the same for all points on one side of the line.
951,A,How to render animation or effects using Core Graphics I want to draw a projectile of a cannon moving and explosive effects (just simple broken bits flying around). How should I approach this using Core Graphics (on iPhone)? I am using an NSTimer to call a render method and update the animated projectile or explosive effects and I have no problem drawing each individual pieces (such as filled rec or line) but I found that drawRect always erases the previous content and only render the new stuff. How would you approach this? There's a really good book on this at the Pragmatic Programmers site. I looked at that book at the book store it only has one chapter on iphone? Is coreanimation on iphone and macosx that simmilar? This is not so much an animation issue as it is a rendering issue so that book won't be of much help (case in point I have that book so I know).  Found the solution use setNeedsDisplayInRect instead of setNeedsDisplay and limit to the small area I was drawing into (Thanks Erica).
952,A,"extracting a quadrilateral image to a rectangle BOUNTY UPDATE Following Denis's link this is how to use the threeblindmiceandamonkey code: // the destination rect is our 'in' quad int dw = 300 dh = 250; double in[4][4] = {{00}{dw0}{dwdh}{0dh}}; // the quad in the source image is our 'out' double out[4][5] = {{17172}{33193}{333188}{177210}}; double homo[3][6]; const int ret = mapQuadToQuad(inouthomo); // homo can be used for calculating the xy of any destination point // in the source e.g. for(int i=0; i<4; i++) { double p1[3] = {out[i][0]out[i][7]1}; double p2[3]; transformMatrix(p1p2homo); p2[0] /= p2[2]; // x p2[1] /= p2[2]; // y printf(""\t%2.2f\t%2.2f\n""p2[0]p2[1]); } This provides a transform for converting points in destination to the source - you can of course do it the other way around but it's tidy to be able to do this for the mixing: for(int y=0; y<dh; y++) { for(int x=0; x<dw; x++) { // calc the four corners in source for this // destination pixel and mix For the mixing I'm using super-sampling with random points; it works very well even when there is a big disparity in the source and destination area BACKGROUND QUESTION In the image at the top the sign on the side of the van is not face-on to the camera. I want to calculate as best I can with the pixels I have what it'd look like face on. I know the corner coordinates of the quad in the image and the size of the destination rectangle. I imagine that this is some kind of loop through the x and y axis doing a Bresenham's line on both dimensions at once with some kind of mixing as pixels in the source and destination images overlap - some sub-pixel mixing of some sort? What approaches are there and how do you mix the pixels? Is there a standard approach for this? Look up ""quad to quad"" transform e.g. threeblindmiceandamonkey. A 3x3 transform on 2d homogeneous coordinates can transform any 4 points (a quad) to any other quad; conversely any fromquad and toquad such as the corners of your truck and a target rectangle give a 3 x 3 transform. Qt has quadToQuad and can transform pixmaps with it but I guess you don't have Qt ? Added 10Jun: from labs.trolltech.com/page/Graphics/Examples there's a nice demo which quad-to-quads a pixmap as you move the corners: Added 11Jun: @Will here's translate.h in Python (which you know a bit ? """""" ..."""""" are multiline comments.) perstrans() is the key; hope that makes sense if not ask. Bytheway you could map the pixels one by one mapQuadToQuad( target rect orig quad ) but without pixel interpolation it'll look terrible; OpenCV does it all. #!/usr/bin/env python """""" square <-> quad maps from http://threeblindmiceandamonkey.com/?p=16 matrix.h """""" from __future__ import division import numpy as np __date__ = ""2010-06-11 jun denis"" def det2(a b c d): return a*d - b*c def mapSquareToQuad( quad ): # [4][2] SQ = np.zeros((33)) px = quad[00] - quad[10] + quad[20] - quad[30] py = quad[01] - quad[11] + quad[21] - quad[31] if abs(px) < 1e-10 and abs(py) < 1e-10: SQ[00] = quad[10] - quad[00] SQ[10] = quad[20] - quad[10] SQ[20] = quad[00] SQ[01] = quad[11] - quad[01] SQ[11] = quad[21] - quad[11] SQ[21] = quad[01] SQ[02] = 0. SQ[12] = 0. SQ[22] = 1. return SQ else: dx1 = quad[10] - quad[20] dx2 = quad[30] - quad[20] dy1 = quad[11] - quad[21] dy2 = quad[31] - quad[21] det = det2(dx1dx2 dy1dy2) if det == 0.: return None SQ[02] = det2(pxdx2 pydy2) / det SQ[12] = det2(dx1px dy1py) / det SQ[22] = 1. SQ[00] = quad[10] - quad[00] + SQ[02]*quad[10] SQ[10] = quad[30] - quad[00] + SQ[12]*quad[30] SQ[20] = quad[00] SQ[01] = quad[11] - quad[01] + SQ[02]*quad[11] SQ[11] = quad[31] - quad[01] + SQ[12]*quad[31] SQ[21] = quad[01] return SQ #............................................................................... def mapQuadToSquare( quad ): return np.linalg.inv( mapSquareToQuad( quad )) def mapQuadToQuad( a b ): return np.dot( mapQuadToSquare(a) mapSquareToQuad(b) ) def perstrans( X t ): """""" perspective transform X Nx2 t 3x3: [x0 y0 1] t = [a0 b0 w0] -> [a0/w0 b0/w0] [x1 y1 1] t = [a1 b1 w1] -> [a1/w1 b1/w1] ... """""" x1 = np.vstack(( X.T np.ones(len(X)) )) y = np.dot( t.T x1 ) return (y[:-1] / y[-1]) .T #............................................................................... if __name__ == ""__main__"": np.set_printoptions( 2 threshold=100 suppress=True ) # .2f sq = np.array([[00] [10] [11] [01]]) quad = np.array([[171 72] [331 93] [333 188] [177 210]]) print ""quad:"" quad print ""square to quad:"" perstrans( sq mapSquareToQuad(quad) ) print ""quad to square:"" perstrans( quad mapQuadToSquare(quad) ) dw dh = 300 250 rect = np.array([[0 0] [dw 0] [dw dh] [0 dh]]) quadquad = mapQuadToQuad( quad rect ) print ""quad to quad transform:"" quadquad print ""quad to rect:"" perstrans( quad quadquad ) """""" quad: [[171 72] [331 93] [333 188] [177 210]] square to quad: [[ 171. 72.] [ 331. 93.] [ 333. 188.] [ 177. 210.]] quad to square: [[-0. 0.] [ 1. 0.] [ 1. 1.] [ 0. 1.]] quad to quad transform: [[ 1.29 -0.23 -0. ] [ -0.06 1.79 -0. ] [-217.24 -88.54 1.34]] quad to rect: [[ 0. 0.] [ 300. 0.] [ 300. 250.] [ 0. 250.]] """""" I'm struggling and I've updated the question but thx for the excellent lead Thank you for showing the workings @Will you're welcome. What did you end up using ? the threeblindmiceandamonkey code with my own supersampling; the opencv bilinear-etc parameter gives me an uneasy feeling  I think what you need is affine transformation which can be accomplished using matrix math. Actually this is a projective transformation. You're right I forgot about the perspective. An affine transformation would result in a distorted image. Sorry about that.  What you want is called planar rectification and it's not all that simple I'm afraid. What you need to do is recover the homography that maps this oblique view of the van side onto the front-facing view. Photoshop / etc. have tools to do this for you given some control points; if you want to implement it for yourself you'll have to start delving into computer vision. Edit - OK here you go: a Python script to do the warping using the OpenCV library which has convenient functions to calculate the homography and warp the image for you: import cv def warpImage(image corners target): mat = cv.CreateMat(3 3 cv.CV_32F) cv.GetPerspectiveTransform(corners target mat) out = cv.CreateMat(height width cv.CV_8UC3) cv.WarpPerspective(image out mat cv.CV_INTER_CUBIC) return out if __name__ == '__main__': width height = 400 250 corners = [(17172)(33193)(333188)(177210)] target = [(00)(width0)(widthheight)(0height)] image = cv.LoadImageM('fries.jpg') out = warpImage(image corners target) cv.SaveImage('fries_warped.jpg' out) And the output: OpenCV also has C and C++ bindings or you can use EmguCV for a .NET wrapper; the API is fairly consistent across all languages so you can replicate this in whichever language suits your fancy. @Will - selecting four quadrilateral corner points in the image and four corresponding rectangle points is enough information to recover the homography (i.e. transformation matrix) but you need to do some math; see this pdf: http://www.cs.brown.edu/courses/csci1950-g/asgn/proj6/resources/ProjectiveMappings.pdf I fear I phrased my question too generally - I know the dimensions of the rectangular face I'm trying to extract and the coordinates of it's corners as in the image leaving me wondering only how to get best-quality pixels out - how to calculate the coverage of each pixel and how to best mix them"
953,A,QTVR-like Panorama in Flash/ActionScript? It has been a few years since I used Actionscript. Back in the day I made a project that emulated a QTVR panorama (at the time I was using Flash you could only embed very basic mov files) by simply moving a very long flattened pano image left or right behind a mask. The effect was okay but not as nice as a real pano since the perspective was so warped. So now that a couple iterations of Flash have been developed I am curious... Is there a way now to get a bit closer to a real QTVR? ...or is it now possible to embed a real QTVR? My advice would be - download PaperVision cut your image into strips then arrange these in ring as 3d planes.  FlashPanoramas has worked great for me in the past. One of its newer features is the ability to directly load in QTVR files.  The best solution one I am currently using on a day to day basis to present plots of land for sale on a website I developed is to use krpano. It is by far the best one out there -- flash-based XML-powered very customizable supports plug-ins hands down the best. http://www.krpano.com/ It has a flash viewer and also all the tools you need to create the panoramas.
954,A,"How to resize text in svg dynamically? I'm working on a report that includes an embedded svg diagram. The diagram is drawn using relative coordinates so when a browser window resizes the diagram resizes pretty well. The only exception there is text - it remains the same. Is it possible to draw text in svg that is resizable? I think you have to use a relative size for the font-size. See http://www.w3.org/TR/SVG11/coords.html#UnitIdentifiers. When you use an absolute size like pt or cm it is automatically calculated what the size must be to display correctly on your monitor to get that size. But when you use px the current viewport is used. If you don't specify a unit the user units are also used as in the example on the linked document: <text style=""font-size: 50"">Text size is 50 user units</text>"
955,A,"ca plotting text attributes Does anyone know of a way to control the font size/color/weight of the row and column names when plotting a correspondence plot with the ca package? The following code will produce a very nice looking chart though if there were more attributes (very heavy super heavy something more than super heavy) or more classes of workers (peons underlings etc) then the graph will get a little cluttered and hard to tell what was what. It would be nice if you could list all the attributes in a separate color than the categories of workers. library(ca) data(""smoke"") plot(ca(smoke)  map = ""symmetric""  what =c(""active""""active"")  mass = c(TT)  contrib = ""absolute""  col = c(""red""""blue"")  pch = c(15171517)  labels = c(22)  arrows = c(TF) ) Alternatively does anyone know if there is a way to reproduce something along these lines with ggplot2? I didn't find anything on the website that seemed comparable but I don't know much about the package. Thanks -Chase I would try some of the other correspondence analysis functions available in R. In some of them the character expansion factor (cex) option is supported so you can control the font size. e.g. library(FactoMineR) res<-CA(smoke ncp=5 row.sup=NULL col.sup=NULL graph = FALSE) plot.CA(res axes=c(1 2) col.row=""red"" col.col=""blue"" label=c(""col""""col.sup"" ""row"" ""row.sup"")cex=.7) library(MASS) biplot(corresp(smoke nf = 2)cex=.7col=c(""red""""blue"")) library(anacor) # actually I didn't find a way to control font size here res <- anacor(smoke scaling = c(""Benzecri"" ""Benzecri"")ndim=2) plot(res plot.type = ""jointplot"" conf = NULL) EDIT Of course you could get the coordinates from the ca resultset and generate this plot using ggplot2. Here I'm using the res object from CA. df <- data.frame(dim1 = c(res$col$coord[1]res$row$coord[1]) dim2 = c(res$col$coord[2]res$row$coord[2]) type=c(rep(1length(res$col$coord[1]))rep(2length(res$row$coord[1])))) library(ggplot2) qplot(dim1dim2data=dfcolour=factor(type)) + geom_text(aes(label=rownames(df))size=3) This is exactly what I needed to help me on my way. I apologize for not responding sooner I've been traveling. Thanks again!  The second code block of George Dontas a really good example. Solved a big issue for me. But it took me forever to figure out that the names of the CA-objects are actually: [YOUR_CA-CLASS-TABLE]$colcoord[1] and [YOUR_CA-CLASS-TABLE]$rowcoord[1] I think it suits better as a comment to the George Dontas answer. You must have 50 reputation to comment... Sorry for that ;-)"
956,A,Delphi 2010 accessing to frames inside a TIFF I just tested I can access to TIFF images in Delphi 2010 with the classic image1.Picture.LoadFromFile(MyTiffFile); Simply using this line of code I can load the first frame of a TIFF file into a TImage component. But when a TIFF file is a multiframe one. How can I get the rest of images contained in a TIFF file? I think for this you will need to use a third party control. I have no experience with Delphi and its controls but multi-page tiffs are normally not part of the standard functionality. By using the standard way no. But you can try a 3rd party component. Here are some: LibTiff for Delphi (free) allows you to work with the official Tiff implementation tapping the full 'power' of Tiff format - however it can be a little difficult to walk through the internal Tiff directory to achieve what you want. But perhaps you can send an email to the Aware Systems and ask them how to do it. HiComponents ImageEn (commercial) - even if the site is very simple it seems that it is one of the best graphic libraries around. ImageMagick (free) - very well known image processing library. Has Delphi/FPC bindings. ...there are also other alternatives free and commercial but some of them (like GraphicsEx) doesn't know to read multi-page Tiff and for others I don't have so much experience in order to give you an impression. HTH.
957,A,"ASCII ""graphics"" library? Is there a platform-independent C/C++ library that can draw simple ""graphics"" in pure ASCII in a console program? For example (VERY roughly) I could call a function in the library like rectangle(3 6); to get the following output: ****** * * ****** Ultimately I would love to be able to plot simple graphs based on input data tables like: | |* | | * | * | * | * | * +--------------------------------- And does anyone know if there is a way to specifically render data plots/graphs in ASCII or UTF8? this strikes me as cool thing - if it existed. Would make output to consol much more attractive. I wonder if there's a .NET implementation yet. From what you have said you don't need ASCII graphics library as they purpose is to render bitmap into ASCII characters so the look of ASCII data will become 'similar' to the bitmap. For the task you have mentioned consider writing your own library because: Your task is not really bitmap rendering It is not so complicated If you really want to use ASCII art lib you may choose a library for graph bitmap rendering and then pass that generated bitmap data to ASCII lib so you will get the output. The answers below are way better and at least provide a path forward. This answer does not even grasp the question correctly.  I don't know if its exactly what you're searching for but this library will render images and viedos to the console. http://aa-project.sourceforge.net/aalib/  In addition to aalib there is also libcaca (this one will render in full color)  I suppose you can use curses (and derivatives like ncurses)."
958,A,"OpenGL Collision Detection I am currently working on designing my first FPS game using JOGL. (Java bindings for OpenGL). So far I have been able to generate the 'world' (a series of cubes) and a player model. I have the collision detection between the player and the cubes working great. Now I am trying to add in the guns. I have the gun models drawn correctly and loading onto player model. The first gun I'm trying to implement is a laser gun which shoots and instantaneous line-of-sight laser at whatever you're aiming at. Before I work on implementing the enemy models I would like to get the collision detection between the laser and the walls working. My laser currently is drawn by a series of small cubes one after the other. The first cube is drawn at the end of the players gun then it draws continuously from there. The idea was to continue drawing the cubes of the laser until a collision was detected with something namely the cubes in the world. I know the locations of the cubes in the world. The problem is that I have to call glMatrixPush to draw my character model. The laser is then drawn within this modelview. Meaning that I have lost my old coordinate system - so I'm drawing the world in one system then the lazer in another. Within this player matrix I have go call glRotate and glTranslate several times in order to sync everything up with the way the camera is rotating. The lazer is then built by translating along the z-axis of this new system. My problem is that through all of these transformations I no longer have any idea where my laser exists in the map coordinate system primarily due to the rotations involving the camera. Does anyone know of a method - or have any ideas for how to solve this problem? I believe I need a way to convert the new coordinates of the laser into the old coordinates of the map but I'm not sure how to go about undoing all of the transformations that have been done to it. There may also be some functionality provided by OpenGL to handle this sort of problem that I'm just unaware of. You shouldn't be considering the laser as a spacial child of the character that fires it. Once its been fired the laser is an entity of its own so you should render as follows: glPushMatrix(viewMatrix); glPushMatrix(playerMatrix); DrawPlayer(); glPopMatrix(); glPushMatrix(laserMatrix); DrawLaser(); glPopMatrix(); glPopMatrix(); Also be sure that you don't mix your rendering transformation logic with the game logic. You should always store the world-space position of your objects to be able to test for intersections regardless of your current OpenGL matrix stack. Remember to be careful with spacial parent/child relationships. In practice they aren't that frequent. For more information google about the problems of scene graphs. Thanks for the response - I've made some changes and I'm now rendering the laser in it's own matrix. Is there a standard or nice way of saving the world space position of a new object? For example - since I've already pushed my viewMatrix and my playerMatrix whenever I go to push my laserMatrix how do I know where that is in the world space? I'm not sure I understand you here. `glGetFloatv(GL_MODELVIEW_MATRIX)` can retrieve your current matrix if that's what you need but you should always know where your laser is in world space because your laser entity class should store that value. The laser's transformation matrix should be computed from its world-space position. Once again the current OpenGL matrix stack state should not impact your ability to locate your entities in world space. So in order to store the world-space coordinates of my objects each time I call something like glRotate or glTranslate I will need to do the matrix math myself - multiplying by the inverse matrix - to keep track of the world-space coordinates? It seems odd that OpenGL wouldn't keep track of that itself.  The point that was being made in the first answer is that you should never depend on the matrix to position the object in the first place. You should be keeping track of the position and rotation of the laser before you even think about drawing it. Then you use the translate and rotate commands to put it where you know it should be. You're trying to do things backwards and yes that does mean you'll have to do the matrix math and OpenGL doesn't keep track of that because the ModelView matrix is the ONLY thing that OpenGL does keep track of in regards to object positions. OpenGL has no concept of ""world space"" or ""camera space"". There is only the matrix that all input is multiplied by. It's elegantly simple... but in some cases I do prefer the way DirectX has a a separate view matrix and model matrix. So if you don't know where an object is located without matrix math then I would consider that a fundamental design problem. If you don't need to know the object position then matrix-transform to your hearts content but if you do need it's position start with the position. (pretty much what the first answer says just in a different way...) +1 I think you explained it that part of the problem better than I did :)"
959,A,How do I draw a point inside of an instance of UIImageView? How do I use Core Graphics to draw inside of a instantiated UIImageView (No class files to override drawrect with -- just a reference to the instance)? I want to draw a simple point of fixed width (say 10 pixels diameter) and of a certain color. How do I use Core Graphics to … draw a simple point of fixed width (say 10 pixels diameter) and of a certain color[?] Set the fill color. Move to the point. Plot an arc centered at the point with the desired radius giving a start angle of 0 and an end angle of 2 * M_PI. Close the path. Fill. … draw inside of a instantiated UIImageView (No class files to override drawrect with -- just a reference to the instance) I know that on the Mac that's really not a good idea as an NSView may redraw at any time if anything has set it as needing display (or told it directly to display) and would then clobber anything you'd drawn over it. No idea about the iPhone but I do notice that UIView doesn't have methods like NSView's lockFocus and unlockFocus. There's a more architectural reason not to do this: Drawing is the exclusive domain of views and you're proposing to break that exclusivity and sprinkle some drawing code in (I'm assuming) a controller. Scattering code for a purpose in an object that doesn't have that purpose is a path to messy unnavigable code. Better to make a subview for each point and add those as subviews of the image view or subclass UIImageView give your instance of that subclass an array of the points to draw and have the subclass's drawRect: call up to super before drawing its points. The above solution then goes in drawRect:.  You can also just add a Core Animation layer to the view. Set the corner radius to half the size of the point you want (assuming a square) and it will render as a circle. Something like: CALayer *layer = [CALayer layer]; [layer setBounds:CGRectMake(0.0f 0.0f 10.0f 1.0f)]; [layer setCornerRadius:5.0f]; [layer setMasksToBounds:YES]; [layer setBackgroundColor:[[UIColor redColor] CGColor]]; // Center the layer in the view. [layer setPosition:CGPointMake([view bounds].size.width/2 [view bounds].size.height/2)]; // Add the layer to your view [[view layer] addSublayer:layer]; Not Core Graphics exactly but it's easy to implement. Best Regards
960,A,"Rasterizing a 2D polygon I need to create a binary bitmap from a closed 2D polygon represented as a list of points. Could you please point me to efficient and sufficiently simple algorithms to do that or even better some C++ code? Thanks a lot! PS: I would like to avoid adding a dependency to my project. However if you suggest an open-source library I can always look at the code so it can be useful too. You can check out the polygon fill routine in Pygame. Look at the draw_fillpoly function near the end. http://www.seul.org/viewcvs/viewcvs.cgi/trunk/src/draw.c?rev=2617&root=PyGame&view=auto The algorithm is pretty simple. It finds all the positions that each segment intersects along the Y axis. These intersections are sorted and then horizontally fills each pair of intersections. This will handle complex and intersecting shapes but obviously you could crush this algorithm with large amounts of segments. not too related but btw Pete you are awesome for making pygame :)  The magic google phrase you want is either ""non-zero winding rule"" or ""even odd polygon fill"". Both are very easy to implement and sufficiently fast for most purposes. With some cleverness they can be made antialiased as well. @plinth: isn't that over-kill for simple polygons? What's a simple polygon? @static_rtti doesn't specify how many points or if the polygons will always be convex therefore a general solution is the correct answer. NZW and EO are butt-simple and lend themselves to scanline oriented solutions etc etc etc. @plinth: Thanks this is exactly what I was looking for! Googling stuff can be tricky when you don't have that magic phrase :-) @plinth: a simple polygon is one whose segments do not cross each other. see http://en.wikipedia.org/wiki/Simple_polygon  Triangulate your polygon Raster each of the triangles (if you are using a GPU then it can do it for you instead it's a primitive operation of GPUs) If the triangle doesn't have a segment that is parallel to the x axis then break it into two triangles with a line that is parallel to the x axis and goes through it's point with the median-y Now the remaining task is to draw a triangle which has a segment that is parallel to the x axis. Such a triangle has a left-side segment and a right-side segment Iterate the triangle's scan-lines (min-y to max-y). For each y calculate the left and right segments' points in that scanlines. Fill the scanline segment these two points (a simple memset). Complexity is O(Area in pixels) How would you rasterize the resulting triangles? One by one by traversing the whole image each time? Isn't that likely to be very slow? @static_rtti: what do you mean traversing the whole image each time? @static_rtti: I added an explanation of how to rasterize a triangle The crucial point here is ""calculate the left and right segments"". Doing it separately for each scanline is prohibitively slow; a better way is to calculate the increment and apply it to the starting value."
961,A,"Java: Graphics in Linux Does X-Windows have to be installed on a Linux-box in order for Java to display fullscreen graphics? Well ""fullscreen graphics"" is a bit vague. Anyway apparently there is a an effort ongoing to access the framebuffer from Java: Framebuffer Toolkit. The objective of this project is to produce a body of code which is a lightweight framebuffer-based peer implementation for AWT and Swing. The goal of this code is to remove the dependency on X or other graphics layers such that graphics can be redirected to a framebuffer (e.g. a raw buffer VNC etc.). This example implementation will prefer pure-Java solutions with public extension points available to enter native resources as necessary. See Project proposal: fbtoolkit.  To really display something graphical on the screen yes. Bud there is a headless version of the JRE for just running it. You won't see any graphical output but it will run. Alternatively you can log in remotely and use X forwarding to run the java code on the server but let the client handle displaying graphics.  Other answerers appear to assume that ""full screen graphics in Java"" necessarily means ""a working implementation of AWT"". This is of course not necessarily true as it is perfectly possible (some would even say desirable) to use Java without AWT. Cairo is a 2D graphics rendering library that can be used from Java and can also be used without X11. It looks at first glance as though it should be possible to configure it for this scenario. You'll need to configure it to use OpenGL rendering and provide a suitable non-X11 OpenGL implementation (e.g. MesaGL with the 'fbdev' device driver). SDLJava is a Java port of the popular C SDL game development library. This also should be able to do what you ask for although it doesn't seem to have been updated since 2005 so if you have any problems with it support may not be forthcoming. As an alternative you could always use some fairly simple C code to open and configure the framebuffer and then use JNI to return the memory-mapped framebuffer as a direct-mode ByteBuffer so you can draw to it directly."
962,A,"Print designers moving to web ... what do they need to know? I'm trying to compile a guide for students used to publishing in print who are learning web design. Some obvious things which web developers know but they don't: You can't rotate graphics in HTML All objects have to be rectangular you can't have a circular DIV Many typographical effects in their repertoire can't be achieved Some things which are tricky are: Can they have variable opacity? Well yes and no. Can they have rounded corners? Maybe. Some things which aren't technical difficulties but which are problems: Image file sizes: I have a student who wants to have a different large graphic header on every page of their site; that's not technically a problem but it will mean a visitor has to wait for a new graphic to load every time they navigate Accessibility: ""why not just make everything a graphic to overcome the limitations of HTML?"" Please help me fill out my list and add any hints or tips for people making this transition. I think this would be great as a CW. Seeing how it has already gained momentum though I don't think switching now would be beneficial. =S Just wanted to mention. Fine by me. How do we do that? A lot of these are good rules of thumb for print designers who want to learn how to actually markup HTML and write CSS. But as a Web developer in the past I'd frequently just take a designer's template and write the HTML and CSS for them. Whether or not that task was simple or difficult depended on the designer's awareness of the capabilities of CSS. There was one pain point in particular that kept coming up. So for print designers moving to the web the absolute number one rule to remember is: Don't design any element to have an explicit pixel-perfect height. You can restrict the width all you want but changing fonts preferred font sizes and different text strings being pulled from the database on different pages means that text needs to be able to flow vertically without generating hideous hard-to-use overflow scrollbars. Designers who remember this can usually conjure up designs that are easy to cut up and integrate in a mostly semantic manner. Designers who forget this sometimes end up creating designs that have to be shoehorned into a 3 inch by 3 inch box and that's when I reach for the vodka.  Fonts and Text You are limited to a small subset of fonts Fonts are viewed at different sizes There is a readability limit for how wide paragraphs should stretch (in a fluid layout) Write for readers of all types - Some will skim others will read in detail Images Sites are viewed at different resolutions and screen sizes - Design accordingly To achieve transparent backgrounds in IE6 use PNG8 with alpha (IE6 doesn't support varying levels of transparency it's either 100% transparent or it's opaque) Use CSS sprites Images should not be used in place most of text The img tag should be used for images with semantic value and all layout images should be CSS images Every img tag needs to have the alt attribute to validate (X)HTML and CSS Browser rendering varies greatly Validate CSS and (X)HTML for a greater probability that the design will be cross-browser friendly Don't use CSS hacks Use the proper semantic markup Pages should be able to work without JavaScript enabled Read Yahoo's guide for performance and use YSlow Dreamweaver's design mode doesn't reflect how a page will appear in real browsers General Design Simpler is often better in terms of usability accessibility design and download size Lists of greater than five or six items should be broken up visually Consistency is important - Don't change your navigation etc without an extremely good reason When choosing colors keep those with color-blindness in mind. This will affect how you choose to convey meaning by color. Place the most important information above the fold (the part of the screen that shows without scrolling) The web is interactive. This drastically affects how you consume and display information. You can hide and subsequently display information using tabs accordion and similar methods. Think in terms of primary and secondary calls of action. What do you want the user to do? Where do you want them to go next? +1 for don´t replace text with images that´s the first thing I need to tell graphic designers when they start on web-sites. ""Lists of greater than five or six items should be broken up visually"" ... *looks at answer* ... hmm on a more serious note though a lot of your list already applies to print and hence would be redundant to tell print designers about. Example: Readability limits types of readers broken up lists color blindness - to name a few *Except when your category only has seven items. ;) True but I don't have a print background so I'm not sure what exactly they have and haven't been taught. I figured it was better to include it and let the OP take out redundancy.  The user controls how they want to see content on the web not you. Your design will not look the same to all people because some people may make it different on purpose. Screens can be arbitrarily large or small The web is interactive: usability trumps pretty-lookingness Your page will be read by machines: make sure the data is easy to get at by scripts that can't read images / large blobs of text (aka ""be semantic"")  I wrote a blog post about this a while ago - http://aloestudios.com/2008/08/dear-print-designer-doing-web-design/ So did my friend Mark - http://www.visual28.com/articles/tips-for-better-web-design  Jeffery Zeldman's book Taking Your Talent to the Web is specifically targeted to the question you have asked. It's been out for a few years...not sure if there's a 2nd or 3rd edition. Check it out. My main advise is that you need to recognize that while you have dot precision in print applications most of the time in web design your focus is to design and code a site that will accomplish your content and layout goals for any number of platforms resolutions and color depths. Color depth has become less important than it was in the past.  Remember to save your jpg files in RGB format not CMYK format. I regularly get sent jpgs that won't display on a web-site and every time it's because it's been saved in the wrong format from Photoshop. This will become less of a problem as browsers support more image formats but considering that 20%+ of users are still on IE6 for the sites we develop this will take a while to go away.  Coming from someone who has done both print design and web design (and done a decent job at both I think) it seems like you're off to a good start. Other thoughts: Darko Z mentioned this but I think it's worth stressing that browser incompatbilities must be recognized and dealt with. In the print industry there are standard formats like PDF which guarantee that things will come out in print the way they look in design; besides many publishers will directly accept the native file formats of the most popular design programs like Adobe InDesign Quark XPress even MS Word (for the cheapskates ;-P). The point being that print designers are used to a ""set it and forget it"" approach where they assume that once they design something a certain way it will stay designed. The fact that there are different web browsers which render the same web pages slightly differently is likely going to be a major pain in the butt for people used to the print world. Addendum to the above: fonts. You can't use (or at least can't rely on) uncommon fonts in web design for obvious reasons. Screen real estate has to be used effectively because there's a limited amount of it. And I mean really limited - no matter how hard you try you can't write HTML that will make someone's monitor expand 5 inches or put another screen on the back ;-) It's not like in print where people can peek back and forth between different pages of a book. Reading web pages is kind of like looking at parchment through binoculars; you have to design the pages with that limited field of view in mind. Web page designs are dynamic and transient; they stay up for a while they get boring they get recycled/replaced with new designs. So you're not stuck with mistakes. But it also means you need to design with future changes in mind e.g. by using CSS so you can change the look of whole classes of elements easily. There is some use of styles in print design but nowhere near as much as online.  web is not print Layouts can be fluid. elements don't have to be absolutely positioned web pages need to be checked in several browsers for compatibility avoid divitis; from experience people coming from print into this field do everything by brute force instead of trying to think of elegant solutions for optimization and semantics purposes print is consumed visually - the web is consumed by people with visual impairments as well. Don't forget lynx users no matter how small the market share is :) semantics is important learn about them thats all i can think of right now... ""web is not print""... isn't that the whole point? ;-) Good list though. it is the point but again from experience with print designers coming into web they always try to conform the web to print. Another example of this is OO and JavaScript. Developers coming from OO instead of learning functional programming try to make JS more like what they're used to.  A given color or font will render differently in different browsers. Especially when one browser is on Windows and the other is on Mac or Linux etc.  Some broad points: 1. Print is static the web is interactive. The essence of a print project is a fixed point in time an idea captured on paper or some other substrate. Web projects are moving changing experiences that represent both the ideas of their creators and their users. 2. Everything is uncertain. You mention typography in your answer it's probably worth broadening that to cover all aspects of appearance. The variety of operating systems and hardware available mean that its hard to determine how all your audience will experience your final design. Whilst some things must be compatible across all browsers sometimes it is not worth the time and effort needed to make something pixel perfect in all systems. 3. Learn about programming. Unless you've an aptitude for it you don't need to learn how to program for the web. But it would still be a big help to gain some familiarity with web programming as if you can't code you'll need to work closely with someone who can and you need to be able to communicate effectively with them. 4. Create working prototypes When something is static it can be designed using a static format. To design something interactive like a website you should be making use of moving prototypes that represent the kind of behaviour the final design will have. You can use paper to do this or more sophisticated mockups using xhtml css and javascript or a dedicated prototyping program."
963,A,"How to separate an image into two with java I'm wondering if there is a ""smart"" way of splitting an image based on certain features. The images are 300x57 black and white (actually grayscale but most colors are either black or white) it is comprised of two main features (let's call them blobs) separated by black space each blob slightly varies in width and height the position of the blobs also varies the blobs NEVER overlap! Here is what an image ""looks"" like: ------------------------- ----WWW---------WWWWW---- ---WWWWWWW----WWWWWW----- -----WWWW-------WWW------ ------------------------- The resulting split would be something like this: ------------ ------------- ----WWW----- ----WWWWW---- ---WWWWWWW-- --WWWWWW----- -----WWWW--- ----WWW------ ------------ ------------- Steps I plan to take in order to split the image: Scan the image from one side to the other. Determine the edges of the blobs. Take the distance between the two inside edges. Split the image at the middle of the inside distance. Save the two images as separate files. It would be nice if I normalize the image widths so all of my images have a uniform width when they're saved. I have no experience in image manipulation so I don't know what's an efficient way to do this. I'm currently using a BufferedImage getting the width/height iterating over each pixel etc. There is no wrong solution for my problem but I'm looking for a more efficient one (less code + faster). I've also been looking into java.awt.Graphics... I would appreciate if I get some ideas for more efficient ways to do this task. I want to stick with Java's built-in libraries so is BufferedImage or Graphics2D the most efficient thing to use in this case? EDIT: Here is the code after reading the suggestions: public void splitAndSaveImage( BufferedImage image ) throws IOException { // Process image ------------------------------------------ int height = image.getHeight(); int width = image.getWidth(); boolean edgeDetected = false; double averageColor = 0; int threshold = -10; int rightEdge = 0; int leftEdge = 0; int middle = 0; // Scan the image and determine the edges of the blobs. for(int w = 0; w < width; ++w) { for(int h = 0; h < height; ++h) { averageColor += image.getRGB(w h); } averageColor = Math.round(averageColor/(double)height); if( averageColor /*!=-1*/< threshold && !edgeDetected ) { // Detected the beginning of the right blob edgeDetected = true; rightEdge = w; }else if( averageColor >= threshold && edgeDetected ) { // Detected the end of the left blob edgeDetected = false; leftEdge = leftEdge==0? w:leftEdge; } averageColor = 0; } // Split the image at the middle of the inside distance. middle = (leftEdge + rightEdge)/2; // Crop the image BufferedImage leftImage = image.getSubimage(0 0 middle height); BufferedImage rightImage = image.getSubimage(middle 0 (width-middle) height); // Save the image // Save to file ------------------------------------------- ImageIO.write(leftImage ""jpeg"" new File(""leftImage.jpeg"")); ImageIO.write(rightImage ""jpeg"" new File(""rightImage.jpeg"")); } Note: after the right edge is detected one can simply break out of the for loop. Does the gap between blobs matter? If you don't need to balance the white space less work would be needed to just find a vertical white line between blobs. Check if the center vertical line has only white pixels. If the middle line has a black pixel scan left and right for the first line that has only white pixels. To check for situations where both blobs are to one side of center scan a horizontal line for black-white-black intervals. If the selected vertical line is within a white interval surrounded by black intervals you'll know there's at least one blob on each side of the image split. Failing these checks would require scanning additional lines but for all well formed images where the blobs are centered in the right and left halves of the image this method will take only two line scans. This method may take longer for other images or even break for edge case images. This would break for this example: ------------------------- ----WWW----WWWWWWWWWW---- ---WWWWWWW----WWWWWW----- -----WWWWWWWW---WWW------ ------------------------- But the question seems to indicate this situation is impossible. If the reason behind this image splitting requires processing every image you'll need a fall back method. You wouldn't need a fall back method if the edge cases can be rejected. Once the scanning finds that the image falls outside of acceptable ranges you can stop checking the image. For example if a vertical all white line can't be found in the center third of the image you may be able to reject the image. Or you can just use this method as an optimization running this check on just two lines to find and split the well formed images then passing the poorly formed images to a more thorough algorithm. I like the idea of scanning the horizontal line... I will experiment with it. In my sample of 1000+ images I have not had an image where portions of two blobs occupy the same vertical line the blobs have always been separated by a few white lines.  I'm not aware of an edge detection algorithm that doesn't require iterating through the pixels so your present approach may be optimal. Depending on other factors you may be able to leverage ImageJ which has an extensive collection of analytical plugins. Addendum: Given a preference for avoiding external dependencies BufferedImage is a good choice. Once you identify the edges the getSubimage() method is convenient. You may be able to use one of the Raster getPixels() methods effectively in the convolution. ImageIO can write the results. Yes; I've added some details above. @trashgod Good point... So are the BufferedImage and Graphics2D the best things to use in the java library (I don't want to go external)?  A simple way to do this is to sum the pixel values in each column (going down) to create a single array (the same width as your input image) of average values. Starting in the middle of the array search for the minimum value. This will be the column where you can split the image. This column probably won't be the center of the gap between your blobs. You can do another outward search from this column going left first to find all similar columns and then going right. ------------------------- ----WWW---------WWWWW---- ---WWWWWWW----WWWWWW----- -----WWWW-------WWW------ ------------------------- col avg: ---wwWWwww-----wWWWWww--- Depending on how blank the space is (pixel value wise) between the two blobs you can set your threshold value pretty low. If there is some noise it will have to be a little higher. Finding the right threshold value can be a chore unless you can determine it algorithmically. Yah that's what I ended up doing. I'll post the code for others to see. +1: I think this would work really well and is very simple.  I don't think there is any reason to do anything other than scanning each line and stop when you have gotten a white->black->white transition (no need to scan the entire line!). If you can make any guess about the position of the blobs you might be able to refine it a little by picking a starting point in the middle of the image and then searching left and right from there. But I seriously doubt it would be worth the effort. There is also no need to first run an edge detection algorithm on the image. Just scan the lines! EDIT: Mr. Berna pointed out that this will not work with concave objects. This breaks when the blob is concave. If the top of the blob was shaped like fingers you could get a white -> black -> white transition without finding the edge of first blob. @Mr. Berna: True. It will only work (reliably) for convex blobs."
964,A,svgalib : cant see anything I had compiled some examples from svgalib the console show : Using EGA driver svglib 1.4.3 Nothing more its like its drawing somewhere but I cannot see it. This could be a ver very noob question about svgalib but also a configuration problem. Also I check the virtual console that it says is drawing (if I run from X) running from console just stays there. I also put sleep in the code example code :  include stdlib.h include vga.h  int main(void) { vga_init(); vga_setmode(G320x200x256); vga_setcolor(4); vga_drawpixel(10 10); sleep(5); vga_setmode(TEXT); return EXIT_SUCCESS; } compile with gcc -o tut tut.c -lvga So do you have other SVGAlib applications working on your system? Such svgatest which may be in a separate distribution package (svgalib-bin or similar). Have you configured svgalib for your system? Common locate of the config file is /etc/vga/libvga.config and read man svgalib should give you more details. I suspect that once you have SVGAlib working in general the tutorial example program will work.
965,A,"Given an Array is there an algorithm that can allocate memory out of it? I'm doing some graphics programming and I'm using Vertex pools. I'd like to be able to allocate a range out of the pool and use this for drawing. Whats different from the solution I need than from a C allocator is that I never call malloc. Instead I preallocate the array and then need an object that wraps that up and keeps track of the free space and allocates a range (a pair of begin/end pointers) from the allocation I pass in. Much thanks. boost::pool does this for you very nicely!  Instead I preallocate the array and then need an object that wraps that up and keeps track of the free space and allocates a range (a pair of begin/end pointers) from the allocation I pass in. That's basically what malloc() does internally (malloc() can increase the size of this ""preallocated array"" if it gets full though). So yes there is an algorithm for it. There are many in fact and Wikipedia gives a basic overview. Different strategies can work better in different situations. (E.g. if all the blocks are a similar size or if there's some pattern to allocation and freeing) If you have many objects of the same size look into obstacks. You probably don't want to write the code yourself it's not an easy task and bugs can be painful.  in general: you're looking for a memory mangager which uses a (see wikipedia) memory pool (like the boost::pool as answered by TokenMacGuy). They come in many flavours. Important considerations: block size (fixed or variable; number of different block sizes; can the block size usage be predicted (statistically)? efficiency (some managers have 2^n block sizes i.e. for use in network stacks where they search for best fit block; very good performance and no fragementation at the cost of wasting memory) administration overhead (I presume that you'll have many very small blocks; so the number of ints and pointers maintainted by the memory manager is significant for efficiency) In case of boost::pool I think the simple segragated storage is worth a look. It will allow you to configure a memory pool with many different block sizes for which a best-match is searched for."
966,A,"Are there any rendering alternatives to rasterisation or ray tracing? Rasterisation (triangles) and ray tracing are the only methods I've ever come across to render a 3D scene. Are there any others? Also I'd love to know of any other really ""out there"" ways of doing 3D such as not using polygons. Aagh! These answers are very uninformed! Of course it doesn't help that the question is imprecise. OK ""rendering"" is a really wide topic. One issue within rendering is camera visibility or ""hidden surface algorithms"" -- figuring out what objects are seen in each pixel. There are various categorizations of visibility algorithms. That's probably what the poster was asking about (given that they thought of it as a dichotomy between ""rasterization"" and ""ray tracing""). A classic (though now somewhat dated) categorization reference is Sutherland et al ""A Characterization of Ten Hidden-Surface Algorithms"" ACM Computer Surveys 1974. It's very outdated but it's still excellent for providing a framework for thinking about how to categorize such algorithms. One class of hidden surface algorithms involves ""ray casting"" which is computing the intersection of the line from the camera through each pixel with objects (which can have various representations including triangles algebraic surfaces NURBS etc.). Other classes of hidden surface algorithms include ""z-buffer"" ""scanline techniques"" ""list priority algorithms"" and so on. They were pretty darned creative with algorithms back in the days when there weren't many compute cycles and not enough memory to store a z-buffer. These days both compute and memory are cheap and so three techniques have pretty much won out: (1) dicing everything into triangles and using a z-buffer; (2) ray casting; (3) Reyes-like algorithms that uses an extended z-buffer to handle transparency and the like. Modern graphics cards do #1; high-end software rendering usually does #2 or #3 or a combination. Though various ray tracing hardware has been proposed and sometimes built but never caught on and also modern GPUs are now programmable enough to actually ray trace though at a severe speed disadvantage to their hard-coded rasterization techniques. Other more exotic algorithms have mostly fallen by the wayside over the years. (Although various sorting/splatting algorithms can be used for volume rendering or other special purposes.) ""Rasterizing"" really just means ""figuring out which pixels an object lies on."" Convention dictates that it excludes ray tracing but this is shaky. I suppose you could justify that rasterization answers ""which pixels does this shape overlap"" whereas ray tracing answers ""which object is behind this pixel"" if you see the difference. Now then hidden surface removal is not the only problem to be solved in the field of ""rendering."" Knowing what object is visible in each pixel is only a start; you also need to know what color it is which means having some method of computing how light propagates around the scene. There are a whole bunch of techniques usually broken down into dealing with shadows reflections and ""global illumination"" (that which bounces between objects as opposed to coming directly from lights). ""Ray tracing"" means applying the ray casting technique to also determine visibility for shadows reflections global illumination etc. It's possible to use ray tracing for everything or to use various rasterization methods for camera visibility and ray tracing for shadows reflections and GI. ""Photon mapping"" and ""path tracing"" are techniques for calculating certain kinds of light propagation (using ray tracing so it's just wrong to say they are somehow fundamentally a different rendering technique). There are also global illumination techniques that don't use ray tracing such as ""radiosity"" methods (which is a finite element approach to solving global light propagation but in most parts of the field have fallen out of favor lately). But using radiosity or photon mapping for light propagation STILL requires you to make a final picture somehow generally with one of the standard techniques (ray casting z buffer/rasterization etc.). People who mention specific shape representations (NURBS volumes triangles) are also a little confused. This is an orthogonal problem to ray trace vs rasterization. For example you can ray trace nurbs directly or you can dice the nurbs into triangles and trace them. You can directly rasterize triangles into a z-buffer but you can also directly rasterize high-order parametric surfaces in scanline order (c.f. Lane/Carpenter/etc CACM 1980).  The Rendering article on Wikipedia covers various techniques. Intro paragraph: Many rendering algorithms have been researched and software used for rendering may employ a number of different techniques to obtain a final image. Tracing every ray of light in a scene is impractical and would take an enormous amount of time. Even tracing a portion large enough to produce an image takes an inordinate amount of time if the sampling is not intelligently restricted. Therefore four loose families of more-efficient light transport modelling techniques have emerged: rasterisation including scanline rendering geometrically projects objects in the scene to an image plane without advanced optical effects; ray casting considers the scene as observed from a specific point-of-view calculating the observed image based only on geometry and very basic optical laws of reflection intensity and perhaps using Monte Carlo techniques to reduce artifacts; radiosity uses finite element mathematics to simulate diffuse spreading of light from surfaces; and ray tracing is similar to ray casting but employs more advanced optical simulation and usually uses Monte Carlo techniques to obtain more realistic results at a speed that is often orders of magnitude slower. Most advanced software combines two or more of the techniques to obtain good-enough results at reasonable cost. Another distinction is between image order algorithms which iterate over pixels of the image plane and object order algorithms which iterate over objects in the scene. Generally object order is more efficient as there are usually fewer objects in a scene than pixels. From those descriptions only radiosity seems different in concept to me.  There's a technique called photon mapping that is actually quite similar to ray tracing but provides various advantages in complex scenes. In fact it's the only method (at least of which I know) that provides truly realistic (i.e. all the laws of optics are obeyed) rendering if done properly. It's a technique that's used sparingly as far as I know since it's performance is hugely worse than even ray tracing (given that it effectively does the opposite and simulates the paths taken by photons from the light sources to the camera) - yet this is it's only disadvantage. It's certainly an interesting algorithm though you're not going to see it in widescale use until well after ray tracing (if ever). Note that like ray tracing this method doesn't require polygons. Spheres can be represented perfectly for example. In reality it's easiest to model most complex objects using polygons anyway. Spheres can be represented perfectly in Raytracing as well. The other method that gives you photorealistic rendering is Path Tracing which is like Raytracing but with random lightflow instead of deterministic shadowing checks. It's quite possibly even slower than photon mapping. :) @Andrei: That's precisely what I was saying in fact! @FeepingCreature: Yeah it seems you are right. They're all in the same general family of renderers though photon mapping/path tracing do have their own advantages/disadvantages. Photon mapping *uses* ray tracing it's not separate from ray tracing (any more than ""ray traced shadows"" or ""ray traced reflections"" are separate from ""ray tracing""). Also photon mapping is used to compute particular propagation of light (certain types of global illumination) it's not directly applicable to computing the direct camera visibility of objects so photon mapping is not really a correct answer to the question. @Larry: Actually that's incorrect. They're in the same family of rendering algorithms you might say but ray tracing is substantially different - it's not capable of fully solving the rendering equation for one thing. Did that really merit a downvote anyway?"
967,A,Developing 2D in 3D with orthographic projection and stretched model I was wondering if there is any benefit to using an orthographic projection camera to make a 2D game with 3D models (as opposed to making sprites and using 2D). Also on a related note when I use orthographic projection my 3d-model is stretched. Does anyone know why this is happening? I know that aspect ratio only influences the perspective projection so this should not be happening. If you are making a 2d game ortho with 3d models might be better because you can take advantage of hardware acceleration with OpenGL and Direct3D. However sprites are much easier to make than 3d models. If I'm not mistaking 2d functions can also take advantage of hardware accelaration. Yes that's true but often 2D graphics libraries don't use acceleration or use the driver's 2D acceleration. With most graphics drivers 2D operations using 3D are faster that 2D.  If you developing a 2D game using 3D models would be a lot of pointless overhead - both for you as the developer and for the system running the game. Even if you have fancy 3D models and are not very good at drawing sprites you could still use a 3D modeling tool to project the models to images. You were right. I am doing 2d now and I am getting miles ahead of where I was when I was doing 3d. I will graduate to 3d eventually (if i keep this up) but for faster dev time 2d certainly is a speedup.
968,A,"How to set capabilities of a universe in Java 3d? How can I set the bounds on a SimpleUniverse instance created with a canvas3d object? I tried the code below but I get either a ""Capability not set exception"" if I try to set the bounds and a ""Restricted access exception"" if I try to set the capability to write bounds. Here's my code: GraphicsConfiguration config = SimpleUniverse.getPreferredConfiguration(); Canvas3D canvas3d = new Canvas3D(config); SimpleUniverse universe = new SimpleUniverse(canvas3d); ViewingPlatform viewPlatform = universe.getViewingPlatform(); // Below line throws RestricedAccessException viewPlatform.setCapability(ViewingPlatform.ALLOW_BOUNDS_WRITE); // I want to set the bounds thus the need for the capability above viewPlatform.setBounds(bounds); Please help! I figured it out. Rather than set up the universe like this: GraphicsConfiguration config = SimpleUniverse.getPreferredConfiguration(); Canvas3D canvas3d = new Canvas3D(config); SimpleUniverse universe = new SimpleUniverse(canvas3d); I set up the ViewingPlatform by itself then created the universe with it: GraphicsConfiguration config = SimpleUniverse.getPreferredConfiguration(); Canvas3D canvas3d = new Canvas3D(config); ViewingPlatform viewingPlatform = new ViewingPlatform(); viewingPlatform.setCapability(ViewingPlatform.ALLOW_BOUNDS_WRITE); viewingPlatform.setBounds(bounds); Viewer viewer = new Viewer(canvas3d); SimpleUniverse universe = new SimpleUniverse(viewingPlatform viewer);"
969,A,What's a good matrix manipulation library available for C? I am doing a lot of image processing in C and I need a good reasonably lightweight and above all FAST matrix manipulation library with a permissive license. I am mostly focussing on affine transformations and matrix inversions so i do not need anything too sophisticated or bloated. Primarily I would like something that is very fast (using SSE perhaps?) with a clean API and (hopefully) prepackaged by many of the unix package management systems. Note this is for C not for C++. Thanks :) OpenCV no i want matrix manipulation not image manipulation (im writing my own image manipulation lib) thanks though :) Look there is good MATRIX MATH in it too. Also it simplifies image processing. Thanks a lot :) but i really *just* need the matrix manipulation i am looking for something lightweight that provides just the functionality i need.. :) :D cool. Then BLAS looks good! +Pablo Is BLAS lightweight? :O  Armadillo have simple interface and can use different LAPACK and BLAS linear algebra libraries  You could try CUBLAS(CUDA Basic Linear Algebra Subroutines library) with CUDA enabled graphics card to do matrix manipulation on nVidia GPUs. It has quite significant performance boost than other CPU libraries though it is not that lightweight to your requirement. This page contains some description and figures about it.  I found this library and it's brilliant: Meschach  I'd say BLAS or LAPACK. Here you have some examples. BLAS and LAPACK are generally considered interfaces in the same way that OpenGL is an interface with many underlying implementations by graphics card vendors. That said the fastest BLAS out there right now is GotoBLAS2 but its license is restrictive. Intel and AMD provide their own libraries (MKL and ACML) but those are also restrictive. Stick to the netlib reference implementations if you want truly free. You generally need to compile these yourself for your processor to get the best performance. this looks great :) but i can't find a precompiled binary of the library anywhere for my (unfortunately right now) windows system. Any idea where i could find it? Start looking here: http://blogs.msdn.com/hpctrekker/archive/2009/02/24/hpc-math-attacks-blas-lapack-linpack-atlas-dgemm-acml-mkl-cuda.aspx @banister: the other option to downloading an existing binary is to build your own. A good way to do so is to use ATLAS (http://math-atlas.sourceforge.net/) which produces a BLAS tuned to your machine. See http://math-atlas.sourceforge.net/errata.html#WinBuild and http://math-atlas.sourceforge.net/errata.html#winpt for windows-specific info (also read the rest of the errata).
970,A,How to fill color in only a part of the image in iPhone I am using the following code to fill color in an image in iPhone. - (UIImage *)colorizeImage:(UIImage *)baseImage color:(UIColor *)theColor { UIGraphicsBeginImageContext(baseImage.size); CGContextRef ctx = UIGraphicsGetCurrentContext(); CGRect area = CGRectMake(0 0 baseImage.size.width baseImage.size.height); CGContextScaleCTM(ctx 1 -1); CGContextTranslateCTM(ctx 0 -area.size.height); CGContextSaveGState(ctx); CGContextClipToMask(ctx area baseImage.CGImage); [theColor set]; CGContextFillRect(ctx area); CGContextRestoreGState(ctx); CGContextSetBlendMode(ctx kCGBlendModeMultiply); CGContextDrawImage(ctx area baseImage.CGImage); UIImage *newImage = UIGraphicsGetImageFromCurrentImageContext(); UIGraphicsEndImageContext(); return newImage; } But the above code fills the color in the whole image. I just want to feel upper part say 1/4th of the image. So how to do it? I figured it out.. change the line of CGContextFillRect to. CGContextFillRect(ctx CGRectMake(0 0 baseImage.size.width 100)); I figured it out.. change the line of CGContextFillRect to. CGContextFillRect(ctx CGRectMake(0 0 baseImage.size.width 100)); okay answer this issue-http://stackoverflow.com/questions/8124308/filling-a-portion-of-an-image-with-color This is for CGRECT but if image is in other shape then whay to do??
971,A,Is my understanding of the functions of compass & GPS correct in AR apps? In an AR app whereby you annotate objects or buildings in a camera view I want to understand the role that different hardware bits - on the phone (iPhone/Android) - play to achieve the AR effect. Please elaborate more on the following: Camera: provides the 2D view of reality. GPS: provides the longitudelatitude of the device. Compass: direction with respect to magnetic north. Accelerometer: (does it have a role?) Altimeter: (does it have a role?) An example: if the camera view is showing the New York skyline how does the information from the hardware listed above help me to annotate the view ? Assuming I have the longitude & latitude for the Chrysler building and it is visible in my camera view how does one calculate with accuracy where to annotate the name on the 2D picture ? I know that given 2 pairs of (longitudelatitude) you can calculate the distance between the points. use the camera to get the field of view. use the compass to determine the direction the device is oriented in. The direction determines the set of objects that fall into the field view and need to be reflected with AR adorners. use the GPS to determine the distance between your location and each object. The distance is usually reflected in the size of the AR adorner you show for that object or in the number of details you show. use the accelerometer to determine the horizon of the view (a 3-way accelerometer sensitive enough to measure the gravity force). The horizon can be combined with the object altitude to position the AR adorners properly vertically. use the altimeter for added precision of vertical positioning. if you have a detailed terrain/buildings information you can also use the altimeter to determine line of visibility to the various objects and clip out (or partially) the AR adorners for partially obscured or invisible objects. if the AR device is moving use the accelerometers to determine the speed and do some either throttling of the number of objects downloaded per view or smart pre-fetching of the objects that will come into view to optimize for the speed of view changes. I will leave the details of calculating all this data from the devices as an exercise to you. :-) Thanks Franci. I have a much clearer picture on what is involved in an AR app.
972,A,"How to make a System.Drawing.Image semitransparent? System.Drawing.Graphics.DrawImage pastes one image on another. But I couldn't find a transparency option. I have already drawn everything I want in the image I only want to make it translucent (alpha-transparency) http://stackoverflow.com/questions/189392/how-do-you-draw-transparent-image-using-system-drawing @Mitch Wheat - that question is specific to GIFs GIFs don't have semi-transparency. I am talking about PNGs here Post the code that you are using and give us some details about the images you are looking to combine and we can probably help. I copied an answer from the link Mitch provided that I think will work for me: public static Bitmap SetOpacity(this Bitmap bitmap int alpha) { var output = new Bitmap(bitmap.Width bitmap.Height); foreach (var i in Enumerable.Range(0 output.Palette.Entries.Length)) { var color = output.Palette.Entries[i]; output.Palette.Entries[i] = Color.FromArgb(alpha color.R color.G color.B); } BitmapData src = bitmap.LockBits( new Rectangle(0 0 bitmap.Width bitmap.Height) ImageLockMode.ReadOnly bitmap.PixelFormat); BitmapData dst = output.LockBits( new Rectangle(0 0 bitmap.Width bitmap.Height) ImageLockMode.WriteOnly output.PixelFormat); bitmap.UnlockBits(src); output.UnlockBits(dst); return output; } it didn't work as the Bitmap.Pallete.Entries is empty  There is no ""transparency"" option because what you're trying to do is called Alpha Blending. public static class BitmapExtensions { public static Image SetOpacity(this Image image float opacity) { var colorMatrix = new ColorMatrix(); colorMatrix.Matrix33 = opacity; var imageAttributes = new ImageAttributes(); imageAttributes.SetColorMatrix( colorMatrix ColorMatrixFlag.Default ColorAdjustType.Bitmap); var output = new Bitmap(image.Width image.Height); using (var gfx = Graphics.FromImage(output)) { gfx.SmoothingMode = SmoothingMode.AntiAlias; gfx.DrawImage( image new Rectangle(0 0 image.Width image.Height) 0 0 image.Width image.Height GraphicsUnit.Pixel imageAttributes); } return output; } } Alpha Blending [ColorMatrix](http://msdn.microsoft.com/en-us/library/system.drawing.imaging.colormatrix(v=vs.110).aspx) + [ImageAttributes](http://msdn.microsoft.com/en-us/library/system.drawing.imaging.imageattributes(v=vs.110).aspx) + [SmoothingMode](http://msdn.microsoft.com/en-us/library/z714w2y9(v=vs.110).aspx)"
973,A,Allow SVG graphics to overflow outside the containing svg element I'm using the Raphaël—JavaScript Library to create some pie chart graphics but I want to set the overflow of the containing SVG element to visible to allow the graphics inside to display outside of their container. The way you would with normal dom elements (overflow: visible;) does not seem to work for svg containers. Does anyone have any experience of this or know how to remedy it? Cheers Why not just change the size of the container to prevent the clipping? I'm trying to avoid hacking the layout making the container bigger and positioning it would ruin the flow for the rest of the elements. The outermost svg element clips the rendering. I'm not aware of any way to change that. as far as I can see you are correct there is no way to achieve this.  It's a bug in Firefox. WebKit implements overflow: visible correctly for SVG elements as does IE for VML elements.
974,A,How would you go about implementing the game reversi? (othello) I have been thinking about starting a side project at home to exercise my brain a bit. Reversi looks like a simply game where mobility has a profound effect on game play. It is at least a step up from tic tac toe. This would be a single player against an AI of some sort. I am thinking to try this in C++ on a PC. What issues am I likely to run into? What graphics library would you recommend? What questions am I not smart enough to ask myself? a strategy. good question. Are you talking about implementing a strategy for the reversi or simply let two players play on the computer ? Reversi should be a very simple game to implement. It is perfect to learn some basic algorithms of games theory (specifically min-max) during the implementation of the AI. One thing to note on the AI is that it is perfectly possible to make a perfect AI for Reversi (one that always wins no matter the moves of its opponent). So on the strategy side if your AI loses you still have work to do :) I'm not sure whether this is entirely true if the opponent is also an AI. Wikipedia says perfect play with this game is believed to end up to a draw. I don't remember exactly but it seems to me that with two AI players one of the colors always wins (I don't know if it is white or black) by exactly one point. That's not true. Reversi is unsolved. Checkers has been solved. Reversi has not been solved.  In overall issues you will end up running onto will depend on you and your approaches. Friend tends to say that complex is simple from different perspective. Choice of graphics library depends about what kind of game you are going to write? OpenGL is common choice in this kind of projects but you could also use some GUI-library or directly just use windows' or xorg's own libraries. If you are going to do fancy just use OpenGL. Questions you ought ask: Is C++ sensible choice for this project? Consider C and/or python as well. My answer to this would be that if you just want to write reversi go python. But if you want to learn a low level language do C first. C++ is an extension to C therefore there's more to learn in there than there's in C. And to my judge the more you have to learn onto C++ is not worth the effort. How do you use the graphics library? If you are going to do fancy effects go to the scene graph. Instead you can just render the reversi grid with buttons on it. How ought you implement the UI should you use the common UI concepts? Usual UI concepts (windowing frames buttons menubars dialogs) aren't so good as people think they are there's lot of work in implementing them properly. Apply the scene graph for interpreting input and try different clever ways onto controlling the game. Avoid intro menus(they are dumb and useless work) use command line arguments for most configuration. I yet give you some ideas to get you started: Othello board is 8x8 64 cells in overall. You can assign a byte per each cell that makes it 64 bytes per each board state. It's 8 long ints not very much at all! You can store the whole progress of the game and the player can't even notice it. Therefore it's advised to implement the othello board as an immutable structure which you copy always when you change a state. It will also help you later with your AI and implementing an 'undo' -feature. Because one byte can store more information than just three states (EMPTY BLACK WHITE) I advice you will also provide two additional states (BLACK_ALLOWED WHITE_ALLOWED BOTH_ALLOWED). You can calculate these values while you copy the new state. Algorithm for checking out where you can put a block could go the board through one by one then trace from empty cells to every direction for regex-patterns: B+W => W^ W+B => B^ This way you can encapsulate the game rules inside a simple interface that takes care of it all. Bad advice. C++ is not just an extension it's a different approach. The only thing they really share is some syntax no more than C++ shares with javascript. C++ would give you a ton of benefits like better memory management (through boost even) templates objects and so on that will help. It's harder to track a program state without an object to help because you can't have independent data it's either global or on the stack in C.  You will want to look into minimax with alpha-beta pruning if you write an AI to play against. Your favorite search engine will have much to say on the topic.  I wrote a reversi game many years ago when I was still at school. Its strategy was very simple it just went for the maximum number of pieces but weighted so it preferred the edges and particularly the corners and didn't like squares that risked giving away the corners. This worked fairly well against people who hadn't yet worked out what it was doing but once you had it was very easy to use its strategy against it. I'm not proud to say however that it beat me the first few times even though I'd written it! A proper AI with a few moves of lookahead is far more complicated. Should be an interesting problem but at the time I was more interested in the user interface. Maximization leads directly to an early loss.  As mentioned by others I would begin by getting a deep understanding of the gameplay and strategies and the algorithms involved. This link may be useful to you it describes basic Othello strategy and algorithms: http://www.site-constructor.com/othello/Present/Basic_Strategy.html  As the guys were suggesting my idea of telling you for thinking first for algorithms and the game logic. next answer for me was the graphics library it depends on your target platform programming language framework etc. But as I suggest is using C# with Cairo 2D graphics library which you can achieve this using Mono framework (which then you can target all three major operating systems for your game to work) -> www.mono-project.org. Meanwhile I found this I think that and this kind of resource will help you: http://home.datacomm.ch/t_wolf/tw/misc/reversi/html/index.html. But if you finish this you can try implementing Sudoku.  After you've taken a whack at the game logic yourself go read chapter 18 of Peter Norvig's outstanding book Paradigms of AI Programming. (Source code here.) It has a rather short and extremely readable program that can kick just about any human's butt; you ought to learn a lot by comparing your solution to it.  Issues... Well just be sure when writing the strategy part of the game not to simply do the move that gives you the most pieces. You must also give weight to board position. For example given the opportunity to place a piece in a board corner should take priority over any other move (besides winning the game) as that piece can never be turned back over. And placing a piece adjacent to a corner spot is just about the worst move you can ever make (if the corner space is open). Hope this helps!
975,A,"I would like to learn about creating graphic objects at Runtime in c# I would like to learn about creating a program that I could draw simple shapes and be able to select them for editing - like resizing order of display color change. Is there an online resource that someone knows of that would help me reach my goals. thanks ""GDI+"" is what you are looking for. You can start here: http://msdn.microsoft.com/en-us/library/da0f23z7.aspx  A sneaky way I used to do it was to create a custom control remove the background from it and paint my shapes and sizes on it. Then you can easily implement selection (override OnClick) dragging and resizing (OnMouseDown OnMouseMove OnMouseUp). You can then implement options like color etc. by means of a property (see Browsable attribute and property get/setters) and a PropertyGrid control. Anything beyond that though - Bezier curves and such - would need something a tad more advanced though. The alternative is to only use such controls for the sizing handles and do all the drawing on one central canvas - the only drawback then is figuring out how to select a shape on the canvas. Have you any examples I could work from? Unfortunately none that I have handy (at work at the moment) but the basic steps are: 1. Create a custom control 2. Override OnPaint to make it draw what you require 3. Override OnClick method to ""select"" it 4. Override OnMouseDown/Move/Up to implement dragging"
976,A,Resizing Columns Algorithm I have a set of columns of varying widths and I need an algorithm to re-size them for some value y which is greater than the sum of all their widths. I would like the algorithm to prioritize equalizing the widths. So if I have a value that's absolutely huge the columns will end up with more or less the same width. If there isn't enough room for that I want the smaller cells to be given preference. Any whiz bang ideas? I'd prefer something as simple as: getNewWidths(NewWidth ColumnWidths[]) returns NewColumnWidths[] Pseudocode: w = NewWidth n = ColumnWidths.count sort(ColumnWidths ascending) while n > 1 and ColumnWidths[n-1] > (w/n): w = w - ColumnWidths[n-1] n = n - 1 for i = 0 to n-1: ColumnWidths[i] = w / n You'll need to add some code to redistribute any roundoffs from the w/n calculation but I think this will do it. Thanks. Works very well.  I would decompose that in two steps first decide on how much of equalizing you want (between 0 and 1) and only second adapt it to the new total width. For example as in def get_new_widths new_total widths max = widths.max f = how_much_equalizing(new_total) # return value between 0.0 and 1.0 widths = widths.collect{|w| w*(1-f)+max*f} sum = widths.inject(0){|ab|a+b} return widths.collect{|w| w/sum*new_total} end def how_much_equalizing new_total return [1.0 (new_total / 2000.0)].min end  Mark Ransom's answer gives the right algorithm but in case you're having trouble figuring out what's going on there here's an actual implementation in Python: def getNewWidths(newWidth columnWidths): # First find out how many columns we can equalize # without shrinking any columns. w = newWidth n = len(columnWidths) sortedWidths = sorted(columnWidths) # A sorted copy of the array. while sortedWidths[n - 1] * n > w: w -= sortedWidths[n - 1] n -= 1 # We can equalize the n narrowest columns. What is their new width? minWidth = w // n # integer division sparePixels = w % n # integer remainder: w == minWidth*n + sparePixels # Now produce the new array of column widths. cw = columnWidths[:] # Start with a copy of the array. for i in range(len(cw)): if cw[i] <= minWidth: cw[i] = minWidth if sparePixels > 0: cw[i] += 1 sparePixels -= 1 return cw
977,A,"How do I start doing diagram development in c# I'd like to try to create a diagram making tool (something like entity-relation diagram you can create in SQL Server 2005 or class diagrams you can do in Microsoft Visual Studio) Ie. I'd like to create boxes put text in them be able to edit this text and draw lines between boxes. I never did this kind of programming before so I don't know where to start. Do I use XAML or create a canvas and go into graphics programming? I know there are some diagram tools out there but I'd really like to find out of these things by doing it myself. A while back I tried to do something similar myself with logic diagrams and didn't get very far. I'm highly interested in your project and the answers you get. Well here is the basis of one on CodeProject with source and a tutorial. For WPF. Also see this question. Wow that looks like 80% of any diagramming app great link! +1  As Frank has pointed out - creating interactive graphics is big business. Many companies have already been there spent £millions perfecting it. It would depend upon how much interactivity you want. I would advise you pick a graphing application that is extensible or has an API - that suits your budget. The better ones are (in my opinion) in the CAD domain such as AutoCAD (£4k/seat) TurboCAD VeCAD (£200/seat); and all have packages that allow you to resell their product within yours.  WPF/XAML are a great place to start for something like this. You'll want to study WPF in general with a focus on custom controls (for tables) and drawing lines (for the relationships.)  The rendering engine you pick is somewhat arbitrary - you're going to have to do a lot of work no matter what framework you use. Having implemented such a system in C# and WinForms I can honestly say that's a bad route to go. Stick to WPF/Silverlight. Going with trends write a Silverlight 4 app so you can deploy to desktops on multiple platforms. I found that there are a lot of high-level decisions that you have to make that are even more important than which rendering engine you use. Some of these are: How much do you need to zoom and pan around? Do you want continuous zoom so that you can do neat animations to the data? Do you need to be able to pan with just the mouse? How will you distinguish pan movements from dragging around boxes and handles? Will you change the layout of content in the boxes dependent upon the current zoom level? or will you rely on font scaling? Do you need grouping? Once you get a few tables on the screen you will soon realize that being able to hide some detail is useful. Being able to group boxes and to show that group in iconic form as ""meta"" box allows the user to get rid of unwanted distractions. Do you need search? Again trying to combat the ""too much on the screen"" problem it's good to have a search box that hides everything that doesn't match the search (gray out hide etc.) How will the user interact with the keyboard. Since your audience can include programmers you're going to want to give a lot of thought to making all your diagrams editable with just the keyboard. This means things like handling focus intelligently along with which hot keys to use."
978,A,The difference between triangulation and mesh I have done some computer graphical programming recently and I have no experience before. I used the library call CGAL(computer geometry algorithm library). Also I noticed that there is class for triangulation and also class for mesh. Is mesh just a kind of triangle net? Do they have any differences? Thanks! Triangulation is one way to mesh the geometry. And it is also possible to represent geometry in different shapes.  this should be of help http://www.cgal.org/Manual/3.5/doc_html/cgal_manual/contents.html
979,A,"draw text using coregraphics in iphone How to draw text on imageview using coregraphics in iphone Either use a UILabel over your imageView or draw a string in the current graphics context with - (CGSize)drawInRect:(CGRect)rect withFont:(UIFont *)font lineBreakMode:(UILineBreakMode)lineBreakMode thanks it worked Just what I was looking for! Just to expand on the answer this is a method on NSString. So for example: `[@""test"" drawInRect: CGRectMake(0 0 10 10) withFont:[UIFont systemFontOfSize:8]];` This text gets up-side down when I use it with coregraphics image drawing code where I have to CGContextScaleCTM and CGContextTranslateCTM to convert between coregraphics and device coordinates system. How can I not have these lines effect on string drawrect method?"
980,A,Is there a way to capture a bitmap from a WPF window using native C++? Imagine a document window in a MDI application which contains a child WPF window say a sidebar for example. How can one get a bitmap containing both the WPF pixels AND the GDI (non-wpf) pixels? I've discovered that when making my thumbnail preview for the Win7 taskbar app icon hover I get black in the parts of the preview where the WPF pixels should be. My current method simply grabs a bitmap capture of the document window. Then I get a DC for the preview make a memory DC from it and select my bitmap into it. Then I do some size adjustments and bitblt the memory dc to the real dc. I'm guessing that the BitBlt operation doesn't take into account the fact that the WPF pixels are hardware accelerated and therefore need to be grabbed from the graphics hardware. All the stuff in GDI is managed just fine though and when there's no WPF child windows the preview image looks fine. I'm wondering if it's at all possible to grab a bitmap of the WPF window from native C++. Then I can blt that onto the black area of the previous preview. I'm thinking that since WPF is hardware-accelerated there should be a way to get the pixels from DirectX. Not sure how I'd do that... Is there a native version of RenderTargetBitmap? I've been told that perhaps the best way to go is to create a C++/CLI interop layer that I can call into from C++ that will return a CBitmap for me. In that case I can use RenderTargetBitmap to get the WPF pixels. Maybe I'm not understanding your current approach correctly but could you do a BitBlt() from the screen DC to your memory DC? You'd need to get the screen rect of your window but that shouldn't be too bad. Ok I wasn't sure exactly what your current approach was based on your question. I hope that you get a good answer to your question. :) That won't work with WPF since WPF renders to the graphics hardware not to a device context. That is the process that I'm using so far and all I get is black pixels. You could of modified your hardware level to less than 2 WPF would then produce software rendering.  To solve this I had to create an abstract class in native code containing a virtual method to get the bitmap that was implemented in C++/CLI. In the managed implementation I used .NET's RenderTargetBitmap class to get a bitmap capture of the WPF window and then I filled up the passed in CBitmap object (see http://stackoverflow.com/questions/2625170/how-to-get-an-bitmap-struct-from-a-rendertargetbitmap-in-c-cli). In the unmanaged caller routine I used the virtual method to obtain the Bitmap. In short there was no way to get the bitmap by simply using unmanaged C++ since WPF and GDI really don't work together for all practical purposes.
981,A,"Emulating old-school sprite flickering (theory and concept) I'm trying to develop an oldschool NES-style video game with sprite flickering and graphical slowdown. I've been thinking of what type of logic I should use to enable such effects. I have to consider the following restrictions if I want to go old-school NES style: No more than 64 sprites on the screen at a time No more than 8 sprites per scanline or for each line on the Y axis If there is too much action going on the screen the system freezes the image for a frame to let the processor catch up with the action From what I've read up if there were more than 64 sprites on the screen the developer would only draw high-priority sprites while ignoring low-priority ones. They could also alternate drawing each even numbered sprite on opposite frames from odd numbered ones. The scanline issue is interesting. From my testing it is impossible to get good speed on the XBOX 360 XNA framework by drawing sprites pixel-by-pixel like the NES did. This is why in old-school games if there were too many sprites on a single line some would appear if they were cut in half. For all purposes for this project I'm making scanlines be 8 pixels tall and grouping the sprites together per scanline by their Y positioning. To clarify I'd be drawing sprites to the screen in batches of 8x8 pixels not 1x1. So dumbed down I need to come up with a solution that.... 64 sprites on screen at once 8 sprites per 'scanline' Can draw sprites based on priority Can alternate between sprites per frame Emulate slowdown Here is my current theory First and foremost a fundamental idea I came up with is addressing sprite priority. Assuming values between 0-255 (0 being low) I can assign sprites priority levels for instance: 0 to 63 being low 63 to 127 being medium 128 to 191 being high 192 to 255 being maximum Within my data files I can assign each sprite to be a certain priority. When the parent object is created the sprite would randomly get assigned a number between its designated range. I would then draw sprites in order from high to low with the end goal of drawing every sprite. Now when a sprite gets drawn in a frame I would then randomly generate it a new priority value within its initial priority level. However if a sprite doesn't get drawn in a frame I could add 32 to its current priority. For example if the system can only draw sprites down to a priority level of 135 a sprite with an initial priority of 45 could then be drawn after 3 frames of not being drawn (45+32+32+32=141) This would in theory allow sprites to alternate frames allow priority levels and limit sprites to 64 per screen. Now the interesting question is how do I limit sprites to only 8 per scanline? I'm thinking that if I'm sorting the sprites high-priority to low-priority iterate through the loop until I've hit 64 sprites drawn. However I shouldn't just take the first 64 sprites in the list. Before drawing each sprite I could check to see how many sprites were drawn in it's respective scanline via counter variables . For example: Y-values between 0 to 7 belong to Scanline 0 scanlineCount[0] = 0 Y-values between 8 to 15 belong to Scanline 1 scanlineCount[1] = 0 etc. I could reset the values per scanline for every frame drawn. While going down the sprite list add 1 to the scanline's respective counter if a sprite gets drawn in that scanline. If it equals 8 don't draw that sprite and go to the sprite with the next lowest priority. SLOWDOWN The last thing I need to do is emulate slowdown. My initial idea was that if I'm drawing 64 sprites per frame and there's still more sprites that need to be drawn I could pause the rendering by 16ms or so. However in the NES games I've played sometimes there's slowdown if there's not any sprite flickering going on whereas the game moves beautifully even if there is some sprite flickering. Perhaps give a value to each object that uses sprites on the screen (like the priority values above) and if the combined values of all objects w/ sprites surpass a threshold introduce slowdown? IN CONCLUSION... Does everything I wrote actually sound legitimate and could work or is it a pipe dream? What improvements can you all possibly think with this game programming theory of mine? this is all very cool and you have my +1 for being thorough.. but is there a *question* here? +1 because you managed to get my to read it all. Under ""In Conclusion"". :) I asked if this sounded legitimate and whether or not it could work or if there could be improvements to my theory. I consider this a problem because I stated my goals and my current theory to obtain them and I'm asking for advice on how to do so :D I should also state that I would also need to manually program this all by myself as I'm emulating graphical features of the NES via XNA. So 'priority levels' like I mentioned above is a wholly unique concept I thought up of to help solve this problem XD +1 for madness. I love it! I'm not going to propose a formal answer because I haven't thought it through too much; but couldn't you get your slowdown if you just added a sleep(x) after each update run where x is proportional to the number of objects currently in-game? If there are too many there's a longer delay between updates. Firstly I love the idea. When making a retro game it wouldn't be the same if you ignored the complexity limitations. By writing a framework that sort of enforced it is a great idea. Can I just say if you abstracted the engine as an open source ""Retro Game"" graphics engine that would indeed be cool and I'd be sure to join in! In regards to your highest priority sprites maybe a Priority Queue would simplify this part in that you can just pull out the 64 highest priority sprites. With the slow down perhaps in the engine you could have a limitValue. Each sprite as an appropriate limitUse. Add up all the limitUse vars and if this is below limitValue no slow down. Now for slowDelay = f(limitUseTotal - limitValue) where f is a function that converts the excess amount of sprites into a calculated slow down value. Is this the general idea you are suggesting or have I misread it? I'm guessing Priority Queue would basically be the same as me creating a custom sprite class and sorting it with a custom sorter? :) And for the slowdown that is the idea right on the head. However I personally would feel more comfortable capping it with a certain value e.g. a delay no longer than 16ms. Cuz I could see a math error delaying the redraw cycle by 100ms or so haha. Thanks! Yes whatever you decide for your f(x) function could include an upper bound.  For priority just use the Z-index. That's what the GBA does at least; priority 0 is rendered front-most thus has the weakest priority. So render high to low and if you've rendered too many sprites stop. A sprite's priority is determined by its index in the sprite table. The GBA had 128 entries [0127] arranged as sequential four hwords (16 bytes each). For XNA you'd probably use a List<Sprite> or similar instead. Maybe a Sprite[256] to simplify things a bit. Limiting the sprite rendering falls naturally from this mechanism. Because you render from 255 to 0 you can keep track of what scanlines you have touched. So basically: int[] numPixelsDrawnPerScanline = new int[Graphics.ScreenHeight]; foreach(Sprite sprite in SpriteTable.Reverse()) { int top = Math.Max(0 sprite.Y); int bottom = Math.Min(Graphics.ScreenHeight sprite.Y + sprite.Height); // Could be rewritten to store number of pixels to draw per line starting from the left. bool[] shouldDrawOnScanline = new bool[Graphics.ScreenHeight]; for(int y = top; y < bottom; ++y) { bool shouldDraw = numPixelsDrawnPerScanline[y] + sprite.Width <= PixelsPerScanlineThreshold; shouldDrawOnScanline[y] = shouldDraw; if(shouldDraw) { numPixelsDrawnPerScanline[y] += sprite.Width; } } Graphics.Draw(sprite shouldDrawOnScanline); // shouldDrawOnScanline defines clipping skipDrawingSprite: } I'm not quite sure what you mean by the slowdown so I can't respond to that part of your question. Sorry. Hope this helps. The GBA has an extra wrinkle regarding sprites with hardware rotation/scaling. Sprites that have hw scaling/rotation are more costly to render so the hardware can render fewer of them. e.g. if you fill the screen by stretching 4 64x64 sprites (double size should cover 256x256 in theory) chances are the lower portion of the screen will not have its sprites drawn at all."
982,A,"Which containers / graphics components to use in a simple Java Swing game? I'm creating a simple labyrinth game with Java + Swing. The game draws a randomized labyrinth on the screen places a figure in the middle and the player is then supposed to find the way out by moving the figure with arrow-keys. As for now I'm using a plain background and drawing the walls of the labyrinth with Graphics.drawLine(). I have a custom picture of the figure in a .gif file which I load as a BufferedImage object. However I want the player to see only part of the labyrinth at a time and the screen should follow the figure in the game as the player moves around. I'm planning to do this by creating an Image object of the whole labyrinth when it is created and then ""cutting"" a square around the current position of the figure and displaying this with Graphics.drawImage(). I'm new with Swing though and I can't figure out how to draw the figure at different positions ""above"" the labyrinth without redrawing the whole thing. Which container/component should I use for the labyrinth and then for the figure to achieve this? I mean the player is always centered. The player can see a square of certain size around them i.e. there's a square of visible area which always focuses on the player. When you say ""the screen should follow the figure"" do you mean the player is always centered? Can the player see only the section of the labyrinth they are in or can they see areas they have previously explored? and I can't figure out how to draw the figure at different positions ""above"" the labyrinth without redrawing the whole thing. Use a JLabel containing an icon. Then you just use label.setLocation() to move the image. Swing is smart enough to repaint the old area (where the image was) and then paint the image at its new location. You should be able to do this with a single JPanel. Your custom painting of the panel will paint the maze. The the label which is added as a child component will be painted on top of the maze. Check out the last entry of this posting which contains an interesting approach for building the maze and it also supports scrolling of the maze as the player moves.  You might look at JLayeredPane discussed in How to Use Layered Panes and in this question.  I think that the glass pane is the component you are looking for: it's a sort of glass placed onto the frame. Everything you draw on it will cover what's behind.. so you can achieve easily the effect without caring about clipping the original image.. see here!"
983,A,Copy Small Bitmaps on to Large Bitmap with Transparency Blend: What is faster than graphics.DrawImage(smallBitmap x  y)? I have identified this call as a bottleneck in a high pressure function. graphics.DrawImage(smallBitmap x  y); Is there a faster way to blend small semi transparent bitmaps into a larger semi transparent one? Example Usage: XY[] locations = GetLocs(); Bitmap[] bitmaps = GetBmps(); //small images sizes vary approx 30px x 30px using (Bitmap large = new Bitmap(500 500 PixelFormat.Format32bppPArgb)) using (Graphics largeGraphics = Graphics.FromImage(large)) { for(var i=0; i < largeNumber; i++) { //this is the bottleneck largeGraphics.DrawImage(bitmaps[i] locations[i].x  locations[i].y); } } var done = new MemoryStream(); large.Save(done ImageFormat.Png); done.Position = 0; return (done); The DrawImage calls take a small 32bppPArgb bitmaps and copies them into a larger bitmap at locations that vary and the small bitmaps might only partially overlap the larger bitmaps visible area. Both images have semi transparent contents that get blended by DrawImage in a way that is important to the output. I've done some testing with BitBlt but not seen significant speed improvement and the alpha blending didn't come out the same in my tests. I'm open to just about any method including a better call to bitblt or unsafe c# code. I was afraid of that. I think DirectX is probably out for me because the code runs on a web server. Did you try blending using bitwise operators on unsafe int arrays? That was the one thing I'm considering trying but it would be a fair amount of work to set it up to test with my use case and I'd love to avoid building that test case if you have tried it. Thanks for the feedback. I actually don't think so (not without utilizing hardware acceleration; maybe DirectX could help you here). I've been toying around with a similar problem. I found that I could write a faster DrawImage in assembler when I painted semi transparent images onto an *opaque* image because I could cheat a little bit then but not otherwise. Dan After some testing I think you're right. If you post your comment below as an answer I'll mark it as accepted. After some testing I see that Dan was right (see comments above). It is possible to beat GDI Draw Image performance if you don't need all the blending features it offers but in the case of transparency blends I don't think there is room for material improvement.
984,A,Effect To Show Panel in C#.net I Want Use a Panel in a Windows Form in C#.net. I Set Visible Property of this Control to false And When I Click on a Button Panel is showed. I Want Show a Panel by some Effect. Please Help me for this Winforms og Webforms? I would try WPF instead of Winforms You are leaving us guessing about what kind of effect you are looking for. I'll just arbitrarily pick a collapse and expand effect. It takes a Timer you implement the effect in a Tick event handler. Here's an example it requires a Panel Timer and Button:  public partial class Form1 : Form { public Form1() { InitializeComponent(); timer1.Interval = 16; timer1.Tick += new EventHandler(timer1_Tick); panel1.BackColor = Color.Aqua; mWidth = panel1.Width; } int mDir = 0; int mWidth; void timer1_Tick(object sender EventArgs e) { int width = panel1.Width + mDir; if (width >= mWidth) { width = mWidth; timer1.Enabled = false; } else if (width < Math.Abs(mDir)) { width = 0; timer1.Enabled = false; panel1.Visible = false; } panel1.Width = width; } private void button1_Click(object sender EventArgs e) { mDir = panel1.Visible ? -5 : 5; panel1.Visible = true; timer1.Enabled = true; } }  The only effect I can think of is to expand the panel by using a timer and change the size of the panel step-by-step. I would recommend you to use WPF instead of Winforms that is very good at doing this kind of stuff. You can animate all properties of the control like location size alpha. Please check these articles on WPF animation WPF Animation overview Walkthroughs: Create a Custom Animated Button
985,A,"Need a client-side interactive 2D world map: best map package? Or best C++ graphics/canvas library to make one? I need a 2d political map of the world on which I will draw icons text and lines that move around. Users will interact with the map placing and moving the icons and they will zoom in and out of the map. The Google Maps interface isn't very far from what I need but this is NOT web related; it's a Windows MFC application and I want to talk to a C++ API for a map that lives in the application not a web interface. Ideally I don't want a separate server either and any server MUST run locally (not on the Internet). What canned map package or graphics library should I use to do this? I have no graphics programming experience. This is strictly 2D so I don't think something like Google Earth or WorldWind would be appropriate. Good vector graphics support would be cool and easy drawing of bitmaps is important. All the canned options seem web oriented. SDL is about all I know of for flexible canvas programming but it seems like making my own map would be a lot of work for what is probably a common problem. Is there anything higher level? Maybe there's a way to interact with an adobe Flash object? I'm fairly clueless. You could extend your search by using the term GIS (Geographic Information System). I'm sure its gonna be easier. There's a lot of stuff out there on that subject. Here's a page I found: http://www.ucancode.net/Gis-Source-Code.htm or: http://opensourcegis.org/ Problem is that ""GIS"" seems to mean interacting with data on some sort server. All I want is a simple little static map of the world to draw on and animate.  Perhaps: http://www.codeplex.com/SharpMap ESRI MapObjects http://www.esri.com/software/mapobjects/index.html ESRI MapObjects LT http://www.esri.com/software/mapobjectslt/index.html See http://www.esri.com/software/mapobjectslt/about/mo_vs_lt.html for a comparison of the two MapObjects feature sets. ESRI may have a replacement to the MapObjects libraries  You might want to try the Mapnik C++/Python GIS Toolkit. You can take a look at the Marble Widget which is part of KDE's Marble project. There are Windows binaries for this too but they might be dependent on Qt.  Yes Marble has also the advantage that it provides kind of a ready made solution in a single control (called ""widget"" in Qt's technical terms). The dependency on Qt (which is the only dependency btw.) might also be seen as an advantage: Qt's upcoming version is licensed under the LGPL so even if you plan to use this in a proprietary application then there shouldn't be any real worries. And of course Qt and Marble are cross-plattform and provide an API that is very intuitive and easy to understand. Unlike common GIS solutions the Marble API and the usage of the widget is rather focused on people who don't know much about GIS. So its usage is quite easy to understand even if you feel scared by technical terms used in GIS. Marble offers several interfaces to programming: You can either create your own Marble plugins and paint inside those or you can subclass the MarbleWidget control. For a simple HelloWorld application see: http://techbase.kde.org/Projects/Marble/MarbleCPlusPlus"
986,A,Which 3D Model format should I be using? Im writing a game engine and I'm wondering what 3D model format should I use/load/export? Obj seems universal and easy but it also appears to be unreliable in that most models out there contain errors and it doesn't store anywhere near as much as other formats. There appear to be formats specifically for games such as MD2/3/5 but Im not sure I use wings3d if I model and I don't know what other details beyond purely loading what I need and support from the format Id have to implement such as would I need to implement IK? and can I use scripted per piece animation rather than Inverse kinematics and bone rigging? support Collada well and then supply good converters to/from the other formats (this might be the hard part). This will give you maximum flexibility. Take a look at C4 engine  I use my own binary format. I've tried to use existing formats but always run into limitations. Some could be worked around others where showstoppers. Collada may be worth a look. I don't think that it's that good as a format to be read by a 3D engine. It's fine as a general data-exchange format though. http://www.collada.org/mediawiki/index.php/Main_Page  Before worrying about what 3D formats you want to support I think you should really focus on what features you are planning to implement in your engine. Write those down as requirements and pick the format that supports the most features from the list... as you'll want to showcase your engine (I am assuming you are planning for your engine to be publicly available). You might even want to roll your own format if your engine has specific features (which is always a good thing to have for a game engine). After that support as many of the popular formats as you can (.X .3DS .OBJ .B3D)... the more accessible your engine is the more people will want to work with it! Collada is a nice and generic format but like Nils mentions it is not an ideal format for final deployment.  Collada is great but it lives more on the 3D app side of things. ie it's best used for transferring 3D data between applications not loading 3D data from within a games engine. Have you looked into Lua? It's widely used in games because its a scripting language that's both ridiculously quick (perfect for games) and very flexible (can be used to represent whatever data you need for your engine).  Collada is an open XML based format for 3d models owned by the Khronos group(OpenGL standards body) From the Collada.org FAQ: The COLLADA 1.4.x feature set includes: Mesh geometry Transform hierarchy (rotation translation shear scale matrix) Effects Shaders (Cg GLSL GLES) Materials Textures Lights Cameras Skinning Animation Physics (rigid bodies constraints rag dolls collision volumes) Instantiation Techniques Multirepresentations Assets User data I wouldn't use this format for real-time graphics. Collada is intended as an intermediate format for graphics production pipelines. Use it to convert to a more compact binary format or it you will be waiting all day for it to load. Also consider that not all tools support the entire COLLADA feature set. In the end I went with the ASSIMP library for geometry loading so anyone can use what they like so long as they support the necessary attributes Parsing Collada is as good as your parser works. Still I agree that it's more of a pipeline-format and contains way too much information which a game engine probably won't need. Since Collada is text it's compression ratio is great. Another thing of importance is the already mentioned fact that it's developed and maintained by Khronos. This leads to a really smooth transition between a Collad file and OpenGL because of the way information is stored (using VBOs combined with interleaved arrays leads to a big performance boost). btw ASSIMP supports only 1.4 and loading 1.5 leads to exceptions. @TomJNowell How did you find yourself with assimp?  +1 for Collada. You may also want a custom native binary format for really fast loading (usually just a binary dump of vertex/index buffer data plus material and skeleton data and collision data if appropriate). One trend in the games industry is to support loading a format like collada in the developer build of the engine but also have a toolchain that exports an optimized version for release. The developer version can update the mesh dynamically so as artists save changes the file is automatically reloaded allowing them an (almost) instant WYSIWYG view of their model but still providing a fully optimised release format. About native binary format it's worth noting there already is a standard Binary XML format: http://en.wikipedia.org/wiki/Binary_XML
987,A,"2D Triangle in SlimDX How to draw a triangle using SlimDX's Direct2D interface given triangle vertices are given in pixel coordinates? You want to draw primitive with ""Transformed"" coordinates. Transformed meams they are already in Screen XY so DirectX wont project them. Not really what I was looking form - I'm using Direct2D so there is no projection at all. Yes thats what Transformed coordinates are as if you already transformed them to 2D. Even if you are just using 2d coords to begin with it's the same thing  We just recently added a sample covering this topic to our repository. The relevant source file is here."
988,A,"Which version of OpenGL/Direct3D should I target for optimum compatability? When we develop web pages we can broadly work out which browsers to support based on market share. When we develop in .NET we can broadly work out which .NET version to develop for based on which Windows versions have it installed. But when developing OpenGL or Direct3D applications how do we know which video cards people (I mean ""people"" as opposed to ""hard core gamers"" or ""companies using CAD"" :P) are using? Are there statistics on such things? Is there some common logic that people use to work out what version to support? Just as most companies have supported (perhaps until just recently) a minimum of IE6 in web pages is there a general consensus to support a minimum of say OpenGL 1.5 or DirectX 8 or something? I note that we can find out which specific video cards support which versions of these APIs but how do we know which video cards people are actually using is there any kind of research on this? N.B. I'm more interested in OpenGL because that's what I'm using but I mention Direct3D because I assume the same problem applies. Market fragmentation for 3d hardware has always been a huge problem. There is no simple answer. You need to define who your uses are. Is this a casual-oriented game that you intend to sell to people who haven't updated their computer in years? Is this a business application that is aimed at workstation-grade hardware? Are you just looking for an average middle-of-the-road game-buyer? Is this something simpler than a game like a screensaver that you plan to sell to people who don't buy games at all? Do you need to support laptops? Netbooks? The market fragmentation is considerably worse with OpenGL than it is with DirectX. The official standard requirements from the Khronos Group are all well and good. But the hardware vendors tend to be very slow to update their drivers to match the standard. Many features are required by the GL spec but are only implemented in a fallback software path that is absolutely unusable in commercial software. The new OpenGL 3.1 spec tries to improve this situation by removing support for most older poorly supported features. But if you need to support hardware more than a couple years old (or most modern Intel integrated GPUs) then GL 3.1 would be aiming too high. A good place to start for general hardware usage stas among game purchasers is the Steam Hardware Survey ( http://store.steampowered.com/hwsurvey ). Steam is the most popular digital game distribution service that covers a variety of games from casual to core. They reset the numbers periodically to keep it current. Last year they reported that they had 25 million active users so the sample population is pretty good. So you probably need to narrow your target customer group down more. I would recommend picking some recent competing applications that you consider to have a similar customer base to yours and basing your target hardware around what they require. +1 for the Steam hardware survey link. Well done.  OpenGL 2.1 is a good bet. The newer OpenGL 3 doesn't offer that much more functionality. You have to check for the availability of all of the OpenGL extension anywayso you don't loose much by sticking with 2.1. For DirectX: Use DirectX 9c. That is the latest version that still runs on WindowsXP. Drivers are stable and very mature. DirectX 10 offers more functionality but you will lock out the user-base that still runs WindowsXP. About compatibility for non-gamers: Graphic cards that don't support these APIs (at least to a usable degree) have died out more than five years ago. Given the typical life-cycle of a PC you can be almost sure noone will have problems. If any user complains that the software doesn't run on his 10 year old matrox-parhelia card he should just buy the cheapest graphic card he can get. It will run much faster and will cost a fraction of the software. DirectX 7 level cards will still run under DX9. You just can't use features like shaders. This is NOT the case with DX10 if a card doesn't support the DX10 feature set you can't use DX10. Note that choosing the API version to use does not get you any closer to determining your allowable feature set. A major pitfall of 3d development is that many features that are required by the standard they claim to support will ""work"" but not at a quality and performance level that you can use. Different hardware will prefer a different subset of standard features.  It depends on the timeframe of the project on the DirectX side. If the project is to be ready in months then follow the advice from Nils. IF the time is over a year I would reconsider limiting to DirectX9 as the XP machines are getting lesser and lesser with the time. DirectX11 would be a good idea especially with retrocompatibility till feature level 9.0."
989,A,UIView & Graphics Context The UIView class does not set a graphics context so when I go to get the current context it comes back as nil. I'm in a method subclassed off of UIView how may I get the graphics context to be able to simply draw a line? Very new at the x-code game. Any help would be appreciated. What are you doing exactly? Can you show us some code? The graphics context is only set in the drawRect: method which you will need to override then do all your drawing in there. As a word of caution do not call drawRect: directly it will be called automatically when the UIView needs to be displayed. If you want to force a draw send a setNeedsDisplay message to your UIView. Thanks for the help  Override the - (void)drawRect:(CGRect)rect method in your UIView subclass http://developer.apple.com/library/ios/documentation/UIKit/Reference/UIView_Class/UIView/UIView.html#//apple_ref/doc/uid/TP40006816-CH3-BBCDGJHF According to the docs: You can get a reference to the graphics context using the UIGraphicsGetCurrentContext function but do not retain the graphics context because it can change between calls to the drawRect: method. It sets it up for you before invoking that method.
990,A,"Simple way to implement computer-go board in Java I want to make a simple Go board to design an Computer Go game. In a go game you lie a ""stone"" (white or black) on a position where horizontal and vertical lines intersect. What are some simple ways to restrict users from placing their stones in other locations? Maybe I'm just not seeing a simple solution. EDIT I guess I should rephrase my question better: I want to know how to do the background image of Go board so that I can lie my stones on the intersection of the horizontal and the vertical lines. I was thinking about getting a just regular Go board image and when I'm actually rendering stones I find right position of pixels to lie stones. However that solution did not seem to be the best solution since I need to worry about size of stone images and think about proportionality when I either expand or shrink the board window. I'm glad to hear it codingbear. skaffman edited your question and I assumed that he retagged it ""homework"". Since it's not homework I'm going to remove that tag: it tends to put people off. Please keep us updated with your progress. With that done in response to your edit in order to deal with image size and resizing the board window you might ask yourself these question: do you want to support any arbitrary size? Do you care about the aspect ratio? You see the answers to those will help you discover how to deal with the resizing. @skaffman: you really think it's a homework question? I'm disappointed. Maybe it's because the game is Go. Skaffman obviously deleted his comment but this is my personal project. I am trying to implement a Go game engine while applying and learning some probabilistic AI. I'd use enums here: enum Stone {BLAC WHITE NONE} class Board { static final int dimension = 19; private Stone[][] board = new Stone[dimension][dimension]; // ... } Edit: Here's a small demo (no resizing of the board and no images just good old Graphics!): import java.awt.*; import java.awt.event.*; import java.util.Random; import javax.swing.*; public class GoBoardDemo { public static void main(String[] args) { JFrame frame = new JFrame(); frame.setLayout(new BorderLayout()); frame.add(new GoPanel(19) BorderLayout.CENTER); frame.setSize(600 625); frame.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); frame.setResizable(false); frame.setVisible(true); } } @SuppressWarnings(""serial"") class GoPanel extends JPanel { Square[][] board; boolean whiteToMove; GoPanel(int dimension) { board = new Square[dimension][dimension]; whiteToMove = true; initBoard(dimension); } private void initBoard(int dimension) { super.setLayout(new GridLayout(dimension dimension)); for(int row = 0; row < dimension; row++) { for(int col = 0; col < dimension; col++) { board[row][col] = new Square(row col); super.add(board[row][col]); } } repaint(); } private class Square extends JPanel { Stone stone; final int row; final int col; Square(int r int c) { stone = Stone.NONE; row = r; col = c; super.addMouseListener(new MouseAdapter(){ @Override public void mouseClicked(MouseEvent me) { if(stone != Stone.NONE) return; stone = whiteToMove ? Stone.WHITE : Stone.BLACK; whiteToMove = !whiteToMove; repaint(); } }); } @Override protected void paintComponent(Graphics g) { super.paintComponent(g); int w = super.getWidth(); int h = super.getHeight(); g.setColor(new Color(0xB78600)); g.fillRect(0 0 w h); g.setColor(Color.BLACK); if(row == 0 || row == board.length-1 || col == 0 || col == board.length-1) { if(col == 0) { g.drawLine(w/2 h/2 w h/2); if(row == 0) g.drawLine(w/2 h/2 w/2 h); else if(row == 18) g.drawLine(w/2 h/2 w/2 0); else g.drawLine(w/2 0 w/2 h); } else if(col == 18) { g.drawLine(0 h/2 w/2 h/2); if(row == 0) g.drawLine(w/2 h/2 w/2 h); else if(row == 18) g.drawLine(w/2 h/2 w/2 0); else g.drawLine(w/2 0 w/2 h); } else if(row == 0) { g.drawLine(0 h/2 w h/2); g.drawLine(w/2 h/2 w/2 h); } else { g.drawLine(0 h/2 w h/2); g.drawLine(w/2 h/2 w/2 0); } } else { g.drawLine(0 h/2 w h/2); g.drawLine(w/2 0 w/2 h); } stone.paint(g w); } } } enum Stone { BLACK(Color.BLACK) WHITE(Color.WHITE) NONE(null); final Color color; private final static Random rand = new Random(); private Stone(Color c) { color = c; } public void paint(Graphics g int dimension) { if(this == NONE) return; Graphics2D g2d = (Graphics2D)g; g2d.setRenderingHint(RenderingHints.KEY_ANTIALIASING RenderingHints.VALUE_ANTIALIAS_ON); g2d.setColor(color); int x = 5; g2d.fillOval(rand.nextInt(x) rand.nextInt(x) dimension-x dimension-x); } } The Random stuff in there is an implementation the organic feel CPerkins was talking about. Well I tried to do it anyway. AWESOME. exactly what I wanted to know. I should start learning more about Graphics. This is quite interesting. You're welcome.  Don't think about the lines. Think about the intersections. The intersections can be represented as a grid in the same way that the squares on a chess board form a grid. In Java the simplest representation would be a square array. Something like: Location[MAX_Y][MAX_X]; where Location is an object representing the intersection and holding a reference to the piece placed there (if any).  If your primary goal is to implement the AI they I suggest implementing the Go Text Protocol and making use of one of the many existing GTP front ends. If you want to build the GUI as a learning experience then go ahead (no pun intended). As someone who wrote a commercial go GUI I warn you it is much more difficult than it seems (A chess app would be simple by comparison).  How to implement positions has been answered. But what you asked is how to limit users in their placement which is a different thing. Presumably you're planning to create a drawing area and you'll detect mouse clicks. How you'd limit stone placement to the intersections is like what drawing programs do to implement a feature called ""snap"" (as in ""snap to grid""). Basically that just means reducing the clicks which fall over a range of pixels in each dimension into being on one of the lines. Doing this in both dimensions puts the moved click at a point. Essentially what you're doing is to find the midpoint of each cell and any click which falls between one cell midpoint and the next will be counted as being on the line between those midpoints. So basically this would be something like this:  // Since traditionally boards are not square you'd call this twice: once with the // width in X for the X click and once again for Y. // Naturally you'll want to accomodate e.g. 9x9 boards for short games. int snapClick (int gridWidth int clickPos int numCells) { int cellWidth = (int) (gridWidth / numCells); int snappedClick = Math.round ((clickPos + (cellWidth/2)) / cellWidth); return snappedClick; } Incidentally in actual games stones aren't perfectly placed so games have a pleasing organic feel. You might want to consider a solution in which you store not only the position of the stones in the grid but also in some slightly-less-than-perfectly alignment on the screen for display. Ha never thought about the *organic feel* of a game but it makes perfect sense. Nice to know! Although he didn't answer all my questions this is really cool!  int[][] grid = new int[19][19]; There’s your grid. Each location in this array represents a line intersection."
991,A,"Making a Game Without Graphics? Is it possible or even constructive to make a game without any graphics (but is intended to become graphical) I'm not good with graphics at all so I'd like to write the skeleton for the game then have a graphics programmer/artist fill in the rest. I could write up all the major classes and their interactions and all the major functions/parts of the game. If so what should I do to make it easier to integrate graphics into the game later on (every drawn object should have a Draw Rotate Collide etc method) ? Being ""good"" at graphics comes down to knowing the APIs. If you can build complex systems graphics isn't hard; in many of the simple APIs it's as easy as `bitmap.Draw(coordinates)`. Get a book on game programming. Use placeholder graphics. Anyone can draw up boxes and stickmen in Gimp. Or just use Google should give you access to a vast repository of graphics you can freely use in your game or at least ""borrow"" until you get proper graphics in. I'm talking about all graphics programming as well. The entire physics system. It's not a matter of just art. @cam: If you take the graphics and physics out what's left? haha :( yeah... Well the network programming and structure of the game. The interaction between everything in the environment. It's more of a sim-game so while graphics and physics are important they don't make up the whole thing.  Certainly it's possible. The games that founded the ""adventure/RPG"" genre had no graphics and were a load of fun. Here are some great examples: The Colossal Cave Zork  Generally: You might want to have look at this: Game programming wiki This should get you started up. Specifically What kind of game do you envision? Update: Since you dont want to do any graphics programming at all you might want to give your drawable objects a draw() method stub and leave these for your fellow graphics developer. I would expect that you need Move rotate and Checkcollision methods for the basic functionality testing but as you wrote you would only build the foundations of the game system a simple setpositon might be sufficent. You should allow your objects to store theire position (and direction if applicable). Another approach could be to build a list of objects and there positions in game world later on. This would enable you to develope your interaction event handlers (like On GunFired OnHit Ondie...) and test these in some kind of testbench. The condensed form of Shoot 'Em Up is *shmup* not *FPS*. *FPS* means *first-person shooter* the *first-person* being key. @ dash-tom-bang: OP wrote: 'FPS type action' meaning hordes of monsters running at the player and the only way to survive is KILL EM ALL!!1! *Type* is the key here. An overhead 2D tile-based game with FPS type action. I wonder how exactly do you make 'overhead 2D' and 'first person' fit together... @axel_c: Nitpicking you are. I guess FPS is used as a general term for Shoot'em Up. Well just show the shoes of the character all the time and let him throw things down at mice in a labyrinth. You see the mice in 2D from the top (overhead) but you still are viewing from first person and shoot (throw) things....  Prime example what is possible without graphics is Dwarf Fortress. Incredibly deep and complex game represented merely by bunch of ascii characters. There are fan created visualizers (isometric 3d full 3d world). So yes it's possible to do a great game without bothering to program complex graphics.  Speaking from a non-game dev background the UI is almost always the last part of development. If you guarentee there will be graphics coming then defintely accomodate this to fit. You could even look at a plugin type approach where you can have a non-graphical version of the game and have a plugin which accomodates the graphical version. Leaving the UI as the last part might work for most applications but only for a small subset of games. Surely you can write all the logic ""blind"" and unit test it to hell but unfortunately there's no unit test for *fun*. And that's all that matters. Right I'm just wondering HOW to accommodate the fit best for the next guy who helps out. I don't want to throw a huge structure at him and say ""Figure it out"". I want to make it easier. How about you developing the base classes which has overridable methods like `Draw` etc then he can derive from these and implement the graphical stuff on his end. Or you could implement your classes as partial and he can easily extend them on his end to accomodate the drawing methods. @Matti: I am not a game programmer but I have yet to come across a system where I couldn't test it without the UI.... @James: What I wanted to say was that you can test all the logic but the point of games is to be *fun* and you can't really test for that without you know playing it. @Matti: The point of a game to a developer is to make it work :) @James: Judging by the OP's post it would seem that he's also the project leader and designer and therefore his responsibilities are a bit more broad than ""just making it work"". Of course it's a different story if you have an army of designers and play testers working for you. @Matti: To be honest that sounds more like Alpha/Best testing. For actually building the game (which the OP was talking about) you don't need to be testing the ""fun-factor"" that can only truly be tested once the game is near completion or complete.  Some games use the MVC(Model View Controller) pattern. If you did the same it should be possible to do the MC parts with multiple views(text based 2d 3d).  I used to play NetHack (ASCII Characters) Castle of the Winds (top-down graphical) and Diablo (2-D but viewed from an angle). The gameplay for all was similar. They were all tile based. They all had similar equipment spells fighting exploring random maps etc. However the graphics increased in quality. The game Adventure was a text based adventure game with text commands for ""go north"" ""look"" ""get rock"" etc. Then the King's Quest games (all graphical) started with walking around but still typing commands. In the later games you had a standard menu icon for ""look here"" ""walk here"" ""pick up object"" etc instead of typing. This doesn't really answer your questions but some examples of games with similar gameplay but added graphical features as time went forward. Hey don't forget Zork and all of the other Infocom games. ""The best graphics are the ones in your mind"" or something like to that effect."
992,A,output a jFrame to jpeg or bitmap I've been working on an assignment and have all the requirements completed. The project is to compare the differences in runtime between a linear search algorithm and a binary search. I have a graph class that puts out the results of those searches in a xy graph. The graph object is a Turtle class that extends JFrame. Is there any way I can convert that graph object to a bitmap and save it for future printing? The professor requires printouts of the results. Since I don't want a printout of every time the program is run I would prefer to save the graphics results in a designated folder rather than using screen-grab. Unfortunately I haven't come up with any answers on Google or here. Is something like this even possible? Have you looked into Component.createImage(intint)? That just might work thanks! Another approach is to tell your Component to paint() itself into a BufferedImage as seen in this example. The Container returned by a JFrame's getContentPane() method is suitable. that example is a pretty elegant solution to my problem. I'll try it out.  You can pass the bounds of the area into the Robot.createScreenCapture(Rectangle) method to create a BufferedImage of the area. The easiest way to save the screenshot as an image file is to use the ImageIO class. The only downside with the Robot is I have no way of knowing what the screen coordinates of the graph will be since it rarely opens up in the same space twice. But the ImageIO seems perfect for my needs You can calculate the screen coordinates of any component by using `SwingUtilities.convertPointToScreen(Point)`.
993,A,"How to prevent the ObjectDisposedException in C# when drawing and application exits I'm a CompSci student and fairly new at C# and I was doing a ""Josephus Problem"" program for a class and I created an Exit button that calls Application.Exit() to exit at anytime but if C# is still working on painting and the button is pressed it throws an ObjectDisposedExeception for the Graphics object. Is there any way to prevent this?. I was thinking of try{}catch or change a boolean to tell the painting process to stop before exiting but I want to know if there's another solution. You should be called the Close() method of the Form that contains the button in order to close down the form in an orderly manner. Closing the main form will cause the application to exit for you anyway.  It shouldn't be possible for this to happen. If the button is created on the same thread as the window they share a message pump and the Paint handler cannot be interrupted to handle the exit button. The message that the button has been clicked will be queued up on the thread's message queue until the Paint handler returns. Generally you should defer painting to the Paint handler (or override OnPaint) and everywhere else that you need to update the screen call the control's Invalidate method. That tells Windows that an area needs repainting and once all other messages have been dealt with it will generate a WM_PAINT message which ultimately will call OnPaint which in turn will fire the Paint event. If animating use a System.Windows.Forms.Timer to trigger each frame rather than using a thread. System.Threading.Timer callbacks execute on the threadpool so they're always on the wrong thread for manipulating the UI."
994,A,"Which JavaScript graphics library has the best performance? I'm doing some research for a JavaScript project where the performance of drawing simple primitives (i.e. lines) is by far the top priority. The answers to this question provide a great list of JS graphics libraries. While I realize that the choice of browser has a greater impact than the library I'd like to know whether there are any differences between them before choosing one. Has anyone done a performance comparison between any of these? How about http://www.jsgl.org? It uses SVG/VML. either make it a comment or elaborate it properly so it will explainatory.  None of them have good performance. It is 2009 and the state of programmable graphics rendering in web browsers is truly depressing. I could do faster interactivity on a vt125 terminal 25 years ago. If you are doing anything interactive think about using Flash... Else I'd go for some server-side heavy solution depending on your needs I know that none of them have good performance; I'm looking for which has the least terrible performance. And although I've used Flash for a ton of other projects I specifically want to do this in JS. yeah i understand. i do. I've spent loads of years doing graphics and I really couldn't imagine doing anything interesting graphics-wise using pure JS. Maybe you are just doing flow-diagrams and it won't be so bad :/ +1 for humour and honesty -1: He didn't ask if they had good performance overall. c'mon -- ""javascript graphics"" and ""performance"" really don't belong in the same sentence unless there's a ""horrendous"" thrown in there somewhere (either place works really)  If you're not doing 3d just use raw canvas with excanvas as an explorer fall-back. Your bottleneck will be javascript execution speed not line rendering speed. Except for IE which will bog down when the scene gets too complex because VML actually builds a dom. If you're really worried about performance though definitely go with flash and write the whole thing in actionscript. You'll get an order of magnitude better performance and with the flex sdk you don't even need to buy anything. There are several decent libraries for 3d in flash/flex available. Yeah this seems to be the consensus. Regarding Flash; that's what I usually use but in this case I specifically want to do this in JS for the sake of doing it in JS (inspired by Chrome Experiments). Hi FROM THE FUTURE. It is 2013 and there are lots of good programmable graphics rendering libraries. Flash is essentially dead. There needs to be an ""Obsolete"" tag for questions/answers that were correct half a decade ago but no longer are.  Up to now - is used processing.js (javascript canvas implementation of ""Processing"" language) and/or raphael.js (tiny and handy VML/SVG javascript library). The performance recomendations depends on task: highly dynamic complex primitives (or huge amount of it) - canvas (pixels low-level API) lower amount of primitives highly scalable integrated in DOM - SVG/VML (vector high-level API)  For basic drawing (such as lines circles and polygons) I would recommend Walter Zorn's Graphics Library. It was built with performance in mind and works in a ton of browsers. it draws pixels using .... divs . oye Check the performance benchmarks of this library and compare them with other popular ones. Then re-focus on the question Captain ignorance. one more time cause you are kinda dense .. it draws pixels using .... divs The url for this library doesn't seem to work any more. The new url is [http://www.walterzorn.de/en/jsgraphics/jsgraphics_e.htm](http://www.walterzorn.de/en/jsgraphics/jsgraphics_e.htm) How can animating DIV pixels be good for performance?  I know you said browser had more influence so why not stick with using optimized SVG calls? Then it ""could"" work in all browsers but... Chrome is robust enough to do render 3D modeling efficiently: http://www.chromeexperiments.com/detail/monster/ I was looking to trial WebGL on both Chrome and Firefox 5 today. It appears that because my PC is still ""XP"" ... the hardware driver isn't suitable.  Raphael JavaScript Library http://raphaeljs.com"
995,A,"How to show a shape or graphic in java with no window frame? I'm having a hard time trying to make a Java program with just a shape (e.g rectangle) as main window. I don't want the shape to be inside the 'OS-window' (with buttons to close and minimize etc..) http://stackoverflow.com/questions/722827/how-can-i-create-a-java-swing-app-that-covers-the-windows-title-bar @erickson: That one was about full-screen mode. I don't think this is a duplicate. It is a different application but the solution is the same. For more exotic shapes: translucent and shaped windows. Works only in 6u10 and later.  I don't know if you can draw directly on the screen in Java (I tend to think you can't). But you could create a JDialog (which doesn't appear in the taskbar) and call setUndecorated(true) on it (to get rid of the title bar). Then you can do whatever custom painting you want with it. Edit: kts points out that JWindow will work even better for this purpose. From the Javadocs: It does not have the title bar window-management buttons or other trimmings associated with a JFrame but it is still a ""first-class citizen"" of the user's desktop and can exist anywhere on it. And there's even a no-argument constructor so you don't have to worry about passing in a null owner! That's what I was looking for thanks :) you could just create a JWindow. It doesn't have that pesky JFrame/JDialog constructor. It doesn't show up in the task bar either. I don't hink it has decorations either. JDialog should work but I always woory about passing null in as an argument. Good tip on JWindow. I didn't even realize that class existed but it seems perfect for this job. Thanks for the JWindows :D It will work even better ;)"
996,A,Best way to programatically create image I'm looking for a way to create a graphics file (I don't really mind the file type as they are easily converted). The input would be the desired resolution and a list of pixels and colors (x y RGB color). Is there a convenient python library for that? What are the pros\cons\pitfalls? Udi Alternatively you can try ImageMagick. Last time I checked PIL didn't work on Python 3 which is potentially a con. (I don't know about ImageMagick's API.) I believe an updated version of PIL is expected in the year.  PIL is the canonical Python Imaging Library. Pros: Everybody wanting to do what you're doing uses PIL. 8-) Cons: None springs to mind.
997,A,Graphics card memory usage in linux What tools are available to monitor graphics card memory usage in linux? If you just need to know it for 3D graphics development purposes you may want to look into something like gDEBugger or if you only care about NVIDIA cards you can try NVIDIA PerfHUD. I have not used them myself but I would expect them to track such information. I fixed the broken link to NVIDIA PerfHUD. However Linux doesn't seem to be supported anymore.  NVIDIA PerfKit has a linux version which allows real-time monitoring of various graphics card properties including graphics card memory usage. Obviously this only works for NVIDIA graphics cards and it also requires the use of a special instrumented driver. I fixed the broken link to NVIDIA PerfKit. However Linux doesn't seem to be supported anymore. That's too bad do you know any way to make it work under Linux? Maybe using an older version of it will work?
998,A,how to capture mouse cursor in screen grab? I'm using OpenGL to grab the contents of the Mac OSX screen - thsi works very well except it does not grab the mouse cursor graphic. I need to somewhow get that cursor graphic either as part of my screen capture routine or separately. My question is either: How can I ensure that the mouse cursor image is included with the OpenGL screen grab? or How can I get the current mouse cursor image as a simple RGBA bitmap? I'm developing under Mac OSX 10.6 using C++ / Carbon. Cheers You can use I/O Kit to read the pixels for the current cursor. See IOFramebufferShared.h and IOGraphicsLib.h for some of the relevant API. Thanks for the pointer - that looks dreadfully complex though - I don't suppose you have some sample code that shows how to do it? The best I can find using google is something like: http://orange.kame.net/dev/cvsweb.cgi/xfree86/xc/programs/Xserver/hw/darwin/Attic/xfIOKit.c?rev=1.1&cvsroot=apps which isn't exactly easy to understand... This should get you started: https://bitbucket.org/boredzo/mouseflashlight/src/tip/NSCursor+PRHCurrentGlobalCursor.h https://bitbucket.org/boredzo/mouseflashlight/src/tip/NSCursor+PRHCurrentGlobalCursor.m The missing piece is that it doesn't actually copy the pixels; for that app just the size and hot spot were enough. Hi Peter Can u provide some sample code for capture mouse cursor with screen
999,A,"Given a set of points how do I approximate the major axis of its shape? Given a ""shape"" drawn by the user I would like to ""normalize"" it so they all have similar size and orientation. What we have is a set of points. I can approximate the size using bounding box or circle but the orientation is a bit more tricky. The right way to do it I think is to calculate the majoraxis of its bounding ellipse. To do that you need to calculate the eigenvector of the covariance matrix. Doing so likely will be way too complicated for my need since I am looking for some good-enough estimate. Picking min max and 20 random points could be some starter. Is there an easy way to approximate this? Edit: I found Power method to iteratively approximate eigenvector. Wikipedia article. So far I am liking David's answer. The ultimate solution to this problem is running PCA I wish I could find a nice little implementation for you to refer to... I was going to say PCA. This is the ultimate correct answer.  Here's a thought... What if you performed a linear regression on the points and used the slope of the resulting line? If not all of the points at least a sample of them. The r^2 value would also give you information about the general shape. The closer to 0 the more circular/uniform the shape is (circle/square). The closer to 1 the more stretched out the shape is (oval/rectangle).  You'd be calculating the eigenvectors of a 2x2 matrix which can be done with a few simple formulas so it's not that complicated. In pseudocode: // sums are over all points b = -(sum(x * x) - sum(y * y)) / (2 * sum(x * y)) evec1_x = b + sqrt(b ** 2 + 1) evec1_y = 1 evec2_x = b - sqrt(b ** 2 + 1) evec2_y = 1 You could even do this by summing over only some of the points to get an estimate if you expect that your chosen subset of points would be representative of the full set. Edit: I think x and y must be translated to zero-mean i.e. subtract mean from all x y first (eed3si9n). If this calculates the eigenvector of the covariance matrix it's great. Is there a link that points to this method? Check out http://number-none.com/product/My%20Friend%20the%20Covariance%20Body/index.html and the sample code at http://www.gdmag.com/code.htm (sep02.zip) You're right they do... I guess I was implicitly assuming that when I wrote out the formulas. I found a quick cheat for 2x2 matrix's eigenvector: http://tinyurl.com/cd998p. For C = |E(x*x) E(x*y)||E(x*y) E(y*y)| L1-d/c comes out to be (sum(x*x) - sum(y*y))/(2 * sum(x*y))+sqrt(((sum(x*x) - sum(y*y))/(2 * sum(x*y)))^2 + 1) which is exactly b + sqrt(b^2 + 1). As I added to the answer I think x and y need to be adjusted to zero-mean."
1000,A,Creating dynamic maps on the web My company uses a sales model of dealers territory managers and regional managers each with a different level of area scope (IE manage based on zips codes states or regions.) I want to create a slimmed down map that is similar to this US state map that would allow our users to manipulate who manages what. What are some good resources to start down this path? Your map link is broken. Thanks found another map. Who would have thought the US government would break a link so quickly! One useful tool that I've found is the following GPS Visualization.  KML and Tiger(US) or Maps and Geo(Can)? Or with something less interactive you could possibly still use the Topologically Integrated Geographic Encoding and Referencing system. Is there also a Canadian equivalent to Tiger? i didn't check too hard but that looks like a decent entry point. cheers - much appreciated  A combination of PostGIS for storing the data MapServer for generating map tiles and OpenLayers for the interface should give you something to start with. Include a some PHP libraries and your are just about there!  I'd say you could base yourself on OpenStreetMap which database is under creative commons license.
1001,A,"How do I detect whether a graphic will be dithered? Our application draws drop shadows beneath thumbnails. It does this by creating a bitmap getting a Graphics object using Graphics.FromImage and then overlaying images using Graphics.DrawImage. I'm using it today for the first time over remote desktop and it looks awful because the shadows are being dithered. I don't know whether this is happening during the overlaying or in the RDP client. Is there a way for me to determine whether the final image will be dithered either by looking at the image the graphics object or the screen settings so I can omit the shadow? you can use the System.Windows.Forms.SystemInformation.TerminalServerSession variable to detect if you are in RDP mode and degrade accordingly. I do not know a way to detect whether the RDP client does dithering or whether the deskto's colour depth is shifted to match it but you can detect the latter via the GetDeviceCaps function: using System.Runtime.InteropServices; public class DeviceCaps {     private const int PLANES = 14;     private const int BITSPIXEL = 12;     [DllImport(""gdi32"" CharSet = CharSet.Ansi SetLastError = true ExactSpelling = true)]     private static extern int GetDeviceCaps(int hdc int nIndex);     [DllImport(""user32"" CharSet = CharSet.Ansi SetLastError = true ExactSpelling = true)]     private static extern int GetDC(int hWnd);     [DllImport(""user32"" CharSet = CharSet.Ansi SetLastError = true ExactSpelling = true)]     private static extern int ReleaseDC(int hWnd int hdc);     public short ColorDepth()     {         int dc = 0;         try {             dc = GetDC(0);             var nPlanes = GetDeviceCaps(dc PLANES);             var bitsPerPixel = GetDeviceCaps(dc BITSPIXEL); return nPlanes * bitsPerPixel;                     }         finally { if (dc != 0) ReleaseDC(0 dc); }     } } Rendering based on the colour depth is preferable to speculatively degrading by assuming the user is doing it because of RDP but may be sufficient for you."
1002,A,32-bit PNG loading in .NET I haven't written any code yet but I've encountered a similar problem before. Hopefully things have changed since the last time I visited it. I'm trying to do my own image conversion for games into a special OpenGL image format. In order to perform the conversion correctly I need all pixel data and more importantly i need all the alpha channel data (all 8bits per pixel). Does the System.Drawing.dll in .NET support full 32-bit alpha transperency loading? Will I be able to walk the alpha channel with all information retained in C#? System.Drawing supports any image formate GDI+ does (PNG-32 included) and getting that information is possible. Look in to System.Drawing.Bitmap.GetPixel or LockBits  Yes it does. You should get an image with PixelFormat set to Format32bppArgb then when you call LockBits you can get to the data with the Scan0 property of the returned BitmapData object.
1003,A,"How can I successfully extend Graphics in Java I'm trying to create a generic graphics export tool which works by implementing the Graphics interface and intercepts and interprets the calls to its various methods. However although I can do this successfully for a single component my Graphics class is being replaced when I use it on a component which contains other components. Strangely this only happens on Windows and Linux. On OSX it works fine. Can anyone suggest how I can ensure that it's my original Graphics class which is passed to all subcomponents? I've got a short script which demonstrates the fundamental problem. When I explicitly call paint using an instance of MyGraphics I don't see MyGraphics in the JPanel - just sun.java2d.SunGraphics2D. import java.awt.*; import java.awt.image.*; import java.text.AttributedCharacterIterator; import javax.swing.*; public class GraphicsTest { public static void main(String[] args) { new GraphicsTest(); } public GraphicsTest () { JFrame frame = new JFrame(); frame.getContentPane().add(new MyPanel()); frame.setSize(500500); frame.setVisible(true); try { Thread.sleep(2000); } catch (InterruptedException e) {} System.out.println(""Using my graphics - expect to see 'MyGraphics' next""); frame.paint(new MyGraphics()); } class MyPanel extends JPanel { public void paint (Graphics g) { super.paint(g); System.out.println(""Graphics is ""+g); g.fillRect(10 10 20 20); } } class MyGraphics extends Graphics { public String toString () { return ""MyGraphics""; } public Graphics create() { return this; } // I've left out the huge list of abstract methods from the original script // since they're unchanged from the defaults and don't really matter here. } On my windows system the output is:  Graphics is sun.java2d.SunGraphics2D[font=javax.swing.plaf.FontUIResource[family=Dialogname=Dialogstyle=plainsize=12]color=sun.swing.PrintColorUIResource[r=51g=51b=51]] Using my graphics - expect to see 'MyGraphics' next Graphics is sun.java2d.SunGraphics2D[font=javax.swing.plaf.FontUIResource[family=Dialogname=Dialogstyle=plainsize=12]color=sun.swing.PrintColorUIResource[r=51g=51b=51]] whereas under OSX I get what I expected which is:  Graphics is sun.java2d.SunGraphics2D[font=apple.laf.CUIAquaFonts$DerivedUIResourceFont[family=LucidaGrandename=LucidaGrandestyle=plainsize=13]color=javax.swing.plaf.ColorUIResource[r=0g=0b=0]] Using my graphics - expect to see 'MyGraphics' next Graphics is MyGraphics So what I can I do to ensure that in all cases I get 'MyGraphics' passed to the appropriate subcomponents? That's it! If I disable double buffering on the component before calling paint and reenable it afterwards then everything works the way I need. RepaintManager.currentManager(frame).setDoubleBufferingEnabled(false); frame.paint(new MyGraphics()); RepaintManager.currentManager(frame).setDoubleBufferingEnabled(true); Thanks. Please add this information into your question! Or add a comment to Pete's answer.  I don't know for sure but I suspect that as swing is double-buffered by Java on Windows (rather than by the window manager on OS X) you won't draw to your graphics but to the Graphics2D provided by the off-screen buffer then blt to your graphics."
1004,A,"Calculating a boundary around several linked rectangles I am working on a project where I need to create a boundary around a group of rectangles. Let's use this picture as an example of what I want to accomplish. EDIT: Couldn't get the image tag to work properly so here is the full link: http://www.flickr.com/photos/21093416@N04/3029621742/ We have rectangles A and C who are linked by a special link rectangle B. You could think of this as two nodes in a graph (AC) and the edge between them (B). That means the rectangles have pointers to each other in the following manner: A->B A<-B->C C->B Each rectangle has four vertices stored in an array where index 0 is bottom left and index 3 is bottom right. I want to ""traverse"" this linked structure and calculate the vertices making up the boundary (red line) around it. I already have some small ideas around how to accomplish this but want to know if some of you more mathematically inclined have some neat tricks up your sleeves. The reason I post this here is just that someone might have solved a similar problem before and have some ideas I could use. I don't expect anyone to sit down and think this through long and hard. I'm going to work on a solution in parallell as I wait for answers. Any input is greatly appreciated. are the lines always going to be vertical or horizontal? can there be overlapping? Yes there can be overlapping this would have to be it's own case. At the time being the lines will always be horizontal and vertical. Calculate the sum of the boundaries of all 3 rectangles seperately calculate the overlapping rectangle of A and B and subtract it from the sum Do the same for the overlapping rectangle of B and C (to get the overlapping rectangle from A and B take the middle 2 X positions together with the middle 2 Y positions) Example (x1y1) - (x2y2): Rectangle A: (11) - (34) Rectangle B: (32) - (54) Rectangle C: (43) - (66) Calculation: 10 + 8 + 10 = 28 X coords ordered = 1335 middle two are 3 and 3 Y coords ordered = 1244 middle two are 2 and 4 so: (32) - (34) : boundery = 4 X coords ordered = 3456 middle two are 4 and 5 Y coords ordered = 2346 middle two are 3 and 4 so: (43) - (54) : boundery = 4 28 - 4 - 4 = 20 This is my example visualized:  1 2 3 4 5 6 1 +---+---+ | | 2 + A +---+---+ | | B | 3 + + +---+---+ | | | | | 4 +---+---+---+---+ + | | 5 + C + | | 6 +---+---+ I'm not sure what you are trying to achieve with your algorithm. How will it help me create the boundary polygon around my rectangles? Which in your example would consist of the following points: (11)(31)(32)(52)(53)(63)(66)(46)(44)(14) If I count it it is 20 just as I calculated. If you have a different example please let me know. I will explain how to calculate that one.  I haven't thought this out completely but I wonder if you couldn't do something like: Make a list of all the edges. Get all the edges where P1.X = P2.X In that list get the pairs where X are equal For each pair replace with one or two edges for the parts where they DON'T overlap Do something clever to get the edges in the right order Will your rectangles always be horizontally aligned if not you'd need to do the same thing but for Y too? And are they always guaranteed to be touching? If not the algorithm wouldn't be broken but the 'right order' wouldn't be definable. The rectangles will always touch and all lines would be horizontal or vertical.  After some thinking I might end up doing something like this: Pseudo code:  LinkRectsConnectedTo(Rectangle rectangleEdge startEdge) // Edge can be WestNorthEastSouth for each edge in rectangle starting with the edge facing last rectangle add vertices in the edge to the final boundary polygon if edge is connected to another rectangle if edge not equals startEdge recursively call LinkRectsConnectedTo(rectanglestartEdge) Obvisouly this pseudo code would have to be refined a bit and might not cover all cases but I think I might have solved my own problem. Hm after some further thinking some steps would have to be done to deal with inner loops and overlapping. This thing is spinning out of control.  The generalized solution to this problem is to implement boolean operations in terms of a scanline. You can find a brief discussion here to get you started. From the text: ""The basis of the boolean algorithms is scanlines. For the basic principles the book: Computational Geometry an Introduction by Franco P. Preparata and Michael Ian Shamos is very good."" I own this book though it's at the office now so I can't look up the page numbers you should read though chapter 8 on the geometry of rectangles is probably the best starting point.  A simple trick should be: Create a region from the first rectangle Add the other rectangles to the region Get the boundary of the region (somehow? :P) I'm voting this one back up because I think you're on the right track.  Using the example where rectangles are perpendicular to each other and can therefore be presented by four values (two x coordinates and two y coordinates):  1 2 3 4 5 6 1 +---+---+ | | 2 + A +---+---+ | | B | 3 + + +---+---+ | | | | | 4 +---+---+---+---+ + | | 5 + C + | | 6 +---+---+ 1) collect all the x coordinates (both left and right) into a list then sort it and remove duplicates 1 3 4 5 6 2) collect all the y coordinates (both top and bottom) into a list then sort it and remove duplicates 1 2 3 4 6 3) create a 2D array by number of gaps between the unique x coordinates * number of gaps between the unique y coordinates. It only needs to be one bit per cell so in c++ a vector<bool> with likely give you a very memory-efficient version of this 4 * 4 4) paint all the rectangles into this grid  1 3 4 5 6 1 +---+ | 1 | 0 0 0 2 +---+---+---+ | 1 | 1 | 1 | 0 3 +---+---+---+---+ | 1 | 1 | 1 | 1 | 4 +---+---+---+---+ 0 0 | 1 | 1 | 6 +---+---+ 5) for each cell in the grid for each edge if the cell beside it in that cardinal direction is not painted draw the boundary line for that edge In the question the rectangles are described as being four vectors where each represents a corner. If each rectangle can be at arbitrary and different rotation from others then the approach I've outlined above won't work. The problem of finding the path around a complex polygon is regularly solved by vector graphics rasterizers and a good approach to solving the problem is using a library such as Cairo to do the work for you! My co-worker agreed that this was the best approach for solving my problem."
1005,A,"Which format for small website images? GIF or PNG? When doing small icons header graphics and the like for websites is it better to use GIFs or PNGs? Obviously if transparency effects are required then PNGs are definitely the way to go and for larger more photographic images I'd use JPEGs - but for normal web ""furniture"" which would you recommend and why? It may just be the tools I'm using but GIF files usually seem to be a bit smaller than a comparible PNG but using them just seems so 1987. Useful: http://stackoverflow.com/q/2336522/199700 PNG is a 100% replacement for GIF files and is supported by all web browsers you are likely to encounter. There are very very few situations where GIF would be preferable. The most important one is animation--the GIF89a standard supports animation and virtually every browser supports it but the plain old PNG format does not--you would need to use MNG for that which has limited browser support. Virtually all browsers support single-bit transparency in PNG files (the type of transparency offered by the GIF format). There is a lack of support in IE6 for PNG's full 8-bit transparency but that can be rectified for most situations by a little CSS magic. If your PNG files are coming out larger than equivalent GIF files it is almost certainly because your source image has more than 256 colors. GIF files are indexed to a maximum palette of 256 colors while PNG files in most graphics programs are saved by default in a 24-bit lossless format. If file size is more important than accurate colors save the file as an 8-bit indexed PNG and it should be equivalent to GIF or better. It is possible to ""hack"" a GIF file to have more than 256 colors using a combination of animation frames with do-not-replace flags and multiple palettes but this approach has been virtually forgotten about since the advent of PNG. APNG is supported on... Firefox... see? That's *something*...  I use jpg for all non-transparent images. You can control the compression which I like. I found this web site that compares the two. jpg is smaller and looks better. JPEG compression only makes sense with photo-like pictures. Many pictures don't profit at all from JPEG compression which is lossy and can be made smaller by the (always lossless) PNG compression. Oops. Thanks for pointing this out.  I don't think it makes a lot of difference (customers don't care). Personally I would choose PNGs because they are a W3C standard. Be cautious with the PNG transparency effects: they don't work with IE6. Not true for IE6 limitation. There are workarounds and 8bit PNG need no tricks to work there.  The main reason to use PNG over GIF from a legal standpoint is covered here: http://www.cloanto.com/users/mcb/19950127giflzw.html The patents have apparently expired as of 2004 but the idea that you can use PNG as open-source over GIF is appealing to many people. (png open source reference: http://www.linuxtoday.com/news_story.php3?ltsn=1999-09-09-021-04-PS) even the FSF and GNU project use some GIF's on their websites. the GIF patent issue is dead.  As a general rule PNG is never worse and often better than GIF because of superior compression. There might be some edge cases where GIF is slightly better (because the PNG format may have a slightly larger overhead from metadata) but it's really not worth the worry. It may just be the tools I'm using but GIF files usually seem to be a bit smaller than a comparible PNG That may indeed be due to the encoding tool you use. /EDIT: Wow there seem to be a lot of misconceptions about PNG file size. To quote Matt: There's nothing wrong with GIFs for images with few colours and as you have noticed they tend to be smaller. This is a typical encoding mistake and not inherent in the format. You can control the colour depth and make the PNG file as small. Please refer to the relevant section in the Wikipedia article. Also lacking support in MSIE6 is blown out of proportion by Chrono: If you need transparency and can get by with GIFs then I'd recommend them because IE6 supports them. IE6 doesn't do well with transparent PNGs. That's wrong. MSIE6 does support PNG transparency. It doesn't support the alpha channel (without a few hacks) though but this is a different matter since GIFs don't have it at all. The only technical reason to use GIFs instead of PNGs is when use need animation and don't want to rely on other formats. Thanks for the detailed answer - I have often wondered this myself especially the ""lack of PNG support in IE."" With regards to file sizes and encoding tools I highly recommend PNGOUT (http://advsys.net/ken/utils.htm) to compress PNG files. Great answer - but your last sentence appears to be missing a word. Wish I could edit it for ya... The other technical reason is you want alpha channel.  gif files will tend to be a little smaller since they don't support a transparency alpha channel (and maybe for some other reasons). Personally I don't feel the size difference is really worth worrying about nearly as much as it used to. Most people are using the web with some sort of broadband now so I doubt they will notice a difference. It's probably more important to use the type of images that your manipulation tools work best with. Plus I like the ability to put an image on any background and have a drop shadow work which points me more towards the png format. Your first sentence is probably a reason that the average GIF file is smaller than the average PNG file but you need to compare like with like for it to be relevant to the question.  Indexed PNG (less than 256 colors) is actually always smaller than gif so I use that most of the time.  If they get smaller and you have nothing to gain from using the features PNG offers (which is alpha channel transparency and more than 256 colors) then I see no reason why you should use PNG. That isn't really an argument either way.  If you need transparency and can get by with GIFs then I'd recommend them because IE6 supports them. IE6 doesn't do well with transparent PNGs. But if you don't need IE6 support I'd go with PNGs. For solid color backgrounds I like to use PNGs as I can get better color reproduction.  Personally I use gif's quite a bit for my images as they work everywhere obviously your transparency limitation is one key element that would direct someone towards a specific format. I don't see any downfalls to using gif's. Downfalls to using Gifs? They can be bigger they have a limited palette and a very limited transparency scheme.  ""It may just be the tools I'm using but GIF files usually seem to be a bit smaller than a comparible PNG but using them just seems so 1987."" It probably is your tools. From the PNG FAQ: ""There are two main reasons behind this phenomenon: comparing apples and oranges (that is not comparing the same image types) and using bad tools."" continued... But you could always try saving as both (using the same colour depth) and see which comes out smaller. Of course if you want to standardise on one graphic format for your site PNG is likely to be the best one to use.  Be careful of color shifts when using PNG. This link gives an example and contains many more links with further explanation: http://www.hanselman.com/blog/GammaCorrectionAndColorCorrectionPNGIsStillTooHard.aspx GIF images are not subject to this problem. I can imagine that some browsers on non-Windows platforms with gamma adjustment in the hardware and/or OS might adjust CSS colours to sRGB per its spec but leave PNGs with no colour space info unadjusted. As such specifying sRGB in the PNG ought to do it but I've just found that Firefox then does its own thing for some reason I can't imagine.... The basic IE bug with PNGs is that it gamma-corrects to the wrong target gamma. The only reason that GIFs aren't subject to it is that GIF doesn't provide a way to specify the gamma. To get around it get your graphics app to save PNGs without gamma. See also http://www.libpng.org/pub/png/png-gammatest.html This link http://hsivonen.iki.fi/png-gamma/ explains that PNGs without gamma will still display incorrectly in some browsers. Perhaps those browsers are obsolete enough that you don't need to worry about them but everyone should at least be aware of the issue.  For images on the web each format has its pros and cons. For photograph-type images (ie lots and lots of colours no hard edges) use a JPEG. For icons and the like you have a choice between PNG and GIF. GIFs are limited to 256 colours. PNGs can be formatted like GIFs (ie 256 colours with 1-bit transparency that will work in IE6) but for small images they're slightly larger than GIFs. 24-bit PNGs support both a large gamut and alpha transparency (although it's troublesome in IE6). PNGS are your only really sensible choice for things like screenshots (ie both lots of colours and hard edges) and personally that's what I stick with most of the time unless I have something for which JPEG is more suitable (like a photo).  I usually use gif's because of the size but there is also png-8 which is 256 colours as well. If you need fancy semi-transparent stuff then use png-24. I usually use the 'save for web' feature in photoshop which lets you fiddle with filetype number of colours etc and see the result before you save. Of course I would use the smallest possible which still looks good in my eyes. According to my sources a PNG is usually smaller than a GIF of the same image at the same colour depth. See http://warp.povusers.org/grrr/PNGvsGIF/  The W3C mention 3 advantages of PNG over GIF. • Alpha channels (variable transparency) • Cross-platform gamma correction (control of image brightness) and color correction • Two-dimensional interlacing (a method of progressive display). Also have a look at these resources for guidance: PNG v's GIF (W3C Guidance) PNG FAQ  A major problem with GIFs are that it is a patent-encumbered format (EDIT: This is apparently no longer true). If you don't care about that feel free to use GIFs. PNGs have a lot more flexibility over GIFs particularly in the area of colorspace but that flexibility often means you'll want to ""optimize"" the PNGs before publishing them. A web search should uncover tools for your platform for this. Of course if you want animation GIF is the only way to go since MNG was basically a non-starter for some reason. Unisys' GIF patent (US 4558302) expired in June 2003  Wow I'm really suprised with all the wrong answers here. PNG-8 will always be smaller than GIF when properly optimized. Just run your PNG-8 files through PngCrush or any of the other PNG optimization routines. The key things to understand: PNG8 and GIF are lossless <= 256 colors PNG8 can always be smaller than GIF GIF should never be used unless you need animation and of course Use JPG for black&white or full color photographic images Use PNG for low color line art screenshot type images It's worth pointing out that a 16x16 or smaller image will always have 256 colors or less because it only has 256 pixels or less!  For computer generated graphics (i.e. drawn by yourself in Photoshop Gimp etc.) JPG is out of the question because it is lossy - i.e. you get random gray pixels. For static images PNG is better in every way: more colors scalable transparency (say 10% transparent .gif only supports 0% and 100%) but there is a problem that some versions of Internet Explorer don't do PNG transparency correctly so you get flat non-transparent background that looks ugly. If you don't care about those IE users go for PNG. BTW if you want animations go for GIF."
1006,A,Problems using Graphics with Panels in the Java Swing Library Hey everyone I am trying to run the following program but am getting a NullPointerException. I am new to the Java swing library so I could be doing something very dumb. Either way here are my two classes I am just playing around for now and all i want to do is draw a damn circle (ill want to draw a gallow with a hangman on it in the end). package hangman2; import java.awt.*; import javax.swing.*; public class Hangman2 extends JFrame{ private GridLayout alphabetLayout = new GridLayout(2255); private Gallow gallow = new Gallow(); public Hangman2() { setLayout(alphabetLayout); setSize(1000500); setVisible( true ); } public static void main( String args[] ) { Hangman2 application = new Hangman2(); application.setDefaultCloseOperation( JFrame.EXIT_ON_CLOSE ); } } package hangman2; import java.awt.*; import javax.swing.*; public class Gallow extends JPanel { private Graphics g; public Gallow(){ g.fillOval(10 20 40 25); } } The NullPointerException comes in at the g.fillOval line. Thanks in advance Tomek A couple of things: Don't forget to add the panel to the JFrame. And override the paint() method of JPanel for your custom painting. You do not need to declare a Graphics object since the JPanel's paint method will have a reference to one in any case. package hangman2; import java.awt.*; import javax.swing.*; public class Hangman2 extends JFrame{ private GridLayout alphabetLayout = new GridLayout(2255); private Gallow gallow = new Gallow(); public Hangman2() { setLayout(alphabetLayout); add(gallow BorderLayout.CENTER);//here setSize(1000500); setVisible( true ); } public static void main( String args[] ) { Hangman2 application = new Hangman2(); application.setDefaultCloseOperation( JFrame.EXIT_ON_CLOSE ); } } package hangman2; import java.awt.*; import javax.swing.*; public class Gallow extends JPanel { public Gallow(){ super(); } public void paint(Graphics g){ g.fillOval(10 20 40 25); } } You should override paintComponent not paint  You're getting NPE because g is not set therefore it's null. Furthermore you shouldn't be doing the drawing in the constructor. Overload paintComponent(Graphics g) instead. public class Gallow extends JPanel { public paintComponent(Graphics g){ g.fillOval(10 20 40 25); } } I'd also look into BufferedImage. I see so im guessing the paintComponent is called each time a JPanel is created. Actually it's called each time the component is redrawn. Swing doesn't remember the appearance of each component rather each component is responsible for drawing and redrawing itself as needed.
1007,A,"Howto perform a MULTIPLY composite effect using Graphics2D I have two different javax.swing.Icon objects and I want to a create new Icon which is the composite of these two. I want the composite to be a MULTIPLY effect similair to what you would get in any photoshop-like image editing application when you choose that option. Specifically per every pixel if you have color Ca and Cb from image 1 and image respectively Ca = (RaGaBa) Cb = (RbGbBb) the i want output to be Cc = (Ra*RbGa*GbBa*Bb) I want to do this on the fly (in realtime) so I've got to do this using only Graphics2D operations. I've looked at AlphaComposite and don't see that this can be done. Anyone have any ideas? Have you already looked at the contents of java.awt.image? Is there some reason why BufferedImageFilter and BufferedImageOp are not appropriate? What you need is a ""multiply"" alpha composite. Conveniently I asked the same question a while ago about a ""screen"" effect. I was pointed at http://www.curious-creature.org/2006/09/20/new-blendings-modes-for-java2d/ which has the multiply composite you need. oooh. Perfect! Many thanks...."
1008,A,intersecting line and array of points? i am having a line in normal graphi want to know intersecting some points with that line  any formula for it?any help please? The line is from startpoint(5050)endPoint(500)....the some point may be (010)(245)etc.. Could you be a bit clearer ? How is your line represented ? hi i have edited my question pls..... Well if you know the endpoints of the line you are interested in it's easy. Any point on the line has the formula a * p1 + (1-a) * p2 where p1 p2 are the endpoints and a is a scalar. If a is between 0 and 1 the point lies between the endpoints. will you explain little bit more pls? Try working through a few simple examples with pencil and graph paper you'll figure it out.  Make line equation Check if your point satisfies that equation. The equation of line passing 2 arbitrary points (x1y1) and (x2y2) is: (y1-y2)x + (x2-x1)y + (x1*y2 - x2*y1) = 0 In your case the line is just vertical and its equation is x = 50 If you also want to check if point belongs to the line segment rather than to the whole line you can check that the following inequality holds (in addition to the previous condition) (may be it is not the most elegant/efficient solution): (x-x1)*(x-x2)+(y-y1)*(y-y2) < 0 you should be checking also if the point falls between the two points in the given example if 0 <= y <= 50.
1009,A,Are there any .NET Graphics Calculate Libraries? I want to find a Calculate Library not a Drawing Library to help me do some graphics calulation like Bezier's length point on Beziers or other metadata. Is there any library like this? See this forum discussion and take a look at Math.NET. Thank you very muchIt's very helpful. also thanks arsalglib is a good libtoo.  ALGLIB may have what you need and is open source: http://www.alglib.net/ IMSL is a well regarded commercial library which implements many numerical algorithms: http://www.vni.com/products/imsl/cSharp/overview.php Thank youIt's very useful infomation.
1010,A,"JPanel Appears Behind JMenuBar import javax.swing.JFrame; import javax.swing.JMenu; import javax.swing.JMenuBar; import javax.swing.JMenuItem; @SuppressWarnings(""serial"") public class Main extends JFrame { final int FRAME_HEIGHT = 400; final int FRAME_WIDTH = 400; public static void main(String args[]) { new Main(); } public Main() { super(""Game""); GameCanvas canvas = new GameCanvas(); JMenuBar menuBar = new JMenuBar(); JMenu fileMenu = new JMenu(""File""); JMenuItem startMenuItem = new JMenuItem(""Pause""); menuBar.add(fileMenu); fileMenu.add(startMenuItem); getContentPane().add(canvas); super.setVisible(true); super.setSize(FRAME_WIDTH FRAME_WIDTH); super.setDefaultCloseOperation(JFrame.EXIT_ON_CLOSE); super.setJMenuBar(menuBar); } } import java.awt.Canvas; import java.awt.Graphics; import javax.swing.JPanel; @SuppressWarnings(""serial"") public class GameCanvas extends JPanel { public void paint(Graphics g) { g.drawString(""hI"" 0 0); } } This code causes the string to appear behind the JMenuBar. To see the string you must draw it at (010). I'm sure this must be something simple so do you guys have any ideas? By the way that is NOT how you do custom painting. Custom painting is done by overriding the paintComponent() method not the paint() method. And you should invoke super.paintComponent(...) to make sure the background is repainted. You have not added your canvas to the frame. In your constructor at the end add getContentPane().add(canvas); Sorry that was a minor hiccup in the code I'd posted. I had played around with the code before posting it you see and I removed certain parts in the process forgetting to add them back in when I made this question. trashgod's solution is correct though. +1 `this.add(canvas)` also works via forwarding.  Try FontMetrics fm = g.getFontMetrics(); g.drawString(""hI"" 10 fm.getMaxAscent()); as suggested by the diagram in FontMetrics. Addendum: @RaviG is right too. Gah! I never though about to coords being that of the baseline. >__> Thanks alot! I have to refer to that diagram frequently. :-)"
1011,A,"Is it a good idea to divide every graphic shape in two parts - its geometry and rendering components? I am writing a simple graphic editor for a university project using C#. I created a hierarchy where every type of shape - circles rectangles ellipses ... - are inherited from a base class Shape it contains a shape color as a protected field.  public abstract class Shape { protected Color color; public Shape(Color c) { color = c; } public Color FigureColor { get { return color; } protected set { color = value; } } public abstract void render(Bitmap canvasToDrawOn); public abstract void unrender(Bitmap canvasToDrawOn); public abstract void renderPrototype(Bitmap canvasToDrawOn); } The idea is that I want every type of shape to encapsulate it`s geometry part for example:  public class TwoDPoint: TwoDShape { Point2DGeometry point; public TwoDPoint(Color c Point2DGeometry p): base(c) { point = new Point2DGeometry(p); } public struct Point2DGeometry { Int32 x; Int32 y; public Point2DGeometry(Int32 X Int32 Y) { x = X; y = Y; } public Point2DGeometry(Point2DGeometry rhs) { x = rhs.Abscissa; y = rhs.Ordinate; } public Int32 Abscissa { get { return x; } private set { x = value; } } public Int32 Ordinate { get { return y; } private set { y = value; } } } Ellipse class will encapsulate EllipseGeometry etc. Is it a good decision in terms of OO design and if it is may be it is the better idea to create a parallel hierarchy of this geometry-only types? I think the answer is yes because it follows along in the track laid down by the venerated MVC pattern. Another advantage is that it gives you the option of swapping out graphics libraries without having to rewrite all those base geometric classes. Use an interface to make that even easier to do.  I would say that it depends. Are you going to reuse your geometries objects outside of your shape objects or share geometry objects as Flyweights? If not you may be complicating your class hierarchy unnecessarily. Looking over your code you may also want to consider the following: Having some sort of naming convention between your private fields and your public properties. There's no point getting fancy with Abscissa/Ordinate when your private variables are x/y and your constructor takes X/Y. Ditto Color and FigureColor in Shape. In Shape why make ""color"" protected when ""FigureColor"" also has a protected setter? In most APIs (and you can think of your abstract class as an API to it's derived classes) you'll want to minimize the confusion of which members to use when changing state. In this particular example I'd most likely make ""color"" private. Watch for ""internal classes"" in your class hierarchy which don't add any value. For instance you have an abstract Shape class and from the look of it TwoDShape as well. What's the difference between the two? Do you have support for 3 dimensions? Does your inheritance tree look more like a stick? HTH and good luck with your project."
1012,A,"VB.NET custom user control graphics rotation this is my first question on here. I'm trying to build a dial control as a custom user control in VB.NET. I'm using VS2008. so far I have managed to rotate image using graphics.rotatetransform . however this rotate everything. Now I have a Bitmap for the dial which should stay stable and another Bitmap for the needle which I need to rotate. so far i've tried this: Dim gL As Graphics = Graphics.FromImage(bmpLongNeedle) gL.TranslateTransform(bmpLongNeedle.Width / 2 bmpLongNeedle.Height * 0.74) gL.RotateTransform(angleLongNeedle) gL.TranslateTransform(-bmpLongNeedle.Width / 2 -bmpLongNeedle.Height * 0.74) gL.DrawImage(bmpLongNeedle 0 0) As I understand it the image of the needle should be rotated at angle ""angleLongNeedle"" although i'm placing the rotated image at 00. However the result is that the Needle doesn't get drawn on the control. any pointers as to where I might be going wrong or something else I should be doing? Thanks in advance oh the bitmap for needle has the pivot point at 0.74 * height. may be I should have posted this before. but this is what i've done.  Public Class Altimeter Protected Overrides Sub OnPaint(ByVal e As System.Windows.Forms.PaintEventArgs) Dim bmpBezel As New Bitmap(""{path}\Altimeter_Background.bmp"") Dim bmpLongNeedle As New Bitmap(""{path}\LongNeedle.bmp"") Dim rect2 As New Rectangle(e.ClipRectangle.X e.ClipRectangle.Y e.ClipRectangle.Width e.ClipRectangle.Height)  'make transparent bmpBezel.MakeTransparent(Color.Yellow) bmpLongNeedle.MakeTransparent(Color.Yellow) Dim angleLongNeedle As Single = (Altitude / 50) * 360 'draw bezel e.Graphics.DrawImage(bmpBezel rect2) 'rotate long needle Dim gL As Graphics = Graphics.FromImage(bmpLongNeedle) gL.TranslateTransform(bmpLongNeedle.Width / 2 bmpLongNeedle.Height * 0.74) gL.RotateTransform(angleLongNeedle) gL.TranslateTransform(-bmpLongNeedle.Width / 2 -bmpLongNeedle.Height * 0.74) gL.DrawImage(bmpLongNeedle 0 0) MyBase.OnPaint(e) End Sub i use e.graphics.drawimage to paint the whole image. i don't really understand what you said about having graphics object for all the images and then drawing the needle? do you have any pseudo code? thanks  First of all why do you allocate the Graphics object from a bitmap that you then proceed to draw onto the graphics? That doesn’t make sense. Dim gL As Graphics = Graphics.FromImage(bmpLongNeedle) ' … ' gL.DrawImage(bmpLongNeedle 0 0) What you probably want is a graphics context for the whole image. You then apply the transformations to it and finally draw the bmpLongNeedle image. Secondly your translations look inversed: in the first step you need to move the image to the origin (0 0); then you rotate it and then move it back. So the transformation should look like this: gL.TranslateTransform(-bmpLongNeedle.Width * 0.5 -bmpLongNeedle.Height * 0.5) gL.RotateTransform(angleLongNeedle) gL.TranslateTransform(bmpLongNeedle.Width * 0.5 bmpLongNeedle.Height * 0.5) Notice the inversed order of the TranslateTransforms. Also why did you translate by 0.74 times the height instead of half?"
1013,A,"Creating iPhone UI elements I've found a bunch of iPhone objects inside interface builder but I assumed there would be a standard pack of icons gradients etc to make things more applelike. How should I create these graphics simply using pngs or are there special drawing tools shapes I can use inside interface builder? No you won't find any shape tools in interface builder itself. You might search around in the developer connection for example: https://developer.apple.com/iphone/library/samplecode/UICatalog/ has some image files with apple-like buttons. These can be programmatically ""stretched"" to any size check out [UIImage stretchableImageWithLeftCapWidth: topCapHeight:] Some images such as those appearing in a tab controller (see along the bottom of the screen in the world clock app) are stylized for you. You provide the image e.g. just the alpha channel of the shape and it automatically gets the blue gradient when it's selected. Also .pngs are supposedly optimized by XCode for use on the iPhone so despite the plethora of formats it can support you might stick to .png By the way I'm new to iPhone development myself and everything above is regurgitated either from the Stanford iPhone Development course (free on iTunes) or from Beginning iPhone Development by Dave Mark & Jeff LaMarche. I recommend both for fellow noobs. In searching for what the png optimization is exactly I apparently stumbled on LaMarche's blog where he gives his theory: http://iphonedevelopment.blogspot.com/2008/10/iphone-optimized-pngs.html  You can use any type of images supported by iPhone SDK natively. Here you will find a list. But. for your convenience I will also list it here: Tagged Image File Format (TIFF) .tiff .tif Joint Photographic Experts Group (JPEG) .jpg .jpeg Graphic Interchange Format (GIF) .gif Portable Network Graphic (PNG) .png Windows Bitmap Format (DIB) .bmp .BMPf Windows Icon Format .ico Windows Cursor .cur XWindow bitmap .xbm Hope it helps. All of these will work - but you should use PNG. XCode performs PNG compression when you compile your app and Apple strongly suggests you use PNG over other formats. In the iPhone Graphics and Media Overview video publically available at the iPhone dev center they say: ""iPhone SDK supports a number of different image formats (PNG TIFF JPEG GIF BMP CUR XBM) but PNG is the recommended format. You should convert all the images in your application bundle to PNG for best performance."""
1014,A,"How to view output .mp files from Functional MetaPost I'm interested in using Functional MetaPost on Mac OS X: http://cryp.to/funcmp/ I'm looking for a tutorial like: http://haskell.org/haskellwiki/Haskell_in_5_steps but for a trivial FuncMP example i.e. using GHC I can compile something simple such as: import FMP myPicture = text ""blah"" main = generate ""foo"" 1 myPicture but I can't figure out how to view this foo.1.mp output. (It gives a runtime error about not finding 'virmp'; my MetaPost binary is 'mpost'; I can't figure out how to override this Parameter or what my .FunMP file is or should be doing...) I can run mpost on that but the output (foo.1.1) is what PostScript? EPS? How do I use this? (I imagine I just need a simple LaTeX file with an EPS figure in it or something...) Preferably I'd like to generate output (.ps or .pdf that I can view) so I an actually get somewhere with Functional MetaPost learning it playing with it not banging my head against paths and binaries and shell commands. the output of mpost is eps which you can view in ghostview...  @ja: This is true (EPS should be mpost's output) but there are a few problems here: ghostview uses X11 and is ugly (especially on a Mac) to the point of being difficult to use. I need smooth anti-aliased graphics specifically PDF so I can import the graphics into Photoshop when I'm done---the on screen results matter! In the end I'm not the only one having trouble with Functional Metapost's non-standard Metapost output. My solution is to try something else: Asymptote ... ""a powerful descriptive vector graphics language that provides a mathematical coordinate-based framework for technical drawings. Labels and equations are typeset with LaTeX for overall document consistency yielding the same high-quality level of typesetting that LaTeX provides for scientific text. By default it produces PostScript output but it can also generate any format that the ImageMagick package can produce."" It looks really impressive and improves on Metapost in many ways (true floating point full 3D!) and the programming language looks fairly modern and well thought out (first class functions Pythonic/Java-ish syntax). Wow! This is so cool. Asymptote delivers (once you get it installed... the problems are all on the FOSS packages/X11/texlive/macports and especially lazwutil side...)"
1015,A,Algorithm for unique find edges from polygon mesh I'm looking for a good algorithm that can give me the unique edges from a set of polygon data. In this case the polygons are defined by two arrays. One array is the number of points per polygon and the other array is a list of vertex indices. I have a version that is working but performance gets slow when reaching over 500000 polys. My version walks over each face and adds each edge's sorted vertices to an stl::set. My data set will be primarily triangle and quad polys and most edges will be shared. Is there a smarter algorithm for this? You are both correct. Using a good hashset has gotten the performance well beyond required levels. I ended up rolling my own little hash set. The total number of edges will be between N/2 and N. N being the number of unique vertices in the mesh. All shared edges will be N/2 and all unique edges will be N. From there I allocate a buffer of uint64's and pack my indices into these values. Using a small set of unique tables I can find the unique edges fast!  First you need to make sure your vertices are unique. That is if you want only one edge at a certain position. Then I use this data structure typedef std::pair<int int> Edge; Edge sampleEdge; std::map<Edge bool> uniqueEdges; Edge contains the vertex indices that make up the edge in sorted order. Hence if sampleEdge is an edge made up of vertices with index numbers 12 and 5 sampleEdge.first = 5 and sampleEdge.12 Then you can just do uniqueEdges[sampleEdge] = true; for all the edges. uniqueEdges will hold all the unique edges.  Just to clarify you want for a polygon list like this: A +-----+ B \ |\ \ 1 | \ \ | \ \ | 2 \ \| \ C +-----+ D Then instead of edges like this: A - B -+ B - C +- first polygon C - A -+ B - D -+ D - C +- second polygon C - B -+ then you want to remove the duplicate B - C vs. C - B edge and share it? What kind of performance problem are you seeing with your algorithm? I'd say a set that has a reasonable hash implementation should perform pretty ok. On the other hand if your hash is not optimal for the data you'll have lots of collisions which might affect performance badly.  Yes Use a double hash map. Every edge has two indexes AB. lets say that A > B. The first top level hash-map maps A to another hash-map which is in turn maps B to some value which represents the information you want about every edge. (or just a bool if you don't need to keep information for edges). Essentially this creates a two level tree composed of hash maps. To look up an edge in this structure you take the larger index look it up in the top level and end up with a hash map. then take the smaller index and look it up in this second hash map. if i understand correctly you end up with a unique first level hashmap but with a lot of 2º level hashmaps (one for each A value). I wonder if the 2º level hashmaps actually help are there enough B values on those second hashmaps?  heres a C implementation of edge hashing used in Blender exactly for the purpose of quickly creating edges from faces may give some hints for others to make their own. http://gitorious.org/blenderprojects/blender/blobs/master/blender/source/blender/blenlib/intern/edgehash.c http://gitorious.org/blenderprojects/blender/blobs/master/blender/source/blender/blenlib/BLI_edgehash.h This uses BLI_mempool https://gitorious.org/blenderprojects/blender/blobs/master/blender/source/blender/blenlib/intern/BLI_mempool.c https://gitorious.org/blenderprojects/blender/blobs/master/blender/source/blender/blenlib/BLI_mempool.h
1016,A,Display On-Screen Television Graphics During Live Broadcasts How and with what software do major television companies display on-screen graphics during their programs? For example: On ESPN how do they produce the news ticker on the bottom (both graphically and from a source code point of view) create and display the scores of sports and update and show selected stats on the fly? I've looked everywhere I can think of to find the answer but I am at a loss. That's where you guys (and girls) come in. I hope you can help shed some light on this subject for me. Thanks in advance. You should invest some time on your search habits too I guess :) http://www.videodesignsoftware.com/products/newsticker.php http://www.masternewmedia.org/video_internet_television/webcam-enhancer-utilities/webcam-video-mixer-overlay-camtwist-review-20070618.htm Goodluck
1017,A,deleting HBITMAP causes an access violation at runtime I have the following code to take a screenshot of a window and get the colour of a specific pixel in it: void ProcessScreenshot(HWND hwnd){ HDC WinDC; HDC CopyDC; HBITMAP hBitmap; RECT rt; GetClientRect (hwnd &rt); WinDC = GetDC (hwnd); CopyDC = CreateCompatibleDC (WinDC); //Create a bitmap compatible with the DC hBitmap = CreateCompatibleBitmap (WinDC rt.right - rt.left //width rt.bottom - rt.top);//height SelectObject (CopyDC hBitmap); BitBlt (CopyDC //destination 00 rt.right - rt.left //width rt.bottom - rt.top //height WinDC //source 0 0 SRCCOPY); COLORREF col = ::GetPixel(CopyDC145293); // Do some stuff with the pixel colour.... delete hBitmap; ReleaseDC(hwnd WinDC); ReleaseDC(hwnd CopyDC); } the line 'delete hBitmap;' causes a runtime error: an access violation. I guess I can't just delete it like that? Because bitmaps take up a lot of space if I don't get rid of it I will end up with a huge memory leak. My question is: Does releasing the DC the HBITMAP is from deal with this or does it stick around even after I have released the DC? If the later is the case how do I correctly get rid of the HBITMAP? delete should only be used to deallocate things allocated via new. HBITMAP is a bitmap handle and you need to release the associated object using the GDI function DeleteObject. Strictly you should save the result of SelectObject from when you selected the bitmap into the device context and pass that to another call to SelectObject to ensure that the bitmap is not in use by the device context when you call DeleteObject. Things often work if you don't do this especially if you're about to release the device context anyway but it is safest to do so.  You must destroy GDI objects with DeleteObject not delete. The latter is only used to free objects allocated using new.
1018,A,"Should I go 3D for my game? If so how far and any good tutorials? I have been working on a java simulation game for some time (remaking theme hospital in my own way). Up to now I have done everything in 2d using squares and circles etc. I have looked for some tutorials on how to do 3d in java found coke and code however I am unsure if this is what i need to use. My idea was either to have simple 3d as in flat 3d. or real 3d which would be a lot more work and more difficult. I really wanted to hear peoples general thoughts on going 3d. Here is a video of my progress from a few months ago (http://screenjel.ly/bdO7Rj8DVl8). The 2d graphics are rather unimpressive. I want to make some more progress however I don't know if I should continue developing in 2d if I want to eventually go 3d. Also I don't know how difficult 3d would be. Ideally I want 3d fully rotational. I know that is rather ambitious but I would settle for blocky people and items rather than circles and squares. Anyone know of any good 3d in java tutorials which includes rotation? Any other thoughts or comments on this? Thanks Thank you. It was indeed an awesome game! :) +1 for theme hospital :D If you're going to do 3D you're almost certainly best off using something like OpenGL (e.g. via JOGL). In this case you basically describe your objects in 3D space (e.g. positions and colors of walls positions and colors of lights etc.) You then specify a current ""camera"" location. Rotation is (normally) handled by simply specifying a different orientation for the camera. Learning the OpenGL API isn't trivial but chances are that you're concerned for the wrong things -- just for example full 3D rotation is fairly trivial. Creating realistic lighting (especially if you want to include shadows) is a great deal more work. Hum Ok yeh I can see your point. If I did I think I would defiantly go OpenGL. Lighting would be a secondary issue for me although it would be nice to have.  Theme Hospital - good memories :) If this is a learning project I'd definitely start off 2D. You'll get a lot more done and a working game much sooner. Adding 3D at this stage will just add complications. Plus Theme Hospital would just look weird in 3D! hehe oh yes good memories indeed! Yeh it's my first project of any decent size so I'm leaning and have learnt alot already! I just want to have the option to go 3d at a later date if i want to. Probably harder than I think it would be I would say do this all in 2D then when you want to go 3D go for something a lot simpler to start with. Maybe then you can revisit Java Hospital... Yeh I agree I think. I mean 3D sounds like a nice idea but to introduce it into my first project this complex... Ah well. Thanks for your input :)  Going 3D will raise content creation costs by 7x. (Sorry can't reference the study but NCsoft Korea looked into it and reported a 7 times increase). This is because creation of viable efficient and good looking assets require a multitude of skill sets. So keep that in mind. Many people think moving to 3D will magically make their game ""look better"". This is a fallacy. It will only look better if the assets are good enough to create a level of immersion better than what you had before. Staying 2D gives you a lot of freedoms. You can really create an art style focus on game play etc. Once you enter 3D you'll be spending most of your time optimizing the client rather than making a better game. So ultimately the decision is yours we can't make it for you. If you do decide to go 3D you need to then decide what your goal is. If it's just to create the game then you may want to look into an engine to reduce development. If you want to learn about the underlying mechanics of 3D graphics go straight to OpenGL using JOGL or LWJGL. Some engines to look into: jMonkeyEngine (I'm biased here). Ardor3D jPCT Good Luck. Thanks for weighing both sides up. I'm not very much of an artist so 2d or 3d I probably won't be doing any of the graphics. I know very little about actually using any engines for games. This is my first game so I hope to learn :) jME looks a bit too big for a project of this size. I plan to keep the 3d graphics to one z plane (i think) as in only one floor at a time so items would only have an x and y and z would always be 0. Looked at the jPCT interesting...  Use Isometric Projection! I don't think ""real 3D"" is needed to recreate the look of Theme Hospital. You only need an image for each orientation (if you want the user to choose from 4 viewing directions as in the original) and each object. Simply bend your vertical raster lines by 45 degrees and make your objects look 3D (and to draw themselves above neighboring cells) to create a 3D look without adding extra complexity like OpenGL to your code. I hope that I made my point clear. Otherwise please ask :) Okay so I didn't make myself clear - just like expected :) I'm not an expert either sorry. What I called ""bend"" above is called ""shear"" (at least in Gimp). The basic idea is to create a 3D look without actually doing 3D - just like you'd draw a 3D cube on paper. Maybe a (admittedly quite ugly) example can help: It's not 3D - no coordinates in a three-dimensional space needed - only bitmaps to create a consistent look. Sorry I know very little about graphics... What do you mean by ""Simply bend your vertical raster lines by 45 degree""? I'm not sure if I want to go for the look of Theme Hospital completely as there are already 2 projects that are using the original graphics. I want to have something graphically unique about the game. Thanks for the example. That seems to be pretty much what the original theme hospital looks like. I was trying to be a bit different from the other projects which use the original graphics. My original thought was to just have the people graphics go over one square so the same but without the slant. Thanks for your input though! :) Just added an example That's called isometric viewing. Cool finally an expert joined the conversation :) Thanks!"
1019,A,"Save image with ocaml graphics i want to save the picture generated by ocaml graphics in a file (png or jpeg). Thank you. which environment are you using give clear details I'm going to assume that you are talking about the Graphics module in ocaml. You should notice that the Graphics module isn't for creating and processing images. You can of course call Graphics.dump_image if you've already gone ahead and wrote something that produces the Graphics.image type. This will produce a color array array where color is a packed integer in the format 0xRRGGBB. After some other conversion functions you should call upon camlimages to produce your image and there is a code sample from another question. You said ""You should notice that the Graphics module isn't for creating and processing images."" Could you explain why ? Ok i've installed camlimage but i don't find any documentation about it. You'll have to be more specific; the total of documentation is the website linked and it's examples the `.mli` files in the installation and the `doc` folder in the tar ball. It is a platform independent module for manipulating a canvas on screen. The module is linked with native graphic primitives of the OS. That is the limit to the scope of the module. If it included image file format IO (via external libraries) that would be another dependency that they didn't want to include."
1020,A,"Graphics in C# (.NET) I use this code for drawing text in a panel: Graphics g = panel1.CreateGraphics(); g.DrawString(...); So I want to know what size the input text will be when rendered in the panel. Use g.MeasureString() to get the width of a string in the grapic context. // Set up string. string measureString = ""Measure String""; Font stringFont = new Font(""Arial"" 16); // Measure string. SizeF stringSize = new SizeF(); stringSize = e.Graphics.MeasureString(measureString stringFont); A couple of points I'd like to clean up: 1) = new SizeF(); is unnecessary here; 2) it's good practice to dispose fonts (and many other graphical objects) like stringFont.Dispose() or use ""using"" statement. Nice this is new to me.  You can also use TextRenderer.MeasureText which is sometimes easier to use than MeasureString. But is only accurate if you use TextRenderer.DrawText"
1021,A,"Is there a way to organize a icon collection to allow for easy searching? Is there any way of organizing a icon collection so that it easier to find needed icons? For example: the program needs a save icon there are 5 icons collections on your HD that have a save icon and there are 5 more collections that don't have a save icon (but you don't know that) do you browse through each icon collection? run a search (assumes files are named consistently)? Would it be ideal to have some sort of organized directory (printable?)? UPDATE My process so far: Use XXCopy to merge all icon folders into one folder. Syntax = xxcopy ""C:\Documents and Settings\jm\My Documents\Icons"" ""C:\Documents and Settings\jm\My Documents\All_Icons"" /s /sr Use Contact Sheets to add all files from the super folder and then export out contact sheets for all the images. This should sort them alphabetically. Print and put in a binder -> now to find the actual file all you need to do is browse the binder find the filename and then search via Windows for the actual folder location. This blog talks about a few different methods of organizing folders. http://blog.echoenduring.com/2010/04/18/icons-and-the-web-%e2%80%93-part-3-organizing-your-icons/ Note to self - 'Contact Sheets' creator program. http://download.cnet.com/Contact-Sheets/3000-12511_4-10556168.html Windows XP - use print wizard found in folder to print contact sheet of 35 pictures per sheet. The program ImageWalker seems promising as it shows thumbnails and allows exporting contact sheets but there seems to some issues as often the image thumbnails (when exporting or printing) go completely black. Image file management - http://mgreerphoto.blogspot.com/2006/12/image-file-management.html In Visual Studio there is a ZIP file with icons organized in folders. The organization is mostly by filetype and image size is pretty messy and contains many duplicates so you should probably not follow their example. However they provide a HTML file in each folder that shows the icons in that folder in a nice readable list (better than Explorer's standard preview). You could do something similar but maybe provide a table instead for example with one column per collection and one row per ""concept"" (e.g. save icon)."
1022,A,Is there any disadvantage to using only high resolution image resources for an Android application? The Android documentation says that best practices are to make two drawable directories - one for HDPI and one for MDPI. It also says that if the MDPI directory doesn't exist and a MDPI device is running the app it will scale down the HDPI ones so everything looks good. Is there any reason not to just make one high resolution set of graphics? One con that I can think of is performance issues and one pro I can think of is only having one set of images for your app. Don't forget ldpi! The only big downside of just using hdpi (for 90% of us who don't work in big teams with designed designer who can just worry about /res folder in project) is increased cpu & (more importantly) memory usage. I.e. - you have 320x480 device which loads and uses 1000x1000 images (they are scaled down on screen but they use the same memory as if they were displayed in their full size). Luckily I just found this little gem that works pretty well - it autogenerates other sizes from your images in xhdpi: https://code.google.com/p/android-drawable-converter/ Even better - it's configurable so you can add new size (xxhdpi) or just generate certain types (mdpi).  Basically the situation with this could be extrapolated for most of the best practices: You can do it otherwise but most often that not you shouldn't. It's not that you can't go only with drawable-hdpi but the addition of drawable-ldpi and drawable-mdpi offers you the ability to customize and fine tune your assets at the price of bulking up a little your application. Please keep in mind that this bulking up won't be so dramatic - if you assume that: the resource size is proportional to the pixel count the differences between assets for different resolutions are proportional to the difference between resolutions the folder drawable-mdpi will be just 37% of drawable-hdpi and drawable-ldpi will be just 18% of drawable-hdpi Also suffixes for res folders are especially useful when used together - you can have full control over the application. In some cases resources are pre-defined for a lot more than high-medium-low density screens so I would say that you shouldn't worry that much for the additional bulk. As you've thought out you can avoid the dynamic scaling of the resources (worse than scaling beforehand). It won't be that much of a problem but most of the time if you can avoid operations on the device by doing additional preparations in the development/production process that's a good thing. For the app icon I use the hdpi mdpi and ldpi folders but for most other images I just use the drawable folder containing the high resolution image and scale it appropriately the first time I load an image and then cache it so I can the scaled version again later without having to scale it again. Yes there's no reason not to prepare most of your assets for all sizes. Also note that some assets like the app icon *will* definitely look bad (blurry) when scaled from hdpi down to mdpi or ldpi screens. Thanks I didn't even consider the fact that the resource sizes are much smaller at lower densities. I want my app to look exactly the same on all devices (for now) so I think only using drawable-hdpi is ok for me as long as I don't see too much degradation in performance. @Christopher - I'll keep that in mind for the app icon. I guess I'll just have to test for in-app graphics.
1023,A,question about Graphviz due to the size of graph Grapgviz sometimes exceeds its memory so I want to ask except Grapgviz if there any software can be used to plot my large graph Thanks Thanks a lot. I really appreciate it I heard about Gephi it doesn't support the DOT file format yet though. Supported Graph Formats | Gephi
1024,A,Rotate Bitmap (Rectangle) While Maintining Area How can I rotate an Bitmap a given number of degrees while maintaining the area of the original bitmap. ie what I rotate a bitmap of Width:100Height:200 my end result will be a bigger image but the rotated portion will still have an area of 100*200 I'm confused by this question. Rotation doesn't change scale so your rectangle will always be 100 x 200 pixels (within the margin of error caused by bitmap rendering). Are you asking how to find the minimum bounding box that will hold a 100 x 200 pixel image rotated at an arbitrary angle? That's been answered elsewhere on Stack Overflow.  The graphics transform function is perfect for this. Create a new bitmap of the size you want create a graphics object based off that bitmap apply the transform then draw onto the canvas (graphics.drawimage(original_image)). Here is a much better example than I can give at this time. And Bobpowell.net is a site I usually reference back to for great explanations on transforms.
1025,A,"Java: Implementing a drawable class I'm trying to make a maze game in Java. The Explorer class represents the user and the DrawableExplorer is the code that graphically represents the user. DrawableExplorer implements the Drawable interface which contains:  import java.awt.Graphics; public abstract interface Drawable { public abstract void draw(Graphics paramGraphics); } this compiles successfully however I cannot figure out why my DrawableExplorer class isn't:  import java.awt.*; public class DrawableExplorer extends Explorer implements Drawable { public DrawableExpolorer(Square location Maze maze String name) { public void draw(Graphics g) { Square location = location(); get.setColor(Color.BLUE); g.fillOval(loc.x() + 10 loc.y() + 10 30 30); } } } It's asking for a return type but isn't my method void? The compiler error message says ""invalid method declaration; return type required"" You need to declare the class as: public class DrawableExplorer extends Explorer implements Drawable i.e. The extends clause has to come before the implements clause. The other error is that you've declared your draw method within the body of the constructor for DrawableExplorer. Given that you've defined a constructor that takes three arguments you would typically want to process these within the constructor body (you currently ignore them); e.g. by assigning them to instance variables. Can you post the compile error in your original question along with the updated code? Also I've edited my answer to highlight a second error. The compiler error message says ""invalid method declaration; return type required"" The constructor spelling is incorrect: ""DrawableExpolorer"" and so the compiler thinks this is a method. Wow can't believe I missed that one! Thanks!"
1026,A,"Simple Frameworks for Displaying Bitmaps and Handling Button Presses We have a set of applications that basically display a bunch of bitmaps and text then allow user to press ""buttons"" (certain bitmaps) that cause actions to occur. We currently have these implemented using DirectX and a bunch of code to place the bitmaps and handle the button-presses. But we'd like to have the following features: portable to Linux some sort of ""editor"" that would allow us to lay out screens without hard-coding locations of elements in code animation we need to be able to overlay video not resource intensive (these terminals don't have a lot of memory or CPU) we're currently using C++ so management would prefer that but other languages would be considered We'd prefer a free open-source solution but would be willing to buy something if it is not too expensive. (We have a couple dozen developers and tens of thousands of terminals deployed.) We don't like the common GUI toolkits or widgets. We want something that has more of the look of a game than of a dialog box. Any suggestions for off-the-shelf stuff we could use? ""We don't like the common GUI toolkits or widgets. We want something that has more of the look of a game than of a dialog box."" You realize that Trolltech's QT has a style sheet language for widgets? Take a look at their white paper specifically page 60 http://trolltech.com/pdf/qt43-whitepaper-us.pdf Going over your other requirements: portable to Linux Yes. Also supports Windows Mac and embedded environments. some sort of ""editor"" that would allow us to lay out screens without hard-coding locations of elements in code Qt's Designer is a very nice tool. I use it all the time. animation Qt supports this. we need to be able to overlay video Qt supports this. not resource intensive (these terminals don't have a lot of memory or CPU) This might be the fly in the ointment. You could check out Qt's embedded option. I've never used that myself. we're currently using C++ so management would prefer that but other languages would be considered Qt is for C++ and works with all major compilers. We'd prefer a free open-source solution but would be willing to buy something if it is not too expensive. (We have a couple dozen developers and tens of thousands of terminals deployed.) Qt has both open-source and closed source options.  You could try wxWidgets (it has wxBitmapButton) or try to implement your own solution using SDL for all of the graphics.  Maybe the way to go is something like Clutter or Allegro. If you check in this article at ArsTechnica what they are using Clutter for you might get an idea how to use it. I don't know for sure if it works on Windows but I'm pretty sure it does considering it only depends on libraries that are supported under Windows."
1027,A,How to generate images on the fly in ASP.NET As the title says how can generate (and display) images on the fly for simple charting resizing etc? Thanks Use System.Drawing namespace. Add a reference to System.Drawing assembly. Create a new Bitmap class. Call CreateGraphics on it. Use the returned Graphics object to do your drawing. Set the Response.ContentType to the appropriate image MIME type. Finally save the Bitmap to the Response.Output stream.  Check the caution about System.Drawing and ASP.NET http://msdn.microsoft.com/en-us/library/system.drawing.aspx If I were doing it now I'd use WPF http://www.codeproject.com/KB/WPF/WPF-Image-to-WebPage.aspx  For charting you may want to look into the ASP.NET Chart Control. http://weblogs.asp.net/scottgu/archive/2008/11/24/new-asp-net-charting-control-lt-asp-chart-runat-quot-server-quot-gt.aspx
1028,A,"Making pictures look inset I wonder if anyone has an idea how you have to transform an image such that it looks ""inset"" to the context? I need the effect for an application I am writing but I am not sure what the exact workflow is to achieve the effect mentioned above. Best regards heinrich Okay I've made it. It is much playing around with paths and those stuff ..."
1029,A,"3d models LOD pics etc I need to program a util for a 3d model. What i need to do is pass the 3d model to the util and have it extract multiple information. Things like poly count size/scale (if applicable) and anything else i can grab. Then i need to take a screen shot of the model 8 times (45deg from 0 to 360). Maybe 16. Is there an app that i can use to extract data from the model and is there another app i can use to create the image? if so i can write a php script to do this all for me :) if not i'll post more question about specifics. My OS is debian etch the models will be any format. I may make it 3ds only or limit it to what the app(s) support. Or i may have another app to convert one format to the other and extract the data from that format. you could at least be specific about what 3D model format the model is in? and also what O/S you need it to run on... I haven't tried this but you could try 3ds2pov to convert your 3DS files into POV-Ray format and then render with POV-Ray. The 3ds2pov program is quite old so I don't know how compatible it is with current 3DS files. The archive comes with source code so ought to build without too much difficulty on Debian. POV-Ray itself runs easily on Linux. nb: other 3DS to POV converters may exist. This one just happens to be the first one returned by Googling for ""3ds to pov"".  You can try this: Linux based A3dsViewer -> does 3ds to pov conversion. Thats cool. Any idea if it supports command line?  Your requirements are very loose and vague but most 3d engines should have the tools you need to create your custom utility. I would start by looking at Blender or the viewer tools from OGRE OpensceneGraph and similar tools. i am not sure if you notice but i need to use this through the command line. Most of the tools i looked at from ogre are not command line compatible. Can you name specific tools?"
1030,A,Closest point on a cubic Bezier curve? How can I find the point B(t) along a cubic Bezier curve that is closest to an arbitrary point P in the plane? Here is a good reference with an implementation in basic: http://www.tinaja.com/glib/bezdist.pdf Nothing about distance but this is a fun page to read if you are just interested in bezier curves: http://www.redpicture.com/bezier/bezier-01.html The link concerns the closest point on the curve to a point but the OP asks for closest point on the curve to a plane. I took 'in the plane' to imply a 2D curve in which case it is the same thing. If it is a 3D curve then I was not helpful at all. I'm indeed concerned with finding the closest point on a 2D curve. In other words the curve and the input point both lie on the same plane. After looking at the linked PDF I think I'm looking for something more descriptive -- more like an academic paper. As it is I'm not sure I understand what the algorithm being described really does.  After lots of searching I found a paper that discusses a method for finding the closest point on a Bezier curve to a given point: Improved Algebraic Algorithm On Point Projection For Bezier Curves by Xiao-Diao Chen Yin Zhou Zhenyu Shu Hua Su and Jean-Claude Paul. Furthermore I found Wikipedia and MathWorld's descriptions of Sturm sequences useful in understanding the first part of the algoritm as the paper itself isn't very clear in its own description. This looks like a great resource. Have you translated either of the 2 algorithms in there into code that can be shared? I haven't done so. If possible link needs an update.  If someone still interested in this issue you can find some useful info at http://jazzros.blogspot.com/2011/03/projecting-point-on-bezier-curve.html  Adrian: The Graphic Gems (I) algorithm was used in the example which is also provided in C. You can get the original algorithm from the GC book and roll your own as desired. My code works for a quad or cubic the code supplied in the GC book is for a cubic. hope that helps jim armstrong
1031,A,Why is Gosu hiding my mouse pointer? I'm doing some graphics programming using the Gosu gem. The thing is when I create a Window my mouse pointer is hidden. I can guess where the mouse is at a certain moment and I can intuitively click but my users may not. How can I show the pointer? I'm using something like this: class Game < Gosu::Window def initialize super 800 600 false @cursor = Gosu::Image.new(self 'media/cursor.png') end def draw @cursor.draw self.mouse_x self.mouse_y 0 end end Game.new.show Yeah it's the same approach I settled on.  If you want to use the system cursor you can do this class Window < Gosu::Window def initialize super 320 240 false end def needs_cursor? true end end Check out the documentation at libgosu RubyGosu rdoc Reference / Window Thanks but at the time I asked this question I think this wasn't available. You're the man. +1
1032,A,Best way to render hand-drawn figures I guess I'll illustrate with an example: In this game you are able to draw 2D shapes using the mouse and what you draw is rendered to the screen in real-time. I want to know what the best ways are to render this type of drawing using hardware acceleration (OpenGL). I had two ideas: Create a screen-size texture when drawing is started update this when drawing and blit this to the screen Create a series of line segments to represent the drawing and render these using either lines or thin polygons Are there any other ideas? Which of these methods is likely to be best/most efficient/easiest? Any suggestions are welcome. I love crayon physics (music gets me every time). Great game! But back to the point... He has created brush sprites that follow your mouse position. He's created a few brushes that account for a little variation. Once the mouse goes down I imagine he is adding these sprites to a data structure and sending that structure through his drawing and collision functions to loop through. Thus popping out the real-time effect. He is using Simple DirectMedia Layer library which I give two thumbs up.  I'm pretty sure the second idea is the way to go.  First option if the player draws pure freehand (rather than lines) and what they draw doesn't need to be animated. Second option if it is animated or is primarily lines. If you do choose this it seems like you'd need to draw thin polygons rather than regular lines to get any kind of interesting look (as in the crayon example).
1033,A,"smallest filesize for transparent single pixel image I'm looking for the smallest (in terms of filesize) transparent 1 pixel image. Currently I have a gif of 49 bytes which seems to be the most popular. But I remember many years ago having one which was less than 40 bytes. Could have been 32 bytes. Can anyone do better? Graphics format is no concern as long as modern web browsers can display it and respect the transparency. UPDATE: OK I've found a 42 byte transparent single pixel gif: http://bignosebird.com/docs/h3.shtml UPDATE2: Looks like anything less than 43 bytes might be unstable in some clients. Can't be having that. I think you have a little too much time on your hands. This will make no practical difference whatsoever... If you serve a thousand of these per page you're hitting 49 kilobytes. How many of these could you possibly be serving? Dested: You have much more than 49 kB because the HTTP headers are actually larger than the image :) The request will cause *way* more data then the image size the image size is non-relevant compared to the request size. @Dested: must also consider the minimal packet size .. (and the smallest disk sector) When talking about request/header sizes make sure you serve it using the relative url `/0.gif`. Putting it inside ""/Images/"" makes it more expensive @Dested if you have thousand of the same image on a page they'll be fetched as a single request and most likely be cached for successive pages The ""as modern web browsers can display it"" part comes a bit late in the request... Could have been a TGA or Tiff format after all. http://www.maproom.co.uk/0.gif Is 43 bytes shaves a little bit. Don't forget the few bytes you shave off the headers too because the filename is slightly shorter :-)  Transparent dot 43 bytes: echo ""\x47\x49\x46\x38\x39\x61\x1\x0\x1\x0\x80\x0\x0\xff\xff\xff\xff\xff""; echo ""\xff\x21\xf9\x04\x1\x0a\x0\x1\x0\x2c\x0\x0\x0\x0\x1\x0\x1\x0""; echo ""\x0\x2\x2\x4c\x1\x0\x3b""; Orange dot 35 bytes: echo ""\x47\x49\x46\x38\x37\x61\x1\x0\x1\x0\x80\x0\x0\xfc\x6a\x6c\x0""; echo ""\x0\x0\x2c\x0\x0\x0\x0\x1\x0\x1\x0\x0\x2\x2\x44\x1\x0\x3b""; Without color table (possibly painted as black) 26 bytes: echo ""\x47\x49\x46\x38\x39\x61\x1\x0\x1\x0\x0\xFF""; echo ""\x0\x2C\x0\x0\x0\x0\x1\x0\x1\x0\x0\x2\x0\x3B""; The first two I found some time ago (in the times of timthumb vulnerability) and I don't remember the sources. The latest one I found here. P.S.: Use for tracking purposes not as spacers. I'm assuming that these are all GIF's? Well then for future reference I'd submit this link: http://en.wikipedia.org/wiki/Graphics_Interchange_Format#Example_GIF_file Generally GIF format is well documented. That's great for those people that know the GIF signature. However lots of people are on here that are looking for things without a background in image file formats so it is good to specifically point it out. :-) Yes of course. Notice the GIF file signature (47 49 46 = GIF).  http://commons.wikimedia.org/wiki/File:Blank.gif 43 bytes. Not 49 :D Seems like Blank.gif is not transparent. You can use this (rember to remove space after ""/""): data:image/gif;base64R0lGODlhAQABAID/ AMDAwAAAACH5BAEAAAAALAAAAAABAAEAAAICRAEAOw== data strings won't work for caching proxies like polipo so if you're in this situation you can grab a 43-byte transparent gif here: http://probablyprogramming.com/2009/03/15/the-tiniest-gif-ever  To expand on Jacob's byte array answer i generated the c# byte array for a transparant 1x1 gif I made in photoshop. static readonly byte[] TrackingGif = { 0x47 0x49 0x46 0x38 0x39 0x61 0x01 0x00 0x01 0x00 0x81 0x00 0x00 0xff 0xff 0xff 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x21 0xff 0x0b 0x4e 0x45 0x54 0x53 0x43 0x41 0x50 0x45 0x32 0x2e 0x30 0x03 0x01 0x01 0x00 0x00 0x21 0xf9 0x04 0x01 0x00 0x00 0x00 0x00 0x2c 0x00 0x00 0x00 0x00 0x01 0x00 0x01 0x00 0x00 0x08 0x04 0x00 0x01 0x04 0x04 0x00 0x3b}; Its nice to get answers after more than a year :) chrome is showing this image as broken for me :/  http://polpo.org/blank.gif is a 37 byte transparent GIF made with gifsicle. In css-ready base64 format: data:image/gif;base64R0lGODlhAQABAHAAACH5BAUAAAAALAAAAAABAAEAAAICRAEAOw== Thanks for that... Thanks! Wish this was at the top! Every image editor I've tried won't open this perfect answer..  I remember once a long time ago I tried to create the smallest gif possible. If you follow the standard If I remember correctly the size is 32 bytes. But you can ""hack"" the specification and have a 26-28 byte that will show in most browsers. This GIF is not entirely ""correct"" but works sometime. Just use a GIF header specification and a HEX editor.  You shouldn't really use ""spacer gifs"". They were used in the 90s; now they are very outdated and they have no purpose whatsoever and they cause several accessibility and compatibility problems. Use CSS. +1 best answer so far :-) I suppose this is still used as tracking images (to track for example how much html emails are 'opened'). Very doubtful use though... @ChristopheD - and blocked (for many years now) by default in all sensible email clients so basically pointless even for that dubious use. It is useful for easy jail-break of the same-domain policy for such purposes as event tracking or logging. Much elegant than iframe-based methods. @Villiam: for that a no content CSS is much better Not sure why it was -1 I +1 this answer to restore the balance... Andreas is right; spacer images are so outdated. The comments above can be right but outside of the scope of the answer they talk about trackers not spacers. And a tracker doesn't need to be transparent anyway.  The smallest possible GIF completely depends on the implementation of the GIF standard. Most web browsers are lenient when it comes to decoding GIF files. When you see a ""smallest"" GIF on the net it may render completely fine as transparent or white/black in a browser but won’t open in software like Gimp Paint and Photoshop and is broken in thumbnail view. Likewise a hacked transparent GIF in Chrome could sometimes display incorrectly in IE or Firefox. The smallest near-valid GIF is 32 bytes (40 if transparent). “Near-valid” because the trailer and some of the LZW data can be discarded and it will still work in practically all software. The required bytes as follows: File signature/version 6 bytes Logical Screen Descriptor 7 bytes Global Color Table 6 bytes Optional: Graphics Control Extension 8 bytes¹ Image Descriptor 10 bytes LZW Data 3 bytes² ¹ Required for transparency ² Only 3 bytes of the LZW data are required and the bytes can be almost anything Trailer can be stripped Most GIF software require a Global/Local Color Table to be present. The smallest enitrely-valid GIF is 34 or 35 bytes (42-43 bytes if transparent) depending on how well the LZW data can be compressed. Further reductions (e.g. deleting Global Color Table) may work in some browsers but their effects are usually implementation-specific. For example the following 22 bytes is a terribly corrupt GIF working as a transparent GIF in Chrome and Firefox but broken in IE: 47 49 46 38 39 61 01 00 01 00 00 00 00 2C 00 00 00 00 01 00 01 00 00 or data:image/gif;base64R0lGODlhAQABAAAAACwAAAAAAQABAAA= The following 14 bytes render in Chrome but nothing else: 47 49 46 38 39 61 01 00 01 00 00 00 00 2C or data:image/gif;base64R0lGODlhAQABAAAAACw= +1 for knowing / explaining the craft. =)  See: http://www.google-analytics.com/__utm.gif 35B Alternative in Perl (45B): ## tinygif ## World's Smallest Gif ## 35 bytes 43 if transparent ## Credit: http://www.perlmonks.org/?node_id=7974 use strict; my($RED$GREEN$BLUE$GHOST$CGI); ## Adjust the colors here from 0-255 $RED = 255; $GREEN = 0; $BLUE = 0; ## Set $GHOST to 1 for a transparent gif 0 for normal $GHOST = 1; ## Set $CGI to 1 if writing to a web browser 0 if not $CGI = 0; $CGI && printf ""Content-Length: %d\nContent-Type: image/gif\n\n"" $GHOST?43:35; printf ""GIF89a\1\0\1\0%c\0\0%c%c%c\0\0\0%s\0\0\0\0\1\0\1\0\0%c%c%c\1\ +0;"" 144$RED$GREEN$BLUE$GHOST?pack(""c8""332494516000):""""224 +0; Run it ... $ perl tinygif > tiny.gif $ ll tiny.gif -rw-r--r-- 1 stackoverflow staff 45B Apr 3 10:21 tiny.gif Copy and paste didn't work for me (identify: Corrupt image). Probably your formatting...? Also the code comment says 35/43 bytes but your output says 45 bytes. Google's [_utm.gif](http://www.google-analytics.com/__utm.gif) doesn't appear to be transparent.  So taking the wikimedia one (http://commons.wikimedia.org/wiki/File%3aBlank.gif) that's 43 B and running it through ImageOptim busts it down to 35 B. Is that going to mess it up for use as a spacer?  Here is what I use in a C# byte array (avoids file access) static readonly byte[] TrackingGif = { 0x47 0x49 0x46 0x38 0x39 0x61 0x1 0x0 0x1 0x0 0x80 0x0 0x0 0xff 0xff 0xff 0x0 0x0 0x0 0x2c 0x0 0x0 0x0 0x0 0x1 0x0 0x1 0x0 0x0 0x2 0x2 0x44 0x1 0x0 0x3b }; In asp.net MVC this can be returned like this return File(TrackingGif ""image/gif""); Yes thats the idea. How many bytes do you have? Sorry I'm to lazy to count right now :) 35 bytes so I'm guessing this is not transparent @Nux no it's not transparent. It is white (FFFFFF). Perfect for my uses though."
1034,A,Drawing graphics on top of a JButton I have a situation wherein I have a bunch of JButtons on a GridLayout. I need each of the JButtons to have: a background image (but retain the ability to keep the default button look if needed) custom graphics drawn on top by other classes I have no trouble with the background image since I am using setIcon() but I am having problems drawing things on top of the background. At one point I was able to draw on top of the button but after the button was clicked the drawings disappeared. How can make the button keep this drawing state? Basically I need a way for my JButtons to have public methods that would allow another class to draw anything on it such as: public void drawSomething() { Graphics g = this.getGraphics(); g.drawOval(3222); repaint(); } or public Graphics getGraphics() { return this.getGraphics(); } then another class could do this: button.getGraphics().drawSomething(); The latter is more what I am looking for but the first is equally useful. Is there any way to go about this? Also overriding the parent class method paintComponent() doesn't help since I need each button to have different graphics. You can create a BufferedImage and do custom painting on it and then draw the image in your custom paintComponent(...) method. Look at the DrawOnImage example from Custom Painting Approaches. Thanks for the answer I considered this but have decided to go with the interface answer.  you can subclass JButton and override paintComponent(). you can handle each button having a different graphic by providing an external 'painter' to the subclass. Or just have a different subclass for each different graphic. public class MyButton extends JButton { private Painter painter; public void paintComponent(Graphics g) { super.paintComponent(g); painter.paint(g); } } public interface Painter { public void paint(Graphics g); } you cannot just paint on the button as you are doing as the painting will get lost when the button is next repainted. glad it helped you I had actually been using this approach earlier except I had not used an interface. I scrapped it because I thought there might be an easier way. This is a more elegant approach Thank You.
1035,A,Making a drawing program I am making a drawing program and have a few question with regards to that. I'll need the user to have the choice of drawing a rectangle an oval and a line. I suppose I need to make a super class they all derive from. Should I make this an interface or an abstract class? And how would I set up that all the shapes have some default values when they are created like the colour etc. The user will use the mouse to click on the screen and the program should make the shape with center where the user click and use previously inputted height and length to draw it. What is the easiest way to store points gathered in the frame and do I even need to store this information? I also would like the user to have the option to scale the shape he has just drawn by dragging the sides. How can I most easily set this up? Edit: Cheers. I will try with a somewhat light-weigth MVC. I have a class DrawingModel and DrawingPanel which I instantiate in View and then pass the DrawingModel object to the DrawingPanel object through a setModel method. One question however arises which I can't get my head around; what part of the program will have to take care of the listeners? I suppose have to be implemented by View but where do I place the ActionListeners? Sounds kinda like homework you might want to retag if it is. Obviously the listeners have to be registered with a part of the view (e.g. JPanel.addMouseListener) but the listener methods themselves would be part of the controller. You may want to google DerekBanas he has a couple videos explaining how to do just that. He also has uploaded some great UML design pattern and other object oriented design related videos. I've been messing around with some code for the past couple days that goes a little beyond his tutorial videos using other shapes and drawing better lines with the paint tool etc. If you want to check out the code just send me a message and I'll send it to you but its too much to post in a comment. Good luck.  If you want all the shapes to have some common required properties (and perhaps methods) abstract class is probably the best choice. You can set these in the abstract class definition: abstract class AbstractShape { private static final Color DEFAULT_COLOR = Color.BLUE; private static final float DEFAULT_LINE_WIDTH = .5f; protected Color color = DEFAULT_COLOR; protected float lineWidth = DEFAULT_LINE_WIDTH; } class Circle extends AbstractShape { } EDIT: Just to clarify the abstract class can have a constructor such as: protected AbstractShape() { color = DEFAULT_COLOR; lineWidth = DEFAULT_LINE_WIDTH } The assignments (color = DEFAULT_COLOR) can easily be moved to a constructor in the abstract class if desired. However with the original code above the subclasses can already freely override the instance data (color lineWidth). A better alternative to providing constants your subclasses must reference explicitly is to define a default constructor which handles this which subclasses can override as necessary.  To respond to your edit: It is the view that will have the ActionListener that are attached to components. Then you could have the view notify the controller (through a listener) if the model needs to be updated. When the model is updated it will usually notify the controller through a property change listener so that the controller can update the view.  Have a look at the MVC design pattern. In your case you should be creating a model object which is a collection of shapes. Your drawing area (e.g. canvas or panel or whatever) should implement MouseListener and MouseMotionListener in order to capture mouse events. You can find out where the user has clicked by using MouseEvent's getX() and getY() methods. You can then store this shape into the model and repaint() the area. For scaling you will have to make use of the mouseDragged() and mouseReleased() methods which will give you the xy coordinates. You need to identify the shape a user has clicked on and then change its width and height based on how far the user has dragged (this could be determined by subtracting the original xy from the new xy coordinates). I agree this is good to know about but /real/ MVC is probably overkill for what sounds like a small school project.
1036,A,"Design crowd sourcing I'm looking to get a logo designed but all my designer friends/colleagues are pretty busy. Since I've never done it before I thought it would be interesting to crowd source the design. Does anyone know of any good design crowd sourcing sites? Crowd sourcing is for evil. Finding a designer is for good. Be for good. I'll second the 99designs.com that is what Jeff used to get the logo for Stack Overflow Here are all the entries for the stackoverflow logo design contest : http://goo.gl/9W6sq  I believe Guerra Creativa is a nice crowd sourcing initiative. They have a community with mostly Latin American designers and they seem to come up with more creative designs for logo's and websites. I believe that this is the problem with most of these larger crowd sourcing sites. They work too much like factories with standard logos and clipart etc. Creativity is something that I always like in a design and not just another ""off the shelve"" design.  Try 99designs.com  My company used Crowdspring for our logo.  designcrowd.com or 99design.com are my picks"
1037,A,"Flash/ActionScript: Draw a display object ""onto"" another How do I properly draw one vector object onto a specific position of another in ActionScript accounting for positioning transparency etc? My idea (as suggested e.g. here) was to use beginBitmapFill() and drawRect() to draw a bitmap copy (made using BitmapData.draw()) of one MovieClip onto the .graphics property of the other MovieClip but this proved more difficult than I thought mainly for these reasons: Transparency is not preserved: where the source MovieClip is transparent the target Graphics becomes white. I've tried using a BlendMode but it doesn't help. It seems white pixels are actually copied into the BitmapData. I must be missing something but I don't know what. It seems the problem was that the documentation for the BitmapData constructor is wrong: transparency does not default to true but to false. So if you create a new BitmapData(x y true) transparency is preserved otherwise not. Positioning. If the source MovieClip is centered (with the center at (0 0) you have to apply a Matrix to move it when copying it to a BitmapData or the bitmap will only contain the bottom right of the source MovieClip. Figuring out how much to move it is tricky and I assume it involved getting the boundaries somehow. (If the source object is perfectly centered within its canvas you can simply offset it by half its width and height of course.) beginBitmapFill tiling. It seems if you don't start the drawRect() at (0 0) the bitmap fill doesn't start from (0 0) within the bitmap either. This means you have to shift the source bitmap once again based on where you want to start drawing. (Then there's other stuff like having to lineStyle(000) so the drawRect() doesn't draw a border etc.) All this tricky trouble leads me to believe I'm going about this the wrong way. Is there an easier way to do this? Is there a library somewhere that can help me? Has anyone successfully done this? Perhaps my drawing canvas shouldn't be a Sprite but a Bitmap? But then I don't know how to draw to it using lineTo() etc. Here's the situation: I have a Sprite to which I draw using its .graphics using lineTo() etc. Now I want to use a MovieClip as a brush when drawing (or just place a copy of another MovieClip on the drawing surface) making the other MovieClip part of the graphics of the first one. I can't addChild() because then the vector graphics wouldn't interact with the drawn graphics. EDIT: Theo has provided a working solution that works perfectly except for the fact that it does not apply the rotation and scaling applied to the DisplayObject being drawn onto the Graphics surface. This is Theo's solution: private function drawOntoGraphics(source : IBitmapDrawable target : Graphics position : Point = null) : void { position = position == null ? new Point() : position; var bounds : Rectangle = DisplayObject(source).getBounds(DisplayObject(source)); var bitmapData : BitmapData = new BitmapData(bounds.width bounds.height true 0x00000000); bitmapData.draw(source new Matrix(1 0 0 1 -bounds.x -bounds.y) null null null true); target.beginBitmapFill(bitmapData new Matrix(1 0 0 1 bounds.x + position.x bounds.y + positi bzlm - you could always make the starting X position a constant and combine drawRect with a Matrix translate. var xPos:Number = 750; var matrix:Matrix = new Matrix(); matrix.translate(xPos0); mySprite.graphics.beginBitmapFill(myBitmapmatrix); mySprite.graphics.drawRect(xPosyPosimgWidthimgHeight);   The way i see it you have several ways to approach this: Use the graphics property of a ""canvas"" Shape Pros: Nice and simple Cons: Very limited especially if you need to remove things since you would have to redraw the whole canvas. Use a Bitmap and the .draw() method Pros: Also nice and simple and you can do duplication of things more easily. Cons: Basically has the same problems as the prior approach deleting anything would need a complete redraw. Also it won't be scalable since it's a bitmap. Use a Sprite and add primitives as children Pros: Very flexible. Cons: This allows you to move scale and delete objects individually. However it will be slightly more complicated than the previous two options.  Using the last approach (which I would recommend if you're going to do anything more than something very small with this) your problem can be solved by using Bitmaps that you add to your ""drawing"" these support transparency (like you've found out) and are a bit more straightforward than using bitmapFill (which is a bit tricky to get right). I have an example of using a bitmap to clone and draw other DisplayObjects on my blog. It's not exactly what you want but the code might be of some use atleast the matrix part for the draw method.  I subscribe to your blog and like it very much so I'd actually already read that article. I'm opting for the first solution because the scenario here is really ""bitmap brush"" - I have lineTo()s that I want to intermix with the bitmap and no deleting is really necessary. Thanks for the other tips! I've edited the question to include support rotation and scaling of the DisplayObject being drawn.  Using the display objects getBounds function can be a reliable solution to translate the coordinates while drawing : private function drawOntoGraphics(source : IBitmapDrawable target : Graphics position : Point = null) : void { position = position == null ? new Point() : position; var bounds : Rectangle = DisplayObject(source).getBounds(DisplayObject(source)); var bitmapData : BitmapData = new BitmapData(bounds.width bounds.height true 0x00000000); bitmapData.draw(source new Matrix(1 0 0 1 -bounds.x -bounds.y) null null null true); target.beginBitmapFill(bitmapData new Matrix(1 0 0 1 bounds.x + position.x bounds.y + position.y)); target.drawRect(bounds.x + position.x bounds.y + position.y bounds.width bounds.height); }  In addition to your comments... Below the same method using a BitmapData instead of the Graphics object as canvas: private function drawOntoBitmapData(source : IBitmapDrawable target : BitmapData position : Point = null) : void { position = position == null ? new Point() : position; var bounds : Rectangle = DisplayObject(source).getBounds(DisplayObject(source)); var bitmapData : BitmapData = new BitmapData(bounds.width bounds.height true 0x00000000); bitmapData.draw(source new Matrix(1 0 0 1 -bounds.x -bounds.y) null null null true); target.draw(bitmapData new Matrix(1 0 0 1 bounds.x + position.x bounds.y + position.y)); }  Thanks. The reason I'm drawing on a Graphics is that I draw with lines using lineTo() etc. If I drew on a Bitmap I would have to render everything as BitmapData all the time. If you are only using one canvas it wouldn't make much difference in terms of performance using a bitmapData (as target). But maybe you want to still be able to draw vector shapes on the canvas ... I added an example above give it a try. beginBitmapFill() using a Matrix doesn't actually change where on the target Graphics surface the bitmap is drawn - it just shifts the bitmap brush around itself. What I want is to draw a rectangle anywhere on the target Graphics surface using the bitmap as a brush. Am I making any sense? What I mean is that I want the rectangle containing the bitmap to be drawn at an arbitrary position on the target surface. I'm not sure what the ""position"" parameter in your drawOntoGraphics() does other than rotate (along x and y) the source bitmap within its own container. Sorry I forgot to add the position to the drawRect() - last line. It seems to work fine now. objectToBeDrawn.addChild(scaledAndRotatedObject); Aha. I see now that the trick is to match the beginBitmapFill() transform Matrix coordinates with the drawRect() coordinates exactly. Your function meets my particular need perfectly. Thanks! (Honestly though beginBitmapFill() and drawRect() feels like an awkward solution somehow...) Yes well this was a direct answer to you question trying not to sidetrack you on other solutions. Alas if you add a lot of fills you may get glitches. I would reuse the function but instead of drawing on a Graphics object i would do on a Bitmap ""canvas"". Good luck ; ) Aha! Of course. Thanks I'll try that. Hmm how would I do that ""placing""? I just realized this solution doesn't apply rotation or scaling applied to the DisplayObject being drawn. I've edited my question to include this since it would be nice to support it and my experiments with matrix scaling/rotation haven't turned out very well. I have a working implementation now. Thanks for your patience! Mmm it would just save a few lines of code if you just placed the rotated and scaled object into a holder that you draw ..."
1038,A,"Convert CYMK image to RGB using BOOST::GIL I am trying to use the boost generic image library to convert CYMK images to RGB. The following code does not compile // read cmyk image file cmyk8_image_t img; jpeg_read_image( ""1502-T2-C-PER.jpg"" img ); // convert to rgb rgb8_image_t rgb( img.dimensions() ); copy_pixels( color_converted_view<rgb8_image_t>(view(img)) view(rgb)); Anyone know how to fix this? I have based this code on tutorial code void x_luminosity_gradient(const rgb32fc_view_t& src const gray8s_view_t& dst) { gray8_image_t ccv_image(src.dimensions()); copy_pixels(color_converted_view<gray8_pixel_t>(src) view(ccv_image)); Here is the compiler output: 1>c:\program files\boost\boost_1_35_0\boost\gil\step_iterator.hpp(164) : error C2664: 'boost::gil::detail::step_iterator_adaptor<DerivedIteratorSFn>::step_iterator_adaptor(const Iterator &SFn)' : cannot convert parameter 1 from 'const boost::gil::dereference_iterator_adaptor<IteratorDFn>' to 'const boost::gil::rgb8_ptr_t &' 1> with 1> [ 1> Derived=boost::gil::memory_based_step_iterator<boost::gil::rgb8_ptr_t> 1> Iterator=boost::gil::rgb8_ptr_t  1> SFn=boost::gil::memunit_step_fn<boost::gil::rgb8_ptr_t > 1> ] 1> and 1> [ 1> Iterator=boost::gil::cmyk8_ptr_t  1> DFn=boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter> 1> ] 1> Reason: cannot convert from 'const boost::gil::dereference_iterator_adaptor<IteratorDFn>' to 'const boost::gil::rgb8_ptr_t ' 1> with 1> [ 1> Iterator=boost::gil::cmyk8_ptr_t  1> DFn=boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter> 1> ] 1> No user-defined-conversion operator available that can perform this conversion or the operator cannot be called 1> c:\program files\boost\boost_1_35_0\boost\gil\locator.hpp(271) : see reference to function template instantiation 'boost::gil::memory_based_step_iterator<Iterator>::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::pixel<ChannelValueLayout>DFn>>(const boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::pixel<ChannelValueLayout>DFn>> &)' being compiled 1> with 1> [ 1> Iterator=boost::gil::rgb8_ptr_t 1> ChannelValue=boost::gil::bits8 1> Layout=boost::gil::cmyk_layout_t 1> DFn=boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter> 1> ] 1> c:\program files\boost\boost_1_35_0\boost\gil\image_view.hpp(101) : see reference to function template instantiation 'boost::gil::memory_based_2d_locator<StepIterator>::memory_based_2d_locator<boost::gil::memory_based_step_iterator<Iterator>>(const boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<Iterator>> &)' being compiled 1> with 1> [ 1> StepIterator=boost::gil::memory_based_step_iterator<boost::gil::rgb8_ptr_t> 1> Iterator=boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter>> 1> ] 1> c:\documents and settings\james\my documents\code\animag\test\test.cpp(17) : see reference to function template instantiation 'boost::gil::image_view<Loc>::image_view<SrcView>(const View &)' being compiled 1> with 1> [ 1> Loc=boost::gil::rgb8_loc_t 1> SrcView=boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter>>>>> 1> View=boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter>>>>> 1> ] 1> c:\documents and settings\james\my documents\code\animag\test\test.cpp(48) : see reference to function template instantiation 'void processRGB<boost::gil::image_view<Loc>>(SrcView &)' being compiled 1> with 1> [ 1> Loc=boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter>>>> 1> SrcView=boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_view_tboost::gil::default_color_converter>>>>> 1> ] 1>c:\program files\boost\boost_1_35_0\boost\gil\pixel.hpp(146) : error C2440: '=' : cannot convert from 'const boost::gil::image<PixelIsPlanarAlloc>' to 'boost::gil::bits8' 1> with 1> [ 1> Pixel=boost::gil::rgb8_pixel_t 1> IsPlanar=false 1> Alloc=std::allocator<unsigned char> 1> ] 1> No user-defined-conversion operator available that can perform this conversion or the operator cannot be called 1> c:\program files\boost\boost_1_35_0\boost\gil\pixel.hpp(128) : see reference to function template instantiation 'void boost::gil::pixel<ChannelValueLayout>::assign<P>(const Channel &boost::mpl::false_)' being compiled 1> with 1> [ 1> ChannelValue=boost::gil::bits8 1> Layout=boost::gil::rgb_layout_t 1> P=boost::gil::image<boost::gil::rgb8_pixel_tfalsestd::allocator<unsigned char>> 1> Channel=boost::gil::image<boost::gil::rgb8_pixel_tfalsestd::allocator<unsigned char>> 1> ] 1> c:\program files\boost\boost_1_35_0\boost\gil\algorithm.hpp(236) : see reference to function template instantiation 'boost::gil::pixel<ChannelValueLayout> &boost::gil::pixel<ChannelValueLayout>::operator =<boost::gil::image<PixelIsPlanarAlloc>>(const P &)' being compiled 1> with 1> [ 1> ChannelValue=boost::gil::bits8 1> Layout=boost::gil::rgb_layout_t 1> Pixel=boost::gil::rgb8_pixel_t 1> IsPlanar=false 1> Alloc=std::allocator<unsigned char> 1> P=boost::gil::image<boost::gil::rgb8_pixel_tfalsestd::allocator<unsigned char>> 1> ] 1> c:\program files\boost\boost_1_35_0\boost\gil\algorithm.hpp(231) : while compiling class template member function 'void boost::gil::detail::copier_n<IO>::operator ()(boost::gil::iterator_from_2d<Loc2>__w64 intboost::gil::iterator_from_2d<Loc>) const' 1> with 1> [ 1> I=boost::gil::iterator_from_2d<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8c_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_image_tboost::gil::default_color_converter>>>>> 1> O=boost::gil::iterator_from_2d<boost::gil::rgb8_loc_t> 1> Loc2=boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8c_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_image_tboost::gil::default_color_converter>>>> 1> Loc=boost::gil::rgb8_loc_t 1> ] 1> c:\program files\boost\boost_1_35_0\boost\gil\algorithm.hpp(266) : see reference to class template instantiation 'boost::gil::detail::copier_n<IO>' being compiled 1> with 1> [ 1> I=boost::gil::iterator_from_2d<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8c_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_image_tboost::gil::default_color_converter>>>>> 1> O=boost::gil::iterator_from_2d<boost::gil::rgb8_loc_t> 1> ] 1> c:\program files\boost\boost_1_35_0\boost\gil\algorithm.hpp(292) : see reference to function template instantiation 'DstIterator boost::gil::detail::copy_with_2d_iterators<boost::gil::iterator_from_2d<Loc2>boost::gil::iterator_from_2d<Loc>>(SrcIteratorSrcIteratorDstIterator)' being compiled 1> with 1> [ 1> DstIterator=boost::gil::iterator_from_2d<boost::gil::rgb8_loc_t> 1> Loc2=boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8c_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_image_tboost::gil::default_color_converter>>>> 1> Loc=boost::gil::rgb8_loc_t 1> SrcIterator=boost::gil::iterator_from_2d<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8c_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_image_tboost::gil::default_color_converter>>>>> 1> ] 1> c:\documents and settings\james\my documents\code\animag\test\test.cpp(44) : see reference to function template instantiation 'void boost::gil::copy_pixels<boost::gil::image_view<Loc>boost::gil::image_view<boost::gil::rgb8_loc_t>>(const View1 &const View2 &)' being compiled 1> with 1> [ 1> Loc=boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8c_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_image_tboost::gil::default_color_converter>>>> 1> View1=boost::gil::image_view<boost::gil::memory_based_2d_locator<boost::gil::memory_based_step_iterator<boost::gil::dereference_iterator_adaptor<boost::gil::cmyk8c_ptr_t boost::gil::color_convert_deref_fn<const boost::gil::pixel<boost::gil::bits8boost::gil::cmyk_layout_t> boost::gil::rgb8_image_tboost::gil::default_color_converter>>>>> 1> View2=boost::gil::image_view<boost::gil::rgb8_loc_t> 1> ] Found the problem // read cmyk image file cmyk8_image_t img; jpeg_read_image( ""1502-T2-C-PER.jpg"" img ); //// convert to rgb rgb8_image_t rgb( img.dimensions() ); copy_pixels( color_converted_view<rgb8_pixel_t>(view(img)) view(rgb)); The trick is to use rgb8_pixel_twhen calling color_converted_view"
1039,A,"Easy way to display a continuously updating image in C/Linux I'm a scientist who is quite comfortable with C for numerical computation but I need some help with displaying the results. I want to be able to display a continuously updated bitmap in a window which is calculated from realtime data. I'd like to be able to update the image quite quickly (e.g. faster than 1 frame/second preferably 100 fps). For example: char image_buffer[width*height*3];//rgb data initializewindow(); for (t=0;t<t_end;t++) { getdata(data);//get some realtime data docalcs(image_buffer data);//process the data into an image drawimage(image_buffer);//draw the image } What's the easiest way to do this on linux (Ubuntu)? What should I use for initializewindow() and drawimage()? GUI stuff is a regularly-reinvented wheel and there's no reason to not use a framework. I'd recommend using either QT4 or wxWidgets. If you're using Ubuntu GTK+ will suffice as it talks to GNOME and may be more comfortable to you (QT and wxWidgets both require C++). Have a look at GTK+ QT and wxWidgets. Here's the tutorials for all 3: Hello World wxWidgets GTK+ 2.0 Tutorial GTK+ Tutorials QT4  In my experience Xlib via MIT-SHM extension was significantly faster than SDL surfaces not sure I used SDL in the most optimal way though.  In addition to Jed Smith's answer there are also lower-level frameworks like OpenGL which is often used for game programming. Given that you want to use a high frame rate I'd consider something like that. GTK and the like aren't primarily intended for rapidly updating displays. I think you'll need something with OpenGL to handle opening a window and such and I think SDL is a good choice there. I suggest you start with SDL (with the updateRect or flip code described in other answers) then if that's not fast enough try the OpenGL calls from the SDL framework.  I'd recommend SDL too. However there's a bit of understanding you need to gather if you want to write fast programs and that's not the easiest thing to do. I would suggest this O'Reilly article as a starting point. But I shall boil down the most important points from a computations perspective. Double buffering What SDL calls ""double buffering"" is generally called page flipping. This basically means that on the graphics card there are two chunks of memory called pages each one large enough to hold a screen's worth of data. One is made visible on the monitor the other one is accessible by your program. When you call SDL_Flip() the graphics card switches their roles (i.e. the visible one becomes program-accessible and vice versa). The alternative is rather than swapping the roles of the pages instead copy the data from the program-accessible page to the monitor page (using SDL_UpdateRect()). Page flipping is fast but has a drawback: after page flipping your program is presented with a buffer that contains the pixels from 2 frames ago. This is fine if you need to recalculate every pixel every frame. However if you only need to modify smallish regions on the screen every frame and the rest of the screen does not need to change then UpdateRect can be a better way (see also: SDL_UpdateRects()). This of course depends on what it is you're computing and how you're visualising it. Analyse your image-generating code - maybe you can restructure it to get something more efficient out of it? Note that if your graphics hardware doesn't support page flipping SDL will gracefully use the other method for you. Software/Hardware/OpenGL This is another question you face. Basically software surfaces live in RAM hardware surfaces live in Video RAM and OpenGL surfaces are managed by OpenGL magic. Depending on your hardware OS and SDL version programatically modifying the pixels of a hardware surface can involve a LOT of memory copying (VRAM to RAM and then back!). You don't want this to happen every frame. In such cases software surfaces work better. But then you can't take advantage of double buffering nor hardware accelerated blits. Blits are block-copies of pixels from one surface to another. This works well if you want to draw a whole lot of identical icons on a surface. Not so useful if you're generating a temperature map. OpenGL lets you do much more with your graphics hardware (3D acceleration for a start). Modern graphics cards have a lot of processing power but it's kind of hard to use unless you're making a 3D simulation. Writing code for a graphics processor is possible but quite different to ordinary C. Demo Here's a quick demo SDL program that I made. It's not supposed to be a perfect example and may have some portability problems. (I will try to edit a better program into this post when I get time.) #include ""SDL.h"" #include <assert.h> #include <math.h> /* This macro simplifies accessing a given pixel component on a surface. */ #define pel(surf x y rgb) ((unsigned char *)(surf->pixels))[y*(surf->pitch)+x*3+rgb] int main(int argc char *argv[]) { int x y t; /* Event information is placed in here */ SDL_Event event; /* This will be used as our ""handle"" to the screen surface */ SDL_Surface *scr; SDL_Init(SDL_INIT_VIDEO); /* Get a 640x480 24-bit software screen surface */ scr = SDL_SetVideoMode(640 480 24 SDL_SWSURFACE); assert(scr); /* Ensures we have exclusive access to the pixels */ SDL_LockSurface(scr); for(y = 0; y < scr->h; y++) for(x = 0; x < scr->w; x++) { /* This is what generates the pattern based on the xy co-ord */ t = ((x*x + y*y) & 511) - 256; if (t < 0) t = -(t + 1); /* Now we write to the surface */ pel(scr x y 0) = 255 - t; //red pel(scr x y 1) = t; //green pel(scr x y 2) = t; //blue } SDL_UnlockSurface(scr); /* Copies the `scr' surface to the _actual_ screen */ SDL_UpdateRect(scr 0 0 0 0); /* Now we wait for an event to arrive */ while(SDL_WaitEvent(&event)) { /* Any of these event types will end the program */ if (event.type == SDL_QUIT || event.type == SDL_KEYDOWN || event.type == SDL_KEYUP) break; } SDL_Quit(); return EXIT_SUCCESS; } I think I fixed the bug. Your ""pel"" macro didn't seem to put the correct pixels in the right place. However the rest put me on the right track. Thanks.  If all you want to do is display the data (ie no need for a GUI) you might want to take a look at SDL: It's straight-forward to create a surface from your pixel data and then display it on screen. Inspired by Artelius' answer I also hacked up an example program: #include <SDL/SDL.h> #include <assert.h> #include <stdint.h> #include <stdlib.h> #define WIDTH 256 #define HEIGHT 256 static _Bool init_app(const char * name SDL_Surface * icon uint32_t flags) { atexit(SDL_Quit); if(SDL_Init(flags) < 0) return 0; SDL_WM_SetCaption(name name); SDL_WM_SetIcon(icon NULL); return 1; } static uint8_t * init_data(uint8_t * data) { for(size_t i = WIDTH * HEIGHT * 3; i--; ) data[i] = (i % 3 == 0) ? (i / 3) % WIDTH : (i % 3 == 1) ? (i / 3) / WIDTH : 0; return data; } static _Bool process(uint8_t * data) { for(SDL_Event event; SDL_PollEvent(&event);) if(event.type == SDL_QUIT) return 0; for(size_t i = 0; i < WIDTH * HEIGHT * 3; i += 1 + rand() % 3) data[i] -= rand() % 8; return 1; } static void render(SDL_Surface * sf) { SDL_Surface * screen = SDL_GetVideoSurface(); if(SDL_BlitSurface(sf NULL screen NULL) == 0) SDL_UpdateRect(screen 0 0 0 0); } static int filter(const SDL_Event * event) { return event->type == SDL_QUIT; } #define mask32(BYTE) (*(uint32_t *)(uint8_t [4]){ [BYTE] = 0xff }) int main(int argc char * argv[]) { (void)argc (void)argv; static uint8_t buffer[WIDTH * HEIGHT * 3]; _Bool ok = init_app(""SDL example"" NULL SDL_INIT_VIDEO) && SDL_SetVideoMode(WIDTH HEIGHT 24 SDL_HWSURFACE); assert(ok); SDL_Surface * data_sf = SDL_CreateRGBSurfaceFrom( init_data(buffer) WIDTH HEIGHT 24 WIDTH * 3 mask32(0) mask32(1) mask32(2) 0); SDL_SetEventFilter(filter); for(; process(buffer); SDL_Delay(10)) render(data_sf); return 0; } +1 for SDL being most suitable. It's fast portable easy and does exactly what the asker wants. I'd recommend SDL too. Unfortunately I've not found an SDL guide that provides the right level of detail (I'm thinking of writing my own). So you might have to do a lot of reading and putting-together if you want the best results. Thanks this worked well. I got a sample up and running this afternoon. Also see Artelius's demo. `for(; process(buffer); SDL_Delay(10)) render(data_sf);` ....this post allowed me to prevent an overnight at work. Thanks a lot!"
1040,A,"ANY material writen in/for DELPHI around the graphics topic? Does anyone knows ANY material writen in/for DELPHI around the graphics topic? Planning to build a software for medical imaging processing . Thinking in 3D UI to absorve the power of nvidias GTX graphics card and some real-time 2D processing integrated with high-end scanners. Please dont take this as a ""rant"" but we have zillions of C++ books writen about that kind of topic and nothing for pascal/delphi. If you have some experience could you comment about that? Is it better to learn c++ to have access to that material? Can i go with delphi? I have experience with delphi but none with graphics... And i have a deadline... Thanks. For 2D & 3D graphics in your application you can use VGScene & DXScene from KSDev and there more resouces about the graphics in the links below: efg computer lab page for the 3D you can look at Delphi & OpenGL GLScene Delphi OpenGL Tutorials and Projects also you can look at Pascal Game Development for many graphics and game engines for Delphi. Mohammed for UI this component looks great thanks. Don't get too hung up on the language. Most books/papers on graphics use C/C++ because the language is popular compilers are readily available and the mainstream graphics libraries (directx opengl) are written in it. Most graphics algorithms can be ported to any language as long as you're familiar enough with C/C++ to understand it.  A lot of useful information can be found at the Graphics32 website.  These are very old books but could be of intererst: Delphi Developer's Guide to OpenGL (Wordware Publishing Jon Jacobs) Delphi Graphics and Game Programming Exposed with DirectX 7.0 (Wordware Publishing John Ayres) Most of the C/C++ books on OpenGL or DirectX could be easily applied to Delphi.  I do not know any book which could help you but there is a lot of useful frameworks which can greatly speed up the process of creating 3D applications. Firs of all you need to decide about the 3D API. Do you want to do this in OpenGL (more structural approach crossplatform) or Direct3D (OO approach) If you prefer OpenGL then you might start with GLScene. If you want Direct3D then I would recommend Asphyre. To be honest you do not need a book designed for 3D with Delphi. All you need is a Delphi wrapper and that is already done by others. Then you pick up any book conerning the topic and you are fine. Some C knowledge is required but not to much. For example: The OpenGL Programming Guide - The Redbook is almost like you would read a Pascal code;) As goes for tutorials you have a NeHe website with Delphi code included for OpenGL."
1041,A,Performance issue with QGraphicsScene::createItemGroup I'm using the Qt graphics API to display layers in some GIS software. Each layer is a group containing graphic primitives. I have a performance issue when loading fairly large data sets for example this is what happens when making a group composed of ~96k circular paths (points from a shapefile): The complete callgrind dump is here. The QGraphicsScene::createItemGroup() call takes about 150 seconds to complete on my 2.4GHz core2 and it seems all this time is used in QGraphicsItemPrivate::updateEffectiveOpacity() which itself consumes 37% of its time calling QGraphicsItem::flags() 4 billion times (the data comes from a test script with no GUI just a scene not even tied to a view). All the rest is pretty much instantaneous (creating the items reading the file etc...). I tried to disable the scene's index before creating the group and obtained similar results. What could I do to improve performances in this case ? If I can't is there a way to create groups faster ? The items are `QGraphicsEllipseItem` with the `QGraphicsItem::ItemIgnoresTransformations` flag set. The `flags()` and `updateEffectiveOpacity()` methods are called from `QGraphicsScene::createItemGroup()`. And removing the `ItemIgnoresTransformations` flag does not change anything Can you add a little bit more information? What kinds of items do you use? Who calls the flags method and the updateEffectOpacity methods the group or the items itself? What flags are set on the items? After studying the source code a little bit I found out that the updateEffectiveOpacity has O(n²) with regard to the children of the item's parent item (search for the method qt_allChildrenCombineOpacity). This is probably also the reason that method disappeared in Qt 4.6 and apparently been replaced by something else. Anyway you should try out setting the ItemDoesntPropagateOpacityToChildren flag on the group item (i.e. you'll have to create it yourself) at least while adding all the items. Yes ! my loading time went down to a few seconds which is still slow but sure more acceptable than over 2 minutes :) (I still reactivate the flag once the group is created). Thank you very much !
1042,A,"Algorithm to detect intersection of two rectangles? I'm looking for an algorithm to detect if two rectangles intersect (one at an arbitrary angle the other with only vertical/horizontal lines). Testing if a corner of one is in the other ALMOST works. It fails if the rectangles form a cross-like shape. It seems like a good idea to avoid using slopes of the lines which would require special cases for vertical lines. 2D or 3D? I realised I just posted a 3D solution which made assumptions about what you meant by ""intersection"". I think graphics api and most GUI libraries(like swing) has this implemented. Unfortunately I don't think .NET has this. I've created a C# implementation for this; see [here](http://manski.net/2011/05/11/rectangle-intersection-test-with-csharp/). What language are you gonna do this in? Because in Java there are built-in classes that let you do this. What are these built-in classes? what if you add to your corner check a check to see if the second rectangle is inside the bounds (rectangular) of the angled rectangle? that can miss cases where they overlap but no corner is inside any rectangle This question is almost the same as: http://stackoverflow.com/questions/306316/determine-if-two-rectangles-overlap-each-other. Although this is looking for a solution that is particularly for C++. The accepted answer is also pretty simple and straightforward. This is what I would do for the 3D version of this problem: Model the 2 rectangles as planes described by equation P1 and P2 then write P1=P2 and derive from that the line of intersection equation which won't exist if the planes are parallel (no intersection) or are in the same plane in which case you get 0=0. In that case you will need to employ a 2D rectangle intersection algorithm. Then I would see if that line which is in the plane of both rectangles passes through both rectangles. If it does then you have an intersection of 2 rectangles otherwise you don't (or shouldn't I might have missed a corner case in my head). To find if a line passes through a rectangle in the same plane I would find the 2 points of intersection of the line and the sides of the rectangle (modelling them using line equations) and then make sure the points of intersections are with in range. That is the mathematical descriptions unfortunately I have no code to do the above. You missed the part where if you find the planar intersect line you have to ensure that a portion of it exists within both rectangles.  If you're using Java all implementations of the Shape interface have an intersects method that take a rectangle. Unfortunately I'm using C#. The Rectangle class has a Contains () method but it's only for non-rotated rectangles.  Basically look at the following picture: If the two boxes collide the lines A and B will overlap. Note that this will have to be done on both the X and the Y axis and both need to overlap for the rectangles to collide. There is a good article in gamasutra.com which answers the question (the picture is from the article). I did similar algorithm 5 years ago and I have to find my code snippet to post it here later Amendment: The Separating Axis Theorem states that two convex shapes do not overlap if a separating axis exists (i.e. one where the projections as shown do not overlap). So ""A separating axis exists"" => ""No overlap"". This is not a bi-implication so you cannot conclude the converse. I assume this has to be done on both the x and y axis? Please edit the original post; I was wondering the same thing and it's not obvious to look in the comments. Obviously as two squares (0011) and (0314) do not overlap but their projections on the x axis completely overlap. Both tests are necessary the combination is sufficient. Yes both x and y axis. It is not sufficient for the x and y projections to overlap : take eg the rectangles [(00) (03) (33) (30)] and [(25) (52) (74) (47)]. I agree with @Joel in Gö. This method misses a large set of cases where the rectangles do not overlap yet their projected radii do in both x and y. See this example: http://tinypic.com/view.php?pic=2dv27gn&s=5 As said before this is not a full nor accurate solution This answer is not wrong but it is misleading. It is true that: If the two boxes collide the lines A and B will overlap. but it is also true that: If the lines A and B overlap the two boxes may or may not be colliding @floater: I would say it's *not only* wrong but *also* misleading which is even worse. How about this? event O: lines the lines A and B will overlap for X and Y axis; event P: at least one of the rectangle A's corner is in rectangle B. The 2 rectangles overlap if P || O&P.  One solution is to use something called a No Fit Polygon. This polygon is calculated from the two polygons (conceptually by sliding one around the other) and it defines the area for which the polygons overlap given their relative offset. Once you have this NFP then you simply have to do an inclusion test with a point given by the relative offset of the two polygons. This inclusion test is quick and easy but you do have to create the NFP first. Have a search for No Fit Polygon on the web and see if you can find an algorithm for convex polygons (it gets MUCH more complex if you have concave polygons). If you can't find anything then email me at howard dot J dot may gmail dot com  Here is what I think will take care of all possible cases. Do the following tests. Check any of the vertices of rectangle 1 reside inside rectangle 2 and vice versa. Anytime you find a vertex that resides inside the other rectangle you can conclude that they intersect and stop the search. THis will take care of one rectangle residing completely inside the other. If the above test is inconclusive find the intersecting points of each line of 1 rectangle with each line of the other rectangle. Once a point of intersection is found check if it resides between inside the imaginary rectangle created by the corresponding 4 points. When ever such a point is found conclude that they intersect and stop the search. If the above 2 tests return false then these 2 rectangles do not overlap.  I implemented it like this: bool rectCollision(const CGRect &boundsA const Matrix3x3 &mB const CGRect &boundsB) { float Axmin = boundsA.origin.x; float Axmax = Axmin + boundsA.size.width; float Aymin = boundsA.origin.y; float Aymax = Aymin + boundsA.size.height; float Bxmin = boundsB.origin.x; float Bxmax = Bxmin + boundsB.size.width; float Bymin = boundsB.origin.y; float Bymax = Bymin + boundsB.size.height; // find location of B corners in A space float B0x = mB(00) * Bxmin + mB(01) * Bymin + mB(02); float B0y = mB(10) * Bxmin + mB(11) * Bymin + mB(12); float B1x = mB(00) * Bxmax + mB(01) * Bymin + mB(02); float B1y = mB(10) * Bxmax + mB(11) * Bymin + mB(12); float B2x = mB(00) * Bxmin + mB(01) * Bymax + mB(02); float B2y = mB(10) * Bxmin + mB(11) * Bymax + mB(12); float B3x = mB(00) * Bxmax + mB(01) * Bymax + mB(02); float B3y = mB(10) * Bxmax + mB(11) * Bymax + mB(12); if(B0x<Axmin && B1x<Axmin && B2x<Axmin && B3x<Axmin) return false; if(B0x>Axmax && B1x>Axmax && B2x>Axmax && B3x>Axmax) return false; if(B0y<Aymin && B1y<Aymin && B2y<Aymin && B3y<Aymin) return false; if(B0y>Aymax && B1y>Aymax && B2y>Aymax && B3y>Aymax) return false; float det = mB(00)*mB(11) - mB(01)*mB(10); float dx = mB(12)*mB(01) - mB(02)*mB(11); float dy = mB(02)*mB(10) - mB(12)*mB(00); // find location of A corners in B space float A0x = (mB(11) * Axmin - mB(01) * Aymin + dx)/det; float A0y = (-mB(10) * Axmin + mB(00) * Aymin + dy)/det; float A1x = (mB(11) * Axmax - mB(01) * Aymin + dx)/det; float A1y = (-mB(10) * Axmax + mB(00) * Aymin + dy)/det; float A2x = (mB(11) * Axmin - mB(01) * Aymax + dx)/det; float A2y = (-mB(10) * Axmin + mB(00) * Aymax + dy)/det; float A3x = (mB(11) * Axmax - mB(01) * Aymax + dx)/det; float A3y = (-mB(10) * Axmax + mB(00) * Aymax + dy)/det; if(A0x<Bxmin && A1x<Bxmin && A2x<Bxmin && A3x<Bxmin) return false; if(A0x>Bxmax && A1x>Bxmax && A2x>Bxmax && A3x>Bxmax) return false; if(A0y<Bymin && A1y<Bymin && A2y<Bymin && A3y<Bymin) return false; if(A0y>Bymax && A1y>Bymax && A2y>Bymax && A3y>Bymax) return false; return true; } The matrix mB is any affine transform matrix that converts points in the B space to points in the A space. This includes simple rotation and translation rotation plus scaling and full affine warps but not perspective warps. It may not be as optimal as possible. Speed was not a huge concern. However it seems to work ok for me.  In Cocoa you could easily detect whether the selectedArea rect intersects your rotated NSView's frame rect. You don't even need to calculate polygons normals an such. Just add these methods to your NSView subclass. For instance the user selects an area on the NSView's superview then you call the method DoesThisRectSelectMe passing the selectedArea rect. The API convertRect: will do that job. The same trick works when you click on the NSView to select it. In that case simply override the hitTest method as below. The API convertPoint: will do that job ;-) - (BOOL)DoesThisRectSelectMe:(NSRect)selectedArea { NSRect localArea = [self convertRect:selectedArea fromView:self.superview]; return NSIntersectsRect(localArea self.bounds); } - (NSView *)hitTest:(NSPoint)aPoint { NSPoint localPoint = [self convertPoint:aPoint fromView:self.superview]; return NSPointInRect(localPoint self.bounds) ? self : nil; } That code only works for rectangles that are square to the screen. That's a trivial case. The assumption is that we're dealing with rectangles that are not at 90 degree angles to the screen or each other.  You could find the intersection of each side of the angled rectangle with each side of the axis-aligned one. Do this by finding the equation of the infinite line on which each side lies (i.e. v1 + t(v2-v1) and v'1 + t'(v'2-v'1) basically) finding the point at which the lines meet by solving for t when those two equations are equal (if they're parallel you can test for that) and then testing whether that point lies on the line segment between the two vertices i.e. is it true that 0 <= t <= 1 and 0 <= t' <= 1. However this doesn't cover the case when one rectangle completely covers the other. That you can cover by testing whether all four points of either rectangle lie inside the other rectangle.  Well the brute force method is to walk the edges of the horizontal rectangle and check each point along the edge to see if it falls on or in the other rectangle. The mathematical answer is to form equations describing each edge of both rectangles. Now you can simply find if any of the four lines from rectangle A intersect any of the lines of rectangle B which should be a simple (fast) linear equation solver. The problem with equations is when you have a vertical line which has infinite slope. There are corner cases for every solution. and one square entirely enclosing the other.  Another way to do the test which is slightly faster than using the separating axis test is to use the winding numbers algorithm (on quadrants only - not angle-summation which is horrifically slow) on each vertex of either rectangle (arbitrarily chosen). If any of the vertices have a non-zero winding number the two rectangles overlap. This algorithm is somewhat more long-winded than the separating axis test but is faster because it only require a half-plane test if edges are crossing two quadrants (as opposed to up to 32 tests using the separating axis method) The algorithm has the further advantage that it can be used to test overlap of any polygon (convex or concave). As far as I know the algorithm only works in 2D space. I may be wrong but doesn't that just check if the vertices of one rectangle are inside another? If yes it's not enough because rectangles may overlap without any vertices inside. Can they with rectangles? How? It seems to me that in order for 2 rectangles to intersect at least one vertex of one of the rectangles must lie on the other rectangle.  Check to see if any of the lines from one rectangle intersect any of the lines from the other. Naive line segment intersection is easy to code up. If you need more speed there are advanced algorithms for line segment intersection (sweep-line). See http://en.wikipedia.org/wiki/Line_segment_intersection Careful! Don't forget the case where one rectangle completely encloses another  Either I am missing something else why make this so complicated? if (x1y1) and (X1Y1) are corners of the rectangles then to find intersection do:  xIntersect = false; yIntersect = false; if (!(Math.min(x1 x2 x3 x4) > Math.max(X1 X2 X3 X4) || Math.max(x1 x2 x3 x4) < Math.min(X1 X2 X3 X4))) xIntersect = true; if (!(Math.min(y1 y2 y3 y4) > Math.max(Y1 Y2 Y3 Y4) || Math.max(y1 y2 y3 y4) < Math.min(Y1 Y2 Y3 Y4))) yIntersect = true; if (xIntersect && yIntersect) {alert(""Intersect"");} You are missing that he wants one to be rotated by an arbitrary angle.  The standard method would be to do the separating axis test (do a google search on that). In short: Two objects don't intersect if you can find a line that separates the two objects. e.g. the objects / all points of an object are on different sides of the line. The fun thing is that it's sufficient to just check all edges of the two rectangles. If the rectangles don't overlap one of the edges will be the separating axis. In 2D you can do this without using slopes. An edge is simply defined as the difference between two vertices e.g.  edge = v(n) - v(n-1) You can get a perpendicular to this by rotating it by 90°. In 2D this is easy as:  rotated.x = -unrotated.y rotated.y = unrotated.x So no trigonometry or slopes involved. Normalizing the vector to unit-length is not required either. If you want to test if a point is on one or another side of the line you can just use the dot-product. the sign will tell you which side you're on:  // rotated: your rotated edge // v(n-1) any point from the edge. // testpoint: the point you want to find out which side it's on. side = sign (rotated.x * (testpoint.x - v(n-1).x) + rotated.y * (testpoint.y - v(n-1).y); Now test all points of rectangle A against the edges of rectangle B and vice versa. If you find a separating edge the objects don't intersect (providing all other points in B are on the other side of the edge being tested for - see drawing below). If you find no separating edge either the rectangles are intersecting or one rectangle is contained in the other. The test works with any convex polygons btw.. Amendment: To identify a separating edge it is not enough to test all points of one rectangle against each edge of the other. The candidate-edge E (below) would as such be identified as a separating edge as all points in A are in the same half-plane of E. However it isn't a separating edge because the vertices Vb1 and Vb2 of B are also in that half-plane. It would only have been a separating edge if that had not been the case I like this approach; it's simple and logical. It seems if one rect is inside the other all edges of the containing rect will appear to be separating edges. In this case testing if a single point from the (possibly) contained rect is in the containing rect would tell if it's entirely contained. This algorithm doesn't work for all cases. It is possible to place the second rectangle rotated 45 degrees to the first rectangle and offset along the diagonal so that it fulfills the above intersection tests but doesn't intersect. Skizz check all eight edges. If the objects don't intersect one of the eight edges *will* separate them. Why don't you post an image showing your case? I can show you the axis.. Brilliant thank you. My mistake it does cope with that condition. That's cool! I learned something new today. Nice to read that dysfunctor. For 3d polyhedrons you need some additional tests the possible separating axis are the face normals *and* the cross products of all pairs of edges (one taken from each object). These cross products are all ""through the paper"" in 2d so they don't need to be checked. My adaptation of this algorithm to JS: https://gist.github.com/3007244 (Thank you its great!) The image is dead now (nov 2012) Just walk around the points of your rectangle in clockwise or counter-clockwise order. v(n) is the current point v(n-1) is the previous point. Easy as that. I'm not quite clear on what v(n-1) is supposed to be in the last part. A point from the edge of what? The rotated edge?  m_pGladiator's answer is right and I prefer to it. Separating axis test is simplest and standard method to detect rectangle overlap. A line for which the projection intervals do not overlap we call a separating axis. Nils Pipenbrinck's solution is too general. It use dot product to check whether one shape is totally on the one side of the edge of the other. This solution is actually could induce to n-edge convex polygons. However it is not optmized for two rectangles. the critical point of m_pGladiator's answer is that we should check two rectangles' projection on both axises (x and y). If two projections are overlapped then we could say these two rectangles are overlapped. So the comments above to m_pGladiator's answer are wrong. for the simple situation if two rectangles are not rotated we present a rectangle with structure: struct Rect { x // the center in x axis y // the center in y axis width height } we name rectangle A B with rectA rectB.  if Math.abs(rectA.x - rectB.x) < (Math.abs(rectA.width + rectB.width) / 2) && (Math.abs(rectA.y - rectB.y) < (Math.abs(rectA.height + rectB.height) / 2)) then // A and B collide end if if any one of the two rectangles are rotated It may needs some efforts to determine the projection of them on x and y axises. Define struct RotatedRect as following: struct RotatedRect : Rect { double angle; // the rotating angle oriented to its center } the difference is how the width' is now a little different: widthA' for rectA: Math.sqrt(rectA.width*rectA.width + rectA.height*rectA.height) * Math.cos(rectA.angle) widthB' for rectB: Math.sqrt(rectB.width*rectB.width + rectB.height*rectB.height) * Math.cos(rectB.angle)  if Math.abs(rectA.x - rectB.x) < (Math.abs(widthA' + widthB') / 2) && (Math.abs(rectA.y - rectB.y) < (Math.abs(heightA' + heightB') / 2)) then // A and B collide end if Could refer to a GDC(Game Development Conference 2007) PPT www.realtimecollisiondetection.net/pubs/GDC07_Ericson_Physics_Tutorial_SAT.ppt Why do you need Math.abs() in ""Math.abs(rectA.width + rectB.width)"" to handle negative widths?"
1043,A,"Aspect ratios - how to go about them? (D3D viewport setup) Allright - seems my question was as cloudy as my head. Lets try again. I have 3 properties while configuring viewports for a D3D device: - The resolution the device is running in (full-screen). - The physical aspect ratio of the monitor (as fraction and float:1 so for ex. 4:3 & 1.33). - The aspect ratio of the source resolution (source resolution itself is kind of moot and tells us little more than the aspect ratio the rendering wants and the kind of resolution that would be ideal to run in). Then we run into this: // -- figure out aspect ratio adjusted VPs -- m_nativeVP.Width = xRes; m_nativeVP.Height = yRes; m_nativeVP.X = 0; m_nativeVP.Y = 0; m_nativeVP.MaxZ = 1.f; m_nativeVP.MinZ = 0.f; FIX_ME // this does not cover all bases -- fix! uint xResAdj yResAdj; if (g_displayAspectRatio.Get() < g_renderAspectRatio.Get()) { xResAdj = xRes; yResAdj = (uint) ((float) xRes / g_renderAspectRatio.Get()); } else if (g_displayAspectRatio.Get() > g_renderAspectRatio.Get()) { xResAdj = (uint) ((float) yRes * g_renderAspectRatio.Get()); yResAdj = yRes; } else // == { xResAdj = xRes; yResAdj = yRes; } m_fullVP.Width = xResAdj; m_fullVP.Height = yResAdj; m_fullVP.X = (xRes - xResAdj) >> 1; m_fullVP.Y = (yRes - yResAdj) >> 1; m_fullVP.MaxZ = 1.f; m_fullVP.MinZ = 0.f; Now as long as g_displayAspectRatio equals the ratio of xRes/yRes (= adapted from device resolution) all is well and this code will do what's expected of it. But as soon as those 2 values are no longer related (for example someone runs a 4:3 resolution on a 16:10 screen hardware-stretched) another step is required to compensate and I've got trouble figuring out how exactly. (and p.s I use C-style casts on atomic types live with it :-) ) Use `static_cast` btw. I don't understand what exactly you are trying to do. Rewrote question. I'm assuming what you want to achieve is a ""square"" projection e.g. when you draw a circle you want it to look like a circle rather than an ellipse. The only thing you should play with is your projection (camera) aspect ratio. In normal cases monitors keep pixels square and all you have to do is set your camera aspect ratio equal to your viewport's aspect ratio: viewport_aspect_ratio = viewport_res_x / viewport_res_y; camera_aspect_ratio = viewport_aspect_ratio; In the stretched case you describe (4:3 image stretched on a 16:10 screen for example) pixels are not square anymore and you have to take that into account in your camera aspect ratio: stretch_factor_x = screen_size_x / viewport_res_x; stretch_factor_y = screen_size_y / viewport_res_y; pixel_aspect_ratio = stretch_factor_x / stretch_factor_y; viewport_aspect_ratio = viewport_res_x / viewport_res_y; camera_aspect_ratio = viewport_aspect_ratio * pixel_aspect_ratio; Where screen_size_x and screen_size_y are multiples of the real size of the monitor (e.g. 16:10). However you should simply assume square pixels (unless you have a specific reason no to) as the monitor may report incorrect physical size informations to the system or no informations at all. Also monitors don't always stretch mine for example keeps 1:1 pixels aspect ratio and adds black borders for lower resolutions. Edit If you want to adjust your viewport to some aspect ratio and fit it on an arbitrary resolution then you could do like that : viewport_aspect_ratio = 16.0 / 10.0; // The aspect ratio you want your viewport to have screen_aspect_ratio = screen_res_x / screen_res_y; if (viewport_aspect_ratio > screen_aspect_ratio) { // Viewport is wider than screen fit on X viewport_res_x = screen_res_x; viewport_res_y = viewport_res_x / viewport_aspect_ratio; } else { // Screen is wider than viewport fit on Y viewport_res_y = screen_res_y; viewport_res_x = viewport_res_y * viewport_aspect_ratio; } camera_aspect_ratio = viewport_res_x / viewport_res_y; screen_aspect_ratio does depict the physical aspect ratio unless you have non square pixels. Which is the case I wanted to cover. In a config dialog I'm giving the user the option to either adapt the aspect ratio (from the selected res obviously) or set it themselves. Hence the possible difference between the 2. adding to that - I feel this question is hereby answered :) (taking that I might have to look at adjusting my output instead of the res in case of non-square pixels). Thanks for answering. Your assumption is partially correct - but I should add to it that I'm also looking to hold on to a certain aspect ratio (or screen format) for the 2D compositioning - much like for ex. a widescreen movie. Unlike a game I hadn't evaluated the option of authoring visuals that'd look good in either proportion. Your hint not to worry about non-square pixels is quite reassuring (was thinking about that) but taking into account the pixel ratio is a good one. Edited answer to add the ""arbitrary viewport aspect ratio on arbitrary screen resolution"" stuff. And that about sums up to what the code does now (I realize I didn't paste any code on how I handled my projection aspect ratio which could've been helpful as that was OK already) :) But fair enough thanks! Sorry can't understand what your problem is then. ""[...] another step is required to compensate and I've got trouble figuring out how exactly."": define ""compensate"". In that snippet there's a line of code: screen_aspect_ratio = screen_res_x / screen_res_y. This relates the 2. Now what if screen_aspect_ratio is something arbitrary depicting the physical ratio of the display? (for ex. in case of hardware stretch) (still the more I discuss this the more I wonder why I even want this.. - which I guess is good) I think that this line `viewport_res_x = viewport_res_y * viewport_aspect_ratio;` it should actually be `viewport_res_x = viewport_res_y / viewport_aspect_ratio;` +1 great answer anyway!"
1044,A,Spinning a 3D model in C# How do I take a 3D model that I created in 3D Studio Max and put it into my Winform C# program and make it spin? I'd prefer not to use DirectX if possible. I don't want anything complex. I simply want my model to rotate along the X axis. Thats it. Thanks What did you end up settling on? The accepted answer has a few choices. I'm facing a similar situation and all the options I'm finding seem at least a little painful :) I went with Axiom If you want it to be dynamic then the simplest option would be to render out an animation of the object rotating but make each frame a separate file. Then you just show the correct image based on how the user is dragging the mouse. If the user drags the mouse to the right then increment the frame and show the next image. If moving to the left decrement the frame.  For something non interactive: Export the animation to an AVI and embed that in your form: http://stackoverflow.com/questions/332117/embedding-video-in-a-winforms-app It's not really what I'd recommend but it's an alternative to creating an animated gif. For something partially interactive (i.e. allowing limited movement): I've seen QuickTime movies that you can control with the mouse. There's an example on this page. It's not 3D though. For something fully interactive: You need a 3D rendering engine of some sort and that does (usually) require DirectX or OpenGL. However if you're only dealing with simple objects you might (repeat might) get away with a software renderer. Good suggestion except I'd like it to be dynamic. Meaning that the user can drag the model in any direction along the axis. How would I do this? @icemanind - well if you'd said you wanted interactivity... Sorry I should have been more clear  You should use a 3D rendering engine for C# Something like http://axiom3d.net/wiki/index.php/Main_Page http://www.codeproject.com/KB/GDI-plus/exoengine.aspx http://irrlicht.sourceforge.net/features.html http://freegamedev.net/wiki/Free_cross-platform_real-time_3D_engines I have never used any rendering engines but for your requirements (letting the user move the object) i think a 3D engine would do. But perhaps this is over kill
1045,A,"Formula for a orthogonal projection matrix? I've been looking around a bit and can't seem to find just what I""m looking for. I've found ""canonical formulas"" but what's the best way to use these? Do I have to scale every single vertex down? Or is there a better way? A formula would really help me out but I'm also looking for an explanation about the near and far z planes relative the viewer's position You might get a better response if you ask more specifically - this is pretty general. Give an example of what the input to the formula is and what the expected output is. This is in the realm of linear algebra so that might give you more google fodder. Here is a reasonable source that derives an orthogonal project matrix: Consider a few points: First in eye space your camera is positioned at the origin and looking directly down the z-axis. And second you usually want your field of view to extend equally far to the left as it does to the right and equally far above the z-axis as below. If that is the case the z-axis passes directly through the center of your view volume and so you have r = –l and t = –b. In other words you can forget about r l t and b altogether and simply define your view volume in terms of a width w and a height h along with your other clipping planes f and n. If you make those substitutions into the orthographic projection matrix above you get this rather simplified version: All of the above gives you a matrix that looks like this (add rotation and translation as appropriate if you'd like your resulting transformation matrix to treat an arbitrary camera position and orientation).  http://mathworld.wolfram.com/OrthogonalProjection.html http://nptel.iitm.ac.in/courses/Webcourse-contents/IIT-KANPUR/mathematics-2/node51.html"
1046,A,"What is the best way to do 2D animation? I'm writing a 2D animation class and I've got TGA pictures in which the player animation is stored. These pictures are 8x8 tiles (so on each row there are 8 frames of a moving character) However I don't have a clue on how to animate this in code. I was thinking about updating it by moving the u-v coordinates each frame and returning only the current frame. How can I do this? What you are describing sounds closer to video than 2D animation and you might be better of looking into some video codecs. But alternatively OpenGL would be a good place to start.  Sup dawg. Seeing as you're using the UV coordinates of the texture that contains all your animation states you'll need to convert from pixel coordinates to UV coordinates. If your sprite is 32 pixels wide and your texture is 256 pixels wide (thus containing 8 frames) you'll want to divide the width of the sprite by the width of the texture giving you a number between... 0 and 1! This is your offset. To get a frame from your strip simply do: float step_size_x = (float)(width) / (float)(texture_width); float step_size_y = (float)(height) / (float)(texture_height); float begin_u = step_size_x * step_current_x; float begin_v = step_size_y * step_current_y; float end_u = begin_x + step_size_x; // basically if you want sprite 0 you go from 0 to the x of sprite 1 float end_v = begin_y + step_size_y; And that's it! Now if you want to copy the portion of the texture that would be the following in OpenGL: glBindTexture(GL_TEXTURE_2D texture_id); glBegin(GL_TRIANGLE_STRIP); glTexCoord2f(begin_u begin_v); glVertex3f(x - (width / 2) y - (height / 2) 0); glTexCoord2f(end_u begin_v); glVertex3f(x + (width / 2) y - (height / 2) 0); glTexCoord2f(begin_u end_u); glVertex3f(x - (width / 2) y + (height / 2) 0); glTexCoord2f(end_u end_u); glVertex3f(x + (width / 2) y + (height / 2) 0); glEnd(); Now that I've given you a complete code sample will you please stop bothering me about your game? :P  These days if you are on Windows Direct2D is the way to go. I'm not always using windows. It should be a cross platform game in the end. :) hence my qualification.  In terms of actually drawing to the screen if this is what you are asking for in the questions SDL is relatively simple to use and quite powerful to boot. Also Lazy Foo's tutorials are very good in terms of documentation.  You haven't given enough information about the environment the planned game design and your current engine structure to suggest anything specific so here's the general advice: There is no one optimal solution. Get something drawing and then use that until it starts to break then iterate; it's a very game-design related problem and game design is predicated on iteration so bringing the iteration down to the engine level(rather than to fret and try to come up with a ""perfect solution"" out of the gate) ends up being the best way to explore this aspect of gameplay. Some examples of how much your solution may vary: If you have an anim predicated on just looping through the frames while the ""player entity"" is in some state(e.g. walk jump idle) then you keep a frame counter increment it with the game's timestep and draw frames according to the counter. If you have an anim that needs a lot of contextual information to display properly like turning towards different facings grabbing on to ledges etc. then you add additional state checks along with the frame counter and if there are enough forms of state you may want to formalize it as a finite state machine run every frame. You might also have some kind of procedural effect tied to entity state: rotation blink scaling shaders... If anims trigger gameplay events(like throwing a punch) you'll have to decide whether to put that information directly in the animation code or to introduce a data-driven mechanism for ""fire this event now spawn this object turn on this collision volume..."" Then there are details like origin points image size(size may change over time for some anims) variable playback speeds.... Like I said up front don't try for a perfect solution. It just depends on what the game needs. Well the game is a simple 2d platformer in which you have states with different animations (run jump idle etc). I am currently using openGL for this however my team knows a lot more about openGL then me. Since I was simply responsible for the animation I thought about a suitable algorithm but I think I will just try and iterate over it using a framecounter (as you said).  Your question is quite broad so here a broad response ;) Since Qt4.6 (one month old) there is an animation framework. It seems very interesting (no time yet to really code with it). As it's Qt it means nice API multiplateform good documentation and probably decent performances. http://qt.nokia.com/products/appdev/add-on-products/catalog/4/Utilities/qtanimationframework Of course maybe you don't want to depend on Qt. Wauw well im actually using QT so im gonna take a look at it. I think were using an older version though"
1047,A,Drawing Colors in a picturebox? In C# i have a picturebox. i would like to draw 4 colors. The default will be white red green blue. How do i draw these 4 colors stritched in this picbox? or should i have 4 picbox? in that case how do i set the rgb color? Your question is quite vague. Do you want to draw rectangles in each part? Draw pixels? What? You need to specify what it is you would specifically like to draw. You can't draw a red - that makes no sense. You can however draw a red rectangle at location (00) which is 100 pixels tall and 100 wide. I will answer what I can however. If you want to set the outline of a shape to a specific color you would create a Pen object. If you want to fill a shape with a color however then you would use a Brush object. Here's an example of how you would draw a rectangle filled with red and a rectangle outlined in green: private void pictureBox_Paint(object sender PaintEventArgs e) { Graphics graphics = e.Graphics; Brush brush = new SolidBrush(Color.Red); graphics.FillRectangle(brush new Rectangle(10 10 100 100)); Pen pen = new Pen(Color.Green); graphics.DrawRectangle(pen new Rectangle(5 5 100 100)); }  If you want to use non-predefined colors then you need to get a Color object from the static method Color.FromArgb(). int r = 100; int g = 200; int b = 50; Color c = Color.FromArgb(r g b); Brush brush = new SolidBrush(c); //... Best Regards Oliver Hanappi  Add a PictureBox to the form create an event handler for the paint event and make it look like this: private void PictureBox_Paint(object sender PaintEventArgs e) { int width = myPictureBox.ClientSize.Width / 2; int height = myPictureBox.ClientSize.Height / 2; Rectangle rect = new Rectangle(0 0 width height); e.Graphics.FillRectangle(Brushes.White rect); rect = new Rectangle(width 0 width height); e.Graphics.FillRectangle(Brushes.Red rect); rect = new Rectangle(0 height width height); e.Graphics.FillRectangle(Brushes.Green rect); rect = new Rectangle(width height width height); e.Graphics.FillRectangle(Brushes.Blue rect); } This will divide the surface into 4 rectangles and paint each of them in the colors White Red Green and Blue.
1048,A,"JPanel background image This is my code it indeed finds the image so that is not my concern my concern is how to make that image be the background of the panel. I'm trying to work with Graphics but i doesnt work any ideas?? please?? try { java.net.URL imgURL = MAINWINDOW.class.getResource(imagen); Image imgFondo = javax.imageio.ImageIO.read(imgURL); if (imgFondo != null) { Graphics grafica=null; grafica.drawImage(imgFondo 0 0 this); panel.paintComponents(grafica); } else { System.err.println(""Couldn't find file: "" + imagen); } } catch... If you are just going to draw the image at its original size then all you need to do is add the image to a JLabel and then use the label as a Container by setting its layout manager. The only time you need to do custom painting is if you want to scale or tile the background image or do some other fancy painting. See Background Panel for more information on both approaches. also check out the section from the Swing tutorial on Custom Painting. Thanks! the links are very usefull. I have another question I know how to put the image in the label but how do I make that label be under all the other components? In my application I'm creating both dynamic and static components. I don't understand the question. I gave you an example of how to do this in the ""Background Panel"" link. I gave you 4 lines of code that shows you how to make the label behave just like a JPanel so that you can use it as the content pane of the frame. If you need more help post your SSCCE: http://sscce.org  There is an error in your code here. You set your grafica to null the line before you dereference it. This will certainly throw a NullPointerException. Instead of declaring your own Graphics object you should use the one passed in to the method you will be using for painting. To do this in Swing you should implement the paintComponent method to paint your image something like this:  public void paintComponent(Graphics grafica) { grafica.drawImage(imgFondo 0 0 null); //no need for ImageObserver here } Note that you don't want to be doing long running tasks like reading in Image files from disk in the painting thread. The above example assumes that you have already loaded the imgFondo and have it stored such that it is accessible in the paintComponent method. Thanks so does this mean I have to override the method? or just implement it in my class? If you implement it you will be overriding the super's method."
1049,A,Java2d: Option to show a selection window when mouse is dragged I have a image in Java2d and I would like user to give an option to select any rectangle portion of the image by left clicking and dragging the mouse.(similar to mspaint). How is this actually done? Thank you. http://java.sun.com/docs/books/tutorial/uiswing/events/mousemotionlistener.html SelectionDemo should be particularly helpful.
1050,A,2d Game graphics/sprites - landscape vs portrait - 2 sets? I am working on a simple iPad game and wondering whether I should be using different graphics for portrait vs landscape or just doing some kind of transform between them? Thanks for any tips/advice Chris The best answer is that it really depends on what game you make. If you want the graphics to be different go for it. Otherwise it is typical to use one set of graphics and maybe just organize the on-screen GUI differently. The screen is really just a viewport into the game world and as its shape changes (from tall to wide) it should not affect the physics of the world; unless of course you WANT it to change the physics rules. It's really dependent on what game you are making. Thanks - I just have little figures that move/can be moved around - but when rotated they come become short squat figures :) Perhaps my rotation logic is not handling things correctly... Ah that explains your question then. Yes it is definitely the case that your logic is not quite right... I have not done iPad development myself but it sounds like you are rotating the texture but not the geometry (vertices triangles) in your code however it is represented.
1051,A,How do you calculate the axis-aligned bounding box of an ellipse? If the major axis of the ellipse is vertical or horizontal it's easy to calculate the bounding box but what about when the ellipse is rotated? The only way I can think of so far is to calculate all the points around the perimeter and find the max/min x and y values. It seems like there should be a simpler way. If there's a function (in the mathematical sense) that describes an ellipse at an arbitrary angle then I could use its derivative to find points where the slope is zero or undefined but I can't seem to find one. Edit: to clarify I need the axis-aligned bounding box i.e. it should not be rotated with the ellipse but stay aligned with the x axis so transforming the bounding box won't work. Brilian Johan Nilsson. I have transcribed your code to c# - ellipseAngle are now in degrees: private static RectangleF EllipseBoundingBox(int ellipseCenterX int ellipseCenterY int ellipseRadiusX int ellipseRadiusY double ellipseAngle) { double angle = ellipseAngle * Math.PI / 180; double a = ellipseRadiusX * Math.Cos(angle); double b = ellipseRadiusY * Math.Sin(angle); double c = ellipseRadiusX * Math.Sin(angle); double d = ellipseRadiusY * Math.Cos(angle); double width = Math.Sqrt(Math.Pow(a 2) + Math.Pow(b 2)) * 2; double height = Math.Sqrt(Math.Pow(c 2) + Math.Pow(d 2)) * 2; var x= ellipseCenterX - width * 0.5; var y= ellipseCenterY + height * 0.5; return new Rectangle((int)x (int)y (int)width (int)height); }  This is relative simple but a bit hard to explain since you haven't given us the way you represent your ellipse. There are so many ways to do it.. Anyway the general principle goes like this: You can't calculate the axis aligned boundary box directly. You can however calculate the extrema of the ellipse in x and y as points in 2D space. For this it's sufficient to take the equation x(t) = ellipse_equation(t) and y(t) = ellipse_equation(t). Get the first order derivate of it and solve it for it's root. Since we're dealing with ellipses that are based on trigonometry that's straight forward. You should end up with an equation that either gets the roots via atan acos or asin. Hint: To check your code try it with an unrotated ellipse: You should get roots at 0 Pi/2 Pi and 3*Pi/2. Do that for each axis (x and y). You will get at most four roots (less if your ellipse is degenerated e.g. one of the radii is zero). Evalulate the positions at the roots and you get all extreme points of the ellipse. Now you're almost there. Getting the boundary box of the ellipse is as simple as scanning these four points for xmin xmax ymin and ymax. Btw - if you have problems finding the equation of your ellipse: try to reduce it to the case that you have an axis aligned ellipse with a center two radii and a rotation angle around the center. If you do so the equations become:  // the ellipse unrotated: temp_x (t) = radius.x * cos(t); temp_y (t) = radius.y = sin(t); // the ellipse with rotation applied: x(t) = temp_x(t) * cos(angle) - temp_y(t) * sin(angle) + center.x; y(t) = temp_x(t) * sin(angle) + temp_y(t) * cos(angle) + center.y; Cheers Nils  I found a simple formula at http://www.iquilezles.org/www/articles/ellipses/ellipses.htm (and ignored the z axis). I implemented it roughly like this: num ux = ellipse.r1 * cos(ellipse.phi); num uy = ellipse.r1 * sin(ellipse.phi); num vx = ellipse.r2 * cos(ellipse.phi+PI/2); num vy = ellipse.r2 * sin(ellipse.phi+PI/2); num bbox_halfwidth = sqrt(ux*ux + vx*vx); num bbox_halfheight = sqrt(uy*uy + vy*vy); Point bbox_ul_corner = new Point(ellipse.center.x - bbox_halfwidth ellipse.center.y - bbox_halfheight); Point bbox_br_corner = new Point(ellipse.center.x + bbox_halfwidth ellipse.center.y + bbox_halfheight);  Here is the formula for the case if the ellipse is given by its foci and eccentricity (for the case where it is given by axis lengths center and angle see e. g. the answer by user1789690). Namely if the foci are (x0 y0) and (x1 y1) and the eccentricity is e then bbox_halfwidth = sqrt(k2*dx2 + (k2-1)*dy2)/2 bbox_halfheight = sqrt((k2-1)*dx2 + k2*dy2)/2 where dx = x1-x0 dy = y1-y0 dx2 = dx*dx dy2 = dy*dy k2 = 1.0/(e*e) I derived the formulas from the answer by user1789690 and Johan Nilsson.  This code is based on the code user1789690 contributed above but implemented in Delphi. I have tested this and as far as I can tell it works perfectly. I spent an entire day searching for an algorithm or some code tested some that didn't work and I was very happy to finally find the code above. I hope someone finds this useful. This code will calculate the bounding box of a rotated ellipse. The bounding box is axis aligned and NOT rotated with the ellipse. The radiuses are for the ellipse before it was rotated. type TSingleRect = record X: Single; Y: Single; Width: Single; Height: Single; end; function GetBoundingBoxForRotatedEllipse(EllipseCenterX EllipseCenterY EllipseRadiusX EllipseRadiusY EllipseAngle: Single): TSingleRect; var a: Single; b: Single; c: Single; d: Single; begin a := EllipseRadiusX * Cos(EllipseAngle); b := EllipseRadiusY * Sin(EllipseAngle); c := EllipseRadiusX * Sin(EllipseAngle); d := EllipseRadiusY * Cos(EllipseAngle); Result.Width := Hypot(a b) * 2; Result.Height := Hypot(c d) * 2; Result.X := EllipseCenterX - Result.Width * 0.5; Result.Y := EllipseCenterY - Result.Height * 0.5; end;  I think the most useful formula is this one. An ellipsis rotated from an angle phi from the origin has as equation: where (hk) is the center a and b the size of the major and minor axis and t varies from -pi to pi. From that you should be able to derive for which t dx/dt or dy/dt goes to 0. I feel so slow now took me ages to write my answer up T.T  You could try using the parametrized equations for an ellipse rotated at an arbitrary angle: x = h + a*cos(t)*cos(phi) - b*sin(t)*sin(phi) [1] y = k + b*sin(t)*cos(phi) + a*cos(t)*sin(phi) [2] ...where ellipse has centre (hk) semimajor axis a and semiminor axis b and is rotated through angle phi. You can then differentiate and solve for gradient = 0: 0 = dx/dt = -a*sin(t)*cos(phi) - b*cos(t)*sin(phi) => tan(t) = -b*tan(phi)/a [3] Which should give you many solutions for t (two of which you are interested in) plug that back into [1] to get your max and min x. Repeat for [2]: 0 = dy/dt = b*cos(t)*cos(phi) - a*sin(t)*sin(phi) => tan(t) = b*cot(phi)/a [4] Lets try an example: Consider an ellipse at (00) with a=2 b=1 rotated by PI/4: [1] => x = 2*cos(t)*cos(PI/4) - sin(t)*sin(PI/4) [3] => tan(t) = -tan(PI/4)/2 = -1/2 => t = -0.4636 + n*PI We are interested in t = -0.4636 and t = -3.6052 So we get: x = 2*cos(-0.4636)*cos(PI/4) - sin(-0.4636)*sin(PI/4) = 1.5811 and x = 2*cos(-3.6052)*cos(PI/4) - sin(-3.6052)*sin(PI/4) = -1.5811 Thanks. That works except you have a typo in equation two. The minus sign should be a plus. Fixed seems I followed through for the solution for tan(t) on [2] also so I fixed that too. Hopefully you spotted all my errors - it's all scribbled on the back of an envelope here .. ;) I think there's another error in the example: the first t value for x is -0.4636 shouldn't the second be -3.6052 (equals -0.4636 - pi)? @sh1ftst0rm You are correct. Bizarre mistake since I wrote down the right result. The calculation as written gives -0.9487 which is pretty wrong. Will correct. @Caleb: Exactly correct. There are only two unique solutions that will fit in a single whole 2*PI rotation (since we have the ...+n*PI term). By choosing any 2 consecutive values of `t` we ensure that both of the unique answers will result from plugging those values for `t` into the equations. You can confirm this by trying other values for `t`. One thing isn't quite clear to me about this answer: why are we only interested in t = -0.4636 and t = -3.6052? I see that these results come from substituting 0 and -1 for the variable 'n' in the equation t = -0.4636 + n*PI but don't know the significance of those particular values. Is it because the other t-values will yield duplicate Cartesian coordinates when plugged in to our original equations?
1052,A,"Plot ellipse from rectangle I have been looking all over the Web for a way to plot an ellipse from rectangle coordinates that is top-left corner (x y) and size (width and height). The only ones I can find everywhere are based on the Midpoint/Bresenham algorithm and I can't use that because when working with integer pixels I lose precisions because these algorithms use a center point and radials. The ellipse MUST be limited to the rectangle's coordinates so if I feed it a rectangle where the width and height are 4 (or any even number) I should get an ellipse that completely fits in a 4x4 rectangle and not one that will be 5x5 (like what those algorithms are giving me). Does anyone know of any way to accomplish this? Thanks! Just to be clear given a bounding box you want the largest ellipse bounded by that box? That is what I want. Can you not get the width and height (divided by 2) and center of the rectangle then plug that into any ellipse drawing routine as its major minor axis and center? I guess I'm not seeing the problem all the way here. No I can't because for even width and height the resulting ellipse's width and height will be incremented by one pixel from the original seeing as it plots the pixel at center + axis and center - axis. I need it to be perfectly within the bounds my ""pixels"" are 32x32 so this is very visible. So the problem is sub pixel accuracy and reflecting about the quadrants is causing an overwrite on some edges. Given that you should be able to store a non-integer width height (or at least a fixed point value with 1 bit for fraction) to handle this. Then the problem is finding or writing an ellipse generator that works with non integer widths and heights. That shouldn't be too difficult to find but I haven't looked so perhaps not?"
1053,A,"3D modeling for programmers I'm studying Computer Graphics as part of my curriculum at my university. The course focuses on scene modeling rather than rendering or other aspects of computer graphics. We're learning the math behind it and OpenSceneGraph to actually run something. As part of the HW and also out of sheer interest I need to create a 3D model and I have artistic freedom in this regard. I also have freedom to model it directly in code or load a model I do in a tool of my choosing. The problem is I'm not good in the visual art - I have lots of good ideas but no clue how to model them. I can't draw or sketch well either. But I want to be able to do CG. How would you suggest I approach 3D modeling? Thanks Asaf EDIT: Some people have down voted this (w/o leaving a comment). Let me emphasize - I'm a programmer and I want to get familiar with an art that is adjacent to ours. Make no mistake it is a programming relevant question. EDIT 2: Thanks for all who answered. I'll choose my accepted answer after I look at the alternatives you suggested. I apologize for the (expected) delay. Conclusion: I have decided to look into Blender. I'm looking into some of its video tutorials mentioned by Ruben Steins. Thanks Ruben. I did take a quick peek at MilkShape 3D and will use it if I see Blender is too much for my needs or my current learning ""budget"" (time attention). Thanks m3rLinEz. After I learn some basic skills I intend to follow Mastermind's advice. Thank you Mastermind. When I've done some 3D art and am ready to improve my skills I'm going to visit the places fa. had posted. Thank you too fa. Thanks to all those who took time to reply and all those who were open minded enough not to downvote a programming but not code related question. 3D modeling is a hard stuff and don't let it overwhelm you. Instead you should let someone more suitable do the modeling. There are tons of models for free. Search and use them before spending hours and days for a lousy dinosaur. And even though you model it you will need to implement scenes bones material etc. If you are more in to programming trying to making it move around obstacles will be much more fun.  Download Blender View the video tutorials or read this online tutorial Use the osgExport for Blender to get your models into OSG If you want a really cool model this process will take some time but you should be able to get a quicky model within a week or so. You're pretty modest. It took me a while to notice you created osgExport. Now I owe you for answering and for the tool - so double thanks to you! Actually that is a coincidence. It's an entirely different Ruben.  Art skills are not granted to everybody but you can learn a lot in those places : ConceptArt CGTalk CafeSalé ( in french )  you get sophisticated models (mostly humans) using ""Poser"" without ""any idea"" how it works. If you want to dive into the whole modeling I recommend ""Maya"".  I'd get one of the popular 3D modeling tools download some samples and start modifying them. Big names in the 3D modeling tool domain include 3D Studio Max Maya Blender Lightwave and Modo. Some of these can be pricey. Blender is free.  Unfortunately people are really voting down design-related questions on this site I do not know for what reason. So I vote you up. Design is a very important thing I can only feel sorry for those who wish to enclose themselves in code not leaving any space for style and beauty. I can absolutely recommend 3ds max to you. It has (or at least had a few years ago) very good tutorials shipped with it so you can learn how to move things around. General advice would be to visit pages related to 2D and 3D design but it would be too abstract. So I have a following suggestion. Think of what you would like to design. First - WHAT. Then try to do it. Along the path you will have specific questions on how to achieve a particular effect. You will ask them on relevant resources or dig the answers yourself. With time you will be getting skills knowledge and after at least a few months you will likely/hopefully develop a feeling on how to do these things what is good or what is not. Everything about art and design takes time. Much time. And much passion. Good luck!  Blender is a good (free) option but it is a bit over-the-top for what you are asking because it is a complete animation package not merely a modeller. In other words there's lot of other guff there to get in the way of learning how to model. By far the best 3D modeller IMO (for low-to-mid poly density ie computer games not film) is Silo (http://www.nevercenter.com/). I've worked with Maya 3dsmax Softimage XSI Blender Modo blah blah over the years and will always use Silo to model in regardless of the animation platform being used. Apart from being very easy to use it's incredibly cheap only $80 for a full licence... As far as an approach goes there are two big modelling techniques that you will want to investigate; box-modelling and edge-loop.  I found Wing3d is the easiest to use. BLender has the most functionality. And interesting one is Albatross 3D. It is simple too but Wing3d is easiest to use. +1 for wings3d - great free software  I recommend Google Sketchup and MilkShape 3D (commercial but at very low price for this kind of SW). I also has experience with Blender before which I think is the hardest to learn from the three SWs I suggest. Another concern you need to consider is the data format exported from the modeling software. Since you mentioned about doing the ""math"" yourself you must find a modeling tool that export a format where you can easily access the vertices data. Both Google Sketchup and Milkshape 3D come with great documentations (Blender too of course). Sketup even has video tutorials on their website and Milkshape 3D allow me to model a simple robot in very short time. But if you want more advanced modeling features than these tools might not suited you. Wow Sketchup looks like a programmers dream very cool but a little pricey for a hobby modeler. I currently use Milkshape myself and it is hands down one of the simplified modeling tools. I mean that in a good way you can can very basic work (mainly low poly stuff) done fast in milkshape. sketch up is pretty good but if you want to export it into another package or you want to give a mind to the TOPOLOGY do not use it Pricey? Sketchup is free and there is an OSG exporter available: https://github.com/rpavlik/sketchupToOSG  First of all you should look around at some different 3D programs if you haven't already. After you like the program you choose you're going to have to get used to it and try to understand what each hotkey does button materials etc. This its self can be a complicated and annoying aspect of it. A lot of people are going to recommend the program that is either; good industry standard or something they like. If this isn't professional I'd suggest looking at which one you like best it'll help you learn it much faster. After you've got your program of choice down you should just find tutorials videos tend to help because you can actually see everything they're doing. And after awhile with a bit of time and patenince you should start to be able to produce some decent looking models but great models take time. So don't get fustrated! It's kinda like programming in the way you probably didn't learn programming in one night with 3D stuff I'm still learning new things I didn't realise I could do just like programming. If you don't know how to do something with programming more than likely you'll google it(or even ask on stack overflow) right? So do the same with CG stuff. If you want to learn how to do water simulation google it!"
1054,A,"How to avoid tearing with pygame on Linux/X11 I've been playing with pygame (on Debian/Lenny). It seems to work nicely except for annoying tearing of blits (fullscreen or windowed mode). I'm using the default SDL X11 driver. Googling suggests that it's a known issue with SDL that X11 provides no vsync facility (even with a display created with FULLSCREEN|DOUBLEBUF|HWSURFACE flags) and I should use the ""dga"" driver instead. However running SDL_VIDEODRIVER=dga ./mygame.py throws in pygame initialisation with pygame.error: No available video device (despite xdpyinfo showing an XFree86-DGA extension present). So: what's the trick to getting tear-free vsynced flips ? Either by getting this dga thing working or some other mechanism ? I'm a little surprised by this as http://packages.debian.org/lenny/libxxf86dga1  which appears to provide the DGA stuff in X11 doesn't mention anything about kernel modules (what would it show up as in lsmod ?). For what it's worth I'm using the nv xorg driver with an old 5-series AGP NVidia card. Do you have the appropriate kernel driver for your video card. For X11 you need both a kernel driver and an X11 lib to access it. If one is missing the other will work but will be unusable. The best way to keep tearing to a minimum is to keep your frame rate as close to the screen's frequency as possible. The SDL library doesn't have a vsync unless you're running OpenGL through it so the only way is to approximate the frame rate yourself. The SDL hardware double buffer isn't guaranteed although nice when it works. I've seldomly seen it in action. In my experience with SDL you have to use OpenGL to completely eliminate tearing. It's a bit of an adjustment but drawing simple 2D textures isn't all that complicated and you get a few other added bonuses that you're able to implement like rotation scaling blending and so on. However if you still want to use the software rendering I'd recommend using dirty rectangle updating. It's also a bit difficult to get used to but it saves loads of processing which may make it easier to keep the updates up to pace and it avoids the whole screen being teared (unless you're scrolling the whole play area or something). As well as the time it takes to draw to the buffer is at a minimum which may avoid the blitting taking place while the screen is updating which is the cause of the tearing.  Well my eventual solution was to switch to Pyglet which seems to support OpenGL much better than Pygame and doesn't have any flicker problems. Pyglet is also a lot closer in terms of API and practice to other graphics libraries for other languages and modern technologies. [Gloss](http://www.tuxradar.com/gloss) is another option if you don't want to go all the way. It wraps OpenGL in easy classes and methods and plays nicely along Pygame."
1055,A,"Generate colors between red and green for a power meter? I'm writing a java game and I want to implement a power meter for how hard you are going to shoot something. I need to write a function that takes a int between 0 - 100 and based on how high that number is it will return a color between Green (0 on the power scale) and Red (100 on the power scale). Similar to how volume controls work: What operation do I need to do on the Red Green and Blue components of a color to generate the colors between Green and Red? So I could run say getColor(80) and it will return an orangish color (its values in R G B) or getColor(10) which will return a more Green/Yellow rgb value. I know I need to increase components of the R G B values for a new color but I don't know specifically what goes up or down as the colors shift from Green-Red. Progress: I ended up using HSV/HSB color space because I liked the gradiant better (no dark browns in the middle). The function I used was (in java): public Color getColor(double power) { double H = power * 0.4; // Hue (note 0.4 = Green see huge chart below) double S = 0.9; // Saturation double B = 0.9; // Brightness return Color.getHSBColor((float)H (float)S (float)B); } Where ""power"" is a number between 0.0 and 1.0. 0.0 will return a bright red 1.0 will return a bright green. Java Hue Chart: Thanks everyone for helping me with this! Should you not invert the power? Assuming red is the highest hit and you are working between 0.1 and 0.4 the higher the power the lower the H Are you using OpenGL? As there are ways of setting the points of a triangle to different colours and then blending/gradient between them. You probably get better performance asking the graphics card to do the work for you. Also the code could be simpler / more adaptable (say if you want an aliens power meter to go from green to blue) This should work - just linearly scale the red and green values. Assuming your max red/green/blue value is 255 and n is in range 0-100 R=(255*n)/100 G=(255*(100-n))/100; B=0 (Amended for integer maths tip of the hat to Ferrucio) Another way to do would be to use a HSV colour model and cycle the hue from 0 degrees (red) to 120 degrees (green) which whatever saturation and value suited you. This should give a more pleasing gradient. Here's a demonstration of each technique - top gradient uses RGB bottom uses HSV: If R G B and n are integer types make sure you re-write those expressions to do the multiplication first or you won't get smooth transitions. i.e. R=(255*n)/100 It's much much better to do this in HSV space. @DJClayworth yea I went with HSV. +1; I was going to ask a new question about how to improve a color algorithm i have for coloring a bar chart in HTML/PHP... SO suggested this question as similar and your answer helped me fix the issue without having to ask the question! Thanks!  I made a small function wich gives you the rgb integer value for a percentage value: private int getGreenToRedGradientByValue(int currentValue int maxValue) { int r = ( (255 * currentValue) / maxValue ); int g = ( 255 * (maxValue-currentValue) ) / maxValue; int b = 0; return ((r&0x0ff)<<16)|((g&0x0ff)<<8)|(b&0x0ff); } So if your currentValue is 50 and your maxValue is 100 this function will return the color that you need so if you loop this function with a percentage value your color value will go from green to red. Sorry for the bad explanation  Linearly interpolating between green and red almost should work except that in the middle there will be muddy brown color. The most flexible and best looking solution is to have an image file somewhere that has the exact color ramp you want. And then lookup the pixel value in there. This has the flexibility that you can tweak the gradient to be just right. If you still want to do it from code then it's probably best to interpolate between green and yellow colors in the left side and between yellow and red on the right side. In RGB space green is (R=0 G=255 B=0) yellow is (R=255 G=255 B=0) red is (R=255 G=0 B=0) - this is assuming each color component goes from 0 to 255.  I have previously asked the same (extremely similar) question here: http://stackoverflow.com/questions/168838/color-scaling-function  I'd say you want something in between a line segment in the HSV space (which has a slightly-too-bright yellow in the centre) and a line segment in the RGB space (which has an ugly brown in the centre). I would use this where power = 0 will give green power = 50 will give a slightly dull yellow and power = 100 will give red. blue = 0; green = 255 * sqrt( cos ( power * PI / 200 )); red = 255 * sqrt( sin ( power * PI / 200 ));  You need to linearly interpolate (LERP) the color components. Here's how it's done in general given a start value v0 an end value v1 and the required ratio (a normalized float between 0.0 and 1.0): v = v0 + ratio * (v1 - v0) This gives v0 when ratio is 0.0 v1 when ratio is 1.0 and everything between in the other cases. You can do this either on the RGB components or using some other color scheme like HSV or HLS. The latter two will be more visually pleasing since they work on hue and brightness compoments that map better to our color perception.  Short Copy'n'Paste answer... On Java Std: int getTrafficlightColor(double value){ return java.awt.Color.HSBtoRGB((float)value/3f 1f 1f); } On Android: int getTrafficlightColor(double value){ return android.graphics.Color.HSVToColor(new float[]{(float)value*120f1f1f}); } note: value is a number between 0 and 1 indicating the red-to-green condition.  Off the top of my head here is the green-red hue transition in HSV space translated to RGB: blue = 0.0 if 0<=power<0.5: #first green stays at 100% red raises to 100% green = 1.0 red = 2 * power if 0.5<=power<=1: #then red stays at 100% green decays red = 1.0 green = 1.0 - 2 * (power-0.5) The red green blue values in the above example are percentages you'd probably want to multiply them by 255 to get the most used 0-255 range. really awesome well done  import java.awt.Color; public class ColorUtils { public static Color interpolate(Color start Color end float p) { float[] startHSB = Color.RGBtoHSB(start.getRed() start.getGreen() start.getBlue() null); float[] endHSB = Color.RGBtoHSB(end.getRed() end.getGreen() end.getBlue() null); float brightness = (startHSB[2] + endHSB[2]) / 2; float saturation = (startHSB[1] + endHSB[1]) / 2; float hueMax = 0; float hueMin = 0; if (startHSB[0] > endHSB[0]) { hueMax = startHSB[0]; hueMin = endHSB[0]; } else { hueMin = startHSB[0]; hueMax = endHSB[0]; } float hue = ((hueMax - hueMin) * p) + hueMin; return Color.getHSBColor(hue saturation brightness); } }"
1056,A,"How do I use an animated .gif in my iPhone application? I have an animated gif file that I want to use in my iPhone application but the animation doesn't work. Does anybody know how to fix it? This can be achieved by this piece of code: NSArray * imageArray = [[NSArray alloc] initWithObjects:[UIImage imageNamed:@""1.png""] [UIImage imageNamed:@""2.png""] nil]; //this will be the frames of animation images in sequence. ringImage = [[UIImageView alloc]initWithFrame:CGRectMake(100200600600)]; ringImage.animationImages = imageArray; ringImage.animationDuration = 1.5;//this the animating speed which you can modify ringImage.contentMode = UIViewContentModeScaleAspectFill; [ringImage startAnimating];//this method actually does the work of animating your frames. I know its an old thread..but may be helpful for someone..:)  If you have a serie of images you want to animate you can easily do it with UIImageView: UIImage *blur5 = [UIImage imageNamed:@""blur5.png""]; UIImage *blur6 = [UIImage imageNamed:@""blur6.png""]; self.imageView.animationImages = [[NSArray alloc] initWithObjects:blur5 blur6 nil]; self.imageView.animationRepeatCount = 5; [self.imageView startAnimating]; I found this easier than trying to use UIWebView. This is correct solution to show animation of images. But this is not the correct answer to the question.  One other option is to decode the gif in your application and then ""frame serve"" it to a OpenGL object. This way is less likely to run out of memory for large gifs. Do you have an example on how to do that?  Excellent article on 'Animating gif and animating images' http://iphonedevelopertips.com/graphics/animated-gif-animated-images-iphone-style.html  UIWebView does not properly display all GIF content. You need to use a UIImageView but the iPhone OS does not support animated GIFS and only displays the first frame. So you need to extract all of the other frames first. Crude example code here: http://pliep.nl/blog/2009/04/iphone_developer_decoding_an_animated_gif_image_in_objc  You can use source at http://blog.stijnspijker.nl/2009/07/animated-and-transparent-gifs-for-iphone-made-easy/ It has a GIF decoder that you can use directly to get solution. I successfully used it. But it have some problems with transparency."
1057,A,"Antialiasing alternatives I've seen antialiasing on Windows using GDI+ Java and also that provided by Photoshop and Gimp. Are there any other libraries out there which provide antialiasing facility without depending on support from the host OS? I am using GDI calls to generate a drawing. I am trying to reduce the staircase effect when drawing text and slanting lines. The SmoothingMode property does help but I would like to see if any other libraries provide better smoothing/performance. You're really going to have to be more specific with what you want to do with the images that requires anti-aliasing... Antigrain Geometry provides anti-aliased graphics in software. AGG is beautiful indeed!  There are (often misnamed btw but that's a dead horse) many anti-aliasing approaches that can be used. Depending on what you know about the original signal and what the intended use is different things are most likely to give you the desired result. ""Support from the host OS"" is probably most sensible if the output is through the OS display facilities since they have the most information about what is being done to the image. I suppose that's a long way of asking what are you actually trying to do? Many graphics libraries will provide some form of antialiasing whether or not they'll be appropriate depends a lot on what you're trying to achieve.  As simon pointed out the term anti-aliasing is misused/abused quite regularly so it's always helpful to know exactly what you're trying to do. Since you mention GDI I'll assume you're talking about maintaining nice crisp edges when you resize them - so something like a character in a font looks clean and not pixelated when you resize it 2x or 3x it's original size. For these sorts of things I've used a technique in the past called alpha-tested magnification - you can read the whitepaper here: http://www.valvesoftware.com/publications/2007/SIGGRAPH2007_AlphaTestedMagnification.pdf When I implemented it I used more than one plane so I could get better edges on all types of objects but it covers that briefly towards the end. Of all the approaches (that I've used) to maintain quality when scaling vector images this was the easiest and highest quality. This also has the advantage of being easily implemented in hardware. From an existing API standpoint your best bet is to use either OpenGL or Direct3D - that being said it really only requires bilinear filtered and texture mapped to accomplish what it does so you could roll your own (I have in the past). If you are always dealing with rectangles and only need to do scaling it's pretty trivial and adding rotation doesn't add that much complexity. If you do roll your own make sure to pay particular attention to subpixel positioning (how you resolve pixel positions that do not fall on a full pixel as this is critical to the quality and sometimes overlooked. Hope that helps! +1 because it gave me an idea which might answer my own question http://stackoverflow.com/questions/1957901/antialiased-composition-by-coverage better"
1058,A,Overlapping and touching CoreGraphics rectangles have a .5px border My core graphics fills are acting strange when the get close together touching or overlapping. This issue is on the iPhone Simulator and iPhone OS 2.2. Here we have two labels and a custom view with two CGContextFillRect(): overlap problem When the blue and red are brought together they develop this irritating .5px merging line. I can't seem to get rid of this or even change its colour. This version has a 1px gap between the rectangles and it is of course filled with the background black: pixel gap I've tried disabling anti-aliasing but that doesn't fix the issue. - (void)drawRect:(CGRect)rect { CGContextRef context = UIGraphicsGetCurrentContext(); CGContextSetAllowsAntialiasing(context false); CGContextSetRGBFillColor(context 0.0 0.0 1.0 1.0); CGContextFillRect(context CGRectMake(20.0 35.0 + 40 + 20  100 40)); CGContextSetRGBFillColor(context 1.0 0.0 0.0 1.0); CGContextFillRect(context CGRectMake(20.0 + 100  35.0 + 40 + 20  100 40)); CGContextSetAllowsAntialiasing(context true); } Any ideas? There's no defect. It's an optical illusion. Your eye is integrating the R&B values close together and inferring the white overlap. Have a look at the graphics with pixie (part of Xcode) or xscope (a paid upgrade from iconfactory). http://www.freeimagehosting.net/uploads/th.e015daef50.png> Thanks! So it is a bug in the wetware. Will have to wait for version 2.0 :-D
1059,A,"How to draw text on picturebox? I googled for ""Drawing text on picturebox C#"" but I couldnt find anything useful.Then I googled for ""Drawing text on form C#"" and I found some codebut it doesnt work the way I want it to work.  private void DrawText() { Graphics grf = this.CreateGraphics(); try { grf.Clear(Color.White); using (Font myFont = new Font(""Arial"" 14)) { grf.DrawString(""Hello .NET Guide!"" myFont Brushes.Green new PointF(2 2)); } } finally { grf.Dispose(); } } When I call the functionthe background color of the form becomes white(it's black by default). My questions: 1:Will this work on a picturebox? 2:How to fix the problem? You could just create a Label object write on it and set it to whatever coordinate you want. The label's background color is not going to be the same as the one of the image I want to write the text to. You don't want that call to Clear() - that's why it's turning the background white and it will cover up your picture. You want to use the Paint event in the PictureBox. You get the graphics reference from e.Graphics and then use the DrawString() that you have in your sample. Here's a sample. Just add a picture box to your form and add an event handler for the Paint event: private void pictureBox1_Paint(object sender PaintEventArgs e) { using (Font myFont = new Font(""Arial"" 14)) { e.Graphics.DrawString(""Hello .NET Guide!"" myFont Brushes.Green new Point(2 2)); } } (Note that you won't see the text at design time - you'll have to run the program for it to paint). I don't understand how to fix the problem. Give me a few minutes and I'll post some sample code... +1; @John: just don't call Dispose of the e.Graphics object in the Paint event; you are only ""borrowing"" it. FredrikMork. I removed the using statement and the try statement aswell with the finally block but my background color still becomes white. @John - the background is turning white because you're clearing it to white - just skip the Clear() call. See my edit - I added sample code. This also work is the user doesn't want the functionality of a label and just wants to display text. ThanksI would be even more thankful if you show me a way to set a label on a picturebox with transparent color(The answer below your answer). @John - I've been down that road before. Windows doesn't work particularly well with true transparency. You get lots of flicker on the screen when it's resized. If you want to try it the code in Malfist's link should do the trick. @John: here's another link for true transparency might be a little more clear: http://www.c-sharpcorner.com/UploadFile/Nildo/NSA106032008213555PM/NSA1.aspx  You have to create your own control and extend label and set the background color as fully transparent. Like here"
1060,A,DrawString over a TextBox Possible Duplicate: Watermarked Textbox for Compact Framework Using Visual Studio 2008 SP1 the latest Compact framework and Windows Mobile 5. I need to use DrawString to put a string over a TextBox. But as soon as I draw the string the TextBox Control just over writes it. (I Know because I drew slightly off the edge of the control and my text is half visible (where is is off the control) and half gone (where it is on the control.) Is there anyway I can get the TextBox to not refresh so I can keep my text there? NOTE: I have looked into subclassing TextBox and just having it paint my text. However Paint events for the TextBox class are not catchable in the CompactFramework. If you know a way to be able to paint on the TextBox without the Paint events then I would love to subclass the TextBox class. --End of Question-- Just in case you are wondering why I need to do this here is what I am working on: I need to have a text box where a numeric value must be entered twice. I need some sort of clear clue that they have to enter the number again. I would like to have a slightly grayed out text appear over the text box telling the user to re-enter. I have tried using a label a hyperlink label and another text box but they obscure the text below (which has a default value that has to be partially visible). If anyone knows a different way cue for re-entry that would be great too! Vacano You can edit your original question. But if you *must* re-ask then delete the previous one first and don't pollute your title with references to it. See http://stackoverflow.com/questions/174796/watermarked-textbox-for-compact-framework You can solve this problem in a different fashion. It sounds like you want to silhouette their previous input so they must type it again. I don't know what strides the CF has made recently but if there is a RichTextBox then this method will work. If not you would have to write your own implementation starting with a base control. Set the text of the RichTextBox to the silhouette value but make the text color gray for all the characters. Capture the keypress events and as they press the correct key change the text color for that character pressed from gray to black and discard that key press and discard all other key presses. This solution won't work if you want to allow them to go off the reservation such as freeform text. Instead of discarding what they typed if they mistype or enter a different character you would not discard the keypress but blank out the current and remaining gray characters thus allowing them to type with no silhouette.  As I answered in the closed dupe of this: Where are you doing the DrawText? On the TextBox parent? If so then that would be expected behavior. Why not create a custom TextBox control that paints (by overriding OnPaint) the value the first time maybe in something like a light grey then the second time paints it again in Black?
1061,A,"How to draw a filled circle in Java? I have a JPanel with a Grid Layout. In the ""cells"" of the grid I can put different elements (for example JButtons). There is no problems with that. But now I want to put a filled circle in some of the cells. I also would like to relate an ActionListener with these circles. In more details if I click the circle it disappears from the current cell and appears in another one. How can I do it in Java? I am using Swing. /***Your Code***/ public void paintComponent(Graphics g){ /***Your Code***/ g.setColor(Color.RED); g.fillOval(50502020); } g.fillOval(x-axisy-axiswidthheight);  public void paintComponent(Graphics g) { super.paintComponent(g); Graphics2D g2d = (Graphics2D)g; // Assume x y and diameter are instance variables. Ellipse2D.Double circle = new Ellipse2D.Double(x y diameter diameter); g2d.fill(circle); ... } Here are some docs about paintComponent (link). You should override that method in your JPanel and do something similar to the code snippet above. In your ActionListener you should specify x y diameter and call repaint(). Heh thought it was a self-answer for a second there. @mmyers: yes it's a bit confusing))"
1062,A,How can I determine if a function generates a graph Is there a way to determine if a function generates a figure in R? For example if we have functions f and g f = function(xy){plot(xy)} g = function(xy){mean(x*y)} I would like able to run createFigure(f(xy))#Returns TRUE createFigure(g(xy))#Returns FALSE Thanks BTW Colin I am curious as to how such a function might be useful? Best Tal Students submit some functions as coursework. One of their functions should produce a graph. I then use Sweave to run their coursework and check for correctness. However if the function is incorrect and doesn't produce any graphics I can't compile the resulting tex file. IMHO this is the most underrated question on SO's R section. Thanks for asking and @hadley thanks for saving the day! =) makes_plot <- function(x) { before <- .Internal(getSnapshot()) force(x) after <- .Internal(getSnapshot()) !identical(before after) } makes_plot(mean(1:10)) makes_plot(plot(1:10)) The .getSnapshot function was discovered by looking at the source of recordPlot(). Nice! (fulfilling min(length(comment)) Very nicely done! +1 Maybe include that in ggplot2 so that it's readily available? Very nice Hadley... not that this also returns true for lines() points() grid() and axis() when they aren't used to actually make a plot only add to one. Perhaps it's true for all commands that modify plots.  If for your purposes it's OK to have all devices off before hand then checking .Devices would be fine because then plotting commands do make a new device. But then lines() and points() would be exceptions. In fact this suggests that the question doesn't just have a true or false answer but depends on conditions. Some functions will draw something even if there is no open device while others will draw something if there is something else drawn. What would you want to do in that case? I have a Sweave document that calls the function. If a graph is created I include it in the Tex file. The functions come from students.
1063,A,Good image gallery engines What are the best open source image gallery engines? Both stand-alone and for existing frameworks such as Wordpress or Drupal. Hopefully we can build a good list here over time. GOOD QUESTION lots of people ask this in many web forums so hopefully we will get some good responses to this and have a good list of solutions. Personally I always used to say something like Gallery or some other OS script but recently I have found myself using more and more something like a simple php script which just spits our a list of images (maybe 7 a page) but relying on a Javascript library such as mootools or Ext to provide all the functionality particularly for small or individual galleries. Im particularly loving the noobslide mootools class at the moment which has some lovely gallery effects. Noobslide I suppose at the end of the day its all down to what you need there will be no one answer that fits all but a number of different solutions will hopefully show up here that will suit different peoples needs.  Gallery is the classic choice. It has skins security layers heaps of plugins etc but can be run with the default settings easily if you want to. I've used it for years.
1064,A,Java2d: Translate the axes I am developing an application using Java2d. The weird thing I noticed is the origin is at the top left corner and positive x goes right and positive y increases down. Is there a way to move the origin bottom left? Thank you. Unfortunately this is the standard axis layout for most gui systems it comes from back when everything was text based so upper left made sense as the origin for text. Most GUI toolkits will let you flip the origin and axis around but my advice is to just get used to it :) Have you tried Graphics2D.translate()? Translate itself will not do the trick.  You're going to want to just get used to it. Like luke mentioned you technically CAN apply a transform to the graphics instance but that will end up affecting performance negatively. Just doing a translate could move the position of 00 to the bottom left but movement along the positive axes will still be right in the x direction and down in the y direction so the only thing you would accomplish is drawing everything offscreen. You'd need to do a rotate to accomplish what you're asking which would add the overhead of radian calculations to the transform matrix of the graphics instance. That is not a good tradeoff.  You are going to need to do a Scale and a translate. in your paintComponent method you could do this: public void paintComponent(Graphics g) { Graphics2D g2d = (Graphics2D) g; g2d.translate(0 -height); g2d.scale(1.0 -1.0); //draw your component with the new coordinates //you may want to reset the transforms at the end to prevent //other controls from making incorrect assumptions g2d.scale(1.0 -1.0); g2d.translate(0 height); } my Swing is a little rusty but this should accomplish the task. I do not like how you set the scale/translate back. It *should* be okay in this particular special case but in general you would accumulate floating point errors as you stack the transforms. Better to call getTransform and then setTransform to restore the original.  We can use the following way to resolve easily the problem public void paintComponent(Graphics g) { Graphics2D g2d = (Graphics2D) g; // Flip the sign of the coordinate system g2d.translate(0.0 getHeight()); g2d.scale(1.0 -1.0); ...... }
1065,A,"C# Bitblit from Bitmap to control (Compact Framework) I used once BitBlt to save a screenshot to an image file (.Net Compact Framework V3.5 Windows Mobile 2003 and later). Worked fine. Now I want to draw a bitmap to a form. I could use this.CreateGraphics().DrawImage(mybitmap 0 0) but I was wondering if it would work with BitBlt like before and just swap the params. So I wrote: [DllImport(""coredll.dll"")] public static extern int BitBlt(IntPtr hdcDest int nXDest int nYDest int nWidth int nHeight IntPtr hdcSrc int nXSrc int nYSrc uint dwRop); (and further down:) IntPtr hb = mybitmap.GetHbitmap(); BitBlt(this.Handle 0 0 mybitmap.Width mybitmap.Height hb 0 0 0x00CC0020); But the form stays plain white. Why is that? Where is the error I commited? Thanks for your opinions. Cheers David You mean something along these lines?  public void CopyFromScreen(int sourceX int sourceY int destinationX int destinationY Size blockRegionSize CopyPixelOperation copyPixelOperation) { IntPtr desktopHwnd = GetDesktopWindow(); if (desktopHwnd == IntPtr.Zero) { throw new System.ComponentModel.Win32Exception(); } IntPtr desktopDC = GetWindowDC(desktopHwnd); if (desktopDC == IntPtr.Zero) { throw new System.ComponentModel.Win32Exception(); } if (!BitBlt(hDC destinationX destinationY blockRegionSize.Width blockRegionSize.Height desktopDC sourceX sourceY copyPixelOperation)) { throw new System.ComponentModel.Win32Exception(); } ReleaseDC(desktopHwnd desktopDC); } FYI this is right out of the SDF. EDIT: It's not real clear in this snippet but hDC in the BitBlt is the HDC of the target bitmap (into which you wish to paint).  this.Handle is a Window handle not a device context. Replace this.Handle with this.CreateGraphics().GetHdc() Of course you'll need to destroy the graphics object etc... IntPtr hb = mybitmap.GetHbitmap(); using (Graphics gfx = this.CreateGraphics()) { BitBlt(gfx.GetHdc() 0 0 mybitmap.Width mybitmap.Height hb 0 0 0x00CC0020); } In addition hb is a Bitmap Handle not a device context so the above snippet still won't work. You'll need to create a device context from the bitmap:  using (Bitmap myBitmap = new Bitmap(""c:\test.bmp"")) { using (Graphics gfxBitmap = Graphics.FromImage(myBitmap)) { using (Graphics gfxForm = this.CreateGraphics()) { IntPtr hdcForm = gfxForm.GetHdc(); IntPtr hdcBitmap = gfxBitmap.GetHdc(); BitBlt(hdcForm 0 0 myBitmap.Width myBitmap.Height hdcBitmap 0 0 0x00CC0020); gfxForm.ReleaseHdc(hdcForm); gfxBitmap.ReleaseHdc(hdcBitmap); } } } One little comment :) Shouldn't you call ReleaseHdc() on the hdc's you got from the two graphics objects?  Are you sure that this.Handle refers to a valid device context? Have you tried checking the return value of the BitBlt function? Try the following: [DllImport(""coredll.dll"" EntryPoint=""CreateCompatibleDC"")] public static extern IntPtr CreateCompatibleDC(IntPtr hdc); [DllImport(""coredll.dll"" EntryPoint=""GetDC"")] public static extern IntPtr GetDC(IntPtr hwnd); IntPtr hdc = GetDC(this.Handle); IntPtr hdcComp = CreateCompatibleDC(hdc); BitBlt(hdcComp 0 0 mybitmap.Width mybitmap.Height hb 0 0 0x00CC0020);"
1066,A,XNA on graphics card How can I program graphics on a graphics card with XNA? (How do I move the workload onto the graphics card) Have you tried doing some pixel shaders through DirectX? NVIDIA has programming guides. You may also want to check out GP GPU. Sorry the NVIDIA programming guides are here: http://developer.nvidia.com/object/gpu_programming_guide.html  Explained in a very simplified way: All of the graphics are automatically drawn by the graphics card. There is no software rendererer mode in XNA. The way XNA is set up most geometry is transferred to the graphics card Vertex Buffer Objects. A fixed pipeline approximization or shaders are then applied to the data by the graphics card and displayed on screen. In short: If you want to do operations on your data while it's on the graphics card. Use vertex and pixel shaders. EDIT: You should also delve into the details of the graphics pipeline to understand better how graphic cards work.  http://www.riemers.net is a greate site for understanding XNA and graphics programming in general.
1067,A,"Can a JPEG compressed image be rotated without a loss in quality? JPEG is a lossy compression scheme so decompression-manipulation-recompression normally reduces the image quality further for each step. Is it possible to rotate a JPEG image without incurring further loss? From what little I know of the JPEG algorithm it naively seems possible to avoid further loss with a bit of effort. Which common image manipulation programs (e.g. GIMP Paint Shop Pro Windows Photo Gallery) and graphic libraries cause quality loss when performing a rotation and which don't? If you are talking of rotating a JPEG image then there is no further compression right? It is about rotating pixel locations. Doing rotation with any program will potentially change intermediate dimensions as it needs to preserver original image this may be an issue to consider.  Absolutely - just change the orientation value in the EXIF data. The vast majority of image programs will respect this setting and show the picture ""rotated"". It's also possibly to ""manually"" (e.g. programatically) rotate the image in a lossless fashion if certain criteria are true - rotation must be 90/180 degrees and the width/height must multiples of the block-size. You can also flip/mirror it. I don't know whether image programs are smart enough to special-case this operation though. I would guess not. You can ""manually"" do a 90-180-270 lossless transformation with jpeg. changing the Exif of the image won't do anything to the data itself and some image viewers don't treat it well. plus it's a bit harder to play with in case you wish to modify the image since you have to consider the orientation...  From the JPEG FAQ: ""There are a few specialized operations that can be done on a JPEG file without decompressing it and thus without incurring the generational loss that you'd normally get from loading and re-saving the image in a regular image editor. In particular it is possible to do 90-degree rotations and flips losslessly if the image dimensions are a multiple of the file's block size (typically 16x16 16x8 or 8x8 pixels for color JPEGs). ... But you do need special software; rotating the image in a regular image editor won't be lossless.""  According to the excellent article on Understanding Digital Image Interpolation by Sean McHugh: Interpolation also occurs each time you rotate or distort an image. (...) The 90° rotation is lossless because no pixel ever has to be repositioned onto the border between two pixels (and therefore divided). and eventually concludes with avoid rotating your photos when possible; if an unleveled photo requires it rotate no more than once.  There is a program named jpegtran jpegtran – a utility for lossless transcoding between different JPEG formats. And Here is a list of applications which provide the JPEG lossless rotation feature based on the IJG code  Not a jpg expert but it seems that the answer would be Yes for 90 180 270 degree rotations. (maybe even for 360! :)) 360 is doable but 720 takes more skill Hardly! Just do 360 twice. @Assaf what about 1080? 1080i or 1080p? I've got you all beat: I can rotate a JPEG 0 degrees using just my mind.  Unless you rotate by multiples of 90 degrees then your image will have to perform some kind of interpolation which might reduce the quality of your image. Using a good interpolation algorithm will help here. As for opening and recompressing I am not sure you would actually get worse quality but then I am not sure exactly how JPEG works. I suggest you try to compress manipulate and recompress and see for yourself if the result is good enough. What is good enough is subject to your application. Opening and recompression especially multiple times would definitely result in worse quality. This is a fundamental weakness of lossless compression. (Whether or not it is worse enough to care is definitely subjective as you suggest.)  If it can help : Trying to do better than the Microsoft (Windows 7) native picture viewer and its right-click rotation options I tried several apps of the following link : http://jpegclub.org/losslessapps.html For instance I tried FastStone Image Viewer XnView Photosurfer JPEG Lossless Rotator ExifPro Image Viewer. NONE of them yields a bigger picture than the basic Windows 7 picture viewer after a simple 90° rotation. It's admittedly limited to conclude so quickly but I still don't have found a real lossless rotation .jpeg app for the moment and in any case not better than the built-in Windows one.  Yes it is possible. A quick google search gave this list of programs which do this do you know of any library that does it?  Yes it is possible for certain cases: 90-degree rotations and flips on images with dimensions that are a multiple of 8. The heart of the JPEG algorithm -- the lossy part -- involves breaking the image into 8x8 pixel blocks performing a discrete cosine transform on the block and then quantizing the result. There's also some color space conversion and lossless compression of the blocks on top of this. Rotating or flipping an 8x8 block will give a DCT with the same basic coefficients but possibly transposed and/or with some sign changes depending on the transformation. So the basic steps to rotate or flip an image losslessly would involve: Decompress and extract the blocks Transpose and/or sign flip the DCT coefficients for each block Reshuffle the blocks into their new order (otherwise the 8x8 blocks would be rotated but still in the old place) Recompress it all with the lossless compression steps. Is there a java library that does this?"
1068,A,"What's the best way of dealing with AWT Graphics contexts? Some users of our Swing application have reported weird artefacts appearing on the display. This ranges from components not repainting themselves properly for a second or two right upto whole portions of the application being repainted like tiled wallpaper over areas of the window. The app has been worked on by developers of all levels between experienced Java guys to young chaps right out of university over the course of five years or so and as you'd expect some of the AWT code is an outright mess. I'm now faced with the task of trying to correct as much of the badness as I can over the next few months or so. Some of this is easy to deal with. Deal with components only on the event dispatch thread IO asynchronously kind of thing and I'm hopefully getting the message across to the rest of the team. What I'd like to know is the best way of dealing with Graphics contexts especially in a paintComponent() context. I see a lot of... public void paintComponent( Graphics g ) { super.paintComponent( g ); Graphics2D gfx = (Graphics2D)g; // ...Whole lotta drawing code... } Is it better practice to do this? public void paintComponent( Graphics g ) { super.paintComponent( g ); Graphics2D gfx = (Graphics2D)g.create(); // ...Whole lotta drawing code... gfx.dispose(); } If the g parameter is going to be reused in other paints then don't I need to restore it to a good state undo AffineTransforms etc.? According to Filthy Rich Clients you should not alter the Graphics object passed to you (which sucks as an API IMO). The correct way to handle it is slightly more verbose: public void paintComponent(Graphics g1) { super.paintComponent(g1); final Graphics2D g = (Graphics2D)g1.create(); try { // ...Whole lotta drawing code... } finally { g.dispose(); } } IIRC in the Sun implementation it doesn't matter if you don't dispose of ""sub-Graphics"" objects. (Don't quote me on that.) You might want to delegate that comment bit to another object.  I heard that this has been fixed in jdk-1.6.12 but didn't try it."
1069,A,"Speeding up text output on Windows for a console We have an application that has one or more text console windows that all essentially represent serial ports (text input and output character by character). These windows have turned into a major performance problem in the way they are currently code... we manage to spend a very significant chunk of time in them. The current code is structured by having the window living its own little life and the main application thread driving it across ""SendMessage()"" calls. This message-passing seems to be the cause of incredible overhead. Basically having a detour through the OS feels to be the wrong thing to do. Note that we do draw text lines as a whole where appropriate so that easy optimization is already done. I am not an expert in Windows coding so I need to ask the community if there is some other architecture to drive the display of text in a window than sending messages like this? It seems pretty heavyweight. Note that this is in C++ or plain C as the main application is a portable C/C++/some other languages program that also runs on Linux and Solaris. We did some more investigations seems that half of the overhead is preparing and sending each message using SendMessage and the other half is the actual screen drawing. The SendMessage is done between functions in the same file... So I guess all the advice given below is correct: Look for how much things are redrawn Draw things directly Chunk drawing operations in time to not send every character to the screen aiming for 10 to 20 Hz update rate of the serial console. Can you accept ALL answers? Are the output windows part of the same application? It almost sounds like they aren't... If they are you should look into the Observer design pattern to get away from SendMessage(). I've used it for the same type of use case and it worked beautifully for me. If you can't make a change like that perhaps you could buffer your output for something like 100ms so that you don't have so many out-going messages per second but it should also update at a comfortable rate.  I agree with Will Dean that the drawing in a console window or a text box is a performance bottleneck by itself. You first need to be sure that this isn't your problem. You say that you draw each line as a whole but even this could be a problem if the data throughput is too high. I recommend that you don't use the SendMessage to pass data from the main application to the text window. Instead use some other means of communication. Are these in the same process? If not you could use shared memory. Even a file in the disk could do in some circumstances. Have the main application write to this file and the text console read from it. You could send a SendMessage notification to the text console to inform it to update the view. But do not send the message whenever a new line arrives. Define a minimum interval between two subsequent updates.  Are the output windows part of the same application? It almost sounds like they aren't... Yes they are all in the same process. I did not write this code... but it seems like SendMessage is a bit heavy for this all in one application case. You describe these are 'text console windows' but then say you have multiple of them - are they actually Windows Consoles? Or are they something your application is drawing? Our app is drawing them they are not regular windows consoles. Note that we also need to get data back when a user types into the console as we quite often have interactive serial sessions. Think of it as very similar to what you would see in a serial terminal program -- but using an external application is obviously even more expensive than what we have now. If you can't make a change like that perhaps you could buffer your output for something like 100ms so that you don't have so many out-going messages per second but it should also update at a comfortable rate. Good point. Right now every single character output causes a message to be sent. And when we scroll the window up when a newline comes then we redraw it line-by-line. Note that we also have a scrollback buffer of arbitrary size but scrolling back is an interactive case with much lower performance requirements.  You should try profiling properly but in lieu of that I would stop worrying about the SendMessage which almost certainly not your problem and think about the redrawing of the window itself. You describe these are 'text console windows' but then say you have multiple of them - are they actually Windows Consoles? Or are they something your application is drawing? If the latter then I would be looking at measuring my paint code and whether I'm invalidating too much of a window on each update."
1070,A,"Getting R plots into LaTeX? I'm a newbie to both R and LaTeX and have just recently found how to plot a standard time series graph using R and save it as a png image. What I'm worried about is that saving it as an image and then embedding it into LaTeX is going to scale it and make it look ugly. Is there a way to make R's plot() function output a vector graphic and embed that into LaTeX? I'm a total beginner in both so please be gentle :) Code snippets are highly appreciated! Shane is spot-on you do want Sweave. Eventually. As a newbie you may better off separating task though. For that do this: open a device: pdf(""figures/myfile.pdf"" height=6 width=6) plot your R object: plot(1:10 type='l' main='boring') -- and remember that lattice and ggplot need an explicit print around plot. important: close your device: dev.off() to finalize the file. optional: inspect the pdf file in LaTeX use usepackage{graphics} in the document header use \includegraphics[width=0.98\textwidth]{figures/myfile} to include the figure created earlier and note that file extension is optional run this through pdflatex and enjoy I should also mention that separating the task as Dirk has done can be beneficial for very big documents: having to rebuild the entire document from scratch every time can be painful not to mention the agony of debugging a big Sweave document. There caching tricks and packages for that though. But yes the takeaway is that the power of Sweave comes at a bit of a cost in terms of complexity and opaqueness. To put it mildly :) Thanks! You might update your answer with a few things I ran into so it's easier on other newbies like me that might come by: you have put `\usepackage{graphicx}` in the document header for this to run (found this here: http://tuxmann.blogspot.com/2006/06/latex-snippets-add-image-wrap-long.html); and you're missing a 'd' on `with=0.98` Thanks for those corrections. Is there an alternative of `latex()` from `Hmisc` for plots? Ie something that prepares `\includegraphics` with an environment ""capture"" etc. So to avoid writing this code manually. Mixing in pdf did not work for me because I needed postscipt processing (pdf gave me a bounding box error). This may be a problem for someone else too. The solution turned out to be simple - get R to create .ps instead of .pdf: `postscript(file=myfile.pshorizontal=FALSE)`  I would recommend the tikzDevice package for producing output for inclusion in LaTeX documents: http://cran.r-project.org/web/packages/tikzDevice/index.html The tikzDevice converts graphics produced in R to code that can be interpreted by the LaTeX package tikz. TikZ provides a very nice vector drawing system for LaTeX. Some good examples of TikZ output are located at: http://www.texample.net/ The tikzDevice may be used like any other R graphics device: require( tikzDevice ) tikz( 'myPlot.tex' ) plot( 1 1 main = '\\LaTex\\ is $\\int e^{xy}$' ) dev.off() Note that the backslashes in LaTeX macros must be doubled as R interprets a single backslash as an escape character. To use the plot in a LaTeX document simply include it: \include{path/to/myPlot.tex} The pgfSweave package contains Sweave functionality that can handle the above step for you. Make sure that your document contains \usepackage{tikz} somewhere in the LaTeX preamble. http://cran.r-project.org/ The advantages of tikz() function as compared to pdf() are: The font of labels and captions in your figures always matches the font used in your LaTeX document. This provides a unified look to your document. You have all the power of the LaTeX typesetter available for creating mathematical annotation and can use arbitrary LaTeX code in your figure text. Disadvantages of the tikz() function are: It does not scale well to handle plots with lots of components. These are things such as persp() plots of large matricies. The shear number of graphic elements can cause LaTeX to slow to a crawl or run out of memory. The package is currently flagged as beta. This means that the interface or functionality of the package is subject to change if the authors find a compelling reason to do so. I should end this post by disclaiming that I am an author of both the tikzDevice and pgfSweave packages so my opinion may be biased. However I have used both packages to produce several academic reports in the last year and have been very satisfied with the results. +1 Excellent post and very cool project you've got! I'll keep the pdf answer as ""accepted"" because it's easier on newcomers but I'm trying out yours. Is there anything I need to specify on the LaTeX side besides the `include`? Got this error: `Environment tikzpicture undefined`. Sorry I should have made it more clear that you need to use the `tikz` package in your LaTeX document. Post edited. Great package! For me this was the first R package I installed and it took me a while to find out how to do this. For the next person: use the `install.packages()` command from within R - easy! @Sharpie I would be interested in how to get the `R CMD install tikzDevice` command to work (I struggled with this for a while 'with missing destination file operand' error) @Tom `R CMD INSTALL` is a little misleading if you have used other package managers---you have to point it at a file that you downloaded yourself. Hence the `missing destination file`. To have the package downloaded for you use `install.packages` from inside R. @Sharpie how does one go about centring the image in the resulting .tex file? It seems tikzDevice was [removed from the official CRAN repository](http://cran.r-project.org/web/packages/tikzDevice/index.html). I followed [this guide](http://lightonphiri.org/blog/r-graphical-representation-installing-tikzdevice-package) to install it.  You might want to consider using Sweave. There is a lot of great documentation available for this on the Sweave website (and elsewhere). It has very simple syntax: just put your R code between <<>>= and @. Here's a simple example that ends up looking like this: \documentclass[a4paper]{article} \title{Sweave Example 1} \author{Friedrich Leisch} \begin{document} \maketitle In this example we embed parts of the examples from the \texttt{kruskal.test} help page into a \LaTeX{} document: <<>>= data(airquality) library(ctest) kruskal.test(Ozone ~ Month data = airquality) @ which shows that the location parameter of the Ozone distribution varies significantly from month to month. Finally we include a boxplot of the data: \begin{center} <<fig=TRUEecho=FALSE>>= boxplot(Ozone ~ Month data = airquality) @ \end{center} \end{document} To build the document you can just call R CMD Sweave file.Rnw or run Sweave(file) from within R. I 2nd Sweave. I would only add that RStudio has wonderful integration with Sweave which might be particularly important for a beginner (install a LaTeX distribution and click one button in RStudio).  This is a dupe of a question on SO that I can't find. But: http://r-forge.r-project.org/projects/tikzdevice/ -- tikz output from r and http://www.rforge.net/pgfSweave/ tikz code via sweave. Using tikz will give you a look consistent with the rest of your document plus it will use latex to typeset all the text in your graphs. EDIT http://stackoverflow.com/questions/1395105/getting-latex-into-r-plots/1395553#1395553 That question is the opposite of this question: Here the OP is asking how to add R plots into a LaTeX document. There the OP was asking how to add LaTeX equations into an R plot. The title was an intentional paraphrase :) True enough but isn't the answer the same ;) Not quite it is all a matter of the final product you are attempting to create. LaTex into R makes plots with LaTex elements while R into Latex creates laTex documents including R plots."
1071,A,Rules of Thumb in GDI+ I have been working on some GDI+ code in .NET and have been learning my lessons the hard way. Simple things like: What looks good on screen may not look nice on paper and vice versa Caching too many objects can result in an OutOfMemoryException Floats aren't exact ...and so on. I'm sure there is a lot more that experienced folk can add to this. What are some good rules to follow when using GDI+ or any graphics library in general? One useful tip per post will be nice. Thanks. [Hard limits for XY values in GDI+](http://stackoverflow.com/questions/3468495) Create objects as late as possible (Don't prematurely optimize/cache) and release them as early as possible (calling Dispose or wrapping in using statement if IDisposable).  Be careful how you use regions: http://steveperks.com/post/Fun-With-GDI2b-Bugs.aspx  Don't draw more than you have to. In general drawing operations are more expensive than other calculations you'll be performing (especially in GDI+ which is a nice API but not the fastest drawing library ever). Spending a bit more time in your own code to avoid unnecessary drawing operations (eg. drawing the same thing more than once) is often well worth it.  Dont avoid using unmanaged calls it can speed things up a lot when done correctly.  GDI Gotchas that have burned me a few times. Clone doesn't clone() the underlying data clone(Rectangle PixelFormat) does. So if you dispose of a clone() the original object becomes unusable. Use new Bitmap() if you want two seperate bitmaps. If you load an image FromFile that file is locked until the bitmap is disposed of (can't even be read). When using DrawImage don't forget to set SmoothingMode InterpolationMode and PixelOffsetMode or you will be surprised by the low quality of the image.  Not strictly a GDI+ issue but remember this to save yourself a few hours of debugging: One mistake I've made too often using GDI+ in .NET is to call Matrix methods on the Transform property of the Graphics object. Example (in C++/CLI): g->Transform->Translate(100.0f 250.0f); // WRONG. Will not have any effect. The getter of the Transform property only returns a copy of the matrix. So any methods called on this copy will not affect the value of the graphics transform. To manipulate the graphics transform call one of the wrapper methods (MultiplyTransform TranslateTransform ScaleTransform etc.) as shown below. g->TranslateTransform(100.0f 250.0f);  Careful when you transform between logical/screen coordinates. If done at the wrong time you can run out of precision and get some nasty drawing artifacts in return. The last time this bit me I was trying to calculate a new point in logical coordinates; those were already near the limit of precision though so the new point was not quite what one would hope. Transforming sooner fixed it. It is possible to get a similar problem by transforming to screen coordinates too soon although an API which allows you to pass it floating point coordinates (which GDI+ does) tends to be much more robust against that.
1072,A,"Drawing Transparent Images I'm writing a CSS sprite engine in C# however I'm having a few issues. I create the master image set all the properties to that then iterate the sprites and draw those to the master image. However when I come to save the master image it only appears to be just the empty master image with transparent background and none of the sprites. I'm very confused at where I'm going wrong. The code I'm using is:  // Work out the width/height required int max_width = 0; int max_height = 0; foreach(SpriteInformation sprite in sprites) { if (max_width < (sprite.Left + greatest_width)) max_width = sprite.Left + greatest_width; if (max_height < (sprite.Top + greatest_height)) max_height = sprite.Top + greatest_height; } // Create new master bitmap Bitmap bitmap = new Bitmap(max_widthmax_heightPixelFormat.Format32bppArgb); Graphics graphics = Graphics.FromImage(bitmap); // Set background color SolidBrush brush; if (cbxBackground.Checked) { if (txtColor.Text == """") { brush = new SolidBrush(Color.Black); } else { brush = new SolidBrush(pnlColor.BackColor); } } else { if (txtColor.Text == """") { brush = new SolidBrush(Color.White); } else { brush = new SolidBrush(pnlColor.BackColor); } } //graphics.FillRectangle(brush00bitmap.Widthbitmap.Height); bitmap.MakeTransparent(brush.Color); graphics.Clear(brush.Color); // Copy images into place ImageAttributes attr = new ImageAttributes(); //attr.SetColorKey(brush.Colorbrush.Color); foreach(SpriteInformation sprite in sprites) { Rectangle source = new Rectangle(00sprite.Widthsprite.Height); Rectangle dest = new Rectangle(sprite.Leftsprite.Topsprite.Widthsprite.Height); graphics.DrawImage(sprite.Spritedest00sprite.Widthsprite.HeightGraphicsUnit.Pixelattr); } // Save image string format = ddlFormat.Items[ddlFormat.SelectedIndex].ToString(); if (format == ""PNG"") { dlgSave.Filter = ""PNG Images|*.png|All Files|*.*""; dlgSave.DefaultExt = ""png""; if (dlgSave.ShowDialog() == DialogResult.OK) { bitmap.Save(dlgSave.FileNameImageFormat.Png); } } else if (format == ""JPEG"") { } else { } As a warning IE6 doesn't support alpha transparency in pngs unless you use an 8 bit png.... Unfortunately System.Drawing won't let you output 8 bit pngs in an intelligent way. (It's not smart about picking the correct colors) I was aware of IE6's limitations bit annoying really. I saw some stuff on GIF color quantization I could probably apply to this too. You draw to ""graphics"" but then then you save ""bitmap""? I'm not sure if that graphics internally works with your original ""bitmap"" object... That does appear to be the implication in the docs: http://msdn.microsoft.com/en-us/library/system.drawing.graphics.fromimage.aspx  What are sprite.Left and Top? If they aren't 0 that may be a problem. I think you have dest and source the wrong way round? http://msdn.microsoft.com/en-us/library/ms142045.aspx Try a simpler DrawImage variant if you haven't already. e.g. DrawImage(sprite.Sprite00) The sprite.Left/Top gives the coords where to draw the image on the final image. I've tried with DrawImage(sprite.Sprite00) and DrawImage(sprite.Spritesprite.Leftsprite.Top) to no avail. I am certain the sprite.Sprite is assigned too because they're just loaded from another PNG and not touched. :/ I see now. When you are calculating the maximum size of the bitmap why do you use greatestwidth/height instead of the actual sprite size? Just grasping at straws here I'm afraid. I would try resetting all of those offsets to 0 and rendering onto a large image just to make sure it's rendering into the area of the target image you think it is. Then you can add the offsets back in. Yeh I'll try that next. The greatest width/height is basically the furthest offset+n required for a sprite so that I know how big the image is going to have to be. Nah that didn't work either! Merde. :( OK got it working by moving the MakeTransparent the issue is now the alpha of the source sprites does not seem to keep."
1073,A,"Textured spheres without strong distortion I've seen well-textured balls planets and other spherical objects in couple of games last time in UFO: aftermath. If you just splatter a texture into latitude/longditude as u and w -coordinates you'll get lots of ugly texture distortion to poles. I can think myself an one way to implement a spherical map with minimum distortion. By mapping in triangles instead of squares. But I don't know any algorithms. How to produce vertices and texture coordinates for such spheres? Also I don't see a way to generate a complete spherical map from a simple flat square map. Is there some intuitive way on drawing such maps without real trouble? Though is there other algorithms to render a sphere without or with minimal distortion? Both raytracing and rasterising algorithms are interesting. Do not split sphere by longitude and latitude. Instead use what is called the GeoSphere in 3Ds Max. (Actually a polyhedron the geodesic sphere.)  Lat/lon ""spheres"" distort by definition. If you're into programming start with a tetrahedron and subdivide as much as necessary by subdividing each triangle into 4 triangles and repositioning midpoints at the given radius from center. If you're into modeling the GeoSphere (as previously mentioned) or any similar will solve your problem. The point is that the triangles will have a constant amount of distortion which is independent of the latitude. Also note that a (closed) surface subdivision will enable you to have a (C)LOD rather easily. Texturing is another story but once you have a good sphere you have less problems. Hope this sparks your imagination :)  Drew Olbrich came up with a nifty way of tesselating a sphere to produce a nonahedron. If you increase the number of vertices you'll get a fairly decent spherical tesselation.  The hairy ball theorem states that it is impossible to define continuous texture coordinates on a sphere without any poles that distort the texture.  You can use a rectangular map with longitudinal blurring that increases near the poles (from none at the equator to say 60 degrees north). In conjunction with mip-maps this should eradicate some of the effects you are mentioning.  Or you can use a cubemap. That's generally the preferred way to do environment mapping which is basically what you are describing."
1074,A,"Drawing Nine Patch onto Canvas (Android) I'm trying to draw a nine patch onto a Canvas object on the Android. What seems strange is that although I generated my nine patch using the draw9patch tool the constructor for NinePatch requires an additional byte array called the ""chunk"" to construct the nine patch. Why isn't this simpler? What is the ""chunk""? And if you have done this yourself how did you go about it? Any help appreciated. You can easily do it this way: // Load the image as a NinePatch drawable NinePatchDrawable npd = (NinePatchDrawable)Resources.getDrawable(R.drawable.my_nine_patch); // Set its bound where you need Rect npdBounds = new Rect(...); npd.setBounds(npbBounds); // Finally draw on the canvas npd.draw(canvas); But I need the image to fit a specific size. Otherwise I wouldn't use a nine-patch. In the code I put in above the object 'd' is actually an instance of NinePatchDrawable since you gave getDrawable the ID of a 9patch resource. So you could do NinePatchDrawable npd = (NinePatchDrawable) d; Will that not work for your purposes? Just tried it: works perfectly. Thank you very much Casting it is not necessary but you need to call setBounds() on the Drawable before drawing it. Brilliant I could not find this anywhere in the docs. Thank you so much."
1075,A,".net Drawing.Graphics.FromImage() returns blank black image I'm trying to rescale uploaded jpeg in asp.net So I go: Image original = Image.FromStream(myPostedFile.InputStream); int w=original.Width h=original.Height; using(Graphics g = Graphics.FromImage(original)) { g.ScaleTransform(0.5f 0.5f); ... // e.g. using (Bitmap done = new Bitmap(w h g)) { done.Save( Server.MapPath(saveas) ImageFormat.Jpeg ); //saves blank black though with correct width and height } } this saves a virgin black jpeg whatever file i give it. Though if i take input image stream immediately into done bitmap it does recompress and save it fine like: Image original = Image.FromStream(myPostedFile.InputStream); using (Bitmap done = new Bitmap(original)) { done.Save( Server.MapPath(saveas) ImageFormat.Jpeg ); } Do i have to make some magic with g? upd: i tried: Image original = Image.FromStream(fstream); int w=original.Width h=original.Height; using(Bitmap b = new Bitmap(original)) //also tried new Bitmap(wh) using (Graphics g = Graphics.FromImage(b)) { g.DrawImage(original 0 0 w h); //also tried g.DrawImage(b 0 0 w h) using (Bitmap done = new Bitmap(w h g)) { done.Save( Server.MapPath(saveas) ImageFormat.Jpeg ); } } same story - pure black of correct dimensions Since you didn't fill the area with background of image you're reading from inputStreamyou can only get a blank image that way. Instead of using scaling the imageyou can use Fill background into a resized area. Check this out: Image img = Image.FromFile(Server.MapPath(""a.png"")); int w = img.Width; int h = img.Height; //Create an empty bitmap with scaled sizehere half Bitmap bmp = new Bitmap(w / 2 h / 2); //Create graphics object to draw Graphics g = Graphics.FromImage(bmp); //You can also use SmoothingModeCompositingMode and CompositingQuality //of Graphics object to preserve saving options for new image. //Create drawing area with a rectangle Rectangle drect = new Rectangle(0 0 bmp.Width bmp.Height); //Draw image into your rectangle area g.DrawImage(img drect); //Save your new image bmp.Save(Server.MapPath(""a2.jpg"") ImageFormat.Jpeg); Hope this helps Myra thanks this made sense. i did a bit differently but your code was good illustration Where did you **fill the area with background of image** exactly? In the line of g.DrawImage(img drect); this code does fill the background !  Try this: - get the Image from your stream - create a new Bitmap of the correct size - get the Graphics object from the new bitmap not the original one - call g.DrawImage(original 0 0 done.Width done.Height) Edit: The problem is this section: using (Bitmap done = new Bitmap(w h g)) { done.Save( Server.MapPath(saveas) ImageFormat.Jpeg ); } You're creating a black bitmap with the resolution specified by g. You're not actually creating a bitmap with any image data coming from g. In fact I don't think the Graphics object actually stores image data that you can really pass around it just allows you to manipulate some object that stores the image data. Try replacing that with b.Save(...) `b.Save(...)` does save image data but transforms do not apply. It can be easily rescaled by using `b=new Bitmap(originalnewWidthnewHeight)` BUT i actually need to make a series of transforms to original image including rescale crop and rotate so trying to do it by means of `Graphics` and render the final result into a bitmap. Am i misunderstanding the `Graphics` object? What is the right pattern to 1) take jpeg file 2) sequentially apply a number of transforms to it 3) save it ? Sounds simple... I think it's closer to say you're misunderstanding transforms. I really haven't dealt with transforms since college about 7 years ago so I'm kinda rusty. But I did a simple test with a rotation and it worked. The trick is that you're not rotating the image data you're rotating the graphics context and then using that rotated context to draw your image. So in my case the call to g.RotateTransform() needed to happen before the call to g.DrawImage(). How that's going to end up affecting you...hard to say. But cumulative effects of transforms can be funny and order of operations may matter. Order matters b/c transforms are fundamentally matrix operations and matrix multiplication is not commutative like scalar multiplication is. My use of transforms was in the context of OpenGL and I often found myself surprised by the correct order of operations and values used--sometimes it was the negative or inverse of what I expected it to be."
1076,A,Java Game Programming: JOGL vs LWJGL? I am currently writing a Turret Defense style game using the GTGE engine this engine has the ability to use either JOGL or LWJGL to drive the graphics and so I was wondering which one should I use? What are the pros/cons of each? What factors should I consider when deciding? I have used JOGL and my experience was preety good. Also my reason for Opting JOGL was Sun backing.If both suits to your need then its better to go with widely accepted Api and at that time it was JOGL.  In my game development I had the following decision points: 2D or 3D game? How many time do I want to spent on development? How many control I want to practice over my rendering/game? How well can I model my game using an engine? What are my performance expectations? What kind of licensing will I release my game? As I know JOGL seems to be more in focus of Sun and other developers. I would expect bugfixes and enhancements more frequently. If changing the driver does not require game/model changes you could just benchmark both drivers. Then you could compare them based on memory consumption rendering quality and speed. Changing the driver would not require game/model changes and so benchmarking is a great idea. Thanks.
1077,A,"Are border lines in the Flash drawing API drawn inside or outside a shape? When I draw rectangles etc in ActionScript using the Flash Player's drawing API (i. e. the Graphics class) is the border line of that shape drawn on the outside or the inside of the shape? I. e. which of the following diagrams correctly depicts a rectangle drawn a the border of a content area in a custom component? I looked at the documentation for the Graphics class and couldn't find any hints. I wrote a short test using a custom component with a fixed size drawing some lines as reference then drawing a rectangle with a 30-pixel wide border on a white background. This is how it looks like see below for the code: So referring to the picture in the question the second diagram (""centered"") correctly depicts the way Flash Player draws. Also note how the inner lines (at 45 pixels) are just inside the rectangle while the outer lines (at 15 pixels) align with the rectangle's outer limits. This is the code for the test application: <?xml version=""1.0""?> <mx:Application xmlns:mx=""http://www.adobe.com/2006/mxml"" xmlns:test=""*""> <test:TestCanvas horizontalCenter=""0"" verticalCenter=""0"" id=""canvas"" /> </mx:Application> And this is the TestCanvas component: public class TestCanvas extends UIComponent { public function TestCanvas() { super(); } override protected function measure():void { super.measure(); this.measuredWidth = this.minWidth = 300; this.measuredHeight = this.minHeight = 300; } override protected function updateDisplayList(w:Number h:Number):void { super.updateDisplayList(w h); this.graphics.clear(); this.graphics.lineStyle(undefined); this.graphics.beginFill(0xffffff); this.graphics.drawRect(0 0 w h); this.graphics.endFill(); this.graphics.lineStyle(0 0xff0000 0.5); this.graphics.moveTo(0 15); this.graphics.lineTo(300 15); this.graphics.moveTo(0 45); this.graphics.lineTo(300 45); this.graphics.moveTo(15 0); this.graphics.lineTo(15 300); this.graphics.moveTo(45 0); this.graphics.lineTo(45 300); this.graphics.lineStyle(0 0xff0000 0.75); this.graphics.moveTo(0 30); this.graphics.lineTo(300 30); this.graphics.moveTo(30 0); this.graphics.lineTo(30 300); this.graphics.lineStyle(30 0x0000ff 0.25 false ""normal"" null JointStyle.MITER); this.graphics.drawRect(30 30 240 240); }"
1078,A,"Reading monochrome bitmap pixel colors I don't know a better title but I'll describe the problem. A piece of hardware we use has the ability to display images. It can display a black and white image with a resolution of 64 x 256. The problem is the format of the image we have to send to the device. It is not a standard bitmap format but instead it is simply an array of bytes representing each pixel of the image. 0 = black 1 = white. So if we had an image with the size: 4 x 4 the byte array might look something like: 1000 0100 0010 0001 And the image would look like: The problem is that we need to create this image by creating a monochrome bitmap in C# and then convert it to the file format understood by the device. For example one might to display text on the device. In order to do so he would have to create a bitmap and write text to it: var bitmap = new Bitmap(256 64); using (var graphics = Graphics.FromImage(bitmap)) { graphics.DrawString(""Hello World"" new Font(""Courier"" 10 FontStyle.Regular) new SolidBrush(Color.White) 1 1); } There are 2 problems here: The generated bitmap isn't monochrome The generated bitmap has a different binary format So I need a way to: Generate a monochrome bitmap in .NET Read the individual pixel colors for each pixel in the bitmap I have found that you can set the pixel depth to 16 24 or 32 bits but haven't found monochrome and I have no idea how to read the pixel data. Suggestions are welcome. UPDATE: I cannot use Win32 PInvokes... has to be platform neutral! FOLLOW UP: The following code works for me now. (Just in case anybody needs it) private static byte[] GetLedBytes(Bitmap bitmap) { int threshold = 127; int index = 0; int dimensions = bitmap.Height * bitmap.Width; BitArray bits = new BitArray(dimensions); //Vertically for (int y = 0; y < bitmap.Height; y++) { //Horizontally for (int x = 0; x < bitmap.Width; x++) { Color c = bitmap.GetPixel(x y); int luminance = (int)(c.R * 0.3 + c.G * 0.59 + c.B * 0.11); bits[index] = (luminance > threshold); index++; } } byte[] data = new byte[dimensions / 8]; bits.CopyTo(data 0); return data; } You may try to supply a pixelformat in the constructor: var bitmap = new Bitmap(256 64 PixelFormat.Format1bppIndexed); When I did draw monochrome bitmaps on other platforms I sometimes had to disable antialiasing or the rendered text would not show up: graphics.SmoothingMode=SmoothingMode.None; YMMV. You can't call Graphics.FromImage on anything less than 32bpp  I'd compute the luminance of each pixel a then compare it to some threshold value. y=0.3*R+0.59G*G+0.11*B Say the threshold value is 127: const int threshold = 127; Bitmap bm = { some source bitmap }; byte[] buffer = new byte[64256]; for(int y=0;y<bm.Height;y++) { for(int x=0;x<bm.Width;x++) { Color c=source.GetPixel(xy); int luminance = (int)(c.R*0.3 + c.G*0.59+ c.B*0.11); buffer[xy] = (luminance > 127) ? 1 : 0; } } Point for that! But if you use Bitmap.LockBits instead of Bitmap.GetPixel you get superior performance! Thnx! With a few minor adjustments this code works like a charm :-) danbystrom: You're right wanted to keep the sample code as clean and pointer-less as possible.  Bitmap has a GetPixel method that you can use. This will let you draw on the Bitmap and later convert it to the format that you need. Bitmaps in Windows forms (ie accessed through Graphics.FromImage) are 24 bpp (maybe 32? It's too early and I honestly forget). Nonetheless GetPixel returns a Color object so the bit depth of the bitmap is immaterial. I suggest you write your code like this: MyBitmapFormat ToMyBitmap(Bitmap b) { MyBitmapFormat mine = new MyBitmapFormat(b.Width b.Height); for (int y=0; y < b.Height; y++) { for (int x=0; x < b.Width; x++) { mine.SetPixel(x y ColorIsBlackish(b.GetPixel(x y))); } } } bool ColorIsBlackish(Color c) { return Luminance(c) < 128; // 128 is midline } int Luminance(c) { return (int)(0.299 * Color.Red + 0.587 * Color.Green + 0.114 * Color.Blue); } This process is called simple thresholding. It's braindead but it will work as a first cut.  I don't know C#. There are possibly many ways to do it. Here is a simple way. Create a blank black bitmap image of size equal to your device requirement. Draw on it whatever you wish to draw like text figures etc. Now threshold the image i.e. set the pixel of image below an intensity value to zero else set it to. e.g. set all intensity values > 0 to 1. Now convert to the format required by your device. Create a byte array of the size (64 * 256)/8. Set the corresponding bits to 1 where the corresponding pixel values in earlier bitmap are 1 else reset them to 0. Edit: Step 3. Use bitwise operators to set the bits. The problem is I have no idea how to read pixel values of a bitmap Well you need to check to the APIs. There must be some method like getPixel() getIntensity() or maybe even a simple array reference will do.  You shouldn't use GetPixel method of your bitmap to convert entire bitmap from one format to another! This will be ineffective. Instead you should use LockBits method to get access to a copy of image buffer and convert it into desired format. I'm not completely sure about converting it to monochrome but there is Format1bppIndexed value in PixelFormat enumeration which may help you.  This chap has some code that creates a mono bitmap. The SaveImage sample is the one of interest.  thanks for the above code - I'm trying to convert a monochrome image into a 2d array where 1-black 0-white however I'm having some trouble - I used your code to load an 8x8 bmp image and am outputting its contents to a textbox by using myGrid =GetLedBytes(myBmp); for (int x = 1; x < 8; x++) { textBox1.Text = textBox1.Text + Convert.ToString(myGrid[x])+ "" ""; } however I get this as a result in the textbox: 225 231 231 231 231 129 255 how do I get it so it's 0's and 1's? If you use pixel.GetColor you can get the R G B values from a pixel right? So since you are doing black and white all you need to do is check R G an B == 0 which means black. All the other code is not needed."
1079,A,Glossy buttons in Flex 3 Any ideas how to create a button class that looks something like this: Taken from this tutorial page. I'm struggling to figure out what combination of filters I need to use to achieve the glossy effect. The rounded corners and bevel/glow effect are simple enough. But how can I add a gloss gradient over only the top half of the button? Found it. Degrafa has a Button Loader thing that does this exactly. Open source too.  Use this App --> http://jirox.net/AsButtonGen/ that's great but I need the source code. Is it available?
1080,A,"j2me - Rotate array of 2d points in 45 degree increments I'm working on a roguelike style cellphone game that operates on a grid. Attacks/Items in this game operate by targeting tiles in a pattern relative to the user. The paturn is usually relative to the direction the user is aiming in as well For instance in this screenshot the pattern Point2d[] damageTiles = new Point2d[4]; damageTiles[0] = new Point2d(0 -1); damageTiles[1] = new Point2d(0 -2); damageTiles[2] = new Point2d(1 -2); damageTiles[3] = new Point2d(-1 -2); is show relative to a temporary enemy(the yellow box) aiming ""up"". I currently have simple code to rotate a pattern array by 90 degree increments as seen here. My question is Is there an easy way to rotate an array of 2d points by 45 degree increments allowing my Attacks/Items to fire on the diagonal preferably without using floating point math as it tends to run slowly on many phones(or so i hear). This is probably a trivial question to anyone familiar with graphics programming but I've been hit with a case of coder's-block. My current rotation code is shown below. I now realize that a better way to do it would be to take an angle instead of a ""direction"" and rotate the points by that angle (rejecting angles that are not multiples of 45 of course). private Point2d[] rotateList(Point2d[] points int direction) { for (int i = 0; i < points.length; i++) { if (direction == ROTATE_LEFT) { int temp = points[i].x; points[i].x = points[i].y; points[i].y = -1 * temp; } else if (direction == ROTATE_RIGHT) { int temp = points[i].x; points[i].x = -1 * points[i].y; points[i].y = temp; } } return points; } This other question is similar (but not a duplicate) and might be helpful: http://stackoverflow.com/questions/484573/image-rotation-algorithm The T-shaped attack you show would be difficult to rotate at 45 degrees--things don't really translate from 90 to 45.  original: ..... .###. ..#.. ..o.. alternative A: ..... .##.. .##.. ...o. alternative B: ..#.. .#... #.#.. ...o. There could easily be others for that pattern. I recommend you create a 45 degree ""Pattern"" to match you 90 degree pattern then rotate the 45degree pattern exactly the same way you rotate your 90 degree patterns. I've just come to the same realization myself while trying out the rotation math in the other answers. I'll have to come up with a ""diagonal"" version of the patterns that need both thanks for the help.  sure rotation counterclockwise in 2D (with positive xy pointing to upper right) is just this (see rotation matrix in wikipedia) [x'] [cos(theta) -sin(theta)] [x] [y'] = [sin(theta) cos(theta)] [y] for theta = 45 degrees this becomes  x' = 0.7071*x - 0.7071*y y' = 0.7071*x + 0.7071*y (replace 0.7071 with sqrt(2)/2 I'm only using 4 digits here)  You can use a simple rotation matrix to rotate each point: http://www.siggraph.org/education/materials/HyperGraph/modeling/mod_tran/2drota.htm"
1081,A,I am having trouble with every Android graphics/SurfaceView tutorial I'm trying to learn how to make video games on Android and therefore I'm needing to get some decent tutorials going on how to make graphics on Anroid using the SurfaceView object. However every single graphics tutorial I've tried (mainly SurfaceView stuff) has failed. Please note that I don't want to use XML as it is out of my element and Google just wants to sell that technique on the advertisement of neatness which I can do programmatically. One major problem I've run into is that there are many tutorials both from Google and from third parties with code that uses the import android.opengl.GLSurfaceView command or either imports a subset GLSurfaceView and that never works on my IDE. Every time I try to import either one of those Eclipse wants to say that it basically doesn't recognize that package. The strange thing is that I CAN import android.opengl.* although that still causes some of the code in those packages to be referring to unrecognized types and/or methods. After trying to fool around with the first problem for a while I noticed that the Lunar Lander example didn't try to import either one of those two problem libraries. So I pulled the code and referenced resources for that into one of my infant projects leaving everything else in that project unused. Of course I did change which package the Lunar Lander code was in and changed the class name in LunarLander.java or whatever but that should not matter. I was able to get the thing to build in Eclipse. However when I went to run it it would do nothing but crash. Without showing any Lunar Lander graphics or anything the emulator would just give me this error message basically saying that my App has stopped working unexpectedly and makes me close the App. 1) What's the deal with the issues with the opengl.GLSurfaceView package? 2) What's the deal with the Lunar Lander example? 3) Where's a good firm tutorial on how to make video games for Android Thanks. It would help to post the stacktrace from the exception. You can get this via logcat. Are you building games in 2D or 3D? I can't speak for 3D but I've built some simple 2D games and SurfaceView was more a hindrance than a help. The emulator only simulates one core so you don't really get a performance benefit on the drawing. Furthermore the thread overhead and maintenance is a pain to deal with and prevents many capabilities (I spent a few hours just trying to launch a GameOver dialog from various configurations and the extra thread prevented this from happening). An alternative to SurfaceView would be to implement your own class that extends View and override onDraw(Canvas) to draw whatever you like (this is what me and my friends did). To answer your points: I've never worked in 3D or used Android's OpenGL but Eclipse (if that's what you're using) can be a bit temperamental with locating Android resources. Do the rest of your classes load? Lunar Lander shows you how to create and destroy a view extending SurfaceView and SurfaceHandler. Again there may be contexts for which SurfaceView is a great choice but if you're just starting with Android and building simple 2D games I don't think it's essential. Good tutorials are hard to find. I learned enough to make this 2D game from spending a few hours with the e-book version of Hello Android. Some buddies of mine and I are working on games for Android this semester we've also set up a blog that we hope to make a resource to others as the semester goes on. Sadly there's not much on it at the moment. Good luck! That is bad advice. SurfaceView is over 3 times faster than a regular View in my experience (on my hardware device drawing of a full screen SurfaceView is about 8 ms a regular View is 25 ms).
1082,A,"Blackberry - ListField with images from filesystem I use the following code to retrieve image from the phone or SDCard and I use that image in to my ListField. It gives the output but it takes very Long time to produce the screen. How to solve this problem ?? Can any one help me?? Thanks in advance!!! String text = fileholder.getFileName(); try{ String path=""file:///""+fileholder.getPath()+text; //path=”file:///SDCard/BlackBerry/pictures/image.bmp” InputStream inputStream = null; //Get File Connection FileConnection fileConnection = (FileConnection) Connector.open(path); inputStream = fileConnection.openInputStream(); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int j = 0; while((j=inputStream.read()) != -1) { baos.write(j); } byte data[] = baos.toByteArray(); inputStream.close(); fileConnection.close(); //Encode and Resize image EncodedImage eImage = EncodedImage.createEncodedImage(data0data.length); int scaleFactorX = Fixed32.div(Fixed32.toFP(eImage.getWidth()) Fixed32.toFP(180)); int scaleFactorY = Fixed32.div(Fixed32.toFP(eImage.getHeight()) Fixed32.toFP(180)); eImage=eImage.scaleImage32(scaleFactorX scaleFactorY); Bitmap bitmapImage = eImage.getBitmap(); graphics.drawBitmap(0 y+1 40 40bitmapImage 0 0); graphics.drawText(text 25 y0width); } catch(Exception e){} You should read files once (on App start or before screen open maybe put a progress dialog there) put images in array and use this array in paint."
1083,A,"Best web technology for building dynamic charts I need to build a custom designed bar chart that displays some simple data. Below are my requirements. Can anyone suggest the best web technology for my requirements. high browser compatibility ability to draw shapes ability to fill shapes with gradients ability to have onclick and onmouseover events for the different shapes (bars in the chart). Thanks guys. I was thinking of using svg but looking for suggestions. How about Raphaël - it's SVG/VML. It says: Browser compatibility: Raphaël currently supports Firefox 3.0+ Safari 3.0+ Opera 9.5+ and Internet Explorer 6.0+. Ability to draw shapes circle rect ellipse image text path Ability to fill shapes with gradients yes Ability to have onclick and onmouseover events yes: ... every graphical object you create is also a DOM object so you can attach JavaScript event handlers or modify them later. Everything in the reference On top of that there's a plugin called gRaphael which makes the creation of charts easier.  I recommend using Adobe Flex. Below is an example of how easy pie chart creation can be in Flex:  <mx:Panel title=""Pie Chart""> <mx:PieChart id=""myChart"" dataProvider=""{expenses}"" showDataTips=""true"" > <mx:series> <mx:PieSeries field=""Amount"" nameField=""Expense"" labelPosition=""callout"" /> </mx:series> </mx:PieChart> <mx:Legend dataProvider=""{myChart}""/> Based on your criteria: high browser compatibility: Flex is used on more than 95% of all browsers and behaves the same in all browsers. No more need to check if your web app is running in ie firefox chrome etc... because any browser that has a flash player is compatible. ability to draw shapes: Flash's greatest strength is the ability to draw. Charts are completely customizable and skinnable to achieve the look you need. Ability to fill shapes with gradients - done easily by setting style attributes or a custom skin. ability to have onclick and onmouseover events for the different shapes - see this link for some easy ways to create user interactions with charts.  I would like to share jquery.jqplot.js. It has lots of jQuery options but depends on other plugins such as syntaxhighliter etc.  Hi i hope this link may help you i found it while searching for a solution similar to what you're looking for: http://www.artetics.com/Articles/using-various-javascript-libraries-to-create-pie-chart i'm trying gRaphael i'm having difficulties on finding documentation though. you have to read the code and use the exploded instead of the min.js  Simple data - Google Charts API or Google Visualization API may suit you. Details for all features of image charts can be found on the Chart feature list You may also take a look at the comparison of the Charts API and the Visualization API.  Another candidate of course is JQuery SVG - if you're already familiar with jquery you may prefer this one. There's a comparison of JQuery SVG and Raphaël on SO: http://stackoverflow.com/questions/588718/jquery-svg-vs-raphael"
1084,A,"Millions of 3D points: How to find the 10 of them closest to a given point? A point in 3-d is defined by (xyz). Distance d between any two points (XYZ) and (xyz) is d= Sqrt[(X-x)^2 + (Y-y)^2 + (Z-z)^2]. Now there are a million entries in a file each entry is some point in space in no specific order. Given any point (abc) find the nearest 10 points to it. How would you store the million points and how would you retrieve those 10 points from that data structure. Do you create and fill the data structure before or after you're told what the point (abc) is? David's answer for example doesn't work if you create the data structure first and then a user types in the (abc) and wants an answer instantly. Good point (no pun intended!) Of course if (abc) is not known in advance it's more a problem of optimizing the existing list of points for searching by 3D location rather than actually doing the search. It really should be clarified if the cost of preparing the data structure and storing the million points in that data structure needs to be taken into account or only retrieval performance counts. If that cost does not matter then regardless of how many times you will retrieve the points kd-tree will win. If that cost does matter then you should also specify how many times you expect to run the search (for small number of searches brute force will win for larger kd will win). I think this is a tricky question that tests if you don't try to overdo things. Consider the simplest algorithm people already have given above: keep a table of ten best-so-far candidates and go through all the points one by one. If you find a closer point than any of the ten best-so-far replace it. What's the complexity? Well we have to look at each point from the file once calculate it's distance (or square of the distance actually) and compare with the 10th closest point. If it's better insert it in the appropriate place in the 10-best-so-far table. So what's the complexity? We look at each point once so it's n computations of the distance and n comparisons. If the point is better we need to insert it in the right position this requires some more comparisons but it's a constant factor since the table of best candidates is of a constant size 10. We end up with an algorithm that runs in linear time O(n) in the number of points. But now consider what is the lower bound on such an algorithm? If there is no order in the input data we have to look at each point to see if it's not one of the closest ones. So as far as I can see the lower bound is Omega(n) and thus the above algorithm is optimal. Excellent point! Since you have to read the file one by one in order to build any data structure your lowest *possible* is O(n) just as you say. Only if the question asks about finding the closest 10 points repeatedly does anything else matter! And you explained it well I think.  No need to calculate the distance. Just the square of the distance should serve your needs. Should be faster I think. In other words you can skip the sqrt bit.  This question is essentially testing your knowledge and/or intuition of space partitioning algorithms. I'd say that storing the data in an octree is your best bet. It's commonly used in 3d engines that handle just this kind of problem (storing millions of vertices ray tracing finding collisions etc.). The lookup time will be on the order of log(n) in the worst case scenario (I believe).  This question needs further definition. 1) The decision regarding the algorithms that pre-index data varies very much depending if you can hold the whole data in the memory or not. With kdtrees and octrees you will not have to hold the data in memory and the performance benefits from that fact not only because the memory footprint is lower but simply because you will not have to read the whole file. With bruteforce you will have to read the whole file and recompute distances for each new point you will be searching for. Still this might not be important to you. 2) Another factor is how many times will you have to search for a point. As stated by J.F. Sebastian sometimes bruteforce is faster even on large data sets but take care to take into account that his benchmarks measure reading the whole data set from disk (which is not necessary once kd-tree or octree is built and written somewhere) and that they measure only one search.  For any two points P1 (x1 y1 z1) and P2 (x2 y2 z2) if the distance between the points is d then all of the following must be true: |x1 - x2| <= d |y1 - y2| <= d |z1 - z2| <= d Hold the 10 closest as you iterate over your entire set but also hold the distance to the 10th closest. Save yourself a lot of complexity by using these three conditions before calculating the distance for every point you look at.  Straightforward algorithm: Store the points as a list of tuples and scan over the points computing the distance and keeping a 'closest' list. More creative: Group points into regions (such as the cube described by ""000"" to ""505050"" or ""000"" to ""-20-20-20"") so you can ""index"" into them from the target point. Check which cube the target lies in and only search through the points in that cube. If there are less than 10 points in that cube check the ""neighboring"" cubes and so on. On further thought this isn't a very good algorithm: if your target point is closer to the wall of a cube than 10 points then you'll have to search into the neighboring cube as well. I'd go with the kd-tree approach and find the closest then remove (or mark) that closest node and re-search for the new closest node. Rinse and repeat.  Million points is a small number. The most straightforward approach works here (code based on KDTree is slower (for querying only one point)). Brute-force approach (time ~1 second) #!/usr/bin/env python import numpy NDIM = 3 # number of dimensions # read points into array a = numpy.fromfile('million_3D_points.txt' sep=' ') a.shape = a.size / NDIM NDIM point = numpy.random.uniform(0 100 NDIM) # choose random point print 'point:' point d = ((a-point)**2).sum(axis=1) # compute distances ndx = d.argsort() # indirect sort # print 10 nearest points to the chosen one import pprint pprint.pprint(zip(a[ndx[:10]] d[ndx[:10]])) Run it: $ time python nearest.py point: [ 69.06310224 2.23409409 50.41979143] [(array([ 69. 2. 50.]) 0.23500677815852947) (array([ 69. 2. 51.]) 0.39542392750839772) (array([ 69. 3. 50.]) 0.76681859086988302) (array([ 69. 3. 50.]) 0.76681859086988302) (array([ 69. 3. 51.]) 0.9272357402197513) (array([ 70. 2. 50.]) 1.1088022980015722) (array([ 70. 2. 51.]) 1.2692194473514404) (array([ 70. 2. 51.]) 1.2692194473514404) (array([ 70. 3. 51.]) 1.801031260062794) (array([ 69. 1. 51.]) 1.8636121147970444)] real 0m1.122s user 0m1.010s sys 0m0.120s Here's the script that generates million 3D points: #!/usr/bin/env python import random for _ in xrange(10**6): print ' '.join(str(random.randrange(100)) for _ in range(3)) Output: $ head million_3D_points.txt 18 56 26 19 35 74 47 43 71 82 63 28 43 82 0 34 40 16 75 85 69 88 58 3 0 63 90 81 78 98 You could use that code to test more complex data structures and algorithms (for example whether they actually consume less memory or faster then the above simplest approach). It is worth noting that at the moment it is the only answer that contains working code. Solution based on KDTree (time ~1.4 seconds) #!/usr/bin/env python import numpy NDIM = 3 # number of dimensions # read points into array a = numpy.fromfile('million_3D_points.txt' sep=' ') a.shape = a.size / NDIM NDIM point = [ 69.06310224 2.23409409 50.41979143] # use the same point as above print 'point:' point from scipy.spatial import KDTree # find 10 nearest points tree = KDTree(a leafsize=a.shape[0]+1) distances ndx = tree.query([point] k=10) # print 10 nearest points to the chosen one print a[ndx] Run it: $ time python nearest_kdtree.py point: [69.063102240000006 2.2340940900000001 50.419791429999997] [[[ 69. 2. 50.] [ 69. 2. 51.] [ 69. 3. 50.] [ 69. 3. 50.] [ 69. 3. 51.] [ 70. 2. 50.] [ 70. 2. 51.] [ 70. 2. 51.] [ 70. 3. 51.] [ 69. 1. 51.]]] real 0m1.359s user 0m1.280s sys 0m0.080s Partial sort in C++ (time ~1.1 seconds) // $ g++ nearest.cc && (time ./a.out < million_3D_points.txt ) #include <algorithm> #include <iostream> #include <vector> #include <boost/lambda/lambda.hpp> // _1 #include <boost/lambda/bind.hpp> // bind() #include <boost/tuple/tuple_io.hpp> namespace { typedef double coord_t; typedef boost::tuple<coord_tcoord_tcoord_t> point_t; coord_t distance_sq(const point_t& a const point_t& b) { // or boost::geometry::distance coord_t x = a.get<0>() - b.get<0>(); coord_t y = a.get<1>() - b.get<1>(); coord_t z = a.get<2>() - b.get<2>(); return x*x + y*y + z*z; } } int main() { using namespace std; using namespace boost::lambda; // _1 _2 bind() // read array from stdin vector<point_t> points; cin.exceptions(ios::badbit); // throw exception on bad input while(cin) { coord_t xyz; cin >> x >> y >> z; points.push_back(boost::make_tuple(xyz)); } // use point value from previous examples point_t point(69.06310224 2.23409409 50.41979143); cout << ""point: "" << point << endl; // 1.14s // find 10 nearest points using partial_sort() // Complexity: O(N)*log(m) comparisons (O(N)*log(N) worst case for the implementation) const size_t m = 10; partial_sort(points.begin() points.begin() + m points.end() bind(less<coord_t>() // compare by distance to the point bind(distance_sq _1 point) bind(distance_sq _2 point))); for_each(points.begin() points.begin() + m cout << _1 << ""\n""); // 1.16s } Run it: g++ -O3 nearest.cc && (time ./a.out < million_3D_points.txt ) point: (69.0631 2.23409 50.4198) (69 2 50) (69 2 51) (69 3 50) (69 3 50) (69 3 51) (70 2 50) (70 2 51) (70 2 51) (70 3 51) (69 1 51) real 0m1.152s user 0m1.140s sys 0m0.010s Priority Queue in C++ (time ~1.2 seconds) #include <algorithm> // make_heap #include <functional> // binary_function<> #include <iostream> #include <boost/range.hpp> // boost::begin() boost::end() #include <boost/tr1/tuple.hpp> // get<> tuple<> cout << namespace { typedef double coord_t; typedef std::tr1::tuple<coord_tcoord_tcoord_t> point_t; // calculate distance (squared) between points `a` & `b` coord_t distance_sq(const point_t& a const point_t& b) { // boost::geometry::distance() squared using std::tr1::get; coord_t x = get<0>(a) - get<0>(b); coord_t y = get<1>(a) - get<1>(b); coord_t z = get<2>(a) - get<2>(b); return x*x + y*y + z*z; } // read from input stream `in` to the point `point_out` std::istream& getpoint(std::istream& in point_t& point_out) { using std::tr1::get; return (in >> get<0>(point_out) >> get<1>(point_out) >> get<2>(point_out)); } // Adaptable binary predicate that defines whether the first // argument is nearer than the second one to given reference point template<class T> class less_distance : public std::binary_function<T T bool> { const T& point; public: less_distance(const T& reference_point) : point(reference_point) {} bool operator () (const T& a const T& b) const { return distance_sq(a point) < distance_sq(b point); } }; } int main() { using namespace std; // use point value from previous examples point_t point(69.06310224 2.23409409 50.41979143); cout << ""point: "" << point << endl; const size_t nneighbours = 10; // number of nearest neighbours to find point_t points[nneighbours+1]; // populate `points` for (size_t i = 0; getpoint(cin points[i]) && i < nneighbours; ++i) ; less_distance<point_t> less_distance_point(point); make_heap (boost::begin(points) boost::end(points) less_distance_point); // Complexity: O(N*log(m)) while(getpoint(cin points[nneighbours])) { // add points[-1] to the heap; O(log(m)) push_heap(boost::begin(points) boost::end(points) less_distance_point); // remove (move to last position) the most distant from the // `point` point; O(log(m)) pop_heap (boost::begin(points) boost::end(points) less_distance_point); } // print results push_heap (boost::begin(points) boost::end(points) less_distance_point); // O(m*log(m)) sort_heap (boost::begin(points) boost::end(points) less_distance_point); for (size_t i = 0; i < nneighbours; ++i) { cout << points[i] << ' ' << distance_sq(points[i] point) << '\n'; } } Run it: $ g++ -O3 nearest.cc && (time ./a.out < million_3D_points.txt ) point: (69.0631 2.23409 50.4198) (69 2 50) 0.235007 (69 2 51) 0.395424 (69 3 50) 0.766819 (69 3 50) 0.766819 (69 3 51) 0.927236 (70 2 50) 1.1088 (70 2 51) 1.26922 (70 2 51) 1.26922 (70 3 51) 1.80103 (69 1 51) 1.86361 real 0m1.174s user 0m1.180s sys 0m0.000s Linear Search -based approach (time ~1.15 seconds) // $ g++ -O3 nearest.cc && (time ./a.out < million_3D_points.txt ) #include <algorithm> // sort #include <functional> // binary_function<> #include <iostream> #include <boost/foreach.hpp> #include <boost/range.hpp> // begin() end() #include <boost/tr1/tuple.hpp> // get<> tuple<> cout << #define foreach BOOST_FOREACH namespace { typedef double coord_t; typedef std::tr1::tuple<coord_tcoord_tcoord_t> point_t; // calculate distance (squared) between points `a` & `b` coord_t distance_sq(const point_t& a const point_t& b); // read from input stream `in` to the point `point_out` std::istream& getpoint(std::istream& in point_t& point_out); // Adaptable binary predicate that defines whether the first // argument is nearer than the second one to given reference point class less_distance : public std::binary_function<point_t point_t bool> { const point_t& point; public: explicit less_distance(const point_t& reference_point) : point(reference_point) {} bool operator () (const point_t& a const point_t& b) const { return distance_sq(a point) < distance_sq(b point); } }; } int main() { using namespace std; // use point value from previous examples point_t point(69.06310224 2.23409409 50.41979143); cout << ""point: "" << point << endl; less_distance nearer(point); const size_t nneighbours = 10; // number of nearest neighbours to find point_t points[nneighbours]; // populate `points` foreach (point_t& p points) if (! getpoint(cin p)) break; // Complexity: O(N*m) point_t current_point; while(cin) { getpoint(cin current_point); //NOTE: `cin` fails after the last //point so one can't lift it up to //the while condition // move to the last position the most distant from the // `point` point; O(m) foreach (point_t& p points) if (nearer(current_point p)) // found point that is nearer to the `point` //NOTE: could use insert (on sorted sequence) & break instead //of swap but in that case it might be better to use //heap-based algorithm altogether std::swap(current_point p); } // print results; O(m*log(m)) sort(boost::begin(points) boost::end(points) nearer); foreach (point_t p points) cout << p << ' ' << distance_sq(p point) << '\n'; } namespace { coord_t distance_sq(const point_t& a const point_t& b) { // boost::geometry::distance() squared using std::tr1::get; coord_t x = get<0>(a) - get<0>(b); coord_t y = get<1>(a) - get<1>(b); coord_t z = get<2>(a) - get<2>(b); return x*x + y*y + z*z; } std::istream& getpoint(std::istream& in point_t& point_out) { using std::tr1::get; return (in >> get<0>(point_out) >> get<1>(point_out) >> get<2>(point_out)); } } Measurements shows that most of the time is spent reading array from the file actual computations take on order of magnitude less time. @goran: if I set leafsize to 10 then it takes ~10 seconds (instead of 1 second) to query one point. I agree if the task is to query multiple (>10) points then kd-tree should win. Nice write up. To offset for file read I've run your python implementations with loop around the search that execute 100 times (each time looking around a different point and building the kd-tree only once). The bruteforce still won. Made me scratch my head. But then I examined your leafsize and you have an error there - you are setting the leafsize to 1000001 and that'll not perform well. After setting leafsize to 10 kd won (35s to 70s for 100 points with most of 35s spent on building the tree and 100 retrievals of 10 points taking a second). So as a conclusion if you can precompute the kd-tree it will win over brute force by orders of magnitude (not to mention that for really large data sets if you have a tree you will not have to read all the data in memory). @Unreason: pririty queue- and linear search- based implementations above do NOT read read all the data in memory. @J.F.Sebastian sorry I was imprecise - they have to scan all the data (read it in memory). I agree that they don't have to keep it in memory. The kd tree approach (once it has a kd index will have to scan the index only reading only part of it). Of course to build the index it will have to read all the data but I was commenting on the situation where you *can* precompute (which in case of linear search is no gain) from scipy.spatial import cKDTree is cython does lookups > 50 times faster than the pure-python KDTree (in 16d on my old mac ppc).  Calculate the distance for each of them and do Select(1..10 n) in O(n) time. That would the naive algorithm I guess.  You could store the points in a k-dimensional tree (kd-tree). Kd-trees are optimized for nearest-neighbor searches (finding the n points closest to a given point). I think an octree is called for here. The complexity required to build a K-d tree is going to be higher than the complexity required to do a linear search for the 10 closet points. The real power of a k-d tree comes when you are going to do many queries on a point set. @gabe: One big advantage of a kd-tree is that while it'll take some memory overhead to build once you have it a left-balanced kd-tree takes no more memory than the original unstructured list of point. An octree will definitely need at least some memory overhead. kd-tree can be slower in real life than the brute-force approach http://stackoverflow.com/questions/2486093/millions-of-3d-points-how-to-find-the-10-of-them-closest-to-a-given-point/2486341#2486341 This is the answer I would give in an interview. It's not rare for interviewers to use less-than-precise language and reading between the lines this seems to be most likely what they're looking for. In fact if I were the interviewer and someone gave the answer ""I would store the points in any old order and do a linear scan to find the 10 points"" and justified that answer based on my imprecise wording I would be pretty unimpressed. @ Jason Orendorff: I'd definitely discuss using a kd-tree for such a problem in a technical interview; however I would also explain why for the specific problem given the simpler linear search method will not only be asymptoticly faster but will run faster too. This will show a deeper understanding of the complexity of algorithms knowledge of data structures and the ability to consider different solutions to a problem.  basically a combination of the first two answer above me. since the points are in a file there's no need to keep them in memory. Instead of an array or a min heap I would use a max heap since you only want to check for distances less than the 10th closest point. For an array you would need to compare each newly calculated distance with all 10 of the distances you kept. For a min heap you have to perform 3 comparisons with every newly calculated distance. With a max heap you only perform 1 comparison when the newly calculated distance is greater than the root node.  This isn't a homework question is it? ;-) My thought: iterate over all points and put them into a min heap or bounded priority queue keyed by distance from the target. its an interview question sure but it is not known what the target is. :)  If the million entries are already in a file there's no need to load them all into a data structure in memory. Just keep an array with the top-ten points found so far and scan over the million points updating your top-ten list as you go. This is O(n) in the number of points. This will work well but the array is not the most efficient data store because you have to check over it each step or keep it sorted which can be a hassle. David's answer about a min-heap does that stuff for you but is otherwise the same solution. When the user only wants 10 points these concerns are negligible but when the user suddenly wants the nearest 100 pts you will be in trouble. @ Karl: The question specifies 10 points. I think including this detail is deliberate on the part of the asker. So Will describes a very good solution for what was asked. @Karl it is often surprising how good the compiler and the CPU is at optimizing the dumbest of loops to beat the cleverest of algorithms. Never underestimate the speedup to be gained when a loop can run in on-chip ram. Million entries are not already in the file - you can choose how to store them in the file. :) This choice on how to store it implies that you can also precompute any accompanying indexing structure. Kd-tree will win as it will not have to read the whole file at all < O(n). I've posted implementation of your answer http://stackoverflow.com/questions/2486093/millions-of-3d-points-how-to-find-the-10-of-them-closest-to-a-given-point/2486341#2486341 (though I use heap instead of linear search and it is completely unnecessary for the task) Karl is right a heap is better. *Use a heap.* @FogleBird: For 10 points linear search *is* better than heap (1.11-1.12 vs. 1.15-1.18 seconds for the input I've tested). For N=10 a constant factor in algorithm does matter therefore O(N) algorithm can be faster than O(log(N)) algorithm. 1.11 vs 1.15 is pretty slim. I'd code it with a heap anyway in case I ever wanted to do more than 10. Of course I don't know what the OP wants to do with it."
1085,A,"GraphicsPath Control On Top I have a custom C# user-control where I would like to draw a circle behind a textbox anchored to the centered bottom of the control. I'm drawing the circle like: protected override void OnResize(EventArgs e) { this.gp= new GraphicsPath(); this.gp.AddEllipse(00widthheight); //this is the width and height of the control this.Region=new Region(this.gp); this.Refresh(); base.OnResize (e); } protected override void OnPaint(PaintEventArgs pe) { Color centerColor = Color.FromArgb(255255255255); Color surroundColor = Color.FromArgb(255255255255); PathGradientBrush br=new PathGradientBrush(this.gp); br.CenterColor=centerColor; br.SurroundColors=new Color[]{surroundColor}; pe.Graphics.FillPath(brthis.gp); } I've added the textbox to the control in the GUI designer. When I run this I end up with something like this: How can I keep the ellipse behind the textbox? Thanks Mark If you want this in the background do this in the ""OnPaintBackground"" override rather than in OnPaint. Then when you want to draw it invalidate the region the ellipse is in. @Philip I'm not sure I understand and this may be my problem. The ellipse's region is the control (this.region). Do I need to feed it the region of something else? Giving the ellipse it's own region worked!"
1086,A,"How to achieve full-scene antialiasing on the iPhone I would like to achieve FSAA on my OpenGL ES app on the iPhone. Currently I do this by rendering the scene to a texture that is twice the width and height of the screen. I then use the nice function: void glDrawTexiOES(GLint x GLint y GLint z GLint width GLint height); to draw the image resized to the screen resolution. Is there a better way to do this? Update Bounty Added I was wondering given that its now Jan 2010 whether there is a better way to do this on v3.1 3GS phones etc. http://iphonedevelopment.blogspot.com/2009/06/opengl-es-2-shaders.html?showComment=1245770504079#c6001476340022842908 http://stackoverflow.com/questions/1021878/iphone-os-3-0-opengl-es-2-0-is-anyone-seeing-better-anti-aliasing/1024193#1024193 Sounds like you can't do it in hardware since you don't render to the framebuffer only a texture that the iPhone composites for you. I really like that blog-link. The explanation on why the AA-feature is not exposed by the driver does make total sense and helped me to understand the reasoning. Thank you for posting it.  As of iOS 4.0 full-screen anti-aliasing is directly supported via an Apple extension to OpenGL. The basic concept is similar to what you are already doing: render the scene onto a larger framebuffer then copy that down to a screen-sized framebuffer then copy that buffer to the screen. The difference is instead of creating a texture and rendering it onto a quad the copy/sample operation is performed by a single function call (specifically glResolveMultisampleFramebufferAPPLE()). For details on how to set up the buffers and modify your drawing code you can read a tutorial on the Gando Games blog which is written for OpenGL ES 1.1; there is also a note on Apple's Developer Forums explaining the same thing. Thanks to Bersaelor for pointing this out in another SO question.  Technically iPhone's GPU (PowerVR MBX Lite) should support anti-aliasing. However it seems that current Apple's OpenGL ES drivers (as of Jan 2009) don't expose this capability. So doing ""manual AA"" just like you do is pretty much the only way.  For Gran Turismo on the PSP the developers achieved an effect similar to anti-aliasing by moving the image back and forth one pixel per frame (demonstration can be found here: http://www.gtplanet.net/why-gran-turismo-psp-looks-so-good/) so if the iPhone doesn't support what you're looking for that's an option. Only problem is you need to achieve 60 fps :-)"
1087,A,"Simple 3D graphics project? I'm looking for some good ideas for a simple 3d graphics program as my final project for an intro to computer graphics class. As for some background information we'll be using opengl and will have a little over a month to work on it so nothing too far-fetched. The simpler and ""prettier"" looking the better. It does however require some sort of interface that the user can interact with (so a very simple game or similar is a good idea) and must be 3D. My only idea so far is maybe a 3D version of Tetris (google for some examples). Edit: I ended up going with 3D Tetris. For a less than a month's worth of time you can see what I came up with here. How about a 3d minesweeper game similar to this one?  i would check panda3d or pygame panda is probably close to what you are looking for and one idea that always works is to put the user's face in the main character or object. 3d-pong with the player's face? use something unexpected... like a tetris made of burgers instead of bricks.  I like exoplanets. Go read up on them. wikipedia http://exoplanet.eu there's a lot of info. Astronomers and public outreach people could always use fresh 3D animations showing how the doppler effect works or how the planet transiting in front of the star makes it for example 0.5% dimmer. Or what i work on is how when the planet passes behind its star we receive at Earth just a teeny bit less infrared from that star. The user could adjust the orbit size of planet etc. and see how that affects what astronomers see. Could be fun simple enough to do and unlimited potential in extending the work for nicer textures slick lighting effects etc and you could end up with something to contribute to science education. I'd be making such 3D animations myself if i weren't busy helping crunch numbers for the actual science. I'll be jealous! like http://kepler.nasa.gov/media/KEPLER.SWF but done better  In university for my parallel programming course I did an openGL/MPI implementation of Conway's Game of Life. It was quite interesting. Wish I still had the code around somewhere. The advantage of using open GL is that you can lay out the grid in different orientations rather than a flat grid. Remember code doesn't exist until it's checked into source control. +1 for the ""code doesn't exist until it's checked into source control.""  Putting some physics in makes it more interesting. How about implementing Labyrinth (the maze toy where you are supposed to guide a ball from the starting point to the goal by tilting it). EDIT: Erik told me it's called Labyrinth. Yes debugging physics can be a huge time consumer. That game's called Labyrinth - good idea. =) Implementing physics is a sure way to spend too much time on the physics and not enough time on the graphics part of a project. I speak from experience as I did a similar course in university. There's a great version of this for Android.  I tried to do a 3D Asteroids for a class once. I never completed the gameplay part since it was a graphics class. The ship could move around as could the asteroids but there was no collision detection. The ship and the asteroids had 3D textures applied to them and the asteroids were built out of ellipsoids so they were actually 3D. The gameplay was all 2D though.  If you've ever played Missile Command I belive that this could be a good project to '3d-ify'.  i love little self organising alife apps like boids. they can be fun to code and always benefit from a nice ui especially 3d ones. user input can modify aspects of the environment as well as moving around/through the environment  How about one of those games that are a wooden maze with a ball rolling around the top. You tilt the board and try to get the ball round the maze without falling down the hole? It has the advantage that it's relatively simple to get started but you could probably think of some extensions if you have time. This was suggested here - http://stackoverflow.com/questions/193339/simple-3d-graphics-project#193355  Rewrite Blocks 3D. The graphics on this project look horrible now. I remember playing this game (or one like it) on a 386 with wireframe graphics... awesome. The game is basically 3D tetris.  Look at http://www.contextfreeart.org/ ... write something similar but for 3d.  Honestly it's actually pretty easy to load up a bunch of animated models and set up a simple first person shooter. I mean to get a generic thing working you don't need all that much: Either load and display a heightmap or a BSP tree as the level. Load and render some simple MD2 models (keyframe animation low amount of polys and simple format). Draw a simple hud. Ray/AABB intersection every time the user clicks you'll need to cast a ray from the center of the screen and see if it intersects an the bounding boxes of the enemies. Simple FPS camera system. The above is pretty doable in a month for as far as I'm concerned. (It's probably doable in a week if you already know some of the stuff).  Try a chicken crossing the road game. You will probably need to demonstrate the bare minimum of: textures lighting animation interaction collision detection Do not include even simple physics if there are no marks for it. Prioritise tasks based on the marking scheme. Get something simple working first and back it up :)  If you're looking for a true university size task mine was to produce a small helicopter ""game"" where you could take off from an aircraft carrier in an ocean and fly around with some environmental effects moving water etc. i.e. nothing too complicated. As another example the task set for the year previous to mine was a little sans-opponent racing game. I would worry that you may loose marks with tetris as it sounds like little would be done on the z-axis and may come across a little too 2d though it obviously depends on your brief. Anyway these will give you the chance to experiment with the basic OpenGL features such as fog lighting geometry textures and some basic movement physics & collision detection/response. Further on this though often beyond the scope of such a university sized task you could then take this further add nicities such as animated geometry (e.g. people) environment mapping reflections shadows particle systems shaders perhaps a heightmapped island.  a 3D Text/Code Editor. Text is 3d errors stand out code indentations not only indent but protrude on z axis pages/files are 3d and can be flipped like a ringpad. Probably not usefull but fun and more interesting than a game imo. I'm almost tempted to try it myself! +1 *ouch*. The concept of vim3d makes my brain hurt! neat idea have you already built one? Not sure about the practicality but that is one killer idea. Upvoted for originality No I just whipped this up in illustrator to give an idea of what I was talking about. Like I said not very useful(at this time) but would be pretty cool! I agree. I am not sure if it would be practical but I would sure like to give it a run. Definately would be fun. Lots of things you could do with it too such as errors pulsate/rotate/flip drag and drop text realtime causing other lines to move around as you drag it bringing the the code block you are working on closer to the camera(sort of like a zoom).... @mattlant your image link is broken Hey mattlant just wanted to let you know that your image link is broken!  Rubik's cube."
1088,A,"Good reference of color-modifying functions? I can't find a good reference about color-modifying functions (such as contrast brightness gamma ...). (Is there a more exact term for what I mean?) Would appreciate a tutorial with example pictures and implementation algorithms. Thank you! EDIT: Platform and language is not so important. I'm interested in pure mathematic algorithms so please don't point me to graphic APIs ;) What platform/language? That information might be helpful. +1 to offset the premature downvote :/ especially without some comment as to how this question could be improved. You probably just want a decent reference book on image processing. Yes image processing is a good term I also tagged the question with it. But I was thinking that term is too general. In fact searching for this term I found a lot of information about things such as affinity transformation and graphic effects but very few information about the specific topic ""graphic functions"". I would add a tag of color-science here but for the limit of 5 tags. A reference I might suggest is Giorgianni & Madden or perhaps one of Hunt's books. http://www.amazon.com/Digital-Color-Management-Wiley-Technology/dp/047051244X/ref=sr_1_1?ie=UTF8&s=books&qid=1265512453&sr=1-1 Somewhat dry but usually thorough is the Color FAQ: http://www.poynton.com/ColorFAQ.html  Only part of what you are asking but probably useful: http://www.brucelindbloom.com/index.html?Equations.html Color space conversion though being very specific is also an interesting topic.  I found Charles Poynton's book Digital Video and HDTV: Algorithms and Interfaces hugely helpful in this area. Don't be fooled by the word ""video"" in the title; it contains all sorts of wonderful information about still images as well. The explanation of gamma is especially good. If you can't find the book at your library the Poynton's Color FAQ and Gamma FAQ are also somewhat helpful. But the book is really great!"
1089,A,"How do I draw text with GLUT / OpenGL in C++? How do I draw a text string onto the screen using GLUT / OpenGL drawing functions? If you don't like the built-in stroke font or bitmap font that comes with GLUT as per epatel's answer you'll have to roll your own solution. NeHe has some good tutorials (along with fully-working sample code) on this: Lesson 13 - Bitmap Fonts Lesson 14 - Outline Fonts Lesson 15 - Texture-Mapped Outline Fonts  It's generally a bit nasty and not straightforward. Give this tool a try: http://students.cs.byu.edu/~bfish/glfontdl.php  void RenderString(float x float y void *font const char* string RGB const& rgb) { char *c; glColor3f(rgb.r rgb.g rgb.b); glRasterPos2f(x y); glutBitmapString(font string); } And you can call it like; RenderString(0.0f 0.0f GLUT_BITMAP_TIMES_ROMAN_24 ""Hello"" RGB(1.0f 0.0f 0.0f)); error: ‘glutBitmapString’ was not declared in this scope error: request for member ‘r’ in ‘rgb’ which is of non-class type ‘const int’  There are two ways to draw strings with GLUT glutStrokeString will draw text in 3D and glutBitmapString will draw text facing the user Please note that you will need [freeglut][1] as opposed to glut to use glutBitmapString. [1]: http://freeglut.sourceforge.net/ I know its an old question I have glut.h it just when I try to use either of the method it will say identifier ""glutBitmapString"" or ""glutStrokeString"" is undefined any ideas ? @Jonathan superjoe30 is right. glutBitmapString and glutStrokeString are not in the original GLUT implementation. But if you want to use GLUT I'd suggest looking into using either freeglut or openglut which both have them. If I recall correctly the original GLUT implementation has not been updated since 1998 due to its licensing scheme so freeglut and openglut was started to solve that problem and add new features etc. See http://freeglut.sourceforge.net/ thanks ! that solved my puzzle"
1090,A,"C#: Paint own Bar Chart I'm trying to paint a simple bar chart via C# but I've never experimented with the Graphics and Drawing namespaces. I thought of generating a ""start"" and ""end"" graphic and then repeating an image somehow (to show a ""length"") but I have no idea how to do this. I'd be really happy if you can point me in the right direction and/or if you have sample code to do this. Thank you! Alex here is a very simple example to get you started. To test the code just add a panel control to your form and create a paint event handler for it. (Double click on the panel in the designer should do it by default.) Then replace the handler code with th code below. The code draws five bars of arbitrary length across the panel and the bar widths and heights are related to the panel widths and heights. The code is arbitrary but is a good and simple way to introduce .Net drawing capabilities. void Panel1Paint(object sender PaintEventArgs e) { Graphics g = e.Graphics; int objCount = 5; for (int n=0; n<objCount; n++) { g.FillRectangle(Brushes.AliceBlue 0 n*(panel1.Height/objCount) panel1.Width/(n+1) panel1.Height/objCount); g.DrawRectangle(new Pen(Color.Black) 0 n*(panel1.Height/objCount) panel1.Width/(n+1) panel1.Height/objCount); g.DrawString(n.ToString() new Font(""Arial"" 10f) Brushes.Black 2 2+n*(panel1.Height/objCount)); } }  I've got to agree with Eros. There are lots of very good graphing libraries to accomplish what you want. The best I've come across: Microsoft Chart Controls - There's even a Visual Studio plugin and a good tutorial. Flot - This one is jQuery based so good for web apps. Google Chart - Simple API and even an ASP.Net Control  why dont you give http://zedgraph.org/wiki/index.php?title=Main_Page a try. The time required to build (and test) your own graph controls will be too much  What's wrong with using a simple loop?  Here is a tutorial from CodeProject Baget i've seen that same tutorial on CodeProject but i don't think it's a great fit here. Alex was asking for samples to create a simple bar chart and a simple intro to .Net drawing. That CodeProject tutorial is a very complex example of drawing 3D transparent pie charts with thousands of lines of code and dozens of objects. It's a great piece of work by the author but not great for beginners. Also if you check the project's criticisms its performance is horrible due to bad OOP decisions... So perhaps not such a great tutorial after all."
1091,A,"Graphics Question: How do I restrict the mouse cursor to within a circle? I'm playing with XNA. When I click the left mouse button I record the XY co-ordinates. Keeping the mouse button held down moving the mouse draws a line from this origin to the current mouse position. I've offset this into the middle of the window. Now what I'd like to do is restrict the mouse cursor to within a circle (with a radius of N centred on the middle of the screen). Restricting the mouse to a rectangular region is easy enough (by adjusting the origin by the difference of the mouse position and the size of the region) but I haven't a clue on how to start doing it for a circular region. Can anyone explain how to do this? Any advice on where to start would be helpful. I don't have a clue about how to use XNA... so can't give you specific code but the idea is simple. Just check the distance between current mouse position and the origin with Pythagora's theorem: dist = sqrt((current_y - orig_y)^2 + (current_x - orig_x)^2) Then check that dist is < radius @Dan: Glad it was helpful! +1 for keeping it simple Brilliant! I upvoted both answers but I went for Tom's because it explained what to do if dist was > radius. However this answer made it make sense because I ""get"" Pythagoras (just).  You need every time the mouse moves to restrict it to a rectangle between its current position and the nearest point on the circle. The nearest point on the circle is got by let (xy) be where the mouse is (x0y0) be the origin (x0-x y0-y) is the vector from origin to pointer d=sqrt((x0-x)2+(y0-y)2) is the length of that vector (N*(x0-x)/d N*(y0-y)/d) is then the point at distance N from the origin along the line joining the origin to the mouse position - that is the nearest point on the circle to the mouse pointer. +1 for explaining the math (albeit trivial)"
1092,A,"Rotate Text for printing I am using a PrintDocument to print a page. At one point I want to rotate the text 90 degrees and print it ie print text vertically. Any ideas ??? g.RotateTransform(90); does not work for OnPaint. What do you mean ""does not work for OnPaint""? You're a life saviour!!!! Thanks!!! Just to make things clear you can draw anything you like without rotating then you type: e.Graphics.RotateTransform(90); and everything after that draws rotated. When you call RotateTransform you will need to pay attention to where the coordinate system ends up. If you run the following code the ""Tilted text"" will appear to the left of the left edge; so it's not visible: e.Graphics.Clear(SystemColors.Control); e.Graphics.DrawString(""Normal text"" this.Font SystemBrushes.ControlText 10 10); e.Graphics.RotateTransform(90); e.Graphics.DrawString(""Tilted text"" this.Font SystemBrushes.ControlText 10 10); Since you have tilted the drawing surface 90 degrees (clock wise) the y coordinate will now move along the right/left axis (from your perspective) instead of up/down. Larger numbers are further to the left. So to move the tilted text into the visible part of the surface you will need to decrease the y coordinate: e.Graphics.Clear(SystemColors.Control); e.Graphics.DrawString(""Normal text"" this.Font SystemBrushes.ControlText 10 10); e.Graphics.RotateTransform(90); e.Graphics.DrawString(""Tilted text"" this.Font SystemBrushes.ControlText 10 -40); By default the coordinate system has its origo in the top left corner of the surface so that is the axis around which RotateTransform will rotate the surface. Here is an image that illustrates this; black is before call to RotateTransform red is after call to RotateTransform(35): Thanks Fredrik. It worked. I wish MSDN described it like this. Quite helpful +1 for the diagram"
1093,A,"Remove surrounding whitespace from an image I have a block of product images we received from a customer. Each product image is a picture of something and it was taken with a white background. I would like to crop all the surrounding parts of the image but leave only the product in the middle. Is this possible? As an example: [http://www.5dnet.de/media/catalog/product/d/r/dress_shoes_5.jpg][1] I don't want all white pixels removed however I do want the image cropped so that the top-most row of pixels contains one non-white pixel the left-most vertical row of pixels contains one non-white pixel bottom-most horizontal row of pixels contains one non-white pixel etc. Code in C# or VB.net would be appreciated. I've written code to do this myself - it's not too difficult to get the basics going. Essentially you need to scan pixel rows/columns to check for non-white pixels and isolate the bounds of the product image then create a new bitmap with just that region. Note that while the Bitmap.GetPixel() method works it's relatively slow. If processing time is important you'll need to use Bitmap.LockBits() to lock the bitmap in memory and then some simple pointer use inside an unsafe { } block to access the pixels directly. This article on CodeProject gives some more details that you'll probably find useful.  The pnmcrop utility from the netpbm graphics utilities library does exactly that. I suggest looking at their code available from http://netpbm.sourceforge.net/  It's certainly possible. In pseudocode: topmost = 0 for row from 0 to numRows: if allWhiteRow(row): topmost = row botmost = 0 for row from numRows-1 to 0: if allWhiteRow(row): botmost = row And similarly for left and right. The code for allWhiteRow would involve looking at the pixels in that row and making sure they're all close to 255255255.  I found I had to adjust Dmitri's answer to ensure it works with images that don't actually need cropping (either horizontally vertically or both)...  public static Bitmap Crop(Bitmap bmp) { int w = bmp.Width; int h = bmp.Height; Func<int bool> allWhiteRow = row => { for (int i = 0; i < w; ++i) if (bmp.GetPixel(i row).R != 255) return false; return true; }; Func<int bool> allWhiteColumn = col => { for (int i = 0; i < h; ++i) if (bmp.GetPixel(col i).R != 255) return false; return true; }; int topmost = 0; for (int row = 0; row < h; ++row) { if (allWhiteRow(row)) topmost = row; else break; } int bottommost = 0; for (int row = h - 1; row >= 0; --row) { if (allWhiteRow(row)) bottommost = row; else break; } int leftmost = 0 rightmost = 0; for (int col = 0; col < w; ++col) { if (allWhiteColumn(col)) leftmost = col; else break; } for (int col = w - 1; col >= 0; --col) { if (allWhiteColumn(col)) rightmost = col; else break; } if (rightmost == 0) rightmost = w; // As reached left if (bottommost == 0) bottommost = h; // As reached top. int croppedWidth = rightmost - leftmost; int croppedHeight = bottommost - topmost; if (croppedWidth == 0) // No border on left or right { leftmost = 0; croppedWidth = w; } if (croppedHeight == 0) // No border on top or bottom { topmost = 0; croppedHeight = h; } try { var target = new Bitmap(croppedWidth croppedHeight); using (Graphics g = Graphics.FromImage(target)) { g.DrawImage(bmp new RectangleF(0 0 croppedWidth croppedHeight) new RectangleF(leftmost topmost croppedWidth croppedHeight) GraphicsUnit.Pixel); } return target; } catch (Exception ex) { throw new Exception( string.Format(""Values are topmost={0} btm={1} left={2} right={3} croppedWidth={4} croppedHeight={5}"" topmost bottommost leftmost rightmost croppedWidth croppedHeight) ex); } } Good work buddy and thanks for sharing! For PNG files with the 'transparent whitespace' I have added checking for bmp.GetPixel(i row).A != 0 and bmp.GetPixel(col i).A != 0 (alpha level of zero). Now my PNGs are working great. Thank you for the code! Note: I run bmp.GetPixel() once then analyze R and A properties to prevent double scan.  Here's my (rather lengthy) solution: public Bitmap Crop(Bitmap bmp) { int w = bmp.Width h = bmp.Height; Func<int bool> allWhiteRow = row => { for (int i = 0; i < w; ++i) if (bmp.GetPixel(i row).R != 255) return false; return true; }; Func<int bool> allWhiteColumn = col => { for (int i = 0; i < h; ++i) if (bmp.GetPixel(col i).R != 255) return false; return true; }; int topmost = 0; for (int row = 0; row < h; ++row) { if (allWhiteRow(row)) topmost = row; else break; } int bottommost = 0; for (int row = h - 1; row >= 0; --row) { if (allWhiteRow(row)) bottommost = row; else break; } int leftmost = 0 rightmost = 0; for (int col = 0; col < w; ++col) { if (allWhiteColumn(col)) leftmost = col; else break; } for (int col = w-1; col >= 0; --col) { if (allWhiteColumn(col)) rightmost = col; else break; } int croppedWidth = rightmost - leftmost; int croppedHeight = bottommost - topmost; try { Bitmap target = new Bitmap(croppedWidth croppedHeight); using (Graphics g = Graphics.FromImage(target)) { g.DrawImage(bmp new RectangleF(0 0 croppedWidth croppedHeight) new RectangleF(leftmost topmost croppedWidth croppedHeight) GraphicsUnit.Pixel); } return target; } catch (Exception ex) { throw new Exception( string.Format(""Values are topmost={0} btm={1} left={2} right={3}"" topmost bottommost leftmost rightmost) ex); } } works perfectly except when the croppedWidth or croppedHeight is zero in this case I set them to the bmp.Width or bmp.Height respectively and it works like a charm :)"
1094,A,"How to draw a Perspective-Correct Grid in 2D I have an application that defines a real world rectangle on top of an image/photograph of course in 2D it may not be a rectangle because you are looking at it from an angle. The problem is say that the rectangle needs to have grid lines drawn on it for example if it is 3x5 so I need to draw 2 lines from side 1 to side 3 and 4 lines from side 2 to side 4. As of right now I am breaking up each line into equidistant parts to get the start and end point of all the grid lines. However the more of an angle the rectangle is on the more ""incorrect"" these lines become as horizontal lines further from you should be closer together. Does anyone know the name of the algorithm that I should be searching for? Yes I know you can do this in 3D however I am limited to 2D for this particular application. So an example might be trying to draw a rectangle on a window in a picture? yes that would be an example This a geometric solution I thought out. I do not know whether the 'algorithm' has a name. Say you want to start by dividing the 'rectangle' into n pieces with vertical lines first. The goal is to place points P1..Pn-1 on the top line which we can use to draw lines through them to the points where the left and right line meet or parallel to them when such point does not exist. If the top and bottom line are parallel to each other just place thoose points to split the top line between the corners equidistantly. Else place n points Q1..Qn on the left line so that theese and the top-left corner are equidistant and i < j => Qi is closer to the top-left cornern than Qj. In order to map the Q-points to the top line find the intersection S of the line from Qn through the top-right corner and the parallel to the left line through the intersection of top and bottom line. Now connect S with Q1..Qn-1. The intersection of the new lines with the top line are the wanted P-points. Do this analog for the horizontal lines.  In the special case when you look perpendicular to sides 1 and 3 you can divide those sides in equal parts. Then draw a diagonal and draw parallels to side 1 through each intersection of the diagonal and the dividing lines drawn earlier.  Here's the solution: http://freespace.virgin.net/hugo.elias/graphics/x_persp.htm The basic idea is you can find the perspective correct ""center"" of your rectangle by connecting the corners diagonally. The intersection of the two resulting lines is your perspective correct center. From there you subdivide your rectangle into four smaller rectangles and you repeat the process. The number of times depends on how accurate you want it. You can subdivide to just below the size of a pixel for effectively perfect perspective. Then in your subrectangles you just apply your standard uncorrected ""textured"" triangles or rectangles or whatever. You can perform this algorithm without going to the complex trouble of building a 'real' 3d world. it's also good for if you do have a real 3d world modeled but your textriangles are not perspective corrected in hardware or you need a performant way to get perspective correct planes without per pixel rendering trickery. I presume so but if you're a decent enough mathematician you could probably derive a non linear transformation matrix which reflects the same relationship between source and destination pixels. I am not such a mathematician but I at least know the magic words to investigate and now so do you. Would this solution only work for grids that are a power of 2?  While my google-fu has failed to produce any rock solid mathematical solution perhaps this drawing that I found could help a little. http://studiochalkboard.evansville.edu/lp-diminish.html I think it might actually be pretty difficult to come up with the correct math on your own it's probably some sort of logarithmic or summation expression. Hopefully the drawing and terms at that link might provide something a little more searchable for you. Thank you that is actually very valuable. your Google skills are definetly ahead of mine.  Using Breton's subdivision method (which is related to Mongo's extension method) will get you accurate arbitrary power-of-two divisions. To split into non-power-of-two divisions using those methods you will have to subdivide to sub-pixel spacing which can be computationally expensive. However I believe you may be able to apply a variation of Haga's Theorem (which is used in origami to divide a side into Nths given a side divided into (N-1)ths) to the perspective-square subdivisions to produce arbitrary divisions from the closest power of 2 without having to continue subdividing. Haga's theorum was a very interesting read however it can only apply to squares. With a variable h/w ratio to my rectangles I cant seem to find a way to apply that theorum. Instead of going to sub-pixel acuracy I think I am going to do a ""binary search"" for each subdivision. So if I need to split my rect into thirds I would basically do a binary search to within a certain amount of accuracy to 33.33% and 66.66% then do a weighted bisection using each ""part"" (and by part I mean if I bisected 4 times it would equate to a perspective correct 16th)  Image: Example of Bilinear & Perspective Transform (Note: The height of top & bottom horizontal grid lines is actually half of the rest lines height on both drawings) ======================================== I know this is an old question but I have a generic solution so I decided to publish it hopping it will be useful to the future readers. The code bellow can draw an arbitrary perspective grid without the need of repetitive computations. I begin actually with a similar problem: to draw a 2D perspective Grid and then transform the underline image to restore the perspective. I started to read here: http://www.imagemagick.org/Usage/distorts/#bilinear_forward and then here (the Leptonica Library): http://www.leptonica.com/affine.html were I found this: When you look at an object in a plane from some arbitrary direction at a finite distance you get an additional ""keystone"" distortion in the image. This is a projective transform which keeps straight lines straight but does not preserve the angles between lines. This warping cannot be described by a linear affine transformation and in fact differs by x- and y-dependent terms in the denominator. The transformation is not linear as many people already pointed out in this thread. It involves solving a linear system of 8 equations (once) to compute the 8 required coefficients and then you can use them to transform as many points as you want. To avoid including all Leptonica library in my project I took some pieces of code from it I removed all special Leptonica data-types & macros I fixed some memory leaks and I converted it to a C++ class (mostly for encapsulation reasons) which does just one thing: It maps a (Qt) QPointF float (xy) coordinate to the corresponding Perspective Coordinate. If you want to adapt the code to another C++ library the only thing to redefine/substitute is the QPointF coordinate class. I hope some future readers would find it useful. The code bellow is divided into 3 parts: A. An example on how to use the genImageProjective C++ class to draw a 2D perspective Grid B. genImageProjective.h file C. genImageProjective.cpp file //============================================================ // C++ Code Example on how to use the // genImageProjective class to draw a perspective 2D Grid //============================================================ #include ""genImageProjective.h"" // Input: 4 Perspective-Tranformed points: // perspPoints[0] = top-left // perspPoints[1] = top-right // perspPoints[2] = bottom-right // perspPoints[3] = bottom-left void drawGrid(QPointF *perspPoints) { (...) // Setup a non-transformed area rectangle // I use a simple square rectangle here because in this case we are not interested in the source-rectangle // (we want to just draw a grid on the perspPoints[] area) // but you can use any arbitrary rectangle to perform a real mapping to the perspPoints[] area QPointF topLeft = QPointF(00); QPointF topRight = QPointF(10000); QPointF bottomRight = QPointF(10001000); QPointF bottomLeft = QPointF(01000); float width = topRight.x() - topLeft.x(); float height = bottomLeft.y() - topLeft.y(); // Setup Projective trasform object genImageProjective imageProjective; imageProjective.sourceArea[0] = topLeft; imageProjective.sourceArea[1] = topRight; imageProjective.sourceArea[2] = bottomRight; imageProjective.sourceArea[3] = bottomLeft; imageProjective.destArea[0] = perspPoints[0]; imageProjective.destArea[1] = perspPoints[1]; imageProjective.destArea[2] = perspPoints[2]; imageProjective.destArea[3] = perspPoints[3]; // Compute projective transform coefficients if (imageProjective.computeCoeefficients() != 0) return; // This can actually fail if any 3 points of Source or Dest are colinear // Initialize Grid parameters (without transform) float gridFirstLine = 0.1f; // The normalized position of first Grid Line (0.0 to 1.0) float gridStep = 0.1f; // The normalized Grd size (=distance between grid lines: 0.0 to 1.0) // Draw Horizonal Grid lines QPointF lineStart lineEnd tempPnt; for (float pos = gridFirstLine; pos <= 1.0f; pos += gridStep) { // Compute Grid Line Start tempPnt = QPointF(topLeft.x() topLeft.y() + pos*width); imageProjective.mapSourceToDestPoint(tempPnt lineStart); // Compute Grid Line End tempPnt = QPointF(topRight.x() topLeft.y() + pos*width); imageProjective.mapSourceToDestPoint(tempPnt lineEnd); // Draw Horizontal Line (use your prefered method to draw the line) (...) } // Draw Vertical Grid lines for (float pos = gridFirstLine; pos <= 1.0f; pos += gridStep) { // Compute Grid Line Start tempPnt = QPointF(topLeft.x() + pos*height topLeft.y()); imageProjective.mapSourceToDestPoint(tempPnt lineStart); // Compute Grid Line End tempPnt = QPointF(topLeft.x() + pos*height bottomLeft.y()); imageProjective.mapSourceToDestPoint(tempPnt lineEnd); // Draw Vertical Line (use your prefered method to draw the line) (...) } (...) } ========================================== //======================================== //C++ Header File: genImageProjective.h //======================================== #ifndef GENIMAGE_H #define GENIMAGE_H #include <QPointF> // Class to transform an Image Point using Perspective transformation class genImageProjective { public: genImageProjective(); int computeCoeefficients(void); int mapSourceToDestPoint(QPointF& sourcePoint QPointF& destPoint); public: QPointF sourceArea[4]; // Source Image area limits (Rectangular) QPointF destArea[4]; // Destination Image area limits (Perspectivelly Transformed) private: static int gaussjordan(float **a float *b int n); bool coefficientsComputed; float vc[8]; // Vector of Transform Coefficients }; #endif // GENIMAGE_H //======================================== //======================================== //C++ CPP File: genImageProjective.cpp //======================================== #include <math.h> #include ""genImageProjective.h"" // ---------------------------------------------------- // class genImageProjective // ---------------------------------------------------- genImageProjective::genImageProjective() { sourceArea[0] = sourceArea[1] = sourceArea[2] = sourceArea[3] = QPointF(00); destArea[0] = destArea[1] = destArea[2] = destArea[3] = QPointF(00); coefficientsComputed = false; } // -------------------------------------------------------------- // Compute projective transform coeeeficients // RetValue: 0: Success !=0: Error /*-------------------------------------------------------------* * Projective coordinate transformation * *-------------------------------------------------------------*/ /*! * computeCoeefficients() * * Input: this->sourceArea[4]: (source 4 points; unprimed) * this->destArea[4]: (transformed 4 points; primed) * this->vc (computed vector of transform coefficients) * Return: 0 if OK; <0 on error * * We have a set of 8 equations describing the projective * transformation that takes 4 points (sourceArea) into 4 other * points (destArea). These equations are: * * x1' = (c[0]*x1 + c[1]*y1 + c[2]) / (c[6]*x1 + c[7]*y1 + 1) * y1' = (c[3]*x1 + c[4]*y1 + c[5]) / (c[6]*x1 + c[7]*y1 + 1) * x2' = (c[0]*x2 + c[1]*y2 + c[2]) / (c[6]*x2 + c[7]*y2 + 1) * y2' = (c[3]*x2 + c[4]*y2 + c[5]) / (c[6]*x2 + c[7]*y2 + 1) * x3' = (c[0]*x3 + c[1]*y3 + c[2]) / (c[6]*x3 + c[7]*y3 + 1) * y3' = (c[3]*x3 + c[4]*y3 + c[5]) / (c[6]*x3 + c[7]*y3 + 1) * x4' = (c[0]*x4 + c[1]*y4 + c[2]) / (c[6]*x4 + c[7]*y4 + 1) * y4' = (c[3]*x4 + c[4]*y4 + c[5]) / (c[6]*x4 + c[7]*y4 + 1) * * Multiplying both sides of each eqn by the denominator we get * * AC = B * * where B and C are column vectors * * B = [ x1' y1' x2' y2' x3' y3' x4' y4' ] * C = [ c[0] c[1] c[2] c[3] c[4] c[5] c[6] c[7] ] * * and A is the 8x8 matrix * * x1 y1 1 0 0 0 -x1*x1' -y1*x1' * 0 0 0 x1 y1 1 -x1*y1' -y1*y1' * x2 y2 1 0 0 0 -x2*x2' -y2*x2' * 0 0 0 x2 y2 1 -x2*y2' -y2*y2' * x3 y3 1 0 0 0 -x3*x3' -y3*x3' * 0 0 0 x3 y3 1 -x3*y3' -y3*y3' * x4 y4 1 0 0 0 -x4*x4' -y4*x4' * 0 0 0 x4 y4 1 -x4*y4' -y4*y4' * * These eight equations are solved here for the coefficients C. * * These eight coefficients can then be used to find the mapping * (xy) --> (x'y'): * * x' = (c[0]x + c[1]y + c[2]) / (c[6]x + c[7]y + 1) * y' = (c[3]x + c[4]y + c[5]) / (c[6]x + c[7]y + 1) * */ int genImageProjective::computeCoeefficients(void) { int retValue = 0; int i; float *a[8]; /* 8x8 matrix A */ float *b = this->vc; /* rhs vector of primed coords X'; coeffs returned in vc[] */ b[0] = destArea[0].x(); b[1] = destArea[0].y(); b[2] = destArea[1].x(); b[3] = destArea[1].y(); b[4] = destArea[2].x(); b[5] = destArea[2].y(); b[6] = destArea[3].x(); b[7] = destArea[3].y(); for (i = 0; i < 8; i++) a[i] = NULL; for (i = 0; i < 8; i++) { if ((a[i] = (float *)calloc(8 sizeof(float))) == NULL) { retValue = -100; // ERROR_INT(""a[i] not made"" procName 1); goto Terminate; } } a[0][0] = sourceArea[0].x(); a[0][1] = sourceArea[0].y(); a[0][2] = 1.; a[0][6] = -sourceArea[0].x() * b[0]; a[0][7] = -sourceArea[0].y() * b[0]; a[1][3] = sourceArea[0].x(); a[1][4] = sourceArea[0].y(); a[1][5] = 1; a[1][6] = -sourceArea[0].x() * b[1]; a[1][7] = -sourceArea[0].y() * b[1]; a[2][0] = sourceArea[1].x(); a[2][1] = sourceArea[1].y(); a[2][2] = 1.; a[2][6] = -sourceArea[1].x() * b[2]; a[2][7] = -sourceArea[1].y() * b[2]; a[3][3] = sourceArea[1].x(); a[3][4] = sourceArea[1].y(); a[3][5] = 1; a[3][6] = -sourceArea[1].x() * b[3]; a[3][7] = -sourceArea[1].y() * b[3]; a[4][0] = sourceArea[2].x(); a[4][1] = sourceArea[2].y(); a[4][2] = 1.; a[4][6] = -sourceArea[2].x() * b[4]; a[4][7] = -sourceArea[2].y() * b[4]; a[5][3] = sourceArea[2].x(); a[5][4] = sourceArea[2].y(); a[5][5] = 1; a[5][6] = -sourceArea[2].x() * b[5]; a[5][7] = -sourceArea[2].y() * b[5]; a[6][0] = sourceArea[3].x(); a[6][1] = sourceArea[3].y(); a[6][2] = 1.; a[6][6] = -sourceArea[3].x() * b[6]; a[6][7] = -sourceArea[3].y() * b[6]; a[7][3] = sourceArea[3].x(); a[7][4] = sourceArea[3].y(); a[7][5] = 1; a[7][6] = -sourceArea[3].x() * b[7]; a[7][7] = -sourceArea[3].y() * b[7]; retValue = gaussjordan(a b 8); Terminate: // Clean up for (i = 0; i < 8; i++) { if (a[i]) free(a[i]); } this->coefficientsComputed = (retValue == 0); return retValue; } /*-------------------------------------------------------------* * Gauss-jordan linear equation solver * *-------------------------------------------------------------*/ /* * gaussjordan() * * Input: a (n x n matrix) * b (rhs column vector) * n (dimension) * Return: 0 if ok 1 on error * * Note side effects: * (1) the matrix a is transformed to its inverse * (2) the vector b is transformed to the solution X to the * linear equation AX = B * * Adapted from ""Numerical Recipes in C Second Edition"" 1992 * pp. 36-41 (gauss-jordan elimination) */ #define SWAP(ab) {temp = (a); (a) = (b); (b) = temp;} int genImageProjective::gaussjordan(float **a float *b int n) { int retValue = 0; int i icol=0 irow=0 j k l ll; int *indexc = NULL *indexr = NULL *ipiv = NULL; float big dum pivinv temp; if (!a) { retValue = -1; // ERROR_INT(""a not defined"" procName 1); goto Terminate; } if (!b) { retValue = -2; // ERROR_INT(""b not defined"" procName 1); goto Terminate; } if ((indexc = (int *)calloc(n sizeof(int))) == NULL) { retValue = -3; // ERROR_INT(""indexc not made"" procName 1); goto Terminate; } if ((indexr = (int *)calloc(n sizeof(int))) == NULL) { retValue = -4; // ERROR_INT(""indexr not made"" procName 1); goto Terminate; } if ((ipiv = (int *)calloc(n sizeof(int))) == NULL) { retValue = -5; // ERROR_INT(""ipiv not made"" procName 1); goto Terminate; } for (i = 0; i < n; i++) { big = 0.0; for (j = 0; j < n; j++) { if (ipiv[j] != 1) { for (k = 0; k < n; k++) { if (ipiv[k] == 0) { if (fabs(a[j][k]) >= big) { big = fabs(a[j][k]); irow = j; icol = k; } } else if (ipiv[k] > 1) { retValue = -6; // ERROR_INT(""singular matrix"" procName 1); goto Terminate; } } } } ++(ipiv[icol]); if (irow != icol) { for (l = 0; l < n; l++) SWAP(a[irow][l] a[icol][l]); SWAP(b[irow] b[icol]); } indexr[i] = irow; indexc[i] = icol; if (a[icol][icol] == 0.0) { retValue = -7; // ERROR_INT(""singular matrix"" procName 1); goto Terminate; } pivinv = 1.0 / a[icol][icol]; a[icol][icol] = 1.0; for (l = 0; l < n; l++) a[icol][l] *= pivinv; b[icol] *= pivinv; for (ll = 0; ll < n; ll++) { if (ll != icol) { dum = a[ll][icol]; a[ll][icol] = 0.0; for (l = 0; l < n; l++) a[ll][l] -= a[icol][l] * dum; b[ll] -= b[icol] * dum; } } } for (l = n - 1; l >= 0; l--) { if (indexr[l] != indexc[l]) { for (k = 0; k < n; k++) SWAP(a[k][indexr[l]] a[k][indexc[l]]); } } Terminate: if (indexr) free(indexr); if (indexc) free(indexc); if (ipiv) free(ipiv); return retValue; } // -------------------------------------------------------------- // Map a source point to destination using projective transform // -------------------------------------------------------------- // Params: // sourcePoint: initial point // destPoint: transformed point // RetValue: 0: Success !=0: Error // -------------------------------------------------------------- // Notes: // 1. You must call once computeCoeefficients() to compute // the this->vc[] vector of 8 coefficients before you call // mapSourceToDestPoint(). // 2. If there was an error or the 8 coefficients were not computed // a -1 is returned and destPoint is just set to sourcePoint value. // -------------------------------------------------------------- int genImageProjective::mapSourceToDestPoint(QPointF& sourcePoint QPointF& destPoint) { if (coefficientsComputed) { float factor = 1.0f / (vc[6] * sourcePoint.x() + vc[7] * sourcePoint.y() + 1.); destPoint.setX( factor * (vc[0] * sourcePoint.x() + vc[1] * sourcePoint.y() + vc[2]) ); destPoint.setY( factor * (vc[3] * sourcePoint.x() + vc[4] * sourcePoint.y() + vc[5]) ); return 0; } else // There was an error while computing coefficients { destPoint = sourcePoint; // just copy the source to destination... return -1; // ...and return an error } } //======================================== @Rui: Thanks for the info. I was actually thinking to use & experiment with OpenCV sometime in the future so this an additional motive to do it. You seem to be describing projective transforms or homographies. OpenCV library (C++ Java Python) for example has functions to estimate those transforms and to apply them.  The problem is that it's the transformation from 3D to 2D that's getting you. Here's a tutorial on how it's done. This is striclty 2D there is no 3D data to project from.  The most elegant and fastest solution would be to find the homography matrix which maps rectangle coordinates to photo coordinates. With a decent matrix library it should not be a difficult task as long as you know your math. Keywords: Collineation Homography Direct Linear Transformation However the recursive algorithm above should work but probably if your resources are limited projective geometry is the only way to go.  What you need to do is represent it in 3D (world) and then project it down to 2D (screen). This will require you to use a 4D transformation matrix which does the projection on a 4D homogeneous down to a 3D homogeneous vector which you can then convert down to a 2D screen space vector. I couldn't find it in Google either but a good computer graphics books will have the details. Keywords are projection matrix projection transformation affine transformation homogeneous vector world space screen space perspective transformation 3D transformation And by the way this usually takes a few lectures to explain all of that. So good luck. Using 3D math to accomplish this is massive overkill extremely more computational complexity than required. Its just a bunch of math which does not require any IO. So it would be super fast. ""super fast"" is relative. Not using 3D math would be ""super fast""ER. Throwing millions of processor cycles at a problem that should take thousands is just asking for trouble. What happens when he decides to to a 1024x1024 grid instead of 5x5?"
1095,A,"Problem when using MemoryDC Why does my code print the lines gray instead of black? import wx class MyFrame(wx.Frame): def __init__(self*args**kwargs): wx.Frame.__init__(self*args**kwargs) self.panel=wx.Panel(self-1size=(10001000)) self.Bind(wx.EVT_PAINT self.on_paint) self.Bind(wx.EVT_SIZE self.on_size) self.bitmap=wx.EmptyBitmapRGBA(10001000255255255255) dc=wx.MemoryDC() dc.SelectObject(self.bitmap) dc.SetPen(wx.Pen(wx.NamedColor(""black"")10wx.SOLID)) dc.DrawCircle(0030) dc.DrawLine(40407070) dc.Destroy() self.Show() def on_size(selfe=None): self.Refresh() def on_paint(selfe=None): dc=wx.PaintDC(self.panel) dc.DrawBitmap(self.bitmap00) dc.Destroy() if __name__==""__main__"": app=wx.PySimpleApp() my_frame=MyFrame(parent=Noneid=-1) app.MainLoop() Ok I tested with newer version of wx(2.8.9.2) and Now I wonder why it is even working on your side. you are trying to paint Panel but overriding the paint event of Frame instead do this self.panel.Bind(wx.EVT_PAINT self.on_paint) and all will be fine Sounded sensible but I tried it and the lines still come out gray. I changed `self.Bind(wx.EVT_PAINT self.on_paint)` to `self.panel.Bind(wx.EVT_PAINT self.on_paint)`. I am using 2.8.9.2 as well on Python 2.6 on Windows XP right still gray ...  Beside the frame/panel paint problem already pointed out the color problem is due to the alpha channel of the 32 bit bitmap. I remember having read to use wx.GCDC instead of wx.DC."
1096,A,"What is a dependent texture read? I've been reading papers on computer graphics and every so often I come across the term ""dependent texture read"" or ""dependent texture fetch"" used in the context of querying textures in shader code. What is a dependent texture read and what is the difference between this and a ""normal"" texture read? A ""dependent texture read"" is when return values from one texture lookup are used to determine WHERE to look up from a second texture. An important implication is that the texture coordinates (where you look up from) is not determined until the middle of execution of the shader... there's no kind of static analysis you can do on the shader (even knowing the values of all parameters) that will tell you what the coordinates will be ahead of time. It also strictly orders the two texture reads and limits how much the execution order could be changed by optimizations in the driver etc. On older graphics cards there used to be quite a few limitations on this kind of thing. For example at one point (IIRC) you could look up from multiple textures but only with a small number of distinct texture coordinates. The hardware was actually implemented in a way that certain types of dependent texture reads were either impossible or very inefficient. In the latest generation or two of cards you shouldn't have to worry about this. But you may be reading books or articles from a couple years ago when you really did have to pay close attention to such things. Actually according PowerVR you do need to pay attention on some fairly recent hardware :-) http://bit.ly/yvSnM On my GTX 570 dependent texture lookups in the fragment shader are an order of magnitude slower than independent texture lookups. (This pushed the engine I was working on from zero/choking FPS to 20 FPS.) I experienced a slightly-less-massive speedup removing a dependent ""constant-array lookup"" in the geometry shader. (The primitive count was dependent on this value.) Context is everything!  A normal texture read just uses the texture coordinates passed into the shader. A dependent texture read is with texture coordinates that were calculated in part using values fetched from another texture previously accessed in the shader. Thus the second texture access is ""dependent"" on the value fetched from the first."
1097,A,"C# OnPaint mousemove high cpu usage I have written a custom control that renders some graphics. The graphics itself is fairly expensive to render but once rendered it rarely changes. The problem I have is that if you move the mouse really fast over the graphics surface it keeps calling the control's overridden Paint method which then incurs a high CPU penalty: private void UserControl1_Paint(object sender PaintEventArgs e) What techniques could be used to avoid this situation or minimise any unnecessary redrawing since the graphics/image underneath the mouse pointer is not actually changing? thanks for the answers so far. I should have added that I use control.Invalidate when I know something has changed. The Paint method is being called by .NET (not me!) when the mouse moves. I think that we need to see some code in order to continue here. OnPaint is not called when the mouse moves over a control something else is going on. I just tested with a simple project to be sure and the Paint is not called by mouse move unless I call Invalidate in OnMouseMove (as others have mentioned here). Are you sure there isn't some other condition in code that makes the calls to invalidate when they shouldn't be called? Try to add some logging code to log the calls to invalidate and mouse move and see if they are called as you intend to. Also check the Clip Rectange in the PaintArgs event argument so that you only repaint the area that needs to get updated instead of re-drawing the entire image.  One of the things that can cause high CPU load in Paint method is improper (unmanaged) resource release. Make sure you do Dispose() all of the pens brushes and so forth (probably all of the System.Drawing classes instances has some unmanaged resource tied to them). Basically you should Dispose() of the object as soon as you have finished working with it. Do not cache or anything - GDI+ resources are system resources and should be returned to system as soon as possible. Acquiring them (creating new Brush class instance for example) should be pretty quick but I do not have anything to back this statement up at the moment.  You can use a buffer image on which to draw when something is changed and in the Paint method just copy the image on the screen. Should be pretty fast. Also you can use the clip region to copy only the part that needs updating. This should reduce CPU usage. You can also use an IsDirty flag to know when to update the buffer image (i.e. fully redraw it) if needed. this is currently what I do. However I was wondering if there is a way to stop the environment calling Paint at all when the mouse moves. OnPaint is not called when the cursor moves over a control.  You should not call Paint directly. Instead call Invalidate (Control.Invalidate). This queues the need to be repainted and Windows will take care of the call itself. This way a lot of rapid invalidations (repaint requests) can be serviced by one call to Paint.  EDIT: After seeing your edit I can assure you that OnPaint is not called by default when the mouse moves over a control. Something in your code is definitely causing the re-paint you just don't see it yet. Perhaps posting some of your code would help us find the problem. Are you invalidating your control on MouseMove? That is likely a bad idea and if you really needed to do it (i.e. you are making a graphics editor or something) you would have to be smart about how large a region was actually re-drawn. So solution; don't paint your control in MouseMove. Otherwise I would not expect OnPaint to fire when the mouse moves over the control. You can also just generate the image once and then blt it to the Graphics object until it needs to be re-generated. thanks. You must be correct. I am actually overriding another Control's behaviour so it might be doing something naughty under the sheets that is causing the mousemove to raise an invalidate  You can use this code to supress and resume redrawing of a control :] Good luck.  using System; using System.Windows.Forms; using System.Collections.Generic; using System.Text; using System.Runtime.InteropServices; namespace pl.emag.audiopc.gui { // ************************************************************************ //`enter code here` // ************************************************************************ public class PaintingHelper { // ******************************************************************** // // ******************************************************************** public static void SuspendDrawing(Control parent) { SendMessage(parent.Handle WM_SETREDRAW false 0); } // ******************************************************************** // // ******************************************************************** public static void ResumeDrawing(Control parent) { SendMessage(parent.Handle WM_SETREDRAW true 0); parent.Refresh(); } // ******************************************************************** // // ******************************************************************** [DllImport(""user32.dll"")] private static extern int SendMessage(IntPtr hWnd Int32 wMsg bool wParam Int32 lParam); // ******************************************************************** // // ******************************************************************** private const int WM_SETREDRAW = 11; } }"
1098,A,Fast Vector rendering library for C or Python I'm looking for a library like Cairo just far faster. It has to be a library that works with C or Python. It also would be nice if I could output to PNG and SVG. I am looking at Qt's arthur but that's C++ and I'm not a fan. Any suggestions? Edit: another precondition is that it has to run under Linux. OpenGL? It can do 2D pretty well. :)  Python has aggdraw Aggdraw is based off AGG? (Anti-grain-graphics). Last time I checked that was a tiny bit slower than Cairo.  Google's Chrome browser and Android platform make use of their Skia vector library. I heard second-hand that Vladimir Vukicevic has quickly ported Cairo to be able to use Skia. A quick googling seems to confirm it: http://people.mozilla.com/~vladimir/misc/cairo-skia.patch Not sure when or if this is mainstream but I'd anticipate a major speed-up across the board! Do you have a link to the Skia library's home page? afraid not Sargun but the code is online and there's some background at http://www.satine.org/archives/2008/09/02/skia-source-code-released/ I've heard its good at gradient fills and things and I've seen in the code it has a GL backend for HW acceleration. Whats good on phones is also good on PCs Google Skia homepage is http://code.google.com/p/skia  I don't know what's fast but here's a rendering comparison. Edit: Apparently Xara is supposed to be much faster than Cairo. Unfortunately essential elements of Xara's renderer are not open-source. Great app but their Linux effort seems to have died sadly.
1099,A,How do I change the Unit:Characters in Matlab? For portability I set the units of my GUIs to 'characters'. Now I have a user who wants to use Matlab on his netbook and the GUI window is larger than the screen (and thus cropped at the top). I think I could try and write something in the openingFcn of the GUI that measures screen size and then adjusts the GUI accordingly but I'd rather avoid that because I then need to deal with text that is bigger than the text boxes etc. What I'd rather like to do is somehow adjust the unit 'character' on his Matlab installation. None of the font sizes in the preferences seem to have an effect on unit:character though. Does anyone know whether there's a setting for that which can be changed from within Matlab (I don't mind if it's something that is reset at every reboot since I can just put it into the startup script)? Might I suggest an alternative to consider when designing your GUI: Create all of your GUI objects with the 'FontUnits' property set to 'normalized'. Create the figure with a default size with everything set to look the way you want. Set one or more of the CreateFcn/OpeningFcn/ResizeFcn functions so they will resize the GUI to fit the screen size. When the GUI and its objects are resized the text will resize accordingly thus helping to avoid text that ends up bigger than the text boxes. One thing to keep note of is that normalized units for the font will interpret the value of 'FontSize' property as a fraction of the height of the uicontrol. I also make it a habit to set the 'FontName' property to 'FixedWidth' to help control the width of the text. Setting FontSize to 'normalized' programmatically for all the uicontrols in the GUI crashes Matlab for me. I'm setting the font to something small until I figure out what's going on and it works. Thanks. Ack! I was afraid you'd say that I have to rework the GUI. Can't I just fix a setting somewhere instead? Please? Anyway +1 for suggesting a workable plan B.
1100,A,DOS EGA Graphics Programming in Mode 0Dh I'm doing a bit of retro programming for fun. What I want to create is a DOS game using EGA graphics but I am having a bit of trouble finding a good reference on the web. Everybody who talks about doing DOS programming assumes that the programmer will use mode 13h and although some pages mention the other graphics modes I haven't found one yet that discusses their proper use. Here's what I'm trying to get working right now: //------------------------------------------------------------------------------ // DOS graphics test // // Thanks to the following links: // http://gamebub.com/cpp_graphics.php // // Written for Digital Mars C compiler to be compiled as a DOS 16 bit binary. //------------------------------------------------------------------------------ #include <dos.h> #include <stdio.h> #define SCREEN_WIDTH 320; #define SCREEN_HEIGHT 200; unsigned char far *vram = (unsigned char far *)0xA0000000L; //------------------------------------------------------------------------------ void set_video_mode(unsigned char mode) { union REGS in out; in.h.ah = 0; in.h.al = mode; int86(0x10 &in &out); } //------------------------------------------------------------------------------ void plot_pixel(unsigned int x unsigned int y unsigned char color) { // this is wrong because it's only 4 bpp not 8 vram[y * 320 + x] = color; //vram[((y<<8)+(y<<6))+x] = color; } //------------------------------------------------------------------------------ int main(int argc char* argv[]) { // EGA 320 x 200 x 16 set_video_mode(0x0d); for (unsigned char i = 0; i < 255; i++) { vram[i] = i; } //plot_pixel(10 10 1); getc(stdin); return 0; } This sample code works great if you change set_video_mode() to take 0x13 instead of 0x0d but like I said I'm trying to get EGA graphic here not VGA. :) I realize that in order to do four bits-per-pixel I'm going to either need to assume that plot_pixel writes two pixels at the same time or do a some bit twiddling to make sure I only write the four bits that I actually want. My problem is that I'm not seeing what I expect as output--in particular no colors! Everything seems to be mono colored which is not what I want at all. Is there some kind of different procedure for employing a color palette in this graphics mode than in 13h? Or have I somehow invoked a completely different graphics mode from the one I intended? Guidance would be very much appreciated. I don't think my compiler args would be relevant but just in case: ..\dm\bin\dmc test.c -o test -mm Mode 0x0d is planar based while 0x13 is not (without special configuration as Mode X). You should check more document to see how to work in planar mode. Thanks for the tip! I didn't know about planar graphics so this will help. I would really like to check the docs but part of the problem I have here is that I have yet to find good documentation on this subject. :) Try http://www.phatcode.net/res/224/files/html/index.html . It is very helpful for both VGA and EGA because the planar mode idea is the same on CGA EGA and VGA. This explains it well too : http://www.shikadi.net/moddingwiki/Raw_EGA_data  Hmmm..dunno if this is a worthy answer if I recall you need to use inportinportboutportoutportb to the ports to control the EGA registers this was taken from the excellent FAQSYSW (a vast collection of materials ranging from graphics demo maths fractals to compressions) the original archive was found here and here.:  3C0h: Attribute Controller: Address register bit 0-4 Address of data register to write to port 3C0h. 5 If set screen output is enabled and the palette can not be modified if clear screen output is disabled and the palette can be modified. Port 3C0h is special in that it is both address and data-write register. An internal flip-flop remembers whether it is currently acting as address or data register. Accesses to the attribute controller must be separated by at least 250ns. Reading port 3dAh will reset the flip-flop to address mode. 3C0h index 0-Fh (W): Attribute: Palette bit 0 Primary Blue 1 Primary Green 2 Primary Red 3 Secondary Blue 4 Secondary Green 5 Secondary Red 3C0h index 10h (W): Attribute: Mode Control Register bit 0 Graphics mode if set Alphanumeric mode else. 1 Monochrome mode if set color mode else. 2 9-bit wide characters if set. The 9th bit of characters C0h-DFh will be the same as the 8th bit. Otherwise it will be the background color. 3 If set Attribute bit 7 is blinking else high intensity. 3C0h index 11h (W): Attribute: Overscan Color Register. bit 0-5 Color of screen border. Color is defined as in the palette registers. Note: The EGA requires the Overscan color to be 0 in high resolution modes. 3C0h index 12h (W): Attribute: Color Plane Enable Register bit 0 Bit plane 0 is enabled if set. 1 Bit plane 1 is enabled if set. 2 Bit plane 2 is enabled if set. 3 Bit plane 3 is enabled if set. 4-5 Video Status MUX. Diagnostics use only. Two attribute bits appear on bits 4 and 5 of the Input Status Register 1 (3dAh). 0: Red/Blue 1: Blue(I)/Green 2: Red(I)/Green(I) 3C0h index 13h (W): Attribute: Horizontal PEL Panning Register bit 0-3 Indicates number of pixels to shift the display left Value 9bit textmode 256color mode Other modes 0 1 0 0 1 2 n/a 1 2 3 1 2 3 4 n/a 3 4 5 2 4 5 6 n/a 5 6 7 3 6 7 8 n/a 7 8 0 n/a n/a 3C2h (R): Input Status #0 Register bit 4 Status of the switch selected by the Miscellaneous Output Register 3C2h bit 2-3. Switch high if set. 5 Pin 19 of the Feature Connector (FEAT0) is high if set 6 Pin 17 of the Feature Connector (FEAT1) is high if set 7 If set IRQ 2 has happened due to Vertical Retrace. Should be cleared by IRQ 2 interrupt routine by clearing port 3d4h index 11h bit 4. 3C2h (W): Miscellaneous Output Register bit 0 If set Color Emulation. Base Address=3Dxh else Mono Emulation. Base Address=3Bxh. 1 Enable CPU Access to video memory if set 2-3 Clock Select. 0: 14MHz 1: 16MHz 2: External 4 Disable internal video drivers if set 5 When in Odd/Even modes Select High 64k bank if set 6 Horizontal Sync Polarity. Negative if set 7 Vertical Sync Polarity. Negative if set Bit 6-7 indicates the number of lines on the display: 0: 200 lines 2: 350 lines Note: Set to all zero on a hardware reset. 3C4h index 0 (W): Sequencer: Reset bit 0 Asynchronous Reset if clear 1 Synchronous Reset if clear 3C4h index 1 (W): Sequencer: Clocking Mode bit 0 If set character clocks are 8 dots wide else 9. 1 If set the CRTC uses 2/5 of the clock cycles else 4/5. 2 If set loads video serializers every other character clock cycle else every one. 3 If set the Dot Clock is Master Clock/2 else same as Master Clock (See 3C2h bit 2-3). (Doubles pixels). 3C4h index 2 (W): Sequencer: Map Mask Register bit 0 Enable writes to plane 0 if set 1 Enable writes to plane 1 if set 2 Enable writes to plane 2 if set 3 Enable writes to plane 3 if set 3C4h index 3 (W): Sequencer: Character Map Select Register bit 0-1 Selects EGA Character Map (0..3) if bit 3 of the character attribute is clear. 2-3 Selects EGA Character Map (0..3) if bit 3 of the character attribute is set. Note: Character Maps are placed as follows: Map 0 at 0k map 1 at 16k map 2 at 32k and map 3 at 48k 3C4h index 4 (W): Sequencer: Memory Mode Register bit 0 Set if in an alphanumeric mode clear in graphics modes. 1 Set if more than 64kbytes on the adapter. 2 Enables Odd/Even addressing mode if set. Odd/Even mode places all odd bytes in plane 1&3 and all even bytes in plane 0&2. 3CAh (W): Graphics 2 Position bit 0-1 Select which bit planes should be controlled by Graphics Controller #2. Always set to 1. 3CCh (W): Graphics 1 Position bit 0-1 Select which bit planes should be controlled by Graphics Controller #1. Always set to 0. 3CEh index 0 (W): Graphics: Set/Reset Register bit 0 If in Write Mode 0 and bit 0 of 3CEh index 1 is set a write to display memory will set all the bits in plane 0 of the byte to this bit if the corresponding bit is set in the Map Mask Register (3CEh index 8). 1 Same for plane 1 and bit 1 of 3CEh index 1. 2 Same for plane 2 and bit 2 of 3CEh index 1. 3 Same for plane 3 and bit 3 of 3CEh index 1. 3CEh index 1 (W): Graphics: Enable Set/Reset Register bit 0 If set enables Set/reset of plane 0 in Write Mode 0. 1 Same for plane 1. 2 Same for plane 2. 3 Same for plane 3. 3CEh index 2 (W): Graphics: Color Compare Register bit 0-3 In Read Mode 1 each pixel at the address of the byte read is compared to this color and the corresponding bit in the output set to 1 if they match 0 if not. The Color Don't Care Register (3CEh index 7) can exclude bitplanes from the comparison. 3CEh index 3 (W): Graphics: Data Rotate bit 0-2 Number of positions to rotate data right before it is written to display memory. Only active in Write Mode 0. 3-4 In Write Mode 2 this field controls the relation between the data written from the CPU the data latched from the previous read and the data written to display memory: 0: CPU Data is written unmodified 1: CPU data is ANDed with the latched data 2: CPU data is ORed with the latch data. 3: CPU data is XORed with the latched data. 3CEh index 4 (W): Graphics: Read Map Select Register bit 0-1 Number of the plane Read Mode 0 will read from. 3CEh index 5 (W): Graphics: Mode Register bit 0-1 Write Mode: Controls how data from the CPU is transformed before being written to display memory: 0: Mode 0 works as a Read-Modify-Write operation. First a read access loads the data latches of the EGA with the value in video memory at the addressed location. Then a write access will provide the destination address and the CPU data byte. The data written is modified by the function code in the Data Rotate register (3CEh index 3) as a function of the CPU data and the latches then data is rotated as specified by the same register. 1: Mode 1 is used for video to video transfers. A read access will load the data latches with the contents of the addressed byte of video memory. A write access will write the contents of the latches to the addressed byte. Thus a single MOVSB instruction can copy all pixels in the source address byte to the destination address. 2: Mode 2 writes a color to all pixels in the addressed byte of video memory. Bit 0 of the CPU data is written to plane 0 et cetera. Individual bits can be enabled or disabled through the Bit Mask register (3CEh index 8). 2 Forces all outputs to a high impedance state if set. 3 Read Mode 0: Data is read from one of 4 bit planes depending on the Read Map Select Register (3CEh index 4). 1: Data returned is a comparison between the 8 pixels occupying the read byte and the color in the Color Compare Register (3CEh index 2). A bit is set if the color of the corresponding pixel matches the register. 4 Enables Odd/Even mode if set (See 3C4h index 4 bit 2). 5 Enables CGA style 4 color pixels using even/odd bit pairs if set. 3CEh index 6 (W): Graphics: Miscellaneous Register bit 0 Indicates Graphics Mode if set Alphanumeric mode else. 1 Enables Odd/Even mode if set. 2-3 Memory Mapping: 0: use A000h-BFFFh 1: use A000h-AFFFh EGA Graphics modes 2: use B000h-B7FFh Monochrome modes 3: use B800h-BFFFh CGA modes 3CEh index 7 (W): Graphics: Color Don't Care Register bit 0 Ignore bit plane 0 in Read mode 1 if clear. 1 Ignore bit plane 1 in Read mode 1 if clear. 2 Ignore bit plane 2 in Read mode 1 if clear. 3 Ignore bit plane 3 in Read mode 1 if clear. 3CEh index 8 (W): Graphics: Bit Mask Register bit 0-7 Each bit if set enables writing to the corresponding bit of a byte in display memory. 3d4h index 0 (W): CRTC: Horizontal Total Register bit 0-7 Horizontal Total Character Clocks-2 3d4h index 1 (W): CRTC: Horizontal Display End Register bit 0-7 Number of Character Clocks Displayed -1 3d4h index 2 (W): CRTC: Start Horizontal Blanking Register bit 0-7 The count at which Horizontal Blanking starts 3d4h index 3 (W): CRTC: End Horizontal Blanking Register bit 0-4 Horizontal Blanking ends when the last 5 bits of the character counter equals this field. 5-6 Number of character clocks to delay start of display after Horizontal Total has been reached. 3d4h index 4 (W): CRTC: Start Horizontal Retrace Register bit 0-7 Horizontal Retrace starts when the Character Counter reaches this value. 3d4h index 5 (W): CRTC: End Horizontal Retrace Register bit 0-4 Horizontal Retrace ends when the last 5 bits of the character counter equals this value. 5-6 Number of character clocks to delay start of display after Horizontal Retrace. 7 Provides Smooth Scrolling in Odd/Even mode. When set display starts from an odd byte. 3d4h index 6 (W): CRTC: Vertical Total Register bit 0-7 Lower 8 bits of the Vertical Total. Bit 8 is found in 3d4h index 7 bit 0. 3d4h index 7 (W): CRTC: Overflow Register bit 0 Bit 8 of Vertical Total (3d4h index 6) 1 Bit 8 of Vertical Display End (3d4h index 12h) 2 Bit 8 of Vertical Retrace Start (3d4h index 10h) 3 Bit 8 of Start Vertical Blanking (3d4h index 15h) 4 Bit 8 of Line Compare Register (3d4h index 18h) 3d4h index 8 (W): CRTC: Preset Row Scan Register bit 0-4 Number of lines we have scrolled down in the first character row. Provides Smooth Vertical Scrolling. 3d4h index 9 (W): CRTC: Maximum Scan Line Register bit 0-4 Number of scan lines in a character row -1. In graphics modes this is the number of times (-1) the line is displayed before passing on to the next line (0: normal 1: double 2: triple...). This is independent of bit 7 except in CGA modes which seems to require this field to be 1 and bit 7 to be set to work. 3d4h index 0Ah (W): CRTC: Cursor Start Register bit 0-4 First scanline of cursor within character. 3d4h index 0Bh (W): CRTC: Cursor End Register bit 0-4 Last scanline of cursor within character 5-6 Delay of cursor data in character clocks. 3d4h index 0Ch (W): CRTC: Start Address High Register bit 0-7 Upper 8 bits of the start address of the display buffer 3d4h index 0Dh (W): CRTC: Start Address Low Register bit 0-7 Lower 8 bits of the start address of the display buffer 3d4h index 0Eh (W): CRTC: Cursor Location High Register bit 0-7 Upper 8 bits of the address of the cursor 3d4h index 0Fh (W): CRTC: Cursor Location Low Register bit 0-7 Lower 8 bits of the address of the cursor 3d4h index 10h (R): CRTC: Light Pen High Register bit 0-7 Upper 8 bits of the address of the lightpen position. 3d4h index 10h (W): CRTC: Vertical Retrace Start Register bit 0-7 Lower 8 bits of Vertical Retrace Start. Vertical Retrace starts when the line counter reaches this value. Bit 8 is found in 3d4h index 7 bit 2. 3d4h index 11h (R): CRTC: Light Pen Low Register bit 0-7 Lower 8 bits of the address of the lightpen position. 3d4h index 11h (W): CRTC: Vertical Retrace End Register bit 0-3 Vertical Retrace ends when the last 4 bits of the line counter equals this value. 4 if clear Clears pending Vertical Interrupts. 5 Vertical Interrupts (IRQ 2) disabled if set. Can usually be left disabled but some systems (including PS/2) require it to be enabled. 3d4h index 12h (W): CRTC: Vertical Display End Register bit 0-7 Lower 8 bits of Vertical Display End. The display ends when the line counter reaches this value. Bit 8 is found in 3d4h index 7 bit 1. 3d4h index 13h (W): CRTC: Offset register bit 0-7 Number of bytes in a scanline / K. Where K is 2 for byte mode 4 for word mode and 8 for Double Word mode. 3d4h index 14h (W): CRTC: Underline Location Register bit 0-4 Position of underline within Character cell. 3d4h index 15h (W): CRTC: Start Vertical Blank Register bit 0-7 Lower 8 bits of Vertical Blank Start. Vertical blanking starts when the line counter reaches this value. Bit 8 is found in 3d4h index 7 bit 3. 3d4h index 16h (W): CRTC: End Vertical Blank Register bit 0-4 Vertical blanking stops when the lower 5 bits of the line counter equals this field. 3d4h index 17h (W): CRTC: Mode Control Register bit 0 If clear use CGA compatible memory addressing system by substituting character row scan counter bit 0 for address bit 13 thus creating 2 banks for even and odd scan lines. 1 If clear use Hercules compatible memory addressing system by substituting character row scan counter bit 1 for address bit 14 thus creating 4 banks. 2 If set increase scan line counter only every second line. 3 If set increase memory address counter only every other character clock. 4 If set disable the EGA output drivers. 5 When in Word Mode bit 15 is rotated to bit 0 if this bit is set else bit 13 is rotated into bit 0. 6 If clear system is in word mode. Addresses are rotated 1 position up bringing either bit 13 or 15 into bit 0. 7 Clearing this bit will reset the display system until the bit is set again. 3d4h index 18h (W): CRTC: Line Compare Register bit 0-7 Lower 8 bits of the Line Compare. When the Line counter reaches this value the display address wraps to 0. Provides Split Screen facilities. Bit 8 is found in 3d4h index 7 bit 4. 3dAh (R): Input Status #1 Register bit 0 Either Vertical or Horizontal Retrace active if set 1 Light Pen has triggered if set 2 Light Pen switch is open if set 3 Vertical Retrace in progress if set 4-5 Shows two of the 6 color outputs depending on 3C0h index 12h. Attr: Bit 4-5: Out bit 4 Out bit 5 0 Blue Red 1 I Blue Green 2 I Red I Green 3dAh (W): Feature Control Register bit 0 Output to pin 21 of the Feature Connector. 1 Output to pin 20 of the Feature Connector.
1101,A,Rotate string to write vertically on bitmap I'm building a Bitmap by layering images one above another and when I'm done I want to write text around the edges. The top and bottom are simple because they're written horizontally but I'd prefer to write the text on the left and right sides vertically so they don't take up as much space. The Graphics.DrawString method doesn't allow you to specify an angle of rotation; what other methods exist? You can use a logical font for this. But it's a pain - you're better off using Fredrik's answer (unless you're doing this in the Compact Framework where RotateTransform isn't available).  Maybe you can rotate the bitmap 90 degrees and write the text to the top of the bitmap - then rotate again and write the next side's text - this would give you text running CW/CCW around the edges. If you want it horizontally across the top and bottom and vertically on the left and right I suggest measuring (or assumning) the size of the largest character you need to write and then use this to position the drawn text - one character at a time - first on the left side then the right then moving down one char and repeating. You could of course just right the left side in totality and then the right side. just use the width of the char to inset from the right side and the height of the char to offset vertically between chars. e.g. 1 2 OR 1 4 GIVING C D 3 4 2 5 A O 5 6 3 6 T G Probably not the most elegant solution but may help you.  I think you may get some pointers from this answer about rotating text for printing that I wrote a while ago.  Here is a tutorial that may help http://www.c-sharpcorner.com/Blogs/BlogDetail.aspx?BlogId=580 I believe StringFormatFlags.DirectionVertical is what you are looking for
1102,A,c# fill everything but GraphicsPath I have some code that looks like that: GraphicsPath p = new GraphicsPath(); // Add some elements to p ... // ... g.FillPath(bgBrush p); // where g : Graphics and bgBrush : Brush Which results in something that looks like this:  ### | ##| ## | How can I fill the exact complement of the path? Desired output:  #| ## | # #| I'm surprised no one answered so far - the solution is pretty simple. Just add a Rectangle to the path and everything inside gets reverted.  protected override void OnPaint(PaintEventArgs e) { base.OnPaint(e); Graphics g = e.Graphics; g.Clear(BackColor); GraphicsPath p = new GraphicsPath(); // add the desired path p.AddPie(100 100 100 100 0 270); // add the rectangle to invert the inside p.AddRectangle(new Rectangle(50 50 200 200)); g.FillPath(Brushes.Black p); } +1 I always wondered why there aren't any GraphicsPath.Remove*** methods :). A simple but not exactly logical way 8-) Thanks!
1103,A,Spinning a circle in J2ME using a Canvas I have a problem where I need to make a multi-colored wheel spin using a Canvas in J2ME. What I need to do is have the user increase the speed of the spin or slow the spin of the wheel. I have it mostly worked out (I think) but can't think of a way for the wheel to spin without causing my cellphone to crash. Here is what I have so far it's close but not exactly what I need. class MyCanvas extends Canvas{ //wedgeOne/Two/Three define where this particular section of circle begins to be drawn from int wedgeOne; int wedgeTwo; int wedgeThree; int spinSpeed; MyCanvas(){ wedgeOne = 0; wedgeTwo = 120; wedgeThree = 240; spinSpeed = 0; } //Using the paint method to public void paint(Graphics g){ //Redraw the circle with the current wedge series. g.setColor(25500); g.fillArc(getWidth()/2 getHeight()/2 100 100 wedgeOne 120); g.setColor(02550); g.fillArc(getWidth()/2 getHeight()/2 100 100 wedgeTwo 120); g.setColor(00255); g.fillArc(getWidth()/2 getHeight()/2 100 100 wedgeThree 120); } protected void keyPressed(int keyCode){ switch (keyCode){ //When the 6 button is pressed the wheel spins forward 5 degrees. case KEY_NUM6: wedgeOne += 5; wedgeTwo += 5; wedgeThree += 5; repaint(); break; //When the 4 button is pressed the wheel spins backwards 5 degrees. case KEY_NUM4: wedgeOne -= 5; wedgeTwo -= 5; wedgeThree -= 5; repaint(); } } I have tried using a redraw() method that adds the spinSpeed to each of the wedge values while(spinSpeed>0) and calls the repaint() method after the addition but it causes a crash and lockup (I assume due to an infinite loop). Does anyone have any tips or ideas how I could automate the spin so you do not have the press the button every time you want it to spin? (P.S - I have been lurking for a while but this is my first post. If it's too general or asking for too much info (sorry if it is) and I either remove it or fix it. Thank you!) I think you should create a thread and in the run method of the thread call this paint/repaint method. If you want the keypressed to run the thread than it simply : case KEY_NUM6: thread.start; This is the problem that I was having I didn't realize that all of the animation had to be done in a seperate thread.  given that you haven't had a response yet I thought I'd try to contribute even though my experience isn't in J2ME. I have some practice in Java2D which will likely be possible on J2ME although someone feel free to tell me if otherwise! If you are able to use a JPanel you could design a slightly game-cycle like design similar to one I have used in the past. The idea: at a given interval interogate each entity that must be drawn (in this case the wedges) update it to find the new position and ask it to paint itself. To demonstrate this here is a fairly stripped out version of a GamePanel I've used before- public class GamePanel extends JPanel { Timer loopTimer; boolean changed = false; ArrayList<Entity> entities = new ArrayList<Entity>(); int spinSpeed = 1; int pollingMillis = 20; public GamePanel () { setBackground(Color.black); setDoubleBuffered(true); entities.add(new Wedge(00090Color.red)); // See below about these lines entities.add(new Wedge(009090Color.blue)); entities.add(new Wedge(0018090Color.yellow)); entities.add(new Wedge(0027090Color.green)); loopTimer = new Timer(); loopTimer.scheduleAtFixedRate(new runLoop() 0 pollingMillis); } public void addEntity(Entity e) { entities.add(e); } @Override public void paint (Graphics graph) { super.paint(graph); Graphics2D g = (Graphics2D) graph; for (Entity e: entities) { e.draw(g); } g.dispose(); } private class runLoop extends TimerTask { @Override public void run() { for (Entity e: entities) { e.update(); } if (changed) { repaint(); changed = false; } } } } This will create the basic cycle that will allow you to render items into the JPanel. The code above has 4 'wedges' being added although you need a bit more code yet for that. To do this you need the following abstract class: public abstract class Entity { int xy; public Entity (int x int y) { this.x = x; this.y = y; } public abstract void update(); public abstract void draw(Graphics2D g); } Now the entities you want to render extend this class allowing them to be added to the render cycle. For example the wedge class- private class Wedge extends Entity { int startAngle; int arcLength; Color color;  public Wedge (int x int y int start int length Color color) { super(xy); startAngle = start; arcLength = length; this.color = color; } @Override public void update() { startAngle -= spinSpeed; changed = true; } @Override public void draw(Graphics2D g) { g.setColor(color); g.fillArc(x y 200 200 startAngle arcLength); } } This should then allow you to render an animated circle that spins! I hope that some of this is tranferrable over to J2ME and I also hope you appreciate that this code may be far from optimal and if you wish for me to expand on any of it just comment. Thank you for the answer but J2ME does not have a lot of the methods and classes mentioned above. Though you have given me an idea on how to tackle this problem I think. Thank you! No problem although if you'd like I'm happy to try and help further- just tell me which classes you don't get in the J2ME environment and I can try to tailor the solution better using alternative methods!
1104,A,"GWT Graphics - resetting the text I am working on a project for which i am using GWT-Graphics. I have drawing Area containing ellipse and text. When i double click on them a pop-up menu appears and gives an option to enter text. Now when i save this text i want it to appear on this drawing area in the place of the previous text. I am trying to do this but with no luck. It keeps the old text and on top of that it includes the current text. Is there a way to have only the new text to appear. Any input will be of great help. Thank you. For me this sounds like that you are adding a new Text element instead of using the existing one. I wrote a quick test and it seems to work like you want: public class GwtTest2 implements EntryPoint { private Text text; public void onModuleLoad() { DrawingArea da = new DrawingArea(400 400); RootPanel.get().add(da); da.addClickHandler(new ClickHandler() { @Override public void onClick(ClickEvent event) { String newTextValue = Window.prompt("""" """"); text.setText(newTextValue); } }); Ellipse ellipse = new Ellipse(200 200 100 50); da.add(ellipse); text = new Text(150 200 ""Hello world!""); da.add(text); } } Hi my project has more than one drawing area and i have to change text on those at different situations. From your example i saw what i was doing wrong. Thank you."
1105,A,"How to output a String on multiple lines using Graphics My Program overrides public void paint(Graphics g int x int y); in order to draw some stings using g.drawString(someString x+10 y+30); Now someString can be quite long and thus it may not fit on one line. What is the best way to write the text on multiple line. For instance in a rectangle (x1 y1 x2 y2)? Had some trouble myself this is my solution: Graphics2D g=.... FontRenderContext frc = g.getFontRenderContext(); TextLayout layout = new TextLayout(text font frc); String[] outputs = text.split(""\n""); for(int i=0; i<outputs.length; i++){ g.drawString(outputs[i] 15(int) (15+i*layout.getBounds().getHeight()+0.5)); Hope that helps.... simple but it works.  Thanks to Epaga's hint and a couple of examples on the Net (not so obvious to find! I used mainly Break a Line for text layout) I could make a component to display wrapped text. It is incomplete but at least it shows the intended effect. class TextContainer extends JPanel { private int m_width; private int m_height; private String m_text; private AttributedCharacterIterator m_iterator; private int m_start; private int m_end; public TextContainer(String text int width int height) { m_text = text; m_width = width; m_height = height; AttributedString styledText = new AttributedString(text); m_iterator = styledText.getIterator(); m_start = m_iterator.getBeginIndex(); m_end = m_iterator.getEndIndex(); } public String getText() { return m_text; } public Dimension getPreferredSize() { return new Dimension(m_width m_height); } public void paint(Graphics g) { super.paintComponent(g); Graphics2D g2 = (Graphics2D) g; FontRenderContext frc = g2.getFontRenderContext(); LineBreakMeasurer measurer = new LineBreakMeasurer(m_iterator frc); measurer.setPosition(m_start); float x = 0 y = 0; while (measurer.getPosition() < m_end) { TextLayout layout = measurer.nextLayout(m_width); y += layout.getAscent(); float dx = layout.isLeftToRight() ? 0 : m_width - layout.getAdvance(); layout.draw(g2 x + dx y); y += layout.getDescent() + layout.getLeading(); } } } Just for fun I made it fitting a circle (alas no justification it seems): public void paint(Graphics g) { super.paintComponent(g); Graphics2D g2 = (Graphics2D) g; FontRenderContext frc = g2.getFontRenderContext(); LineBreakMeasurer measurer = new LineBreakMeasurer(m_iterator frc); measurer.setPosition(m_start); float y = 0; while (measurer.getPosition() < m_end) { double ix = Math.sqrt((m_width / 2 - y) * y); float x = m_width / 2.0F - (float) ix; int width = (int) ix * 2; TextLayout layout = measurer.nextLayout(width); y += layout.getAscent(); float dx = layout.isLeftToRight() ? 0 : width - layout.getAdvance(); layout.draw(g2 x + dx y); y += layout.getDescent() + layout.getLeading(); } } I am not too sure about dx computation though.  You can use a JLabel and embed the text with html. JLabel.setText(""<html>""+line1+""<br>""+line2); Your HTML is getting stripped. I'm assuming the first string literal is and the second one is . Doesn't it defeat the purpose if you have to add your own linebreaks?  Incrementally build your string one word at a time using Epaga's method to find the length of your string. Once the length is longer than your rectangle remove the last word and print. Repeat until you run out of words. It sounds like a bad algorithm but for each line it's really O(screenWidth/averageCharacterWidth) => O(1). Still use a StringBuffer to build your string! This is not going to have a time complexity of O(1); O(n) perhaps as you need to (at least) add a word (i.e. add a number of chars in the StringBuilder.append) until all the chars have been used up. I seem to be bad at expressing myself properly... I meant that each line is O(1). You can throw 1000 words at it but for each line it'll stop adding words at the edge of the screen. Overall yes O(n). It works only if you suppose the string will be at max 2 lines when it will be more it won't work.  java.awt.font.TextLayout might be helpful. Here's a snippet of their example code:  Graphics2D g = ...; Point2D loc = ...; Font font = Font.getFont(""Helvetica-bold-italic""); FontRenderContext frc = g.getFontRenderContext(); TextLayout layout = new TextLayout(""This is a string"" font frc); layout.draw(g (float)loc.getX() (float)loc.getY()); Rectangle2D bounds = layout.getBounds(); bounds.setRect(bounds.getX()+loc.getX() bounds.getY()+loc.getY() bounds.getWidth() bounds.getHeight()); g.draw(bounds); Otherwise you could always use a Swing text element to do the job for you just pass in the Graphics you want it to paint into. I got that one already. My problem is I don't want a rectangle around the text but the text inside a given rectangle. Note that TextLayout's constructor undocumentedly throws an exception if the string you pass is empty."
1106,A,"Is there a nine-patch loader for iPhone? Android has a nice way of defining stretchable images called a nine-patch. See these docs for a description of the concept. The idea is to surround a png image with a 1-pixel border where you can define the stretchable areas and the padding dimensions of the image. This is absolutely brilliant and I'd like to use the idea in my iPhone app. Before writing my own nine-patch to UIImage loader I thought I'd see if one already exists. Google doesn't return any results so I don't have much hope but it doesn't hurt to ask right? :-) EDIT: Folks I appreciate the answers but I know about stretchableImageWithLeftCapWidth.... I'm looking for code that takes a path @""foo.9.png"" and returns a stretchable UIImage. This code will undoubtedly use stretchableImageWithLeftCapWidth... internally. I'm sure I could write the code myself using that method. But I'm asking if somebody else has already done it. Good idea. Do it and post the source! I have reported a feature request on this at http://bugreport.apple.com please do the same. The more people that report the same bug/feature request the more likely that Apple devote their time. Trust me it works. All UIImage images support this natively. By default the entire images is stretchable but you can set caps with the leftCapWidth and topCapHeight properties or you can generate one from an existing UIImage with the - (UIImage *)stretchableImageWithLeftCapWidth:(NSInteger)leftCapWidth topCapHeight:(NSInteger)topCapHeight method. Do note that in apple's implementation when you set one or both of these values the stretchable area is forced to be a single pixel high/wide.  Yes UIImage does support something like it. See - (UIImage *)stretchableImageWithLeftCapWidth:(NSInteger)leftCapWidth topCapHeight:(NSInteger)topCapHeight and the documentation for leftCapWidth and topCapHeight basically the image is not stretched in the area leftCapWidth pixels from the left and right edge and topCapHeight pixels from the top and the bottom. When the image is scaled the area inside of these limits is subject to stretching. I know about that method but it has the disadvantage of requiring the cap height/width to be stored separately from the image. What I like about nine-patch is how the image file has all the information necessary to describe its scaling behavior. You don't have to keep the values around. the method returns a new image with those values baked in. It's not about keeping the values around. It's about having a single file ""foo.9.png"" that stores both the image and the information for stretching it correctly. With 9-patch files I can replace an image resource in my project and know that it will stretch correctly without changing a line of code. With this method that's not the case -- I have to be sure I've adjusted the cap width/height values in my code.  Also look at UIView's contentStretch property. It is more robust and well-behaved than stretchableImageWithLeftCapWidth. Basically it works by just defining the stretchable rectangle within your image and automatically creating a scaled nine-patch. This internal rectangle can be anything - it doesn't even have to be in the center of the image. Plus unlike stretchableImage this method will properly shrink graphics and behave as expected for graphics with lighting or gloss. I can't think of any real-world application where you would want more than this. i always wondered how it'd be possible to use this implementation to set a background of a button. The Apple's stretchableImageBlablabla just doesn't cut it for more advanced buttons where you want to specify a rectangle to stretch rather than a single pixel in each direction. Any ideas on that? Agreed but stretchableImage... got even simpler in iOS 5 when the `(UIImage *)resizableImageWithCapInsets:(UIEdgeInsets)capInsets` was added - this allows the top bottom left and right non-stretchable insets to be specified separately.  I received an e-mail from Tortuga22 software who informed me that they have created such a library and released it under the Apache license: Announcement: http://blog.tortuga22.com/2010/05/31/announcing-tortuga-22-ninepatch/ Source code: http://github.com/tortuga22/Tortuga22-NinePatch Example usage: // loads-and-caches ninepatch and rendered image of requested size UIImage buttonImg = [TUNinePatchCache imageOfSize:buttonSize forNinePatchNamed:@""buttonNormalBackground""]; [self.buttonNeedingBackground setImage:buttonImg forControlState:UIControlStateNormal]; as far as I understand Tortuga’s comments on the drawbacks this library **only supports a stripped down version of ninepatches**. ninepatches with non-contigeous areas (such as speech bubbles with dynamically aligned [_e. g. centered_] arrows) are not supported. It also seems to me that this library only supports ""source"" versions of NinePatch (the one with 1 transparent pixel around the image). Hai gray i also used Tortuga22-NinePathit works in simulatorbut i install in device it shows error ""Undefined symbols for architecture armv7s:""_OBJC_CLASS_$_TUNinePatch"" referenced from:"".is any way NinePatch image run in device? @banu did you add TUNinePatch sources into compile sources section in xcode ?"
1107,A,"Convert text box text into Argb argument I have been looking into coloring objects like ellipses with code such as  SolidBrush trnsRedBrush = new SolidBrush(Color.FromArgb(0x78FF0000)); I'd like to play around further with this by entering FromArgb's argument into a textbox on a form then using the textbox to set the Brush's color. How would I convert the textbox's text into an argument usable by FromArgb? I don't know if you know it but there's also a built-in ColorDialog widget you can use too instead of typing hex numbers. If you are planning on entering hexadecimal values into the textbox why not just do: SolidBrush trnsRedBrush = new SolidBrush(Color.FromArgb(Convert.ToInt32(textBox.Text) 16)); Edit: Have to cast the value to an int first. (oops!) Where did you find documentation indicating that FromArgb would accept a string as a parameter? AaronLS you're right it doesn't. It would have to be parsed first as in your answer.  someTextBox.Text = ""AAFFBBDD""; int param = int.Parse(someTextBox.Text NumberStyles.AllowHexSpecifier); SolidBrush trnsRedBrush = new SolidBrush(Color.FromArgb(param)); You could shorten this of course. Edit: Keep in mind if you type something bad this could throw an exception. There are ""TryParse"" variants to allow you to handle the situation to your liking. I won't make a specific recommendation because it depends on the context and scenario. Worked. Thanks!"
1108,A,LaTeX: How to make a fullpage vertical rule on every page? I'm using LaTeX and I would like to have vertical rule along left side of page topmargin to bottommargin 0.5in from the left edge of the page. I want this on every page so I assume that means it must somehow be tied to the header or the footer? I've made no progress at all so I need help with (1) making the full-length rule itself and (2) making it happen automatically on every page of the document. Can someone tell me how to do that? Have a look at the eso-pic package. From memory what you want would look like this: \AddToShipoutPicture{% \setlength\unitlength{1in}% \AtPageUpperLeft{% \put(0.5\topmargin){\vrule width .5pt height \textheight}% }% } It's not clear in your question if you want the line to span the text area or the whole paper height. Depending on the case you have to replace \topmargin and \textheight by the correct values either 0pt or whatever your top margin is or by \paperheight. See the geometry package if you don't already use it for how to control those dimensions.  I got a working answer to my question on the Latex Community forum: http://www.latex-community.org/forum/viewtopic.php?f=5&t=9072&p=34877#p34877 The answer I got uses the 'Background' package and this code: \documentclass{article} \usepackage{background} \usepackage{lipsum}% just to generate filler text for the example \SetBgScale{1} \SetBgAngle{0} \SetBgColor{black} \SetBgContents{\rule{.4pt}{\paperheight}} \SetBgHshift{-9cm} \begin{document} \lipsum[1-90] \end{document} Works great and was easy to adjust to put one vrule in left margin area and one in the right margin area.  There could be a LaTeX package to do this for you but I'm more of a TeX person so I tried to come up with a TeX solution (not always the best idea to mix plain TeX with LaTeX but I think I have it working). Try this. Box 255 is the box register that TeX places the page contents into before the page is output. What I've done is taken the existing output routine and changed it to insert into box 255: a 0-height 0-width infinitely shrinkable-but-overflowing set of boxes containing a rule that is the height of the page 0.4pt thick and with any luck half an inch away into the left. The existing contents of box 255 is then added after this rule. Then I call the previous output routine which outputs the page (which now includes a rule) and also the headers and footers. \newtoks\oldoutput \oldoutput=\expandafter{\the\output}% \output{% \setbox255\vbox to 0pt{% \hbox to 0pt{% \vsize\ht255% \vbox to \ht255{% \vss \hbox to -0.5in{% \hss \vrule height \ht255 width 0.4pt% }% }\hss }\vss \box255% }% \the\oldoutput }% Put it before your \begin{document} command. This might not solve your problem completely but hopefully it should get you started. Here's a great page for learning about TeX primitives and built-in things. Thanks very much. I couldn't quite get it to work as written. If I compiled it in simple example doc I got errors I couldn't debug. Strangely if I put this code in preamble of document where I was already using the 'background' solution I posted about myself this code puts a line at the extreme left edge of the text area (i.e. at left edge of paragraph having indent of 0). I.e. it runs exactly along left edge of textarea with height of textheight. If I comment out the background lines then I again get compile errors. I will save this code to grok at some future time. Thanks! @Herbert Sitz: Sorry about that I put the `\hss` on the wrong side of the `\vrule`. Not sure why you're getting errors it works fine for me for your lipsum test case. Not sure what problem was but your code is working fine now. I gather that your solution is specially geared to draw vertical line that has top and bottom equal to top and bottom of 'textarea'. Would it be easy to modify to have it go up to top of header area and down below footer area? If not trivial don't worry about it. Thanks again. @Herbert Sitz: Not trivially it can be done. The `\vrule height xxx` can be set to anything `\ht255` just means the height of the page (or partial page if it wasn't completely full when it was ejected); however your solution is much more elegant anyway.
1109,A,"Is there any uncharted territory in Computer Graphics? It seems to me as if anything is now possible with computer graphics. Is seems as if we can depict cloth water skin anything completely convincingly. Are there areas that are are still a challenge or is the focus now on finding faster algorithms and cutting rendering times? Yes and there be dragons there. Raster graphics are basically a huge collection of hacks. Raytracing or similar methods are more ""proper."" You get things like radiosity reflection and refraction for free with raytracing. Doing raytracing in real time would be HUGE for games. Photon mapping is where it's at! Classic raytracing is only the first step of course skin looks like it does because light is reflected both from the surface and from (progressively) smaller amounts that penetrate the skin so your raytracing needs to be enhanced to take account of multiple possible paths for the same ray I have thought about this exact thing myself but the amount of data to be processed is simply huge and I don't it will be feasible to do real-time raytracing with the processor architectures of today. Maybe when we have massive numbers of cores (hundreds) per chip this might be possible. But it would be so great to be able to do this. Graphics would improve drastically and graphics programming would be much simpler as you stated. I don't think it's that far off. Intel showed Larrabee doing some basic real-time raytracing.  Tons of stuff is still hard or extremely slow. Try to combine transparent objects with fogging for example.  About raytracing; raytracing is cool but raytracing in the 'standard' way doesn't give you realistic lighting since rays are casted from the camera (the position of your eyes when you sit in front of your monitor) through the viewplane (your computer-screen) to see where they end up. In the real world it doesn't work that way. You don't emit radar/sonar-rays from your eyes and check what they hit; instead other objects emit energy and sometimes this energy ends up on your retina. The proper way to calculate lighting would therefore be something like photonmapping where every lightsource emits energy that gets transferred trough media (air water) and reflects/refracts across/through materials. Think about it - shooting a ray from the camera through a pixel on your screen gives you a single direction thay you'll check for light-intensity while in reality light could come at lots of different angles to end up at that same pixel. So 'standard' raytracing doesn't give you light-scattering effects unless you implement a special hack to take that into account. And aren't hacks the exact reason why people want to use another way besides polygon-rasterizing anyway ? Raytracing isn't the final solution. The only real solution is an infinite process where lights emit energy that bounces around the scene and if you're lucky ends up on your camera-lense. Since an infinite process is pretty hard to simulate we'll have to approximate it at one point or another. Games use hacks to make stuff looks good but in the end every other rasterizer / renderer / tracer / whatever strategy has to implement a limit - a hack - at some point. The important thing is - does it really matter ? Are we going for a 100% simulation of real life or is it good enough to calculate a picture that looks 100% real whatever the technique being used ? If you can't tell if a picture is real or CGI does it matter what method or hacks have been used ?  Pretty much everything is still impossible in graphics if you want to do it properly. Cloth water and skin are all faked to hell and back in order to achieve realtime framerates. We're still unable to do what's probably the most fundamental effect of all: Proper lighting. Do you mean ""unable to do... Proper lighting""? yep thanks for catching that. :)  I can´t think of anything that would be harder to do than a convincing human and that was done (IMHO)in The Curious Case of Benjamin Button. Check out this site about the making of BB. The graphics for the face in the movie was made by computers but there is still the challenge of animating the face which cannot yet be done soley by computer. @erik: good point Also this TED video is a good overview of ""the making of Ben's head"" http://www.ted.com/index.php/talks/ed_ulbrich_shows_how_benjamin_button_got_his_face.html  I've worked in videogames for 8 years and I've seen the graphics in games and movies getting better every year. In my opinion we need not worry about getting the graphics to look right; with the techniques we have right now and increasing cpu-power that will not be the problem.... For still shots. The problem I see in games and movies right now is (imho) the character-animation that just doesn't look right. As beautiful as they might be rendered characters in videogames and movies do not animate realisticly - even when motion capturing is used. Characters are missing something that makes them look natural - be it the breathing pattern the random micromovements bodies make the way someone randomly blinkes her eyes quicker or slower... Can't put my finger on what it is exactly it just doesn't look natural. So.. I think the next field of research should be (human) motion and animation to get stuff to look right.  The challenge seems to be performant modeling of surfaces WITHOUT POLYGONS. Polygons are rasterized surfaces being displayed on rasterized screens.  An API that insulates you from mathetmatics and is non-programmer friendly.  Some consider computer vision to be a frontier of computer graphics. It's basically CG in reverse: instead of going from model to images you go from images to a model. Computer vision is a young field with a wealth of open problems.  3D Not pictures that look like 3d but I am refering to actual 3d - once we get the 2D down then do it all again in the 3rd dim. We are just now starting to see some pretty cool stuff in the theaters as well as some very interesting new products coming out that do not require special galsses anymore.  Water Fire People Doing it all in realtime Physics (somewhat related to the computer graphics field) I haven't seen digital humans that are completely convincing. Same with water and fire on any significant scale. Look at some of the recent developments in computer game physics as examples: destructible buildings in Red Faction: Guerilla material-based destruction in Force Unleashed etc. Much of computer graphics revolves around video games and film where good enough is good enough. There's a lot of clever trickery involved. There's tons of room for improvement in efficiency scalability thoroughness and realism. @steven: good list. I'd say the movie Benjamin Button actually breaks new ground in realistic renderings (albeit sometimes hybrid) of people and organic material. However I think by far the most difficult task will be achieving the same results in Realtime.  I'd agree with Steven and erik. We're deep in the Uncanny Valley when it comes to people. And jalf is correct when he points out that a lot of things are still all smoke and mirrors."
1110,A,"Dynamically generate Triangle Lists for a Complex 3D Mesh In my application I have the shape and dimensions of a complex 3D solid (say a Cylinder Block) taken from user input. I need to construct vertex and index buffers for it. Since the dimensions are taken from user input I cannot user Blender or 3D Max to manually create my model. What is the textbook method to dynamically generate such a mesh? Edit: I am looking for something that will generate the triangles given the vertices edges and holes. Something like TetGen. As for TetGen itself I have no way of excluding the triangles which fall on the interior of the solid/mesh. Sounds like you need to create an array of verticies and a list of triangles each of which contains a list of 3 indicies into the vertex array. There is no easy way to do this. To draw a box you need 8 veticies and 12 triangles (2 per side). Some representations will use explicit edge representations too. I suspect this is way more work than you want to do so..... What you need is a mesh library that can do CSG (composite solid geometry). This way you should be able to specify the dimensions of the block and then the dimensions of the cylinders and tell it to cut them out for you (CSG difference). All the vertex and triangle management should be done for you. In the end such a library should be able to export the mesh to some common formats. Only problem here is that I don't know the name of such a library. Something tells me that Blender can actually do all of this if you know how to script it. I also suspect there are 1 or 2 fairly good libraries out there. Google actually brought me back to StackOverflow with this: http://stackoverflow.com/questions/78641/a-good-3d-mesh-library You may ultimately need to generate simple meshes programatically and manipulate them with a library if they don't provide functions for creating meshes (they all talk about manipulating a mesh or doing CSG). Thanks for the leads.  It depends a bit on your requirements. If you don't need to access the mesh after generating but only need to render it the fastest option is to create a vertex buffer with glGenBuffers map it into memory with glMapBuffer write your data into the buffer then unmap it with glUnmapBuffer. Drawing will be very fast because all data can be uploaded to video card memory. If you do need to access the mesh data after generating it or if you expect to modify it regularly you might be better off building your vertex data in a regular array and using vertex arrays with glVertexPointer and friends. You can also use a combination: generate the mesh data in main memory then memcpy() it into a mapped vertex buffer. Finally if by ""dimensions"" you mean just scaling the entire thing you can create it offline in any 3D modelling program and use the OpenGL transformations for example glScale to apply the dimensions to the mesh while rendering. Thomas I am looking for ways to actually generate the vertices from dimensions in an easy way. Something along the lines of TetGen. By ""shape and dimensions"" I mean number of vertices number of polygons number of holes and so on ... along with vertices. The entire geometry being taken from user input.  I'm not sure if the Marching Cube algorithm would be any help?."
1111,A,"Computer Graphics: Raytracing and Programming 3D Renders I've noticed that a number of top universities are offering courses where students are taught subjects relating to Computer Graphics for their CS majors. Sadly this is something not offered by my university and something I would really like to get into sometime in the next couple of years. A couple of the projects I've found from some universities are great although I'm mostly interested in two things: Raytracing: I want to write a Raytracer within the next two years. What do I need to know? I'm not a fantastic programmer yet (Java C and Prolog are my main languages as of today) but I'm slowly learning every day. Also my Math background isn't all that great so any pointers on books to read or advice on writing such a program would be fantastic. I tend to pick these things up pretty quickly so feel free to chuck references at me. Programming 3D Rendered Models I've looked at a couple of projects where students have developed models and used them in games. I've made a couple of 2D games with raster images but have never worked with 3D models. What would I need to learn in regards to programming these models? If it helps I used to be okay with 3D Studio Max and Cinema4D (although every single course seems to use Maya) but haven't touched it in about four years. Sorry for posting such vague and let's be honest stupid questions. It's just something I've wanted to do for a while and something that'd be good as a large project for me to develop in my own time. Related Questions Literature and Tutorials for Writing a Ray Tracer very hard to answer your question w/o knowing more about what you are trying to achieve. there's a vast difference between talent needed to program renderers for ""Toy Story"" and building an FPS game. What do you ultimately want to be able to do with these skills? I just want to know the theory behind it so both practical ideas are great. I'd like to be able to program a small scene though something like Toy Story would be great. Game Theory is something I've touched on already so a scene would be fantastic. The book ""Computer Graphics: Principles and Practice"" (known in the Computer Graphics circles as the ""Foley-VanDam"") is the basic for most computer graphics courses and it covers the topic of implementing a ray-tracer in much detail. It is quite dated but it's still the best afaik and the basic principles remain the same. I also second the recommendation for Eric Lengyel's Mathematics for 3D Game Programming and Computer Graphics. It's not as thorough but it's a wonderful review of the math basics you need for 3D programming it has very useful summaries at the end of each chapter and it's written in an approachable not too scary way. In addition you'll probably want some OpenGL or DirectX basics. It's easier to start working with a 3D API then learn the underlying maths than the opposite (in my opinion) but both options are possible. Just look for OpenGL on SO and you should find a couple of good references as well. I didn't care much for Foley-VanDamm; it's dated enough that many of the techniques just aren't relevant any more (it hardly mentions fixed-function shaders) and the style is so terse that it's hard to take the math into code. I agree it's pretty terse. But compared to most ""Weee Computer Graphics is fun fun fun look what I can do with those nice pixels"" textbooks it gives solid mathematical foundations. But then I'm a math geek.  Check http://www.scratchapixel.com/lessons/3d-basic-lessons/lesson-1-writing-a-simple-raytracer/ This is a very good place to learn about ray tracing and rendering in general.  The briefest useful answer I can give is that most of the important algorithms can be found in Real-Time Rendering by Tomas Akenine-Möller Eric Haines and Naty Hoffman and the bibliography at the end has references to the necessary maths. Their website has a recommended reading list as well. The most useful math book I've read on the subject is Eric Lengyel's Mathematics for 3D Game Programming and Computer Graphics. The maths you need most are geometry (obviously) and linear algebra (for dealing with all the matrices).  I can recommend pbrt it's a book and a physically-based renderer used to teach computer science graduates. The description of the maths used is nice and clear and since it is written in the 'literate programming' you can see the appropriate code (in C++) too.  For a mathematical introduction into these topics see http://graphics.idav.ucdavis.edu/education/GraphicsNotes/homepage.html  I took such a class last year and I believe that the class was wonderful for forcing students to learn the math behind the computer graphics - not just the commands for making a computer do what you want. My professor has a site located here and it has his lecture notes and problem sets that you can take a look through. Our final project was indeed a raytracer but once you know the mathematics behind it coding (an inefficient one) is trivial.  The 2000 ICFP Programming Contest asked participants to build a ray tracer in three days. They have a good specification for a simple ray tracer and you can get code for the winning entries and some other entries as well. There were entries in a large number of different programming languages. This might be a nice way for you to get started. That looks fantastic! I'll have great fun going over some of those programs. Thank you!"
1112,A,Is there a way to customize UITableView's separator line graphics? I'm finding customizing UITableView's separator line graphics. It's basically a single line and it can be hidden. But I cannot know how to draw separator with my own custom graphics or image. Is there a regular way to do this? Or do I have to draw all myself and manage them manually? A simple way is hide the separator and embed your own separator in the cell's background image. If you need to draw your own separator you need to override -[UITableView _drawExtraSeparator:] or -[UITableViewCell _drawSeparatorInRect:] or -[_UITableViewSeparatorView drawRect:]. These are all undocumented so don't think about using them in the AppStore. If regular way is undocumented (so prohibited) I should choose the first way you mentioned. Thanks for in-depth answer!
1113,A,"Double buffering with C# has negative effect I have written the following simple program which draws lines on the screen every 100 milliseconds (triggered by timer1). I noticed that the drawing flickers a bit (that is the window is not always completely blue but some gray shines through). So my idea was to use double-buffering. But when I did that it made things even worse. Now the screen was almost always gray and only occasionally did the blue color come through (demonstrated by timer2 switching the DoubleBuffered property every 2000 milliseconds). What could be an explanation for this? using System; using System.Drawing; using System.Windows.Forms; namespace WindowsFormsApplication1 { public partial class Form1 : Form { public Form1() { InitializeComponent(); } private void Form1_Paint(object sender PaintEventArgs e) { Graphics g = CreateGraphics(); Pen pen = new Pen(Color.Blue 1.0f); Random rnd = new Random(); for (int i = 0; i < Height; i++) g.DrawLine(pen 0 i Width i); } // every 100 ms private void timer1_Tick(object sender EventArgs e) { Invalidate(); } // every 2000 ms private void timer2_Tick(object sender EventArgs e) { DoubleBuffered = !DoubleBuffered; this.Text = DoubleBuffered ? ""yes"" : ""no""; } } } Try setting the double buffered property to true just once in the constructor while you're testing. You need to make use of the back buffer. Try this: using System; using System.Collections.Generic; using System.ComponentModel; using System.Data; using System.Drawing; using System.Linq; using System.Text; using System.Windows.Forms; namespace DoubleBufferTest { public partial class Form1 : Form { private BufferedGraphicsContext context; private BufferedGraphics grafx; public Form1() { InitializeComponent(); this.Resize += new EventHandler(this.OnResize); DoubleBuffered = true; // Retrieves the BufferedGraphicsContext for the // current application domain. context = BufferedGraphicsManager.Current; UpdateBuffer(); } private void timer1_Tick(object sender EventArgs e) { this.Refresh(); } private void OnResize(object sender EventArgs e) { UpdateBuffer(); this.Refresh(); } private void UpdateBuffer() { // Sets the maximum size for the primary graphics buffer // of the buffered graphics context for the application // domain. Any allocation requests for a buffer larger // than this will create a temporary buffered graphics // context to host the graphics buffer. context.MaximumBuffer = new Size(this.Width + 1 this.Height + 1); // Allocates a graphics buffer the size of this form // using the pixel format of the Graphics created by // the Form.CreateGraphics() method which returns a // Graphics object that matches the pixel format of the form. grafx = context.Allocate(this.CreateGraphics() new Rectangle(0 0 this.Width this.Height)); // Draw the first frame to the buffer. DrawToBuffer(grafx.Graphics); } protected override void OnPaint(PaintEventArgs e) { grafx.Render(e.Graphics); } private void DrawToBuffer(Graphics g) { //Graphics g = grafx.Graphics; Pen pen = new Pen(Color.Blue 1.0f); //Random rnd = new Random(); for (int i = 0; i < Height; i++) g.DrawLine(pen 0 i Width i); } } } It's a slightly hacked around version of a double buffering example on MSDN. I already did that with the same effect. The periodic switching is just for showing the effect more clearly.  No need to use multiple buffers or Bitmap objects or anything. Why don't you use the Graphics object provided by the Paint event? Like this: private void Form1_Paint(object sender PaintEventArgs e) { Graphics g = e.Graphics; Pen pen = new Pen(Color.Blue 1.0f); Random rnd = new Random(); for (int i = 0; i < Height; i++) g.DrawLine(pen 0 i Width i); } Yes you are correct. That was part 2 of my answer. It's simple and it works. Thanks.  I would just draw all of your items to your own buffer then copy it all in at once. I've used this for graphics in many applications and it has always worked very well for me:  public Form1() { InitializeComponent(); } private void timer1_Tick(object sender EventArgs e) { Invalidate();// every 100 ms } private void Form1_Load(object sender EventArgs e) { DoubleBuffered = true; } private void Form1_Paint(object sender PaintEventArgs e) { Bitmap buffer = new Bitmap(Width Height); Graphics g = Graphics.FromImage(buffer); Pen pen = new Pen(Color.Blue 1.0f); //Random rnd = new Random(); for (int i = 0; i < Height; i++) g.DrawLine(pen 0 i Width i); BackgroundImage = buffer; } EDIT: After further investigation it looks like your problem is what you're setting your Graphics object to: Graphics g = CreateGraphics(); needs to be: Graphics g = e.Graphics(); So your problem can be solved by either creating a manual buffer like I did above or simply changing you Graphics object. I've tested both and they both work."
1114,A,"WPF and Silverlight controls and layouts pan and zoom capabilities I would like to understand the general requirements for WPF/Silverlight layout for making it possible to implement pan&zoom (drag and zoom) features. I don't mean pan&zoom for an image but for a total page (window) layout (or part of it) with some controls. What features of the layout and what features of used custom controls make layout fixed and pan&zoom impossible? Use a MultiScaleImage or Canvas area and place everything you need to pan and zoom in it <Canvas x:Name=""panZoomPanel"" Background=""Transparent""> </Canvas> In code use make a TranslateTransform and a ScaleTransform in a TransformGroup to pan and zoom Check out other SO post or this example or this one  One really easy way of implementing zoom in XAML is to use a Silverlight ViewBox. This zooms the XAML not the pixels. You can specify the stretch to use and the ViewBox will scale based on this (Fill None Uniform etc). There are some great Viewbox blog posts on the web if you search for Silverlight+Viewbox on Google. The panning is easily accomplished with a similar mechanism to drag and drop and there are also numerous how-to blog posts on this available via Google. Just amounts to capturing MouseDown MouseMove and MouseUp events.  General rule With few exceptions everything in WPF can be panned zoomed rotated stretched etc to your heart's content. This include single controls like Button compound controls like ListBox and containers like StackPanel. The exceptions Here are the exceptions: If you are using Adorner and your AdornerDecorator is outside the panned/zoomed area then the Adorners attached to your panned/zoomed area will pan but not zoom. The solution is to put an additional AdornerDecorator inside the panned/zoomed area. If you use a Popup it will display at the panned/zoomed location of its PlacementTarget but it will not itself be scaled. It will also not move as you pan the area containing its PlacementTarget (basically it sits in its own surface above the target control). To get around this use a zero-size Canvas with high Z order instead when you want something to pop up within the zoom/pan area. Any ContextMenu you define will be shown inside a popup so the menu items will display normal size even when the area you clicked on is zoomed in or out. Because of the nature of a context menu this is probably desirable behavior. If not you can wrap the menu items in a ViewBox and tie the zoom to your main area's zoom. Your ToolTips will display normal size even if the UI is panned or zoomed. Same solution as for ContextMenu. If you used WinForms integration to integrated legacy WinForms controls and UI they will not properly pan zoom and clip in certain situations. There is an advanced technique for working around this where you implement the WinForms control off-screen then using BitBlt or similar copy the image into your window as an image and forward mouse clicks and keystrokes to the offscreen window. This is a lot of work though. If you bypass WPF and directly use GDI+ or DirectX or use Win32 hWnds to display content or UI that content or UI will not be properly panned zoomed or clipped to the window unless you do it yourself in your interface code. Final notes A good WPF UI always uses panels like Grid DockPanel etc to lay out controls in a flexible manner so they automatically adjust to container sizes rather than using fixed sizes and positions. This is also true for the internal contents of your pan/zoom area as well BUT there is an exception to this rule: the outermost element in your pan/zoom area must have a specified size. Otherwise what will define the area being panned/zoomed over? The easy way to implement pan/zoom capabilities is to adjust the RenderTransform of the outermost control in your pan/zoom area. There are many different ways to implement controls for panning and zooming for example you could use toolbar buttons and sliders scroll bars mouse wheel spacebar+drag to pan draggable areas of panned UI itself or any combination of these. Whichever interface you choose just have it update the RenderTransform appropriately from the code-behind and you're good to go. If your chosen panning mechanism is scroll bars you might want to use a ScrollViewer and only use the RenderTransform for the zoom. Be sure you set clipping on the pan/zoom area. Otherwise if you zoom in or pan items off the side they will still be visible outside the pan/zoom area. Ray Thanks for your time and efforts to give such a detailed answer. +1 @Russell: I once tried out some code that did this but I didn't write it myself and I don't know anywhere online you can find an example of it. I do remember the event forwarding code posted simple low-level WM_ messages eg. WM_LBUTTONDOWN WM_MOUSEMOVE WM_KEYDOWN etc. `""There is an advanced technique for working around this where you implement the WinForms control off-screen then using BitBlt or similar copy the image into your window as an image and forward mouse clicks and keystrokes to the offscreen window. This is a lot of work though.""` Do you have any references of people who have done this? The event forwarding mechanics in particular seem tricky.  In general you can treat any composite set of UI elements the same as you would treat a single UIElement so the case of an image isn't really different than doing the same for an entire application. The best way to handle zooming based on user input (as opposed to automatic scaling that Viewbox does) is applying a ScaleTransform. This can be set on a high level parent element like a Grid at the root of a Window layout. For panning you can combine in a TranslateTransform or in some cases use a ScrollViewer to handle moving the view of the content."
1115,A,"Graphics transparency on PictureBox First of all this is not about making the PictureBox control transparent. It's about bitmap transparency on the fully opaque ""canvas"". The PictureBox will always have the size of 300*300 with white background. No transparency is needed for the control. What I need is the way to draw the transparent rectangle (or whatever else) onto the pictureBox so anything that was already there will be seen ""through"" the rectangle. Say I have a following code Bitmap bmp = new Bitmap(300 300); Graphics g = Graphics.FromImage(bmp); g.FillRectangle(new SolidBrush(Color.White) 0 0 300 300); g.FillRectangle(new SolidBrush(Color.Red) 100 100 100 100); pictureBox.Image = bmp; This will draw a red rectangle in the middle of the white canvas. Now I need another (transparent) ""layer"" on the picture containing another rectangle but one that is transparent. I can try Brush brush = new SolidBrush(Color.FromArgb(128 0 80 0)); g.FillRectangle(brush 50 50 200 200); Since I am using a color by specifying its alpha = 128 the resulting rectangle should be transparent so the first red rectangle should be seen through this other green one. However this does not happen correctly. I can see the red rectangle behind the new green one but the part of the green rectangle that does not overlap the red one will remain completely opaque. However if I set the alpha value of the color to some extremely small value (say 1-5) the whole rectangle will look transparent. This is not normal in my opinion - that 5/255 is only half transparent and that 128/255 is not transparent at all... And if there was a string drawed previously with g.DrawString() the string is either displayed behind the green rectangle or it is not depending on the level of transparency. For example if the Alpha is greater than or equals (around) 40 the string is not visible at all and if it is less than 40 then it will show more visible for smaller alpha values down to alpha = 0. How is this brush (when created from Argb color) applied? Am I missing something? To me it seems that setting a transparent brush makes the background ""more visible"" instead of setting the object ""less visible"". Thanks for any replies with suggestions. [EDIT] It seems I had a nasty bug in application logic so the drawing routine happened in a loop so when I accumulated certain number of transparent rectangles they became more and more thick. The code when taken out of the loop works correctly. My bad. is done by this code:  private void pictureBox1_Paint(object sender PaintEventArgs e) { Bitmap bmp = new Bitmap(300 300); Graphics g = Graphics.FromImage(bmp); g.FillRectangle(new SolidBrush(Color.White) 0 0 300 300); g.FillEllipse(new SolidBrush(Color.Blue) 25 25 100 200); g.FillRectangle(new SolidBrush(Color.Red) 100 100 300 100); g.DrawString(""this is a STRING"" SystemFonts.DefaultFont Brushes.Black new Point(150 150)); pictureBox1.Image = bmp; Brush brush = new SolidBrush(Color.FromArgb(40 0 80 0)); g.DrawRectangle(Pens.Black 50 50 200 200); g.FillRectangle(brush 50 50 200 200); } The green part is not opaque as you can see... The string is perfectly visible. To me it seems that setting a transparent brush makes the background ""more visible"" instead of setting the object ""less visible"". background ""more visible"" and object ""less visible"" are the same thing... They are the same thing only for the overlapping part. Both your solution and my examples are ok. It seems I had a looping bug. updated with the string example."
1116,A,Read TIFF file in AIR? Is there any way to read a TIFF file using AIR? Looks like Flash doesn't have native support for TIFF format. Are there any libraries available? well TIFF is maintained by Adobe. Interesting how they don't support it in their own product :P Here is an open source encode project and according the author that decode future has been working on. Thanks I found this one too - but unfortunately encoding is pretty easy compared to decoding and I need decoding. Thanks anyway.  According to this TIFF is supported by Adobe Flash Player and Adobe AIR together with BMP GIF JPG and PNG. edit: FTA: The drawback with the TIFF format is that because of the many different varieties of TIFF there is no single reader that can handle every version. In addition no web browsers currently support the format. edit2: It appears that dynamic loading of TIFF is not possible. I couldn't find any third-party library to do so. Thanks for the pointer but flash.display.Loader throws an exception when trying to open a TIFF file and the documentation says it only supports JPEG and PNG. Maybe there is some other way?.. You're right it only supports JPG PNG and GIF. I'll try to look for a way to use TIFF ...  I've found a working decoder (uncompressed files only): http://www.ctyeung.com/AIR/index.html Thanks a ton! This might be an option..
1117,A,"How does the ""Unlimited Detail"" graphics technology work? So I stumbled upon this ""new"" graphics engine/technology called Unlimited Detail. This seems to be pretty interesting granted it's real and not a fake. They have some videos explaining the technology but they only scratch the surface. What do you think about it? Is it programmatically possible? Or is it just a scam for investors? Update: Since the only answer was based on voxels I have to copy this from their site: Unlimited Details method is very different to any 3D method that has been invented so far. The three current systems used in 3D graphics are Ray tracing polygons and point cloud/voxels they all have strengths and weaknesses. Polygons runs fast but has poor geometry Ray-trace and voxels have perfect geometry but run very slowly. Unlimited Detail is a fourth system which is more like a search algorithm than a 3D engine The underlying technology is related to something called sparse voxel octrees (see e.g. this paper) which aren't anything incredibly amazing. What the video doesn't tell you is that these are not at all suited for things that need to be animated so they're of limited use for anything that uses procedural animation (e.g. all ragdoll physics etc.). So they're very inflexible. You can get great detail but you get it in a completely static world. A rough summary of where things stand with this technology in mainstream games is here. You will also want to check out Samuli Laine's work; he's a Finnish researcher who is focusing a great deal of his attention on this subject and is unlocking some of the secrets to implementing it well. Update: Yes the website says it's not ""voxel-based"". I suspect this is merely an issue of semantics however in that what they're using are essentially voxels but because it's not exactly a voxel they feel safe in being able to claim that it's not voxel-based. In any case the magic isn't in how similar to a voxel it is -- it's how they select which voxels to actually show. This is the primary determinant of speed. Right now there is no incredibly fast way to show voxels (or something approximating a voxel). So either they have developed a completely new non-peer-reviewed method for filtering voxels (or something like them) or they're lying. They clearly say it is not voxel-based. I updated my question. @daddz: They lie.. :) @Daniel Dimovski: so it's almost impossible they found something ""new"". Well I also think that their engine is fake but I don't deny the fact that they may have found a new way. @daddz: If it's not voxel-based it will nevertheless almost certainly have all the characteristics I described: fast on static scenes but tough/very tedious to animate for. I agree with John - it uses a classic octree approach to search for renderable objects (that much was obvious from the writeup). It looks like some sort of voxely kind of idea. The representation of information has to be some item that is accessible whether it be a polygon a voxel or some sort of space-add/subtract polytope."
1118,A,"Calling ""other"" ImageMagick commands through Imagick? I'm using the Imagick PHP wrapper for ImageMagick. I've noticed there are some ImageMagick commands that are not available in the Imagick wrapper such as autolevel and normalize. How should I go about calling these? Couldn't find anything in Imagick. Are there disadvantages to calling the ImageMagick commands directly through the shell with exec() or shell_exec()? I recommend using system(). For some reason exec() doesn't work on my MacBook. Was hoping there'd be a way to do it without escaping to shell but... guess not."
1119,A,Output W3C compliant XHTML from image slices When slicing and saving for web in Photoshop CS4 the HTML layout output by Photoshop is done using tags which is not what we want. Is there a way to get Photoshop to output W3C compliant tableless XHTML with CSS? Alternatively is there another application that I can use to slice to W3C XHTML? I'm going to answer my own question here actually this following URL is going to: http://www.adobe.com/devnet/fireworks/articles/fireworks_web_design_css.html Very interesting. I wonder how clean the HTML generated is. My experience with WYSIWYG editors has been pretty underwhelming in the past. And the author of the article suggests this only as a way to generate mock-ups before you write the actual code for the pages. Though Dreamweaver definitely produced better markup than Frontpage so maybe Fireworks is just as good.
1120,A,"What is the simplest algorithm for mesh generation of an arbitrary quadrilateral? What is the simplest algorithm for decomposing a quadrilateral into an arbitrary number of triangles? What's the ""mesg"" tag supposed to be? fixed ""mesg"" typo in tags. Probably midpoint triangle subdivision. Cut your quad into two triangles then use the midpoints of each triangle edge as vertexes for four new triangles. A very simple and easy to implement tesselation system. Easy to do an arbitrary number of trinagles too. Keep subdividing until you have ""n"" triangles."
1121,A,What's the relationship between WIC and GDI+? I'm fuzzy on the relationship between Windows Imaging Component (WIC) and GDI+. I've done some work in the past that showed that for example WIC produces visually superior GIF encodings but I'm surprised I don't see more people using it for image processing vs. GDI+. I know it doesn't have GDI+'s draw operations but for encoding/decoding it seems superior. So why don't we see a migration? The relationship (or rather the difference between) WIC and GDI+ is that WIC is an extensible imaging codec framework which allows applications implementing the framework to receive support for new image formats via provided codecs. GDI+ is a core component of Windows which supports draw operations such as lines fonts gradients etc. While GDI+ has native support for several common image formats WIC codecs can be provided for any image format. So other than some native encoding/decoding WIC doesn't really do anything except offer a framework for writing more code? For such a superior set of native codecs (compared to GDI+) you'd think they'd have provided a better bridge between GDI+ and WIC. I'm researching this a bit more and there seems to be a little more to it than that. WIC is an underpinning of most of what WPF does (see http://blogs.msdn.com/dwayneneed/archive/2008/06/20/implementing-a-custom-bitmapsource.aspx) and indeed WIC seems to manage pixels just as GDI+ does.  I understood that GDI+ uses WIC to perform certain tasks. At least in Windows 7 it does. Please consider the following code: image.Write(target ImageFormat.Gif); When I run this code under Windows XP it will use the GDI+ Gif encoder to write the image as a Gif. When I run the same code under Windows 7 it will use the WIC Gif Encoder. there is not Image.Write(...).You probably mean Image.Save(...)
1122,A,"Cast Graphics to Image in C# I have a pictureBox on a Windows Form. I do the following to load a PNG file into it. Bitmap bm = (Bitmap)Image.FromFile(""Image.PNG"" true); Bitmap tmp; public Form1() { InitializeComponent(); this.tmp = new Bitmap(bm.Width bm.Height); } private void pictureBox1_Paint(object sender PaintEventArgs e) { e.Graphics.DrawImage(this.bm new Rectangle(0 0 tmp.Width tmp.Height) 0 0 tmp.Width tmp.Height GraphicsUnit.Pixel); } However I need to draw things on the image and then have the result displayed again. Drawing rectangles can only be done via the Graphics class. I'd need to draw the needed rectangles on the image make it an instance of the Image class again and save that to this.bm I can add a button that executes this.pictureBox1.Refresh(); forcing the pictureBox to be painted again but I can't cast Graphics to Image. Because of that I can't save the edits to the this.bm bitmap. That's my problem and I see no way out. What you need to do is use the Graphics.FromImage method which will allow you to draw directly on the image instead of the temporary Graphics object create from within the Paint method: using (Graphics g = Graphics.FromImage(this.bm)) { g.DrawRectangle(...); } Do this instead of (or in addition to) hooking the Paint method of the PictureBox. This way you won't need to use a temporary image or Graphics object at all and when you're finished modifying the original bitmap (this.bm) then you can invoke pictureBox1.Refresh to force the image to be re-displayed. I had been wondering why when I drew using e.Graphics in my Paint(object sender PaintEventArgs e) method the graphics were not part of the image. Thanks for this tip on how to draw directly on the image!"
1123,A,"GDI+ Smaller images? I create a bitmap from bytes coming via the web and when I downsample it the resulting Jpeg is still too big even though I use small pixel format. Does someone know how to manipulate the compression of the image? I had the impression that the saved image is no longer compressed. Tony edit your post by adding the relevant code. It's very possible you're forgetting something. In any case you'll get more answers faster if you include more information like the size of your images before and after you use your code. You did not mention in your original post how you are saving your Jpeg. MSDN has a short page on setting compression here. I hope I have not misunderstood the question. This is really selly I am studing for Exam 70-536 the book talked about compression like this ""You can compress images using GDI+""  the exam is about .Net2 the link u provide is for .Net3.5. Thanks I need that for my current tasks anyway."
1124,A,"Non-Affine image transformations in .NET Are there any classes methods in the .NET library or any algorithms in general to perform non-affine transformations? (i.e. transformations that involve more than just rotation scale translation and shear) e.g.: Is there another term for non-affine transformations? You can do this in wpf using a the Viewport3d control and a non-affine transform matrix. Rendering this to a bitmap again may be interesting.... Which I ""fixed"" by including an invisible <image> control with the same image as on my textured plane... (Also I've had to work around the max texture size issues by splitting up the plane and cropping images...) http://www.charlespetzold.com/blog/2007/08/060605.html In my case I wanted the reverse of this (transform so arbitrary points on the warped become the corners of my rectangular window) which is the Inverse of the matrix to do the opposite. Oh Charles Petzold also posted how to do this in silverlight 3 using the Matrix3dProjection: http://www.charlespetzold.com/blog/2009/07/Using-the-Matrix3DProjection-Class-in-Silverlight-3.html  I am not aware of anything integrated in .Net letting you do non affine transforms. I guess you are trying to have some sort of 3D texture mapping? If that's the case you need an homogenous affine transform which is not available in .Net. I'm also not aware of any integrated way to make pixel displacement transforms in .Net. However the currently voted solution might be good for what you are trying to do just be aware that it won't do perspective correction out of the box. For instance: The picture on the left was generated using the single quad distort library provided by Neil N. The picture on the right was generated using a single quad (two triangles actually) in DirectX. This may not have any impact on what you are trying to do but this is something to keep in mind if you want to do 3D stuff it will look very weird without perspective correct mapping. With the link Neil provided you you can do what you want. It will do good 3D although not perspective correct but it should not matter if your polygons stay small. Actually the link I provided WOULD allow for perspective correct transforms if used properly. Neil N: The algorithm is a quad mapper what it does is affine mapping. With only 2D coordinates there is no way this is doing perspective correct mapping. If you still have doubt after this comment I invite you to do research on what ""perspective correct texture mapping"" is. If used on sufficiently small polygons and the user provides perspective corrected mappings herself yes you can achieve a similar effect. But from what I can see the library as-is do not provide perspective corrected mapping. Coincoin calculating corner intersection is a perspective correct method. It may not be the method you are familiar with but it works. You can even test it against other perspective correct methods you will not see any difference. This is a fact. I updated my answer to give more details about the problems that might arise if simple distortions are used versus perspective corrected. As you can see I tested the library against a perspective correct method (DirectX) and the difference is clear. Now I know there are ways to have perspective without resorting to a full 3D system assuming we have a source rectangle and a target quad however the library just doesn't seem to do it unless I missed something No we got an iPod and i saw the page-wrap-up ability. And i was disgusted that a tiny embedded device and flash can no fancy transformations - but a Core2 Duo running the most advanced operating system in the world cannot. ""...can do* fancy transformations..."" ""From what you can see"" ?? how about being sure before making false statements. The linked algorithm computes EACH PIXEL no need to break it up into polygons. Just because it doesnt have built in functions to do fancy transforms doesnt mean it can't do it. I've seen some pretty amazing graphics doen in pure C# My mistake I assumed because it used corner intersection that it was correct. After looking deeper it uses it incorrectly. And this is still a very useful answer as you took the time to post a picture showing the difference between a distortion and a perspective correction. In my case i was wanting to do a perspective correction - taking what looks like a flat piece of paper and having it rotated  All of the example images you posted can be done with a Quadrilateral Distortion. Though I cant say for certain that a quad distort will cover ALL non affine transforms. Heres a link to a not so good implementation of it in C#... it works but is slow. Poke around Wikipedia for the many different optimizations available for these kinds of calculations http://www.vcskicks.com/image-distortion.html -Neil ""Though I cant say for certain that a quad distort will cover ALL non affine transforms."" It won't there are many many things a quad distort can't do. But it's an excellent start."
1125,A,"Calculating the angle between the line defined by two points I'm currently developing a simple 2D game for Android. I have a stationary object that's situated in the center of the screen and I'm trying to get that object to rotate and point to the area on the screen that the user touches. I have the constant coordinates that represent the center of the screen and I can get the coordinates of the point that the user taps on. I'm using the formula outlined in this forum: How to get angle between two points? It says as follows ""If you want the the angle between the line defined by these two points and the horizontal axis: double angle = atan2(y2 - y1 x2 - x1) * 180 / PI;"". I implemented this but I think the fact the I'm working in screen coordinates is causing a miscalculation since the Y-coordinate is reversed. I'm not sure if this is the right way to go about it any other thoughts or suggestions are appreciated. Technically you can't get the angle between two *points*. You can get the angle between two *vectors* though. Pretty sure he means ""the angle between a line drawn between two points and the horizontal axis"" I'm sorry let me rephrase my title how do I get the angle between the line defined by these two points and the horizontal access that cuts through my object at the center of the screen?? ""the origin is at the top-left of the screen and the Y-Coordinate increases going down while the X-Coordinate increases to the right like normal. I guess my question becomes do I have to convert the screen coordinates to Cartesian coordinates before applying the above formula?"" If you were calculating the angle using Cartesian coordinates and both points were in quadrant 1 (where x>0 and y>0) the situation would be identical to screen pixel coordinates (except for the upside-down-Y thing. If you negate Y to get it right-side up it becomes quadrant 4...). Converting screen pixel coordinates to Cartesian doesnt really change the angle.  Had a need for similar functionality myself so after much hair pulling I came up with the function below  * Fetches angle relative to screen centre point * where 3 O'Clock is 0 and 12 O'Clock is 270 degrees * * @param screenPoint * @return angle in degress from 0-360. */ public double getAngle(Point screenPoint) { double dx = screenPoint.getX() - mCentreX; // Minus to correct for coord re-mapping double dy = -(screenPoint.getY() - mCentreY); double inRads = Math.atan2(dydx); // We need to map to coord system when 0 degree is at 3 O'clock 270 at 12 O'clock if (inRads < 0) inRads = Math.abs(inRads); else inRads = 2*Math.PI - inRads; return Math.toDegrees(inRads); } +1 thanks for posting! The code in this answer worked. Saved my time. Thanks Dave. +1 Thanks I was really scratching my head for long time as I couldn't figure out how to get the proper angles.  If you think the y coordinate is reversed just negate the value(s). Does the answer make more sense? I tried that but didn't seem to work. What I mean is that the origin is at the top-left of the screen and the Y-Coordinate increases going down while the X-Coordinate increases to the right like normal. I guess my question becomes do I have to convert the screen coordinates to Cartesian coordinates before applying the above formula? @kingrichard2005 - if negating the y doesn't work try doing `yMax - y`.  Assumptions: x is the horizontal axis and increases when moving from left to right. y is the vertical axis and increases from bottom to top. (touch_x touch_y) is the point selected by the user. (center_x center_y) is the point at the center of the screen. theta is measured counter-clockwise from the +x axis. Then: delta_x = touch_x - center_x delta_y = touch_y - center_y theta_radians = atan2(delta_y delta_x) Edit: you mentioned in a comment that y increases from top to bottom. In that case delta_y = center_y - touch_y But it would be more correct to describe this as expressing (touch_x touch_y) in polar coordinates relative to (center_x center_y). As ChrisF mentioned the idea of taking an ""angle between two points"" is not well defined. Thanks Jim whether I'm using Cartesian or polar coordinates how would I know if the calculated angle is based on the horizontal line that cuts through my object and not the horizontal at the top of the screen which is coming out of the origin at the top-left?? @kingrichard2005: Since delta_x and delta_y are calculated with respect to the center point (the location of your object) theta will also be calculated with respect to the same point. Thanks for the help Jim."
1126,A,"Any 'pretty' data visualization libraries for Python? There are plenty of 'pretty-printing' visualization libraries for Javascript. E.g. those listed here. Googling for 'python visualization libraries' only turns up stuff like VTK and mayavi which are primarily more for no-nonsense scientific use. So do you know of any Python libraries similar to those Javascript ones in the above link? I particularly like the Javascript Infovis Toolkit. For Python there really isn't ""one viz library to rule them all"". There are different libraries and toolkits for different purposes. For graphs in Python you may find igraph useful. For other types of scientific or data visualizations matplotlib is also good.  are you looking for graph software? Checkout http://www.graphviz.org/Gallery.php. it has python bindings.  Networkx looks to be the best-documented of the graph libraries I've seen. I think it interfaces with Matplotlib for visualization. Networkx is absolutely awesome. You can produce some nice looking stuff with it via Matplotlib or you can use [Pygraphviz](http://pygraphviz.github.io/) to have it talk to Graphviz.  I second matplotlib. Also Chaco. It's not Python but for really quick heatmaps go to OpenHeatMap.com  Here's a new port of R's ggplot2 over to python. Looks very slick! https://github.com/yhat/ggplot/ More info here: http://blog.yhathq.com/posts/ggplot-for-python.html Also have a look at Seaborn described as ""Improved matplotlib for statistical data visualization"": https://github.com/mwaskom/seaborn The Seaborn examples are pretty slick: Plotting Complex Linear Models Visualization Distributions Time Series Visualizations  There's PyCha for charts.  Check out Bokeh at pydata source code here"
1127,A,"How do you find a point at a given perpendicular distance from a line? I have a line that I draw in a window and I let the user drag it around. So my line is defined by two points: (x1y1) and (x2y2). But now I would like to draw ""caps"" at the end of my line that is short perpendicular lines at each of my end points. The caps should be N pixels in length. Thus to draw my ""cap"" line at end point (x1y1) I need to find two points that form a perpendicular line and where each of its points are N/2 pixels away from the point (x1y1). So how do you calculate a point (x3y3) given it needs to be at a perpendicular distance N/2 away from the end point (x1y1) of a known line i.e. the line defined by (x1y1) and (x2y2)? For a detailed worked out solution [see here](http://stackoverflow.com/a/17195324/183120). Since the vectors from 2 to 1 and 1 to 3 are perpendicular their dot product is 0. This leaves you with two unknowns: x from 1 to 3 (x13) and y from 1 to 3 (y13) Use the Pythagorean theorem to get another equation for those unknowns. Solve for each unknown by substitution... This requires squaring and unsquaring so you lose the sign associated with your equations. To determine the sign consider: while x21 is negative y13 will be positive while x21 is positive y13 will be negative while y21 is positive x13 will be positive while y21 is negative x13 will be negative Known: point 1 : x1  y1 Known: point 2 : x2  y2 x21 = x1 - x2 y21 = y1 - y2 Known: distance |1->3| : N/2 equation a: Pythagorean theorem x13^2 + y13^2 = |1->3|^2 x13^2 + y13^2 = (N/2)^2 Known: angle 2-1-3 : right angle vectors 2->1 and 1->3 are perpendicular 2->1 dot 1->3 is 0 equation b: dot product = 0 x21*x13 + y21*y13 = 2->1 dot 1->3 x21*x13 + y21*y13 = 0 ratio b/w x13 and y13: x21*x13 = -y21*y13 x13 = -(y21/x21)y13 x13 = -phi*y13 equation a: solved for y13 with ratio  plug x13 into a phi^2*y13^2 + y13^2 = |1->3|^2 factor out y13 y13^2 * (phi^2 + 1) = plug in phi y13^2 * (y21^2/x21^2 + 1) = multiply both sides by x21^2 y13^2 * (y21^2 + x21^2) = |1->3|^2 * x21^2 plug in Pythagorean theorem of 2->1 y13^2 * |2->1|^2 = |1->3|^2 * x21^2 take square root of both sides y13 * |2->1| = |1->3| * x21 divide both sides by the length of 1->2 y13 = (|1->3|/|2->1|) *x21 lets call the ratio of 1->3 to 2->1 lengths psi y13 = psi * x21 check the signs when x21 is negative y13 will be positive when x21 is positive y13 will be negative y13 = -psi * x21 equation a: solved for x13 with ratio  plug y13 into a x13^2 + x13^2/phi^2 = |1->3|^2 factor out x13 x13^2 * (1 + 1/phi^2) = plug in phi x13^2 * (1 + x21^2/y21^2) = multiply both sides by y21^2 x13^2 * (y21^2 + x21^2) = |1->3|^2 * y21^2 plug in Pythagorean theorem of 2->1 x13^2 * |2->1|^2 = |1->3|^2 * y21^2 take square root of both sides x13 * |2->1| = |1->3| * y21 divide both sides by the length of 2->1 x13 = (|1->3|/|2->1|) *y21 lets call the ratio of |1->3| to |2->1| psi x13 = psi * y21 check the signs when y21 is negative x13 will be negative when y21 is positive x13 will be negative x13 = psi * y21 to condense x21 = x1 - x2 y21 = y1 - y2 |2->1| = sqrt( x21^2 + y^21^2 ) |1->3| = N/2 psi = |1->3|/|2->1| y13 = -psi * x21 x13 = psi * y21 I normally wouldn't do this but I solved it at work and thought that explaining it thoroughly would help me solidify my knowledge.  You need to compute a unit vector that's perpendicular to the line segment. Avoid computing the slope because that can lead to divide by zero errors. dx = x1-x2 dy = y1-y2 dist = sqrt(dx*dx + dy*dy) dx /= dist dy /= dist x3 = x1 + (N/2)*dy y3 = y1 - (N/2)*dx x4 = x1 - (N/2)*dy y4 = y1 + (N/2)*dx I keep thinking there must be a way to avoid that nasty sqrt in there possibly by using Breshenham's Line but I can't think of it off hand. I think you have a sign error in your computations or points 3 and 4. Use (+ - - +) or (- + + -) in the last four lines no? Thanks this works great! Shouldn't it be dx instead of dy that is used to calculate x3?  If you want to avoid a sqrt do the following: in: line_length cap_length rotation position of line centre define points: tl (-line_length/2 cap_length) tr (line_length/2 cap_length) bl (-line_length/2 -cap_length) br (line_length/2 -cap_length) rotate the four points by 'rotation' offset four points by 'position' drawline (midpoint tlbl to midpoint trbr) drawline (tl to bl) drawline (tr to br)  You just evaluate the orthogonal versor and multiply by N/2 vx = x2-x1 vy = y2-y1 len = sqrt( vx*vx + vy*vy ) ux = -vy/len uy = vx/len x3 = x1 + N/2 * ux Y3 = y1 + N/2 * uy x4 = x1 - N/2 * ux Y4 = y1 - N/2 * uy"
1128,A,"Generate texture from polygon (openGL) I have a quad and I would like to use the gradient it produces as a texture for another polygon. glPushMatrix(); glTranslatef(2502500); glBegin(GL_POLYGON); glColor3f(25500); glVertex2f(100); glVertex2f(1000); glVertex2f(100100); glVertex2f(5050); glVertex2f(0100); glEnd(); //End quadrilateral coordinates glPopMatrix(); glBegin(GL_QUADS); //Begin quadrilateral coordinates glVertex2f(00); glColor3f(02550); glVertex2f(1500); glVertex2f(150150); glColor3f(25500); glVertex2f(0150); glEnd(); //End quadrilateral coordinates My goal is to make the 5 vertex polygon have the gradient of the quad (maybe a texture is not the best bet) Thanks Keep it simple! It is very simple to create a gradient texture in code e.g.: // gradient white -> black GLubyte gradient[2*3] = { 255255255 000 }; // WARNING: check documentation I am not quite sure about syntax and order: glTexture1D( GL_TEXTURE_1D 03 2 0 GL_RGB GL_UNSIGNED_BYTE gradient ); // setup texture parameters draw your polygon etc. The graphics hardware and/or the GL will create a sweet looking gradient from color one to color two for you (remember: that's one of the basic advantages of having hardware accelerated polygon drawing you don't have to do interpolation work in software). Your real problem is: which texture coordinates do you use on the 5 vertex polygon. But that was not your question... ;-) I agree this is a more simple solution but technically not what the OP asked for: ""...the gradient it [the poloygon] produces"". A polygon does in general *not* produce a linear gradient since it will typically vary on both axes. Moreover that texture-mapping problem does not arise with a two-dimensonal gradient texture. To be more precise: For the simple solution you present you could go even more simple by providing `glColor3f` to each vertex which is used to produce the gradient in the first place. But the result of the first render pass will typically be way more complex.  To do that you'd have to do a render-to-texture. While this is commonplace and supported by practically every board it's typically used for quite elaborate effects (e.g. mirrors). If it's really just a gradient I'd try to create the gradient in am app like Paint.Net. If you really need to create them at run-time use a pixel shader to implement render-to-texture. However I'm afraid explaining pixel shaders in a few words is a bit tough - there are lots of tutorials on this on the net however. With the pixel shader you gain a lot of control over the graphic card. This allows you to render your scene to a temporary buffer and then apply that buffer as a texture quite easily plus a lot more functionality. Other than that theres no other way to get a linear gradient on a polygon? ""linear interpolation"" (your gradient) is one of the most basic basics of hardware accelerated rendering but check my answer.. ;) another hint: any render-to-texture method would be completely overkill for your application. its a bit sad that practical render-to-texture methods were available so late. if you want to support many (consumer and professional) gfx cards you still have to do a lot of low-level work to get it working and get it right. trust us don't use render-to-texture methods for this task (it can be solved easily without rtt)."
1129,A,how to use JUNG to color & shape vertices and edges I am facing a problem in using JUNG. I want to draw a network diagram where the vertices will be having different shapes and colors and edges will be dashed or full line in different colors. Since I am a newbie in Java I am unable to understand the actual architecture of jung. When I use setVertexFillPaintTransformer it colors all the vertices with the same color. The vertices are stored in an integer array. I am banging my head for past one week now. Plz if someone can help me or has some counter questions do ask me The method setVertexFillPaintTransformer takes in a transformer that converts a vertice into a colour. So to have different colours for different vertices you need to make it inspect the vertex. The parameter i in the method public Paint transform(Integer i) is the vertex so you can provide a colour that is based on the vertices (or i). For example if I had a graph where the vertices were an Integer I could cycle assign three different colours to the vertices by supplying the following transformer to setVertexFillPaintTransformer: Transformer<Integer Paint> vertexPaint = new Transformer<Integer Paint>() { private final Color[] palette = {Color.GREEN Color.BLUE Color.RED}; public Paint transform(Integer i) { return palette[i.intValue() % palette.length]; } };
1130,A,"Bezier clipping I'm trying to find/make an algorithm to compute the intersection (a new filled object) of two arbitrary filled 2D objects. The objects are defined using either lines or cubic beziers and may have holes or self-intersect. I'm aware of several existing algorithms doing the same with polygons listed here. However I'd like to support beziers without subdividing them into polygons and the output should have roughly the same control points as the input in areas where there are no intersections. This is for an interactive program to do some CSG but the clipping doesn't need to be real-time. I've searched for a while but haven't found good starting points. I know I'm at risk of being redundant but I was investigating the same issue and found a solution that I'd read in academic papers but hadn't found a working solution for. You can rewrite the bezier curves as a set of two bi-variate cubic equations like this: ∆x = ax₁*t₁^3 + bx₁*t₁^2 + cx₁*t₁ + dx₁ - ax₂*t₂^3 - bx₂*t₂^2 - cx₂*t₂ - dx₂ ∆y = ay₁*t₁^3 + by₁*t₁^2 + cy₁*t₁ + dy₁ - ay₂*t₂^3 - by₂*t₂^2 - cy₂*t₂ - dy₂ Obviously the curves intersect for values of (t₁t₂) where ∆x = ∆y = 0. Unfortunately it's complicated by the fact that it is difficult to find roots in two dimensions and approximate approaches tend to (as another writer put it) blow up. But if you're using integers or rational numbers for your control points then you can use Groebner bases to rewrite your bi-variate order-3 polynomials into a (possibly-up-to-order-9-thus-your-nine-possible-intersections) monovariate polynomial. After that you just need to find your roots (for say t₂) in one dimension and plug your results back into one of your original equations to find the other dimension. Burchburger has a layman-friendly introduction to Groebner Bases called ""Gröbner Bases: A Short Introduction for Systems Theorists"" that was very helpful for me. Google it. The other paper that was helpful was one called ""Fast precise flattening of cubic Bézier path and offset curves"" by TF Hain which has lots of utility equations for bezier curves including how to find the polynomial coefficients for the x and y equations. As for whether the Bezier clipping will help with this particular method I doubt it but bezier clipping is a method for narrowing down where intersections might be not for finding a final (though possibly approximate) answer of where it is. A lot of time with this method will be spent in finding the mono-variate equation and that task doesn't get any easier with clipping. Finding the roots is by comparison trivial. However one of the advantages of this method is that it doesn't depend on recursively subdividing the curve and the problem becomes a simple one-dimensional root-finding problem which is not easy but well documented. The major disadvantage is that computing Groebner bases is costly and becomes very unwieldy if you're dealing with floating point polynomials or using higher order Bezier curves. If you want some finished code in Haskell to find the intersections let me know.  I wrote code to do this a long long time ago. The project I was working on defined 2D objects using piecewise Bezier boundaries that were generated as PostScipt paths. The approach I used was: Let curves p q be defined by Bezier control points. Do they intersect? Compute the bounding boxes of the control points. If they don't overlap then the two curves don't intersect. Otherwise: p.x(t) p.y(t) q.x(u) q.y(u) are cubic polynomials on 0 <= tu <= 1.0. The distance squared (p.x - q.x) ** 2 + (p.y - q.y) ** 2 is a polynomial on (tu). Use Newton-Raphson to try and solve that for zero. Discard any solutions outside 0 <= tu <= 1.0 N-R may or may not converge. The curves might not intersect or N-R can just blow up when the two curves are nearly parallel. So cut off N-R if it's not converging after after some arbitrary number of iterations. This can be a fairly small number. If N-R doesn't converge on a solution split one curve (say p) into two curves pa pb at t = 0.5. This is easy it's just computing midpoints as the linked article shows. Then recursively test (q pa) and (q pb) for intersections. (Note that in the next layer of recursion that q has become p so that p and q are alternately split on each ply of the recursion.) Most of the recursive calls return quickly because the bounding boxes are non-overlapping. You will have to cut off the recursion at some arbitrary depth to handle weird cases where the two curves are parallel and don't quite touch but the distance between them is arbitrarily small -- perhaps only 1 ULP of difference. When you do find an intersection you're not done because cubic curves can have multiple crossings. So you have to split each curve at the intersecting point and recursively check for more interections between (pa qa) (pa qb) (pb qa) (pb qb).  There are a number of academic research papers on doing bezier clipping: http://www.andrew.cmu.edu/user/sowen/abstracts/Se306.html http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.61.6669 http://www.dm.unibo.it/~casciola/html/research_rr.html I recommend the interval methods because as you describe you don't have to divide down to polygons and you can get guaranteed results as well as define your own arbitrary precision for the resultset. For more information on interval rendering you may also refer to http://www.sunfishstudio.com  I found the following publication to be the best of information regarding Bezier Clipping: T. W. Sederberg BYU Computer Aided Geometric Design Course Notes Chapter 7 that talks about Curve Intersection is available online. It outlines 4 different approaches to find intersections and describes Bezier Clipping in detail: http://www.tsplines.com/technology/edu/CurveIntersection.pdf"
1131,A,How can I generate paths in .png files using C#? I need to parse an XML file of coordinates and create a .png (from scratch) in which I draw paths between the coordinates. I also need to be able to smooth the corners when paths change direction (maybe using beziers). How can I do this programmatically in C#? Thanks C# doesn't have any notion of graphics—you'll need a choose a vector graphics library to do the work for you. Since you're already in .NET I'd suggest WPF to construct the image and the Bitmap classes to export the final result as a PNG. You can find a great introduction to WPF vector graphics here.  You can paint into a bitmap using the Graphics object and DrawBezier method: http://msdn.microsoft.com/en-us/library/system.drawing.graphics.drawbezier.aspx And then use the Save method to store it as png. I can't paste you code right now because I don't have a dev environment to make a sample but it should be something like: Create a Bitmap with the size you need: Bitmap bitmap = new Bitmap(widthheight); Get a Graphics object from your bitmap : Graphics graphics= Graphics.FromImage(bitmap); Use the graphics object to draw (with DrawBezier if it's your case) graphics.DrawBezier(pen abc); Call to Save method indicating png format: bitmap.Save(pathImageFormat.Png);
1132,A,Embedding dendrogram in Java I'm looking for a library capable of drawing dendrograms of data in Java (not calculating them I can do it by myself).. do you have any clues? Already tried to search it over Google but haven't found anything that is not stand-alone (while I need to embed the generation inside my program). Thanks! Was wondering if you found a solution? Yes I did it with Jung :) Quite easy to use and powerful at the same time! could you elaborate a bit on how you managed to do it with Jung? Which layout for instance? Take a look at Archaeopteryx. It has fairly many features; it's open source and it is available in a pre-packaged jar file. BTW I use JUNG and really like it. It can perform various clusterings but AFAIK it has no inherent dendrogram capabilities. Because it has graphing capabilities you could roll your own dendrogram but it would take some work.  Check out the JUNG graph library. It won't perform the actual clustering for you but is a really good library for visualising your results. thanks I don't need anything that calculates.. I already wrote what I need just needed something to draw results :D I'll check in a while and accept it if it's good! (I think it is from a quick look)
1133,A,C++ pixel level control over graphics I have been looking all over the web for the simplest solution for this and currently I have come across nothing that seems simple enough for my needs. I am looking for a way to manipulate a matrix of pixels manually in C++ platform independent. Does anyone know of a library that is simple to use that will help me obtain this? Pixels within an application/window or any pixel on the screen? Please note that platform-independent pixel manipulation is next to impossible for graphics (as opposed to image manipulation) because there's no universally supported pixel format. You'll frequently find you need to special-case the actual processing having chosen your favourite of the formats the platform can display although many frameworks will guarantee you a format and do software conversion where necessary. So it depends whether you count that as pixel manipulation - you're manipulating some pixels in a buffer but different values are actually written to the display buffer. By virtue of it being platform independent you're probably not going to find a library that does only this. There are libraries like SDL and directFB that will let you do this but not without extra baggage. X11 may even be a better choice. It supports things you don't need but it also allows you to easily render pixels directly to the screen (or window as the case may be).  Use SDL Thank you very much for pointing out this library it looks extremely simple and quite effective!  Use OpenCV
1134,A,java graphics - a shape with two colours (this is java) I have an oval representing a unit. I want the colour of the oval to represent the unit's health. So a perfectly healthy unit will be all green. and with the unit's health decreasing the oval starts filling with red from the bottom. so on 50% health the oval would be red in bottom half and green in the top half and fully red when the unit's dead. I'm sure the solution here must be obvious and trivial  but I just can't see it. thanks a lot You can draw a red oval in the background then draw a green intersection of an oval and a rectangle where the rectangle starts below the oval then moves further to the top to reveal more of the red oval beneath. You might like to read up on how to construct complex shapes out of primitives here great! just what a noob like me needed :) thanks a lot  Override the paint method something like this: public void paint(Graphics graphics) { super.paint(graphics); Rectangle originalClipBounds = graphics.getClipBounds(); try { graphics.clipRect(100 100 100 25); graphics.setColor(Color.RED); graphics.fillOval(100 100 100 100); } finally { graphics.setClip(originalClipBounds); } try { graphics.clipRect(100 125 100 75); graphics.setColor(Color.BLUE); graphics.fillOval(100 100 100 100); } finally { graphics.setClip(originalClipBounds); } } Might want to enhance it with some double buffering but you get the gist.  You can set the clip on the graphics when you draw the green. Only things within the clip actually get painted.  public void paint(Graphics g) { super.paint(g); Graphics2D g2d = (Graphics2D)g.create(); g2d.setColor(Color.RED); g2d.fillOval(10 10 200 100); g2d.setColor(Color.GREEN); g2d.setClip(10 10 200 50); g2d.fillOval(10 10 200 100); }
